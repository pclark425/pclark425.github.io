<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8406 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8406</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8406</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-1db437a51571ed4bfbd5fb241d898b8c18c0fd04</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1db437a51571ed4bfbd5fb241d898b8c18c0fd04" target="_blank">Language Models Use Trigonometry to Do Addition</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, this work presents the first representation-level explanation of an LLM's mathematical capability.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the"Clock"algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8406.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8406.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter autoregressive transformer used as the primary model in this study; uses simple (non-gated) MLPs and tokenizes integers 0–361 as single tokens, which enabled neuron- and MLP-level mechanistic analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer, 6B parameters, simple MLP implementation MLP(x)=σ(x W_up) W_down, tokenizer treats integers 0–361 (with leading space) as single tokens; studied in-depth because of simple MLPs enabling neuron interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-token single-token-integer addition a+b with a,b ∈ [0,99] (single-token two-digit addition); also evaluated on a-23, integer division by 5, multiplication by 1.5 (even a), mod 2, and solving x-a=0.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Numbers represented as a generalized helix: a linear component plus Fourier (cos/sin) features with prominent periods T = [2,5,10,100]; model implements a Clock-style algorithm: attention heads move a and b helices to the last token, MLPs (14–18) manipulate a and b helices to build helix(a+b), and later MLPs (19–27) and some heads read helix(a+b) to output logits; neuron preactivations are periodic and can be fit with helix-inspired functions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>PCA to reduce dims; helix fitting via linear regression into a basis B(a) = [a, cos(2π/T_i a), sin(2π/T_i a)]; activation patching (Meng et al. style) using fitted helix or full activations; path patching to separate direct vs indirect effects; attribution/gradient-based patching (Kramár et al. style) to identify top neurons; neuron preactivation fitting by gradient descent to helix-inspired parametric forms; ablations of helix subspace (removing C^† components).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Addition accuracy: 80.5% on a+b for a,b ∈ [0,99]. Other tasks (GPT-J): a-23 = 89.61%, a//5 = 97.78%, a*1.5 (even a) = 80.00%, a mod 2 = 95.56%, x-a=0 (solve) = 95.56%. Causal metrics: helix fit yields max_l logit-difference (LD) 7.21 vs full layer patching 8.34 (Table 1); patching k=11 MLPs achieves ~95% of full MLP effect; patching k=20 attention heads restores ~83.9% of head-effect; top k≈4587 neurons (≈1% of neurons) sufficient to restore ~80% accuracy when mean-ablating others.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Most numeric errors are off-by-10 or +10 (≈45.7% off-by -10, 27.9% off-by +10 of incorrect numeric answers), indicating structured mistakes; failure to fully carry was a hypothesis but a chi-squared test falsified a carry-specific failure pattern; other failure modes include imperfect readout from helix(a+b) to logits, helix fit underperforming PCA on some tasks (division, multiplication, solving for x), and inability to isolate exact trig computations in MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fourier analysis of layer-0 residuals shows sparse frequency peaks at T=[2,5,10,100]; PCA shows a linear principal component for two-digit numbers; helix fits (with those T) project residuals into a low-parameter subspace that causally restores behavior in activation-patching experiments (helix-fit patching approaches full layer patching); attention-head and MLP-level activation/path patching identifies attention heads that transport a and b helices and MLPs that build and read a+b helix; neuron preactivations are fit well by helix-inspired functions and top neurons have Fourier periods matching T list; ablating helix dimensions is ~as damaging as ablating whole layer.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Helix fit underperforms PCA on integer division, multiplication by 1.5, and the x-solving task (Table 1), so helix is not a complete description for all numeric tasks; Llama3.1-8B shows weaker helix fit on last-token hidden states (suggesting alternate algorithms or differences due to gated MLPs); the precise algebraic/trigonometric operation used to combine helix(a) and helix(b) into helix(a+b) could not be isolated (authors hypothesize trigonometric identities but cannot conclusively demonstrate them).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8406.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6.9-billion-parameter autoregressive transformer from the Pythia suite; uses simple MLPs and single-token integer tokenization up to 557, replicated many helix/Clock findings in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pythia: A suite for analyzing large language models across training and scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer, ~6.9B parameters, simple MLPs, tokenizer treats integers up to 557 as single tokens; analyzed in appendices to replicate main GPT-J results.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same addition dataset a+b with a,b ∈ [0,99] (single-token two-digit addition); helix fitting and patching also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Also encodes numbers using generalized helix-like Fourier + linear features; authors find evidence that Pythia uses helix representations and Clock-like manipulation to perform addition (Appendix C.2 / D).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Helix fitting and activation patching, PCA baselines, replication of neuron/MLP/head patching analyses in appendices (same methods as GPT-J).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Addition accuracy: 77.2% on a+b for a,b ∈ [0,99]. Helix/circle fits outperform baselines at many parameter counts in Appendix C.2 (visual results replicated).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Paper reports similar qualitative error patterns; no detailed per-error statistics provided for Pythia in main text, but helix fits are causally relevant though exact fit strength may vary by layer.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Appendix shows helix fit and activation-patching replication on Pythia-6.9B (Fig. 19), indicating Fourier features and helix-subspace are causally implicated similarly to GPT-J.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No deep neuron-level mechanistic dissection shown in main text for Pythia; relative strength of helix fit vs PCA varies by task and layer as with GPT-J.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8406.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter autoregressive transformer with gated (gMLP) blocks; extremely high addition accuracy but shows weaker helix fits on some last-token representations, suggesting algorithmic variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The llama 3 herd of models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer, ~8B parameters, uses gated MLP (MLP(x)=σ(x W_gate) ◦ (x W_in) W_out), tokenizes integers up to 999 as single tokens; obtained 98.0% accuracy on the addition dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition a+b for a,b ∈ [0,99] and similar auxiliary numeric tasks (reported in appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Paper finds helix-like numerical features causally implicated on input tokens but weaker helix modeling of last-token hidden states compared to GPT-J and Pythia; authors hypothesize that gated MLPs allow alternative algorithmic implementations or additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same pipeline (helix fitting, activation patching, PCA baselines) applied in appendices; causal ablations of helix subspace show relevance on early layers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Addition accuracy: 98.0% on a+b for a,b ∈ [0,99]. Helix fits on input tokens are causal (Appendix C.2 / D) though last-token helix fits are weaker (Fig. 23).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Authors note Llama3 struggles with larger a,b values in heatmaps and that its last-token helix fits are weaker, implying that its internal algorithm might differ; limited neuron-level analysis prevents deeper error attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Activation-patching shows helical structure in the residual stream is causally relevant for the input tokens; ablation experiments show helix space ablations are damaging; weaker last-token fits are documented.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Weaker helix fit on last-token states versus GPT-J/Pythia suggests that Llama3 may implement a different arithmetic algorithm or use additional representational structure; gated MLPs complicate direct neuron-level interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8406.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Helix (generalized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized helix numerical representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric representation modeling a number's residual stream as a linear component plus multiple Fourier (cos/sin) components with learned projection C: helix(a) = C B(a)^T where B(a) = [a, cos(2π/T_i a), sin(2π/T_i a) ...].</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed in GPT-J, replicated in Pythia-6.9B and Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Feature manifold fit per-layer: PCA to 100 dims followed by linear regression into the helix basis with k Fourier features; prominent T = [2,5,10,100] identified and used; helix dimensions are approximately orthogonal; continuity tests project unseen numbers smoothly.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used for addition a+b and found causally relevant for subtraction, integer division by 5, multiplication by 1.5 (partial), modulo 2, and solving x-a=0 (varied success across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Representation-level description: numbers are embedded on a (generalized) helix: a shared linear direction plus multiple periodic sin/cos features (digits, parity, etc.), enabling angular-type addition/manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Helix fitting (linear regression after PCA), activation patching using the fitted helix (patch helix projection instead of full activation), ablation of helix subspace (removing C^† components), train/test split and randomized-order controls to rule out overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Helix-fit activation patching restores a large fraction of causal effect: for a+b, helix fit LD ≈ 7.21 vs full layer patch LD ≈ 8.34 (Table 1). Helix and circular fits often outperform PCA baseline in patching experiments (Fig. 4, Fig. 19). Ablating helix dims affects performance roughly as much as ablating the whole layer (Fig. 21).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Helix does not fully capture structure for some numeric tasks (underperforms PCA on division, multiplication, solving x); helix trained on randomized a is not causally relevant (sanity check); certain low-frequency components and linear component interactions (e.g., T=100 sin vs linear) complicate fit.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fourier spectrum of layer-0 residuals shows sparse peaks at T=[2,5,10] (and T=100 by inductive bias); PCA shows linear trends locally; helix-projection patching causally restores behavior; neuron-level Fourier decomposition matches helix periods; continuity experiments (projecting unseen numbers) show manifold behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not a complete description: some tasks require additional structure (PCA sometimes better); helix does not explain three-digit numbers (PC1 discontinuity at a=100), and cannot by itself reveal the precise arithmetic operator used in MLPs to combine helices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8406.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clock algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clock algorithm (helix rotation addition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithmic hypothesis where numbers embedded on circular/helix features are combined by angular (clock-like) manipulation — effectively rotating/combining sinusoidal components to produce helix(a+b), then reading out logits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (primary), evidence also in Pythia-6.9B and Llama3.1-8B (appendices)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Operational decomposition observed in GPT-J: (1) embed a and b as helices on their tokens; (2) attention heads (layers ~9–14) move these helices to last token; (3) MLPs 14–18 manipulate helices to form helix(a+b) (mixed heads may assist); (4) MLPs 19–27 and some attention heads read helix(a+b) and write logits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-digit single-token addition a+b (primary); conceptually applicable to modular/addition-like arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Mechanistic process: transport of helix features via attention, MLP nonlinearities combining Fourier components (authors hypothesize use of trig identities like cos(a+b)=cos a cos b - sin a sin b), creation of a+b helix in an internal subspace, then linearized readout to logits.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation and path patching at the attention-head, MLP, and neuron levels; head categorization by DE/TE and recoverability with helix models; neuron preactivation fitting and direct-effect path patching to locate senders/receivers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Circuit-level patching: k=11 MLPs (MLP14–27 subset) achieve ≈95% of MLP effect; k=20 attention heads restore ≈83.9% of head effect; helix(a+b) models last-token hidden states well at particular layers; overall model accuracies as reported per-model (e.g., GPT-J 80.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Authors cannot isolate the exact arithmetic computation (e.g., explicit trig multiplications) inside MLPs; mixed heads complicate a clean sender/receiver separation; Llama3 shows weaker last-token helix fits suggesting alternative algorithms; errors can arise either during helix construction (carry-like failures) or during readout to logits — empirical test falsified simple carry failure hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Last-token hidden states are well-modeled by helix(a,b,a+b); attention heads separate into a/b transport heads and a+b output heads; MLPs split into two functional groups (MLP14–18 build helix(a+b), MLP19–27 output logits) per DE/TE and helix(a+b)/TE metrics; neuron fits and Fourier spectra match the helix periods; ablations/patching show causal necessity of these components.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Cannot find explicit neuron-level implementation of trig identities; helix-based Clock story is less clear for Llama3; for some tasks PCA still outperforms helix; some attention heads are 'mixed' making a crisp decomposition messy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8406.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation patching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal intervention technique that swaps activations from a 'clean' run into a 'corrupted' run to measure the contribution of specific activations (tokens/layers/components) to output logits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Locating and editing factual associations in GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used across GPT-J, Pythia-6.9B, Llama3.1-8B analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Patch clean residual-stream activations (or fitted subspace activations) into corrupted prompts and measure logit difference for the correct answer; applied at token residuals, per-head outputs, per-MLP outputs, and groups of neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to addition and related numeric tasks to measure causal contributions of helix fits, PCA components, full activations, heads, MLPs, and neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Used to test whether a fitted representation (e.g., helix projection) contains the information the model uses and to quantify the causal effect (LD metric).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation swapping with averaging over many clean/corrupted pairs; comparisons to PCA, circular and polynomial fits; path patching and head/MLP/ neuron-level variants used to separate direct vs indirect effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Produces logit-difference (LD) and restoration-of-accuracy statistics; e.g., helix fits can approach full layer patch LD (a+b: helix 7.21 vs full 8.34), patching k heads/groups achieves given percentages of full effect (k=20 heads ≈83.9%); k=11 MLPs ≈95% effect.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Requires averaging and clean/complete prompts (only patch on prompts the model can successfully complete) to reduce noise; intervention granularity limited by compute (e.g., full per-neuron activation patching is expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Key method used to causally implicate helix subspace and to locate circuit components (heads, MLPs, neurons) that implement addition.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Patch-based results depend on choice of patched activations (full vs fitted); some downstream effects are indirect and hence interpretation requires path patching to separate mediation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8406.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path patching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of activation patching that isolates causal effects along specific computational edges (sender→receiver paths) to separate direct from mediated effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Localizing model behavior with path patching.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-J attention heads and MLPs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Patch only the activation on the path from a sender node (e.g., an attention head) to a receiver node (e.g., an MLP) and measure how much of the sender's total effect on the final logits is recoverable along that path.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to determine which heads write to which MLPs and which components MLPs rely on during addition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Enables categorization of heads into a/b transport heads, a+b output heads, and mixed heads by measuring how much of their effect is direct vs mediated and which downstream components they influence.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Path-specific activation swaps and measuring downstream component LD; used in conjunction with head categorization metrics and helix-model recoverability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to show that a+b heads rely predominantly on upstream MLPs, a,b heads impact downstream MLPs more, and mixed heads take input from both a,b heads and upstream MLPs (Figs. 29 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Path-level decomposition can still be fuzzy when multiple parallel paths exist; mixed heads reduce neat sender/receiver separation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Path patching supports the Clock decomposition by showing which components send helix features to which downstream MLPs and which heads receive from upstream MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Mixed heads and indirect mediation complicate interpretation; path patching is computationally expensive at fine granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8406.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8406.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuron helix fit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Helix-inspired neuron preactivation fit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric fit of individual neuron preactivations as sums of linear terms in a, b, a+b and cos/sin terms with periods T = [2,5,10,100]; used to explain neuron-level readout of helix features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Analyzed primarily in GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>For neuron n in layer l, model N_n^l(a,b) = Σ_{t∈{a,b,a+b}} c_t t + Σ_{T∈[2,5,10,100]} c_{T,t} cos(2π/T (t - d_{T,t})), with coefficients fit by gradient descent; fitted only for a sparse set (~1% ≈ 4587) of top-impact neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to model neuron preactivations across addition dataset (a+b) and used to distinguish neurons that read a/b vs a+b features.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shows neurons read sinusoidal and linear helix components (periodic in a,b,a+b); neurons in MLPs14–18 read a,b helices and have fits dominated by a/b terms, whereas neurons in MLPs19–27 have fits dominated by a+b terms and higher DE/TE ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fit neuron preactivations to parametric helix-inspired functions via SGD; evaluate by patching fitted preactivations into model while mean-ablating other neurons and measuring accuracy/logit-difference; compute DE/TE by path patching per neuron.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Patching fitted preactivations for top k neurons recovers ~75% of the performance of patching their actual preactivations (Fig. 9); more impactful neurons tend to have lower NRMSE under the fit (Fig. 35). ~700 neurons achieve 80% of direct effect; ~84% of top DE neurons are after layer 18.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fitted neurons do not capture 100% of behavior; mean-ablating some neurons increases average logit for correct answer but not accuracy (asymmetric effects). The fit is approximate and cannot fully reconstruct exact nonlinear interactions inside MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Neuron preactivations' Fourier decomposition matches helix periods; neuron-level fits align with MLP-level functional split (build vs read helix); neuron patching experiments show fitted neurons causally important.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Fitting all neurons is computationally infeasible; fits rely on chosen periods T; some neurons' behavior remains unexplained and some ablations have non-intuitive aggregate effects on accuracy vs logit-difference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Use Trigonometry to Do Addition', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Pre-trained large language models use fourier features to compute addition <em>(Rating: 2)</em></li>
                <li>The clock and the pizza: Two stories in mechanistic explanation of neural networks <em>(Rating: 2)</em></li>
                <li>Arithmetic without algorithms: Language models solve math with a bag of heuristics <em>(Rating: 2)</em></li>
                <li>Localizing model behavior with path patching <em>(Rating: 1)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8406",
    "paper_id": "paper-1db437a51571ed4bfbd5fb241d898b8c18c0fd04",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-J",
            "name_full": "GPT-J (6B)",
            "brief_description": "A 6-billion-parameter autoregressive transformer used as the primary model in this study; uses simple (non-gated) MLPs and tokenizes integers 0–361 as single tokens, which enabled neuron- and MLP-level mechanistic analysis.",
            "citation_title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
            "mention_or_use": "use",
            "model_name": "GPT-J (6B)",
            "model_description": "Autoregressive transformer, 6B parameters, simple MLP implementation MLP(x)=σ(x W_up) W_down, tokenizer treats integers 0–361 (with leading space) as single tokens; studied in-depth because of simple MLPs enabling neuron interpretation.",
            "arithmetic_task_type": "Two-token single-token-integer addition a+b with a,b ∈ [0,99] (single-token two-digit addition); also evaluated on a-23, integer division by 5, multiplication by 1.5 (even a), mod 2, and solving x-a=0.",
            "mechanism_or_representation": "Numbers represented as a generalized helix: a linear component plus Fourier (cos/sin) features with prominent periods T = [2,5,10,100]; model implements a Clock-style algorithm: attention heads move a and b helices to the last token, MLPs (14–18) manipulate a and b helices to build helix(a+b), and later MLPs (19–27) and some heads read helix(a+b) to output logits; neuron preactivations are periodic and can be fit with helix-inspired functions.",
            "probing_or_intervention_method": "PCA to reduce dims; helix fitting via linear regression into a basis B(a) = [a, cos(2π/T_i a), sin(2π/T_i a)]; activation patching (Meng et al. style) using fitted helix or full activations; path patching to separate direct vs indirect effects; attribution/gradient-based patching (Kramár et al. style) to identify top neurons; neuron preactivation fitting by gradient descent to helix-inspired parametric forms; ablations of helix subspace (removing C^† components).",
            "performance_metrics": "Addition accuracy: 80.5% on a+b for a,b ∈ [0,99]. Other tasks (GPT-J): a-23 = 89.61%, a//5 = 97.78%, a*1.5 (even a) = 80.00%, a mod 2 = 95.56%, x-a=0 (solve) = 95.56%. Causal metrics: helix fit yields max_l logit-difference (LD) 7.21 vs full layer patching 8.34 (Table 1); patching k=11 MLPs achieves ~95% of full MLP effect; patching k=20 attention heads restores ~83.9% of head-effect; top k≈4587 neurons (≈1% of neurons) sufficient to restore ~80% accuracy when mean-ablating others.",
            "error_types_or_failure_modes": "Most numeric errors are off-by-10 or +10 (≈45.7% off-by -10, 27.9% off-by +10 of incorrect numeric answers), indicating structured mistakes; failure to fully carry was a hypothesis but a chi-squared test falsified a carry-specific failure pattern; other failure modes include imperfect readout from helix(a+b) to logits, helix fit underperforming PCA on some tasks (division, multiplication, solving for x), and inability to isolate exact trig computations in MLPs.",
            "evidence_for_mechanism": "Fourier analysis of layer-0 residuals shows sparse frequency peaks at T=[2,5,10,100]; PCA shows a linear principal component for two-digit numbers; helix fits (with those T) project residuals into a low-parameter subspace that causally restores behavior in activation-patching experiments (helix-fit patching approaches full layer patching); attention-head and MLP-level activation/path patching identifies attention heads that transport a and b helices and MLPs that build and read a+b helix; neuron preactivations are fit well by helix-inspired functions and top neurons have Fourier periods matching T list; ablating helix dimensions is ~as damaging as ablating whole layer.",
            "counterexamples_or_challenges": "Helix fit underperforms PCA on integer division, multiplication by 1.5, and the x-solving task (Table 1), so helix is not a complete description for all numeric tasks; Llama3.1-8B shows weaker helix fit on last-token hidden states (suggesting alternate algorithms or differences due to gated MLPs); the precise algebraic/trigonometric operation used to combine helix(a) and helix(b) into helix(a+b) could not be isolated (authors hypothesize trigonometric identities but cannot conclusively demonstrate them).",
            "uuid": "e8406.0",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Pythia-6.9B",
            "name_full": "Pythia-6.9B",
            "brief_description": "A 6.9-billion-parameter autoregressive transformer from the Pythia suite; uses simple MLPs and single-token integer tokenization up to 557, replicated many helix/Clock findings in appendices.",
            "citation_title": "Pythia: A suite for analyzing large language models across training and scaling.",
            "mention_or_use": "use",
            "model_name": "Pythia-6.9B",
            "model_description": "Autoregressive transformer, ~6.9B parameters, simple MLPs, tokenizer treats integers up to 557 as single tokens; analyzed in appendices to replicate main GPT-J results.",
            "arithmetic_task_type": "Same addition dataset a+b with a,b ∈ [0,99] (single-token two-digit addition); helix fitting and patching also tested.",
            "mechanism_or_representation": "Also encodes numbers using generalized helix-like Fourier + linear features; authors find evidence that Pythia uses helix representations and Clock-like manipulation to perform addition (Appendix C.2 / D).",
            "probing_or_intervention_method": "Helix fitting and activation patching, PCA baselines, replication of neuron/MLP/head patching analyses in appendices (same methods as GPT-J).",
            "performance_metrics": "Addition accuracy: 77.2% on a+b for a,b ∈ [0,99]. Helix/circle fits outperform baselines at many parameter counts in Appendix C.2 (visual results replicated).",
            "error_types_or_failure_modes": "Paper reports similar qualitative error patterns; no detailed per-error statistics provided for Pythia in main text, but helix fits are causally relevant though exact fit strength may vary by layer.",
            "evidence_for_mechanism": "Appendix shows helix fit and activation-patching replication on Pythia-6.9B (Fig. 19), indicating Fourier features and helix-subspace are causally implicated similarly to GPT-J.",
            "counterexamples_or_challenges": "No deep neuron-level mechanistic dissection shown in main text for Pythia; relative strength of helix fit vs PCA varies by task and layer as with GPT-J.",
            "uuid": "e8406.1",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3.1-8B",
            "name_full": "Llama3.1-8B",
            "brief_description": "An 8-billion-parameter autoregressive transformer with gated (gMLP) blocks; extremely high addition accuracy but shows weaker helix fits on some last-token representations, suggesting algorithmic variations.",
            "citation_title": "The llama 3 herd of models",
            "mention_or_use": "use",
            "model_name": "Llama3.1-8B",
            "model_description": "Autoregressive transformer, ~8B parameters, uses gated MLP (MLP(x)=σ(x W_gate) ◦ (x W_in) W_out), tokenizes integers up to 999 as single tokens; obtained 98.0% accuracy on the addition dataset.",
            "arithmetic_task_type": "Addition a+b for a,b ∈ [0,99] and similar auxiliary numeric tasks (reported in appendices).",
            "mechanism_or_representation": "Paper finds helix-like numerical features causally implicated on input tokens but weaker helix modeling of last-token hidden states compared to GPT-J and Pythia; authors hypothesize that gated MLPs allow alternative algorithmic implementations or additional structure.",
            "probing_or_intervention_method": "Same pipeline (helix fitting, activation patching, PCA baselines) applied in appendices; causal ablations of helix subspace show relevance on early layers.",
            "performance_metrics": "Addition accuracy: 98.0% on a+b for a,b ∈ [0,99]. Helix fits on input tokens are causal (Appendix C.2 / D) though last-token helix fits are weaker (Fig. 23).",
            "error_types_or_failure_modes": "Authors note Llama3 struggles with larger a,b values in heatmaps and that its last-token helix fits are weaker, implying that its internal algorithm might differ; limited neuron-level analysis prevents deeper error attribution.",
            "evidence_for_mechanism": "Activation-patching shows helical structure in the residual stream is causally relevant for the input tokens; ablation experiments show helix space ablations are damaging; weaker last-token fits are documented.",
            "counterexamples_or_challenges": "Weaker helix fit on last-token states versus GPT-J/Pythia suggests that Llama3 may implement a different arithmetic algorithm or use additional representational structure; gated MLPs complicate direct neuron-level interpretation.",
            "uuid": "e8406.2",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Helix (generalized)",
            "name_full": "Generalized helix numerical representation",
            "brief_description": "A parametric representation modeling a number's residual stream as a linear component plus multiple Fourier (cos/sin) components with learned projection C: helix(a) = C B(a)^T where B(a) = [a, cos(2π/T_i a), sin(2π/T_i a) ...].",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed in GPT-J, replicated in Pythia-6.9B and Llama3.1-8B",
            "model_description": "Feature manifold fit per-layer: PCA to 100 dims followed by linear regression into the helix basis with k Fourier features; prominent T = [2,5,10,100] identified and used; helix dimensions are approximately orthogonal; continuity tests project unseen numbers smoothly.",
            "arithmetic_task_type": "Used for addition a+b and found causally relevant for subtraction, integer division by 5, multiplication by 1.5 (partial), modulo 2, and solving x-a=0 (varied success across tasks).",
            "mechanism_or_representation": "Representation-level description: numbers are embedded on a (generalized) helix: a shared linear direction plus multiple periodic sin/cos features (digits, parity, etc.), enabling angular-type addition/manipulation.",
            "probing_or_intervention_method": "Helix fitting (linear regression after PCA), activation patching using the fitted helix (patch helix projection instead of full activation), ablation of helix subspace (removing C^† components), train/test split and randomized-order controls to rule out overfitting.",
            "performance_metrics": "Helix-fit activation patching restores a large fraction of causal effect: for a+b, helix fit LD ≈ 7.21 vs full layer patch LD ≈ 8.34 (Table 1). Helix and circular fits often outperform PCA baseline in patching experiments (Fig. 4, Fig. 19). Ablating helix dims affects performance roughly as much as ablating the whole layer (Fig. 21).",
            "error_types_or_failure_modes": "Helix does not fully capture structure for some numeric tasks (underperforms PCA on division, multiplication, solving x); helix trained on randomized a is not causally relevant (sanity check); certain low-frequency components and linear component interactions (e.g., T=100 sin vs linear) complicate fit.",
            "evidence_for_mechanism": "Fourier spectrum of layer-0 residuals shows sparse peaks at T=[2,5,10] (and T=100 by inductive bias); PCA shows linear trends locally; helix-projection patching causally restores behavior; neuron-level Fourier decomposition matches helix periods; continuity experiments (projecting unseen numbers) show manifold behavior.",
            "counterexamples_or_challenges": "Not a complete description: some tasks require additional structure (PCA sometimes better); helix does not explain three-digit numbers (PC1 discontinuity at a=100), and cannot by itself reveal the precise arithmetic operator used in MLPs to combine helices.",
            "uuid": "e8406.3",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Clock algorithm",
            "name_full": "Clock algorithm (helix rotation addition)",
            "brief_description": "An algorithmic hypothesis where numbers embedded on circular/helix features are combined by angular (clock-like) manipulation — effectively rotating/combining sinusoidal components to produce helix(a+b), then reading out logits.",
            "citation_title": "Progress measures for grokking via mechanistic interpretability.",
            "mention_or_use": "use",
            "model_name": "GPT-J (primary), evidence also in Pythia-6.9B and Llama3.1-8B (appendices)",
            "model_description": "Operational decomposition observed in GPT-J: (1) embed a and b as helices on their tokens; (2) attention heads (layers ~9–14) move these helices to last token; (3) MLPs 14–18 manipulate helices to form helix(a+b) (mixed heads may assist); (4) MLPs 19–27 and some attention heads read helix(a+b) and write logits.",
            "arithmetic_task_type": "Two-digit single-token addition a+b (primary); conceptually applicable to modular/addition-like arithmetic.",
            "mechanism_or_representation": "Mechanistic process: transport of helix features via attention, MLP nonlinearities combining Fourier components (authors hypothesize use of trig identities like cos(a+b)=cos a cos b - sin a sin b), creation of a+b helix in an internal subspace, then linearized readout to logits.",
            "probing_or_intervention_method": "Activation and path patching at the attention-head, MLP, and neuron levels; head categorization by DE/TE and recoverability with helix models; neuron preactivation fitting and direct-effect path patching to locate senders/receivers.",
            "performance_metrics": "Circuit-level patching: k=11 MLPs (MLP14–27 subset) achieve ≈95% of MLP effect; k=20 attention heads restore ≈83.9% of head effect; helix(a+b) models last-token hidden states well at particular layers; overall model accuracies as reported per-model (e.g., GPT-J 80.5%).",
            "error_types_or_failure_modes": "Authors cannot isolate the exact arithmetic computation (e.g., explicit trig multiplications) inside MLPs; mixed heads complicate a clean sender/receiver separation; Llama3 shows weaker last-token helix fits suggesting alternative algorithms; errors can arise either during helix construction (carry-like failures) or during readout to logits — empirical test falsified simple carry failure hypothesis.",
            "evidence_for_mechanism": "Last-token hidden states are well-modeled by helix(a,b,a+b); attention heads separate into a/b transport heads and a+b output heads; MLPs split into two functional groups (MLP14–18 build helix(a+b), MLP19–27 output logits) per DE/TE and helix(a+b)/TE metrics; neuron fits and Fourier spectra match the helix periods; ablations/patching show causal necessity of these components.",
            "counterexamples_or_challenges": "Cannot find explicit neuron-level implementation of trig identities; helix-based Clock story is less clear for Llama3; for some tasks PCA still outperforms helix; some attention heads are 'mixed' making a crisp decomposition messy.",
            "uuid": "e8406.4",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Activation patching",
            "name_full": "Activation patching",
            "brief_description": "A causal intervention technique that swaps activations from a 'clean' run into a 'corrupted' run to measure the contribution of specific activations (tokens/layers/components) to output logits.",
            "citation_title": "Locating and editing factual associations in GPT.",
            "mention_or_use": "use",
            "model_name": "Used across GPT-J, Pythia-6.9B, Llama3.1-8B analyses",
            "model_description": "Patch clean residual-stream activations (or fitted subspace activations) into corrupted prompts and measure logit difference for the correct answer; applied at token residuals, per-head outputs, per-MLP outputs, and groups of neurons.",
            "arithmetic_task_type": "Applied to addition and related numeric tasks to measure causal contributions of helix fits, PCA components, full activations, heads, MLPs, and neurons.",
            "mechanism_or_representation": "Used to test whether a fitted representation (e.g., helix projection) contains the information the model uses and to quantify the causal effect (LD metric).",
            "probing_or_intervention_method": "Activation swapping with averaging over many clean/corrupted pairs; comparisons to PCA, circular and polynomial fits; path patching and head/MLP/ neuron-level variants used to separate direct vs indirect effects.",
            "performance_metrics": "Produces logit-difference (LD) and restoration-of-accuracy statistics; e.g., helix fits can approach full layer patch LD (a+b: helix 7.21 vs full 8.34), patching k heads/groups achieves given percentages of full effect (k=20 heads ≈83.9%); k=11 MLPs ≈95% effect.",
            "error_types_or_failure_modes": "Requires averaging and clean/complete prompts (only patch on prompts the model can successfully complete) to reduce noise; intervention granularity limited by compute (e.g., full per-neuron activation patching is expensive).",
            "evidence_for_mechanism": "Key method used to causally implicate helix subspace and to locate circuit components (heads, MLPs, neurons) that implement addition.",
            "counterexamples_or_challenges": "Patch-based results depend on choice of patched activations (full vs fitted); some downstream effects are indirect and hence interpretation requires path patching to separate mediation.",
            "uuid": "e8406.5",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Path patching",
            "name_full": "Path patching",
            "brief_description": "A variant of activation patching that isolates causal effects along specific computational edges (sender→receiver paths) to separate direct from mediated effects.",
            "citation_title": "Localizing model behavior with path patching.",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-J attention heads and MLPs",
            "model_description": "Patch only the activation on the path from a sender node (e.g., an attention head) to a receiver node (e.g., an MLP) and measure how much of the sender's total effect on the final logits is recoverable along that path.",
            "arithmetic_task_type": "Used to determine which heads write to which MLPs and which components MLPs rely on during addition.",
            "mechanism_or_representation": "Enables categorization of heads into a/b transport heads, a+b output heads, and mixed heads by measuring how much of their effect is direct vs mediated and which downstream components they influence.",
            "probing_or_intervention_method": "Path-specific activation swaps and measuring downstream component LD; used in conjunction with head categorization metrics and helix-model recoverability.",
            "performance_metrics": "Used to show that a+b heads rely predominantly on upstream MLPs, a,b heads impact downstream MLPs more, and mixed heads take input from both a,b heads and upstream MLPs (Figs. 29 etc.).",
            "error_types_or_failure_modes": "Path-level decomposition can still be fuzzy when multiple parallel paths exist; mixed heads reduce neat sender/receiver separation.",
            "evidence_for_mechanism": "Path patching supports the Clock decomposition by showing which components send helix features to which downstream MLPs and which heads receive from upstream MLPs.",
            "counterexamples_or_challenges": "Mixed heads and indirect mediation complicate interpretation; path patching is computationally expensive at fine granularity.",
            "uuid": "e8406.6",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Neuron helix fit",
            "name_full": "Helix-inspired neuron preactivation fit",
            "brief_description": "A parametric fit of individual neuron preactivations as sums of linear terms in a, b, a+b and cos/sin terms with periods T = [2,5,10,100]; used to explain neuron-level readout of helix features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Analyzed primarily in GPT-J",
            "model_description": "For neuron n in layer l, model N_n^l(a,b) = Σ_{t∈{a,b,a+b}} c_t t + Σ_{T∈[2,5,10,100]} c_{T,t} cos(2π/T (t - d_{T,t})), with coefficients fit by gradient descent; fitted only for a sparse set (~1% ≈ 4587) of top-impact neurons.",
            "arithmetic_task_type": "Used to model neuron preactivations across addition dataset (a+b) and used to distinguish neurons that read a/b vs a+b features.",
            "mechanism_or_representation": "Shows neurons read sinusoidal and linear helix components (periodic in a,b,a+b); neurons in MLPs14–18 read a,b helices and have fits dominated by a/b terms, whereas neurons in MLPs19–27 have fits dominated by a+b terms and higher DE/TE ratios.",
            "probing_or_intervention_method": "Fit neuron preactivations to parametric helix-inspired functions via SGD; evaluate by patching fitted preactivations into model while mean-ablating other neurons and measuring accuracy/logit-difference; compute DE/TE by path patching per neuron.",
            "performance_metrics": "Patching fitted preactivations for top k neurons recovers ~75% of the performance of patching their actual preactivations (Fig. 9); more impactful neurons tend to have lower NRMSE under the fit (Fig. 35). ~700 neurons achieve 80% of direct effect; ~84% of top DE neurons are after layer 18.",
            "error_types_or_failure_modes": "Fitted neurons do not capture 100% of behavior; mean-ablating some neurons increases average logit for correct answer but not accuracy (asymmetric effects). The fit is approximate and cannot fully reconstruct exact nonlinear interactions inside MLPs.",
            "evidence_for_mechanism": "Neuron preactivations' Fourier decomposition matches helix periods; neuron-level fits align with MLP-level functional split (build vs read helix); neuron patching experiments show fitted neurons causally important.",
            "counterexamples_or_challenges": "Fitting all neurons is computationally infeasible; fits rely on chosen periods T; some neurons' behavior remains unexplained and some ablations have non-intuitive aggregate effects on accuracy vs logit-difference.",
            "uuid": "e8406.7",
            "source_info": {
                "paper_title": "Language Models Use Trigonometry to Do Addition",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2
        },
        {
            "paper_title": "Pre-trained large language models use fourier features to compute addition",
            "rating": 2
        },
        {
            "paper_title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks",
            "rating": 2
        },
        {
            "paper_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "rating": 2
        },
        {
            "paper_title": "Localizing model behavior with path patching",
            "rating": 1
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1
        }
    ],
    "cost": 0.022171749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models Use Trigonometry to Do Addition</h1>
<p>Subhash Kantamneni ${ }^{1}$ Max Tegmark ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the "Clock" algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) display surprising and significant aptitude for mathematical reasoning (Ahn et al., 2024; Satpute et al., 2024), which is increasingly seen as a benchmark for LLM capabilities (OpenAI; Glazer et al., 2024). Despite LLMs' mathematical proficiency, we have limited understanding of how LLMs process even simple mathematical tasks like addition. Understanding mathematical reasoning is valuable for ensuring LLMs' reliability, interpretability, and alignment in high-stakes applications.</p>
<p>In this study, we reverse engineer how GPT-J, Pythia-6.9B, and Llama3.1-8B compute the addition problem $a+b$ for $a, b \in[0,99]$. Remarkably, we find that LLMs use a form</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustrating the Clock algorithm. We find that LLMs represent numbers on a helix. When computing the addition problem $a+b$, LLMs rotate the $a$ and $b$ helices, as if on a clock, to create the $a+b$ helix and read out the final answer.
of the "Clock" algorithm to compute addition, which was previously proposed by Nanda et al. (2023a) as a mechanistic explanation of how one layer transformers compute modular addition (and later named by Zhong et al. (2023)).</p>
<p>To compute $a+b$, all three LLMs represent $a$ and $b$ as a helix on their tokens and construct helix $(a+b)$ on the last token, which we verify with causal interventions. We then focus on how GPT-J implements the Clock algorithm by investigating MLPs, attention heads, and even specific neurons. We find that these components can be understood as either constructing the $a+b$ helix by manipulating the $a$ and $b$ helices, or using the $a+b$ helix to produce the answer in the model's logits. We visualize this procedure in Fig. 1 as rotating the dial of a clock.</p>
<p>Our work is in the spirit of mechanistic interpretability (MI), which attempts to reverse engineer the functionality of machine learning models. However, most LLM MI research focuses either on identifying circuits, which are the minimal set of model components required for computations,</p>
<p>or understanding features, which are the representations of concepts in LLMs. A true mechanistic explanation requires understanding both how an LLM represents a feature and how downstream components manipulate that feature to complete a task. To our knowledge, we are the first work to present this type of description of an LLM’s mathematical capability, identifying that LLMs represent numbers as helices and compute addition by manipulating these helices with the interpretable Clock algorithm.</p>
<h2>2. Related Work</h2>
<p>Circuits. Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities (Olah et al., 2020; Elhage et al., 2021). For example, Olsson et al. (2022) found that in-context learning is primarily driven by induction attention heads, and Wang et al. (2023) identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with Nanda et al. (2023b) attempting to understand how MLPs are used in factual recall, and Hanna et al. (2023) investigating MLP outputs while studying the greater-than operation in GPT-2.</p>
<p>Features. Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions (Park et al., 2023; Elhage et al., 2022), culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features (Huben et al., 2024; Bricken et al., 2023; Templeton et al., 2024; Gao et al., 2024; Rajamanoharan et al., 2024). However, recent work from Engels et al. (2024) found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. Levy &amp; Geva (2024) and Zhu et al. (2025) model LLMs’ representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with Marks et al. (2024) constructing circuits from SAE features and Makelov et al. (2024) using attention-based SAEs to identify the features used in Wang et al. (2023)’s IOI task.</p>
<p>Reverse engineering addition. Liu et al. (2022) first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, Nanda et al. (2023a) introduced the “Clock” algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, Zhong et al. (2023) found the “Pizza” algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models. Stolfo et al. (2023) identifies the circuit used by LLMs in addition problems, and Nikankin et al. (2024) claims that LLMs use heuristics implemented by specific neurons rather than a definite algorithm to compute arithmetic. Zhou et al. (2024) analyze a fine-tuned GPT-2 and found that Fourier components in numerical representations are critical for addition, while providing preliminary results that larger base LLMs might use similar features.</p>
<h2>3. Problem Setup</h2>
<p>Models As in Nikankin et al. (2024), we analyze 3 LLMs: GPT-J (6B parameters) (Wang &amp; Komatsuzaki, 2021), Pythia-6.9B (Biderman et al., 2023), and Llama3.1-8B (Grattafiori et al., 2024). All three models are autoregressive transformers which process tokens $x_{0},...,x_{n}$ to produce probability distributions over the likely next token $x_{n+1}$ (Vaswani et al., 2017). The $i$ th token is embedded as $L$ hidden state vectors (also known as the residual stream), where $L$ is the number of layers in the transformer. Each hidden state is the sum of multilayer perceptron (MLP) and attention (attn) layers.</p>
<p>$$
\begin{aligned}
h_{i}^{l} &amp; =h_{i}^{l-1}+a_{i}^{l}+m_{i}^{l}, \
a_{i}^{l} &amp; =\operatorname{attn}^{l}\left(h_{1}^{l-1},h_{2}^{l-1}, \ldots, h_{i}^{l-1}\right), \
m_{i}^{l} &amp; =\operatorname{MLP}^{l}\left(a_{i}^{(l)}+h_{i}^{l-1}\right) .
\end{aligned}
$$</p>
<p>GPT-J and Pythia-6.9B use simple MLP implementations, namely $\operatorname{MLP}(x)=\sigma\left(x W_{\text {up }}\right) W_{\text {down }}$, where $\sigma(x)$ is the sigmoid function. Llama3.1-8B uses a gated MLP, $\operatorname{MLP}(x)=\sigma\left(x W_{\text {gate }}\right) \circ\left(x W_{\text {in }}\right) W_{\text {out }}$, where $\circ$ represents the Hadamard product (Liu et al., 2021). GPT-J tokenizes the numbers $[0,361]$ (with a space) as a single token, Pythia6.9B tokenizes $[0,557]$ as a single token, and Llama3.1-8B tokenizes $[0,999]$ as a single token. We focus on the singletoken regime for simplicity.</p>
<p>Data To ensure that answers require only a single token for all models, we construct problems $a+b$ for integers $a, b \in[0,99]$. We evaluate all three models on these 10,000 addition problems, and find that all models can competently complete the task: GPT-J achieves 80.5\% accuracy, Pythia6.9B achieves 77.2\% accuracy, and Llama3.1-8B achieves 98.0\% accuracy. For the prompts used and each model’s performance heatmap by $a$ and $b$, see Appendix A. Despite Llama3.1-8B’s impressive performance, in the main paper we focus our analysis on GPT-J because its simple MLP allows for easier neuron interpretation. We report similar results for Pythia-6.9B and Llama3.1-8B in the Appendix.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Number representations are both periodic and linear. Top The residual stream after layer 0 in GPT-J is sparse in the Fourier domain when batching the hidden states for $a \in[0,360]$ together. We average the magnitude of the Fourier transform of the batched matrix $h_{360}^{0}$ across the model dimension. Bottom In addition to this periodicity, the first PCA component is roughly linear in $a$ for $a \in[0,99]$.</p>
<h2>4. LLMs Represent Numbers as a Helix</h2>
<p>To generate a ground up understanding of how LLMs compute $a+b$, we first aim to understand how LLMs represent numbers. To identify representational trends, we run GPT-J on the single-token integers $a \in[0,360]$. We do not use $a=361$ because 360 has more integer divisors, allowing for a simpler analysis of periodic structure. We conduct analysis on $h_{360}^{0}$, which is the residual stream following layer 0 with shape [360, model_dim]. We choose to use the output of layer 0 rather than directly analyzing the embeddings because prior work has shown that processing in layer 0 is influential for numerical tasks (Nikankin et al., 2024).</p>
<h3>4.1. Investigating Numerical Structure</h3>
<p>Linear structure. To investigate structure in numerical representations, we perform a PCA (F.R.S., 1901) on $h_{360}^{0}$ and find that the first principal component (PC1) for $a \in[0,360]$ has a sharp discontinuity at $a=100$ (Fig. 15, Appendix B), which implies that GPT-J uses a distinct representation for three-digit integers. Instead, in the bottom of Fig. 2, we plot PC1 for $h_{99}^{0}$ and find that it is well approximated by a line in $a$. Additionally, when plotting the Euclidean distance between $a$ and $a+\delta n$ for $a \in[0,9]$ (Fig. 14, Appendix B), we see that the distance is locally linear. The existence of linear structure is unsurprising - numbers are semantically linear, and LLMs often represent concepts linearly.</p>
<p>Periodic Structure. We center and apply a Fourier transform to $h_{360}^{0}$ with respect to the number $a$ being represented and the model_dim. In Fig. 2, we average the resulting spectra across model_dim and observe a sparse Fourier domain with high-frequency components at $T=[2,5,10]$.</p>
<p>Additionally, when we compare the residual streams of all pairs of integers $a_{1}$ and $a_{2}$, we see that there is distinct periodicity in both their Euclidean distance and cosine similarity (Fig. 13, Appendix B). These Fourier features were also identified by Zhou et al. (2024), and although initially surprising, are sensible. The units digit of numbers in base 10 is periodic $(T=10)$, and it is reasonable that qualities like evenness $(T=2)$ are useful for tasks.</p>
<h3>4.2. Parameterizing the Structure as a Helix</h3>
<p>To account for both the periodic and linear structure in numbers, we propose that numbers can be modeled helically. Namely, we posit that $h_{a}^{l}$, the residual stream immediately preceding layer $l$ for some number $a$, can be modeled as</p>
<p>$$
\begin{aligned}
h_{a}^{l}= &amp; \operatorname{helix}(a)=C B(a)^{T} \
B(a)= &amp; {\left[a, \cos \left(\frac{2 \pi}{T_{1}} a\right), \sin \left(\frac{2 \pi}{T_{1}} a\right)\right.} \
&amp; \left.\ldots, \cos \left(\frac{2 \pi}{T_{k}} a\right), \sin \left(\frac{2 \pi}{T_{k}} a\right)\right]
\end{aligned}
$$</p>
<p>$C$ is a matrix applied to the basis of functions $B(a)$, where $B(a)$ uses $k$ Fourier features with periods $T=\left[T_{1}, \ldots T_{k}\right]$. The $k=1$ case represents a regular helix; for $k&gt;1$, the independent Fourier features share a single linear direction. We refer to this structure as a generalized helix, or simply a helix for brevity.</p>
<p>We identify four major Fourier features: $T=[2,5,10,100]$. We use the periods $T=[2,5,10]$ because they have significant high frequency components in Fig. 2. We are cautious of low frequency Fourier components, and use $T=100$ both because of its significant magnitude, and by applying the inductive bias that our number system is base 10.</p>
<h3>4.3. Fitting a Helix</h3>
<p>We fit our helical form to the residual streams on top of the $a$ token for our $a+b$ dataset. In practice, we first use PCA to project the residual stream at each layer to 100 dimensions. To ensure we do not overfit with Fourier features, we consider all combinations of $k$ Fourier features, with $k \in[1,4]$. If we use $k$ Fourier features, the helical fit uses $2 k+1$ basis functions (one linear component, $2 k$ periodic components). We then use linear regression to find some coefficient matrix $C_{\mathrm{PCA}}$ of shape $100 \times 2 k+1$ that best satisfies $\operatorname{PCA}\left(h_{a}^{l}\right)=C_{\mathrm{PCA}} B(a)^{T}$. Finally, we use the inverse PCA transformation to project $C_{\mathrm{PCA}}$ back into the model's full residual stream dimensionality to find $C$.</p>
<p>We visualize the quality of our fit for layer 0 when using all $k=4$ Fourier features with $T=[2,5,10,100]$ in Fig. 3. To do so, we calculate $C^{\dagger} h$, where $C^{\dagger}$ is the MoorePenrose pseudo-inverse of $C$. Thus, $C^{\dagger} h$ represents the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Helix subspace visualized. For GPT-J's layer 0 output, we project each of the numbers $a \in[0,99]$ onto our fitted $T=$ $[2,5,10,100]$ helix subspace, and visualize it. In the top row, we plot $\sin \left(\frac{2 \pi}{T_{i}} a\right)$ vs $\cos \left(\frac{2 \pi}{T_{i}} a\right)$ for each $T_{i} \in T$ and plot all $a$ congruent under $a \bmod T$ in the same color and annotate their mean. The bottom row contains the linear component subplot.
projection of the residual stream into the helical subspace. When analyzing the columns of $C$, we find that the Fourier features increase in magnitude with period and are mostly orthogonal (Appendix C.1).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Helix causal intervention results. We use activation patching to causally determine if our fits preserve the information the model uses to compute $a+b$. We find that our helical and circular fits are strongly causally implicated, often outperforming the PCA baseline.</p>
<h3>4.4. Evaluating the Quality of the Helical Fit</h3>
<p>We want to causally demonstrate that the model actually uses the fitted helix. To do so, we employ activation patching. Activation patching isolates the contribution of specific model components towards answer tokens (Meng et al., 2022; Heimersheim \&amp; Nanda, 2024). Specifically, to evaluate the contribution of some residual stream $h_{a}^{l}$ on the $a$ token, we first store $h_{a, \text { clean }}^{l}$ when the model is run on a "clean" prompt $a+b$. We then run the model on the corrupted prompt $a^{\prime}+b$ and store the model logits for the clean answer of $a+b$. Finally, we patch in the clean $h_{a, \text { clean }}^{l}$ on the corrupted prompt $a^{\prime}+b$ and calculate $L D_{a}^{l}=\operatorname{logit}<em _corrupted="{corrupted" _text="\text">{\text {patched }}(a+b)-\operatorname{logit}</em>(a+b)$, where
$L D_{a}^{l}$ is the logit difference for $h_{a}^{l}$. By averaging over 100 pairs of clean and corrupted prompts, we can evaluate $h_{a}^{l}$ 's ability to restore model behavior to the clean answer $a+b$. To reduce noise, all patching experiments only use prompts the model can successfully complete.}</p>
<p>To leverage this technique, we follow Engels et al. (2024) and input our fit for $h_{a, \text { clean }}^{l}$ when patching. This allows us to causally determine if our fit preserves the information the model uses for the computation. We compare our $k$ Fourier feature helical fit with four baselines: using the actual $h_{a, \text { clean }}^{l}$ (layer patch), the first $2 k+1$ PCA components of $h_{a, \text { clean }}^{l}(\mathrm{PCA})$, a circular fit with $k$ Fourier components (circle), and a polynomial fit with basis terms $B(a)=\left[a, a^{2}, \ldots a^{2 k+1}\right]$ (polynomial). For each value of $k$, we choose the combination of Fourier features that maximizes $\frac{1}{L} \sum_{l} L D_{a}^{l}$ as the best set of Fourier features.</p>
<p>In Fig. 4, we see that the helical fit is most performant against baselines, closely followed by the circular fit. This implies that Fourier features are predominantly used to compute addition. Surprisingly, the $k=4$ full helical and circular fits dominate the strong PCA baseline and approach the effect of layer patching, which suggests that we have identified the correct "variables" of computation for addition. Additionally, we note a sharp jump between the fit for layer 0's input (the output of the embedding) and layer 1's input, aligning with evidence from Nikankin et al. (2024) that layer 0 is necessary for numerical processing.</p>
<p>In Appendix C.2, we provide evidence that Llama3.1-8B and Pythia-6.9B also use helical numerical representations. Additionally, we provide evidence that our fits are not overfitting by using a train-test split with no meaningful effect on our results. The helix functional form is not overly expressive, as a helix trained on a randomized order of $a$ is not causally relevant. We also observe continuity when values of $a$ that the helix was not trained on are projected into the</p>
<p>Table 1. Performance of fits across tasks. We calculate $\max <em a="a">{l} L D</em>$ for each fit across a variety of numerical tasks. While the helix fit is competitive, we find that it underperforms the PCA baseline on three tasks.}^{l</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Full</th>
<th style="text-align: center;">PCA</th>
<th style="text-align: center;">Helix</th>
<th style="text-align: center;">Circle</th>
<th style="text-align: center;">Poly</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$a+b=$</td>
<td style="text-align: center;">$\mathbf{8 . 3 4}$</td>
<td style="text-align: center;">6.13</td>
<td style="text-align: center;">$\mathbf{7 . 2 1}$</td>
<td style="text-align: center;">6.83</td>
<td style="text-align: center;">3.09</td>
</tr>
<tr>
<td style="text-align: center;">$a-23=$</td>
<td style="text-align: center;">$\mathbf{7 . 4 5}$</td>
<td style="text-align: center;">6.16</td>
<td style="text-align: center;">$\mathbf{7 . 0 5}$</td>
<td style="text-align: center;">6.52</td>
<td style="text-align: center;">2.93</td>
</tr>
<tr>
<td style="text-align: center;">$a / / 5=$</td>
<td style="text-align: center;">$\mathbf{5 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{5 . 2 4}$</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.55</td>
</tr>
<tr>
<td style="text-align: center;">$a * 1.5=$</td>
<td style="text-align: center;">$\mathbf{7 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{6 . 6 5}$</td>
<td style="text-align: center;">5.16</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">4.84</td>
</tr>
<tr>
<td style="text-align: center;">$a \bmod 2=$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">$\mathbf{1 . 1 2}$</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;">$x-a=0, x=$</td>
<td style="text-align: center;">$\mathbf{6 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{4 . 7 7}$</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">4.34</td>
<td style="text-align: center;">2.98</td>
</tr>
</tbody>
</table>
<p>helical subspace. This satisfies the definition of a nonlinear feature manifold proposed by Olah \&amp; Jermyn (2024), and provides additional evidence for the argument of Engels et al. (2024) against the strongest form of the Linear Representation Hypothesis.</p>
<h3>4.5. Is the Helix the Full Picture?</h3>
<p>To identify if the helix sufficiently explains the structure of numbers in LLMs, we test on five additional tasks.</p>
<ol>
<li>$a-23$ for $a \in[23,99]$</li>
<li>$a / / 5$ (integer division) for $a \in[0,99]$</li>
<li>$a * 1.5$ for even $a \in[0,98]$</li>
<li>$a \bmod 2$ for $a \in[0,99]$</li>
<li>If $x-a=0$, what is $x=$ for $a \in[0,99]$</li>
</ol>
<p>For each task, we fit full helices with $T=[2,5,10,100]$ and compare against baselines. In Table 1, we describe our results on these tasks by listing $\max <em a="a">{l} L D</em>$ entirely (Fig. 21, Appendix C.1).}^{l}$, which is the maximal causal power of each fit (full plot and additional task details in Appendix C.2). Notably, while the helix is causally relevant for all tasks, we see that it underperforms the PCA baseline on tasks 2, 3, and 5. This implies that there is potentially additional structure in numerical representations that helical fits do not capture. However, we are confident that the helix is used for addition. When ablating the helix dimensions from the residual stream (i.e. ablating $C^{\dagger}$ from $h_{a}^{\dagger}$ ), performance is affected roughly as much as ablating $h_{a}^{\dagger</p>
<p>Thus, we conclude that LLMs use a helical representation of numbers to compute addition, although it is possible that additional structure is used for other tasks.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Last token hidden states are well-modeled by helix $(a+b)$. We use activation patching to show that $h_{=}^{l}$ for GPT-J is well modeled by helices, in particular helix $(a+b)$.</p>
<h2>5. LLMs Use the Clock Algorithm to Compute Addition</h2>
<h3>5.1. Introducing the Clock Algorithm</h3>
<p>Taking inspiration from Nanda et al. (2023a), we propose that LLMs manipulate helices to compute addition using the "Clock" algorithm.</p>
<p>To compute $a+b=$, GPT-J</p>
<ol>
<li>Embeds $a$ and $b$ as helices on their own tokens.</li>
<li>A sparse set of attention heads, mostly in layers 9-14, move the $a$ and $b$ helices to the last token.</li>
<li>MLPs 14-18 manipulate these helices to create the helix $a+b$. A small number of attention heads help with this operation.</li>
<li>MLPs 19-27 and a few attention heads "read" from the $a+b$ helix and output to model logits.</li>
</ol>
<p>Since we have already shown that models represent $a$ and $b$ as helices (Appendix C.2), we provide evidence for the last three steps in this section. In Fig. 5 we observe that last token hidden states are well modeled by $h_{=}^{l}=\operatorname{helix}(a, b, a+b)$, where helix $(x, y)$ is shorthand to denote helix $(x)+\operatorname{helix}(y)$. Remarkably, despite only using 9 parameters, at some layers helix $(a+b)$ fits last token hidden states better than a 27 dimensional PCA. The $a+b$ helix having such causal power implies it is at the heart of the computation.</p>
<p>In Appendix D, we show that other LLMs also use helix $(a+$ $b)$. Since the crux of the Clock algorithm is computing the answer helix for $a+b$, we take this as compelling evidence that all three models use the Clock algorithm. However, we would like to understand how specific LLM components implement the algorithm. To do so, we focus on GPT-J.</p>
<p>In Fig. 6, we use activation patching to determine which last token MLP and attention layers are most influential for</p>
<p>the final result. We also present path patching results, which isolates how much components directly contribute to logits. For example, MLP18’s total effect (TE, activation patching) includes both its indirect effect (IE), or how MLP18’s output is used by downstream components like MLP19, and its direct effect (DE, path patching), or how much MLP18 directly boosts the answer logit. In Fig. 6, we see that MLPs dominate direct effect.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. MLPs drive computation of a + b. By using activation and path patching, we find that MLPs are most implicated in constructing the final answer, along with early attention layers.</p>
<p>We now investigate specific attention heads, MLPs, and individual neurons.</p>
<h3>5.2. Investigating Attention Heads</h3>
<p>In GPT-J, every attention layer is the sum of 16 attention heads whose outputs are concatenated. We activation and path patch each attention head on the last token and rank them by total effect (TE). To determine the minimal set of attention heads required, we activation patch k attention heads at once, and find the minimum k such that their combined total effect approximates patching in all attention heads. In Appendix D.1, we see that patching k = 17 heads achieves 80% of the effect of patching in all 448 attention heads, and we choose to round up to k = 20 heads (83.9% of effect).</p>
<p>Since attention heads are not as influential as MLPs in Fig. 6, we hypothesize that they primarily serve two roles: 1) moving the a, b helices to the last token to be processed by downstream components (a, b heads) and 2) outputting the a + b helix directly to logits (a + b heads). Some mixed heads output all three a, b, and a + b helices. We aim to categorize as few attention heads as mixed as possible.</p>
<p>To categorize attention heads, we turn to two metrics. c_{a,b} is the confidence that a certain head is an a, b head, which we quantify with c_{a,b} = (1 - DE) The first term represents the fractional indirect effect of the attention head, and the second term represents the head’s total effect recoverable by just using the a, b helices instead of helix(a, b, a + b). Similarly, we calculate c_{a+b} as the confidence the head is an a + b head, using c_{a+b} = DE helix(a+b) TE helix(a,b,a+b).</p>
<p>We sort the k = 20 heads by c = max(c_{a,b}, c_{a+b}). If a head is an a + b head, we model its output using the a + b helix and allow it only to output to logits (no impact on downstream components). If a head is an a, b head, we restrict it to outputting helix(a, b). For m = [1, 20], we allow m heads with the lowest c to be mixed heads, and categorize the rest as a, b or a + b heads. We find that categorizing m = 4 heads as mixed is sufficient to achieve almost 80% of the effect of using the actual outputs of all k = 20 heads. Thus, most important attention heads obey our categorization. We list some properties of each head type below.</p>
<ul>
<li>a, b heads (11/20): In layers 9-14 (but two heads in l = 16, 18), attend to the a, b tokens, and output a, b helices which are used mostly by downstream MLPs.</li>
<li>a + b heads (5/20): In layers 24-26 (but one head in layer 19), attend to the last token, take their input from preceding MLPs, and output the a + b helix to logits.</li>
<li>Mixed heads (4/20): In layers 15-18, attend to the a, b, and a + b tokens, receive input from a, b attention heads and previous MLPs, and output the a, b, and a + b helices to downstream MLPs.</li>
</ul>
<p>For evidence of these properties refer to Appendix D.1. Notably, only mixed heads are potentially involved in creating the a + b helix, which is the crux of the computation, justifying our conclusion from Fig. 6 that MLPs drive addition.</p>
<h3>5.3. Looking at MLPs</h3>
<p>GPT-J seems to predominantly rely on last token MLPs to compute a + b. To identify which MLPs are most important, we first sort MLPs by total effect, and patch in k = [1, L = 28] MLPs to find the smallest k such that we achieve 95% of the effect of patching in all L MLPs. We use a sharper 95% threshold because MLPs dominate computation and because there are so few of them. Thus, we use k = 11 MLPs in our circuit, specifically MLPs 14-27, with the exception of MLPs 15, 24, and 25 (see Appendix D.2 for details).</p>
<p>We hypothesize that MLPs serve two functions: 1) reading from the a, b helices to create the a + b helix and 2) reading from the a + b helix to output the answer in model logits. We make this distinction using two metrics: helix(a + b)/TE, or the total effect of the MLP recoverable from modeling its output with helix(a + b), and DE/TE ratio. In Fig. 7, we see that the outputs of MLPs 14-18 are progressively better modeled using helix(a + b). Most of their effect is indirect and thus their output is predominantly used by downstream components. At layer 19, helix(a + b) becomes a worse</p>
<p><sup>1</sup>For more on path patching, we refer readers to Goldowsky-Dill et al. (2023); Wang et al. (2023)</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Two stages of circuit MLPs. MLPs 14-18's outputs are well modeled by helix $(a+b)$, suggesting that they build this helical representation, while MLPs 19-27 higher DE/TE ratio implies they output the answer to model logits.
fit and more MLP output affects answer logits directly. We interpret this as MLPs 14-18 "building" the $a+b$ helix, which MLPs 19-27 translate to the answer token $a+b$.</p>
<p>However, our MLP analysis has focused solely on MLP outputs. To demonstrate the Clock algorithm conclusively, we must look at MLP inputs. Recall that GPT-J uses a simple MLP: $\operatorname{MLP}(x)=\sigma\left(x W_{\text {up }}\right) W_{\text {down }} \cdot x$ is a vector of size $(4096$,$) representing the residual stream, and W_{\text {up }}$ is a $(4096,16384)$ projection matrix. The input to the MLP is thus the 16384 dimensional $x W_{\text {up }}$. We denote the $n$th dimension of the MLP input as the $n$th neuron preactivation, and move to analyze these preactivations.</p>
<h3>5.4. Zooming in on Neurons</h3>
<p>Activation patching the $27 * 16384$ neurons in GPT-J is prohibitively expensive, so we instead use the technique of attribution patching to approximate the total effect of each neuron using its gradient (see <em>Kramár et al. (2024)</em>). We find that using just $1 \%$ of the neurons in GPT-J and mean ablating the rest allows for the successful completion of $80 \%$ of prompts (see Appendix D.2). Thus, we focus our analysis on this sparse set of $k=4587$ neurons.</p>
<h3>5.4.1. Modeling Neuron Preactivations</h3>
<p>For a prompt $a+b$, we denote the preactivation of the $n$th neuron in layer $l$ as $N_{n}^{l}(a, b)$. When we plot a heatmap of $N_{n}^{l}(a, b)$ for top neurons in Fig. 8, we see that their preactivations are periodic in $a, b$, and $a+b$. When we Fourier decompose the preactivations as a function of $a+b$, we find that the most common periods are $T=[2,5,10,100]$, matching those used in our helix parameterization (Appendix D.2). This is sensible, as the $n$th neuron in a layer applies $W_{\text {up }}^{n}$ of shape $(4096$,$) to the residual stream, which we have effectively modeled as a helix $(a, b, a+b)$. Subsequently, we model the preactivation of each top neuron as</p>
<p>$$
N_{n}^{l}(a, b)=\sum_{t=a, b, a+b} c_{t} t+\sum_{T=[2,5,10,100]} c_{T t} \cos \left(\frac{2 \pi}{T}\left(t-d_{T t}\right)\right)
$$</p>
<p>For each neuron preactivation, we fit the parameters $c$ and $d$ in Eq. 3 using gradient descent (see Appendix D. 2 for details). In Fig. 8, we show the highest magnitude fit component for a selection of top neurons.</p>
<p>We evaluate our fit of the top $k$ neurons by patching them into the model, mean ablating all other neurons, and measuring the resulting accuracy of the model. In Fig. 9, we see that our neuron fits provide roughly $75 \%$ of the performance of using the actual neuron preactivations. Thus, these neurons are well modeled as reading from the helix.</p>
<h3>5.4.2. Understanding MLP Inputs</h3>
<p>We use our understanding of neuron preactivations to draw conclusions about MLP inputs. To do so, we first path patch each of the top $k$ neurons to find their direct effect and calculate their DE/TE ratio. For each neuron, we calculate the fraction of their fit that helix $(a+b)$ explains, which we approximate by dividing the magnitude of $c_{T, a+b}$ terms by the total magnitude of $c_{T t}$ terms in Eq. 3. For each circuit MLP, we calculate the mean of both of these quantities across top neurons, and visualize them in Fig. 10.</p>
<p>Once again, we see a split at layer 19, where earlier neurons’ preactivation fits rely on $a, b$ terms, while later neurons use $a+b$ terms and write to logits. Since the neuron preactivations represent what each MLP is "reading" from, we combine this result with our evidence from Section 5.3 to summarize the role of MLPs in addition.</p>
<ul>
<li>MLPs 14-18 primarily read from the $a, b$ helices to create the $a+b$ helix for downstream processing.</li>
<li>MLPs 19-27 primarily read from the $a+b$ helix to write the answer to model logits.</li>
</ul>
<p>Thus, we conclude our case that LLMs use the Clock algorithm to do addition, with a deep investigation into how GPT-J implements this algorithm.</p>
<h3>5.5. Limitations of Our Understanding</h3>
<p>There are several aspects of LLM addition we still do not understand. Most notably, while we provide compelling evidence that key components create helix $(a+b)$ from helix $(a, b)$, we do not know the exact mechanism they use to do so. We hypothesize that LLMs use trigonometric identities like $\cos (a+b)=\cos (a) \cos (b)-\sin (a) \sin (b)$ to create helix $(a+b)$. However, like the originator of the Clock algorithm <em>Nanda et al. (2023a)</em>, we are unable to isolate this computation in the model. This is unsurprising, as</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Neuron preactivations and fits. We visualize the preactivations $N_{n}^{l}(a, b)$ for four top neurons. Each neuron has clear periodicity in its preactivations, which we model using a helix inspired functional form.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Evaluating neuron fits. Patching in our fitted preactivations for the top $k$ neurons is roughly as effective as patching in their actual preactivations and ablating all other neurons.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Neuron trends. Neurons in MLPs 14-18 primarily read from the $a$, $b$ helices, while MLPs 19-27 primarily read from the $a+b$ helix and write to logits.</p>
<h2>6. Conclusion</h2>
<p>We find that three mid-sized LLMs represent numbers as generalized helices and manipulate them using the interpretable Clock algorithm to compute addition. While LLMs could do addition linearly, we conjecture that LLMs use the Clock algorithm to improve accuracy, analogous to humans using decimal digits (which are a generalized helix with $T=[10,100, \ldots]$ ) for addition rather than slide rules. In Appendix E, we present preliminary results that GPT-J would be considerably less accurate on "linear addition" due to noise in its linear representations. Future work could analyze if LLMs have internal error-correcting codes for addition like the grid cells presented in <em>Zlokapa et al. (2024)</em>.</p>
<p>The use of the Clock algorithm provides striking evidence that LLMs trained on general text naturally learn to implement complex mathematical algorithms. Understanding LLM algorithms is important for safe AI and can also provide valuable insight into model errors, as shown in Appendix F. We hope that this work inspires additional investigations into LLM mathematical capabilities, especially as addition is implicit to many reasoning problems.</p>
<h2>Acknowledgments</h2>
<p>We thank Josh Engels for participating in extensive conversations throughout the project. We also thank Vedang Lad, Neel Nanda, Ziming Liu, David Baek, and Eric Michaud for their helpful suggestions. This work is supported by the Rothberg Family Fund for Cognitive Science and IAIFI through NSF grant PHY-2019786.</p>
<h2>References</h2>
<p>Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: Progresses and challenges, 2024. URL https: //arxiv.org/abs/2402.00157.</p>
<p>Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.</p>
<p>Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html.</p>
<p>Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., HatfieldDodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.</p>
<p>Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M., and Olah, C. Toy models of superposition. Transformer Circuits Thread, 2022. URL https://transformer-circuits. pub/2022/toy_model/index.html.</p>
<p>Engels, J., Michaud, E. J., Liao, I., Gurnee, W., and Tegmark, M. Not all language model features are linear, 2024. URL https://arxiv.org/abs/2405. 14860 .</p>
<p>Fiotto-Kaufman, J., Loftus, A. R., Todd, E., Brinkmann, J., Juang, C., Pal, K., Rager, C., Mueller, A., Marks, S., Sharma, A. S., Lucchetti, F., Ripa, M., Belfki, A., Prakash, N., Multani, S., Brodley, C., Guha, A., Bell, J., Wallace, B., and Bau, D. Nnsight and ndif: Democratizing access to foundation model internals. 2024. URL https://arxiv.org/abs/2407.14561.
F.R.S., K. P. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11): 559-572, 1901. doi: 10.1080/14786440109462720.</p>
<p>Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders, 2024. URL https : //arxiv.org/abs/2406.04093.</p>
<p>Glazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen, E., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A., de Oliveira Santos, E., Järviniemi, O., Barnett, M., Sandler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine, L., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T., Enugandla, S. V., and Wildon, M. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411. 04872 .</p>
<p>Goldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A. Localizing model behavior with path patching, 2023. URL https://arxiv.org/abs/2304.05969.</p>
<p>Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., et al. The llama 3 herd of models, 2024. URL https : //arxiv.org/abs/2407.21783.</p>
<p>Hanna, M., Liu, O., and Variengien, A. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=p4PckNQR8k.</p>
<p>Heimersheim, S. and Nanda, N. How to use and interpret activation patching, 2024. URL https://arxiv.org/ abs/2404.15255.</p>
<p>Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=F76bwRSLeK.</p>
<p>Kramár, J., Lieberum, T., Shah, R., and Nanda, N. Atp*: An efficient and scalable method for localizing llm behaviour to components, 2024. URL https://arxiv.org/ abs/2403.00745.</p>
<p>Levy, A. A. and Geva, M. Language models encode numbers using digit representations in base 10, 2024. URL https://arxiv.org/abs/2410.11781.</p>
<p>Liu, H., Dai, Z., So, D., and Le, Q. V. Pay attention to MLPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=KBnXrODoBW.</p>
<p>Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark, M., and Williams, M. Towards understanding grokking: An effective theory of representation learning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=6at6rB3IZm.</p>
<p>Makelov, A., Lange, G., and Nanda, N. Towards principled evaluations of sparse autoencoders for interpretability and control. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. URL https: //openreview.net/forum?id=MHIX9H8aYF.</p>
<p>Marks, S., Rager, C., Michaud, E. J., Belinkov, Y., Bau, D., and Mueller, A. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models, 2024. URL https://arxiv.org/abs/2403. 19647.</p>
<p>Meng, K., Bau, D., Andonian, A. J., and Belinkov, Y. Locating and editing factual associations in GPT. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=-h6WAS6eE4.</p>
<p>Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023a. URL https: //openreview.net/forum?id=9XFSbDPmdW.</p>
<p>Nanda, N., Rajamanoharan, S., Kramar, J., and Shah, R. Fact finding: Attempting to reverseengineer factual recall on the neuron level, Dec 2023b. URL https://www.alignmentforum. org/posts/iGuwZTHWb6DFY3sKB/ fact-finding-attempting-to-reverse-engineer</p>
<p>Nikankin, Y., Reusch, A., Mueller, A., and Belinkov, Y. Arithmetic without algorithms: Language models solve math with a bag of heuristics. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=O9YTt26r2P. under review.
nostalgebraist. interpreting GPT: the
logit lens LessWrong - lesswrong.com. https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens, 2020. [Accessed 14-01-2025].</p>
<p>Olah, C. and Jermyn, A. What is a linear representation? what is a multidimensional feature?, 2024. URL https://transformer-circuits. pub/2024/july-update/index.html# linear-representations.</p>
<p>Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.</p>
<p>Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.</p>
<p>OpenAI. URL https://openai.com/index/ learning-to-reason-with-llms.</p>
<p>Park, K., Choe, Y. J., and Veitch, V. The linear representation hypothesis and the geometry of large language models. In Causal Representation Learning Workshop at NeurIPS 2023, 2023. URL https://openreview. net/forum?id=T0PoOJg8cK.</p>
<p>Rajamanoharan, S., Conmy, A., Smith, L., Lieberum, T., Varma, V., Kramár, J., Shah, R., and Nanda, N. Improving dictionary learning with gated sparse autoencoders, 2024. URL https://arxiv.org/abs/2404.16014.</p>
<p>Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., et al. Gemma 2: Improving open language models at a practical size, 2024.</p>
<p>Satpute, A., Gießing, N., Greiner-Petter, A., Schubotz, M., Teschke, O., Aizawa, A., and Gipp, B. Can lms master math? investigating large language models on math stack exchange. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24, pp. 2316-2320, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657945. URL https://doi . org/10.1145/3626772.3657945.</p>
<p>Stolfo, A., Belinkov, Y., and Sachan, M. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. pp. 7035-7052, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.435. URL https:// aclanthology.org/2023.emnlp-main.435.</p>
<p>Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https: //transformer-circuits.pub/2024/ scaling-monosemanticity/index.html.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf.</p>
<p>Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.</p>
<p>Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=NpsVSN6o4ul.</p>
<p>Yip, C. H., Agrawal, R., Chan, L., and Gross, J. Modular addition without black-boxes: Compressing explanations of mlps that compute numerical integration, 2024.</p>
<p>Zhong, Z., Liu, Z., Tegmark, M., and Andreas, J. The clock and the pizza: Two stories in mechanistic explanation of neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=S5wmbQc1We.</p>
<p>Zhou, T., Fu, D., Sharan, V., and Jia, R. Pre-trained large language models use fourier features to compute addition. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=i4MutM2TZb.</p>
<p>Zhu, F., Dai, D., and Sui, Z. Language models encode the value of numbers linearly. In Rambow, O., Wanner, L., Apidianaki, M., Al-Khalifa, H., Eugenio, B. D., and Schockaert, S. (eds.), Proceedings of the 31st International Conference on Computational Linguistics, pp. 693-709, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025. coling-main.47/.</p>
<p>Zlokapa, A., Tan, A. K., Martyn, J. M., Fiete, I. R., Tegmark, M., and Chuang, I. L. Fault-tolerant neural networks from biological error correction codes. Phys. Rev. E, 110:054303, Nov 2024. doi: 10.1103/PhysRevE.110. 054303. URL https://link.aps.org/doi/10. 1103/PhysRevE.110.054303.</p>
<h2>A. Performance of all models on $a+b=$</h2>
<p>We test three models, GPT-J, Pythia-6.9B, and Llama3.1-8B on the task $a+b=$. At first, we attempted to prompt each model with just $a+b=$, but we achieved significantly better results by including additional instructions in the prompt. After non-exhaustive testing, we used the prompts listed in Table 2 to test each model for all 10000 addition prompts (for $a, b \in[0,99]$ ). We plot a heatmap of the accuracy of the model by $a$ and $b$ in Fig. 11. All three models are able to competently complete the task, with Llama3.1-8B achieving an impressive $98 \%$ accuracy. However, in the main paper we focus on analyzing GPT-J because it employs simple MLPs that are easier to interpret. We note that all three models struggle with problems with larger values of $a$ and $b$.</p>
<h2>B. Additional Results on the Structure of Numbers</h2>
<p>Our Fourier decomposition results in Section 4.1 are sensitive to the number of $a$ values analyzed. In particular, we find that the $T=2$ Fourier component is not identified when analyzing $h_{361}^{d}$, but is identified when analyzing $h_{360}^{d}$ (Fig. 12). While we consider this sensitivity to sample size to be a limitation of our Fourier analysis, we note that the Fourier analysis is itself preliminary. We find that the $T=2$ Fourier feature is causally relevant when fitting the residual stream in Section 4.4. Additionally, in later sections we find that neurons often read from the helix using the $T=2$ Fourier feature, indicating its use downstream (Fig. 34). Thus, we identify $T=2$ as an important Fourier feature.</p>
<p>We compare the residual stream of GPT-J after layer 0 on the inputted integers $a_{1}, a_{2} \in[0,99]$ using Euclidean distance and cosine similarity in Fig. 13. We visually note periodicity in the representations, with a striking period of 10 . To analyze the similarity between representations further, we calculate the Euclidean distance between $a$ and $a+\delta n$ for all values of $\delta n$. In Fig. 14, we see that representations continue to get more distant from each other for $a \in[0,99]$ as $\delta n$ grows, albeit sublinearly. This provides evidence that LLMs represent numbers with more than just periodic features. When $a$ is restricted to $a \in[0,9]$, we observe a linear relationship in $\delta n$, implying some local linearity. The first principal component of the numbers $a \in[0,360]$ (shown in Fig. 15) is also linear with a discontinuity at $a=100$. Thus, our focus on two digit addition is justified, as three-digit integers seem to be represented in a different space.</p>
<h2>C. Additional Helix Fitting Results</h2>
<h2>C.1. Helix Properties</h2>
<p>For the input of layer 0 of GPT-J, we plot the magnitude of the helical fit's Fourier features. We do so by taking the magnitude of columns of $C$ in Eq. 2. We find that these features roughly increase in magnitude as period increases, which matches the ordering in the Fourier decomposition presented in Fig. 2.</p>
<p>Additionally, we visualize the cosine similarity matrix between columns of $C$, which represents the similarity between helix components. In Fig. 17, we observe that components are mostly orthogonal, as expected. A notable exception is the similarity between the $T=100 \sin$ component and the linear component.</p>
<p>To ensure that the helix represents a true feature manifold, we design a continuity experiment inspired by Olah \&amp; Jermyn (2024). We first fit all $a$ that do not end with 3 with a $T=[100]$ helix. Then, we project $a=3,13, \ldots, 93$ into that helical space in Fig. 18. We find that each point is projected roughly where we expect it to be. For example, 93 is projected between 89 and 95 . We take this as evidence that our helices represent a true nonlinear manifold.</p>
<h2>C.2. Additional Causal Experiments for Helix Fits</h2>
<p>We first replicate our helix fitting activation patching results on Pythia-6.9B and Llama3.1-8B in Fig. 19.</p>
<p>To ensure the helix fits are not overfitting, we use a traintest split. We train the helix with $80 \%$ of $a$ values and patch using the other $20 \%$ of $a$ values (left of Fig. 20). We observe that the helix and circular fits still outperform the PCA baseline. We also randomize the order of $a$ and find that the randomized helix is not causally relevant (middle of Fig. 20), suggesting that the helix functional form is not naturally over expressive. Finally, we demonstrate that our results hold when fitting the $b$ token on $a+b$ with helix $(b)$ (right of Fig. 20). Note that when activation patching fits on the $b$ token, we use clean/corrupted prompt pairs of the form $(a+b^{\prime}, a+b)$, in contrast to the $\left(a^{\prime}+b, a+b\right)$ pairs we used for the $a$ token.</p>
<p>We perform an ablation experiment by ablating the columns of $C^{\dagger}$ from each $h_{a}^{l}$. In Fig. 21, we see that ablating the helix dimensions from the residual stream like this affects performance about as much as ablating the entire layer, providing additional causal evidence that the helix is necessary for addition. However, when we attempt to fit $a$ with helix $(a)$ for other tasks in Fig. 22, we find that while the fit is effective, it sometimes underperforms PCA baselines. This suggests that while the helix is sufficient for addition, additional structure is required to capture the entirety of numerical representations. For a description of the prompts</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. All models are able to competently perform the task $a+b$, with Llama3.1-8B performing best.</p>
<p>Table 2. The prompts used and accuracy of each model on the addition task $a+b$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Name</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: left;">Output ONLY a number. ${a}+{b}=$</td>
<td style="text-align: center;">$80.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-6.9B</td>
<td style="text-align: left;">Output ONLY a number. ${a}+{b}=$</td>
<td style="text-align: center;">$77.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-8B</td>
<td style="text-align: left;">The following is a correct addition problem. $\backslash \mathrm{n}{a}+{b}=$</td>
<td style="text-align: center;">$98.0 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. The $T=2$ Fourier feature is prominent when analyzing $h_{360}^{3}$, but not when analyzing $h_{361}^{0}$.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. We see clear periodicity in GPT-J's layer 0 representations of the numbers from $0-99$.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. For GPT-J layer 0, the Euclidean distance between $a$ and $a+\delta n$ is approximately linear for $a \in[0,9]$, and sublinear for $a \in[0,99]$. This provides additional evidence that GPT-J uses more than periodic features to represent numbers.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. The first principal component of GPT-J layer 0 for numbers $a \in[0,360]$ shows a discontinuity for three-digit $a$, implying that three-digit numbers are represented in a different space.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16. We plot the magnitude of each column of $C$, the coefficient matrix for the helical basis, to calculate the importance of each Fourier feature. Feature magnitude roughly increases with period, with the notable omission of the $T=2 \sin$ component. We do not plot the linear component's magnitude because its output is a different scale.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17. We plot the cosine similarity between columns of $C$, the coefficient matrix for the helical basis. Features are roughly orthogonal, which we expect for a helix, with the exception of the $T=100 \sin$ component and the linear component $a$. We ignore the $T=2 \sin$ component because of its negligible magnitude.</p>
<p>Helix Superimposed in gpt-j-6B Layer 0
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18. We fit all $a \in[0,99]$ that do not end with 3 using a helix with $T=[100]$, and project the residual stream for $a=$ $3,13, \ldots, 93$ onto the space. We find that there is continuity in the manifold, which we take as evidence that numbers are represented as a nonlinear feature manifold.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19. We replicate our patching results on Pythia-6.9B and Llama3.1-8B. The helical and circular fits outperform baselines at most numbers of parameters.
used and accuracy of GPT-J on these other tasks, see Table 3.</p>
<h2>D. Additional Clock algorithm evidence</h2>
<p>We show that helix $(a+b)$ fits last token hidden states for Pythia-6.9B and Llama3.1 8B in Fig. 23. Notably, the results for Llama3.1-8B are less significant than those for GPT-J and Pythia-6.9B. This is surprising, since the helical fit on the $a$ token is causal for Llama3.1-8B in Fig. 19, and is a sign that Llama3.1-8B potentially uses additional algorithms to compute $a+b$. We hypothesize that this might be due to Llama3.1-8B using gated MLPs, which could lead to the emergence of algorithms not present in GPT-J and Pythia-6.9B, which use simple MLPs. <em>Nikankin et al. (2024)</em>’s analysis of Llama3-8B’s top neurons in addition problems identifies neurons with activation patterns unlike those we identified in GPT-J. Due to this evidence, along with the importance of MLPs in the addition circuit, we consider it likely that Llama3.1-8B implements modified algorithms, but we do not investigate further.</p>
<h2>D.1. Attention Heads</h2>
<p>In Fig. 24, we use activation patching to show that a sparse set of attention heads are influential for addition. In Fig. 25, we find that patching in $k=20$ heads at once is sufficient to restore more than $80 \%$ of the total effect of patching all $k=448$ heads. In Fig. 26, we also find that all attention heads in GPT-J are well modeled using helix $(a, b, a+b)$. We judge this by the fraction of a head’s total effect recoverable when patching in a helical fit.</p>
<p>We categorize heads as $a, b, a+b$, and mixed heads using a confidence score (detailed in Section 5.2). To make our categorization useful, we aim to categorize as few heads as mixed as possible. We find that using $m=4$ mixed heads is sufficient to achieve almost $80 \%$ of the effect of patching the actual outputs of the $k=20$ heads (Fig. 27), although using $m=0$ mixed heads still achieves 70\% of the effect. In Fig. 28, we analyze the properties of each head type. $a+b$ heads tend to attend to the last token and occur in layers 19 onwards. $a, b$ heads primarily attend to the $a$ and $b$ tokens and occur prior to layer 18. Mixed heads attend to the $a, b$, and last tokens, and occur in layers 15-18.</p>
<p>To understand what each head type reads and writes to, we use a modification of the path patching technique we have discussed so far. Specifically, we view mixed and $a, b$ heads as “sender” nodes, and view the total effect of each downstream component if only the direct path between the sender node and the component is patched in (not mediated by any other attention heads or MLPs). In Fig. 29, we find that both $a, b$ and mixed heads generally impact downstream MLPs most. Similarly, we consider mixed and $a+b$ heads</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20. Left When training the helix with $80 \%$ of $a$ values and activation patching with the other $20 \%$ of $a$ values, we see that the helix and circular fits still outperform the PCA baseline. Middle We randomize $a$ while fitting the helix and see that the randomized helix is not causally relevant. Right We show that our results for fitting the $a$ token can be extended to fitting the $b$ token on the prompt $a+b$ with helix(b).</p>
<p>Table 3. GPT-J's task performance with corresponding domains, prompts, and accuracies.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$a-23$</td>
<td style="text-align: left;">$a \in[23,99]$</td>
<td style="text-align: left;">Output ONLY a number. a-23=</td>
<td style="text-align: left;">$89.61 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$a / / 5$</td>
<td style="text-align: left;">$a \in[0,99]$</td>
<td style="text-align: left;">Output ONLY a number. a//5=</td>
<td style="text-align: left;">$97.78 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$a * 1.5$</td>
<td style="text-align: left;">$a \in[0,98]$</td>
<td style="text-align: left;">Output ONLY a number. a*1.5=</td>
<td style="text-align: left;">$80.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$a \bmod 2$</td>
<td style="text-align: left;">$a \in[0,99]$</td>
<td style="text-align: left;">Output ONLY a number. a modulo 2=</td>
<td style="text-align: left;">$95.56 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$x-a=0, x=$</td>
<td style="text-align: left;">$a \in[0,99]$</td>
<td style="text-align: left;">Output ONLY a number. x-a=0, x=</td>
<td style="text-align: left;">$95.56 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21. We find that ablating the helix dimensions from the residual stream is roughly as destructive as ablating the entire layer, providing additional evidence that GPT-J uses a numerical helix to do addition.
as "receiver" nodes, and patch in the path between all upstream components and the receiver node to determine what components each head relies on to achieve its causal effect. We find that $a+b$ heads rely predominantly on upstream MLPs, while mixed heads use both $a, b$ heads and upstream MLPs. This indicates that mixed heads may have some role in creating helix $(a+b)$.</p>
<h2>D.2. MLPs and Neurons</h2>
<p>In Fig. 30, we see that patching $k=11$ MLPs achieves $95 \%$ of the effect of patching all MLPs. We consider these MLPs to be circuit MLPs. Zooming in at the neuron level, we find that roughly $1 \%$ of neurons are required to achieve an $80 \%$ success rate on prompts while mean ablating all other neurons (Fig. 31). Note the use of accuracy over logit difference as a metric in this case. Fig. 31 shows that ablating some neurons actually helps performance as measured by logit difference, while hurting accuracy. To account for this seemingly contradictory result, we hypothesize that ablating some neurons asymmetrically boosts the answer token across prompts, such that some prompts are boosted significantly while other prompts are not affected. We do not investigate this further as it is not a major part of our argument and instead use an accuracy threshold.</p>
<p>When plotting the distribution of top neurons across layers in Fig. 32, we find that almost $75 \%$ of top neurons are located in the $k=11$ circuit MLPs we have identified. We then path patch each of these neurons to calculate their direct effect. In Fig. 33, we see that roughly 700 neurons</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 22. When testing our helical fit on other numerical tasks, we find that it performs competently, but does not always outperform the PCA baseline. This implies that additional structure may be present in numerical representations.</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure 23. We present helical fits on the last token for Pythia-6.9B and Llama3.1-8B. The fits are weaker for Llama3.1-8B, potentially indicating the use of non-Clock algorithms. are required to achieve $80 \%$ of the direct effect of patching in all $k=4587$ top neurons. $84 \%$ of the top DE neurons occur after layer 18, which corresponds with our claim that MLPs 19-27 primarily write the correct answer to logits.</p>
<p>When we Fourier decompose the $k=4587$ top neurons’ preactivations with respect to the value of $a+b$ in Fig. 34, we see spikes at periods $T=[2,5,10,100]$. These are the exact periods of our helix parameterization. To leverage this intuition, we fit the neuron preactivation patterns using the helix inspired functional form detailed in Eq. 2. We use a stochastic gradient descent optimizer with $\mathrm{l} t=1 e-1$, epochs $=2500$ and a cosine annealing learning rate scheduler to minimize the mean squared error of the fit. In Fig. 35, we see that more important neurons with larger total effect are fit better with this functional form, as measured by normalized root mean square error (NRMSE).</p>
<h2>E. Why Use the Clock Algorithm at All?</h2>
<p>We conjecture that LLMs use the Clock algorithm as a form of robust, error correcting code. If LLMs used a linear representation of numbers to do addition, that representation would have to be extremely precise to be effective.</p>
<p>To preliminarily test this conjecture, we take the first 50 PCA dimensions of the number representations for $a \in[0,99]$ in GPT-J after layer 0 and fit a line $\ell$ to it. The resulting line</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Figure 24. A sparse set of attention heads have causal effects on the output when patched (total effect visualized).</p>
<p><img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>Figure 25. Patching $k=20$ heads at once restores more than $80 \%$ of model behavior of patching all $k=448$ heads.</p>
<p><img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>Figure 26. Attention head outputs are well modeled with helix $(a, b, a+b)$. Total effect shown.</p>
<p><img alt="img-26.jpeg" src="img-26.jpeg" /></p>
<p>Figure 27. $m=4$ mixed heads are sufficient to achieve $80 \%$ of effect of patching in all $k=20$ heads normally. Even using $m=0$ mixed heads achieves $70 \%$ of the effect.</p>
<p><img alt="img-27.jpeg" src="img-27.jpeg" /></p>
<p>Figure 28. Left $a+b$ heads generally occur late in the network, while $a, b$ heads occur earlier. Mixed heads occur in middle layers. Middle $a+b$ heads attend more to the last token. Right $a, b$ heads attend mostly to tokens $a$ and $b$.
<img alt="img-28.jpeg" src="img-28.jpeg" /></p>
<p>Figure 29. We present path patching results for top attention heads. In the top row, we view one $a, b$ head (L14H13) and one mixed head (L18H10) as senders. We plot the total effect of each downstream component when only the path between the sender head and that component is patched into the model. We find that both $a, b$ and mixed heads write primarily to MLPs. Similarly, when viewing an $a+b$ head (L24H10) and mixed head (L18H10) as receivers, we see that that the $a+b$ head is primarily dependent on the output from preceding MLPs. While that is true for the mixed head as well, we see that the mixed head also takes input from other $a, b$ heads, implying that it could have some role in creating the $a+b$ helix.</p>
<p><img alt="img-29.jpeg" src="img-29.jpeg" /></p>
<p>Figure 30. $k=11$ MLPs are required to achieve $95 \%$ of the effect of patching in all MLPs.
<img alt="img-30.jpeg" src="img-30.jpeg" /></p>
<p>Figure 31. Top We see that using around $1 \%$ of top neurons and mean ablating the rest can restore the model to $80 \%$ accuracy. Bottom When measuring logit difference, we find that mean ablating some neurons on average increases the logit for the correct answer, but does not improve accuracy. We choose not to investigate this further.
<img alt="img-31.jpeg" src="img-31.jpeg" /></p>
<p>Figure 32. When we analyze approximately $1 \%$ of last token neurons most causally implicated in addition for GPT-J, we find that nearly $75 \%$ of them are in circuit MLPs, which is expected.
<img alt="img-32.jpeg" src="img-32.jpeg" /></p>
<p>Figure 33. Top We find that roughly 700 neurons achieve $80 \%$ of the direct effect of patching in all $k=4587$ high impact neurons. Bottom More than $80 \%$ of these high direct effect neurons are in layers 19 onwards, which we have identified as being responsible for translating the $a+b$ helix to answer logits.</p>
<p><img alt="img-33.jpeg" src="img-33.jpeg" /></p>
<p>Figure 34. Top neurons' preactivations are periodic in $a+b$ with top periods of $T=[2,5,10,100]$. Percentages shown for Fourier periods within $5 \%$ of $T$.
<img alt="img-34.jpeg" src="img-34.jpeg" /></p>
<p>Figure 35. When calculating the NRMSE of the helix inspired fit for neuron preactivations, we find that more impactful neurons (with higher total effect) are typically fit better.
<img alt="img-35.jpeg" src="img-35.jpeg" /></p>
<p>Figure 36. We find that using a line with $R^{2}=0.997$ leads to addition that performs considerably worse than GPT-J. We attribute this to the representational precision required to do addition along a line, indicating a possible reason LLMs choose to use helical representations.
has an $R^{2}$ of 0.997 , indicating a very good fit. We consider all problems $a_{1}+a_{2}$. We do addition on this line by taking $\ell\left(a_{1}\right)+\ell\left(a_{2}\right)$. If $\ell\left(a_{1}\right)+\ell\left(a_{2}\right)$ is closest to $\ell\left(a_{1}+a_{2}\right)$, we consider the addition problem successful.</p>
<p>We then take the percentage of successful addition problems where the answer $a_{1}+a_{2}$ is less than some threshold $\alpha$, and compare the accuracy as a function of $\alpha$ for GPT-J and linear addition. Surprisingly, we find that for $\alpha=100$, linear addition has an accuracy of less than $20 \%$, while GPT-J has an accuracy of more than $80 \%$ (Fig. 36).</p>
<p>Thus, even with very precise linear representations, doing linear addition leads to errors. We interpret LLMs use of modular circles for addition as a built-in redundancy to avoid errors from their imperfect representations.</p>
<h2>F. Investigating Model Errors</h2>
<p>Given that GPT-J implements an algorithm to compute addition rather than relying on memorization, why does it still make mistakes? For problems where GPT-J answers incorrectly with a number, we see that it is most often off by -10 ( $45.7 \%$ ) and 10 ( $27.9 \%$ ), cumulatively making up over $70 \%$ of incorrect numeric answers (Fig 37). We offer two hypotheses for the source of these errors: 1) GPT-J is failing to "carry" correctly when creating the $a+b$ helix or 2) reading from the $a+b$ helix to answer logits is flawed.</p>
<p>We test the first hypothesis by analyzing the distribution of GPT-J errors. If carrying was the problem, we would expect that when the model is off by -10 , the units digits of $a$ and $b$ add up to 10 or more. Using a Chi-squared test with a threshold of $\alpha=0.05$, we see that the units digit of $a$ and $b$ summing to more than 10 is not more likely for when the model's error is -10 than otherwise (Fig. 38). This falsifies our first hypothesis. Thus, we turn to understanding how the $a+b$ helix is translated to model logits.</p>
<p>Since MLPs most contribute to direct effect, we begin inves-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Massachusetts Institute of Technology. Correspondence to: Subhash Kantamneni $&lt;$ subhashk@mit.edu $&gt;$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8149 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8149</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8149</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-279410835</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.13886v2.pdf" target="_blank">Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</a></p>
                <p><strong>Paper Abstract:</strong> Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc., as in"twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8149.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8149.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based reasoning-oriented language model from OpenAI evaluated on multilingual linguistic-number puzzles; shows strong arithmetic ability when operators are explicit, but fails when numeral compositional operators are implicit or unfamiliar.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI reasoning model (o1-mini), transformer-based; paper cites Jaech et al. 2024 for the system card. Exact model size not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Linguistic-mathematical puzzles mapping unknown-language numerals to Hindu–Arabic integers; operations include addition, multiplication, subtraction and identifying squares/cubes in examples (small integers typical of linguistics olympiad problems).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Relies on explicit operator tokens/symbols to perform arithmetic-like composition; otherwise appears to pattern-match or memorize forms rather than internally infer implicit compositional operators; authors hypothesize separate/layered circuits/subspaces for linguistic vs mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Prompt engineering interventions and controlled task manipulations: (1) making operators explicit (familiar symbol, unfamiliar symbol, unfamiliar word), (2) randomizing morpheme tokens (single-character vs multi-token dummy words), (3) providing contextual cues (language, base, note about implicit operations), (4) minimal-pair ablations across numeral parameters, (5) base-change experiments with token randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative and task-level results reported: o1-mini reaches near-ceiling accuracy on problems when operators are explicit and familiar (e.g., '+' used); performance degrades markedly in IMPLICIT operator condition. On multi-character (more linguistic) inputs, explicit-unfamiliar operators produced worse or no improvement. Error statistics: across 150 trials, o1-mini reproduced an input number as the answer in 11 multi-token trials and 3 single-token trials when operators were not explicit/familiar; in 50% of single-character cases lacking explicit/familiar operators, all five repeated answers were distinct and incorrect. In base experiments o1-mini solved all base-variant problems (robust to base, per Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When operators are implicit: (a) pattern-matching (e.g., returning another square/cube rather than computing), (b) copying a number present in the prompt as the answer, (c) high variance/inconsistency across repeated runs, (d) inability to infer implicit compositional operators; when operator is explicit but unfamiliar, models often perform worse (confusion between operator and numeral tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Performance strongly and reliably improves when operators are explicitly provided with familiar mathematical symbols (experimental manipulation). Ablation/minimal-pair experiments show that numeral-system basic parameters (base, order, digit representation) do not by themselves prevent solving, implying the deficit is specifically in inferring implicit operators. Robust solving across base-randomization experiments (Table 6) shows arithmetic competence separate from the inability to infer linguistic composition.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>o1-mini can solve all minimal-pair template problems and base-variant problems when operators are explicit, demonstrating that arithmetic capability exists; failure is therefore specific to inferring implicit operator structure rather than a general incapacity for arithmetic. Explicit-but-unfamiliar operator tokens sometimes reduce performance, showing that arbitrary operator markers are not sufficient — the model relies on familiar operator symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8149.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B distilled reasoning model (DeepSeek-R1 / Qwen-7B distill) evaluated locally; frequently failed to produce answers in the required numeric format and struggled with multi-token unfamiliar operator conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled 7B Qwen-based reasoning model (DeepSeek-R1-distill-Qwen-7B) run locally; paper notes it often did not produce answers in the required numeric-only format and was excluded from some aggregated analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same linguistic-mathematical puzzle suite (unknown-language numeral interpretation; addition/multiplication/subtraction/small powers).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal mechanistic analysis performed; behavior indicates poor task formatting control and difficulty mapping unfamiliar linguistic tokens to arithmetic composition when operator cues are absent or unfamiliar.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same prompt-manipulation interventions as other models (explicit operators familiar/unfamiliar, implicit, token randomization, contextual cues, minimal-pair ablations); also compared single-character vs multi-character dummy-token settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Often failed to produce answers in the correct (numeric-only) format; performance in multi-character explicit-unfamiliar condition showed 0% improvement in Figure 7. Excluded from some tables due to output-format issues. No numeric accuracy percentages reported beyond these qualitative descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Frequently outputs non-numeric or verbose text instead of the required numeric answer; poor handling of unfamiliar operator words/symbols in multi-token (linguistic) condition; inconsistent or no output in expected format.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral evidence from experiment runs showing format failures and zero or negative improvement for explicit-unfamiliar operators (Figure 7) supports the conclusion that the model does not robustly map unfamiliar linguistic markers to arithmetic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>DeepSeek-R1's failures may be partly due to output-formatting tendencies rather than pure arithmetic inability; the paper excludes it from some aggregated comparisons because of this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8149.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability OpenAI transformer model evaluated as a comparator; solved minimal-pair template problems and was robust to numeral-base manipulations, indicating intact arithmetic and abstraction when tasks are presented in suitable form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capability transformer LLM (GPT-4) used as an evaluation baseline; specific size/architecture details not provided in this paper but treated as a state-of-the-art comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Linguistic-number puzzles (same set); minimal-pair ablations, base experiments and explicit/implicit operator variants.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Demonstrates the ability to apply arithmetic when operators or problem template are explicit; no detailed internal mechanistic probing performed, but behavioral pattern indicates capacity to perform symbolic arithmetic and infer rules in the controlled minimal-pair settings.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Evaluated across the same experimental manipulations: explicit operator variants, minimal pairs varying numeral parameters, base-randomization tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Solved all minimal-pair template problems (reported) and was robust across base-change experiments (Table 6); contrasted with GPT-3.5 which struggled on numeral base and combination items.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not reported as failing on the controlled minimal-pair and base experiments; implicit-operator linguistic puzzles remain challenging in general for models unless operators are made explicit (this applies to other models too).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral success in minimal-pair and base-robustness experiments indicates GPT-4 can implement the necessary arithmetic/compositional reasoning when the task representation is explicit/standardized, supporting the hypothesis that failures in other models/conditions are due to representation of implicit operators rather than lack of arithmetic competence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even GPT-4's success is tied to task standardization; the paper does not show GPT-4 inferring arbitrary implicit numeral composition in the same open-ended way humans do, leaving open whether GPT-4 forms an internal analogue of implicit numeral operators from few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8149.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 'o' variant of GPT-4 evaluated on base-change experiments; solved all base-variant problems in the paper's tests, showing robustness to numeral-base representation when prompted appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variant of GPT-4 referenced in the experiments; architecture/size details not specified in this paper. Used alongside GPT-4 and o1-mini in base experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Base-variant numeral mapping experiments and minimal-pair templates involving small integers and arithmetic composition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behavior indicates correct mapping between token representations and numeric values when problem tokens are standardized; no internal mechanistic probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Base-change experiments with two randomization schemes: single-character A/B/C/D tokens and random-token dummy words; minimal-pair ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Solved every problem in both base-randomization conditions (per Table 6), demonstrating invariance to base representation in the controlled tests.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>None reported for the base experiments in this paper; general caveat that implicit-operator linguistic puzzles remain challenging in other settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral robustness across randomized tokenizations and base mappings indicates GPT-4o can apply arithmetic and rule-based mapping when the task representation is standardized and operators (if needed) are represented in a way it recognizes.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Does not demonstrate the ability to infer arbitrary implicit compositional operators from few examples in more naturalistic linguistic presentations — the success is limited to controlled templates and explicit/reliable cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8149.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used OpenAI model evaluated as a baseline; solved many template problems but struggled with numeral base and combination parameters in minimal-pair ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-turbo; architecture and training are standard transformer LLM details (not exhaustively specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Linguistic-number puzzles and minimal-pair ablation tests involving base, ordering and combination of numerals.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behavioral pattern implies weaker abstraction/generalization for some numeral-system parameters (base and combination) compared to more advanced models; no mechanistic probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Minimal-pair ablation experiments and standard explicit/implicit operator variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to 'struggle with numeral base and combination' in minimal-pair tests (Table 3) whereas GPT-4 and o1-mini solved all pairs; specific accuracy numbers for GPT-3.5 are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Trouble generalizing across changes in numeral base and complex numeral combination patterns; likely sensitive to representation/tokenization and less robust to base randomization than stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Relative failure on base/combination minimal-pairs contrasted with GPT-4/o1-mini success indicates limited abstraction/generalization in GPT-3.5 for numerals represented in unfamiliar or randomized formats.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No evidence that GPT-3.5 completely lacks arithmetic competence; rather shows reduced robustness to representation changes relative to more capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8149.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qwen-2-7b-vl-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>qwen-2-7b-vl-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned multi-modal Qwen variant queried by the authors; produced 0 accuracy across tested conditions, typically outputting verbose non-numeric responses instead of the required numeric-only answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>qwen-2-7b-vl-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Qwen 7B visual-language variant (qwen-2-7b-vl-instruct) referenced and queried via API/local runs; size approx 7B as indicated by name.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Linguistic-number puzzles requiring numeric-only answers (small integer arithmetic, composition).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal mechanism identified; behavior indicates strong tendency to produce long textual answers rather than concise numeric outputs when given these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Prompted with the same explicit/unfamiliar/implicit operator variants and context manipulations; excluded from main analyses because outputs did not match required numeric-only format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported as 0 across all tested conditions in this paper; outputs were usually longer textual answers without the simple numeric output required.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Generates verbose or incorrectly formatted answers (not numeric-only), preventing measurement of arithmetic correctness; thus failures are at least partly an output-formatting/behavioral issue.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical observation of consistent non-numeric outputs across conditions in the authors' evaluation runs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Format failures prevent definitive claims about the model's arithmetic competence absent additional instruction to produce numeric-only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8149.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8149.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A base Llama-3.1 8B model evaluated as a baseline; returned 0 accuracy across tested conditions, often producing longer textual answers instead of numeric-only responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3.1-8B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base Llama-3.1 8B model (not instruction tuned) used as a baseline in the authors' tests; excluded from detailed analyses because it did not provide numeric-only answers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same suite of linguistic-number puzzles (small integer arithmetic and numeral composition).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No mechanistic probing; behavior indicates inability to produce required formatted numeric answers under the prompt conditions used.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Queried with the same explicit/unfamiliar/implicit operator variants; performance excluded due to output-format errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported as 0 across all tested conditions; typically generated longer text answers rather than simple numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Generates non-numeric/verbose outputs (formatting failure) rather than numeric answers; thus cannot be reliably scored for arithmetic correctness in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical observation in experiments (consistent generation of longer text without numeric-only answer).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Formatting behavior may mask true arithmetic ability if prompted differently; authors excluded this model from analyses for that reason.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Number Cookbook: Number Understanding of Language Models and How to Improve It <em>(Rating: 2)</em></li>
                <li>Evaluation of OpenAI o1: Opportunities and challenges of AGI <em>(Rating: 2)</em></li>
                <li>DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 1)</em></li>
                <li>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8149",
    "paper_id": "paper-279410835",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "o1-mini",
            "name_full": "OpenAI o1-mini",
            "brief_description": "A transformer-based reasoning-oriented language model from OpenAI evaluated on multilingual linguistic-number puzzles; shows strong arithmetic ability when operators are explicit, but fails when numeral compositional operators are implicit or unfamiliar.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "o1-mini",
            "model_description": "OpenAI reasoning model (o1-mini), transformer-based; paper cites Jaech et al. 2024 for the system card. Exact model size not specified in this paper.",
            "arithmetic_task_type": "Linguistic-mathematical puzzles mapping unknown-language numerals to Hindu–Arabic integers; operations include addition, multiplication, subtraction and identifying squares/cubes in examples (small integers typical of linguistics olympiad problems).",
            "mechanism_or_representation": "Relies on explicit operator tokens/symbols to perform arithmetic-like composition; otherwise appears to pattern-match or memorize forms rather than internally infer implicit compositional operators; authors hypothesize separate/layered circuits/subspaces for linguistic vs mathematical reasoning.",
            "probing_or_intervention_method": "Prompt engineering interventions and controlled task manipulations: (1) making operators explicit (familiar symbol, unfamiliar symbol, unfamiliar word), (2) randomizing morpheme tokens (single-character vs multi-token dummy words), (3) providing contextual cues (language, base, note about implicit operations), (4) minimal-pair ablations across numeral parameters, (5) base-change experiments with token randomization.",
            "performance_metrics": "Qualitative and task-level results reported: o1-mini reaches near-ceiling accuracy on problems when operators are explicit and familiar (e.g., '+' used); performance degrades markedly in IMPLICIT operator condition. On multi-character (more linguistic) inputs, explicit-unfamiliar operators produced worse or no improvement. Error statistics: across 150 trials, o1-mini reproduced an input number as the answer in 11 multi-token trials and 3 single-token trials when operators were not explicit/familiar; in 50% of single-character cases lacking explicit/familiar operators, all five repeated answers were distinct and incorrect. In base experiments o1-mini solved all base-variant problems (robust to base, per Table 6).",
            "error_types_or_failure_modes": "When operators are implicit: (a) pattern-matching (e.g., returning another square/cube rather than computing), (b) copying a number present in the prompt as the answer, (c) high variance/inconsistency across repeated runs, (d) inability to infer implicit compositional operators; when operator is explicit but unfamiliar, models often perform worse (confusion between operator and numeral tokens).",
            "evidence_for_mechanism": "Performance strongly and reliably improves when operators are explicitly provided with familiar mathematical symbols (experimental manipulation). Ablation/minimal-pair experiments show that numeral-system basic parameters (base, order, digit representation) do not by themselves prevent solving, implying the deficit is specifically in inferring implicit operators. Robust solving across base-randomization experiments (Table 6) shows arithmetic competence separate from the inability to infer linguistic composition.",
            "counterexamples_or_challenges": "o1-mini can solve all minimal-pair template problems and base-variant problems when operators are explicit, demonstrating that arithmetic capability exists; failure is therefore specific to inferring implicit operator structure rather than a general incapacity for arithmetic. Explicit-but-unfamiliar operator tokens sometimes reduce performance, showing that arbitrary operator markers are not sufficient — the model relies on familiar operator symbols.",
            "uuid": "e8149.0",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1-distill-Qwen-7B",
            "brief_description": "A 7B distilled reasoning model (DeepSeek-R1 / Qwen-7B distill) evaluated locally; frequently failed to produce answers in the required numeric format and struggled with multi-token unfamiliar operator conditions.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-distill-Qwen-7B",
            "model_description": "Distilled 7B Qwen-based reasoning model (DeepSeek-R1-distill-Qwen-7B) run locally; paper notes it often did not produce answers in the required numeric-only format and was excluded from some aggregated analyses.",
            "arithmetic_task_type": "Same linguistic-mathematical puzzle suite (unknown-language numeral interpretation; addition/multiplication/subtraction/small powers).",
            "mechanism_or_representation": "No internal mechanistic analysis performed; behavior indicates poor task formatting control and difficulty mapping unfamiliar linguistic tokens to arithmetic composition when operator cues are absent or unfamiliar.",
            "probing_or_intervention_method": "Same prompt-manipulation interventions as other models (explicit operators familiar/unfamiliar, implicit, token randomization, contextual cues, minimal-pair ablations); also compared single-character vs multi-character dummy-token settings.",
            "performance_metrics": "Often failed to produce answers in the correct (numeric-only) format; performance in multi-character explicit-unfamiliar condition showed 0% improvement in Figure 7. Excluded from some tables due to output-format issues. No numeric accuracy percentages reported beyond these qualitative descriptions.",
            "error_types_or_failure_modes": "Frequently outputs non-numeric or verbose text instead of the required numeric answer; poor handling of unfamiliar operator words/symbols in multi-token (linguistic) condition; inconsistent or no output in expected format.",
            "evidence_for_mechanism": "Behavioral evidence from experiment runs showing format failures and zero or negative improvement for explicit-unfamiliar operators (Figure 7) supports the conclusion that the model does not robustly map unfamiliar linguistic markers to arithmetic operations.",
            "counterexamples_or_challenges": "DeepSeek-R1's failures may be partly due to output-formatting tendencies rather than pure arithmetic inability; the paper excludes it from some aggregated comparisons because of this.",
            "uuid": "e8149.1",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A high-capability OpenAI transformer model evaluated as a comparator; solved minimal-pair template problems and was robust to numeral-base manipulations, indicating intact arithmetic and abstraction when tasks are presented in suitable form.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capability transformer LLM (GPT-4) used as an evaluation baseline; specific size/architecture details not provided in this paper but treated as a state-of-the-art comparator.",
            "arithmetic_task_type": "Linguistic-number puzzles (same set); minimal-pair ablations, base experiments and explicit/implicit operator variants.",
            "mechanism_or_representation": "Demonstrates the ability to apply arithmetic when operators or problem template are explicit; no detailed internal mechanistic probing performed, but behavioral pattern indicates capacity to perform symbolic arithmetic and infer rules in the controlled minimal-pair settings.",
            "probing_or_intervention_method": "Evaluated across the same experimental manipulations: explicit operator variants, minimal pairs varying numeral parameters, base-randomization tests.",
            "performance_metrics": "Solved all minimal-pair template problems (reported) and was robust across base-change experiments (Table 6); contrasted with GPT-3.5 which struggled on numeral base and combination items.",
            "error_types_or_failure_modes": "Not reported as failing on the controlled minimal-pair and base experiments; implicit-operator linguistic puzzles remain challenging in general for models unless operators are made explicit (this applies to other models too).",
            "evidence_for_mechanism": "Behavioral success in minimal-pair and base-robustness experiments indicates GPT-4 can implement the necessary arithmetic/compositional reasoning when the task representation is explicit/standardized, supporting the hypothesis that failures in other models/conditions are due to representation of implicit operators rather than lack of arithmetic competence.",
            "counterexamples_or_challenges": "Even GPT-4's success is tied to task standardization; the paper does not show GPT-4 inferring arbitrary implicit numeral composition in the same open-ended way humans do, leaving open whether GPT-4 forms an internal analogue of implicit numeral operators from few examples.",
            "uuid": "e8149.2",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "An 'o' variant of GPT-4 evaluated on base-change experiments; solved all base-variant problems in the paper's tests, showing robustness to numeral-base representation when prompted appropriately.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Variant of GPT-4 referenced in the experiments; architecture/size details not specified in this paper. Used alongside GPT-4 and o1-mini in base experiments.",
            "arithmetic_task_type": "Base-variant numeral mapping experiments and minimal-pair templates involving small integers and arithmetic composition.",
            "mechanism_or_representation": "Behavior indicates correct mapping between token representations and numeric values when problem tokens are standardized; no internal mechanistic probes reported.",
            "probing_or_intervention_method": "Base-change experiments with two randomization schemes: single-character A/B/C/D tokens and random-token dummy words; minimal-pair ablations.",
            "performance_metrics": "Solved every problem in both base-randomization conditions (per Table 6), demonstrating invariance to base representation in the controlled tests.",
            "error_types_or_failure_modes": "None reported for the base experiments in this paper; general caveat that implicit-operator linguistic puzzles remain challenging in other settings.",
            "evidence_for_mechanism": "Behavioral robustness across randomized tokenizations and base mappings indicates GPT-4o can apply arithmetic and rule-based mapping when the task representation is standardized and operators (if needed) are represented in a way it recognizes.",
            "counterexamples_or_challenges": "Does not demonstrate the ability to infer arbitrary implicit compositional operators from few examples in more naturalistic linguistic presentations — the success is limited to controlled templates and explicit/reliable cues.",
            "uuid": "e8149.3",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A widely-used OpenAI model evaluated as a baseline; solved many template problems but struggled with numeral base and combination parameters in minimal-pair ablations.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI GPT-3.5-turbo; architecture and training are standard transformer LLM details (not exhaustively specified in this paper).",
            "arithmetic_task_type": "Linguistic-number puzzles and minimal-pair ablation tests involving base, ordering and combination of numerals.",
            "mechanism_or_representation": "Behavioral pattern implies weaker abstraction/generalization for some numeral-system parameters (base and combination) compared to more advanced models; no mechanistic probes reported.",
            "probing_or_intervention_method": "Minimal-pair ablation experiments and standard explicit/implicit operator variants.",
            "performance_metrics": "Reported to 'struggle with numeral base and combination' in minimal-pair tests (Table 3) whereas GPT-4 and o1-mini solved all pairs; specific accuracy numbers for GPT-3.5 are not provided.",
            "error_types_or_failure_modes": "Trouble generalizing across changes in numeral base and complex numeral combination patterns; likely sensitive to representation/tokenization and less robust to base randomization than stronger models.",
            "evidence_for_mechanism": "Relative failure on base/combination minimal-pairs contrasted with GPT-4/o1-mini success indicates limited abstraction/generalization in GPT-3.5 for numerals represented in unfamiliar or randomized formats.",
            "counterexamples_or_challenges": "No evidence that GPT-3.5 completely lacks arithmetic competence; rather shows reduced robustness to representation changes relative to more capable models.",
            "uuid": "e8149.4",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "qwen-2-7b-vl-instruct",
            "name_full": "qwen-2-7b-vl-instruct",
            "brief_description": "An instruction-tuned multi-modal Qwen variant queried by the authors; produced 0 accuracy across tested conditions, typically outputting verbose non-numeric responses instead of the required numeric-only answers.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "qwen-2-7b-vl-instruct",
            "model_description": "Instruction-tuned Qwen 7B visual-language variant (qwen-2-7b-vl-instruct) referenced and queried via API/local runs; size approx 7B as indicated by name.",
            "arithmetic_task_type": "Linguistic-number puzzles requiring numeric-only answers (small integer arithmetic, composition).",
            "mechanism_or_representation": "No internal mechanism identified; behavior indicates strong tendency to produce long textual answers rather than concise numeric outputs when given these tasks.",
            "probing_or_intervention_method": "Prompted with the same explicit/unfamiliar/implicit operator variants and context manipulations; excluded from main analyses because outputs did not match required numeric-only format.",
            "performance_metrics": "Accuracy reported as 0 across all tested conditions in this paper; outputs were usually longer textual answers without the simple numeric output required.",
            "error_types_or_failure_modes": "Generates verbose or incorrectly formatted answers (not numeric-only), preventing measurement of arithmetic correctness; thus failures are at least partly an output-formatting/behavioral issue.",
            "evidence_for_mechanism": "Empirical observation of consistent non-numeric outputs across conditions in the authors' evaluation runs.",
            "counterexamples_or_challenges": "Format failures prevent definitive claims about the model's arithmetic competence absent additional instruction to produce numeric-only outputs.",
            "uuid": "e8149.5",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "llama-3.1-8B",
            "name_full": "Llama-3.1-8B (base)",
            "brief_description": "A base Llama-3.1 8B model evaluated as a baseline; returned 0 accuracy across tested conditions, often producing longer textual answers instead of numeric-only responses.",
            "citation_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
            "mention_or_use": "use",
            "model_name": "llama-3.1-8B (base)",
            "model_description": "Base Llama-3.1 8B model (not instruction tuned) used as a baseline in the authors' tests; excluded from detailed analyses because it did not provide numeric-only answers.",
            "arithmetic_task_type": "Same suite of linguistic-number puzzles (small integer arithmetic and numeral composition).",
            "mechanism_or_representation": "No mechanistic probing; behavior indicates inability to produce required formatted numeric answers under the prompt conditions used.",
            "probing_or_intervention_method": "Queried with the same explicit/unfamiliar/implicit operator variants; performance excluded due to output-format errors.",
            "performance_metrics": "Accuracy reported as 0 across all tested conditions; typically generated longer text answers rather than simple numbers.",
            "error_types_or_failure_modes": "Generates non-numeric/verbose outputs (formatting failure) rather than numeric answers; thus cannot be reliably scored for arithmetic correctness in these tasks.",
            "evidence_for_mechanism": "Empirical observation in experiments (consistent generation of longer text without numeric-only answer).",
            "counterexamples_or_challenges": "Formatting behavior may mask true arithmetic ability if prompted differently; authors excluded this model from analyses for that reason.",
            "uuid": "e8149.6",
            "source_info": {
                "paper_title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
            "rating": 2,
            "sanitized_title": "number_cookbook_number_understanding_of_language_models_and_how_to_improve_it"
        },
        {
            "paper_title": "Evaluation of OpenAI o1: Opportunities and challenges of AGI",
            "rating": 2,
            "sanitized_title": "evaluation_of_openai_o1_opportunities_and_challenges_of_agi"
        },
        {
            "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
            "rating": 1,
            "sanitized_title": "embers_of_autoregression_understanding_large_language_models_through_the_problem_they_are_trained_to_solve"
        }
    ],
    "cost": 0.013896249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles
15 Oct 2025</p>
<p>Antara Raaghavi Bhattacharya antara@alumni.harvard.edu 
School of Engineering and Applied Sciences
Harvard University</p>
<p>Department of Linguistics
Harvard University</p>
<p>Isabel Papadimitriou 
Kempner Institute for the Study of Natural and Artificial Intelligence
Harvard University</p>
<p>Kathryn Davidson 
Department of Linguistics
Harvard University</p>
<p>David Alvarez-Melis 
School of Engineering and Applied Sciences
Harvard University</p>
<p>Kempner Institute for the Study of Natural and Artificial Intelligence
Harvard University</p>
<p>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles
15 Oct 20251A8AAB6A0430AAF761913265D661E06AarXiv:2506.13886v2[cs.CL]
Across languages, numeral systems vary widely in how they construct and combine numbers.While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully.We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language.Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols (+, ×, etc, as in "twenty + three").In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance.While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure.We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.</p>
<p>Introduction</p>
<p>Language models reason and solve problems using language.What is the connection (and the integration) between their linguistic systems and their impressive reasoning abilities?To investigate this question, we run a suite of experiments to analyze how language models solve puzzles about diverse linguistic number systems.People represent numbers through language, using rule-based systems that are simultaneously linguistic and mathematical (Ifrah, 2000;Dehaene, 2011;Carey, 2004;Le Corre and Carey, 2007;Ionin and Matushansky, 2006;Hammarström, 2010;Comrie, 2011).Unlike most mathematical reasoning problems, where the mathematical operators are explicit, a numeral system contains implicit operations for describing numerals, and there is considerable variety in how this is done across the world's languages.For example, French vingt-neuf (20 + 9), Bengali untirīsh (30 − 1), Tamil irupatti on ¯patu (2 × 10 + (10 -1)), and Birom bākūrū bībā ná vE tùNūn (2 × 12 + 5) all evaluate to the Hindu-Arabic numeral 29.</p>
<p>We investigate the capabilities of language models to solve puzzles about linguistic number systems, drawn from linguistics competitions (Linguistics Olympiads) where high-school students have to reason through data about unknown languages and explain the linguistic rules governing the data (Derzhanski and Payne, 2010).While language models approach human performance on several language-based benchmarks (Hendrycks et al., 2020;Kojima et al., 2022;Beguš et al., 2023), and recent reasoning models deliberately optimized for logical and mathematical reasoning show remarkable performance improvements for many structured mathematical reasoning tasks (Zhong et al., 2024;Jaech et al., 2024), LLMs perform extremely poorly at solving linguistic-mathematical puzzles about systems of numbers in different languages (Derzhanski and Veneva, 2018;Bean et al., 2024).</p>
<p>Why do language models fail to solve these problems at the intersection of language and math -what specifically causes this failure?And how much of this failure is due to the linguistic vs. the mathematical aspects of the problem?</p>
<p>We present a method to systematically isolate individual parameters of number construction and combination and investigate how they affect language model performance.We establish that most individual mathematical features (like base) do not hinder the ability of sufficiently advanced language models to solve such problems.However, unless the mathematical operations in a problem are made explicit through familiar symbols (+, ×, etc.), models cannot consistently solve the problem.This indicates that, at least within the domain of linguistic-mathematical problems, models cannot infer the compositional structure of numerals like humans can, or sufficiently abstract notions like operators.We discuss our findings in the broader context of human language, concluding that flexible, adaptive use of language across domains appears to remain challenging for LLMs.</p>
<p>Background</p>
<p>Linguistic and cognitive connections</p>
<p>People acquire systems of number representation as part of learning language, and are consequently able to construct arbitrary numerals using the rules that they learn.Although the system of rules may be language-specific, the general framework of numeral construction and combination is a fundamental cognitive ability (Hurford, 1987;Feigenson et al., 2004).Performing mathematics in a symbolic sense requires explicit instruction (e.g. a child would not inherently know what + connotes), but once this symbolic meaning has been learned, people can generalize it to apply to any numbers (Sarnecka et al., 2015).Numeral operations in language can be marked both explicitly (e.g.und in German einundzwanzig) and implicitly (as in English twenty-one), with larger numerals often using a combination of implicit and explicit operations (five hundred and one = 5 [×] 100 + 1).Even when operations are implicit, people can understand and infer the cross-linguistic compositional structure of numerals (Ionin and Matushansky, 2006).</p>
<p>In a linguistics contest, a high-school student would not need to know any mathematical concepts beyond basic arithmetic to reason through number system problems and infer the rules needed to solve them.The challenge lies instead in whether models can learn and infer such rules from limited data -a characteristic capacity of humans acquiring language.</p>
<p>Mathematical ability in language models</p>
<p>Recent language models seem to display strong numerical understanding and processing abilities if presented with purely mathematical problems in standard formats (Yang et al., 2024), particularly for small numbers and simple mathematical operations (of the kinds used in linguistics contest problems).Current reasoning models appear to perform well at arithmetic and algebra, math word problems (Ahn et al., 2024), and difficult mathematical contest questions equivalent to advanced collegelevel math problems (Fang et al., 2024;Chervonyi et al., 2025), although their problem-solving ability is sometimes inconsistent (McCoy et al., 2023;Shojaee et al., 2025).If such models are unable to solve linguistic-mathematical problems involving much simpler mathematics, and introducing linguistic structure into the problem causes their reasoning ability to break down, this indicates limitations in the scope of their reasoning -models may be unable to apply their reasoning flexibly across domains in the ways that humans do.</p>
<p>Methods</p>
<p>Models.We used OpenAI o1-mini (Jaech et al., 2024) and DeepSeek-R1-distill-Qwen-7B (Guo et al., 2025) reasoning models to conduct our experiments, querying o1-mini via the API and running DeepSeek locally.All code and data used for our experiments are available at https://github.com/antara-raaghavi/multilingual-number-puzzles.Results for explicit operator experiments, for the single-character variable case (For the results on multi-character variables, see Section B Figure 7).Making operators explicit shows performance improvement over the IMPLICIT condition, but this is only substantially and reliably the case when the operator is made explicit with a familiar symbol like "+".Error bars denote standard error of the mean.10 problems, 5 iterations per problem.</p>
<p>We additionally queried an instruction-tuned model (qwen-2-7b-vl-instruct) and a base model (llama-3.1-8B),both of which had an accuracy of 0 across all conditions that we test.These models almost always generated longer text answers without numbers rather than the simple numerical answer required, and were hence excluded from our analyses.</p>
<p>Data.We obtained data for linguistics olympiad problems from two publicly available datasets: Lin-gOly (Bean et al., 2024) and Linguini (Sánchez et al., 2024), filtering both datasets for problems tagged as "number systems".After filtering, we had 15 problems from the LingOly and 8 problems from the Linguini dataset.Not every problem in the dataset could be standardized in the ways that our experiments required.The entire dataset was thus manually evaluated for suitable problems, and 10 problems were chosen for evaluation, all in distinct languages (see Section D).These problems spanned a range of difficulty from the first round of the UK Linguistics Olympiad to the International Linguistics Olympiad (most challenging).</p>
<p>Experiments</p>
<p>The effect of explicit operators in problems</p>
<p>Since so many of the mathematical operators in numeral structure are implicit (e.g. in English we say 'twenty three' to mean 'twenty + three'), our first experiment investigates how this implicit struc-ture affects how models solve the problems.To do this, we standardize and convert the 10 existing linguistic number system problems to mathematical problems, and vary how explicit the operators are, as shown in Table 1.First, we standardize all problems to control for model tokenization and task-external knowledge effects: we identify all meaningful morphemes, standardize all phonological changes, and replace them with dummy words as described in detail in Section A. This standardized version of each problem is what we call the IMPLICIT setting, since the mathematical operations are largely implicit, as they are in language.Taking these IMPLICIT problems as our baselines, we then make the operators explicit in three ways: 1) as the familiar mathematical operator symbols that perform the operation (e.g.'+' for addition), 2) as symbols that are unfamiliar for performing that operation, and 3) as whole words sampled from the tokenizer.A full example prompt with a puzzle in four variations is provided in Section B.</p>
<p>We present our results in Figure 1.In all cases, the presence of explicit operations with familiar symbols yields significant improvements over the default IMPLICIT condition (o1-mini performs at ceiling).In the multi-character setting (more linguistic), models perform better on average in the IMPLICIT condition than in the case with an explicit operator as an unfamiliar random word (vid.Figure 7).It is likely harder to differentiate between function words (operators) and number words (nu-merals) in such a setting -this finding is consistent with work that has shown human solvers also find a problem to be more difficult when the operator word is explicit but unfamiliar (Derzhanski and Veneva, 2018).Overall, our results demonstrate that it is difficult for models to reason about the abstract idea that linguistic quantities might contain operators, if the operators are not explicitly provided using familiar symbols.</p>
<p>Error analysis</p>
<p>We observe some common patterns of error in the model responses.For the three problems which involved squares and cubes of numbers, when the operators were not explicit and familiar, o1-mini almost always responded by pattern-matching (e.g.providing another square/cube number) instead of solving the problem, as seen in Table 2. o1-mini also reproduced a number given in the input question as the answer in several cases (11 for the multitoken condition, and 3 for the single-token condition, across 150 trials) when the operators were not explicit and familiar.</p>
<p>Condition</p>
<p>Single Multi explicit symbol 8 4 explicit random word 0 8 implicit 14 5 Further, when o1-mini answered a problem incorrectly, its responses were often inconsistent across the five trials of that problem.Notably, in 50% of single-character cases lacking explicit and familiar operators, all five responses were distinct and incorrect.This further shows that performance appears to depend on the presence of explicit operator cues; in their absence, o1-mini does not reliably solve the problem.</p>
<p>Providing contextual information</p>
<p>Our first experiment showed that in the absence of problem-specific instructions, when given a linguistic-mathematical problem directly, LLMs struggle to solve it unless the operations are both explicit and familiar.This leaves open the question of whether providing additional problem-specific information would affect the model performance.We thus modulate the context of the problem in three different ways.We query the same four prob-lem variants as described in Table 1, additionally providing the following contextual information:</p>
<p>Language: "Here is a puzzle based on numbers in the {language} language."</p>
<p>Base: "Here is a puzzle based on numbers in a language that uses a base-{n} numeral system."</p>
<p>Implicit operations: "Here is a puzzle based on numbers in a language.In this language, numbers may be constructed through implicit operations like addition (twenty-nine = 20 + 9) or multiplication (five hundred = 5 × 100)."[only for IMPLICIT condition]</p>
<p>Figure 2: Language and base information only helps in the IMPLICIT case.Effect of adding language or numeral base information, plotted as a difference from the baseline values in Figure 1 for o1-mini.In cases with explicit operators, conflating overtly mathematical and linguistic information appears to confuse the models.</p>
<p>We compare these to the baseline results from Section 4.1 for o1-mini, presenting our results in Figure 2. In cases other than the implicit operator condition, the model seems to recognize the problem as requiring a more mathematical kind of reasoning, so providing linguistic information seems to confuse the model and average performance is worse.However, in the implicit operator (A B) condition, model performance improves significantly, perhaps because the setting of the problem is less overtly mathematical.In Figure 3, we show that providing information about the implicit reasoning needed is not as significant a boost as activating knowledge about the specific language.</p>
<p>Ablations: constructed minimal-pair problems</p>
<p>In order to ensure that it is the difference in operators (as opposed to other features of the numeral system) that explains the models' inability to solve these problems, we performed an ablation study to test whether models could handle other aspects of numeral construction and combination.Our experiment is inspired by the notion of a linguistic minimal pair, a pair of linguistic items that differ in exactly one meaningful element.We construct minimal pairs of simple, synthetic number system problems, where every element is the same except for one specific parameter that differs between two paired problems.We tested five major parameters of numeral systems, as described in Table 3.In all cases, GPT-4 and more advanced models could solve the template problems.It thus appears that most basic "building blocks" of number systems (e.g. the base of the system, the order of numerals, etc.) did not affect model performance in isolation, but the models consistently fail to solve number problems that involve constructing and combining complex numerals.Table 3: Minimal pair results: GPT-4 and o1-mini solve all problem pairs, GPT-3.5-turbostruggles with numeral base and combination.DeepSeek-R1-distill-Qwen-7B does not produce an answer in the right format for most settings, so it is excluded from this table.Further data on testing all bases 4-19 linked in Table 6.</p>
<p>Discussion and Conclusions</p>
<p>We study the entanglement between linguistic and numeric knowledge in language models, focusing on the ability of models to use mathematical reasoning in problems that display the implicit numerical structure in language.In the setting of these linguistic-mathematical puzzles, we show that the overtness and familiarity of operators affects the performance of language models, although many humans are able to understand how numeral systems work and hence solve the problems without needing specified operators.However, a broader study with different controls and parameter settings remains open for future work.Since all our evaluation was standardized and closed-form, we welcome research on open-ended evaluation of reasoning task responses.Current language models seem to display some level of emergent modular structure (Teehan et al., 2022;Lepori et al., 2023)perhaps linguistic and mathematical tasks activate separate circuits or subspaces in models, and understanding the ways in which reasoning fine-tuning and reinforcement learning interacts with linguistic pretraining is another promising avenue for future research.Investigating such questions enriches our understanding of both computational and human approaches to representing numbers in language.</p>
<p>The ability to understand language and abstract rule-governed systems is a fascinating aspect of human intelligence, and we hope that our research provides some insight into the understanding of this remarkable human trait.</p>
<p>Limitations</p>
<p>We acknowledge the possibility that our results are explained by limitations in the training data and the small size of our dataset, as language models often equal human performance on benchmarks for which they have large quantities of similar-enough training data (Achiam et al., 2023).Perhaps an LLM trained on a massive corpus of linguistic number system problems would be able to solve new, previously unseen number system problems.But the data today are far too limited for such an approach, and crucially, a human solver who is familiar with existing number system problems can generalize to unseen problems extremely well!Even a human solver who is unfamiliar with existing number system problems can in theory solve any problem they are provided just by logically reasoning.Importantly, we note that although this may not be true of the average human, when comparing the top end of humans with the top-performing current language models, it is clear that intuiting rules from human-scale data is still challenging for LLMs.</p>
<p>A Randomization procedure for task-external knowledge and tokenization handling</p>
<p>In this section, we address the specific changes we make across linguistic number system problems to convert them into templates suitable for our dataset.</p>
<p>In order to truly test whether the model is solving a problem, it should not be affected by factors external to the problem, such as flawed tokenization or the usage of memorized knowledge external to the provided task.1</p>
<p>Our strategy to remediate this issue is thus: in the single-letter token setting, we separated all characters by whitespaces to ensure correct tokenization.In the multi-token setting, we identified all meaningful morphemes in the problems and standardized them to remove any phonological changes, such that every morpheme had exactly one surface representation.We separated every meaningful morpheme with whitespaces, and mapped each morpheme to a randomly generated multi-token "dummy word" for each iteration of each experiment.We created each of these "dummy words" by randomly sampling short tokens (length ≤ 3) from the language models' respective tokenizer vocabularies, and concatenating tokens together to create unfamiliar words.</p>
<p>For tokenizers which use schemes like byte-pair encoding, any input string will get mapped to some sequence of tokens that are present in the vocabulary, so there is no situation in which the model will see an unknown token.Since the dummy words themselves have no meaning, the model cannot directly draw on task-external linguistic information to solve the presented problems.For simplicity we restricted the random draw to those containing only romanized (Latin alphabet) characters.We also excluded tokens that contained any numeral symbols from 0-9, to ensure that the the mathematical correctness of the problems was not affected.</p>
<p>B Multi-character-variable results from</p>
<p>Exp 1 We provide an example of our four variations of the puzzle in Table 4.To query all four variants, we used the same prompt "Here is a puzzle.Can you solve it?Please output only the answer (in place of the ??) and nothing else!".</p>
<p>C Base experiment</p>
<p>In order to understand whether sufficiently advanced language models would show performance that was invariant to changes in the base, we conducted a more fine-grained minimal pair experiment into the effect of numeral base on problem performance.Here, the solver would see the Hindu-Arabic numerals corresponding to the English base-10 representation of the numbers, because the problem was presented in English.But the unknown symbols corresponded to the numbers as expressed in a different base, as shown in Figure 6.We conducted two different versions of this experiment.First, we mapped the unknown symbols to the single-character whitespaced A, B, C, and D tokens, as in Figure 6.In the second version, each of the four unknown symbols (A, B, C, D) was instead represented by a corresponding random token drawn from the tokenizer vocabulary, to ensure that the context of the specific tokens A, B, C, and D was not influencing our results.</p>
<p>We tested four increasingly sophisticated GPT models (GPT-3.5-turbo,GPT-4, GPT-4o, and o1mini) on both versions of the experiment and provide results in Table 6.GPT-4o and o1-mini solved all problems in both conditions, displaying performance that was robust to the base of the problem.</p>
<p>D Table of languages</p>
<p>The 10 problems that we used for our analyses.The problems range in difficulty from the first and second rounds of the UK Linguistics Olympiad (UKLO R1 and R2) to the International Linguistics Olympiad, which typically has the most challenging problems.Base GPT-3.5-turboGPT-4 GPT-4o o1-mini ABCD Random ABCD Random ABCD Random ABCD Random</p>
<p>Language ISO code
4 ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 5 ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 6 ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 7 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 8 ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 9 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 11 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 12 ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ 13 ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 14 ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 15 ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 16 ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✓ 17 ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ 18 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 19 ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓
Figure 1 :
1
Figure1: Making operators explicit significantly improves performance.Results for explicit operator experiments, for the single-character variable case (For the results on multi-character variables, see Section B Figure7).Making operators explicit shows performance improvement over the IMPLICIT condition, but this is only substantially and reliably the case when the operator is made explicit with a familiar symbol like "+".Error bars denote standard error of the mean.10 problems, 5 iterations per problem.</p>
<p>Figure 3 :
3
Figure3: Extra information improves performance on IMPLICIT problems (A B).Information about implicitness is helpful, but not as much as more direct information like the problem language.Error bars denote standard error of the mean.5 iterations / problem.</p>
<p>Figure 4 :
4
Figure 4: Example of full minimal pair template problem, for the Order parameter, where we varied whether digits are read left-to-right or right-to-left.</p>
<p>Figure 5 :
5
Figure 5: Drehu (IOL 2010) problem</p>
<p>pagig) + masaad + opbob = 31 (masaad × pagig) + masaad + buylen = 26 . . .(ajssci × pagig) + (ajssci × kould) = 50 (innops × pagig) + innops + opbob = ?? Implicit masaad pagig nge masaad opbob = 31 masaad pagig nge masaad buylen = 26 . . .ajssci pagig nge ajssci kould = 50 innops pagig nge innops opbob = ?? Explicit + unfamiliar (Greek) (masaad β pagig) α masaad α opbob = 31 (masaad β pagig) α masaad α buylen = 26 . . .(ajssci β pagig) α (ajssci β kould) = 50 (innops β pagig) α innops α opbob = ?? Explicit + unfamiliar (random) (masaad hibcat pagig) xebrut masaad xebrut opbob = 31 (masaad hibcat pagig) xebrut masaad xebrut buylen = 26 . . .(ajssci hibcat pagig) xebrut (ajssci hibcat kould) 50 (innops hibcat pagig) xebrut innops xebrut opbob = ??</p>
<p>Figure 6 :
6
Figure 6: Setup for base experiment</p>
<p>Figure 7 :
7
Figure 7: Both o1-mini and DeepSeek struggle with the explicit-unfamiliar condition (o1 shows negative improvement, DeepSeek shows 0%) in the multi-character setting.Error bars denote standard error of the mean.5 iterations / problem tested for 10 problems.</p>
<p>Figure 8 :
8
Figure8: Results per language, (a) single-character (b) multi-character: performance varies significantly by problem and operator type.Note that Drehu (dhv) and Georgian (kat) are two of the easiest problems in our dataset: much of the difficulty for human solvers is in the phonological changes and unintuitive order that the numbers are presented in, both of which we standardize away for our controlled datasets.Without those parameters, the systems are straightforward vigesimal-decimal systems like French, which the models have almost certainly had exposure to.</p>
<p>Table 1 :
1
A demonstration of the experimental conditions for our explicit operators experiment.We add explicit operators to our base IMPLICIT problems, using both the familiar symbols for addition/multiplication/subtraction, as well as unfamiliar symbols and words to symbolize the operation.
VariableOperatorExampleExplicitness Familiarity TypeSingle character Implicit--A BExplicitFamiliarSymbol A + BExplicitUnfamiliar Symbol A α BExplicitUnfamiliar WordA xebrut BMulti-character Implicit--gbaifi pagigExplicitFamiliarSymbol gbaifi + pagig. . .. . .. . .. . .</p>
<p>Table 2 :
2
Incorrect pattern-matched square / cube answers (out of 15 possible trials)</p>
<p>Table 4 :
4
Example of four problem variants in the multicharacter setting, corresponding to Drehu (IOL 2010) dataset problem in Figure5.</p>
<p>Base Level
Drehudhv20IOLGeorgiankat20 UKLO R1Gumatjgnn5 UKLO R1Ndomnqm6IOLNgkolmpukcd6 UKLO R1Northern Pamepmq8 UKLO R1Umbu-Unguubu24IOLWaoraniauc5 UKLO R1Yorubayor20 UKLO R2Yup'ikesu20 UKLO R2</p>
<p>Table 5 :
5
Languages and problem features in final dataset (after removing/standardizing phenomena)</p>
<p>Table 6 :
6
Base experiment results: GPT-4o / o1-mini solve every problem, regardless of randomization</p>
<p>Memorized knowledge would also help a human solver, but people are much less likely to know the number systems of different (particularly low-resource) languages. Although linguistics olympiad contestants might know more number systems than the average person, there are over 7,000 human languages, so the probability of knowing a specific system is low. Moreover, since LLM training corpora scrape large portions of the internet, the breadth of their memorized knowledge far exceeds that of an average human.
AcknowledgmentsThe authors gratefully thank Tom McCoy and Kaden Holladay for helpful discussions in the initial stages of this project.This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023arXiv preprint</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.00157Large Language Models for Mathematical Reasoning: Progresses and Challenges. 2024arXiv preprint</p>
<p>Simi Andrew M Bean, Harry Hellsten, Jabez Mayne, Ethan A Magomere, Ryan Chi, Scott A Chi, Hannah Rose Hale, Kirk, arXiv:2406.06196Lingoly: A benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. 2024arXiv preprint</p>
<p>Large linguistic models: Analyzing theoretical linguistic abilities of LLMs. Gašper Beguš, Ryan Rhodes, arXiv:2305.009482023arXiv preprint</p>
<p>Bootstrapping &amp; the origin of concepts. Susan Carey, Daedalus. 13312004</p>
<p>Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2. Yuri Chervonyi, H Trieu, Miroslav Trinh, Xiaomeng Olšák, Hoang Yang, Marcelo Nguyen, Junehyuk Menegali, Vikas Jung, Quoc V Verma, Thang Le, Luong, arXiv:2502.035442025arXiv preprint</p>
<p>Typology of numeral systems. Bernard Comrie, Numeral types and changes worldwide. Trends in Linguistics. Studies and monographs. 2011118</p>
<p>Stanislas Dehaene, The number sense: How the mind creates mathematics. Oxford University Press USA2011</p>
<p>The Linguistics Olympiads: Academic competitions in linguistics for secondary school students. Ivan Derzhanski, Thomas Payne, 2010Linguistics at school: language awareness in primary and secondary education</p>
<p>Linguistic Problems on Number Names. Ivan Derzhanski, Milena Veneva, Proceedings of the Third International Conference on Computational Linguistics in Bulgaria (CLIB 2018). the Third International Conference on Computational Linguistics in Bulgaria (CLIB 2018)2018</p>
<p>MathOdyssey: Benchmarking Mathematical Problem-solving Skills in Large Language Models using Odyssey Math Data. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou, arXiv:2406.183212024arXiv preprint</p>
<p>Core systems of number. Lisa Feigenson, Stanislas Dehaene, Elizabeth Spelke, Trends in cognitive sciences. 872004</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, arXiv:2501.12948Xiao Bi, and 1 others. 2025. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint</p>
<p>Rarities in numeral systems. Rethinking universals: How rarities affect linguistic theory. Harald Hammarström, 201045</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Language and number: The emergence of a cognitive system. James R Hurford, B. Blackwell1987</p>
<p>The Universal History of Numbers. Georges Ifrah, 2000Harvill London</p>
<p>The composition of complex cardinals. Tania Ionin, Ora Matushansky, Journal of semantics. 2342006</p>
<p>Alex Beutel, Alex Carney, and 1 others. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, arXiv:2412.16720OpenAI o1 System Card. 2024arXiv preprint</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>One, two, three, four, nothing more: An investigation of the conceptual sources of the verbal counting principles. Le Mathieu, Susan Corre, Carey, Cognition. 10522007</p>
<p>Break it down: Evidence for structural compositionality in neural networks. Michael Lepori, Thomas Serre, Ellie Pavlick, Advances in Neural Information Processing Systems. 202336</p>
<p>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve. Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Eduardo Sánchez, Belen Alastruey, Christophe Ropers, Pontus Stenetorp, Mikel Artetxe, Marta R Costa-Jussà, arXiv:2409.12126Linguini: A benchmark for language-agnostic linguistic reasoning. 2024arXiv preprint</p>
<p>How counting leads to children's first representations of exact, large numbers. Barbara W Sarnecka, Meghan C Goldman, Emily B Slusser, 2015Oxford University Press</p>
<p>Samy Bengio, and Mehrdad Farajtabar. 2025. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity. Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, arXiv:2506.06941arXiv preprint</p>
<p>Emergent Structures and Training Dynamics in Large Language Models. Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan, 10.18653/v1/2022.bigscience-1.11Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguistics2022</p>
<p>Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang, arXiv:2411.03766Number Cookbook: Number Understanding of Language Models and How to Improve It. 2024arXiv preprint</p>
<p>Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, arXiv:2409.18486Evaluation of OpenAI o1: Opportunities and challenges of AGI. 2024arXiv preprintXiaowei Yu, and 1 others</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9471 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9471</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9471</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-7ca300c16abbd38382dec5b7ea6809fee570be54</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ca300c16abbd38382dec5b7ea6809fee570be54" target="_blank">ReFT: Reasoning with Reinforced Fine-Tuning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Reformed Fine-Tuning first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers.</p>
                <p><strong>Paper Abstract:</strong> One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9471",
    "paper_id": "paper-7ca300c16abbd38382dec5b7ea6809fee570be54",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004939,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReFT: Reasoning with Reinforced Fine-Tuning</h1>
<p>Trung Quoc Luong<em>, Xinbo Zhang</em>, Zhanming Jie*, Peng Sun ${ }^{\dagger}$, Xiaoran Jin, Hang Li<br>ByteDance Research<br>{trung.luong, zhangxinbo.freya, allan.jie}@bytedance.com<br>{wanhesong, xiaoran.jin, lihang.lh}@bytedance.com</p>
<h4>Abstract</h4>
<p>One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-ofThought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problemsolving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problemsolving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for $\mathrm{ReFT}^{1}$.</p>
<h2>1 Introduction</h2>
<p>The state-of-the-art approaches to solving math problems (Luo et al., 2023; Wang et al., 2023a) employ Supervised Fine-Tuning (SFT) to train the models using Chain-of-Thought (CoT) annotations (Wei et al., 2022). As shown in Figure 1, a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of question $(x)$, CoT $(e)$, and answer $(y)$ in GSM8K (Cobbe et al., 2021a). The SFT process iterates several epochs on the training data. The proposed ReFT warm-up from SFT and performs RL training on the same data.</p>
<p>CoT annotation outlines the intermediate reasoning steps toward solving a math problem.</p>
<p>Usually there is one CoT annotation for each question in the training data, i.e., one correct reasoning path, which is utilized in SFT. We observe that this may result in relatively weak generalization abilities of the SFT models. It is often the case that multiple valid CoT annotations exist for the same question (Cobbe et al., 2021a; Zhang et al., 2023), underscoring the need for a more powerful fine-tuning approach. To address this problem, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) (Figure 1 bottom).</p>
<p>ReFT commences with a warm-up stage involving Supervised Fine-Tuning (SFT) in one or two epochs (Figure 1, shaded box). This initial stage equips the model with the ability to generate correct responses to mathematical problems to some extent, as demonstrated in prior work (Cobbe et al., 2021a). Next, ReFT proceeds to further refine the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison between SFT and ReFT on the presence of CoT alternatives.</p>
<p>model through the utilization of an online Reinforcement Learning (RL) algorithm <em>Sutton and Barto (2018)</em>, specifically Proximal Policy Optimization (PPO) <em>Schulman et al. (2017)</em> in this paper. In this way, ReFT is able to sample multiple correct reasoning paths or CoT annotations and learn from them (Figure 2, right).</p>
<p>Since the training data include ground-truth answers, the golden rewards can be naturally derived from them when training PPO. Consequently, there is no requirement for a separately trained reward model. In contrast, RLHF <em>Ouyang et al. (2022)</em> has to utilize a reward model that is learned from human-labeled data.</p>
<p>During the warm-up stage, ReFT acquires a certain level of accuracy by supervised learning. In the RL stage, ReFT further enhances its ability by reinforcement learning through sampling various CoT reasoning paths. In this way, ReFT gets much richer supervision signals than SFT. This approach enables ReFT to greatly improve generalization in math problem-solving <em>Gao et al. (2018); Brown et al. (2020)</em>. Note that ReFT outperforms SFT by using the same training questions, without relying on extra or augmented training questions. In fact, ReFT does not conflict with such data engineering and can be seamlessly combined with it.</p>
<p>Our contributions are as follows:</p>
<ul>
<li>We introduce a novel fine-tuning approach, reinforced fine-tuning (ReFT), which utilizes reinforcement learning to solve math problems. ReFT exhibits enhanced generalization capabilities compared to conventional supervised fine-tuning when trained on the same dataset.</li>
<li>We conduct extensive experiments using two foundational models, CodeLLAMA <em>Roziere et al. (2023)</em> and Galactica <em>Taylor et al. (2022)</em>, on three standard datasets: GSM8K <em>Cobbe et al. (2021a)</em>, MathQA <em>Amini et al. (2019)</em>, and SVAMP <em>Patel et al. (2021)</em>. Our experiments cover both natural language and program-based CoTs, demonstrating the significantly improved performance and generalization ability of ReFT.</li>
<li>Additionally, we demonstrate that ReFT benefits from both majority voting <em>Wang et al. (2023b)</em> and reward model reranking <em>Uesato et al. (2022)</em> at inference-time, further improving its performance.</li>
</ul>
<h2>2 Related Work</h2>
<h3>Math Problem Solving</h3>
<p>Recent research efforts focus on CoT prompt design and data engineering. Most of them attempted to make CoT comprehensive and fine-grained to present the step-by-step reasoning solutions <em>Nye et al. (2021); Fu et al. (2023); Zhou et al. (2023b); Khot et al. (2023); Zelikman et al. (2022); Imani et al. (2023); Miao et al. (2023)</em>. <em>Gao et al. (2023)</em> further proposed to use the Python program as CoT prompt, demonstrating more accurate reasoning steps and significant improvements over the natural language CoT <em>Wei et al. (2022)</em>. <em>Zhou et al. (2023a)</em> introduced a prompting method that generates code to verify the intermediate reasoning step with GPT-4 <em>OpenAI (2023)</em>, thus achieving state-of-the-art performance on GSM8K <em>Cobbe et al. (2021a)</em> and MATH <em>Hendrycks et al. (2021)</em>. Another line of work focuses on improving the quality of CoT <em>Wang et al. (2023a); Liu et al. (2023); Yu et al. (2023)</em> and increasing the amount of CoT data <em>Luo et al. (2023); Yue et al. (2023)</em> from OpenAI's ChatGPT (gpt-3.5-turbo) or GPT-4<sup>2</sup>.</p>
<h3>Reinforcement Learning</h3>
<p>Our work is mostly related to the recent work that applies PPO <em>Schulman et al. (2017)</em> to natural language process for aligning human preferences <em>Ouyang et al. (2022)</em>. Since then, several training algorithms have been proposed to efficiently improve the alignment, including direct preference optimization (DPO) <em>Rafailov et al. (2023)</em>, identity preference optimization (IPO) <em>Azar et al. (2023)</em>, and Kahneman-Tversky optimization (KTO) <em>Ethayarajh et al. (2023)</em>. Other than the purpose of alignment, we aim to adopt reinforcement learning as a fine-tuning paradigm to improve performance over conventional supervised fine-tuning.</p>
<p>Specifically for solving math problems, <em>Uesato et al. (2022)</em> and <em>Lightman et al. (2023)</em> trained an outcome-based or process-based reward model to</p>
<p><sup>2</sup>https://chat.openai.com/</p>
<p>perform reranking <em>Cobbe et al. (2021a)</em> to achieve much better performance over SFT and majority voting <em>Wang et al. (2023b)</em>. While our approach aims to improve the performance of the policy itself, these reward model reranking approaches can be easily integrated into the resulting policy model.</p>
<h2>3 Method</h2>
<p>In this work, we focus on natural language CoT (N-CoT) <em>Wei et al. (2022)</em> (Figure 1) and programbased CoT <em>Gao et al. (2023)</em> (P-CoT) using Python. <em>Gao et al. (2023)</em> proposed the programbased CoT for math problem solving. We can simply execute the program to obtain the answer. To ensure clarity and avoid ambiguity, we use the terms N-CoT and P-CoT to represent natural language and program-based CoTs, respectively.</p>
<h3>3.1 Reinforced Fine-Tuning</h3>
<p>The proposed Reinforced Fine-Tuning (ReFT) process consists of two stages: the warm-up stage and the reinforcement learning stage. The overall algorithm is shown in Algorithm 1.</p>
<p>Warm-up In this stage, the policy is fine-tuned for a few epochs on a dataset comprising of the " $($ question, CoT)" tuples: $(\boldsymbol{x}, \boldsymbol{e})$. It enables the model to have basic problem-solving skills to generate a proper response ${ }^{3}$. Formally, the CoT generation process can be decomposed into a sequence of next token prediction actions. The last action token, <eos>, signals the generation process to terminate. The CoT $\boldsymbol{e}$ is written as:</p>
<p>$$
\boldsymbol{e}=\left[a_{1}, a_{2}, \ldots, a_{L-1}, a_{L}=\langle\text { eos }\rangle\right]
$$</p>
<p>where $L$ represents the maximum length. At timestep $t$, the action $a_{t}$ is sampled from a policy $\boldsymbol{\pi}<em t="t">{\boldsymbol{\theta}}\left(\cdot \mid s</em>$ :}\right)$ where $a_{t}$ can be any token in the vocabulary and the state $s_{t}$ comprises of all tokens in the question and all tokens generated so far. After each action, the resulting state $s_{t+1}$ is the concatenation of the current state $s_{t}$ and the action $a_{t</p>
<p>$$
s_{t+1}= \begin{cases}\boldsymbol{x}, &amp; t=0 \ {\left[s_{t}, a_{t}\right],} &amp; 1 \leq t \leq L\end{cases}
$$</p>
<p>As the produced action is the <eos> token, the resulting state $s_{L+1}$ is the terminal state and the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>generation process is finished. With this notation, the loss function for a sample can be written as:</p>
<p>$$
\mathcal{L}<em _boldsymbol_e="\boldsymbol{e">{S F T}(\boldsymbol{\theta})=-\mathbb{E}</em>} \sim \mathcal{D}}\left[\sum_{t=1}^{L} \log \left(\boldsymbol{\pi<em t="t">{\boldsymbol{\theta}}\left(a</em>\right)\right)\right]
$$} \mid s_{t</p>
<p>Reinforcement Learning In this stage, the policy improves its performance via a form of online self-learning using a dataset comprising of (question, answer) tuples: $(\boldsymbol{x}, \boldsymbol{y})$. Specifically, the policy model learns by repeatedly sampling responses (Figure 2), evaluating the response's answer correctness, and updating its parameters in an online fashion (line 7-14 in Algorithm 1). We employ PPO <em>Schulman et al. (2017)</em> with a clipped objective algorithm for training. Following <em>Ziegler et al. (2019)</em>, the value model $V_{\phi}$ is constructed by appending a linear value head on top of the last hidden states of the policy model $\pi_{\theta}$, which is the model after the warm-up stage. The reward of 0 is given for all action resulting in non-terminal state. At the terminal state, we use a reward function that directly compares the answer extracted from the state's CoT and the ground-truth answer $\boldsymbol{y}$. Here, the reward function returns 1 if the answer is deemed correct, otherwise 0 is returned. On dataset whose answers are all numeric, partial reward <em>Zhong et al. (2017); Le et al. (2022)</em> of 0.1 can be applied when the answer can be extracted and it is of numeric type. For $1 \leq t \leq L$, we write</p>
<p>$$
r\left(s_{t}, a_{t}, s_{t+1}\right)= \begin{cases}1, &amp; \operatorname{EXTRACT}\left(s_{t+1}\right)=\boldsymbol{y} \ 0.1, &amp; \operatorname{EXTRACT}\left(s_{t+1}\right) \neq \text { null, } \neq \boldsymbol{y} \ 0, &amp; \operatorname{EXTRACT}\left(s_{t+1}\right)=\text { null }\end{cases}
$$</p>
<p>Such a partial reward can help reduce the effect of learning from sparse reward <em>Riedmiller et al. (2018); Trott et al. (2019)</em>. In addition, following <em>Zheng et al. (2023)</em>, our total reward is the sum of the reward function score and the KullbackLeibler (KL) divergence <em>Kullback and Leibler (1951)</em> between the learned RL policy and initial policy scaled by a coefficient factor $\beta$.</p>
<p>$$
\begin{aligned}
r_{\text {total }}\left(s_{t}, a_{t}, s_{t+1}\right) &amp; =r\left(s_{t}, a_{t}, s_{t+1}\right) \
&amp; -\beta K L\left(\boldsymbol{\pi}<em t="t">{\boldsymbol{\theta}}\left(\cdot \mid s</em>}\right), \boldsymbol{\pi<em t="t">{\boldsymbol{\theta}}^{(0)}\left(\cdot \mid s</em>\right)\right)
\end{aligned}
$$</p>
<p>The generalized advantage estimate <em>Schulman et al. (2018)</em> is used for advantage calculation:</p>
<p>$$
\hat{A}<em l="0">{t}=\sum</em>
$$}^{L-t}(\gamma \lambda)^{l} \delta_{t+l</p>
<p>Input: $\mathcal{D}<em _theta="\theta">{\text {train }}={(\boldsymbol{x}, \boldsymbol{e}, \boldsymbol{y})}$ : Tuples of (question, CoT, answer), $W$ : number of warm-up steps, $T$ : number of RL steps, $U$ : number of updates per RL step, $\boldsymbol{\pi}</em>$ : Initial policy.
Output: $\pi_{\theta}$ : Final policy
$\pi_{\theta}=\pi_{\theta}^{(0)}$
// Warm-up stage
for $i \leftarrow 1$ to $W$ do
$x, e, y \sim \mathcal{D}}^{(0)<em _text="\text" _train="{train">{\text {train }}$
// Sample mini-batch from $\mathcal{D}</em>$
// Sample mini-batch from $\mathcal{D}}<em _text="\text" _train="{train">{\text {train }}$
// Reinforcement learning stage
// Sample mini-batch without CoT
// On-policy CoT sampling
// Extract the answer from CoT
for $i \leftarrow 1$ to $T$ do
$x, _, y \sim \mathcal{D}</em>$
\&amp; $\sim \pi_{\theta}(\boldsymbol{x})$
// Sample mini-batch without CoT
// On-policy CoT sampling
// Extract the answer from CoT
$\dot{\boldsymbol{y}} \leftarrow \operatorname{EXTRACT}(\hat{\boldsymbol{e}})$
$\pi_{\theta_{\text {old }}} \leftarrow \pi_{\theta}, V_{\phi_{\text {old }}} \leftarrow V_{\phi}$
Compute $\delta_{t}, \hat{A}}<em t="t">{t}, \hat{R}</em>}$ using $\pi_{\boldsymbol{\theta<em _boldsymbol_phi="\boldsymbol{\phi">{\text {old }}}, V</em><em L="L" R="R">{\text {old }}}, \boldsymbol{x}, \hat{\boldsymbol{e}}, \hat{\boldsymbol{y}}$ and $\boldsymbol{y}$
for $j \leftarrow 1$ to $U$ do
$\boldsymbol{\theta}, \boldsymbol{\phi}=\operatorname{OPtIMIZATION} _\operatorname{STEP}\left(\mathcal{L}</em>)\right)$
// Equation 2
return $\pi_{\theta}$
where the Temporal Difference (TD) is defined as
$\delta_{t^{\prime}}=-V_{\phi}\left(s_{t^{\prime}}\right)+r_{\text {total }}\left(s_{t^{\prime}}, a_{t^{\prime}}, s_{t^{\prime}+1}\right)+\gamma V_{\phi}\left(s_{t^{\prime}+1}\right)$
with the terminal state value $V_{\phi}\left(s_{L+1}\right):=0, \lambda \in$ $(0,1]$ is the discount factor for rewards, and $\gamma \in$ $[0,1]$ is the discount factor for TD. For the estimate of return, we leverages the $\lambda$-return $\hat{R}_{t}$, which can be written as the sum of the generalized advantage estimate and the value estimate:}(\boldsymbol{\theta}, \boldsymbol{\phi</p>
<p>$$
\hat{R}<em t="t">{t}=\hat{A}</em>\right)
$$}+V_{\phi}\left(s_{t</p>
<p>Lastly, the policy and value objectives can be written as in two equations below</p>
<p>$$
\begin{gathered}
\mathcal{L}<em _boldsymbol_e="\boldsymbol{e">{\text {policy }}(\boldsymbol{\theta})=-\mathbb{E}</em>} \sim \boldsymbol{\pi<em _old="{old" _text="\text">{\theta</em>}}}}\left[\min \left(\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)} \hat{A<em _theta="\theta">{t}\right.\right. \
\left.\left.\operatorname{clip}\left(\frac{\pi</em>}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}, 1-\epsilon, 1+\epsilon\right) \hat{A<em _text="\text" _value="{value">{t}\right)\right] \
\mathcal{L}</em>}}(\boldsymbol{\phi})=\frac{1}{2} \mathbb{E<em _old="{old" _theta__text="\theta_{\text">{\boldsymbol{e} \sim \boldsymbol{\pi}</em>}}}}\left[\max \left(\left|V_{\phi}\left(s_{t}\right)-\hat{R<em t="t">{t}\right|^{2}\right.\right. \
\left.\left.\left|\operatorname{clip}\left(\hat{R}</em>}-V_{\phi}\left(s_{t}\right), \hat{A<em t="t">{t}-\epsilon, \hat{A}</em>\right)\right]
\end{gathered}
$$}+\epsilon\right)\right|^{2</p>
<p>where $\pi_{\theta_{\text {old }}}, V_{\phi_{\text {old }}}$ are used for sampling CoT and computing $\hat{A}<em t="t">{t}, \hat{R}</em>$. The unified loss function is the weighted sum of the above objectives.</p>
<p>$$
\mathcal{L}<em _policy="{policy" _text="\text">{R L}(\boldsymbol{\theta}, \boldsymbol{\phi})=\mathcal{L}</em>
$$}}+\alpha \mathcal{L}_{\text {value }</p>
<p>where $\alpha$ is the coefficient for the value objective.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets</h3>
<p>We conduct experiments on three math problem datasets: GSM8K (Cobbe et al., 2021a), SVAMP (Patel et al., 2021) and MathQA (Amini et al., 2019). For both GSM8K and SVAMP, the format of answers is a numeric value. In MathQA, the format is instead a list of multiple choices (i.e., ABCD). Table 1 presents the statistics of all datasets. We perform few-shot prompting (Wei et al., 2022; Gao et al., 2023) using GPT-3.5-turbo to obtain both the N-CoT and P-CoT annotations ${ }^{4}$. The NCoT and P-CoT annotations are obtained following Jie et al. (2023). We also conducted an additional experiment on a numeric version of MathQA (Jie and Lu, 2023) where the format is also a numeric value. Such experiments are used to demonstrate our assumptions of potential reward hacking phenomenon (Skalse et al., 2022) on MathQA (§4.4).</p>
<h3>4.2 Baseline</h3>
<p>We compare ReFT with SFT and self-training (Xie et al., 2020; Amini et al., 2022) baselines. SFT simply fine-tunes the language model on the train-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8k</th>
<th>SVAMP</th>
<th>MathQA_{MCQ}</th>
<th>MathQA_{numeric}</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train N-CoT</td>
<td>7,465</td>
<td>3,076</td>
<td>14,862</td>
<td>8,955</td>
</tr>
<tr>
<td>Train P-CoT</td>
<td>7,356</td>
<td>3,043</td>
<td>15,250</td>
<td>7,672</td>
</tr>
<tr>
<td>Test</td>
<td>1,319</td>
<td>1,000</td>
<td>1,605</td>
<td>1,605</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the train and test datasets.
ing data. Experiments with self-training methods ensure a relatively fair comparison because these methods share the mechanism that the samples generated from the model are used for training.</p>
<p>We implemented Offline Self-Training (OfflineST) <em>He et al. (2020)</em>, and Online <em>Hoi et al. (2021)</em> Self-Training (Online-ST). The Offline-ST method is similar to expert iteration <em>Anthony et al. (2017); Uesato et al. (2022); Zelikman et al. (2022)</em>. We first use the SFT checkpoint from the early checkpoint to sample the CoTs and verify them against the ground truth. We only retain those expert samples that have a correct answer. We perform SFT on the combination of original training data and the expert samples.</p>
<p>The Online-ST method is made to be closely comparable to ReFT. Following ReFT, Online-ST has the same warm-up process. After that, we perform continual training with the samples generated on the fly. At each training step, the model first samples CoTs for a batch and only retains those with correct answers. The resulting batch consists of both sampled and ground-truth CoTs. We then update the model parameters on this batch with the supervised fine-tuning objective $\mathcal{L}_{SFT}$. Compared with ReFT, Online-ST neither makes use of negative responses (with an incorrect answer) nor has a dedicated mechanism to prevent the model from significantly diverging from the initial model, which can manifest as task-specific overfitting and training instability.</p>
<h3>4.3 Experimental Setup</h3>
<p>We conduct experiments with two foundation models: Galactica-6.7B (Taylor et al., 2022) and CodeLLAMA-7B (Roziere et al., 2023). Both models are reported to have strong performance in math solving and are commonly adopted in recent literature on reasoning tasks *Yue et al. (2023); Luo</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2023).</p>
<p>In addition to the comparison with baselines, we also apply common techniques, majority voting <em>Wang et al. (2023b)</em> and reward model reranking <em>Lightman et al. (2023)</em> on GSM8K.</p>
<p>Hyper-parameters In all experiments, the training is done with 8 A100-80GB GPUs using DeepSpeed <em>Rajbhandari et al. (2020); Rasley et al. (2020)</em> Zero stage 2 and HuggingFace Accelerate <em>Gugger et al. (2022)</em>. During the warm-up stage of ReFT, we use AdamW <em>Loshchilov and Hutter (2017)</em> optimizer with 10% warm-up ratio. The batch size is 48 and learning rate is $1e-5$. The maximum length is set to 1024. The number of epochs in the warmup stage is 2 in all settings except on MathQA_{MCQ} and MathQA_{numeric} where we use up to 5 and 10 respectively. The model is trained for 300 epochs with a learning rate of $3e-7$. Following Ziegler et al. (2019), the $\lambda, \gamma, \alpha, \epsilon$ and $U$ in PPO are set to 1 , $0.95,5,0.2$, and 2 , respectively. The KL coefficient $\beta$ is set to 0.01 for P-CoT and is set to 0.05 for N CoT experiments. Further hyperprameter settings about ReFT can be found in Appendix B.</p>
<p>For SFT baseline, we train the model for 40 epochs and choose the checkpoint with best performance. This number of epochs has been chosen to be sufficiently large to ensure SFT converges. For Offline-ST baseline, we sample the CoTs by using the checkpoint from the ReFT warm-up stage. Using the generation temperature of 1.0 and max length of 1024, we sample 100 CoTs for each question and only keep those with a correct answer. Following Singh et al. (2023), we then subsample the CoTs to 10 random unique CoTs per question to balance difficulties of questions. The number of fine-tune epoch is set to 20, which is sufficiently large to ensure the training to converge. As mentioned in $\S 4.2$, the Online-ST baseline tries to mimic the same setting as in ReFT. We have the same warm-up process and the hyperparameter setting is roughly the same as ReFT.</p>
<p>Reward Model Reranking Following <em>Cobbe et al. (2021a); Uesato et al. (2022)</em>, we train a reward model (RM) to determine the correctness of the CoT. To construct the RM training data, we use the model from the warm-up stage and perform sampling to obtain 100 CoTs for each question in the training set. The CoTs are deduplicated and the binary labels can be obtained by comparing the extracted answer against the ground truth.</p>
<p>As a common practice, the reward model is a</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Size</th>
<th>GSM8K</th>
<th></th>
<th>SVAMP</th>
<th></th>
<th>MathQA_{MCQ}</th>
<th></th>
<th>Average</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>N-CoT</td>
<td>P-CoT</td>
<td>N-CoT</td>
<td>P-CoT</td>
<td>N-CoT</td>
<td>P-CoT</td>
<td>N-CoT</td>
<td>P-CoT</td>
</tr>
<tr>
<td>Galactica + SFT</td>
<td>6.7B</td>
<td>42.68</td>
<td>58.83</td>
<td>54.50</td>
<td>70.09</td>
<td>58.07</td>
<td>64.61</td>
<td>51.75</td>
<td>64.51</td>
</tr>
<tr>
<td>Galactica + Offline Self-Training</td>
<td>6.7B</td>
<td>42.60</td>
<td>60.72</td>
<td>57.90</td>
<td>72.30</td>
<td>$\mathbf{6 0 . 7 5}$</td>
<td>67.04</td>
<td>53.75</td>
<td>66.69</td>
</tr>
<tr>
<td>Galactica + Online Self-Training</td>
<td>6.7B</td>
<td>47.84</td>
<td>62.93</td>
<td>59.40</td>
<td>$\mathbf{7 4 . 5 9}$</td>
<td>59.38</td>
<td>61.24</td>
<td>55.54</td>
<td>66.25</td>
</tr>
<tr>
<td>Galactica + ReFT</td>
<td>6.7B</td>
<td>$\mathbf{4 8 . 1 4}$</td>
<td>$\mathbf{6 8 . 9 1}$</td>
<td>$\mathbf{6 1 . 4 0}$</td>
<td>74.09</td>
<td>58.13</td>
<td>$\mathbf{7 0 . 4 7}$</td>
<td>$\mathbf{5 5 . 8 9}$</td>
<td>$\mathbf{7 1 . 1 6}$</td>
</tr>
<tr>
<td>CodeLLAMA + SFT</td>
<td>7B</td>
<td>43.59</td>
<td>63.68</td>
<td>58.09</td>
<td>75.40</td>
<td>56.01</td>
<td>64.79</td>
<td>52.56</td>
<td>67.96</td>
</tr>
<tr>
<td>CodeLLAMA + Offline Self-Training</td>
<td>7B</td>
<td>45.10</td>
<td>68.00</td>
<td>60.20</td>
<td>77.69</td>
<td>59.81</td>
<td>68.53</td>
<td>55.04</td>
<td>71.41</td>
</tr>
<tr>
<td>CodeLLAMA + Online Self-Training</td>
<td>7B</td>
<td>44.66</td>
<td>67.85</td>
<td>58.60</td>
<td>77.40</td>
<td>56.95</td>
<td>68.85</td>
<td>53.40</td>
<td>71.37</td>
</tr>
<tr>
<td>CodeLLAMA + ReFT</td>
<td>7B</td>
<td>$\mathbf{5 3 . 3 0}$</td>
<td>$\mathbf{7 5 . 2 8}$</td>
<td>$\mathbf{6 4 . 5 0}$</td>
<td>$\mathbf{7 9 . 1 9}$</td>
<td>$\mathbf{6 0 . 1 3}$</td>
<td>$\mathbf{7 1 . 8 3}$</td>
<td>$\mathbf{5 9 . 3 1}$</td>
<td>$\mathbf{7 5 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Value accuracy of ReFT and the baselines fine-tuned with two foundation models on all datasets.
language model that is initialized from the best SFT checkpoint <em>Cobbe et al. (2021a); Ouyang et al. (2022)</em>. Similar to the outcome-based reward model (ORM) <em>Uesato et al. (2022)</em>, the reward model is trained to predict a binary label that indicates the "correct" or "incorrect" solution. Once the input passes through the reward model, classification is conducted with a linear classifier on the hidden state of the last token. Finally, the solution with the highest "correct" score among the candidates is selected as the final answer. We train the RM model for 3 epochs using a batch size of 24, the maximum length of 700 and a linear learning rate schedule with $10 \%$ warm-up period and the max learning rate of $1 \mathrm{e}-6$.</p>
<p>Evaluation We report value accuracy for both N -CoT and P-CoT on all datasets. For majority voting and reranking (Table 4), we sample 100 CoTs for evaluation. In voting, the valid answer with majority counts is chosen as the final answer for computing accuracy. In reranking, we choose the CoT with the highest score and extract the answer.</p>
<h3>4.4 Results</h3>
<p>ReFT Outperforms SFT Table 2 compares the performance among the baselines and proposed ReFT on GSM8K, SVAMP, and MathQA datasets. We can observe that ReFT consistently achieves much better performance over the SFT except on MathQA $_{\text {MCQ }}$ N-CoT. Specifically, we have closed to 10-point and 12-point improvement over SFT with CodeLLAMA on GSM8K N-CoT and P-CoT, respectively. On average, we achieve 6.7-point and 7.4-point improvements with CodeLLAMA on all datasets in N -CoT and P -CoT, respectively. Notably, no additional annotations or reward models are used in ReFT. Such strong results demonstrate robust generalization of ReFT (see Analysis §5) and huge potential for further exploring the training</p>
<p>Question: The diagonals of a rhombus are 18 cm and 22 cm . Find its area?
A) 277, B) 266, (C) 199, D) 288, E) 212</p>
<p>Generated CoT: To find the area of a rhombus, we need to multiply the lengths of its diagonals and divide the result by 2.
Area of rhombus $=$ (Product of diagonals) / 2
Area of rhombus $=(18 \mathrm{~cm} \times 22 \mathrm{~cm}) / 2$
Area of rhombus $=344 \mathrm{~cm}^{2} / 2$
Area of rhombus $=172 \mathrm{~cm}^{2}$
Therefore, the answer is: C
Figure 3: Example prediction of MathQA $_{\text {MCQ }}$ reveals reward hacking.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">N-CoT</th>
<th style="text-align: center;">Galactica</th>
<th style="text-align: center;">CodeLLAMA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: center;">40.08</td>
<td style="text-align: center;">37.32</td>
</tr>
<tr>
<td style="text-align: left;">Offline Seft-Training</td>
<td style="text-align: center;">44.23</td>
<td style="text-align: center;">41.24</td>
</tr>
<tr>
<td style="text-align: left;">Online Seft-Training</td>
<td style="text-align: center;">43.78</td>
<td style="text-align: center;">38.06</td>
</tr>
<tr>
<td style="text-align: left;">ReFT</td>
<td style="text-align: center;">$\mathbf{4 5 . 2 3}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 2 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Value accuracy of ReFT and the baselines with two foundation models on MathQA $_{\text {numeric }}$ benchmark
data with reinforcement learning <em>Lu et al. (2023)</em>.
Offline self-training includes the sampling data from the initial policy for fine-tuning. We can see this simple baseline can improve the performance compared with SFT <em>He et al. (2020); Gulcehre et al. (2023)</em> but the improvements are far behind the one made by ReFT. Such comparisons indicate that "exploring" is essential in ReFT to have good performance. Though online self-training achieves some more improvements with Galactica, it is still far behind ReFT on average. This result indicates that incorrect instances are also very essential to guide the model for better exploration. Comparisons with self-training also suggest the proposed approach with on-policy sampling and reinforcement learning is better than standard data augmentation approaches.</p>
<p>Reward Hacking for MathQA Our investigation of the negative results on MathQA $_{\text {MCQ }}$ in-</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Size</th>
<th>GSM8K</th>
<th></th>
<th>Extra SFT</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>N-CoT</td>
<td>P-CoT</td>
<td>Data</td>
</tr>
<tr>
<td>Galactica + SFT + Voting</td>
<td>6.7B</td>
<td>52.8</td>
<td>62.9</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>Galactica + ReFT + Voting</td>
<td>6.7B</td>
<td>58.5</td>
<td>71.8</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>Galactica + SFT + Reranking</td>
<td>6.7B</td>
<td>57.5</td>
<td>73.4</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>Galactica + ReFT + Reranking</td>
<td>6.7B</td>
<td>59.2</td>
<td>76.4</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>CodeLLAMA + SFT + Voting</td>
<td>7B</td>
<td>53.5</td>
<td>68.0</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>CodeLLAMA + ReFT + Voting</td>
<td>7B</td>
<td>63.2</td>
<td>78.0</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>CodeLLAMA + SFT + Reranking</td>
<td>7B</td>
<td>62.9</td>
<td>77.0</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>CodeLLAMA + ReFT + Reranking</td>
<td>7B</td>
<td>66.0</td>
<td>81.2</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>Other Foundation Models 1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath (Luo et al., 2023)</td>
<td>7B</td>
<td>54.9</td>
<td>-</td>
<td>$\sqrt{1}$ (90k)</td>
</tr>
<tr>
<td>WizardMath (Luo et al., 2023)</td>
<td>13B</td>
<td>63.9</td>
<td>-</td>
<td>$\sqrt{1}$ (90k)</td>
</tr>
<tr>
<td>MathCoder (Wang et al., 2023a)</td>
<td>7B</td>
<td>67.8</td>
<td>-</td>
<td>$\sqrt{1}$ (90k)</td>
</tr>
<tr>
<td>MAmmoTH-Coder (Yue et al., 2023)</td>
<td>7B</td>
<td>22.2</td>
<td>58.8</td>
<td>$\sqrt{1}$ (260k)</td>
</tr>
<tr>
<td>MAmmoTH-Coder (Yue et al., 2023)</td>
<td>70B</td>
<td>72.4</td>
<td>76.7</td>
<td>$\sqrt{1}$ (260k)</td>
</tr>
<tr>
<td>DeepStickMath (Shan et al., 2024)</td>
<td>7B</td>
<td>86.2</td>
<td>86.7</td>
<td>$\sqrt{1}$ (720k)</td>
</tr>
<tr>
<td>GPT-3.5-turbo (Jie et al., 2023)</td>
<td>N.A.</td>
<td>75.3</td>
<td>78.0</td>
<td>N.A.</td>
</tr>
<tr>
<td>GPT-4 (OpenAI, 2023; Zhou et al., 2023a)</td>
<td>N.A.</td>
<td>93.0</td>
<td>97.0</td>
<td>N.A.</td>
</tr>
</tbody>
</table>
<p>Table 4: Solving accuracy of majority voting and reward model reranking for SFT and ReFT on GSM8K. We also include existing approaches for comparison.
dicates that ReFT suffers from the reward hacking (Skalse et al., 2022) on the multi-choice question during training. Figure 3 shows how the sampled solutions produce "inaccurate rewards", which makes the RL training suffer. As we can see, the sampled CoT obtains an incorrect answer "172" which is not half of the product of "18" and "22". However, the final reasoning step still predicts the option " $C$ " as the final answer as the model would always predict one of the options from ${\mathrm{A}, \mathrm{B}, \mathrm{C}, \mathrm{D}, \mathrm{E}}$ regardless of the correctness of intermediate $\mathrm{CoT}^{8}$. Thus, such a misleading CoT will receive a positive reward " 1 " and misguide the model to treat this as a correct CoT. The underlying reward hacking phenomenon severely tampers the model training (Everitt et al., 2021). This is also the reason that we chose the checkpoint with longer warm-up steps for MathQA N-CoT to reduce the reward hacking effect.</p>
<p>To further demonstrate the negative effect of MCQ questions, we experiment on the MathQA variant by Jie and Lu (2023), MathQA ${ }<em _MCQ="{MCQ" _text="\text">{\text {numeric }}$ (Table 1), which removes the options in the question, and directly predicts the numeric answer. Table 3 presents the comparison against the baselines. We can observe that ReFT consistently outperforms the baselines using both Galactica and CodeLLAMA. Ideally, we could reduce the reward hacking effect on MathQA ${ }</em>$ if we can obtain a more finegrained reward (e.g., process-based reward (Lightman et al., 2023)) for the intermediate reasoning steps. However, the development of a reliable}</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>process-based reward model is expensive, and requires extensive manual annotations of reasoning steps. Recognizing these challenges, we consider controlling reward hacking and its analysis as an important problem to be addressed in future work.</p>
<h2>Majority Voting and Reranking Benefit ReFT</h2>
<p>Following Wang et al. (2023b); Uesato et al. (2022); Lightman et al. (2023), we also perform majority voting and reward model reranking to show that ReFT can benefit from these common techniques. Specifically, we perform sampling from both SFT and ReFT policies. We sample 100 CoT solutions for each question and employ the reward model described in $\S 4.3$ to perform reranking. Results in Table 4 demonstrate that ReFT consistently achieves the best performance on GSM8K by reward model reranking. ReFT + Voting significantly outperforms SFT + Voting by 8.6 points on average across all settings. ReFT with reranking outperforms SFT with reranking by more than 3 points.</p>
<p>Compared with existing open-source approaches (Luo et al., 2023; Wang et al., 2023a; Yue et al., 2023) (Table 4 bottom ${ }^{9}$ ), our best P-CoT variant achieves the best performance with accuracy 81.2 on GSM8K. In addition, these approaches mainly include extra data generated from ChatGPT and perform distillation during fine-tuning. In contrast, we improve the policy itself by exploiting the potential of existing training data and pushing the limit of the policy performance. Our best result reported in Table 4, i.e., the CodeLLAMA + ReFT + Reranking with P-CoT setting, even surpasses GPT-3.5-turbo. However, we obtain the result with a model that is only in the size of 7B.</p>
<p>Experiments with Small Model Intuitively, exploration could lead to imperfect demonstration</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Training reward of ReFT, evaluation accuracy, KL against training epoch on GSM8K P-CoT.</p>
<table>
<thead>
<tr>
<th>Model Setting</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>CodeLLAMA + ReFT</td>
<td>75.28</td>
</tr>
<tr>
<td>– remove partial reward</td>
<td>74.40</td>
</tr>
<tr>
<td>– KL coefficient β = 0</td>
<td>collapse</td>
</tr>
<tr>
<td>– non-shared value model</td>
<td>75.15</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study on GSM8K P-CoT.</p>
<p>with a small language model. We conduct an experiment on P-CoT data using Galactica-125M[10], Codeparrot-small[11] and Codegen-350M[12]. Table 5 shows the performance comparison between SFT and ReFT. Surprisingly, ReFT still outperforms SFT on three datasets. Such improvements demonstrate the robustness of ReFT during the exploration of reasonable programs.</p>
<p><strong>Ablation Study</strong> We perform the ablation study using CodeLLAMA on GSM8K P-CoT (Table 6). Without the partial reward, ReFT obtains a lower accuracy 74.4 but it is still much better than SFT. As mentioned in §3.1, such a partial reward can help reduce the effect of sparse reward (Trott et al., 2019) during training. In addition, the policy distribution will easily collapse to produce unexpected results (i.e., 0 accuracy) if we set the KL coefficient β to 0. It is certainly critical to impose constraints on the space that the policy explores (Ouyang et al., 2022). The initial warm-up step essentially makes such constraints and allows the policy to further explore within the range that is governed by β. We also experiment with a separate value model (Andrychowicz et al., 2021; Cobbe et al., 2021b), where the torso parameters are initialized the same as the policy model. We found that such a setting allows the policy to converge faster in early RL training, but eventually reaches an on par performance. Compared to the original setting of a shared value model, it is, however, twice the com-</p>
<p>putation overhead due to one extra forward-pass, as well as twice the memory cost due to the storage of the separate value net. Finally, in Appendix C we give a case study to show how the generated P-CoT evolve for SFT and ReFT.</p>
<h2>5 Analysis</h2>
<p><strong>Generalization</strong> Figure 4 shows the mean reward, evaluation accuracy, and the KL divergence during training of ReFT[13] on GSM8K P-CoT using CodeLLAMA as foundation model. SFT converges and becomes overfiting when approaching 40th epoch. However, we can see the mean reward is around 80% to 90% for the ReFT policy at 40th epoch, and the value accuracy is also increasing. In addition, we can see that the KL divergence (Figure 4 (c)) is very large in the beginning and then maintains a reasonable value between 0 and 10. The stable KL divergence indicates our policy performs exploration within a space that contains appropriate programs. The underlying reinforcement learning mechanism greatly improves the generalization ability of ReFT (Brown et al., 2020).</p>
<p><strong>Qualitative Evaluation</strong> We perform a human evaluation to qualitatively assess the output from the SFT model, Warmup checkpoint, and ReFT model. The evaluation uses 50 questions and samples the solutions in GSM8K test set that can be solved correctly by all three models. We ask four different annotators to score the reasoning path according to the following criteria, each scored on a scale from 0 to 1.</p>
<ul>
<li>Logic: evaluates if the logic leading to the answer is correct.</li>
<li>Naming: evaluates if the variable conveys appropriate and reasonable semantics</li>
<li>Compactness: evaluates if the reasoning paths contain redundant information.</li>
</ul>
<p>[13] for illustration purpose, we only show the mean reward and KL for 60 epochs.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Logic</th>
<th>Naming</th>
<th>Compactness</th>
<th>Overall Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT</td>
<td>0.986</td>
<td>0.988</td>
<td>0.994</td>
<td>2.967</td>
</tr>
<tr>
<td>Warmup</td>
<td>0.949</td>
<td>0.982</td>
<td>0.990</td>
<td>2.920</td>
</tr>
<tr>
<td>ReFT</td>
<td>0.992</td>
<td>0.990</td>
<td>0.996</td>
<td>2.982</td>
</tr>
</tbody>
</table>
<p>Table 7: Qualitative scores of models from three methods trained on GSM8k P-CoT dataset.</p>
<p>A perfect score of 3 indicates good performance across these three dimensions. To ensure the evaluation is impartial and faithful, we strictly follow the setting: (1) The origin of each reasoning path (from SFT, Warmup, or ReFT) is anonymized to prevent annotator bias. (2) Four different annotators are responsible for different portions of the samples.</p>
<p>As seen in table 7, though the overall scores are quite close, ReFT performs slightly better than SFT, and outperforms the Warmup variant. Note that SFT is inherently trained to learn from the ground truth, thus, it is likely to have a high score. This comparative analysis underscores the robustness of ReFT in generating accurate and semantically coherent reasoning paths.</p>
<p>When ReFT surpasses SFT? To further investigate the relationship between ReFT and SFT, we perform ReFT training with different number of warm-up steps from SFT. Figure 5 shows the value accuracy of different ReFT variants against SFT. Specifically, if the warmup step is 3, that means the policy initialize from the $3^{r d}$-epoch SFT checkpoint. We can see that the performance of all ReFT policies decreases right after the warm-up in the beginning, until the training epoch reaches around 8. Because the linear layer in the shared value model is randomly initialized, and it could take a few epochs to adjust the distribution. Starting from the $30^{\text {th }}$ epoch, SFT converges and all ReFT variants are still improving. We can also see that all variants outperform SFT by a significant margin and there is no obvious advantage of any specific ReFT variant.</p>
<h2>6 Conclusion</h2>
<p>We have introduced reinforced fine-tuning (ReFT) as a new method for fine-tuning models to solve math problems. In contrast to SFT, ReFT optimizes a non-differentiable objective by exploring multiple CoT annotations in the search for the correct answer, rather than relying on a single annotation.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Accuracy comparison between SFT and ReFT with different number of warm-up epoch.</p>
<p>Through extensive experimentation on three datasets using two foundation models, we have demonstrated that ReFT outperforms SFT in terms of performance and generalization ability. Moreover, we have showcased the compatibility of models trained with ReFT with techniques such as majority voting (Wang et al., 2023b) and reward model reranking (Cobbe et al., 2021a; Uesato et al., 2022).</p>
<p>Furthermore, ReFT has exhibited superior performance compared to several publicly available open-source models of comparable sizes in math problem-solving. This demonstrates the effectiveness and practical value of the ReFT approach.</p>
<h2>7 Future Work</h2>
<p>We have made the first attempt of applying reinforcement learning, specifically the PPO algorithm (Schulman et al., 2017), to fine-tune of LLMs for math problem-solving. Our future work includes utilization of offline reinforcement learning techniques (Levine et al., 2020; Gulcehre et al., 2023), development of a warm-up free method to enhance training efficiency and performance, thereby reducing the gap with the reranking method. Additionally, Lightman et al. (2023) suggests that a well-trained process-based reward model (PRM) can significantly enhance performance. Hence, it would be worthwhile to explore the implementation of process-based rewards in reinforcement learning training. Lastly, as ReFT is a versatile approach, we intend to apply it to more general reasoning tasks where the inference can be formalized with CoT.</p>
<h2>Limitations</h2>
<p>Training Efficiency As depicted in Figure 4 (b), it is evident that ReFT necessitates a greater number of epochs to reach convergence compared to SFT. This is primarily due to the fact that ReFT optimizes a non-differentiable objective and requires exploration of the generation space to attain correct answers. While a larger learning rate may expedite convergence, it also makes the policy more susceptible to instability and potential collapse. Alternatively, using a larger batch size is a viable option; however, it comes at the expense of increased computational costs.</p>
<p>Reward Hacking Our reward function relies solely on the final answer to determine the reward. However, as demonstrated in the experiments conducted on the MathQA ${ }_{\mathrm{MCQ}}$ N-CoT dataset, the policy can be easily manipulated if the possible space of final answers is limited, such as $A, B, C, D$. To mitigate the issue of reward hacking, it may be necessary to employ a more detailed or process-based reward function that takes into account a broader range of factors.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of NAACL.</p>
<p>Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Emilie Devijver, and Yury Maximov. 2022. Self-training: A survey. arXiv preprint arXiv:2202.12040.</p>
<p>Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. 2021. What matters in onpolicy reinforcement learning? a large-scale empirical study. In Proceedings of ICLR.</p>
<p>Thomas Anthony, Zheng Tian, and David Barber. 2017. Thinking fast and slow with deep learning and tree search. In Proceedings of NeurIPS.</p>
<p>Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. 2023. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036.</p>
<p>Daniel S Brown, Wonjoon Goo, and Scott Niekum. 2020. Better-than-demonstrator imitation learning
via automatically-ranked demonstrations. In Proceedings of Conference on Robot Learning, pages 330-359.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021a. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. 2021b. Phasic policy gradient. In Proceedings of ICML.</p>
<p>Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. 2023. Human-centered loss functions (halos). Technical report, Contextual AI.</p>
<p>Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. 2021. Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective. Synthese, 198(Suppl 27):6435-6467.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In Proceedings of ICLR.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided language models. In Proceedings of ICML.</p>
<p>Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. 2018. Reinforcement learning from imperfect demonstrations. arXiv preprint arXiv:1802.05313.</p>
<p>GemmaTeam. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.</p>
<p>Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. 2022. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/ accelerate.</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced selftraining (rest) for language modeling. arXiv preprint arXiv:2308.08998.</p>
<p>Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In Proceedings of ICLR.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</p>
<p>Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2021. Online learning: A comprehensive survey. Neurocomputing, 459:249-289.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398.</p>
<p>Zhanming Jie and Wei Lu. 2023. Leveraging training data in few-shot prompting for numerical reasoning.</p>
<p>Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. 2023. Design of chain-ofthought in math problem solving. arXiv preprint arXiv:2309.11054.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In Proceedings of ICLR.</p>
<p>Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):79-86.</p>
<p>Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In Proceedings of NeurIPS.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. 2023. Tinygsm: achieving&gt; 80\% on gsm8k with small language models. arXiv preprint arXiv:2312.09241.</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen, et al. 2023. Reinforcement learning, bit by bit. Foundations and Trends® in Machine Learning, 16(6):733865 .</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.</p>
<p>Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2023. GPT-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? In Proceedings of NAACL.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Proceedings of NeurIPS.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of SIGKDD.</p>
<p>Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. 2018. Learning by playing solving sparse reward tasks from scratch. In Proceedings of ICML.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.</p>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. High-dimensional continuous control using generalized advantage estimation.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300.</p>
<p>Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2023. Beyond human data: Scaling self-training for problemsolving with language models.</p>
<p>Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and characterizing reward gaming. In Proceedings of NeurIPS.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. 2019. Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. In Proceedings of NeurIPS.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275.</p>
<p>Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020. Trl: Transformer reinforcement learning. https://github. com/huggingface/tr1.</p>
<p>Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023a. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. arXiv preprint arXiv:2310.03731.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In Proceedings of ICLR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of NeurIPS.</p>
<p>Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020. Self-training with noisy student improves imagenet classification. In Proceedings of CVPR, pages 10687-10698.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D Goodman. 2022. Star: Self-taught reasoner bootstrapping reasoning with reasoning. In Proceedings of NeurIPS.</p>
<p>Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, and Andrew Lan. 2023. Interpretable math word problem solution generation via step-by-step planning. In Proceedings of ACL.</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023. Secrets of rlhf in large language models part i: Ppo.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023a. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023b. Least-to-most prompting enables complex reasoning in large language models. In Proceedings of ICLR.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
<p>Question: Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</p>
<p>N-CoT: We need to calculate her hourly rate and then multiply it by the amount of time she worked.</p>
<p>First, we need to convert 50 minutes to hours.
There are 60 minutes in an hour, so 50 minutes is equal to $50 / 60=$ $5 / 6$ hours.</p>
<p>Next, we can calculate Weng's earnings by multiplying her hourly rate by the amount of time she worked:
$\$ 12 /$ hour $\times 5 / 6$ hour $=\$ 10$.
Therefore, Weng earned $\$ 10$ for 50 minutes of babysitting.
The answer is 10 .
P-CoT:
def solution():
<strong><em>Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</em></strong> hourly_rate $=12$ minutes_worked $=50$ hours_worked $=$ minutes_worked $/ 60$ earnings $=$ hourly_rate * hours_worked result $=$ earnings return result</p>
<p>Figure 6: N-CoT and P-CoT examples on GSM8K</p>
<h2>A Examples of N-CoT and P-CoT Representations</h2>
<p>We present examples of natural language CoT and program-based CoT from GSM8K dataset in Figure 6. We follow Jie et al. (2023) to perform fewshot prompting and obtain the CoT representations. The natural language CoT is generally the same as the one presented in Wei et al. (2022). The format program-based CoT is similar to the one in PAL (Gao et al., 2023), where we use a function to solve the problems. The function starts with a Python docstring that repeats the question and then a list of statements as reasoning steps.</p>
<h2>B Detailed Hyperparameter Setting</h2>
<p>Supervised Fine-Tuning We train the model for 40 epochs with the batch size of 48 and the maximum length of 1024.. For small models, we increase the learning rate to $2 e-5$, and the number of epoch for training MathQA ${ }_{\mathrm{MCQ}}$ to 100 epochs.</p>
<p>ReFT Warm-up We perform warm-up for 2 epochs on GSM8K, SVAMP for both N-CoT and P-CoT. For MathQA ${ }<em _MCQ="{MCQ" _text="\text">{\text {MCQ }}$, we perform warmup for 5 epochs on MathQA ${ }</em>}}$ N-CoT and 2 epochs on MathQA ${ <em _numeric="{numeric" _text="\text">{\text {MCQ }}$ P-CoT. Specifically for MathQA ${ }</em>$, we perform warm-up for 10 epochs because this dataset is much harder and the number of reasoning chains is longer than other datasets. For Galactica-125m and Codegen-350M, the warm-
up period is 10 epochs for GSM8K and SVAMP and is 40 epochs for MathQA ${ }_{\text {MCQ }}$. For Code-parrot, we increases the warm-up period to 40 epochs on all datasets to obtain reasonable warm-up performance.}</p>
<p>ReFT RL The maximum length for question is set to 300 , and the maximum length during sampling is set to 700 . The batch size is 32 , which is smaller than SFT due to extra memory consumption of the value model. The number of updates per RL step (i.e., ppo epoch) is set to 2 (Ziegler et al., 2019). We do not employ any weight decay and dropout following Ziegler et al. (2019). For small models, we train for 700 epochs with the learning rate of $3 e-6$, the global batch size of 256 and the $\alpha$ of 5,1 and 0.1 for Galactica-125m, Codeparrotsmall and Codegen-350M model respectively.</p>
<h2>C Case Study</h2>
<p>We show how SFT and ReFT evolve by investigating the generated P-CoT for a specific question. Figure 7 reports the responses of SFT and ReFT at checkpoint epoch 1, 3 and 5. At epoch 1, ReFT is in warmup stage so that its generated P-CoT looks similar to that of SFT. However, later at epoch 3 and 5, the P-CoTs differ. ReFT responses becomes shorter and correct while SFT reponses remains incorrect.</p>
<h2>D Attempts with DPO and IPO</h2>
<p>In addition to the PPO algorithm in this work, we initially experimented with DPO (Rafailov et al., 2023) and IPO (Azar et al., 2023) in our early attempts on the GSM8K dataset. We obtained the preference data by sampling from the warmup checkpoint and adapted the implementation of these algorithms from the Transformer Reinforcement Learning (TRL) framework (von Werra et al., 2020). Preliminary results indicate that their performance is on par with the "Offline Self-Learning" baseline on GSM8K. This could be explained by the following reasons. Being offline methods, DPO and IPO cannot explore more CoT paths by themselves, thus, their performances are limited by the quality of data sampled from an sub-optimal policy. In addition, their policies are set to maximize the reward given by their implicitly trained reward models. Our preference data, which is sampled from the train set, might not be ideal for reward modeling as there may be neither incorrect responses for</p>
<p>easy questions nor correct responses for hard questions. On the other hand, PPO not only is an online method that explores CoT solution paths automatically, but it also does not require reward model training as the reward function can be directly used. Thus, PPO is chosen in our later experiments.</p>
<h1>Example from GSM8K</h1>
<h2>Question:</h2>
<p>If Sam and Harry have 100 feet of fence between them, and they agree to split it with Harry getting 60 feet more than Sam, how much is left over for Sam?</p>
<h2>SFT</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h2>ReFT</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: P-CoT responses of SFT and ReFT checkpoints at epoch 1,3 and 5 to the same question in GSM8K dataset. Reponses in green frame are correct while responses in red frame are incorrect.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ We only show 60 epochs for illustration purposes. The performance for the later epoch is shown in Figure 4 (b).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Numbers are taken from original papers. The N-CoT and P-CoT results for MAmmoTH-Coder are reported in their appendix.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5196 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5196</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5196</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-257834038</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.17491v3.pdf" target="_blank">Language Models can Solve Computer Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5196.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5196.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Criticism and Improvement (RCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting architecture that asks an LLM to (1) generate an output, (2) critique/find problems in that output, and (3) produce an improved output conditioned on the critique; applied both explicitly (sampling/producing a critique text) and implicitly (updating outputs without an explicit sampled critique).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4); also evaluated with GPT-3 (davinci) and text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned OpenAI GPT-3 family models with RL from human feedback (gpt-3.5-turbo, gpt-4) accessed via OpenAI API; other base models (GPT-3 davinci, text-davinci-002) used for ablations. Exact parameter counts for InstructGPT/RLHF models are undisclosed in paper; GPT-3 davinci ~175B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Recursive Criticism and Improvement (explicit & implicit RCI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-step generate-then-reflect loop: (a) produce initial output (plan/action/answer), (b) prompt LLM to 'find problems with this output' (explicit RCI) or internally condition a revised output on the previous output (implicit RCI), then (c) generate an improved output. RCI is applied at three grounding stages for computer tasks: task grounding (plan-level explicit RCI), state grounding (implicit RCI to map abstract steps to concrete elements), and agent grounding (implicit RCI to ensure action format/executability). Explicit critique sampling is optional (explicit vs implicit RCI).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MiniWoB++ (web-based computer tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of 55 simulated web UI tasks (mouse & keyboard actions, HTML state) requiring varying reasoning complexity from simple clicks to multi-step compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported as state-of-the-art among LLM-based methods on MiniWoB++; paper reports RCI agent achieves on-average: +41 percentage points over WebN-T5-3B (finetuned T5 with 12K demonstrations), +37 percentage points over prior supervised-learning SotA (aggregated), +27 percentage points over prior RL SotA, and ~6 percentage points better than CC-Net when CC-Net is evaluated without dictionary-based typing. (Paper presents task-level results in Figure 10 / Table 18; exact per-task numbers are reported there.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines (LLM approaches without RCI or finetuned SL/RL methods) achieve substantially lower success rates: WebN-T5-3B and CC-Net (with/without dictionary typing) are cited as much lower (see comparison deltas above). The paper's ablations removing RCI at grounding stages show substantial performance degradation (each grounding/RCl contribution nearly equally important).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: large average success-rate gains on MiniWoB++ compared to supervised and RL baselines (see reported +41pp vs WebN-T5-3B, +6pp vs CC-Net(no-dict), and improvements across aggregated SotA categories). Ablations show removing explicit plan RCI, state-grounding implicit RCI, or agent-grounding implicit RCI each reduces success rates significantly, indicating RCI contributes causally; RCI reduces required expert demonstrations dramatically (authors report ~120x fewer samples than WebN-T5-3B and ~11,000x fewer than CC-Net). Qualitative: example traces show RCI corrects logical/grounding errors (plan improvement, mapping to XPath, formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RCI is computationally more expensive than single-sample prompting; performance depends on base LLM quality (GPT-3 variants underperform compared to instruction-tuned models); long HTML contexts and limited context length degrade performance; RCI/grounding underperforms on long-horizon planning tasks (guess-number, search-engine, use-spinner), tasks requiring multi-step strategic reasoning (tic-tac-toe, use-autocomplete), and tasks requiring visual rendering (count-shape). Implicit/explicit RCI loop counts must be tuned (agent-grounding loop set to max 3 empirically).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5196.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCI-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RCI applied to natural-language reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of RCI prompting to arithmetic and commonsense reasoning datasets, asking the LLM to critique and then revise intermediate reasoning or final answers, optionally combined with chain-of-thought (CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT-3 + RLHF (gpt-3.5-turbo); some experiments across model variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-3.5-turbo (instruction-tuned, RLHF) used as the primary LLM for reasoning experiments; models accessed via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Recursive Criticism and Improvement (generate-then-criticize then improve); combined variants with Chain-of-Thought (CoT + RCI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Start from zero-shot or CoT prompt to produce an answer; then prompt LLM to 'Review your previous answer and find problems' (explicit critique) and 'Based on the problems you found, improve your answer' to generate a corrected answer. The loop is allowed up to a maximum number of iterations and can be terminated early using external feedback (correct label). RCI can be used zero-shot, few-shot, or combined with CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Eight reasoning benchmarks (arithmetic & commonsense): GSM8K, MultiArith, AddSub, AQuA, SVAMP, SingleEq, CommonSenseQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic word problems (grade-school and multi-step) and commonsense QA datasets used in prior CoT work (Kojima et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitatively reported as 'substantially outperforms' standard zero-shot prompting across all eight benchmarks (Zero-Shot + RCI); RCI combined with CoT yields synergistic improvements and in many arithmetic tasks Zero-Shot-CoT + RCI and Few-Shot-CoT + RCI attain the highest scores (see Table 1 & Table 2 in paper). Exact numeric accuracies are reported in the paper's tables (not reproduced verbatim in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard zero-shot prompting (no RCI) yields lower accuracy across the same benchmarks; Chain-of-Thought (CoT) baselines are improved further when combined with RCI compared to CoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports consistent accuracy gains across the eight benchmarks when adding RCI to zero-shot prompting; RCI yields improvements even on simple arithmetic tasks (SingleEq, AddSub) where CoT offers little help. The combination RCI+CoT outperforms either alone, indicating complementarity. Authors note the results are summarized in Table 1/2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RCI loop without external feedback is prone to 'false negative critics' (the model incorrectly judges a correct answer as flawed), which can reduce performance; the authors therefore use external label feedback to stop RCI loops in experiments (they set a max of 2 loops and use ground-truth to terminate). When run without external feedback, RCI achieves zero-shot-level performance on only about half the benchmarks and underperforms on others (see Appendix 17).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5196.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit RCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Recursive Criticism and Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RCI variant where the model is prompted to explicitly produce a critique text describing problems in a previous plan/answer, and then an improved output is sampled conditioned on that critique and the original output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4) (used for planning-level improvements)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT models (see above); explicit RCI was primarily used for improving high-level plans (task grounding) in computer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>explicit RCI (sampled critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LLM is asked to 'Find problems with this plan' and produce an explicit critique paragraph; an improved plan is then sampled conditioned on both the critique and the earlier plan. Typically one explicit RCI pass sufficed for many MiniWoB++ tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task grounding in MiniWoB++ (plan generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generating step-by-step plans for web UI tasks before sampling concrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Authors state that a single explicit RCI pass 'suffices for most MiniWoB++ tasks' and that explicit RCI on generated plans improves plan success rate; concrete per-task improvements are reported in task-level analyses (figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without explicit plan critique, task grounding ablation reduces success rates substantially (ablation results in Figure 5); each grounding stage's removal produces notable degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Ablation shows that including explicit RCI at the plan (task-grounding) stage improves success on long-horizon tasks and contributes significantly to overall success; illustrative plan critique examples in Figures demonstrate correction of underspecified plans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Explicit RCI loops beyond one pass provided diminishing returns for most MiniWoB++ tasks; relying on explicit critiques can increase runtime and token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5196.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implicit RCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Recursive Criticism and Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RCI variant where the model updates previous outputs directly (without emitting an explicit critique) to improve state- and agent-grounding (mapping abstract actions to concrete HTML elements and admissible action formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT models; implicit RCI used primarily to refine task-grounded actions to be feasible in the current state and to ensure action admissibility for the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>implicit RCI (direct update)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LLM is prompted to consider the current state, task, and the task-grounded action and to directly output a revised, state-grounded or agent-grounded action (no explicit critique text is produced). Agent-grounding implicit RCI loop is repeated up to a maximum loop count (empirically 3) until an executable action is produced.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>State-grounding and agent-grounding steps in MiniWoB++</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refining planned actions into concrete, executable instructions (e.g., selecting correct XPath, formatting type instructions) given the HTML state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Implicit RCI as part of the full RCI pipeline contributes to the high overall success rate; authors report that state- and agent-grounding improvements are essential and that empirical optimal agent-grounding loop count is 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation removing implicit RCI for state or agent grounding causes substantial drops in success; in simple tasks, removing agent grounding decreased success by ~60% relative to baseline without agent grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Ablation experiments demonstrate substantial degradation when implicit RCI is removed from state or agent grounding steps (Figure 5). Examples in the appendix show implicit RCI fixes formatting and grounding mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>State-grounding can occasionally produce task-irrelevant actions (example click-dialog-2) and authors disable state grounding for that task; repeating implicit RCI more than once for state grounding did not improve success rate and can add cost; loop count tuning needed to avoid infinite loops or wasted tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5196.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning+acting interleaving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related published approach that interleaves chain-of-thought style reasoning traces with action generation to help LLMs plan and correct behavior in interactive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReAct (interleaved reasoning and acting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Interleaves reasoning steps and actions, letting the model produce thoughts and actions sequentially to reduce hallucination and support plan updates.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as a method that interleaves reasoning and acting; RCI is presented as an extended architecture in the same landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; mentioned as complementary prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5196.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (LLM with trial-and-error memory and self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that improves agent performance by having LLMs reflect on past trial-and-error experiences and update future behavior, but requires explicit task-specific success feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (trial-and-error self-reflection with memory)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM stores trial-and-error trajectories and uses reflective updates based on explicit success signals from the environment to improve future attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned as a related approach that uses previous trial-and-error experiences; authors note Reflexion needs multiple rounds of explicit task-specific success feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires task-specific success feedback and so may not scale as easily for many new tasks without such feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5196.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5196.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed method that iteratively refines generated outputs using localized and aspect-based self-feedback from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative localized reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Uses localized, aspect-focused feedback to iteratively refine outputs; similar in spirit to RCI but uses more structured, localized feedback mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as closely related recent work in iterative self-refinement; paper claims RCI is the first to demonstrate self-critiquing capability solely with implicit feedback for improving reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated within this paper; included as related work for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can Solve Computer Tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5196",
    "paper_id": "paper-257834038",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "RCI",
            "name_full": "Recursive Criticism and Improvement (RCI)",
            "brief_description": "A prompting architecture that asks an LLM to (1) generate an output, (2) critique/find problems in that output, and (3) produce an improved output conditioned on the critique; applied both explicitly (sampling/producing a critique text) and implicitly (updating outputs without an explicit sampled critique).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4); also evaluated with GPT-3 (davinci) and text-davinci-002",
            "model_description": "Instruction-finetuned OpenAI GPT-3 family models with RL from human feedback (gpt-3.5-turbo, gpt-4) accessed via OpenAI API; other base models (GPT-3 davinci, text-davinci-002) used for ablations. Exact parameter counts for InstructGPT/RLHF models are undisclosed in paper; GPT-3 davinci ~175B parameters.",
            "reflection_method_name": "Recursive Criticism and Improvement (explicit & implicit RCI)",
            "reflection_method_description": "Two-step generate-then-reflect loop: (a) produce initial output (plan/action/answer), (b) prompt LLM to 'find problems with this output' (explicit RCI) or internally condition a revised output on the previous output (implicit RCI), then (c) generate an improved output. RCI is applied at three grounding stages for computer tasks: task grounding (plan-level explicit RCI), state grounding (implicit RCI to map abstract steps to concrete elements), and agent grounding (implicit RCI to ensure action format/executability). Explicit critique sampling is optional (explicit vs implicit RCI).",
            "num_iterations": null,
            "task_name": "MiniWoB++ (web-based computer tasks)",
            "task_description": "A suite of 55 simulated web UI tasks (mouse & keyboard actions, HTML state) requiring varying reasoning complexity from simple clicks to multi-step compositional tasks.",
            "performance_with_reflection": "Reported as state-of-the-art among LLM-based methods on MiniWoB++; paper reports RCI agent achieves on-average: +41 percentage points over WebN-T5-3B (finetuned T5 with 12K demonstrations), +37 percentage points over prior supervised-learning SotA (aggregated), +27 percentage points over prior RL SotA, and ~6 percentage points better than CC-Net when CC-Net is evaluated without dictionary-based typing. (Paper presents task-level results in Figure 10 / Table 18; exact per-task numbers are reported there.)",
            "performance_without_reflection": "Baselines (LLM approaches without RCI or finetuned SL/RL methods) achieve substantially lower success rates: WebN-T5-3B and CC-Net (with/without dictionary typing) are cited as much lower (see comparison deltas above). The paper's ablations removing RCI at grounding stages show substantial performance degradation (each grounding/RCl contribution nearly equally important).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: large average success-rate gains on MiniWoB++ compared to supervised and RL baselines (see reported +41pp vs WebN-T5-3B, +6pp vs CC-Net(no-dict), and improvements across aggregated SotA categories). Ablations show removing explicit plan RCI, state-grounding implicit RCI, or agent-grounding implicit RCI each reduces success rates significantly, indicating RCI contributes causally; RCI reduces required expert demonstrations dramatically (authors report ~120x fewer samples than WebN-T5-3B and ~11,000x fewer than CC-Net). Qualitative: example traces show RCI corrects logical/grounding errors (plan improvement, mapping to XPath, formatting).",
            "limitations_or_failure_cases": "RCI is computationally more expensive than single-sample prompting; performance depends on base LLM quality (GPT-3 variants underperform compared to instruction-tuned models); long HTML contexts and limited context length degrade performance; RCI/grounding underperforms on long-horizon planning tasks (guess-number, search-engine, use-spinner), tasks requiring multi-step strategic reasoning (tic-tac-toe, use-autocomplete), and tasks requiring visual rendering (count-shape). Implicit/explicit RCI loop counts must be tuned (agent-grounding loop set to max 3 empirically).",
            "uuid": "e5196.0",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "RCI-Reasoning",
            "name_full": "RCI applied to natural-language reasoning benchmarks",
            "brief_description": "Application of RCI prompting to arithmetic and commonsense reasoning datasets, asking the LLM to critique and then revise intermediate reasoning or final answers, optionally combined with chain-of-thought (CoT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT-3 + RLHF (gpt-3.5-turbo); some experiments across model variants",
            "model_description": "gpt-3.5-turbo (instruction-tuned, RLHF) used as the primary LLM for reasoning experiments; models accessed via OpenAI API.",
            "reflection_method_name": "Recursive Criticism and Improvement (generate-then-criticize then improve); combined variants with Chain-of-Thought (CoT + RCI)",
            "reflection_method_description": "Start from zero-shot or CoT prompt to produce an answer; then prompt LLM to 'Review your previous answer and find problems' (explicit critique) and 'Based on the problems you found, improve your answer' to generate a corrected answer. The loop is allowed up to a maximum number of iterations and can be terminated early using external feedback (correct label). RCI can be used zero-shot, few-shot, or combined with CoT prompts.",
            "num_iterations": 2,
            "task_name": "Eight reasoning benchmarks (arithmetic & commonsense): GSM8K, MultiArith, AddSub, AQuA, SVAMP, SingleEq, CommonSenseQA, StrategyQA",
            "task_description": "Arithmetic word problems (grade-school and multi-step) and commonsense QA datasets used in prior CoT work (Kojima et al.).",
            "performance_with_reflection": "Qualitatively reported as 'substantially outperforms' standard zero-shot prompting across all eight benchmarks (Zero-Shot + RCI); RCI combined with CoT yields synergistic improvements and in many arithmetic tasks Zero-Shot-CoT + RCI and Few-Shot-CoT + RCI attain the highest scores (see Table 1 & Table 2 in paper). Exact numeric accuracies are reported in the paper's tables (not reproduced verbatim in main text).",
            "performance_without_reflection": "Standard zero-shot prompting (no RCI) yields lower accuracy across the same benchmarks; Chain-of-Thought (CoT) baselines are improved further when combined with RCI compared to CoT alone.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports consistent accuracy gains across the eight benchmarks when adding RCI to zero-shot prompting; RCI yields improvements even on simple arithmetic tasks (SingleEq, AddSub) where CoT offers little help. The combination RCI+CoT outperforms either alone, indicating complementarity. Authors note the results are summarized in Table 1/2.",
            "limitations_or_failure_cases": "RCI loop without external feedback is prone to 'false negative critics' (the model incorrectly judges a correct answer as flawed), which can reduce performance; the authors therefore use external label feedback to stop RCI loops in experiments (they set a max of 2 loops and use ground-truth to terminate). When run without external feedback, RCI achieves zero-shot-level performance on only about half the benchmarks and underperforms on others (see Appendix 17).",
            "uuid": "e5196.1",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Explicit RCI",
            "name_full": "Explicit Recursive Criticism and Improvement",
            "brief_description": "RCI variant where the model is prompted to explicitly produce a critique text describing problems in a previous plan/answer, and then an improved output is sampled conditioned on that critique and the original output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4) (used for planning-level improvements)",
            "model_description": "Instruction-tuned GPT models (see above); explicit RCI was primarily used for improving high-level plans (task grounding) in computer tasks.",
            "reflection_method_name": "explicit RCI (sampled critique)",
            "reflection_method_description": "The LLM is asked to 'Find problems with this plan' and produce an explicit critique paragraph; an improved plan is then sampled conditioned on both the critique and the earlier plan. Typically one explicit RCI pass sufficed for many MiniWoB++ tasks.",
            "num_iterations": 1,
            "task_name": "Task grounding in MiniWoB++ (plan generation)",
            "task_description": "Generating step-by-step plans for web UI tasks before sampling concrete actions.",
            "performance_with_reflection": "Authors state that a single explicit RCI pass 'suffices for most MiniWoB++ tasks' and that explicit RCI on generated plans improves plan success rate; concrete per-task improvements are reported in task-level analyses (figures/tables).",
            "performance_without_reflection": "Without explicit plan critique, task grounding ablation reduces success rates substantially (ablation results in Figure 5); each grounding stage's removal produces notable degradation.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Ablation shows that including explicit RCI at the plan (task-grounding) stage improves success on long-horizon tasks and contributes significantly to overall success; illustrative plan critique examples in Figures demonstrate correction of underspecified plans.",
            "limitations_or_failure_cases": "Explicit RCI loops beyond one pass provided diminishing returns for most MiniWoB++ tasks; relying on explicit critiques can increase runtime and token usage.",
            "uuid": "e5196.2",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Implicit RCI",
            "name_full": "Implicit Recursive Criticism and Improvement",
            "brief_description": "RCI variant where the model updates previous outputs directly (without emitting an explicit critique) to improve state- and agent-grounding (mapping abstract actions to concrete HTML elements and admissible action formatting).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT-3 + RLHF (gpt-3.5-turbo, gpt-4)",
            "model_description": "Instruction-tuned GPT models; implicit RCI used primarily to refine task-grounded actions to be feasible in the current state and to ensure action admissibility for the agent.",
            "reflection_method_name": "implicit RCI (direct update)",
            "reflection_method_description": "The LLM is prompted to consider the current state, task, and the task-grounded action and to directly output a revised, state-grounded or agent-grounded action (no explicit critique text is produced). Agent-grounding implicit RCI loop is repeated up to a maximum loop count (empirically 3) until an executable action is produced.",
            "num_iterations": 3,
            "task_name": "State-grounding and agent-grounding steps in MiniWoB++",
            "task_description": "Refining planned actions into concrete, executable instructions (e.g., selecting correct XPath, formatting type instructions) given the HTML state.",
            "performance_with_reflection": "Implicit RCI as part of the full RCI pipeline contributes to the high overall success rate; authors report that state- and agent-grounding improvements are essential and that empirical optimal agent-grounding loop count is 3.",
            "performance_without_reflection": "Ablation removing implicit RCI for state or agent grounding causes substantial drops in success; in simple tasks, removing agent grounding decreased success by ~60% relative to baseline without agent grounding.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Ablation experiments demonstrate substantial degradation when implicit RCI is removed from state or agent grounding steps (Figure 5). Examples in the appendix show implicit RCI fixes formatting and grounding mismatches.",
            "limitations_or_failure_cases": "State-grounding can occasionally produce task-irrelevant actions (example click-dialog-2) and authors disable state grounding for that task; repeating implicit RCI more than once for state grounding did not improve success rate and can add cost; loop count tuning needed to avoid infinite loops or wasted tokens.",
            "uuid": "e5196.3",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (reasoning+acting interleaving)",
            "brief_description": "Related published approach that interleaves chain-of-thought style reasoning traces with action generation to help LLMs plan and correct behavior in interactive settings.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "ReAct (interleaved reasoning and acting)",
            "reflection_method_description": "Interleaves reasoning steps and actions, letting the model produce thoughts and actions sequentially to reduce hallucination and support plan updates.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in related work as a method that interleaves reasoning and acting; RCI is presented as an extended architecture in the same landscape.",
            "limitations_or_failure_cases": "Not evaluated in this paper; mentioned as complementary prior work.",
            "uuid": "e5196.4",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (LLM with trial-and-error memory and self-reflection)",
            "brief_description": "Related work that improves agent performance by having LLMs reflect on past trial-and-error experiences and update future behavior, but requires explicit task-specific success feedback.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion (trial-and-error self-reflection with memory)",
            "reflection_method_description": "LLM stores trial-and-error trajectories and uses reflective updates based on explicit success signals from the environment to improve future attempts.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned as a related approach that uses previous trial-and-error experiences; authors note Reflexion needs multiple rounds of explicit task-specific success feedback.",
            "limitations_or_failure_cases": "Requires task-specific success feedback and so may not scale as easily for many new tasks without such feedback.",
            "uuid": "e5196.5",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "A recently proposed method that iteratively refines generated outputs using localized and aspect-based self-feedback from LLMs.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-Refine (iterative localized reflection)",
            "reflection_method_description": "Uses localized, aspect-focused feedback to iteratively refine outputs; similar in spirit to RCI but uses more structured, localized feedback mechanisms.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited as closely related recent work in iterative self-refinement; paper claims RCI is the first to demonstrate self-critiquing capability solely with implicit feedback for improving reasoning.",
            "limitations_or_failure_cases": "Not evaluated within this paper; included as related work for comparison.",
            "uuid": "e5196.6",
            "source_info": {
                "paper_title": "Language Models can Solve Computer Tasks",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 1,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        }
    ],
    "cost": 0.018217499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models can Solve Computer Tasks
16 Nov 2023</p>
<p>Geunwoo Kim 
Pierre Baldi pfbaldi@ics.uci.edu 
Stephen Mcaleer smcaleer@cs.cmu.edu </p>
<p>University of California
Irvine</p>
<p>University of California
Irvine</p>
<p>Carnegie Mellon University</p>
<p>Language Models can Solve Computer Tasks
16 Nov 2023E8B93C91A53A8FDFD855950E42BA16AAarXiv:2303.17491v3[cs.CL]
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problemsolving.Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands.However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks.In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI).The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark.We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function.Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback.We find that RCI combined with CoT performs better than either separately.Our code can be found here: https://github.com/posgnu/rci-agent.</p>
<p>Introduction</p>
<p>A long-standing goal in artificial intelligence has been to create generally-intelligent agents that can accomplish cognitive tasks as well as humans.Such agents should be able to solve any computer task a human can by communicating via natural language.By automating repetitive tasks and providing assistance in complex problem-solving, generally-intelligent virtual agents may radically increase productivity.</p>
<p>Recently, large language models (LLMs) have shown remarkable in-context learning capabilities across a variety of domains and tasks [12,69,5,17,26,64,8,46,6].Although LLMs can impressively manipulate text and can use high-level API tools [59,48,41], previous approaches to using LLMs that directly take keyboard and mouse actions on computers have had difficulty compared to imitation learning and reinforcement learning approaches [24].LLMs that take keyboard and mouse actions on computers face a number of obstacles, such as ensuring that generated actions are task-appropriate (task grounding), feasible in the agent's current state (state grounding), and admissible to be executed (agent grounding).The previous best-performing approaches for taking actions on computers have not used LLMs.Instead, they have trained networks from scratch to predict actions given prompts and screenshots or DOM information, either via supervised learning (SL) from expert demonstrations, reinforcement learning (RL) on a handcrafted reward signal, or both (SL+RL) [30].Although SL+RL works well on a number of individual computer tasks, since it requires expert data and a reward function for every task, it has not been shown to generalize to novel tasks in a few-shot setting.</p>
<p>In this work, we show that a pre-trained LLM agent can successfully execute computer tasks guided by natural language.Our method employs a simple prompting scheme, which we call Recursive Criticism and Improvement (RCI), that significantly outperforms existing LLM methods for automating computer tasks.RCI works by first having the LLM generate an output based on zero-shot prompting.Then, RCI prompts the LLM to identify problems with the given output.After the LLM has identified problems with the output, RCI prompts the LLM to generate an updated output.</p>
<p>When applying RCI to computer tasks, we improve task grounding, state grounding, and agent grounding sequentially.Firstly, task grounding prompts the LLM with the task text, instructing it to generate a high-level plan.Secondly, state grounding connects high-level concepts derived from the task grounding step with actual HTML elements present in the current state, subsequently outputting the appropriate action.Finally, agent grounding ensures the correct formatting of the action output obtained from the state grounding step.RCI is applied to each of these three steps; however, we find that critiquing the state-grounding step is only necessary once.</p>
<p>We evaluate the RCI approach on the MiniWoB++ benchmark [61], and show it surpasses existing SL, RL, and LLM approaches.Furthermore, it proves itself to state-of-the-art compared to existing methods, using only a small number of demonstrations per task instead of tens of thousands, and without relying on a task-specific reward function.This significant reduction in required demonstrations and the elimination of task-specific reward functions make our method more practical and accessible for new tasks.Furthermore, as the capabilities of LLMs continue to improve, one can expect the performance of our method to improve as well.</p>
<p>In addition to its success in automating computer tasks, we also showcase the effectiveness of RCI prompting in enhancing the reasoning abilities of LLMs on a suite of natural language reasoning tasks.When external feedback is given, our method achieves a significant performance increase over zeroshot prompting and slightly improves upon chain-of-thought [73] (CoT) prompting.Interestingly, RCI and CoT have a synergistic effect, and their combination outperforms all other methods.In summary, our work presents a new powerful and practical approach to enabling LLM agents to execute computer tasks guided by natural language.The RCI prompting scheme not only outperforms previous methods in computer tasks, but also improves reasoning abilities for LLMs more broadly, making it a significant contribution in the development of intelligent agents.</p>
<p>Methods</p>
<p>RCI Prompting</p>
<p>The self-critiquing ability of LLMs has demonstrated that LLMs can find errors in their own output by themselves [58,20,3].In light of this, we introduce a simple reasoning architecture called RCI prompting, where we prompt LLMs to find problems in their output and improve the output based on what they find.This architecture is designed to further enhance the reasoning ability of LLMs by inserting a critique step before generating the final answer.Figure 2 compares example traces of RCI prompting and baseline prompting methods on GSM8K dataset where language models should answer grade school math problems.While baselines elicit answers with a single step of prompting, RCI consists of two steps: criticize the previous answer (e.g., "Review your previous answer and find problems with your answer") and improve the answer based on the critique (e.g., "Based on the problems you found, improve your answer").In this way, RCI prompting finds errors (e.g., the overall sum of money only considered Valerie and her brother) in the previous answer and generates an improved answer (e.g., money from Valerie's mother is included in the total) conditioned on the critique.The iterative process of RCI can be continued until specific conditions are satisfied, which could include receiving feedback from the environment, reaching the maximum predetermined number of iterations, or adhering to certain heuristics.We define two approaches for achieving RCI: explicit RCI and implicit RCI.Explicit RCI includes the critique in the prompt to generate improved output and implicit RCI updates the previous output directly without sampling a critique explicitly.Examples of explicit RCI and implicit RCI applied to computer tasks are illustrated in Figure 3 where the action plan is improved based on an explicit critique while actions are updated without an explicit critique.This process is described in more detail in the following section.</p>
<p>RCI for Computer Tasks</p>
<p>In this section we describe the application of RCI to computer tasks via a decomposition of action selection into three reasoning steps: task grounding, state grounding, and agent grounding.The first step, task grounding, involves generating a plan for task-solving and conditioning actions on this plan, with RCI being used to improve the plan's success rate.The state grounding subsection discusses the importance of grounding actions in the environment for language-based agents and how implicit RCI is used to refine task-grounded actions to be feasible in the current state.Lastly, the agent grounding</p>
<p>Explicit RCI</p>
<p>Here is a plan to solve the task using the instructions specified: 1. Type "rm [filename]" command to delete the file ending with ".rb".2. Press "enter" key Find problems with this plan.This plan does not specify the filename of the file ending with the extension .rb.Without knowing the filename, the agent cannot delete the file Based on this, the improved plan for the agent to complete the task are as follows.1. Type "ls" command to list all files in the terminal.2. Type "rm [filename]" command to delete the file ending with ".rb".3. Press "enter" key after typing each command to execute the command.</p>
<p>Plan Critique</p>
<p>Improve</p>
<p>According to the current plan, the next proper instruction should be <code>type "rm [filename]"</code>.</p>
<p>Considering the output on the webpage, the specific instruction for solving the task should be <code>type</code>instruction with "rm code.rb"command.</p>
<p>The task is to use the terminal below to delete a file ending with the extension .rb.</p>
<p>Task</p>
<p>Therefore, the single instruction that matches one of the regular expressions is `type "rm code.rb"3</p>
<p>Implicit RCI</p>
<p>Figure 3: An illustrative execution trace of the agent for terminal tasks with RCI prompting.The language model generates a step-by-step plan for the high-level task described in natural language, which in this case involves using the terminal to delete a file ending with ".rb".We then run an explicit RCI on this plan, where we sample an improved plan based on the critique and the previous plan, resulting in an improvement in the task-grounding of the plan.For each step, we first sample the task-grounded action that follows the improved plan, and then the implicit RCI updates the task-grounded actions sequentially to provide state-grounding and agent-grounding.Finally, the agent-grounded action is executed by the instruction-following agent on the environment.The prompts are highlighted, and the remaining text shows the outputs generated by the language model.step focuses on ensuring that actions are admissible for the computer agent by employing implicit RCI and conditioning agent-grounded actions on the current state, task, and other grounded actions, with a loop count set to optimize performance.</p>
<p>Problem Setting</p>
<p>We assume that we are given an instruction-following computer agent that can execute a set of admissible actions given some natural language instructions.An instruction that is not part of the admissible actions will be ignored.At every step, we receive a high-level natural language task prompt and a state of the environment.Given the current state and task, we sample the most probable action from LLMs.The generated natural language action is then fed into the computer agent.Sampling the actions in a fully generative manner presents a challenge, as the actions must consider the given task, feasibility in the current state, and admissibility for the computer agent simultaneously.Therefore, we propose decomposing this action sampling into three reasoning steps each of which considers task grounding, state grounding, and agent grounding.Task grounding improves actions to be more effective in solving the given task, state grounding ensures the feasibility of actions in the current state, and agent grounding considers the executability of actions given the specification of the computer agent.We first sample a step-by-step plan to solve the given task which improves the task grounding.Next, the task-grounded action is sampled conditioned on the current state, task, and the generated plan.The state-grounded actions is generated conditioned on the task-grounded action.</p>
<p>If the task-grounded action is not executable by the computer agent, the agent-grounded action is sampled.For each sampling of grounded action, we use RCI prompting to make LLM consider some specific information for grounding.</p>
<p>Grounding Language Model in Computer Tasks</p>
<p>Task grounding.In the action sampling process, the first step involves generating a plan of actionable steps for task solving from LLMs.Subsequently, actions are sampled from the same LLMs, taking into account the present state, task, and generated plan.The benefits of conditioning on the plan for improved grounding of actions are twofold.First, it enables LLMs to identify the stage of task solving at which the agent is located, serving as a memory module.Second, we can perform explicit RCI on the generated plan to further improve the plan's success rate.Although the number of explicit RCI loops can be arbitrary, we observe that a single pass of explicit RCI suffices for most of MiniWoB++ tasks.</p>
<p>State grounding.In language-based agents, grounding actions in the environment is a crucial step to enable real-world task performance.The aim of this phase is to enhance the task-grounded actions to be feasible in the current state.Although the actions generated in the preceding phase may align with the task, they may lack the specificity required to be executed in the current context.For example, if the assigned task is to forward an email from Bob to Alice and the action obtained from the task grounding phase is to click on an email from Bob in the email inbox, it is necessary to establish a connection between the abstract concept of "email from Bob" and the concrete element, such as the email heading, in the current webpage state represented by HTML.To achieve this goal, we perform the implicit RCI and prompt the LLMs to consider the current state, which subsequently outputs refined state-grounded actions.Moreover, the state-grounded action is additionally conditioned on the task-grounded action.We avoid repeating the implicit RCI cycle more than once as it does not impact the success rate based on our observations.Agent grounding.To ensure the successful integration of language-based methodologies in decision-making processes, it is imperative to establish a scalable framework that guarantees the admissibility of actions derived from the language model.While the preceding steps of sampling produce a state-grounded action that is both feasible and grounded in the task, it may not be executable by the agent due to issues such as improper formatting.To address this, Implicit RCI is employed, whereby an agent-grounded action is sampled conditioned on the current state, task, task-grounded action, and state-grounded action.The LLMs are prompted to consider specifications of the computer agent.The implicit RCI is repeatedly run until the resulting action is executable, with a maximum loop count set to limit the number of iterations.Empirical analysis on MiniWoB++ tasks suggests that setting the loop count to 3 yields optimal performance.</p>
<p>3 Evaluation</p>
<p>Reasoning tasks</p>
<p>In our grounding enhancement process, RCI prompts the LLM to criticize its prior output, considering the given context (e.g., current task, state, and agent), which ultimately leads to improved output.We first demonstrate the effectiveness of RCI prompts in augmenting the reasoning capabilities of LLMs across a range of reasoning benchmarks.We compare RCI to Chain-of-Thought (CoT) prompting, a state-of-the-art method recognized for its effectiveness in reasoning tasks.</p>
<p>Specifically, we compare our approach with Few-Shot-CoT [73] where a few chain-of-thought demonstrations are given as examples in prompting, and Zero-Shot-CoT [33] that elicit multiple reasoning steps by simply adding "Let's think step by step" to the prompt.Following Kojima et al. [33], our evaluation is conducted with 8 datasets from two categories of reasoning: arithmetic and commonsense.Please refer to Appendix C.2 for a comprehensive depiction of the datasets.We use the same experimental setting with their answer extraction method except that we use InstructGPT-3 + RLHF (gpt-3.5-turbo)as the underlying language model.We use the same prompts that CoT uses and we also use the answer cleansing approach used in CoT, but we only used answer extraction prompting in zero-shot CoT experiments.We also use the same few-shot examples that were introduced in [73] to evaluate Few-Shot CoT's performance on five arithmetic reasoning tasks.A threshold is established by setting the maximum number of RCI loops to two, terminating the loop once the output aligns with the ground-truth data.We observed that in the absence of this external feedback mechanism, the RCI process is prone to false negative critics, subsequently leading to a decrease in performance.Experimental results indicate that RCI without external feedback achieves zero-shot performance in half of the benchmark tests, but underperforms in others, as shown in Appendix 17.</p>
<p>Comparison with Zero-Shot.RCI prompting is better at solving reasoning tasks compared to zeroshot prompting.Table 1 summarizes the accuracy of our approach (Zero-Shot + RCI) and standard zero-shot prompting for each reasoning benchmark.Zero-Shot + RCI substantially outperforms the standard prompting in all benchmarks including arithmetic (GSM8K, MultiArith, AddSub, AQUA, SVAMP, SingleEq) and common sense (CommonSenseQA, StrategyQA) tasks.RCI prompting even achieves score gains from two arithmetic reasoning tasks (SingleEq and AddSub), which do not require multi-step reasoning.This distinguishes our RCI prompting from the previous CoT prompting methods [73,33] that are not useful in simple reasoning tasks.It is also worth noting that RCI prompting achieves a significant performance gain in commonsense reasoning tasks (CommonSenseQA and StrategyQA).While Wei et al. [73]  Table 2: Chain-of-Thought prompting exhibits a synergistic effect when coupled with RCI prompting in arithmetic reasoning tasks.</p>
<p>Computer tasks</p>
<p>Setup</p>
<p>MiniWoB++ benchmark suite.The miniwob++ task suite is selected as the main benchmark to evaluate our computer agent.MiniWoB++ [36], an extension of MiniWoB [61], is a web-based simulation environment that offers a diverse range of computer tasks, from simple button-clicking to complex compositional tasks requiring advanced reasoning, such as solving math problems.Its shared action space, including keyboard and mouse, and a common state space centered around HTML code enables our proposed agent to be thoroughly evaluated in ample tasks.Additionally, the varying levels of complexity between tasks enable a systematic evaluation of our work.The action space consists of two operations each of which controls the keyboard and mouse.The first action enables typing of arbitrary characters or special keys such as Backspace and Enter.The second action involves moving and clicking the mouse, allowing the agent to interact with visible HTML elements on a webpage.All actions can be executed through natural language instructions defined by regular expressions that are presented within the initial prompts provided to the LLMs.The regular expressions employed in our evaluation are presented in Appendix D. Our action space definition is similar to previous works,  18.(b) Relationship between performance and amount of expert training data.Our agent displays comparable performance to the current state-of-the-art scores on the MiniWoB++ benchmark, despite using the least amount of data.</p>
<p>such as [25,32,36], in which clicking actions directly interact with HTML elements.However, for typing actions, we extend beyond simple form-filling by using keyboard-based typing actions.Instead of relying on dictionary-based typing actions [30], where the agent simply chooses from a predefined dictionary of texts, our approach requires the agent to predict the proper text input.Our approach, therefore, has a better generalization capability for diverse computer tasks.The state space of our agent consists solely of HTML code.</p>
<p>Model choices.For the purpose of evaluating the effectiveness of RCI prompting, multiple language models are used in our experiments.Specifically, we employ three models, namely, GPT-3 (davinci) [5], InstructGPT-3 (text-davinci-002) [47,72,57], and InstructGPT-3 + RLHF (gpt-3.5turbo,gpt-4) [47].Unless otherwise specified, we primarily evaluate our computer agent with the InstructGPT-3 + RLHF models (gpt-3.5-turbo,gpt-4).Additionally, we use GPT-3 and InstructGPT-3 models for ablation studies.All the models were obtained through the OpenAI API, and further details can be found in Appendix C.1.</p>
<p>Evaluated tasks.We employ a set of 55 tasks to enable fair comparisons with baselines, as previous works are only evaluated on a subset of tasks consistently.Furthermore, to assess the performance of models on challenging tasks, we have selected tasks that involve free-form language typing actions, which have been reported to have an almost-zero success rate in previous works (e.g., terminal).Notably, certain commonly evaluated tasks in prior works are excluded due to the excessive length of HTML code for some UI components, which are described in Appendix C.3.</p>
<p>Metrics Consistent with prior studies, our main evaluation criterion is the success rate, which measures the ability of our agent to actually complete the assigned task.This rate is calculated as the proportion of successful episodes, which are defined as those in which the agent receives a positive reward.We identified two modes of failure: the production of unexecutable actions and task failure.</p>
<p>When the agent generates an unexecutable action following the implicit RCI step, it fails immediately.Moreover, an episode is considered unsuccessful when the agent, despite effectively executing the plan generated, is unable to accomplish the task and thus receives no reward.</p>
<p>Outperforming baselines on MiniWoB++ task suite</p>
<p>We present Figure 4a which summarizes the average success rate of our agent and baseline models over the MiniWoB++ benchmark.The results demonstrate significant outperformance of our approach over supervised learning models.Specifically, we observe a 41% higher score than the WebN-T5-3B, which employs a finetuned large language model with 12K expert demonstration data.Our approach also outperforms reinforcement learning approaches that require an order of magnitude more interactions with the environment.Among all the baselines, our approach achieves the second highest score.The sole model that surpasses our agent is the CC-Net, which involves co-training of reinforcement learning and imitation learning.However, a direct comparison with CC-Net is not possible since it uses dictionary-based typing actions.In other words, CC-Net selects text from a predefined list for typing actions in some tasks, while our approach is fully generative.Thus, CC-Net (without dictionary-based action) in Figure 4a serves as our appropriate comparison and we outperform it by 6%.The performance data for CC-Net (with no dictionary-based action) is obtained from the ablation study section in their paper [30].</p>
<p>Another comparative analysis is performed to evaluate the performance of our agent in contrast to the state-of-the-art agents in three categories, namely supervised learning, reinforcement learning, and a combination of both.To facilitate a fair comparison, we specifically isolate LLM-based state-ofthe-art approaches, which share similarities with our approach to solving computer tasks.The best per-task performance achieved by each category is then aggregated, and the outcomes are presented as SotA in Figure 4a.The result shows that our agent surpasses SotA by 37 percentage points in supervised learning and by 27 percentage points in reinforcement learning.Notably, our proposed RCI prompting method outperforms the SotA LLM approach [24], even when the latter employs both finetuning and few-shot examples in prompts.This outcome highlights the effectiveness of our approach in extracting vital knowledge for computer tasks from language models.Our agent even achieves a slight edge over SotA (less than 1 percentage point) in the combined use of supervised and reinforcement learning, which employs significantly more expert data and online interactions.We also provide task-level performance comparisons in Figure 10, where tasks are arranged in ascending order based on the difference between our agent's performance and the baseline.We observed three main failure modes of our agent: (i) underperformance in tasks that require long-horizon planning (e.g., guess-number, search-engine, use-spinner), (ii) difficulty in selecting appropriate actions for tasks that require multi-step reasoning (e.g., tic-tac-toe, use-autocomplete), and (iii) lower scores in tasks that rely on visual rendering of HTML code to solve the task (e.g., count-shape).These failures are explained in more detail in Appendix F.</p>
<p>Lowest sample complexity</p>
<p>Figure 4b provides a comparative analysis of the total number of samples used in several models and their mean performance.We begin by discussing CC-Net [30] model, which employs 2.4 million expert demonstrations (equivalent to 6,300 hours) collected from 77 human participants across 104 tasks for behavior cloning.This amounts to an average of 23,076 demonstrations per task.In contrast, the WebN-T5-3B [24] model uses 12,000 expert demonstrations to fine-tune its pre-trained T5 model.</p>
<p>Rather than directly updating model parameters with demonstration data, our approach involves integrating two to three demonstrations into the prompt for in-context learning, which biases the model output without any parameter updates.This approach allows our agent to generalize to unseen tasks with only a handful of demonstrations.Our results show that our agent achieved a higher success rate than all baselines, requiring 120x fewer samples than WebN-T5-3B and 11,000x fewer samples than CC-Net.Given the challenges of obtaining expert demonstrations for computer tasks, our findings demonstrate the practicality of our approach in automating such tasks.</p>
<p>Ablating the groundings</p>
<p>This section examines the impact of grounding improvement on task success rates.We conduct ablations to isolate the contributions of task, state, and agent grounding improvements by eliminating RCI prompting at each stage.We categorize tasks by three different difficulty levels to provide a more detailed understanding of the effects of grounding improvements across a diverse range of tasks.We conducted a task grounding ablation by eliminating the plan sampling stage.This modification entails generating actions directly from the state, without the need for conditioning on a step-by-step plan.State grounding is evaluated by directly applying the agent-grounding update to task-grounded actions.Lastly, we ablate the implicit RCI of the agent grounding by letting the state-grounded action be the final output of the agent.Figure 5 illustrates the performance degradation resulting from each ablation of grounding.Our results indicate that each grounding contribution is essential to solving computer tasks, with each contributing almost equally to the overall success rate.The reason for this is partially due to the fact that the three methods of improving grounding are not mutually exclusive, but rather complementary, with one enhancement in grounding contributing to multiple action groundings.Examples of cross-grounding improvement are provided in Appendix E.</p>
<p>Moreover, it has been observed that state grounding plays a crucial role in enabling an agent to use relevant information during episodes, particularly in scenarios where the initial state does not offer sufficient information to accomplish the task, such as terminal task.Interestingly, task grounding significantly improves the success rate when a task requires a long-horizon action plan, such as the click checkboxes large task.We also observe that agent grounding significantly enhances the feasibility of actions.Notably, in simpler tasks, the success rate decreases by 60% in contrast to the baseline without the agent grounding.This finding is of particular significance as it distinguishes our work from prior investigations [1,28], which employ additional trained model components.In contrast, our study solely relies on the reasoning ability of language models.</p>
<p>Ablating the language model</p>
<p>The performance of our agent is contingent on the quality of the underlying pre-trained language models used, so enhancing language models can lead to an improvement in the agent's performance.In this section, we present a comparison of the agent's performance using three distinct language models: GPT-3, InstructGPT-3, and InstructGPT-3 + RLHF (gpt-3.5-turbo).Our objective is to investigate the relationship between LLMs' capability and their ability to solve MiniWoB++ tasks.The experimental setting employed in Section 3.2.4 is replicated in this study.Figure 6 depicts the average success rate of three language models on tasks of varying difficulty levels.Our results reveal that LLMs struggle to effectively complete tasks without instruction fine-tuning.This may be attributed to the absence of intricate prompt engineering, as our observations have indicated that GPT-3 displays sufficient competence in comprehending HTML code, regular expressions, and engaging in reasoning.</p>
<p>Limitations</p>
<p>In the course of our work, several limitations became apparent that may serve as potential avenues for further research.One central concern is our primary focus on the InstructGPT-3 + RLHF models (gpt-3.5-turbo,gpt-4), leaving the generalization ability of RCI to other models unexplored.The versatility of RCI across diverse models remains a pertinent question, suggesting that future studies should expand their scope to determine the robustness and adaptability of RCI.Handling lengthy HTML presents another challenge.The current model grapples with extensive HTML states.While it has been suggested that efficiency might be bolstered by pruning HTML states to exclude non-critical elements, the task itself is non-trivial.A fundamental constraint of LLMs is the limited context length, which can hamper handling extensive HTML states effectively.Addressing this may require architectural adjustments or novel parsing methods.Our agent's action space, mainly restricted to clicks and typing, limits its web navigation capabilities.There's a need to diversify its actions for a more seamless experience.Furthermore, The agent's focus on short-term decisions overlooks the necessity for long-term strategy, especially in tasks requiring coordinated sequences.Broadening this focus is essential for versatile applications.Lastly, the intricate UI components populating contemporary websites present a challenge for LLMs to fully understand the HTML states.The subtle nuances of such components, which may not be discernible through HTML alone, underscore the need for adding more modalities to the state definition.Addressing these issues is crucial to enhance the RCI agent, making it more adaptable and efficient in practical applications.</p>
<p>Discussion</p>
<p>This work is part of a growing literature showing that LLMs might be all you need for hard decisionmaking problems [76].In contrast to imitation learning and reinforcement learning approaches, LLMs can solve novel tasks in a zero-shot or few-shot manner, and don't require task-dependent expert data or a reward function.Furthermore, we expect that as the capabilities of LLMs and foundation models increase, our method will naturally improve as well.However, we find that current capabilities of LLMs aren't as powerful as task-dependent SL+RL approaches on some computer tasks.Also, RCI is more expensive to run compared to approaches that just sample once from the LLM.There are many avenues for future research in increasing the capacity of LLMs in decision-making tasks.First, our experiments use LLMs on HTML code, but ideally methods based on multimodal foundation models [16,55,2,46] will be able to take actions based on text, images, audio, and video as input [4,18,44,71].Second, the results presented in this paper all use pre-trained LLMs.We expect the performance of our method to increase when using LLMs fine-tuned to solve computer tasks.</p>
<p>Importantly, current LLMs are poor at reasoning tasks, such as playing tic-tac-toe, because they do not think ahead.Although RCI improves reasoning capabilities in LLMs, there exists much work to be done on increasing the reasoning capabilities in LLMs.This will be crucial to accomplish hard cognitive tasks on computers that require thinking ahead.Similar to other prompting-based approaches for reasoning in LLMs, RCI can be viewed as using the LLM's output to write to an external memory, which is later retrieved to choose an action.LLMs with memory have been demonstrated to be computationally universal [60], meaning that in principle all that is needed to run arbitrary programs is the right prompt.Since RCI represents a basic version of this powerful framework, we anticipate the development of more advanced RCI variations in the future.There is a vast array of potential methods that repeatedly feed the output of particular prompts into the LLM.For example, multiple different LLMs can simulate the information exchange between team members in an organization.This would enable the merging of diverse perspectives to tackle complex problems.In such a context, incorporating game theory and multi-agent systems research could significantly enhance the overall performance.Reinforcement learning could be used to discover effective structures involving loops and prompts [81], either through human feedback or a given reward function.This optimization process can be further refined by exploring the space of potential loop and prompt structures, identifying those that yield the best results, and fine-tuning the model accordingly [75].</p>
<p>Appendix A Broader Impacts</p>
<p>Although the results presented in this paper are only on a research benchmark, if we extrapolate forward the capabilities of these models and methods, we anticipate vast broader impacts that have the potential to revolutionize numerous industries.By allowing LLMs to execute tasks on computers, our approach can enhance the capabilities of AI assistants and automation tools.This could lead to increased efficiency, reduced labor costs, and improved user experiences across any sector which uses computers to do work.We are most excited about gains in productivity in science and education, including AI research, which will lead to even faster development of new beneficial technologies and treatments.</p>
<p>However, there are many potential misuses and unintended consequences associated with allowing these models to take actions in the world.Malicious actors may leverage LLMs to automate cyberattacks, manipulate information, or propagate disinformation on a large scale.Additionally, the potential loss of jobs due to widespread automation could lead to economic disruption and increased income inequality.There are also obvious security risks of running LLMs on computers (or even virtual machines) such as prompt injection attacks.Perhaps most alarming, future LLMs taking actions on computers may lead to catastrophic runaway chains of events, especially if LLMs are integrated widely in the economy.</p>
<p>To mitigate these risks, it is crucial for researchers, policymakers, and industry leaders to work together to establish regulations and ethical guidelines that govern the development and deployment of such technologies.Ensuring transparency, accountability, and fairness in AI systems will be vital in harnessing the benefits while minimizing potential harm.We also believe that the time has come where we as a research community must discuss possible ways to coordinate to slow down the pace of developing highly-disruptive technology, if necessary.</p>
<p>B Related Works B.1 Automated computer tasks</p>
<p>The automation of computer tasks is an important topic for both information retrieval and natural language processing [43,50,49,31,65].Recent efforts have focused on the development of reinforcement learning agents that interact with websites using raw mouse and keyboard actions [61].MiniWoB, a benchmark proposed in [61], has been extended in MiniWoB++ [36], which has become a widely-used platform for studying models for computer tasks.Reinforcement learning and imitation learning have been employed in several studies to tackle MiniWoB++ tasks [36,25,32,23].However, achieving human-level performance requires a significant amount of expert demonstration data (6,300 hours), as demonstrated in [30].Recent work [24,19] has suggested the use of large language models (LLMs) to comprehend HTML code and vision transformer [15] to extract screenshot image features, with a few-shot in-context approach showing promising results without extensive RL exploration.Nevertheless, significant amounts of expert demonstration data are still required to finetune LLMs.On the contrary, the agent we suggest needs less than two demonstrations per task on average and doesn't necessitate any finetuning.WebGPT [42] and WebShop [77] show that LLMs can automate some web-based tasks by introducing a handful of custom commands such as Search <query> and Next Page.As a result, these methods are limited in scope and do not work on general computer tasks which require keyboard strokes and mouse clicks.In contrast, our approach can tackle open-domain tasks at scale.</p>
<p>B.2 LLMs with actions</p>
<p>In recent years, there have been significant advancements in large language models (LLMs), leading to new possibilities for utilizing natural language for decision-making tasks.One approach involves augmenting LLMs with executable actions [41].Huang et al. [28] showed that LLMs can be used to plan and achieve simple household tasks, utilizing a method for grounding the actions generated by LLMs by comparing their embeddings with a predefined list of admissible actions.However, their work did not consider state grounding.Another study by Ahn et al. [1] proposed SayCan, which grounded the actions by multiplying each candidate action's probability under FLAN [72] with the action's value function, serving as an indicator for the suitability of actions.Huang et al. [29] proposed an extension to the SayCan model called Inner Monologue, which incorporates a feedback loop for state grounding.However, Inner Monologue still requires a pre-trained languageconditioned robot policy with underlying reasoning capabilities that are not free-formed and flexible, thereby hindering generalization to diverse task domains.Similarly, Zeng et al. [80] employed a combination of LLMs with a visual-language model (VLM) and a pre-trained language-conditioned robot policy [63] to perform open vocabulary pick-and-place robotic tasks.Meanwhile, Dasgupta et al. [13] used Chinchilla [26] as a planner for an agent in the PycoLab environment, but their actor module requires pre-training with reinforcement learning (RL) to follow natural language instructions.In a related line of research, Carta et al. [7] employed online RL fine-tuning to achieve functional grounding of LLMs in the BabyAI-Text environment.In contrast to these previous approaches, our method does not rely on additional model components beyond LLMs for grounding actions.Instead, we propose the use of RCI prompting, which enables LLMs to update their actions to be grounded autonomously.As a result, our approach can scale to a wider range of action spaces, including keyboard and mouse actions.Furthermore, prior approaches have been limited by the need for fine-tuning.In contrast, our RCI prompting method is a zero-shot approach that overcomes these limitations.More recently, an approach to improve the efficacy of LLMs involves their integration with APIs, allowing them to use external tools such as information retrieval systems, code interpreters, and web browsers [59,68,40,22].Notably, these external tools necessitate manual engineering and may be constrained in their functionality.In contrast, our agent is equipped with a general computer interface, enabling it to access a wide range of functionalities offered by computers.</p>
<p>B.3 LLMs with reasoning</p>
<p>Recent research has also demonstrated that large language models (LLMs) exhibit enhanced performance in compositional tasks when they produce traces of the underlying reasoning process along with the final answer, as evidenced by studies such as [73,45,33,52].This discovery has led to the emergence of a new line of research where reasoning capabilities are used to address tasks beyond reasoning [78,29], or enhance reasoning proficiency [33,37,70,39,79,53,75,66,14].Furthermore, various reasoning architectures have been proposed, expanding from naive prompting, such as Selection-Inference [11], Least-to-Most [82], and Faithful reasoning [10].In the existing literature, a work closely related to our research is ReAct [78] which interleaves reasoning and action for resolving the issue of hallucination and error propagation as well as helping the model induce, track, and update action plans.An alternative method, Reflexion [62], extends ReAct by improving its performance by allowing LLMs to consider previous trial and error experiences.Nevertheless, due to the necessity of multiple rounds of explicit task-specific success feedback from trial and error, this approach may not scale as effortlessly as ours because it requires task-specific success feedback.Similarly, Corrective Re-prompting, as proposed by Raman et al. [54] necessitates the establishment of task-specific preconditions.RCI pertains to an extended reasoning architecture where LLMs are instructed to find errors in their outputs and improve them accordingly, which can further be used to ground actions generated from LLMs in decision-making problems.Saunders et al. [58] used a similar approach to ours by utilizing the self-critiquing ability of LLMs to generate critical feedback on summaries produced by LLMs.The aim is to accelerate the human evaluation process by uncovering possible errors in the generated summaries.Likewise, Ganguli et al. [20], employed LLMs to morally self-correct their outputs to prevent the generation of harmful content.</p>
<p>The most recent work that is in the same vein with RCI is Self-Refine [38] which uses localized and aspect-based feedback to iteratively refine outputs from LLMs.However, our work is, to the best of our knowledge, the first to demonstrate the self-critiquing capability of LLMs only with implicit feedback (e.g., "Find problems with this plan") in enhancing reasoning proficiency.</p>
<p>C Experimental setup C.1 Language models</p>
<p>In our evaluation, various pre-trained language models were used.RCI prompting on reasoning tasks is evaluated using gpt-3.5-turbo,which is presented in Table 1 and Table 2. Our primary evaluation on MiniWoB++ tasks is conducted using gpt-3.5-turboand gpt-4, as shown in Figure 4a and Figure 10.We also used davinci, text-davinci-002, and gpt-3.</p>
<p>C.2 Reasoning tasks</p>
<p>We conducted an evaluation of RCI prompting on eight datasets, encompassing two categories of reasoning tasks: arithmetic and commonsense.In the domain of arithmetic reasoning, we considered six datasets: SingleEq [34], AddSub [27], MultiArith [56], AQuA [35], GSM8K [9], and SVAMP [51].</p>
<p>For commonsense reasoning, we utilized the CommonsenseQA dataset [67] and the StrategyQA dataset [21].To ensure a fair comparison with baselines, we specifically selected tasks that were employed in the work of Kojima et al. [33].In the experiment on reasoning tasks, we enable RCI loop to get implicit feedback to correct outputs.We fix the maximum number of loops to 2. Following previous works [38,62,74], we use the correct label to decide when to stop the RCI loop.In our setting, we can consider the correct label to be another source of feedback (label feedback).</p>
<p>C.3 MiniWoB++ task selection</p>
<p>In order to ensure a fair and comprehensive evaluation, a subset of MiniWoB++ tasks we use in the evaluation is selected from the evaluation of WebN-T5-3B [24], the most recent work on MiniWoB++ tasks, which employs LLMs.However, certain tasks such as book-flight, choose-date-easy, choosedate-medium, choose-date, and click-pie have been excluded from our evaluation due to their HTML code exceeding the maximum context length of language models.On the other hand, some of the challenging tasks such as terminal and simple-algebra have been included in the evaluation.The choice of these tasks is determined by the suboptimal performance of CC-Net [30], which currently represents the state-of-the-art model in the field.The purpose of this inclusion is to showcase the potential of leveraging LLMs in computer tasks, in contrast to the conventional approaches of Supervised Learning (SL) and Reinforcement Learning (RL).While our agent has not been evaluated on tasks that necessitate additional actions, such as drag and copy &amp; paste, we posit that their inclusion can be readily achieved through the expansion of the actions space specification within the prompts.</p>
<p>C.4 MiniWoB++ task selection for ablation studies</p>
<p>In ablation studies, we categorize the tasks based on the success rate achieved by our agent with gpt-3.5-turbo.We select a subset of tasks from three levels of difficulty, as depicted in Table 4.</p>
<p>C.5 Modifications on MiniWoB++ tasks</p>
<p>In Table 5, we outline several modifications that were incorporated into the MiniWoB++ benchmark for the purpose of our evaluation with language models that have a limited context length.</p>
<p>Tasks Modifications social-media-all</p>
<p>We constrain the quantity of media components ranging from three to six.social-media social-media-some email-inbox-forward-nl-turk</p>
<p>The quantity of randomly generated emails has been restricted to a range of three to six.email-inbox-forward-nl email-inbox-nl-turk</p>
<p>D Prompts for MiniWoB++ tasks</p>
<p>We have an autonomous computer control agent that can perform a set of instructions to control computers.First, given the instruction that matches the regular expression, <type regex>, it can type a list of characters via the keyboard.This instruction should specify the target keyboard input for the agent to type.Before this typing instruction, you should first locate the cursor by clicking the input box with the click instruction.Second, given the instruction that matches the regular expression, <press regex>, it can press a specific key on the keyboard.Third, given the instruction that matches the regular expression, <clickoption regex>, it can click an option HTML element in a list with an XPath that is visible on the webpage.The target of this instruction should be a valid XPath.Fourth, given the instruction that matches the regular expression, <movemouse regex>, it can move the mouse cursor on an HTML element with an XPath that is visible on the webpage.Lastly, given the instruction that matches the regular expression, <clickxpath regex>, it can click an HTML element with an XPath that is visible on the webpage.The target of this instruction should be a valid XPath.<type regex> = "^type\s.{1,}$"<press regex> = "^press\s(enter|arrowleft|arrowright|arrowup|arrowdown| backspace)$" <clickoption regex> = "^clickoption\s.{1,}$"<movemouse regex> = "^movemouse\s.{1,}$"<clickxpath regex> = "^clickxpath\s.{1,}$"Listing 1: Regular expressions for specifying the admissible actions.</p>
<p>Based on the current plan and task, the next proper instruction should be Table 7: Task grounding.</p>
<p>Considering the output on the webpage, the specific instruction should be Table 8: State grounding.</p>
<p>This action does not match the regular expressions.The updated instruction that matches one of the regular expressions is `Table 9: Agent grounding.</p>
<p>E Examples of cross grounding</p>
<p>task: click on the "ok" button on the webpage.plan: 1. type "sit arcu nullam:" 2. clickxpath //button[@data-wob_ref="6"]</p>
<p>We have a history of instructions that have been already executed by the autonomous agent so far.</p>
<p>No instruction has been executed yet.</p>
<p>Based on the current plan and the task, the first instruction should be <code>type "sit arcu nullam:"</code>.Therefore, considering the output on the webpage, this instruction for solving the task should be <code>clickxpath //button[@data-wob_ref="6"]</code>.</p>
<p>Table 10: The state grounding optimizes the action plan.In the initial plan, the inclusion of the type instruction is deemed superfluous for accomplishing the task.</p>
<p>task: Find the email by Clarinda and reply to them with the text "Vel id at.". plan:</p>
<ol>
<li>clickxpath //div[@class='email-sender' and text()='Clarinda'] 2. clickxpath //span[@class='email-reply'] 3. clickxpath //<em>[@id="reply-text"] 4. type Vel id at. 5. clickxpath //</em>[@id="send-reply"]</li>
</ol>
<p>We have a history of instructions that have been already executed by the autonomous agent so far.0: clickxpath //div[@class='email-sender' and text()='Clarinda'] 1: clickxpath //span[@class='email-reply'] 2: clickxpath //*[@id="reply-text"]</p>
<p>Based on the current plan and the task, the next proper instruction after the history should be <code>type Vel id at</code>.Therefore, considering the output on the webpage, this instruction should be `type Vel id at.T able 11: The state grounding corrects actions to be grounded in the task.Although the target of typing action in the action plan is "Vel id at." the generated task-grounded action lacked the last punctuation mark.Nonetheless, the state grounding update was successful in remedying this discrepancy, resulting in the state-grounded action being "Vel id at."Consequently, the state-grounded action now matches the typing action specified in the action plan.</p>
<p>F.1 click-dialog-2</p>
<p>The state-grounding process is occasionally inadequate for maintaining task-grounding of actions.Specifically, in the click-dialog-2 task, grounding actions in the current state results in task-irrelevant actions.Consequently, we opt to exclude the state grounding step for this particular task.</p>
<p>task: click the button in the dialog box labeled "x".plan:</p>
<ol>
<li>Clickxpath //div[@class='ui-dialog-titlebar ui-corner-all ui-widget-header ui-helper-clearfix ui-draggable-handle']/button[@title='Close']</li>
</ol>
<p>We have a history of instructions that have been already executed by the autonomous agent so far.</p>
<p>No instruction has been executed yet.</p>
<p>Based on the current plan and task, the first instruction should be <code>clickxpath //div[@class='ui-dialog-titlebar ui-corner-all ui-widget-header ui-helper-clearfix ui-draggablehandle']/button[@title='Close']</code>.Therefore, considering the output on the webpage, this instruction should be <code>clickxpath //div[@class='ui-dialog-buttonset']/button[@title='OK']</code>to click the button in the dialog box labeled "x".</p>
<p>Table 12: Example trajectory on click-dialog-2.The state grounding updates the task-grounded actions to be irrelevant to the task.</p>
<p>F.2 click-menu</p>
<p>We find that including explanatory trajectories in the few-shot examples can significantly reduce the number of required few-shot examples.Specifically, in the click-menu task, the selection of the target component necessitates the mouse hovering over the expandable menu.Therefore, we provided explanations regarding the mouse hovering technique in the few-shot examples.</p>
<p>F.3 count-shape</p>
<p>This task necessitates an agent to visually perceive rendered images on a canvas HTML element and enumerate the relevant shapes.Unfortunately, our agent encounters difficulties in tackling such tasks which entail the need for perceiving visual representations, as it solely depends on HTML observation.</p>
<p>task: select Leonelle &gt; Jane &gt; Amabel from the HTML code provided.plan: 1. movemouse //<em>[text()="Leonelle"] // we need to hover the mouse to expand the menu 2. movemouse //</em>[text()="Jane"] // we need to hover the mouse to expand the menu 3. clickxpath //*[text()="Amabel"] task: select the option "Joye &gt; Phylis" from the HTML code provided.plan:</p>
<ol>
<li>movemouse //<em>[text()="Joye"] // we need to hover the mouse to expand the menu 2. clickxpath //</em>[text()="Phylis"]</li>
</ol>
<p>F.4 guess-number</p>
<p>When considering decision-making that involves a long-term perspective, our agent encounters difficulties in generating actions that are grounded in the current task.For instance, in this guessnumber task, the plan generated by our agent involves the sequential typing of numbers from 0 to 9 in order to solve the task.However, due to the high number of steps required (over 50), our agent occasionally produces actions that deviate from the original plan, ultimately resulting in irreparable failure.</p>
<p>F.5 login-user-popup</p>
<p>This task involves the identification of an optimal strategy for an agent to handle an unpredictable pop-up window that emerges during a task.Due to the unexpected nature of the pop-up window, pre-planning the closure of the window is not feasible.Our agent is designed to adapt the agent's actions to the current state, so it should generate an appropriate instruction to close the pop-up window in the state-grounding step.Nevertheless, there are instances where it is unsuccessful in executing the pop-up window closure, leading to task failure.Based on the current plan and the task, the next proper instruction should be <code>type 2</code>.</p>
<p>Table 15: Example trajectory on guess-number.The agent fails to generate task-grounded action when long-horizon decision making is required.</p>
<p>F.6 tic-tac-toe</p>
<p>We also examine the causes of the poor success rate in playing tic-tac-toe.Two distinct factors are identified as responsible for its failure.The first factor is that our agent is unable to adopt a defensive strategy when its opponent is only one move away from victory and there is no immediate opportunity to win the game.The second factor relates to the inability to consider the possibility of its attack being blocked by the opponent.A case in point is the bottom-left illustration in Figure 9, where our agent's fourth move is ineffective since its diagonal direction is already obstructed by the opponent.</p>
<p>F.7 use-autocomplete</p>
<p>In use-autocomplete task, our agent demonstrated an ability to select words beginning with specific characters.However, it struggles when it comes to selecting words that ended with particular characters.This difficulty arose from the need to identify the correct word from the autocomplete list, a process that required the agent to press the down arrow button the exact number of times needed to locate the desired word within the list.</p>
<p>task: Enter an item that starts with "Se" and ends with "ia".plan:</p>
<ol>
<li>click the input field (e.g., clickxpath //<em>[@id="tags"]) 2. type the starting word (e.g., type Se) 3. press the down arrow key to select the word ends with "ia" (e.g., press arrowdown) 5. select the word (e.g., press enter) 6. click the submit button (e.g., clickxpath //</em>[@id="subbtn"])     Ours (w/ GPT-4) depicts the performance outcomes obtained through the use of the GPT-4 model for some tasks, which are visually highlighted in the color blue.The performance of baseline models has been sourced from prior studies [30,24].The average success rates of the tasks highlighted with violet color are shown in Figure 3.The state-of-the-art (SotA) in supervised learning (SL) is represented by the works of [30,24] while the SotA in reinforcement learning (RL) includes the studies of [30,25,32].Furthermore, the SotA in the combined application of SL and RL consists of the contributions of [30,61,36].Combined result of models proposed prior to CC-Net [30] is denoted as Others, which include [61,36,25,32].This corresponds to Aggregated SotA (Augmented) baseline in previous works [30].We generously estimate the performance of CC-Net (RL) based on their figures.</li>
</ol>
<p>G Additional results</p>
<p>Arithmetic</p>
<p>Figure 1 :
1
Figure 1: MiniWoB++ environment.Every task contains a natural language prompt in yellow.The agent then uses keyboard strokes and mouse clicks to accomplish the task.</p>
<p>FewFigure 2 :
2
Figure 2: Illustrative examples of explicit RCI prompting and baseline prompting approaches on the GSM8K dataset.RCI prompting effectively addresses logical errors that arise in the baseline prompting approaches.Prompts text is displayed in violet color.</p>
<p>Figure 4 :
4
Figure 4: (a) Average performance comparison with baselines.Our agent with RCI prompting achieves state-of-the-art performance in MiniWoB++ environment.The tasks that were included in the averaging process are indicated in Table18.(b) Relationship between performance and amount of expert training data.Our agent displays comparable performance to the current state-of-the-art scores on the MiniWoB++ benchmark, despite using the least amount of data.</p>
<p>Figure 5 :
5
Figure 5: Ablation analysis on the different types of grounding across tasks with varying degrees of difficulty.The experimental design employs the use of InstructGPT-3 + RLHF model (gpt-3.5-turbo).</p>
<p>Figure 6 :
6
Figure 6: Ablation study on different language models across tasks of varying degrees of difficulty.</p>
<p>F</p>
<p>Figure 7: Screenshots.</p>
<p>Figure 8: Screenshots.</p>
<p>Figure 9 :
9
Figure 9: Failure examples of tic-tac-toe task.</p>
<p>lo g in -u s e r -p o p u p e n te r -te x t-d y n a m ic fo c u s -te x t fo c u s -te x t-2 id e n ti fy -s h a p e lo g in -u s e r n a v ig a te -tr e e s e a r c h -e n g in e s o c ia l-m e d ia -a ll c h o o s e -li s t c li c k -ta b -2 -h a r d e n te r -ti m e c li c k -c o ll a p s ib le -2 g u e s s -n u m b e r e m a il -in b o x c li c k -s c r o ll -li s t c li c k -c h e c k b o x e s -tr a n s fe r c li c k -s h a p e c li c k -m e n u e m a il -in b o x -n l-tu r k lo g in -u s e r -p o p u p te x t-tr a n s fo r m c li c k -c h e c k b o x e s -s o ft c li c k -c h e c k b o x e s -la r g e e m a il -in b o x -fo r w a r d -n l-tu r k s o c ia l-m e d ia -s o m e u s e -s p in n e r m u lt i-la y o u ts c li c k -s h a d e s g r id -c o o r d in a te e m a il -in b o x -fo r w a r d -n l m u lt i-o r d e r in g s s im p le -a lg e b r a te r s s -n u m b e r c o u n t-s h a p e u s e -a u to c o m p le te lo g in -u s e r -p o p u p ti c -ta c -to e e m a il -in b o x -fo r w a r d -n l-tu r k e n te r -d a te m u lt i-la y o u ts u s e -s p in n e r c li c k -w id g e t e m a il -in b o x e m a il -in b o x -fo r w a r d -n l e n te r -p a s s w o r d e n te r -te x t e n te r -te x t-d y n a m ic fo c u s -te x t fo c u s -te x t-2 g r id -c o o r d in a te id e n ti fy -s h a p e lo g in -u s e r m u lt i-o r d e r in g s s e a r c h -e n g in e c h o o s e -li s t c li c k -c h e c k b o x e s -s o ft c li c k -c h e c k b o x e s -tr a n s fe r n a v ig a te -tr e e c li c k -c h e c k b o x e s c li c k -c o ll a p s ib le -2 c li c k -ta b -2 c li c k -s h a p e e n te r -ti m e c li c k -m e n u s o c ia l-m e d ia s o c ia l-m e d ia -s o m e te x t-tr a n s fo r m c li c k -c h e c k b o x e s -la r g e s im p le -a lg e b r a s o c ia l-m e d ia -a ll c li c k -s c r o ll -li s t te r</p>
<p>Figure 10 :
10
Figure 10: The task-level performance comparison with the state-of-the-art (SotA) baselines.The y-axis represents the residual values, which are obtained by subtracting the performance of SotA from our agent's performance.</p>
<p>Initial state Current state Task grounding State grounding Agent grounding
1 st step2 nd steprd step</p>
<p>Table 1 :
1
RCI prompting increases the reasoning capability of LLMs on all of eight reasoning benchmarks.
reported that only a substantially large
[58]pting does not yield significant performance gains.RCI prompting has a synergistic collaborative impact on the two CoT baselines.Namely, Zero-Shot CoT + RCI and Few-Shot CoT + RCI attain the highest scores on four out of the five tasks.These findings suggest a promising avenue for future research: combining RCI with other prompting methods for CoT, such as self-consistency[58].</p>
<p>Table 3 :
3
5-turbo for our ablation study on MiniWoB++ tasks.For all model usage, a maximum token length of 256 and a temperature value of 0, indicating greedy decoding, are used.All models are accessed through the OpenAI API between January 2023 and March 2023.Description of language models.( * 1) We identify the model size of GPT-3 by referring to the official document that OpenAI provides (https://beta.openai.com/docs/model-index-for-researchers).( * 2) The size of InstructGPT-based models remains undisclosed by its provider.
Language model# of parameters Max. tokens API provider API nameGPT-3175 B( * 1)2,049OpenAI API davinciInstructGPT-3( * 2)4,097OpenAI API text-davinci-002InstructGPT-3 + RLHF ( * 2)4,096OpenAI API gpt-3.5-turboInstructGPT-3 + RLHF ( * 2)8,192OpenAI API gpt-4</p>
<p>Table 4 :
4
The tasks used in the ablation study are classified according to their level of difficulty.
click-shape0.98easy [0.9, 1]click-widget0.98enter-date0.96click-checkboxes-soft0.72medium [0.6, 0.9)click-collapsible-20.62click-tab-20.74click-tab-2-hard0.56hard [0, 0.6)count-shape0.4guess-number0.2</p>
<p>Table 5 :
5
Modifications on MiniWoB++ tasks.</p>
<p>Table 6 :
6
Agent specification.</p>
<p>Table 13 :
13
Example trajectory on click-menu.Providing explanatory trajectory enhances the sampleefficiency of few-shot examples.task: answer the question "How many large items are there?".
plan:1. Clickxpath //button[text()="2"]</p>
<p>Table 14 :
14
Example trajectory on count-shape.The agent struggle to solve tasks that requires visual rendering of HTML.</p>
<p>Table 16 :
16
Example trajectory on use-autocomplete.The agent fails to identify how many times it has to press the down-arrow key.</p>
<p>Table 17 :
17
In the absence of external feedback, RCI prompting on reasoning benchmarks exhibits performance equivalent to, or below that of a zero-shot approach.
Common Sense</p>
<p>Table 18 :
18
Comprehensive task-level success rate evaluation of baseline models in MiniWoB++ tasks.
TASKOursOurs (w/WebN-CC-NetCC-NetCC-NetOthersSotASotASotAGPT-4)T5-3B(SL +(RL)(SL)(SL +(SL)(RL)(SL +RL)RL)RL)bisect-anglen/an/an/a0.971.000.290.800.291.000.97book-flightn/an/a0.000.870.000.001.000.001.000.87chase-circlen/an/an/a0.930.930.801.000.800.931.00choose-date-easyn/an/a0.030.990.990.42n/a0.420.990.99choose-date-mediumn/an/a0.000.990.020.26n/a0.260.020.99choose-daten/an/a0.000.970.010.121.000.121.000.97choose-list1.001.000.260.990.990.190.260.260.990.99circle-centern/an/an/a0.971.000.360.980.361.000.98click-button-sequence1.001.001.001.001.000.471.001.001.001.00click-button1.001.001.001.000.800.781.001.001.001.00click-checkboxes-large0.940.940.220.710.000.000.840.220.000.71click-checkboxes-soft0.720.960.540.950.120.040.940.540.120.95click-checkboxes-transfer1.001.000.630.990.550.360.640.630.550.99click-checkboxes1.001.000.960.980.450.321.000.961.000.98click-collapsible-20.621.000.000.980.880.170.990.170.880.98click-collapsible1.001.000.001.001.000.811.000.811.001.00click-color1.001.000.271.001.000.821.000.821.001.00click-dialog-21.001.000.241.001.000.881.000.881.001.00click-dialog1.001.001.001.001.000.951.001.001.001.00click-link1.001.001.000.990.940.591.001.001.001.00click-menu-2n/an/an/a0.830.960.520.160.520.960.83click-menu1.001.000.370.940.480.220.130.380.480.94click-option1.001.000.870.990.780.211.000.871.001.00click-pien/an/a0.510.970.920.151.000.511.000.97click-scroll-list1.001.000.000.600.590.010.070.010.590.60click-shades1.001.000.001.000.020.040.990.040.021.00click-shape0.980.980.530.950.500.110.640.540.500.95click-tab-2-easyn/an/an/a0.990.940.61n/a0.610.940.99click-tab-2-hard0.760.980.120.980.870.19n/a0.190.870.98click-tab-2-mediumn/an/an/a0.990.960.54n/a0.540.960.99click-tab-20.741.000.180.980.910.271.000.271.000.98click-tab1.001.000.741.001.000.951.001.001.001.00click-test-21.001.001.001.001.000.951.001.001.001.00click-test-transfern/an/an/a1.001.000.94n/a0.941.001.00click-test1.001.001.001.001.001.001.001.001.001.00click-widget0.980.981.001.001.000.561.001.001.001.00copy-paste-2n/an/an/a0.630.000.010.000.010.000.63copy-pasten/an/an/a0.790.000.040.000.040.000.79count-shape0.400.400.410.850.700.210.760.430.700.85count-sidesn/an/an/a1.001.000.740.300.741.001.00drag-boxn/an/an/a1.000.190.610.310.610.191.00drag-cuben/an/an/a0.790.950.230.180.230.950.79drag-itemn/an/an/a1.000.000.61n/a0.610.001.00drag-items-gridn/an/an/a0.980.000.050.010.050.000.98drag-itemsn/an/an/a0.990.000.130.410.130.000.99drag-shapesn/an/an/a0.990.230.260.920.260.230.99drag-sort-numbersn/an/an/a0.970.000.110.660.110.000.97email-inbox-deleten/an/an/a1.001.000.221.000.221.001.00email-inbox-forward-nl-turk0.940.940.331.000.000.00n/a0.330.001.00email-inbox-forward-nl1.001.000.601.000.000.00n/a0.600.001.00email-inbox-forwardn/an/an/a1.000.000.01n/a0.010.001.00email-inbox-importantn/an/an/a1.001.000.30n/a0.301.001.00email-inbox-nl-turk0.980.980.231.000.460.050.930.260.461.00email-inbox-noscrolln/an/an/a1.000.480.13n/a0.130.481.00email-inbox-replyn/an/an/a1.000.000.00n/a0.000.001.00email-inbox-star-replyn/an/an/a1.000.470.11n/a0.110.471.00email-inbox0.980.980.381.000.580.090.990.380.581.00enter-date0.960.960.001.001.000.021.000.021.001.00enter-password1.001.000.971.000.010.021.000.971.001.00enter-text-2n/an/an/a0.980.000.040.000.040.000.98enter-text-dynamic1.001.000.981.001.000.391.000.981.001.00enter-text1.001.000.891.001.000.351.000.991.001.00enter-time1.001.000.000.970.890.040.900.040.890.97find-midpointn/an/an/a0.970.970.350.310.350.970.97find-wordn/an/an/a0.880.000.050.000.050.000.88focus-text-21.001.001.001.001.000.961.001.001.001.00focus-text1.001.001.001.001.000.991.001.001.001.00grid-coordinate1.001.000.491.000.020.661.000.660.021.00guess-number0.200.200.001.000.210.200.210.001.00highlight-text-2n/an/an/a1.000.340.400.130.400.341.00highlight-textn/an/an/a1.001.000.510.900.511.001.00identify-shape0.761.000.881.001.000.681.000.891.001.00login-user-popup0.680.680.721.000.100.02n/a0.720.101.00login-user1.001.000.821.000.000.001.000.821.001.00moving-itemsn/an/an/a0.880.690.130.780.130.690.88multi-layouts0.720.960.831.000.000.001.000.830.001.00multi-orderings1.001.000.881.000.000.001.000.880.001.00navigate-tree0.861.000.910.990.940.321.000.991.000.99number-checkboxesn/an/an/a0.990.000.000.160.000.000.99read-table-2n/an/an/a0.940.000.000.000.000.000.94read-tablen/an/an/a0.970.000.010.000.010.000.97resize-textarean/an/an/a1.000.680.270.110.270.681.00right-anglen/an/an/a0.980.980.260.380.260.980.98scroll-text-2n/an/an/a1.001.000.880.960.881.001.00scroll-textn/an/an/a0.960.000.040.000.040.000.96search-engine1.001.000.341.000.010.151.000.341.001.00simon-saysn/an/an/a0.000.000.020.280.020.000.28
AcknowledgementThis material is based upon work supported by the National Science Foundation under Grant #2127309 to the Computing Research Association for the CIFellows 2021 Project.
Do as I can, not as I say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.016912022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Thomas Carta, Clment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.02662Grounding large language models in interactive environments with online reinforcement learning. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Why can GPT learn in-context? Language models secretly perform gradient descent as meta optimizers. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei, arXiv:2212.105592022arXiv preprint</p>
<p>Collaborating with language models for embodied reasoning. Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, Rob Fergus, Second Workshop on Language and Reinforcement Learning. 2022</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, arXiv:2207.10342Jascha Sohl-Dickstein, et al. Language model cascades. Rif A Saurous2022arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, International Conference on Learning Representations. 2020</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378PaLM-E: An embodied multimodal language model. 2023arXiv preprint</p>
<p>GLaM: Efficient scaling of language models with mixture-of-experts. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>Instruction-finetuned foundation models for multimodal web navigation. Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, Izzeddin Gur, Workshop on Reincarnating Reinforcement Learning at ICLR. 2023</p>
<p>The capacity for moral self-correction in large language models. Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil Lukoit, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, arXiv:2302.074592023arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Improving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nat Mcaleese, Maja Trbacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>Environment generation for zero-shot compositional reinforcement learning. Izzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, Aleksandra Faust, Advances in Neural Information Processing Systems. 202134</p>
<p>Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, arXiv:2210.03945Noah Fiedel, and Aleksandra Faust. Understanding HTML with large language models. 2022arXiv preprint</p>
<p>Learning to navigate the web. Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, Dilek Hakkani-Tur, International Conference on Learning Representations. 2019</p>
<p>An empirical analysis of compute-optimal large language model training. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, EMNLP. 2014</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 6th Annual Conference on Robot Learning. 2022</p>
<p>A data-driven approach for learning to control computers. David Peter C Humphreys, Tobias Raposo, Gregory Pohlen, Rachita Thornton, Alistair Chhaparia, Josh Muldal, Petko Abramson, Adam Georgiev, Timothy Santoro, Lillicrap, International Conference on Machine Learning. PMLR2022</p>
<p>Do BERTs learn to use browser user interface? Exploring multi-step tasks with unified vision-and-language berts. Taichi Iki, Akiko Aizawa, arXiv:2203.078282022arXiv preprint</p>
<p>DOM-Q-NET: Grounded RL on structured language. Sheng Jia, Jamie Ryan Kiros, Jimmy Ba, International Conference on Learning Representations. 2019</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of ACL. ACL2017</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang, International Conference on Learning Representations. 2018</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, International Conference on Learning Representations. 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. Aman Madaan, Amir Yazdanbakhsh, arXiv:2209.076862022arXiv preprint</p>
<p>Teaching language models to support answers with verified quotes. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, arXiv:2203.111472022arXiv preprint</p>
<p>Augmented language models: a survey. Grgoire Mialon, Roberto Dess, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozire, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>End-to-end goal-driven web navigation. Rodrigo Nogueira, Kyunghyun Cho, Advances in Neural Information Processing Systems. 292016</p>
<p>Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox, arXiv:2301.120502023arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Deep Learning for Code Workshop at ICLR. 2022</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>ART: Automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.090142023arXiv preprint</p>
<p>Mapping natural language commands to web elements. Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu, Percy Liang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Zero-shot entity extraction from web pages. Panupong Pasupat, Percy Liang, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational Linguistics20141</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of NAACL. NAACL2021</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.033502022arXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.033502022arXiv preprint</p>
<p>Planning with large language models via corrective re-prompting. Foundation Models for Decision Making workshop at NeurIPS. Vanya Shreyas Sundara Raman, Eric Cohen, Ifrah Rosen, David Idrees, Stefanie Paulius, Tellex, 2022</p>
<p>Jost Tobias Springenberg, et al. A generalist agent. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimnez, Yury Sulsky, Jackie Kay, Transactions on Machine Learning Research. 2022</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 2016EMNLP</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, International Conference on Learning Representations. 2022</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Dale Schuurmans, arXiv:2301.04589Memory augmented large language models are computationally universal. 2023arXiv preprint</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, International Conference on Machine Learning. PMLR2017</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>CLIPort: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2022</p>
<p>Using deepspeed and megatron to train megatron-turing NLG 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint</p>
<p>Learning webbased procedures by reasoning over explanations and demonstrations in context. Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic, Christopher Meek, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Recitation-augmented language models. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Eyvind Johannes Von Oswald, Ettore Niklasson, Joo Randazzo, Alexander Sacramento, Andrey Mordvintsev, Max Zhmoginov, Vladymyrov, arXiv:2212.07677Transformers learn in-context by gradient descent. 2022arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, arXiv:2211.000532022arXiv preprint</p>
<p>Chain of thought imitation with procedure cloning. Mengjiao Sherry, Yang , Dale Schuurmans, Pieter Abbeel, Ofir Nachum, Advances in Neural Information Processing Systems. 202235</p>
<p>Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, arXiv:2303.04129Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. 2023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik R Narasimhan, Advances in Neural Information Processing Systems. 2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations. 2023</p>
<p>STaR: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, International Conference on Learning Representations. 2023</p>
<p>Test-time prompt editing via reinforcement learning. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, Tem-Pera, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
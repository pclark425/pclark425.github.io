<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1611 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1611</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1611</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-218486872</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2005.00811v1.pdf" target="_blank">Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments. This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance. We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1611.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1611.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Evolve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolve-graph Commonsense Curriculum (KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-style strategy that incrementally reveals commonsense knowledge to an RL agent by only exposing ConceptNet relations for entities the agent has seen or interacted with so far, aiming to focus exploration and avoid overwhelming the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG agent (KG Evolve variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A text-based RL agent trained with Advantage Actor-Critic (A2C). Uses a hierarchical GRU encoder for observations and admissible actions, maintains a dynamic belief graph from observations, extracts a commonsense subgraph from ConceptNet (Numberbatch embeddings), merges belief and KG subgraphs into G_t, encodes G_t with stacked GCN layers, and selects actions via an MLP over concatenated [graph; state; actions].</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based interactive game environment modeled as a POMDP where the agent receives textual observations (token sequences) and issues text commands from a set of admissible actions; used to generate tasks like kitchen cleanup and cooking recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup (put objects in correct locations), cooking recipe retrieval (collect ingredient for recipe).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks decompose into sequences of primitive text actions (e.g., take object, open container, put object) that combine to complete multi-step goals (e.g., place multiple objects in their target locations); belief and commonsense graphs provide relational structure between entities.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>evolve-graph (incremental commonsense exposure)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Commonsense graph is not provided all at once; at each timestep the agent's commonsense subgraph contains only relations for entities observed or interacted with up to that timestep. The merged graph G_t thus grows as the agent encounters new entities, providing focused, incremental knowledge relevant to current exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>incremental exposure based on agent's observations / entities encountered (observation-driven ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Kitchen cleanup: up to 10 target objects + 5 distractors (multi-object tidy tasks); Cooking recipes: from single-ingredient (difficulty level 1, same room) to harder settings mentioned (difficulty level 10, 3 ingredients across 6 rooms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: On the Kitchen Cleanup task (averaged over 5 runs, 500 episodes per run) KG Evolve achieved higher average score and required fewer interactions than KG Full, Simple (text-only), and Random baselines (exact numeric scores not reported in text). On the Cooking Recipe tasks, Belief+KG Evolve outperformed Belief+KG Full, but GATA Full outperformed all in that specific setup (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: The KG Full (all commonsense knowledge given at start) variant performed worse than KG Evolve on kitchen cleanup, leading to noisier exploration and lower average performance; Simple (text-only) and Random baselines performed worse than knowledge-aware agents overall.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper compares KG Evolve vs KG Full (KG Evolve wins on kitchen cleanup). It also compares Belief+KG Evolve vs Belief+KG Full on cooking recipes (Evolve better than Full); additionally compares GATA Full/Evolve variants where GATA Full performed best on the simple cooking-recipe instances.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gradual, observation-driven exposure of commonsense knowledge (KG Evolve) improves exploration efficiency and task performance compared to exposing the full commonsense graph at once; exposing the full KG up front can overwhelm the agent and lead to noisy exploration. However, in some task regimes (e.g., cooking recipe tasks where ground-truth full belief graph is very informative), full belief information can outperform commonsense-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1611.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Full</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-graph Commonsense Baseline (KG Full)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline variant where the agent is provided the full commonsense subgraph (all extracted ConceptNet relations relevant to the game) at the beginning of each episode, rather than incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG agent (KG Full variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as the KG Evolve agent (A2C, hierarchical GRU encoders, belief graph, Numberbatch initial node embeddings, stacked GCN graph encoder, MLP action scorer), but with the commonsense graph provided in full at episode start.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based POMDP environment where observations are text descriptions and actions are textual commands drawn from an admissible list.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup (place up to 10 objects correctly), cooking recipe retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Multi-step procedures composed of primitive actions (take/open/put) and involving relational object-location knowledge; full graph provides many inter-object relations but not organized by task progression.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same as KG Evolve: kitchen tasks up to 10 objects + 5 distractors; cooking tasks from single-ingredient to harder multi-room settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: KG Full performed worse than KG Evolve on the Kitchen Cleanup task; providing the full commonsense graph up front tends to overwhelm the agent and leads to noisier exploration and lower performance (exact metrics not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared against KG Evolve (incremental) and Simple/Random baselines: KG Evolve outperforms KG Full on kitchen cleanup; for cooking recipes Belief+KG Full is outperformed by Evolve variants, while GATA Full can outperform both in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing the entire commonsense knowledge graph at once can be detrimental; agents may be overwhelmed by irrelevant relations and perform worse than when commonsense is fed incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1611.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief+KG Evolve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief Graph + Evolve-graph Commonsense (Belief+KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent variant that aggregates the dynamically constructed belief graph (from observations) with an evolve-graph commonsense subgraph that is fed incrementally as the agent encounters entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG agent (Belief+KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent combines a dynamically generated belief graph (state estimation from text) with an incremental commonsense subgraph from ConceptNet; both graphs are merged into G_t, encoded via Numberbatch embeddings and GCN layers, and used together with GRU-based state/action encodings for action selection under A2C training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A textual POMDP sandbox used to create kitchen cleanup and recipe tasks; the environment internally uses a belief graph as ground-truth state information.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks and cooking procedures</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup (put apple in refrigerator, plate in cabinet), cooking recipe retrieval (collect ingredient for recipe).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures decompose into ordered primitive actions; the belief graph captures observed state relations while the commonsense graph adds background relations to inform action choices.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>evolve-graph merged with belief graph</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Commonsense relations corresponding to entities observed/interacted so far are incrementally added and merged with the evolving belief graph; the merged graph G_t focuses the agent on concepts relevant to current subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>agent-observation-driven incremental exposure (entities seen/interacted)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Kitchen cleanup up to 10 objects + 5 distractors; cooking recipe tasks from single-ingredient (level 1) to more complex multi-room multi-ingredient setups (discussed but not fully evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: On kitchen cleanup, Belief+KG Evolve outperforms baselines (Simple, Random) and the Full variants in average score and efficiency (reported averaged over 5 runs). On cooking recipe tasks, Belief+KG Evolve performed better than Belief+KG Full but was outperformed by GATA Full in the evaluated (easy) recipe setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Belief+KG Full (no incremental feeding) performed worse than the evolve variant in tested settings; Simple baseline (text-only) performed worse than belief/knowledge graph methods.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper contrasts Belief+KG Evolve vs Belief+KG Full and reports Evolve is better for tested tasks; also compares to GATA Full/Evolve and Simple baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Merging incremental commonsense with a dynamically built belief graph helps concentrate exploration and improves performance versus providing all knowledge up-front; however, depending on task generation (e.g., simple single-ingredient cooking tasks), ground-truth full belief graphs (GATA Full) can still be superior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1611.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief+KG Full</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief Graph + Full Commonsense Graph (Belief+KG Full)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent variant that aggregates the dynamically generated belief graph with the complete commonsense subgraph extracted from ConceptNet provided at the start of the episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG agent (Belief+KG Full)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same core architecture as other Belief+KG variants (GRU encoders, dynamic belief graph, KG subgraph from ConceptNet encoded via Numberbatch and GCN), but with the commonsense graph fully revealed at episode start and merged with the belief graph.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based POMDP environment used to create kitchen and recipe tasks; interactions are text observations and tokenized textual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup, cooking recipe ingredient retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures are sequences of primitive actions; merging full KG yields many relations across entities which may not align with immediate subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same task ranges as other variants (10-object kitchen, recipe difficulties described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Generally underperformed the evolve variants on kitchen cleanup and some cooking recipe evaluations due to noisy exploration; in some cooking settings the ground-truth full belief graph (GATA Full) outperformed commonsense-based variants.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared to Belief+KG Evolve (incremental), Belief+KG Full showed inferior performance in many tested cases; compared also to GATA Full where GATA Full sometimes performed best.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full provision of commonsense knowledge combined with belief graph can be less effective than an incremental curriculum; too much pre-provided knowledge can cause noisy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1611.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior graph-based agent that learns a dynamic belief graph to represent the game state and uses it for planning and generalization in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA (Full / Evolve variants used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-based RL agent that constructs a belief graph from observations and uses it for action selection and generalization; in this paper GATA's belief graph is provided as either full (ground-truth) or evolved (observed) and used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual POMDP environment; GATA leverages belief graphs that correspond to state information used by TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural tasks in text games (household / cooking tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Cooking recipe tasks, environment navigation and object manipulation within TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Uses explicit belief-graph structure representing relations among entities to support planning across multi-step procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Used on cooking recipe tasks of varying difficulty; in this paper GATA Full performed best on level-1 recipe tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>GATA Full (ground-truth belief graph) significantly outperformed commonsense-augmented agents on the evaluated simple cooking-recipe games in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GATA Full provides strong performance when ground-truth belief information is available and tasks do not benefit from external commonsense; highlights that commonsense knowledge is not universally beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1611.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-only Simple Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline agent that chooses actions using only the textual observation encoding and ignores any commonsense knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Simple (text-only) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses hierarchical GRU encoders over observations and admissible actions and selects actions via A2C training without graph-based commonsense or belief graph augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual POMDP environment with textual observations and admissible textual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household / cooking tasks (textual decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup, cooking recipe retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Implicit in text encodings; does not maintain explicit compositional graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same as other agents (up to 10-object tasks, cooking recipe variations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Simple baseline underperformed knowledge-aware agents (KG variants and GATA) across evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using only textual information yields weaker performance than methods that incorporate belief graphs or commonsense knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1611.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1611.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Action Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that selects a random admissible action at each step, used to measure lower-bound performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Selects an admissible action uniformly at random each timestep; used as a naive lower-bound for performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual POMDP environment with admissible action lists at each timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household / cooking tasks (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Kitchen cleanup, cooking recipe retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same task specifications used in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Random baseline performed worst of compared agents on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a lower-bound; knowledge-aware methods substantially outperform random action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on textbased games. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Ledeepchef: Deep reinforcement learning agent for families of text-based games. <em>(Rating: 1)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1611",
    "paper_id": "paper-218486872",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "KG Evolve",
            "name_full": "Evolve-graph Commonsense Curriculum (KG Evolve)",
            "brief_description": "A curriculum-style strategy that incrementally reveals commonsense knowledge to an RL agent by only exposing ConceptNet relations for entities the agent has seen or interacted with so far, aiming to focus exploration and avoid overwhelming the agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG agent (KG Evolve variant)",
            "agent_description": "A text-based RL agent trained with Advantage Actor-Critic (A2C). Uses a hierarchical GRU encoder for observations and admissible actions, maintains a dynamic belief graph from observations, extracts a commonsense subgraph from ConceptNet (Numberbatch embeddings), merges belief and KG subgraphs into G_t, encodes G_t with stacked GCN layers, and selects actions via an MLP over concatenated [graph; state; actions].",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "A text-based interactive game environment modeled as a POMDP where the agent receives textual observations (token sequences) and issues text commands from a set of admissible actions; used to generate tasks like kitchen cleanup and cooking recipes.",
            "procedure_type": "commonsense procedures / household tasks",
            "procedure_examples": "Kitchen cleanup (put objects in correct locations), cooking recipe retrieval (collect ingredient for recipe).",
            "compositional_structure": "Tasks decompose into sequences of primitive text actions (e.g., take object, open container, put object) that combine to complete multi-step goals (e.g., place multiple objects in their target locations); belief and commonsense graphs provide relational structure between entities.",
            "uses_curriculum": true,
            "curriculum_name": "evolve-graph (incremental commonsense exposure)",
            "curriculum_description": "Commonsense graph is not provided all at once; at each timestep the agent's commonsense subgraph contains only relations for entities observed or interacted with up to that timestep. The merged graph G_t thus grows as the agent encounters new entities, providing focused, incremental knowledge relevant to current exploration.",
            "curriculum_ordering_principle": "incremental exposure based on agent's observations / entities encountered (observation-driven ordering)",
            "task_complexity_range": "Kitchen cleanup: up to 10 target objects + 5 distractors (multi-object tidy tasks); Cooking recipes: from single-ingredient (difficulty level 1, same room) to harder settings mentioned (difficulty level 10, 3 ingredients across 6 rooms).",
            "performance_with_curriculum": "Qualitative: On the Kitchen Cleanup task (averaged over 5 runs, 500 episodes per run) KG Evolve achieved higher average score and required fewer interactions than KG Full, Simple (text-only), and Random baselines (exact numeric scores not reported in text). On the Cooking Recipe tasks, Belief+KG Evolve outperformed Belief+KG Full, but GATA Full outperformed all in that specific setup (exact numbers not provided).",
            "performance_without_curriculum": "Qualitative: The KG Full (all commonsense knowledge given at start) variant performed worse than KG Evolve on kitchen cleanup, leading to noisier exploration and lower average performance; Simple (text-only) and Random baselines performed worse than knowledge-aware agents overall.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper compares KG Evolve vs KG Full (KG Evolve wins on kitchen cleanup). It also compares Belief+KG Evolve vs Belief+KG Full on cooking recipes (Evolve better than Full); additionally compares GATA Full/Evolve variants where GATA Full performed best on the simple cooking-recipe instances.",
            "transfer_generalization": null,
            "key_findings": "Gradual, observation-driven exposure of commonsense knowledge (KG Evolve) improves exploration efficiency and task performance compared to exposing the full commonsense graph at once; exposing the full KG up front can overwhelm the agent and lead to noisy exploration. However, in some task regimes (e.g., cooking recipe tasks where ground-truth full belief graph is very informative), full belief information can outperform commonsense-based approaches.",
            "uuid": "e1611.0",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "KG Full",
            "name_full": "Full-graph Commonsense Baseline (KG Full)",
            "brief_description": "Baseline variant where the agent is provided the full commonsense subgraph (all extracted ConceptNet relations relevant to the game) at the beginning of each episode, rather than incrementally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG agent (KG Full variant)",
            "agent_description": "Same architecture as the KG Evolve agent (A2C, hierarchical GRU encoders, belief graph, Numberbatch initial node embeddings, stacked GCN graph encoder, MLP action scorer), but with the commonsense graph provided in full at episode start.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "Text-based POMDP environment where observations are text descriptions and actions are textual commands drawn from an admissible list.",
            "procedure_type": "commonsense procedures / household tasks",
            "procedure_examples": "Kitchen cleanup (place up to 10 objects correctly), cooking recipe retrieval tasks.",
            "compositional_structure": "Multi-step procedures composed of primitive actions (take/open/put) and involving relational object-location knowledge; full graph provides many inter-object relations but not organized by task progression.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Same as KG Evolve: kitchen tasks up to 10 objects + 5 distractors; cooking tasks from single-ingredient to harder multi-room settings.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Qualitative: KG Full performed worse than KG Evolve on the Kitchen Cleanup task; providing the full commonsense graph up front tends to overwhelm the agent and leads to noisier exploration and lower performance (exact metrics not reported).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared against KG Evolve (incremental) and Simple/Random baselines: KG Evolve outperforms KG Full on kitchen cleanup; for cooking recipes Belief+KG Full is outperformed by Evolve variants, while GATA Full can outperform both in some settings.",
            "transfer_generalization": null,
            "key_findings": "Providing the entire commonsense knowledge graph at once can be detrimental; agents may be overwhelmed by irrelevant relations and perform worse than when commonsense is fed incrementally.",
            "uuid": "e1611.1",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Belief+KG Evolve",
            "name_full": "Belief Graph + Evolve-graph Commonsense (Belief+KG Evolve)",
            "brief_description": "Agent variant that aggregates the dynamically constructed belief graph (from observations) with an evolve-graph commonsense subgraph that is fed incrementally as the agent encounters entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG agent (Belief+KG Evolve)",
            "agent_description": "Agent combines a dynamically generated belief graph (state estimation from text) with an incremental commonsense subgraph from ConceptNet; both graphs are merged into G_t, encoded via Numberbatch embeddings and GCN layers, and used together with GRU-based state/action encodings for action selection under A2C training.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "A textual POMDP sandbox used to create kitchen cleanup and recipe tasks; the environment internally uses a belief graph as ground-truth state information.",
            "procedure_type": "commonsense procedures / household tasks and cooking procedures",
            "procedure_examples": "Kitchen cleanup (put apple in refrigerator, plate in cabinet), cooking recipe retrieval (collect ingredient for recipe).",
            "compositional_structure": "Procedures decompose into ordered primitive actions; the belief graph captures observed state relations while the commonsense graph adds background relations to inform action choices.",
            "uses_curriculum": true,
            "curriculum_name": "evolve-graph merged with belief graph",
            "curriculum_description": "Commonsense relations corresponding to entities observed/interacted so far are incrementally added and merged with the evolving belief graph; the merged graph G_t focuses the agent on concepts relevant to current subgoals.",
            "curriculum_ordering_principle": "agent-observation-driven incremental exposure (entities seen/interacted)",
            "task_complexity_range": "Kitchen cleanup up to 10 objects + 5 distractors; cooking recipe tasks from single-ingredient (level 1) to more complex multi-room multi-ingredient setups (discussed but not fully evaluated).",
            "performance_with_curriculum": "Qualitative: On kitchen cleanup, Belief+KG Evolve outperforms baselines (Simple, Random) and the Full variants in average score and efficiency (reported averaged over 5 runs). On cooking recipe tasks, Belief+KG Evolve performed better than Belief+KG Full but was outperformed by GATA Full in the evaluated (easy) recipe setting.",
            "performance_without_curriculum": "Belief+KG Full (no incremental feeding) performed worse than the evolve variant in tested settings; Simple baseline (text-only) performed worse than belief/knowledge graph methods.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper contrasts Belief+KG Evolve vs Belief+KG Full and reports Evolve is better for tested tasks; also compares to GATA Full/Evolve and Simple baseline.",
            "transfer_generalization": null,
            "key_findings": "Merging incremental commonsense with a dynamically built belief graph helps concentrate exploration and improves performance versus providing all knowledge up-front; however, depending on task generation (e.g., simple single-ingredient cooking tasks), ground-truth full belief graphs (GATA Full) can still be superior.",
            "uuid": "e1611.2",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Belief+KG Full",
            "name_full": "Belief Graph + Full Commonsense Graph (Belief+KG Full)",
            "brief_description": "Agent variant that aggregates the dynamically generated belief graph with the complete commonsense subgraph extracted from ConceptNet provided at the start of the episode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG agent (Belief+KG Full)",
            "agent_description": "Same core architecture as other Belief+KG variants (GRU encoders, dynamic belief graph, KG subgraph from ConceptNet encoded via Numberbatch and GCN), but with the commonsense graph fully revealed at episode start and merged with the belief graph.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "Text-based POMDP environment used to create kitchen and recipe tasks; interactions are text observations and tokenized textual actions.",
            "procedure_type": "commonsense procedures / household tasks",
            "procedure_examples": "Kitchen cleanup, cooking recipe ingredient retrieval.",
            "compositional_structure": "Procedures are sequences of primitive actions; merging full KG yields many relations across entities which may not align with immediate subgoals.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Same task ranges as other variants (10-object kitchen, recipe difficulties described in paper).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Qualitative: Generally underperformed the evolve variants on kitchen cleanup and some cooking recipe evaluations due to noisy exploration; in some cooking settings the ground-truth full belief graph (GATA Full) outperformed commonsense-based variants.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared to Belief+KG Evolve (incremental), Belief+KG Full showed inferior performance in many tested cases; compared also to GATA Full where GATA Full sometimes performed best.",
            "transfer_generalization": null,
            "key_findings": "Full provision of commonsense knowledge combined with belief graph can be less effective than an incremental curriculum; too much pre-provided knowledge can cause noisy exploration.",
            "uuid": "e1611.3",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "GATA",
            "name_full": "Graph Aided Transformer Agent (GATA)",
            "brief_description": "A prior graph-based agent that learns a dynamic belief graph to represent the game state and uses it for planning and generalization in text-based games.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "mention_or_use": "mention",
            "agent_name": "GATA (Full / Evolve variants used as baselines)",
            "agent_description": "Graph-based RL agent that constructs a belief graph from observations and uses it for action selection and generalization; in this paper GATA's belief graph is provided as either full (ground-truth) or evolved (observed) and used for comparison.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "Textual POMDP environment; GATA leverages belief graphs that correspond to state information used by TextWorld.",
            "procedure_type": "procedural tasks in text games (household / cooking tasks)",
            "procedure_examples": "Cooking recipe tasks, environment navigation and object manipulation within TextWorld.",
            "compositional_structure": "Uses explicit belief-graph structure representing relations among entities to support planning across multi-step procedures.",
            "uses_curriculum": null,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Used on cooking recipe tasks of varying difficulty; in this paper GATA Full performed best on level-1 recipe tasks.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "GATA Full (ground-truth belief graph) significantly outperformed commonsense-augmented agents on the evaluated simple cooking-recipe games in this paper.",
            "transfer_generalization": null,
            "key_findings": "GATA Full provides strong performance when ground-truth belief information is available and tasks do not benefit from external commonsense; highlights that commonsense knowledge is not universally beneficial.",
            "uuid": "e1611.4",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Simple baseline",
            "name_full": "Text-only Simple Baseline",
            "brief_description": "Baseline agent that chooses actions using only the textual observation encoding and ignores any commonsense knowledge graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Simple (text-only) agent",
            "agent_description": "Uses hierarchical GRU encoders over observations and admissible actions and selects actions via A2C training without graph-based commonsense or belief graph augmentation.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "Textual POMDP environment with textual observations and admissible textual actions.",
            "procedure_type": "household / cooking tasks (textual decision-making)",
            "procedure_examples": "Kitchen cleanup, cooking recipe retrieval.",
            "compositional_structure": "Implicit in text encodings; does not maintain explicit compositional graph structure.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Same as other agents (up to 10-object tasks, cooking recipe variations).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Qualitative: Simple baseline underperformed knowledge-aware agents (KG variants and GATA) across evaluated tasks.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Using only textual information yields weaker performance than methods that incorporate belief graphs or commonsense knowledge.",
            "uuid": "e1611.5",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Random baseline",
            "name_full": "Random Action Baseline",
            "brief_description": "A baseline agent that selects a random admissible action at each step, used to measure lower-bound performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Random agent",
            "agent_description": "Selects an admissible action uniformly at random each timestep; used as a naive lower-bound for performance comparisons.",
            "agent_size": null,
            "environment_name": "TextWorld",
            "environment_description": "Textual POMDP environment with admissible action lists at each timestep.",
            "procedure_type": "household / cooking tasks (baseline comparison)",
            "procedure_examples": "Kitchen cleanup, cooking recipe retrieval.",
            "compositional_structure": null,
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Same task specifications used in evaluations.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Qualitative: Random baseline performed worst of compared agents on evaluated tasks.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Serves as a lower-bound; knowledge-aware methods substantially outperform random action selection.",
            "uuid": "e1611.6",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Ledeepchef: Deep reinforcement learning agent for families of text-based games.",
            "rating": 1
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        }
    ],
    "cost": 0.014737999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
Pushkar Shukla pushkarshukla@ttic.edu 
Mrinmaya Sachan mrinmaya@ttic.edu 
Pavan Kapanipathi kapanipa@us.ibm.com 
Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020AD650EF8DCF26DE441D89D4F14C87F68arXiv:2005.00811v1[cs.AI]
In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments.This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance.We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
<p>Introduction</p>
<p>Over the years, simulation environments and games have been used extensively to showcase and drive advances in reinforcement learning technology.A recent environment that has received much focus is TextWorld (TW) (Ct et al., 2018), where an agent must interact with an external environment to achieve goals while maximizing reward -all of this using only the modality of text.TextWorld and similar text-based tasks seek to bring advances in natural language processing (NLP) and question answering solutions to agent-based reinforcement learning techniques, and vice-versa.</p>
<p>A common thread inherent in solutions to some of the NLP tasks is that mere text-based techniques cannot achieve or beat the human-level performance and that NLP systems must instead learn how to utilize additional knowledge from external sources such as knowledge bases (KBs) and knowledge graphs (KGs) to improve their overall performance.Figure 1 presents a running example that illustrates this: in the figure, the additional knowledge that must be utilized effectively by the * Both student authors contributed equally.</p>
<p>agent is presented in the bottom left corner under the ConceptNet heading.</p>
<p>In general, the use of external knowledge to improve the accuracy of NLP tasks has garnered significant attention from the community.Specifically, for tasks like natural language inference (NLI), recent work (Kapanipathi et al., 2020;Wang et al., 2019) has shown that while external knowledge can bring in useful information, this must be balanced by the context-specific relevance of the new information fed into the system.If this is not done properly, there is a very high risk of overwhelming the agent/algorithm with too much information, leading to poor decisions and performance.</p>
<p>In this paper, we present a novel approach to the use of external knowledge from the Concept-Net (Liu and Singh, 2004;Speer et al., 2017) knowledge graph to reduce the exploration space for a Reinforcement Learning (RL) agent.Specifically, we consider an RL based agent that is able to model the world around it at two levels -a local, or belief, graph that describes its current belief of the state of the world; and a global or commonsense graph of entities that are related to that state -and the interaction between those two levels.The belief graph provides the agent with a symbolic way to represent its current perception of the world, which can be easily combined with symbolic commonsense knowledge from the commonsense graph.This two-level representation of the world and the knowledge about it follows the model proposed in the Graph Aided Transformer Agent (GATA) (Adhikari et al., 2020) framework.</p>
<p>Using this model, we are able to show a significant improvement in the performance of an RL agent in a kitchen cleanup task that is set in the TextWorld setting.An example of such a kitchen cleanup task is shown in Figure 1: the agent is given an initial observation (which is used to pro-</p>
<p>Goal</p>
<p>Clean up the kitchen</p>
<p>ConceptNet</p>
<p>Apple</p>
<p>Refrigerator AtLocation</p>
<p>Plate</p>
<p>Cabinet AtLocation</p>
<p>Agent</p>
<p>Best action trajectory 1.Take the apple from the table 2. Take the plate from the table 3. Open the refrigerator 4. Put the apple in the refrigerator 5. Open the cabinet 6.Put the plate in the cabinet Plausible Actions 1. Open the cabinet 2. Eat the apple 3. Put the apple in the cabinet 4. ... duce the first iteration of the agent's belief graph), with the final goal of cleaning up the kitchen.The agent has to produce the list of actions that are necessary to achieve this goal: that list is given on the right hand side.Finally, the additional external knowledge from the ConceptNet knowledge graph -which makes up the global graph for our agent -is shown at the bottom left.In the case of this running example, the agent may discover from ConceptNet that apples are usually located in refrigerators, and plates are located in cabinets.We will use this kitchen cleanup instance as a running example throughout the paper.</p>
<p>By evaluating our approach on two different tasks -a kitchen cleanup task as above, and an additional cooking recipe task -we can show that the interaction between the belief and commonsense graphs can reduce the exploration of the RL agent in comparison to the purely text-based model.However, we are also able to demonstrate a more nuanced point: merely providing an agent with commonsense knowledge is not sufficient to improve its performance.Indeed, oftentimes it is detrimental to the agent's performance.We show that this is due to the agent being overwhelmed with too much commonsense knowledge, and discuss how different tasks and settings have different demands on the knowledge that is used by an agent.</p>
<p>Related Work</p>
<p>We start out with a look at work that is related to our focus area, which we categorize into three primary areas below.Our work sits at the intersection of knowledge graphs and the use of commonsense (and external) knowledge to make reinforcement learning more efficient; and our improvements are showcased in TextWorld and adjacent text-based domains.</p>
<p>Knowledge Graphs</p>
<p>Graphs have become a common way to represent knowledge.These knowledge graphs consist of a set of concepts (nodes) connected by relationships (edges).Well-known knowledge graphs (KGs) that are openly available include Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007), WordNet (Miller, 1995), and ConceptNet (Speer et al., 2017).Each of these KGs comprises different types of knowledge.For the tasks that our work considers, we found that the commonsense knowledge available in ConceptNet is more suitable than the encyclopedic knowledge from DBpedia or Freebase -we hence focus on this.Since our approach considers the KG as a generic graph structure, it is amenable to the use of any of the KGs mentioned here.</p>
<p>Knowledge graphs have been used to perform reasoning to improve performance in various domains, particularly within the NLP community.In particular, KGs have been leveraged for tasks such as Entity Linking (Hoffart et al., 2012), Question Answering (Sun et al., 2018;Das et al., 2017;Atzeni and Atzori, 2018), Sentiment Analysis (Recupero et al., 2015;Atzeni et al., 2018) and Natural Language Inference (Kapanipathi et al., 2020).Different techniques have been explored for their use.In most cases, knowledge graph embeddings such as TransH (Wang et al., 2014) and Com-plEx (Trouillon et al., 2016) are used to vectorize the concepts and relationships in a KG as input to a learning framework.Reinforcement learning has also been used to find relevant paths in a knowledge graph for knowledge base question answering (Das et al., 2017).Sun et al. (2018) and Kapanipathi et al. (2020) find sub-graphs from the corresponding KGs and encode them using a graph convolutional networks (Kipf and Welling, 2016) for question answering and natural language inference respectively.</p>
<p>External Knowledge for Sample Efficient Reinforcement Learning</p>
<p>A key challenge for current reinforcement learning (RL) technology is the low sample efficiency (Kaelbling et al., 1998).RL techniques require a large amount of interaction with the environment which can be very expensive.This has prevented the use of RL in real-world decision-making problems.In contrast, humans possess a wealth of commonsense knowledge which helps them solve problems in the face of incomplete information.Inspired by this, there have been a few recent attempts on adding prior or external knowledge to RL approaches.Notably, Garnelo et al. (2016) propose Deep Symbolic RL, which combines aspects of symbolic AI with neural networks and reinforcement learning as a way to introduce common sense priors.However, their work is mainly theoretical.There has also been some work on policy transfer (Bianchi et al., 2015), which studies how knowledge acquired in one environment can be re-used in another environment; and experience replay (Wang et al., 2016;Lin, 1992Lin, , 1993) ) which studies how an agent's previous experiences can be stored and then later reused.In contrast to the above, in this paper, we explore the use of commonsense knowledge stored in knowledge graphs such as ConceptNet as a way to improve sample efficiency in text-based RL agents.To the best of our knowledge, there is no prior work that explores how commonsense knowledge can be used to make RL agents more efficient.</p>
<p>RL Environments and TextWorld</p>
<p>Games are a rich domain for studying grounded language and how information from the text can be utilized in controlled applications.Notably, in this line of research, Branavan et al. ( 2012) builds an RL-based game player that utilizes text manuals to learn strategies for Civilization II; and Narasimhan et al. (2015) build an RL-based game player for multi-user Dungeon games.In both cases, the text is analyzed and control strategies are learned jointly using feedback from the gaming environment.Similarly, in the vision domain, there has been work on building automatic video game players (Koutnk et al., 2013;Mnih et al., 2016).</p>
<p>Our work builds on a recently introduced text-based game TextWorld (Ct et al., 2018).TextWorld is a sandbox learning environment for training and evaluating RL-based agents on textbased games.Since its introduction and other such tools, there has been a large body of work devoted to improving performance on this benchmark.One interesting line of work on TextWorld is on learning symbolic (typically graphical) representations of the agent's belief of the state of the world.Notably, Ammanabrolu and Riedl (2019) proposed KG-DQN and Adhikari et al. ( 2020) proposed GATA; both represent the game state as a belief graph learned during exploration.This graph is used to prune the action space, enabling more efficient exploration.Similar approaches for building dynamic belief graphs have also been explored in the context of machine comprehension of procedural text (Das et al., 2018).In our work, we also represent the world as a belief graph.Moreover, we also explore how the belief graph can be used with commonsense knowledge for efficient exploration.</p>
<p>The LeDeepChef system (Adolphs and Hofmann, 2019), which investigates the generalization capabilities of text-based RL agents as they learn to transfer their cooking skills to neverbefore-seen recipes in unfamiliar house environments, is also related to our work.They achieve transfer by additionally supervising the model with a list of the most common food items in Freebase, allowing their agent to generalize to hitherto unseen recipes and ingredients.</p>
<p>Finally, Zahavy et al. (2018) propose the Action-Elimination Deep Q-Network (AE-DQN) which learns to predict invalid actions in the textadventure game Zork, and eliminates them using contextual bandits.This allows the model to efficiently handle the large action space.The use of common sense knowledge in our work potentially has the same effect of down-weighting implausible actions.</p>
<p>TextWorld as a POMDP</p>
<p>Text-based games can be seen as partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state.As an agent interacts with a TextWorld game instance, at each turn, several lines of text describe the state of the game; and the player can issue a text command to change the state in some desirable way (typically, in order to move towards a goal).</p>
<p>Formally, let (S, T, A, , O, R, ) denote the underlying TextWorld POMDP.Here, S denotes the set of states, A denotes the action space, T denotes the state transition probabilities,  denotes the set of observations, O denotes the set of conditional observation probabilities, and   [0, 1] is the discount factor.The agent's observation o t at time step t depends on the current state s t and the previous action a t1 .The agent receives a reward at time step t: r t = R(s t , a t ) and the agent's goal is to maximize the expected discounted sum of rewards:
E[  t  t r t ].
TextWorld allows the agent to perceive and interact with the environment via the modality of text.Thus, the observation o t is presented by the environment as a sequence of tokens (o t = {o 1 t , . . .o N t }).Similarly, each action a is also denoted as a sequence of tokens {a 1 , . . ., a M }.</p>
<p>Model Description</p>
<p>In order to solve the above POMDP, we design a model that can leverage commonsense knowledge and learn a graph-structured representation of its belief of the world state.The high-level architecture of the model contains three major components, namely the input encoder, a graphbased knowledge extractor, and the action prediction module.The input encoding layers are used for encoding the observation at time step t and the list of admissible actions.The graph-based knowledge extractor tries to extract knowledge from two different sources.First, it makes use of external commonsense knowledge to improve the ability of the agent to select the correct action at each time step.Secondly, the belief about the environment (world state) perceived by the agent is also captured by a belief graph that is generated dynamically from the textual observations from the game.The information from both sources is then aggregated together in a single graph.The actionprediction module takes as input the encoded ob-servation states, the encoded list of admissible actions and the encoded aggregated graph, and predicts an action for each step.Figure 2 provides a compact visualization of our approach.We describe the various components of our model below.</p>
<p>Input Encoder</p>
<p>At any time step t, the agent observes a textual description of the current state provided as a sequence of tokens o t = (o 1 t , . . ., o N t ).Given the current observation o t , we use pre-trained GloVe embeddings (Pennington et al., 2014)
i = (a 1 i , . . . , a M i )  A t as a sequence of d-dimensional pretrained GloVe embeddings c 1 i , . . . , c M i .
The agent relies on a hierarchical encoder architecture to model the current state as a vector s t , based on o t and the previous observations.First, a GRU-based encoder is used to process the sequence x 1 t , . . ., x N t of the GloVe embeddings associated with o t .This allows representing the current observation as a single h-dimensional vector o t  R h , where h is the output dimensionality of the GRU.Formally, o t is computed as o t = h N t , with h k t = GRU(h k1 t , x k t ), for k = 1, . . ., N. In the previous equation, GRU() refers to the forward propagation of a gated recurrent unit (Cho et al., 2014).Then, the sequence of previous observations up to o t is encoded in a similar way into a vector s t = GRU(s t1 , o t )  R h .We do the same to represent each admissible action a i as
a i = a M i , with a k i = GRU(a k1 i , c k i ), for k = 1, . . . , M.</p>
<p>Graph-based Knowledge Integration</p>
<p>We enhance our text-based RL agent by allowing it to access a graph that captures both commonsense knowledge and the agent's current belief of the world state.Formally, we assume that, at each time step t, the agent has access to a graph G t = (V t , E t ), where V t is the set of nodes and E t  V 2 t denotes the edges of the graph.The graph is updated dynamically at each time step t and new nodes are either added or deleted based on the textual observation o t .</p>
<p>As mentioned, G t encodes both commonsense knowledge and the belief of the world state.Commonsense knowledge is extracted from the history of the observations by linking the entities mentioned in the text to an external KG.This allows extracting a commonsense knowledge graph, which is a subgraph of the external source of knowledge providing information about the entities of interest.In our experiments, we use Con-ceptNet (Speer et al., 2017) as the external knowledge graph.On the other hand, the observation o t is also used to update a dynamically generated belief graph as in recent work by Adhikari et al. (2020).The graph aggregation is performed by merging the belief and commonsense knowledge graphs based on the entity mentions.This helps to reduce the noise extracted from updating both the belief and the commonsense graphs.As shown in Figure 2, the commonsense knowledge graph, and the belief graph are updated based on the observations, and then they are aggregated to form a single graph G t .The graph G t at time step t is processed by a graph encoder as follows.First, pretrained KG embeddings are used to map the set of nodes V t into a feature matrix
E t = [e 1 t , . . . , e |V t | t ]  R f |V t | ,
where each column e i t  R f is the embedding of node i  V t .We use Numberbatch embeddings (Speer et al., 2017) to create the matrix E t .Such feature matrix provides initial node embeddings that are iteratively updated by message passing between the nodes of G t , using L stacked GCN (graph convolutional network) layers (Kipf and Welling, 2016), where L is an hyperparameter of the model.The output of this process is an updated matrix
Z t = [z 1 t , . . . , z |V | t ]  R h|V t | .
We then compute a graph encoding g t for G t by simply averaging over the columns of Z t , namely:
g t = 1 |V t | |V t |  i=1 z i t .
In our experiments, we use the updated KG embeddings to create a graph-based encoding vector for each action as described in Section 4.1, in addition to the graph encoding g t .This approach has shown to be a better integration of the knowledge graph at each time step.</p>
<p>Action Prediction</p>
<p>The action t selected by the agent at time step t is computed based on the state embedding s t , the graph embedding g t and the action candidates a 1 , . . ., a |A t | .First, all these vectors are concatenated together into a single vector r t = [g t ; s t ; a 1 ; . . .; a |A t | ].Then, we compute a vector p t  R |A t | with a probability score for each action a i  A t as:
p t = so f tmax(W 1  ReLU(W 2  r t + b 2 ) + b 1 )
where W 1 ,W 2 , b 1 , and b 2 are learnable parameters of the model.The final action chosen by the agent is then given by the one with the maximum probability score, namely t = arg max i p t,i .</p>
<p>Learning</p>
<p>Following the winning strategy in the First-TextWorld competition (Adolphs and Hofmann, 2019), we use the Advantage Actor-Critic (A2C) framework (Mnih et al., 2016) to train the agent and optimize the action selector on reward signals from training games.</p>
<p>Experiments</p>
<p>In this section, we report on experiments to study the role of commonsense knowledge-based RL agents in the TextWorld environment.We evaluate and compare our agent on two sets of game instances: 1) Kitchen Cleanup Task, and 2) Cooking Recipe Task.</p>
<p>Kitchen Cleanup Task</p>
<p>First, we use TextWorld (Ct et al., 2018) to generate a game/task to assess the performance gain using commonsense knowledge graphs such as ConceptNet.We generate the game with 10 objects relevant to the game, and 5 distractor objects spread across the room.The goal of the agent is to tidy the room (kitchen) by putting the objects in the right place.We create a set of realistic kitchen cleanup goals for the agent: for instance, take apple from the table and put apple inside the refrigerator.Since information on concepts that map to the objects in the room is explicitly provided in ConceptNet (Apple  AtLocation  Refrigerator), the main hypothesis underlying the creation of this game is that leveraging the commonsense knowledge could allow the agent to achieve a higher reward while reducing the number of interactions with the environment.</p>
<p>The agent is presented with the textual description of a kitchen, consisting of the location of different objects in the kitchen and their spatial relationship to the other objects.The agent uses this information to select the next action to perform in the environment.Whenever the agent takes an object and puts it in the target location, it receives a reward and its total score goes up by one point.The maximum score that can be achieved by the agent in this kitchen cleanup task is equal to 10.In addition to the textual description, we extract the commonsense knowledge graph from ConceptNet based on the text description.Figure 3 shows an instance of the commonsense knowledge graph created during the agent's interaction with the environment.Note that even for the simple kitchen cleanup task that we model (see Figure 1 for details), the commonsense knowledge graph contains more than 20 entities (nodes) and a similar number of relations (edges).This visualization is useful, as it lends a basis for our upcoming discussion on agents being overwhelmed with too much commonsense knowledge.</p>
<p>Results on Kitchen Cleanup</p>
<p>We compare our knowledge-aware RL agents (KG Full and KG Evolve) against two baselines for performance comparison: Random, where the agent chooses an action randomly at each step; and Simple, where the agent chooses the next action using the text description alone and ignores the commonsense knowledge graph.The knowledgeaware RL agents, on the other hand, use the commonsense knowledge graph to choose the next action.The graph is provided in either full-graph setting where all the commonsense relationships between the objects are given at the beginning of the game (KG Full); or evolve-graph setting where only the commonsense relationship between the objects seen/interacted by the agent until the current steps are revealed (KG Evolve).We record the average score achieved by each agent and the average number of interactions (moves) with the environment as our evaluation metrics.Figure 4 shows the results for the kitchen cleanup task averaged over 5 runs, with 500 episodes per run.</p>
<p>Discussion of Kitchen Cleanup</p>
<p>As expected, we see that agents that use the textual description and additionally the commonsense knowledge outperform the baseline random agent.We are also able to demonstrate clearly that the knowledge-aware agent outperforms the simple agent with the help of commonsense knowledge.The knowledge-aware agent with the evolve-graph setting outperforms both the simple agent as well as the agent with the full-graph setting.We believe that when an agent has access to the full commonsense knowledge graph at the beginning of the game, the agent gets overwhelmed by the amount of knowledge given; and is prone to making noisy explorations in the environment.On the other hand, feeding the commonsense knowledge gradually during the agent's learning process provides more focus to the exploration, and drives it toward the concepts related to the rest of the goals.These results can also be seen as an RL-centric agent-based validation of similar results shown in the broader NLP literature by the work of (Kapanipathi et al., 2020).</p>
<p>Cooking Recipe Task</p>
<p>Next, we evaluate the performance of our agent on the cooking recipe task by using 20 different games generated by (Adhikari et al., 2020).These games follow a recipe-based cooking theme, with a single ingredient in a single room (difficulty level 1).The goal is to collect that specific ingredient to prepare a meal from a given recipe.As in our previous task, we compare our agent with the Simple agent.In addition to the simple agent, we compare our agent with the GATA agent (Adhikari et al., 2020) which uses the belief graph for effective planning and generalization.As used throughout this paper, the belief graph represents the state of the current game based on the textual description from the environment.Similar to commonsense knowledge, the belief graph can be fed to the agent as a full-graph (GATA Full) or an evolve-graph (GATA Evolve) and then aggregated as the current graph.It is worth noting that the full belief graph is considered as the ground truth state information in the TextWorld environment: it is the graph used by the TextWorld environment internally to modify the state information and the list of admissible actions.On the other hand, the evolve-belief graph is generated based on the observed state information.</p>
<p>Results on Cooking Recipe</p>
<p>We compare both the simple and GATA agents against our agent which uses the commonsense knowledge extracted from ConceptNet.As before, we consider a full-graph setting and evolvegraph setting where either the full commonsense knowledge graph is available at the beginning of the game, or it is fed incrementally as the game proceeds, respectively.For this task, we aggregate the commonsense knowledge graph with the belief graph (Belief+KG Full and Belief+KG Evolve).</p>
<p>Figure 5 shows the results for the Cooking recipe task averaged over 5 runs and 20 games, with 100 episodes per run.As before, all the agents outperform the simple agent, which shows that using different state representations such as the belief graph and additional information such as commonsense knowledge improves the performance of an agent.</p>
<p>Discussion of Cooking Recipe</p>
<p>We observe that the evolve-graph setting for both the GATA and Belief+KG performs better than the Belief+KG Full, as feeding more information can lead to noisy exploration as observed in the earlier task.More interestingly, we observe that GATA Full performs significantly better than the other agents.We believe that the reason for this result is the difficulty of the task at hand, and the process via which these cooking games are generated.Since the cooking recipe task (difficulty level 1) entails retrieving a single ingredient from the same room that the agent is present in, there are no meaningful concepts related to the current state that can be leveraged from the commonsense knowledge for better exploration.Even with the difficult task setting in this game environment (dif-ficulty level 10 with 3 ingredients spread across 6 rooms), the ingredients are randomly chosen and spread across the rooms.In such a game setting, the ground truth full belief graph is more beneficial than the commonsense knowledge graph.This is an interesting negative result, in that it shows that there can still be scenarios and domains where commonsense knowledge may not necessarily help an agent.We are actively exploring further settings of the cooking recipe task in order to understand and frame this effect better.</p>
<p>Conclusion</p>
<p>Previous approaches for text-based games like TextWorld primarily focused on text understanding and reinforcement learning for learning control policies and were thus sample inefficient.In contrast, humans utilize their commonsense knowledge to efficiently act in the world.As a step towards bridging this gap, we investigated the novel problem of using commonsense knowledge to build efficient RL agents for text-based games.</p>
<p>We proposed a technique that symbolically represents the agent's belief of the world, and then combines that belief with commonsense knowledge from the ConceptNet knowledge graph in order to act in the world.We evaluated our approach on multiple tasks and environments and showed that commonsense knowledge can help the agent act efficiently and accurately.We also showcased some interesting negative results with respect to agents being overwhelmed with too much commonsense knowledge.We are currently actively studying this problem, and future work will report in more detail on this phenomenon.</p>
<p>Figure 1 :
1
Figure 1: An illustration of our Kitchen Cleanup game.The agent perceives the world via text and has been given the goal of cleaning up the kitchen.As shown here, the agent can leverage commonsense knowledge from ConceptNet to reduce the exploration and achieve the goal.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Example of the commonsense knowledge graph extracted from ConceptNet for the Kitchen Cleanup Task</p>
<p>Figure 5 :
5
Figure 5: Comparison of agents for the Cooking Recipe task with belief graph and/or commonsense knowledge graph (averaged over 5 runs).</p>
<p>Observation</p>
<p>You've entered a kitchen.You see a closed cabinet and a refrigerator.Here's a dining table.You see a plate and an apple on the table.</p>
<p>to represent o t as a sequence of d-dimensional vectors x 1 t , . . ., x N t , where each x k t  R d is the glove embedding of the k-th observed token o k t , k = 1, . . ., N. Similarly, given the set A t of admissible actions at time step t, we represent each action a</p>
<p>AcknowledgmentsWe thank Sadhana Kumaravel, Gerald Tesauro, and Murray Campbell for their feedback and help with this work.
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L Hamilton, arXiv:2002.09127Learning dynamic knowledge graphs to generalize on textbased games. 2020arXiv preprint</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, ArXiv, abs/1909.016462019</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>What is the cube root of 27? Question answering over codeontology. Mattia Atzeni, Maurizio Atzori, The Semantic Web -ISWC 2018 -17th International Semantic Web Conference. Lecture Notes in Computer Science. Springer2018</p>
<p>Using frame-based resources for sentiment analysis within the financial domain. Mattia Atzeni, Amna Dridi, Diego Reforgiato Recupero, Progress in AI. 742018</p>
<p>Dbpedia: A nucleus for a web of open data. Sren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. 2007</p>
<p>Transferring knowledge as heuristics in reinforcement learning: A case-based approach. Reinaldo Ac Bianchi, Luiz A CelibertoJr, Paulo E Santos, Jackson P Matsuura, Ramon Lopez De Mantaras, Artificial Intelligence. 2262015</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of dataAcM2008</p>
<p>Learning to win by reading manuals in a monte-carlo framework. David Srk Branavan, Regina Silver, Barzilay, Journal of Artificial Intelligence Research. 432012</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, C aglar Glc ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/d14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. Springer2018</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew Mccallum, arXiv:1711.058512017arXiv preprint</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew Mccallum, CoRR, abs/1810.056822018</p>
<p>Marta Garnelo, Kai Arulkumaran, Murray Shanahan, arXiv:1609.05518Towards deep symbolic reinforcement learning. 2016arXiv preprint</p>
<p>Kore: keyphrase overlap relatedness for entity disambiguation. Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, Gerhard Weikum, Proceedings of the 21st ACM international conference on Information and knowledge management. the 21st ACM international conference on Information and knowledge management2012</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Infusing knowledge into the textual entailment task using graph convolutional networks. Pavan Kapanipathi, Veronika Thost, Sankalp Siva, Spencer Patel, Ibrahim Whitehead, Avinash Abdelaziz, Maria Balakrishnan, Kshitij Chang, Chulaka Fadnis, Bassem Gunasekara, Nicholas Makni, Mattei, 2020AAAIKartik Talamadupula, and Achille Fokoue</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Evolving largescale neural networks for vision-based reinforcement learning. Jan Koutnk, Giuseppe Cuccu, Jrgen Schmidhuber, Faustino Gomez, Proceedings of the 15th annual conference on Genetic and evolutionary computation. the 15th annual conference on Genetic and evolutionary computation2013</p>
<p>Self-improving reactive agents based on reinforcement learning, planning and teaching. Long-Ji Lin, Machine learning. 83-41992</p>
<p>Reinforcement learning for robots using neural networks. Long-Ji Lin, 1993Carnegie-Mellon Univ Pittsburgh PA School of Computer ScienceTechnical report</p>
<p>Conceptnet-a practical commonsense reasoning tool-kit. Hugo Liu, Push Singh, BT technology journal. 2242004</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. 2016</p>
<p>Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941Language understanding for textbased games using deep reinforcement learning. 2015arXiv preprint</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, 10.3115/v1/d14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Sentilo: Frame-based sentiment analysis. Diego Reforgiato Recupero, Valentina Presutti, Sergio Consoli, Aldo Gangemi, Andrea Giovanni Nuzzolese, 10.1007/s12559-014-9302-zCognitive Computation. 722015</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robert Speer, Joshua Chin, Catherine Havasi, AAAI. 2017</p>
<p>Open domain question answering using early fusion of knowledge bases and text. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018Ruslan Salakhutdinov, and William Cohen</p>
<p>Complex embeddings for simple link prediction. Tho Trouillon, Johannes Welbl, Sebastian Riedel, ric Gaussier, Guillaume Bouchard, International Conference on Machine Learning. 2016</p>
<p>Improving natural language inference using external knowledge in the science questions domain. Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Knowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, AAAI. 2014</p>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, arXiv:1611.01224Sample efficient actorcritic with experience replay. 2016arXiv preprint</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in Neural Information Processing Systems. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5350 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5350</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5350</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-062fab31d30478b57457c8b7a94d7467f5bd770c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/062fab31d30478b57457c8b7a94d7467f5bd770c" target="_blank">GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Big Data</p>
                <p><strong>Paper TL;DR:</strong> GraphLLM is introduced, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs and equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power ofgraph learning models.</p>
                <p><strong>Paper Abstract:</strong> The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5350.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5350.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Text (Graph2Text) conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that serialize graph data into natural-language descriptions so that a pre-trained LLM can directly process graph instances as text; commonly used in prompting and fine-tuning baselines for LLM-based graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph2Text (natural-language serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert a graph into a natural-language textual description that includes (a) per-node textual features/descriptions and (b) an explicit textual encoding of graph structure (e.g., adjacency list or edge list). In this paper node descriptions (≈34–82 tokens each) are generated/used verbatim and graph edges are expressed in sentence form such as "Atom 1 is connected with: Atom 2, Atom 3,..." (adjacency list) or an explicit list of edges (edge list). The paper's dataset examples and prompts (Appendix D) follow this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>various: molecular graphs (substructure counting), social/friendship graphs (maximum triplet sum), synthetic wormhole graphs with node costs (shortest path), bipartite applicant-job graphs (matching)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Sequential textual encoding of nodes+edges; high contextual length (scales linearly with node count and tokenized node descriptions); implicitly encodes structure (LLM must infer graph topology from sequence); interpretable to humans; does not require auxiliary graph model; prone to long-context inefficiency and possible information extraction difficulty for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Four fundamental graph reasoning tasks used in the paper: (1) Substructure counting (molecular triangles), (2) Maximum triplet sum (friendship graphs), (3) Shortest path with node activation costs (wormhole graphs), (4) Maximum bipartite matching (applicant-job). Graph2Text variants used both for prompting (zero-shot, few-shot, chain-of-thought) and for fine-tuning/adaptation (LoRA, prefix tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact Match Accuracy (EM). Reported performance for Graph2Text-based baselines (LLaMA2-7B/13B) varies by method and task: many prompting methods achieve very low accuracies (often <0.3); best Graph2Text fine-tuning variants reach roughly ~0.5–0.8 on some tasks but are inconsistent across tasks. The paper reports an average EM across the four tasks for the top-performing Graph2Text method of ~47.35% (vs GraphLLM 98.19%). Specific examples (LLaMA2-7B, Edge list / random order): LoRA(attn) results per Table 2 roughly: Substructure ~0.50, MaxTriplet ~0.42, ShortestPath ~0.20, Bipartite ~0.65 (numbers are per-task means ± s.d.). Additional tests with gpt-3.5/gpt-4 on 30 samples (Table 3) show variable and often inadequate performance (e.g., gpt-4 few-shot CoT: Substructure 0.5, MaxTriplet 0.9333, ShortestPath 0.8667, Bipartite 0.8667 on the 30-sample subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against GraphLLM (the paper's proposed alternative that encodes graph information into a fixed-size prefix via a graph transformer), Graph2Text performs substantially worse: GraphLLM achieves mean EM 98.19% vs top Graph2Text ~47.35% across four tasks. The paper also compares two Graph2Text structural encodings (adjacency list vs edge list) and finds no consistent advantage of one over the other. Graph2Text methods degrade strongly as graph size increases because context length grows and can exceed the model tokenizer limit (examples in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Long context tokens (node text + structure) leading to inefficiency and context-limit OOL (out-of-length) failures as graphs grow; LLMs must infer relational/topological structure from sequential text which is harder than operating on native graph representations; inconsistent performance across tasks and representation variants; even strong LLMs (gpt-4) can fail on these graph reasoning tasks when using Graph2Text; fine-tuning Graph2Text corpora (prior work) has not reliably elicited graph reasoning without specialized graph modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphLLM: Boosting Graph Reasoning Ability of Large Language Model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5350.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5350.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdjacencyList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph2Text via adjacency-list natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete Graph2Text encoding that lists, for each node, the node's textual description followed by a natural-language line enumerating its neighbors ("Node i is connected with: Node j, Node k, ...").</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency-list Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node include its textual description and then an adjacency sentence listing its neighbors (e.g., "Atom 1 is connected with: Atom 2, Atom 3,..."). Node descriptions are tokenized with the LLM tokenizer and appended; the full adjacency-list text is passed as the LLM input prompt or training example.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>same mixed set as above: molecular graphs, friendship graphs, wormhole graphs, bipartite graphs (used by the four tasks in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Human-readable, structured per-node; yields long sequential text proportional to (#nodes × avg tokens per node) + (#edges encoded redundantly via adjacency lists); sensitive to node ordering; tends to produce larger contexts than edge-list encodings because neighbors are repeated in multiple adjacency lines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as the Graph2Text input format for all four graph reasoning tasks and evaluated under zero-shot/few-shot prompting and fine-tuning (LoRA, prefix tuning) baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact Match Accuracy (EM). Example numbers from Table 2 (LLaMA2-7B, adjacency list): Zero-shot EMs often <0.4; LoRA and prefix tuning have task-dependent EMs (e.g., LoRA(attn) Substructure ~0.50, MaxTriplet ~0.44, ShortestPath ~0.21, Bipartite ~0.74). Overall inferior to GraphLLM (GraphLLM achieves ~0.999/0.958/0.973/0.998 on the four tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly with edge-list Graph2Text and found no consistent advantage; adjacency-list Graph2Text yields similar poor scaling behavior and performance as edge-list when used with prompting or fine-tuning. Both are far worse than GraphLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High redundancy (edges appear multiple times), long token sequences (see Table 4: adjacency-list Graph2Text average contexts per task range ~1.2K–3.3K tokens depending on method), sensitive to graph size and node count, leads to LLM context overflow for moderately larger graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphLLM: Boosting Graph Reasoning Ability of Large Language Model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5350.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5350.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EdgeList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph2Text via edge-list natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Graph2Text encoding that represents graph structure as an explicit list of edges in natural language (e.g., "Atom 1 - Atom 2", or lines saying which nodes are connected), sometimes presented in random order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge-list Graph2Text (random order variant)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode graph structure via a sequence of edge statements (an explicit listing of node pairs forming edges), combined with per-node textual descriptions elsewhere in the prompt. The paper evaluates a random-order edge list variant as one of two Graph2Text structural encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graphs, friendship graphs, wormhole graphs, bipartite graphs (same four tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Potentially more compact than adjacency lists (edges listed once), order-sensitive (random order used in experiments), still grows with number of edges and node descriptions, requires LLM to reconstruct topology from sequential edge records.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Same four graph reasoning tasks; used in prompting (zero/few-shot/CoT) and fine-tuning baselines (LoRA, prefix tuning) reported in Tables 2 and 4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact Match Accuracy (EM). Example numbers (Table 2, LLaMA2-7B, Edge list random order): Zero-shot and few-shot EMs remain low; LoRA(attn) Substructure ~0.50, MaxTriplet ~0.42, ShortestPath ~0.20, Bipartite ~0.64. Performance trends similar to adjacency-list; no consistent improvement over adjacency-list.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Edge-list performs comparably to adjacency-list in these experiments; neither structural encoding solved the core weaknesses of Graph2Text. Both are outperformed by GraphLLM by large margins. The paper explicitly states that choice of adjacency vs edge-list "doesn't lead to a consistent enhancement" in Graph2Text performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still produces long contexts once node descriptions are included (Table 4 shows average token lengths for Graph2Text edge-list similar to adjacency-list); LLM must infer graph topology from a flat sequence; random edge order induces variability; catastrophic scaling as graph size increases (performance drops and/or context limit exceeded).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphLLM: Boosting Graph Reasoning Ability of Large Language Model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data ? <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5350",
    "paper_id": "paper-062fab31d30478b57457c8b7a94d7467f5bd770c",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Graph2Text",
            "name_full": "Graph-to-Text (Graph2Text) conversion",
            "brief_description": "A family of methods that serialize graph data into natural-language descriptions so that a pre-trained LLM can directly process graph instances as text; commonly used in prompting and fine-tuning baselines for LLM-based graph reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Graph2Text (natural-language serialization)",
            "representation_description": "Convert a graph into a natural-language textual description that includes (a) per-node textual features/descriptions and (b) an explicit textual encoding of graph structure (e.g., adjacency list or edge list). In this paper node descriptions (≈34–82 tokens each) are generated/used verbatim and graph edges are expressed in sentence form such as \"Atom 1 is connected with: Atom 2, Atom 3,...\" (adjacency list) or an explicit list of edges (edge list). The paper's dataset examples and prompts (Appendix D) follow this approach.",
            "graph_type": "various: molecular graphs (substructure counting), social/friendship graphs (maximum triplet sum), synthetic wormhole graphs with node costs (shortest path), bipartite applicant-job graphs (matching)",
            "representation_properties": "Sequential textual encoding of nodes+edges; high contextual length (scales linearly with node count and tokenized node descriptions); implicitly encodes structure (LLM must infer graph topology from sequence); interpretable to humans; does not require auxiliary graph model; prone to long-context inefficiency and possible information extraction difficulty for LLMs.",
            "evaluation_task": "Four fundamental graph reasoning tasks used in the paper: (1) Substructure counting (molecular triangles), (2) Maximum triplet sum (friendship graphs), (3) Shortest path with node activation costs (wormhole graphs), (4) Maximum bipartite matching (applicant-job). Graph2Text variants used both for prompting (zero-shot, few-shot, chain-of-thought) and for fine-tuning/adaptation (LoRA, prefix tuning).",
            "performance_metrics": "Exact Match Accuracy (EM). Reported performance for Graph2Text-based baselines (LLaMA2-7B/13B) varies by method and task: many prompting methods achieve very low accuracies (often &lt;0.3); best Graph2Text fine-tuning variants reach roughly ~0.5–0.8 on some tasks but are inconsistent across tasks. The paper reports an average EM across the four tasks for the top-performing Graph2Text method of ~47.35% (vs GraphLLM 98.19%). Specific examples (LLaMA2-7B, Edge list / random order): LoRA(attn) results per Table 2 roughly: Substructure ~0.50, MaxTriplet ~0.42, ShortestPath ~0.20, Bipartite ~0.65 (numbers are per-task means ± s.d.). Additional tests with gpt-3.5/gpt-4 on 30 samples (Table 3) show variable and often inadequate performance (e.g., gpt-4 few-shot CoT: Substructure 0.5, MaxTriplet 0.9333, ShortestPath 0.8667, Bipartite 0.8667 on the 30-sample subset).",
            "comparison_to_other_representations": "Compared against GraphLLM (the paper's proposed alternative that encodes graph information into a fixed-size prefix via a graph transformer), Graph2Text performs substantially worse: GraphLLM achieves mean EM 98.19% vs top Graph2Text ~47.35% across four tasks. The paper also compares two Graph2Text structural encodings (adjacency list vs edge list) and finds no consistent advantage of one over the other. Graph2Text methods degrade strongly as graph size increases because context length grows and can exceed the model tokenizer limit (examples in Figure 4).",
            "limitations_or_challenges": "Long context tokens (node text + structure) leading to inefficiency and context-limit OOL (out-of-length) failures as graphs grow; LLMs must infer relational/topological structure from sequential text which is harder than operating on native graph representations; inconsistent performance across tasks and representation variants; even strong LLMs (gpt-4) can fail on these graph reasoning tasks when using Graph2Text; fine-tuning Graph2Text corpora (prior work) has not reliably elicited graph reasoning without specialized graph modules.",
            "uuid": "e5350.0",
            "source_info": {
                "paper_title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AdjacencyList",
            "name_full": "Graph2Text via adjacency-list natural-language serialization",
            "brief_description": "A concrete Graph2Text encoding that lists, for each node, the node's textual description followed by a natural-language line enumerating its neighbors (\"Node i is connected with: Node j, Node k, ...\").",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Adjacency-list Graph2Text",
            "representation_description": "For each node include its textual description and then an adjacency sentence listing its neighbors (e.g., \"Atom 1 is connected with: Atom 2, Atom 3,...\"). Node descriptions are tokenized with the LLM tokenizer and appended; the full adjacency-list text is passed as the LLM input prompt or training example.",
            "graph_type": "same mixed set as above: molecular graphs, friendship graphs, wormhole graphs, bipartite graphs (used by the four tasks in the paper)",
            "representation_properties": "Human-readable, structured per-node; yields long sequential text proportional to (#nodes × avg tokens per node) + (#edges encoded redundantly via adjacency lists); sensitive to node ordering; tends to produce larger contexts than edge-list encodings because neighbors are repeated in multiple adjacency lines.",
            "evaluation_task": "Used as the Graph2Text input format for all four graph reasoning tasks and evaluated under zero-shot/few-shot prompting and fine-tuning (LoRA, prefix tuning) baselines.",
            "performance_metrics": "Exact Match Accuracy (EM). Example numbers from Table 2 (LLaMA2-7B, adjacency list): Zero-shot EMs often &lt;0.4; LoRA and prefix tuning have task-dependent EMs (e.g., LoRA(attn) Substructure ~0.50, MaxTriplet ~0.44, ShortestPath ~0.21, Bipartite ~0.74). Overall inferior to GraphLLM (GraphLLM achieves ~0.999/0.958/0.973/0.998 on the four tasks).",
            "comparison_to_other_representations": "Compared directly with edge-list Graph2Text and found no consistent advantage; adjacency-list Graph2Text yields similar poor scaling behavior and performance as edge-list when used with prompting or fine-tuning. Both are far worse than GraphLLM.",
            "limitations_or_challenges": "High redundancy (edges appear multiple times), long token sequences (see Table 4: adjacency-list Graph2Text average contexts per task range ~1.2K–3.3K tokens depending on method), sensitive to graph size and node count, leads to LLM context overflow for moderately larger graphs.",
            "uuid": "e5350.1",
            "source_info": {
                "paper_title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "EdgeList",
            "name_full": "Graph2Text via edge-list natural-language serialization",
            "brief_description": "A Graph2Text encoding that represents graph structure as an explicit list of edges in natural language (e.g., \"Atom 1 - Atom 2\", or lines saying which nodes are connected), sometimes presented in random order.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Edge-list Graph2Text (random order variant)",
            "representation_description": "Encode graph structure via a sequence of edge statements (an explicit listing of node pairs forming edges), combined with per-node textual descriptions elsewhere in the prompt. The paper evaluates a random-order edge list variant as one of two Graph2Text structural encodings.",
            "graph_type": "molecular graphs, friendship graphs, wormhole graphs, bipartite graphs (same four tasks)",
            "representation_properties": "Potentially more compact than adjacency lists (edges listed once), order-sensitive (random order used in experiments), still grows with number of edges and node descriptions, requires LLM to reconstruct topology from sequential edge records.",
            "evaluation_task": "Same four graph reasoning tasks; used in prompting (zero/few-shot/CoT) and fine-tuning baselines (LoRA, prefix tuning) reported in Tables 2 and 4.",
            "performance_metrics": "Exact Match Accuracy (EM). Example numbers (Table 2, LLaMA2-7B, Edge list random order): Zero-shot and few-shot EMs remain low; LoRA(attn) Substructure ~0.50, MaxTriplet ~0.42, ShortestPath ~0.20, Bipartite ~0.64. Performance trends similar to adjacency-list; no consistent improvement over adjacency-list.",
            "comparison_to_other_representations": "Edge-list performs comparably to adjacency-list in these experiments; neither structural encoding solved the core weaknesses of Graph2Text. Both are outperformed by GraphLLM by large margins. The paper explicitly states that choice of adjacency vs edge-list \"doesn't lead to a consistent enhancement\" in Graph2Text performance.",
            "limitations_or_challenges": "Still produces long contexts once node descriptions are included (Table 4 shows average token lengths for Graph2Text edge-list similar to adjacency-list); LLM must infer graph topology from a flat sequence; random edge order induces variability; catastrophic scaling as graph size increases (performance drops and/or context limit exceeded).",
            "uuid": "e5350.2",
            "source_info": {
                "paper_title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 2
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data ?",
            "rating": 2
        },
        {
            "paper_title": "Natural language is all a graph needs",
            "rating": 2
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 1
        }
    ],
    "cost": 0.012965249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GraphLLM: Boosting Graph Reasoning AbilITY OF LARGE LANGUAGE MODEL</h1>
<p>Ziwei Chai ${ }^{1 <em>}$ Tianjie Zhang ${ }^{1 </em>}$ Liang $\mathbf{W u}^{3}$ Kaiqiao Han ${ }^{1}$ Xiaohai $\mathbf{H u}^{2}$<br>Xuanwen Huang ${ }^{1}$ Yang Yang ${ }^{1}$<br>${ }^{1}$ Zhejiang University ${ }^{2}$ University of Washington<br>${ }^{3}$ State Grid Zhejiang Electric Power Supply Co. Ltd., China</p>
<h4>Abstract</h4>
<p>The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This integration equips LLMs with the capability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of $54.44 \%$, alongside a noteworthy context reduction of $96.45 \%$ across various graph reasoning tasks. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>The AI community has witnessed the emergence of powerful pre-trained Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023), which leads to the pursuit of the potential realization of Artificial General Intelligence (AGI). Inspired by the fact that an intelligent agent, like the human brain, processes information of diverse types, there is a trend towards empowering LLMs to understand various forms of data, such as audio (Huang et al., 2023) and images (Alayrac et al., 2022). Despite significant strides in interpreting multimodal information (Yin et al., 2023), empowering LLMs to understand graph data remains relatively unexplored. Graphs, which represent entities as nodes and relationships as edges, are ubiquitous in numerous fields, e.g. molecular networks, social networks. An intelligent agent is expected to reason with graph data to facilitate many tasks such as drug discovery (Stokes et al., 2020) and chip design (Mirhoseini et al., 2021).</p>
<p>Current efforts have revealed that LLM's performance on some fundamental graph reasoning tasks is (unexpectedly) subpar. As noted by Wang et al. (2023a), even with tailor-made prompts, LLMs muster an accuracy of barely $33.5 \%$ when tasked with calculating the shortest path on a graph with up to 20 nodes. Their research also highlighted that fine-tuning OPT-2.7B (Zhang et al., 2022) failed to elicit the graph reasoning ability. Similarly, our experiments indicate that fine-tuning more recent LLaMA2-7B/13B (Touvron et al., 2023) still results in underwhelming performances in several fundamental graph reasoning tasks. This raises an essential question: What hinders the ability of LLMs on graph reasoning tasks?</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of Graph2Text vs. GraphLLM. The LLM is tasked with computing the minimum quantity of dark matter necessary to transition from the starting wormhole to the ending wormhole, given the connectivity graph and the textual descriptions of each node.</p>
<p>We posit that the key obstacle to LLMs' graph reasoning ability can be attributed to the prevailing practice of converting graphs into natural language descriptions (Graph2Text). A majority of the existing attempts to apply LLMs to graph data, such as the studies by <em>Wang et al. (2023a); Guo et al. (2023); Ye et al. (2023)</em>, employ Graph2Text strategy to convert graph data into textual descriptions. While the Graph2Text-based methodology facilitates direct processing of graph data by LLMs through textual descriptions, it introduces following inherent shortcomings that curtail the ability of LLMs on graph reasoning tasks:</p>
<ol>
<li>LLMs, when using the Graph2Text strategy, are compelled to discern implicit graph structures from sequential text. In contrast to dedicated graph learning models that inherently process graph structures, LLMs may face inefficiencies in learning on graph based on sequential graph descriptions.</li>
<li>The Graph2Text-based methodology inherently results in a lengthy context of graph description, as illustrated in Figure 1. This could pose a challenge for LLMs to identify essential information for graph reasoning tasks from the lengthy contexts (Liu et al., 2023).</li>
</ol>
<p>To tackle the aforementioned limitations and enhance the ability of LLMs in graph reasoning, we introduce GraphLLM. Contrary to the Graph2Text strategy of converting graphs into textual descriptions, GraphLLM's core idea is to synergistically integrate a graph learning module (graph transformer) with the LLM to enhance graph reasoning ability. By synergizing the LLM and the graph transformer, GraphLLM harnesses the strengths of both and offers a more powerful and efficient solution to applying LLMs for graph reasoning tasks. Specifically, GraphLLM possesses the following two key advantages over Graph2Text-based methodology:</p>
<ol>
<li><strong>Collaborative Synergy.</strong> GraphLLM takes an end-to-end approach to integrate graph learning models and LLMs within a single, cohesive system. By synergizing with graph learning models, LLMs can harness its superior expressive power on graph data. Compared to Graph2Text-based methodology, GraphLLM achieves an average accuracy improvement from 43.75% to 98.19% on four fundamental graph reasoning tasks.</li>
<li><strong>Context Condensation.</strong> GraphLLM condenses graph information into a concise, fixed-length prefix, thereby circumventing the need of Graph2Text strategy to produce lengthy graph descriptions. Compared to Graph2Text-based methodology, GraphLLM substantially reduces the context length by 96.45%.</li>
</ol>
<p>Our experiments on four fundamental graph reasoning tasks covering text substructure counting, maximum triplet sum, shortest path, and bipartite graph matching, demonstrate that GraphLLM boosts the graph reasoning ability of LLM by an average accuracy improvement of 54.44%, while achieving a remarkable context reduction of 96.45% and 3.42x inference acceleration.</p>
<h1>2 Preliminary</h1>
<p>Definition 2.1. (Input Graph) Given an instance of instruction pair (Input, Instruction, Response), the Input graph is a set $\mathbb{V}$ of $n$ node $\left{\boldsymbol{d}<em 1="1">{0}, \boldsymbol{d}</em>}, \ldots, \boldsymbol{d<em i="i">{n-1}\right}$, where $\boldsymbol{d}</em> \rightarrow{0,1}$ is defined as follows:}$ is the textual feature ${ }^{2}$ of $i$-th node, with graph structure $\mathcal{E}$ on $\mathbb{V}$. The graph structure $\mathcal{E}: \mathbb{V} \times \mathbb{V</p>
<p>$$
\mathcal{E}\left(\boldsymbol{d}<em j="j">{i}, \boldsymbol{d}</em>}\right)= \begin{cases}1, &amp; \text { if there is a relationship between } \boldsymbol{d<em j="j">{i} \text { and } \boldsymbol{d}</em>
$$} \ 0, &amp; \text { otherwise }\end{cases</p>
<p>Thus the Input graph $\mathcal{G}$ can be denoted as a tuple ${\mathbb{V}, \mathcal{E}}$. Graph2Text-based methodology introduces graph description language $\mathcal{A}(\mathbb{V}, \mathcal{E}) \rightarrow$ TextDescription.
Definition 2.2. (Fine-tuning on Graph Reasoning Tasks) Given a pre-trained LLM $\mathcal{M}$ with parameters $\boldsymbol{\theta}$, a dataset of $m$ instruction pairs $\left{\left(\right.\right.$ Input $<em i="i">{i}$, Instruction $\left.</em>}, \text { Response <em _ldots_="\ldots," i="0," m-1="m-1">{i}\right)</em>}$, where each Input $<em i="i">{i}$ is a graph $\mathcal{G}</em>}=\left{\mathbb{V<em i="i">{i}, \mathcal{E}</em>$ by minimizing the following loss function:}\right}$, and a task-specific objective function $\mathcal{L}$, the finetuning process aims to learn task-specific parameters $\boldsymbol{\theta}^{*</p>
<p>$$
\boldsymbol{\theta}^{*}=\arg \min <em i="0">{\boldsymbol{\theta}^{\prime}} \sum</em>\right)
$$}^{m-1} \mathcal{L}\left(\mathcal{M}\left(\mathbb{V}, \mathcal{E}, \text { Instruction; } \boldsymbol{\theta}^{\prime}\right) ; \text { Response </p>
<p>where $\mathcal{M}\left(; \boldsymbol{\theta}^{\prime}\right)$ represents the output of the fine-tuned LLM $\mathcal{M}$ with parameters $\boldsymbol{\theta}^{\prime}$. Note that in Eq. (2) the subscripts of $\mathbb{V}, \mathcal{E}$, Instruction and Response are omitted for clarity.</p>
<p>Prefix Tuning Given a pre-trained LLM with an $L$-layer transformer, prefix tuning prepends $K$ trainable continuous tokens (prefixes) to the keys and values of the attention at every transformer layer. Taking the $l$-th attention layer as an example $(l&lt;L)$, prefix vectors $\boldsymbol{P}<em l="l">{l} \in \mathbb{R}^{K \times d^{\mathrm{M}}}$ is concatenated with the original keys $\boldsymbol{K}</em>^{} \in \mathbb{R<em> \times d^{\mathrm{M}}}$ and values $\boldsymbol{V}_{l} \in \mathbb{R}^{</em> \times d^{\mathrm{M}}}$, where $d^{\mathrm{M}}$ is the dimension of LLM, formulated as:</p>
<p>$$
\boldsymbol{K}<em l="l">{l}^{\prime}=\left[\boldsymbol{P}</em>} ; \boldsymbol{K<em l="l">{l}\right] ; \boldsymbol{V}</em>}^{\prime}=\left[\boldsymbol{P<em l="l">{l} ; \boldsymbol{V}</em>
$$}\right] \in \mathbb{R}^{(K+*) \times d^{\mathrm{M}}</p>
<p>The new prefixed keys $\boldsymbol{K}<em l="l">{l}^{\prime}$ and values $\boldsymbol{V}</em>}^{\prime}$ are then subjected to the $l$-th attention layer of LLM. For simplicity, we denote the vanilla attention computation as $\boldsymbol{O<em l="l">{l}=\operatorname{Attn}\left(\boldsymbol{Q}</em>}, \boldsymbol{K<em l="l">{l}, \boldsymbol{V}</em>\right)$. The computation of attention becomes:</p>
<p>$$
\boldsymbol{O}<em l="l">{l}=\operatorname{Attn}\left(\boldsymbol{Q}</em>},\left[\boldsymbol{P<em l="l">{l} ; \boldsymbol{K}</em>}\right],\left[\boldsymbol{P<em l="l">{l} ; \boldsymbol{V}</em>\right]\right)
$$</p>
<p>In vanilla prefix tuning, prefixes are initialized from a trainable parameter tensor $\mathbf{P} \in \mathbb{R}^{L \times K \times d^{\mathrm{M}}}$.</p>
<h2>3 GraphLLM</h2>
<h3>3.1 General Framework of GraphLLM</h3>
<p>Reason on Graphs Since graphs inherently represent entities and their interrelationships, reasoning on graphs requires simultaneous consideration of both the entities (nodes) and their relationships (edges). Consequently, graph reasoning tasks encompass two sub-objectives: node understanding and structure understanding. For example, in the context of counting specific substructures within a molecular graph, one must discern the types of atoms from node descriptions (node understanding) and recognize the chemical bonds derived from the graph's structure (structure understanding). In the proposed GraphLLM, we intentionally devise modules addressing these dual objectives.
As demonstrated in Figure 2, GraphLLM consists of the following three main steps:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of reasoning on a toy molecular graph with GraphLLM. The LLM is requested to identify the number of C-C-O triangles in the molecule.</p>
<ol>
<li>Node Understanding (§3.2): A textual transformer encoder-decoder is used to extract semantic information crucial to solving graph reasoning tasks from node textual descriptions. The encoderdecoder is newly initialized and updated with the guidance of the pre-trained LLM.</li>
<li>Structure Understanding (§3.3): A graph transformer is employed to learn on the graph structure by aggregating the node representations obtained from the textual encoder-decoder. In this way, the graph representation produced by the graph transformer can incorporate both node semantic information and graph structure information simultaneously.</li>
<li>Graph-enhanced Prefix Tuning for LLMs (§3.4): GraphLLM derives the graph-enhanced prefix from the graph representation. During graph-enhanced prefix tuning, the LLM synergizes with the graph transformer by end-to-end fine-tuning, therefore boosting the LLM's capability in conducting graph reasoning tasks with proficiency.</li>
</ol>
<h1>3.2 Encoder-Decoder for Node Understanding</h1>
<p>The goal of the encoder-decoder is to extract the required information from the nodes based on the specific graph reasoning task. For example, when identifying substructures within molecule, it is necessary to extract atom types from the descriptions of the atoms. For the shortest path task, discerning the cost associated with each node from their descriptions is essential. Therefore, GraphLLM employs a textual transformer encoder-decoder architecture to adaptively extract node information required for graph reason tasks.</p>
<p>Specifically, a textual transformer encoder first applies self-attention to the node description, generating a context vector that captures the semantic meaning pertinent to graph reasoning tasks. Subsequently, a transformer decoder produce the node representation $\mathbf{H}<em i="i">{i}$ through the cross-attention between the context vector $\boldsymbol{c}</em>$ is a newly-initialized trainable embedding. For convenience, we provide a brief overview of the computation process of the encoder-decoder in Eq. (5). Detailed information can be found in Appendix A.1.}$ and the query $\mathbf{Q}$. The query $\mathbf{Q</p>
<p>$$
\begin{aligned}
\boldsymbol{c}<em i="i">{i} &amp; =\text { TransformerEncoder }\left(\boldsymbol{d}</em>} \boldsymbol{W<em i="i">{\mathrm{D}}\right) \
\mathbf{H}</em>\right)
\end{aligned}
$$} &amp; =\text { TransformerDecoder }\left(\mathbf{Q} ; \boldsymbol{c}_{i</p>
<p>where $\boldsymbol{d}<em _mathrm_D="\mathrm{D">{i} \in \mathbb{R}^{<em> \times d^{\mathrm{ld}}}$ is the embeddings ${ }^{3}$ of the textual description of node $i$ ( $</em>$ represents description's length). $\boldsymbol{W}</em>}} \in \mathbb{R}^{d^{\mathrm{ld}} \times d}$ is a down-projection matrix to reduce the dimension. $\boldsymbol{c<em i="i">{i} \in \mathbb{R}^{* \times d}$ is node $i$ 's context vector and $\mathbf{H}</em>$ is learnable query embedding, where $L$ is the layer number of LLM transformer and $K$ is the length of prefix.} \in \mathbb{R}^{L \times K \times d}$ is the node $i^{\prime}$ representation. $\mathbf{Q} \in \mathbb{R}^{L \times K \times d</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In GraphLLM, we adopt a lightweight transformer encoder-decoder (0.05B parameters for LLaMA 2 7B backbone). In practice, a newly-initialized encoder-decoder can effectively learn to capture node information required for graph reasoning tasks under the guidance of the pre-trained LLM.</p>
<h1>3.3 Graph Transformer for Structure Understanding</h1>
<p>Aiming at structure understanding, GraphLLM utilizes a graph transformer to learn from the graph structure. In our framework, the core advantage of the graph transformer over other commonly used graph learning modules (Kipf \&amp; Welling, 2017; Veličković et al., 2018) lies in its decoupling of node information and structural information. In the graph transformer, both the positional encoding, which captures the structural information of the graph, and the node representations are independently fed into the transformer blocks and subsequently updated during the learning process. We empirically find that the decoupling of node understanding and structure understanding enhances GraphLLM's graph reasoning ability. The graph transformer primarily consists of two key designs: positional encoding and attention mechanism on graph.</p>
<p>The positional encoding $\boldsymbol{e}<em i_="i," j="j">{i, j}$ between node $i$ and node $j$ is initialized using relative random walk probabilities (RRWP) encoding (Ma et al., 2023). Let $\boldsymbol{A}$ be the adjacency matrix of a graph ${\mathbb{V}, \mathcal{E}}$ and $\boldsymbol{D}$ be the degree matrix. Define the random walk matrix $\boldsymbol{M}:=\boldsymbol{D}^{-1} \boldsymbol{A}, \boldsymbol{I}$ the identity matrix. The positional encoding $\boldsymbol{e}</em>$ can be formulated as follows:}$ for each node pair $i, j \in \mathcal{V</p>
<p>$$
\begin{aligned}
\boldsymbol{R}<em i_="i," j="j">{i, j} &amp; =\left[\boldsymbol{I}</em>}, \boldsymbol{M<em i_="i," j="j">{i, j}, \boldsymbol{M}</em>}^{2}, \ldots, \boldsymbol{M<em i_="i," j="j">{i, j}^{C-1}\right] \in \mathbb{R}^{C} \
\boldsymbol{e}</em>
\end{aligned}
$$} &amp; =\Phi\left(\boldsymbol{R}_{i, j}\right) \in \mathbb{R}^{d</p>
<p>in which $C$ is a parameter controlling the maximum length of random walks considered. $\boldsymbol{R}<em i_="i," j="j">{i, j}$ is updated by an elementwise MLP $\Phi: \mathbb{R}^{C} \rightarrow \mathbb{R}^{d}$ to get the relative positional encoding $\boldsymbol{e}</em>$, which encodes the structural relationship between node $i$ and node $j$.</p>
<p>We adopt attention design of the graph transformer introduced by Ma et al. (2023). Note that the graph transformer adapts self attention on $\boldsymbol{h}<em i="i">{i}:=\mathbf{H}</em>}[l, k,:] \in \mathbb{R}^{d}(l \in[0, L-1] ; k \in[0, K-1])$ of each index $[l, k]$ independently. Given $\boldsymbol{h<em i="i">{i}^{(0)}=\boldsymbol{h}</em>}, \boldsymbol{e<em i_="i," j="j">{i, j}^{(0)}=\boldsymbol{e}</em>$, the $t$-th layer of graph transformer $(t&lt;T)$ can be formulated as:</p>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{e}}<em _mathrm_Q="\mathrm{Q">{i, j}^{(t)}=\sigma\left(\rho\left(\left(\boldsymbol{W}</em>}} \boldsymbol{h<em _mathrm_K="\mathrm{K">{i}^{(t)}+\boldsymbol{W}</em>}} \boldsymbol{h<em _mathrm_Ew="\mathrm{Ew">{j}^{(t)}\right) \odot \boldsymbol{W}</em>}} \boldsymbol{e<em _mathrm_Eb="\mathrm{Eb">{i, j}^{(t)}\right)+\boldsymbol{W}</em>}} \boldsymbol{e<em i="i" j="j">{i, j}^{(t)}\right) \in \mathbb{R}^{d} \
&amp; \alpha</em>}=\operatorname{Softmax<em _mathrm_A="\mathrm{A">{j \in \mathbb{V}}\left(\boldsymbol{W}</em>}} \hat{\boldsymbol{e}<em i="i">{i, j}^{(t)}\right) \in \mathbb{R} \
&amp; \boldsymbol{h}</em>}^{(t+1)}=\sum_{j \in \mathbb{V}} \alpha_{i j} \cdot \boldsymbol{W<em j="j">{\mathrm{V}} \boldsymbol{h}</em>
\end{aligned}
$$}^{(t)} \in \mathbb{R}^{d</p>
<p>where $\boldsymbol{W}<em _mathrm_K="\mathrm{K">{\mathrm{Q}}, \boldsymbol{W}</em>}}, \boldsymbol{W<em _mathrm_Eb="\mathrm{Eb">{\mathrm{Ew}}, \boldsymbol{W}</em>}}, \boldsymbol{W<em _mathrm_A="\mathrm{A">{\mathrm{V}} \in \mathbb{R}^{d \times d}$ and $\boldsymbol{W}</em>}} \in \mathbb{R}^{1 \times d}$ are learnable weight matrices; $\odot$ indicates elementwise multiplication; and $\rho(\boldsymbol{x}):=(\operatorname{ReLU}(\boldsymbol{x}))^{1 / 2}-(\operatorname{ReLU}(-\boldsymbol{x}))^{1 / 2}$. We also include feed-forward module, residual connection and normalization in our implementation, but they are omitted here for simplicity, which are detailed shown in Appendix A.2. The representation $\mathbf{H<em i="i">{i}$ of node $i$ is derived by gathering $\boldsymbol{h}</em>$ of each index $[l, k]$.
For node-level graph reasoning tasks, the Input graph representation $\mathbf{G}=\mathbf{H}}^{(T)<em _in="\in" _mathbb_V="\mathbb{V" i="i">{i}$, where node $i$ is to be inferred. For graph-level graph reasoning tasks, the Input graph representation $\mathbf{G}=\sum</em>|$ by mean-pooling on the graph.}} \mathbf{H}_{i} /|\mathbb{V</p>
<h3>3.4 Graph-enhanced Prefix Tuning for LLMs</h3>
<p>To produce a Response in human language for a graph reasoning task, LLMs utilize graphenhanced tunable prefix derived from the graph representation $\mathbf{G}$ during the tuning process. Specifically, the graph-enhanced prefix $\mathbf{P}$ is obtained by applying a linear projection to the graph representation $\mathbf{G}$ as illustrated in Eq. (8), where $\boldsymbol{W}_{\mathrm{U}} \in \mathbb{R}^{d \times d^{5 t}}$ is a matrix converting the dimension.</p>
<p>$\mathbf{P}=\mathbf{G} W_{\mathrm{U}}+\mathbf{B}$
Then $\mathbf{P} \in \mathbb{R}^{L \times K \times d^{\mathrm{M}}}$ is prepended to each attention layer of the LLM as shown in Eqs. (3) and (4).
Connection to Prefix Tuning It's worth noting that when $W_{\mathrm{U}}$ is a zero matrix, GraphLLM degenerates into vanilla prefix tuning as $\mathbf{P}=\mathbf{G} \mathbf{0}+\mathbf{B}$. From this perspective, GraphLLM is an enhancement of prefix tuning. In GraphLLM, the LLM synergizes with the powerful graph transformer to incorporate additional context information crucial to graph reasoning into the prefix. Consequently, the LLM can produce appropriate response for the graph reasoning task by interpreting the contexts encapsulated within the graph-enhanced prefix.</p>
<h1>4 EXPERIMENT</h1>
<p>In this section, we aim to empirically substantiate three central hypotheses posited in this study.</p>
<ul>
<li>Q1: Does GraphLLM effectively enhance the graph reasoning ability of the LLM?</li>
<li>Q2: Can GraphLLM address the issue of lengthy context caused by Graph2Text strategy?</li>
<li>Q3: How does GraphLLM perform in terms of computational efficiency?</li>
</ul>
<h3>4.1 EXPERIMENTAL SETTINGS</h3>
<p>Graph Reasoning Tasks We follow the design of the graph reasoning tasks in Wang et al. (2023a), which proposes a series of graph reasoning tasks with varying complexity on randomly generated graphs. Note that in Wang et al. (2023a), the nodes are identified and described by a single number index simply. This over-simplification potentially hinders a comprehensive evaluation of the model's capabilities in node understanding. Consequently, we develop four graph reasoning tasks where each node has a textual entity description of around 50 tokens. These tasks can simultaneously test the abilities of node understanding and structure understanding, which are both crucial for graph reasoning tasks. We present the illustration of the graph tasks in Figure 3, and the dataset statistics are provided in Table 1.</p>
<p>Table 1: Statistics of the graph reasoning task datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Substructure Counting</th>
<th style="text-align: center;">Maximum Triplet Sum</th>
<th style="text-align: center;">Shortest Path</th>
<th style="text-align: center;">Bipartite Graph Matching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Avg. $</td>
<td style="text-align: center;">\nabla</td>
<td style="text-align: center;">/$ Avg. $</td>
<td style="text-align: center;">\mathcal{E}</td>
<td style="text-align: center;">$</td>
</tr>
<tr>
<td style="text-align: center;">No. of Tokens in Node Desc.</td>
<td style="text-align: center;">52-59</td>
<td style="text-align: center;">39-82</td>
<td style="text-align: center;">48-58</td>
<td style="text-align: center;">34-61</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the graph reasoning tasks. Each Input graph consists of a number of nodes characterized by textual node descriptions and the graph structure between the nodes.</p>
<ul>
<li>Task 1: Substructure Counting Let $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ be a molecular graph, where each atom in $\mathcal{V}$ has a text description $\boldsymbol{d}_{i}$ that includes the element type of the atom. LLMs are tasked with counting the number of specific substructure, e.g. carbon-carbon-oxygen triangle.</li>
<li>Task 2: Maximum Triplet Sum Let $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ be a friendship graph, where each person in $\mathcal{V}$ has a text description $\boldsymbol{d}_{i}$ that includes the age of the person. In this task, LLMs are instructed to identify the maximum cumulative age among all possible triplets formed by selecting a specific individual, their direct friends, and the friends of those friends.</li>
<li>Task 3: Shortest Path Let $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ be a graph that represents interconnected wormholes. Each wormhole in $\mathcal{V}$ requires a different amount of dark matter for activation, which is included in the text description $\boldsymbol{d}_{i}$ of each node. Activating a wormhole enables spatial jumps to any connected wormhole. LLMs are required to compute the path from the starting wormhole to the destination wormhole that requires the least amount of dark matter.</li>
<li>Task 4: Bipartite Graph Matching Let $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ be a graph that depicts the application relationship between applicants and jobs. An edge in $\mathcal{E}$ represents an applicant applying for a specific job. Each job can only accept one applicant and a job applicant can be appointed for only one job. The text description $\boldsymbol{d}_{i}$ of each node provides information about either the job or the applicant. LLMs are required to compute the maximum possible number of applicants who can find the jobs they are interested in.</li>
</ul>
<p>Each task consists of 2,000/2,000/6,000 graph instance for training/validation/test. The textual descriptions of the nodes are generated by gpt-3.5-turbo according to specific instructions and manually verified. The graph descriptions of these tasks using Graph2Text strategy are presented in the Appendix D.</p>
<p>Baselines We compare GraphLLM with two categories of approaches: prompting and fine-tuning. The prompting approaches encompass the following strategies: zero-shot prompting, few-shot incontext learning (Brown et al., 2020) and few-shot chain-of-thought (CoT) prompting (Wei et al., 2022). The fine-tuning approaches include widely adopted prefix tuning (Li \&amp; Liang, 2021) and LoRA (Hu et al., 2022). Due to context length limit, all tasks are confined to one shot for fewshot methods. For LoRA, we apply low rank adaption only on attention module (attn) and on both attention module and feed-forward networks (attn+ffn) (Zhang et al., 2023). For all the baselines, we follow Wang et al. (2023a); Guo et al. (2023) to design prompts which describe the Input graph in natural language (Graph2Text). To analyze the performance gap that may emerge from utilizing different graph description languages, we utilized two prevalent methods to describe the graph structure: adjacency list and edge list.</p>
<p>Imeplementations We use LLaMA 2 7B/13B (Touvron et al., 2023) as our LLM backbone. For all tested methods, we set the temperature $\tau$ to 0 to ensure that the LLM's response is deterministic. We adopt Exact Match Accuracy as metrics for the four graph reasoning tasks. All experiments are conducted on $4 \times 80 \mathrm{G}$ A100 GPUs. Complete experiment setups such as hyperparameters, batch size, optimizer, learning rates are in Appendix B.
Table 2: Performance on Graph Reasoning Tasks. Shown is the mean $\pm$ s.d. of 3 runs with different random seeds. Highlighted are the top and second-best.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Format</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">LLaMA2-7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA2-13B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Substructure <br> Counting</td>
<td style="text-align: center;">Maximum Triplet Sum</td>
<td style="text-align: center;">Shortest <br> Path</td>
<td style="text-align: center;">Bipartite Graph <br> Matching</td>
<td style="text-align: center;">Substructure <br> Counting</td>
<td style="text-align: center;">Maximum Triplet Sum</td>
<td style="text-align: center;">Shortest <br> Path</td>
<td style="text-align: center;">Bipartite Graph <br> Matching</td>
</tr>
<tr>
<td style="text-align: center;">Adjacency <br> List</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">0.2260</td>
<td style="text-align: center;">0.1110</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.3630</td>
<td style="text-align: center;">0.0145</td>
<td style="text-align: center;">0.0925</td>
<td style="text-align: center;">0.0010</td>
<td style="text-align: center;">0.1180</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">0.2735</td>
<td style="text-align: center;">0.1445</td>
<td style="text-align: center;">0.0575</td>
<td style="text-align: center;">0.3280</td>
<td style="text-align: center;">0.2780</td>
<td style="text-align: center;">0.1430</td>
<td style="text-align: center;">0.0520</td>
<td style="text-align: center;">0.2675</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">0.2177</td>
<td style="text-align: center;">0.0585</td>
<td style="text-align: center;">0.1089</td>
<td style="text-align: center;">0.2399</td>
<td style="text-align: center;">0.2150</td>
<td style="text-align: center;">0.0544</td>
<td style="text-align: center;">0.1552</td>
<td style="text-align: center;">0.1048</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRA(attn)</td>
<td style="text-align: center;">0.5012 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4427 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.2119 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.7383 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4926 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4080 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.1251 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.7792 _ $_{\text {, } 0.01}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRA(attn+ffn)</td>
<td style="text-align: center;">0.5400 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4723 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.1652 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.6041 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4948 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4274 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.1181 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.8016 _ $_{\text {, } 0.01}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prefix Tuning</td>
<td style="text-align: center;">0.5803 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.3857 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.2173 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.5534 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4610 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.3377 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.1608 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4640 _ $_{\text {, } 0.01}$</td>
</tr>
<tr>
<td style="text-align: center;">Edge List <br> (Random Order)</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">0.2460</td>
<td style="text-align: center;">0.1260</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.4325</td>
<td style="text-align: center;">0.0805</td>
<td style="text-align: center;">0.1265</td>
<td style="text-align: center;">0.0010</td>
<td style="text-align: center;">0.0055</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">0.2610</td>
<td style="text-align: center;">0.1420</td>
<td style="text-align: center;">0.0111</td>
<td style="text-align: center;">0.3687</td>
<td style="text-align: center;">0.2655</td>
<td style="text-align: center;">0.1423</td>
<td style="text-align: center;">0.1110</td>
<td style="text-align: center;">0.5230</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">0.2127</td>
<td style="text-align: center;">0.0565</td>
<td style="text-align: center;">0.1069</td>
<td style="text-align: center;">0.1411</td>
<td style="text-align: center;">0.2320</td>
<td style="text-align: center;">0.0767</td>
<td style="text-align: center;">0.1351</td>
<td style="text-align: center;">0.0464</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRA(attn)</td>
<td style="text-align: center;">0.5035 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4224 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.2011 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.6457 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4920 _ $_{\text {, } 0.12}$</td>
<td style="text-align: center;">0.4143 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.1240 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.6319 _ $_{\text {, } 0.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRA(attn+ffn)</td>
<td style="text-align: center;">0.5101 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4552 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.2011 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.5446 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4904 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4489 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.1558 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.6126 _ $_{\text {, } 0.01}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prefix Tuning</td>
<td style="text-align: center;">0.3925 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.3780 _ $_{\text {, } 0.11}$</td>
<td style="text-align: center;">0.1656 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.4599 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.3319 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.3529 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.1240 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.5228 _ $_{\text {, } 0.01}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphLLM</td>
<td style="text-align: center;">0.9990 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9577 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9726 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9981 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9898 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9392 _ $_{\text {, } 0.00}$</td>
<td style="text-align: center;">0.9619 _ $_{\text {, } 0.01}$</td>
<td style="text-align: center;">0.9534 _ $_{\text {, } 0.00}$</td>
</tr>
</tbody>
</table>
<h1>4.2 Performance on Graph Reasoning Tasks (Q1)</h1>
<p>Table 2 delineates the performance differentials between GraphLLM and Graph2Text-based methodologies across the four graph reasoning tasks. From this comparative analysis, we can infer several key insights: (1). The zero-shot, few-shot, and chain-of-thought Graph2Text-based prompting methods deliver subpar performance, indicating the limitations of LLMs in generalizing to graph reasoning tasks without additional fine-tuning. (2). Even with fine-tuning on graph reasoning tasks, Graph2Text-based methodology significantly lag behind the performance achieved by GraphLLM. This discrepancy suggests that the Graph2Text-based approaches can constitute a significant obstacle preventing LLMs from adapting to graph reasoning tasks. (3). The choice between the two primary graph description languages (adjacency/edge list) doesn't lead to a consistent enhancement in the performance of Graph2Text-based methods. This finding confirms that the impediments introduced by the Graph2Text methodology aren't tied to a specific graph description language. (4). On average, GraphLLM achieves an Exact Match Accuracy of $\mathbf{9 8 . 1 9 \%}$ over the four tasks, in contrast to the top-performing Graph2Text-based method, which manages only $47.35 \%$. This difference underscores the effectiveness of our approach in facilitating LLMs in graph reasoning tasks.</p>
<p>Evaluation on Stronger LLMs We also evaluate the Graph2Text strategy on more powerful gpt-3.5-turbo and gpt-4, illustrated on Table 3. The results indicate that even the advanced gpt-4 falls short in basic graph reasoning tasks, limiting its application in more complex scenarios such as drug design. GraphLLM provides a lightweight fine-tuning method that enables the LLM to synergize with graph reasoning modules. Notably, GraphLLM with LLaMA 2 7B as the backbone LLM shows relative improvements of $2.61 \%, 99.8 \%, 12.22 \%$,
Table 3: Performance of gpt-3.5-turbo and gpt-4 with Graph2Text strategy (converting input graph into adjacency list described in natural language), evaluated on 30 random samples due to the money cost.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Substructure <br> Counting</th>
<th style="text-align: center;">Maximum Triplet <br> Sum</th>
<th style="text-align: center;">Shortest <br> Path</th>
<th style="text-align: center;">Bipartite Graph <br> Matching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-3.5- <br> turbo</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">0.2667</td>
<td style="text-align: center;">0.5667</td>
<td style="text-align: center;">0.2000</td>
<td style="text-align: center;">0.1000</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: center;">0.3000</td>
<td style="text-align: center;">0.3000</td>
<td style="text-align: center;">0.2667</td>
<td style="text-align: center;">0.0667</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Few-shot CoT</td>
<td style="text-align: center;">0.3667</td>
<td style="text-align: center;">0.7000</td>
<td style="text-align: center;">0.7333</td>
<td style="text-align: center;">0.2667</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">0.6000</td>
<td style="text-align: center;">0.7333</td>
<td style="text-align: center;">0.6667</td>
<td style="text-align: center;">0.3333</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: center;">0.5000</td>
<td style="text-align: center;">0.8667</td>
<td style="text-align: center;">0.5667</td>
<td style="text-align: center;">0.5000</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Few-shot CoT</td>
<td style="text-align: center;">0.5000</td>
<td style="text-align: center;">0.9333</td>
<td style="text-align: center;">0.8667</td>
<td style="text-align: center;">0.8667</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2-7B GraphLLM</td>
<td style="text-align: left;">0.9990</td>
<td style="text-align: center;">0.9577</td>
<td style="text-align: center;">0.9726</td>
<td style="text-align: center;">0.9981</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>and $15.16 \%$ compared to gpt-4 few-shot CoT on the four fundamental graph reasoning tasks, respectively.</p>
<h3>4.3 Comparative Analysis on Context Reduction (Q2)</h3>
<p>Table 4 demonstrates the LLM context length for graph reasoning tasks utilizing Graph2Text-based methods and GraphLLM, respectively. Notably, GraphLLM reduces the context length by a substantial $\mathbf{9 6 . 4 5 \%}$ across the four graph reasoning tasks averagely. This substantial reduction is achieved as GraphLLM encodes both node descriptions and structural information into a fixed-length prefix ( 5 additional prefix tokens in our GraphLLM's implementation). In contrast, Graph2Text-based methods describe the graph in natural language, including both node descriptions and graph structure. This approach inherently results in an extended context, potentially hampering the efficiency and effectiveness of LLMs on graph reasoning.</p>
<p>Figure 4 illustrates the performance of Graph2Text-based methods and GraphLLM on the substructure counting task when the size of graph increases. More concretely, the average node number of the graph instances in the substructure counting dataset is incrementally increased from 15 to 45 , with a step size of 10 . We compare GraphLLM with Graph2Text-based prefix tuning and LoRA, ignoring other less effective baseline methods. We additionally compare GraphLLM with Graph2Text-based few-shot CoT on gpt-3.5-turbo-16k, because the context limit of gpt-3.5-turbo/gpt-4 is exceeded when the node number reaches 25 . We observe that with the increase in graph size, the context size of the Graph2Text-based method also expands, leading to a corresponding decline in performance. It is noteworthy that as the graph size increases to 45 nodes, Graph2Text-based methods with LLaMA 2 as backbone exceeds the context length limit ( 4096 tokens), and the performance of gpt-3.5-turbo-16k also dropped to 0 . In comparison, GraphLLM still retains an accuracy of 0.9645 . This stability highlights the robustness of GraphLLM, contrasting with the declining performance and efficiency observed in Graph2Text-based methods as graph size expands.</p>
<p>Table 4: Context length of different methods on graph reasoning tasks, measured by average token number processed by the LLaMA 2 tokenizer. A/B shown is the context length of Graph2Text-based methods with adjacency list/edge list as graph description language.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Avg. Context Length</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Substructure</td>
<td>Maximum Triplet</td>
<td>Shortest Path</td>
<td>Bipartite Graph</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Matching</td>
</tr>
<tr>
<td>Zero-shot</td>
<td>$1.3 \mathrm{~K} / 1.3 \mathrm{~K}$</td>
<td>$1.4 \mathrm{~K} / 1.4 \mathrm{~K}$</td>
<td>$1.8 \mathrm{~K} / 1.7 \mathrm{~K}$</td>
<td>$1.2 \mathrm{~K} / 1.2 \mathrm{~K}$</td>
</tr>
<tr>
<td>Few-shot</td>
<td>$2.6 \mathrm{~K} / 2.5 \mathrm{~K}$</td>
<td>$2.8 \mathrm{~K} / 2.8 \mathrm{~K}$</td>
<td>$3.1 \mathrm{~K} / 2.9 \mathrm{~K}$</td>
<td>$2.4 \mathrm{~K} / 2.7 \mathrm{~K}$</td>
</tr>
<tr>
<td>Few-shot CoT</td>
<td>$2.8 \mathrm{~K} / 2.7 \mathrm{~K}$</td>
<td>$3.0 \mathrm{~K} / 2.9 \mathrm{~K}$</td>
<td>$3.3 \mathrm{~K} / 3.1 \mathrm{~K}$</td>
<td>$2.5 \mathrm{~K} / 2.8 \mathrm{~K}$</td>
</tr>
<tr>
<td>LoRA</td>
<td>$1.3 \mathrm{~K} / 1.3 \mathrm{~K}$</td>
<td>$1.4 \mathrm{~K} / 1.4 \mathrm{~K}$</td>
<td>$1.8 \mathrm{~K} / 1.7 \mathrm{~K}$</td>
<td>$1.2 \mathrm{~K} / 1.2 \mathrm{~K}$</td>
</tr>
<tr>
<td>Prefix Tuning</td>
<td>$1.3 \mathrm{~K} / 1.3 \mathrm{~K}$</td>
<td>$1.4 \mathrm{~K} / 1.4 \mathrm{~K}$</td>
<td>$1.8 \mathrm{~K} / 1.7 \mathrm{~K}$</td>
<td>$1.2 \mathrm{~K} / 1.2 \mathrm{~K}$</td>
</tr>
<tr>
<td>GraphLLM</td>
<td>$0.040 \mathrm{~K}(; \mathbf{9 6 . 9 2 \%})$</td>
<td>$0.052 \mathrm{~K}(; \mathbf{9 6 . 2 9 \%})$</td>
<td>$0.048 \mathrm{~K}(; \mathbf{9 7 . 1 8 \%})$</td>
<td>$0.055 \mathrm{~K}(; \mathbf{9 5 . 4 2 \%})$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance on substructure counting tasks when increasing the node number $|\mathbb{V}|$ of graph instances. A(B) represents context length and the corresponding performance. "OOL" denotes exceeding context length limit.</p>
<h1>4.4 ANALYSIS ON COMPUTATIONAL EFFICIENCY (Q3)</h1>
<p>Inference Acceleration Figure 5 illustrates the comparison of inference times on substructure counting task between GraphLLM and Graph2Text-based methods. Notably, GraphLLM achieves a speedup of $\mathbf{3 . 4 2}$ times compared to the best-performing Graph2Textbased method. The complete results of the inference time for other tasks are provided in the Appendix. The experimental results indicate that the inference acceleration achieved by GraphLLM, due to the context reduction for graph reasoning tasks, considerably surpasses the additional time overhead introduced by the graph learning module.</p>
<p>Figure 5: Avg. inference time on the substructure counting task on LLaMA 2 7B .</p>
<h2>5 Related Work</h2>
<p>LLMs exhibit the ability to understand diverse types of information and craft contextually relevant text responses, including but not limited to images (Wang et al., 2023b), audio (Huang et al., 2023), and point clouds (Xu et al., 2023). Endeavors to empower LLMs with the ability to understand graph data have been ongoing. Generally, these efforts can be categorized into two main categories. The first category includes models that employ a large language model to interface with individual graph models or APIs (Zhang, 2023; Wei et al., 2023). Nevertheless, these interactive systems still encounter limitations in accessing the internal graph reasoning process, which hinder their ability to seamlessly integrate graph learning and large language models. The second category includes models that employ an end-to-end training strategy. Notably, Wang et al. (2023a) make an attempt to fine-tune an opt-2.5B model on a Graph2Text corpus of basic graph reasoning tasks. However, their efforts fail to elicit graph reasoning ability of LLMs. The task of enhancing the graph reasoning ability of LLMs in an end-to-end manner remains unresolved. To our knowledge, our work stands out as a pioneering effort in successfully integrating the graph learning model with LLMs, demonstrably enhancing graph reasoning ability. GraphLLM takes a unified, end-to-end approach to integrate graph learning models and LLMs, enhancing the overall efficiency by synergizing the strengths of both within a single, cohesive system.</p>
<h2>6 DISCUSSION</h2>
<p>We introduce GraphLLM, an integrated end-to-end approach that synergizes LLMs with graph learning models to enhance the graph reasoning capabilities of LLMs. We hope our work can provide insights and guidance for future research in the domain of enabling LLMs to comprehend graph data and tackle advanced graph-related tasks.</p>
<h1>REFERENCES</h1>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, volume 35, pp. 23716-23736, 2022.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.</p>
<p>Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking. ArXiv, abs/2305.15066, 2023.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9.</p>
<p>Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech, music, sound, and talking head, 2023.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https: //openreview.net/forum?id=SJU4ayYgl.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / 2021$.acl-long. 353 .</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023.</p>
<p>Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet Kumar Dokania, Mark Coates, Philip H. S. Torr, and Ser Nam Lim. Graph inductive biases in transformers without message passing. In International Conference on Machine Learning, 2023.</p>
<p>Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter, and Jeff Dean. A graph placement methodology for fast chip design. Nature, 594(7862):207-212, jun 2021. doi: $10.1038 / \mathrm{s} 41586-021-03544-\mathrm{w}$.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.</p>
<p>Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?, 2023a.</p>
<p>Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, 2023b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.</p>
<p>Lanning Wei, Zhiqiang He, Huan Zhao, and Quanming Yao. Unleashing the power of graph learning through llm-based autonomous agents, 2023.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=ryGs6iA5Km.</p>
<p>Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds, 2023.</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs, 2023.</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2023.</p>
<p>Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt, 2023.</p>
<p>Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=lq62uWRJjiY.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.</p>
<h1>A Detailed Formulation of GraphLLM</h1>
<h2>A. 1 Details of Textual Transformer Encoder-Decoder</h2>
<p>Details of the textual transformer encoder-decoder architecture are shown in Figure 6. In each layer of the transformer encoder, the sequential node textual features pass through the multi-head selfattention module without masking, allowing for a contextual understanding of the text sequence. The resulting encoded sequence, $\boldsymbol{c}<em i="i">{i}$, engages in the cross-attention with fixed-size query embeddings in the transformer decoder. This process enables query embeddings to extract essential information from it. Finally, the output of the transformer decoder, $\mathbf{H}</em>$, serving as the node representation.
}$, contains specific information from the encoded sequence $\boldsymbol{c}_{i<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Architecture of the textual transformer encoder-decoder in GraphLLM.</p>
<h2>A. 2 Complete Formulation of Graph Transformer</h2>
<p>A complete graph transformer layer comprises a multi-head attention module, a feed-forward network, along with the residual connection and layer normalization associated with each of these components. For the $t$-th layer in the graph transformer, the attention computation, excluding the multi-head part, is as follows:</p>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{e}}<em _mathrm_Q="\mathrm{Q">{i, j}^{(t)}=\sigma\left(\rho\left(\left(\boldsymbol{W}</em>}} \boldsymbol{h<em _mathrm_K="\mathrm{K">{i}^{(t)}+\boldsymbol{W}</em>}} \boldsymbol{h<em _mathrm_Ew="\mathrm{Ew">{j}^{(t)}\right) \odot \boldsymbol{W}</em>}} \boldsymbol{e<em _mathrm_Eh="\mathrm{Eh">{i, j}^{(t)}\right)+\boldsymbol{W}</em>}} \boldsymbol{e<em i="i" j="j">{i, j}^{(t)}\right) \in \mathbb{R}^{d} \
&amp; \alpha</em>}=\operatorname{Softmax<em i="i">{j \in \mathbb{V}</em>}}\left(\boldsymbol{W<em i_="i," j="j">{\mathrm{A}} \hat{\boldsymbol{e}}</em> \
&amp; \hat{\boldsymbol{h}}}^{(t)}\right) \in \mathbb{R<em _in="\in" _mathbb_V="\mathbb{V" j="j">{i}^{(t)}=\sum</em><em i="i" j="j">{i}} \alpha</em>} \cdot \boldsymbol{W<em j="j">{\mathrm{V}} \boldsymbol{h}</em>
\end{aligned}
$$}^{(t)} \in \mathbb{R}^{d</p>
<p>where $\boldsymbol{W}<em _mathrm_K="\mathrm{K">{\mathrm{Q}}, \boldsymbol{W}</em>}}, \boldsymbol{W<em _mathrm_Eh="\mathrm{Eh">{\mathrm{Ew}}, \boldsymbol{W}</em>}}, \boldsymbol{W<em _mathrm_A="\mathrm{A">{\mathrm{V}} \in \mathbb{R}^{d \times d}$ and $\boldsymbol{W}</em> ; \odot$ indicates elementwise multiplication.}} \in \mathbb{R}^{1 \times d}$ are learnable weight matrices; $\sigma$ is a non-linear activation (ReLU by default); $\rho(\boldsymbol{x}):=(\operatorname{ReLU}(\boldsymbol{x}))^{1 / 2}-(\operatorname{ReLU}(-\boldsymbol{x}))^{1 / 2</p>
<p>The different attention heads are combined as a whole, and this combination is then subject to a residual connection and passed through layer normalization to obtain the output of the multi-head attention module.</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{h}<em h="h" i_="i,">{i}^{(t), \text { attn }}=\operatorname{LayerNorm}\left(\operatorname{Concat}\left(\left{\hat{\boldsymbol{h}}</em>\right}}^{(t)<em h="h">{h=1}^{N</em>}}\right) \boldsymbol{W<em i="i">{\mathrm{O}}+\boldsymbol{h}</em>\right) \
&amp; \boldsymbol{e}}^{(t)<em h="h" i_="i," j_="j,">{i, j}^{(t), \text { attn }}=\operatorname{LayerNorm}\left(\operatorname{Concat}\left(\left{\hat{\boldsymbol{e}}</em>\right}}^{(t)<em h="h">{h=1}^{N</em>}}\right) \boldsymbol{W<em i_="i," j="j">{\mathrm{Eo}}+\boldsymbol{e}</em>\right)
\end{aligned}
$$}^{(t)</p>
<p>where $\boldsymbol{W}<em _mathrm_Eo="\mathrm{Eo">{\mathrm{O}}, \boldsymbol{W}</em>}} \in \mathbb{R}^{d \times d}$ are learnable weight matrices, $N_{h}$ denotes the number of attention heads and $\boldsymbol{h<em i_="i," j="j">{i}^{(t), \text { attn }}, \boldsymbol{e}</em>$ are the normalized outputs of the attention module.}^{(t), \text { attn }</p>
<p>The feed-forward network, the corresponding residual connection and layer normalization can be formulated as:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{h}<em i="i">{i}^{(t+1)}=\operatorname{LayerNorm}\left(\text { Feedforward }\left(\boldsymbol{h}</em>}^{(t), \text { attn }}\right)+\boldsymbol{h<em i_="i," j="j">{i}^{(t), \text { attn }}\right) \
&amp; \boldsymbol{e}</em>}^{(t+1)}=\operatorname{LayerNorm}\left(\text { Feedforward }\left(\boldsymbol{e<em i_="i," j="j">{i, j}^{(t), \text { attn }}\right)+\boldsymbol{e}</em>\right)
\end{aligned}
$$}^{(t), \text { attn }</p>
<p>where $\boldsymbol{h}<em i_="i," j="j">{i}^{(t+1)}, \boldsymbol{e}</em>$ are the outputs of the entire $t$-th graph transformer layer.}^{(t+1)</p>
<h1>B Setup</h1>
<p>We provide the hyperparameters of our method for different graph tasks in Table 5. For the baseline methods that require fine-tuning of LLM, we ensure fair comparison by training them for the same number of epochs as GraphLLM. Additionally, we conducted a search for some important hyperparameters. Specifically, we search the rank parameter of the LoRA from a set ${4,8, \mathbf{1 6}}$ and the number of prefix tokens in prefix tuning from a set ${5, \mathbf{1 0}, 20}$.</p>
<p>Table 5: Hyperparameters of GraphLLM for the four datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Substructure <br> Counting</th>
<th style="text-align: center;">Maximum Triplet <br> Sum</th>
<th style="text-align: center;">Shortest Path</th>
<th style="text-align: center;">Bipartite Graph <br> Matching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Textual Encoder</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Textual Decoder</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Graph Transformer</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Hidden dim</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: left;">Heads</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Graph pooling</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">mean</td>
</tr>
<tr>
<td style="text-align: left;">Prefix</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">PE dim</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">Warmup epochs</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">$1 \mathrm{e}-1$</td>
<td style="text-align: center;">$1 \mathrm{e}-1$</td>
<td style="text-align: center;">$1 \mathrm{e}-1$</td>
<td style="text-align: center;">$1 \mathrm{e}-1$</td>
</tr>
<tr>
<td style="text-align: left;">Tunable parameters</td>
<td style="text-align: center;">0.0933 B</td>
<td style="text-align: center;">0.0933 B</td>
<td style="text-align: center;">0.0933 B</td>
<td style="text-align: center;">0.0933 B</td>
</tr>
</tbody>
</table>
<h2>C SUPPLEMENTAL EXPERIMENT RESULTS</h2>
<h2>C. 1 Inference Time</h2>
<p>In Table 6, we provide the inference time of different methods on LLaMA 2 7B and 13B. The results are calculated by taking the average inference time of all instances in the test set. From the results, we can observe that due to the reduction in context for graph reasoning tasks, GraphLLM exhibits a significant advantage in terms of inference time compared to Graph2Text-based methods. Furthermore, this advantage becomes more pronounced as the LLM's scale increases.</p>
<h2>C. 2 Ablation Study on Graph Transformer</h2>
<p>We experiment with different design choices on the structure understanding module. Specifically, we replace the aggregation mechanism via attention in graph transformer with other commonly used graph learning layer. Here we adopt GIN (Xu et al., 2019) and GAT (Veličković et al., 2018), while keeping the other modules unchanged in each case. Table 7 shows the experimental results on the four graph reasoning tasks. GIN variant and GAT variant only achieve average accuracies of $24.2 \%$ and $17.3 \%$, respectively. The significant disparity in accuracy between GIN variant, GAT variant,</p>
<p>Table 6: Inference time on the four graph reasoning tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Format</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">LLaMA2-7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA2-15B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Maximum Path <br> Sum</td>
<td style="text-align: center;">Substructure <br> Counting</td>
<td style="text-align: center;">Shortest <br> Path</td>
<td style="text-align: center;">Bipartite Graph <br> Matching</td>
<td style="text-align: center;">Maximum Path <br> Sum</td>
<td style="text-align: center;">Substructure <br> Counting</td>
<td style="text-align: center;">Shortest <br> Path</td>
<td style="text-align: center;">Bipartite Graph <br> Matching</td>
</tr>
<tr>
<td style="text-align: center;">Adjectory <br> List</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">0.1673</td>
<td style="text-align: center;">0.1541</td>
<td style="text-align: center;">0.2649</td>
<td style="text-align: center;">0.1385</td>
<td style="text-align: center;">0.2878</td>
<td style="text-align: center;">0.2670</td>
<td style="text-align: center;">0.4517</td>
<td style="text-align: center;">0.2402</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">0.4269</td>
<td style="text-align: center;">0.6506</td>
<td style="text-align: center;">0.5168</td>
<td style="text-align: center;">0.6116</td>
<td style="text-align: center;">0.7479</td>
<td style="text-align: center;">1.1150</td>
<td style="text-align: center;">0.8848</td>
<td style="text-align: center;">1.0604</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">4.9367</td>
<td style="text-align: center;">2.4122</td>
<td style="text-align: center;">5.5155</td>
<td style="text-align: center;">4.2542</td>
<td style="text-align: center;">8.2850</td>
<td style="text-align: center;">4.1148</td>
<td style="text-align: center;">9.2961</td>
<td style="text-align: center;">7.2886</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRAcatto)</td>
<td style="text-align: center;">0.1710</td>
<td style="text-align: center;">0.1568</td>
<td style="text-align: center;">0.2804</td>
<td style="text-align: center;">0.1420</td>
<td style="text-align: center;">0.2926</td>
<td style="text-align: center;">0.2698</td>
<td style="text-align: center;">0.4601</td>
<td style="text-align: center;">0.2455</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRAcatto+flu)</td>
<td style="text-align: center;">0.1818</td>
<td style="text-align: center;">0.1654</td>
<td style="text-align: center;">0.2842</td>
<td style="text-align: center;">0.1503</td>
<td style="text-align: center;">0.3071</td>
<td style="text-align: center;">0.2842</td>
<td style="text-align: center;">0.4813</td>
<td style="text-align: center;">0.2586</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prefix Tuning</td>
<td style="text-align: center;">0.1846</td>
<td style="text-align: center;">0.1694</td>
<td style="text-align: center;">0.2947</td>
<td style="text-align: center;">0.1519</td>
<td style="text-align: center;">0.3161</td>
<td style="text-align: center;">0.2910</td>
<td style="text-align: center;">0.4963</td>
<td style="text-align: center;">0.2603</td>
</tr>
<tr>
<td style="text-align: center;">Edge List <br> (Random Order)</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">0.1642</td>
<td style="text-align: center;">0.1447</td>
<td style="text-align: center;">0.2521</td>
<td style="text-align: center;">0.1486</td>
<td style="text-align: center;">0.2869</td>
<td style="text-align: center;">0.2508</td>
<td style="text-align: center;">0.4355</td>
<td style="text-align: center;">0.2573</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">0.4216</td>
<td style="text-align: center;">0.6259</td>
<td style="text-align: center;">0.4938</td>
<td style="text-align: center;">0.6560</td>
<td style="text-align: center;">0.7350</td>
<td style="text-align: center;">1.0678</td>
<td style="text-align: center;">0.8457</td>
<td style="text-align: center;">1.1473</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">4.8385</td>
<td style="text-align: center;">2.3190</td>
<td style="text-align: center;">5.3227</td>
<td style="text-align: center;">4.5724</td>
<td style="text-align: center;">8.1369</td>
<td style="text-align: center;">3.9703</td>
<td style="text-align: center;">9.0008</td>
<td style="text-align: center;">7.8130</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRAcatto)</td>
<td style="text-align: center;">0.1678</td>
<td style="text-align: center;">0.1465</td>
<td style="text-align: center;">0.2569</td>
<td style="text-align: center;">0.1511</td>
<td style="text-align: center;">0.2923</td>
<td style="text-align: center;">0.2539</td>
<td style="text-align: center;">0.4431</td>
<td style="text-align: center;">0.2622</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoRAcatto+flu)</td>
<td style="text-align: center;">0.1783</td>
<td style="text-align: center;">0.1556</td>
<td style="text-align: center;">0.2714</td>
<td style="text-align: center;">0.1597</td>
<td style="text-align: center;">0.3054</td>
<td style="text-align: center;">0.2670</td>
<td style="text-align: center;">0.4636</td>
<td style="text-align: center;">0.2758</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prefix Tuning</td>
<td style="text-align: center;">0.1777</td>
<td style="text-align: center;">0.1623</td>
<td style="text-align: center;">0.2757</td>
<td style="text-align: center;">0.1632</td>
<td style="text-align: center;">0.3080</td>
<td style="text-align: center;">0.2821</td>
<td style="text-align: center;">0.4724</td>
<td style="text-align: center;">0.2825</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphLLM</td>
<td style="text-align: center;">0.0449</td>
<td style="text-align: center;">0.0484</td>
<td style="text-align: center;">0.0734</td>
<td style="text-align: center;">0.0523</td>
<td style="text-align: center;">0.0583</td>
<td style="text-align: center;">0.0616</td>
<td style="text-align: center;">0.0937</td>
<td style="text-align: center;">0.0665</td>
</tr>
</tbody>
</table>
<p>and GraphLLM indicates that the practice of decoupling node information and structural information plays an essential role in improving GraphLLM's structure understanding ability, subsequently enhancing the graph reasoning capability.</p>
<p>Table 7: Ablation study on graph transformer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ablation</th>
<th style="text-align: center;">Maximum Triplet <br> Sum</th>
<th style="text-align: center;">Substructure <br> Counting</th>
<th style="text-align: center;">Shortest <br> Path</th>
<th style="text-align: center;">Bipartite Graph <br> Matching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GT $\rightarrow$ GINConv</td>
<td style="text-align: center;">$0.2237 \pm .0060$</td>
<td style="text-align: center;">$0.3878 \pm .0650$</td>
<td style="text-align: center;">$0.2122 \pm .0053$</td>
<td style="text-align: center;">$0.1427 \pm .0019$</td>
</tr>
<tr>
<td style="text-align: left;">GT $\rightarrow$ GATConv</td>
<td style="text-align: center;">$0.1819 \pm .0053$</td>
<td style="text-align: center;">$0.2598 \pm .0102$</td>
<td style="text-align: center;">$0.1443 \pm .0017$</td>
<td style="text-align: center;">$0.1052 \pm .0015$</td>
</tr>
<tr>
<td style="text-align: left;">GraphLLM</td>
<td style="text-align: center;">$0.9577 \pm .0058$</td>
<td style="text-align: center;">$0.9990 \pm .0007$</td>
<td style="text-align: center;">$0.9726 \pm .0011$</td>
<td style="text-align: center;">$0.9981 \pm .0015$</td>
</tr>
</tbody>
</table>
<h1>D Examples of Graph Reasoning Tasks</h1>
<h2>Substructure Counting</h2>
<h2>Input:</h2>
<p>Here are the descriptions of 15 atoms in a molecule.
Atom 1: The Carbon atom has an atomic number of 6 , denoted as "C". Carbon has an electronegativity value of approximately 3.25 . The covalent radius of a Carbon atom is about ...
Atom 2: The Carbon atom has an atomic number of 6 , denoted as "C". Carbon has an electronegativity value of approximately 2.16 . The covalent radius of a Carbon atom is about ...
Atom 3: The Oxygen atom has an atomic number of 8 , denoted as "O". Oxygen has an electronegativity value of approximately 3.52 . The covalent radius of a Oxygen atom is about ...</p>
<p>Atom 15: The Nitrogen atom has an atomic number of 7 , denoted as " N ". Nitrogen has an electronegativity value of approximately 2.95 . The covalent radius of a Nitrogen atom is ...
These atoms are connected as the following undirected graph to form the molecule:
Atom 1 is connected with: Atom 2, Atom 3, Atom 4, Atom 5, Atom 6, Atom 7, Atom 8.
Atom 2 is connected with: Atom 1, Atom 3, Atom 5, Atom 8.
Atom 3 is connected with: Atom 1, Atom 2.
...
Atom 15 is connected with: Atom 8, Atom 14.
How many carbon-carbon-oxygen triangles containing Atom 1 are in the molecule?</p>
<h2>Response on gpt-4 Failure Case :</h2>
<p>A carbon-carbon-oxygen triangle would consist of two carbon atoms and one oxygen atom. Since Atom 1 is a carbon atom and must be included in the triangle, we are looking for triangles where the other two atoms are one carbon and one oxygen.</p>
<p>Looking at the connections of Atom 1, it is connected to the following carbon atoms: Atom 2, Atom 4, Atom 6, Atom 7. It is also connected to the following oxygen atoms: Atom 3, Atom 5, Atom 8 .</p>
<p>Now we need to see if any of these carbon atoms are connected to any of these oxygen atoms.</p>
<ul>
<li>Atom 2 (carbon) is connected to Atom 3 (oxygen) and Atom 8 (oxygen).</li>
<li>Atom 4 (carbon) is not connected to any of the oxygen atoms connected to Atom 1.</li>
<li>Atom 6 (carbon) is not connected to any of the oxygen atoms connected to Atom 1.</li>
<li>Atom 7 (carbon) is not connected to any of the oxygen atoms connected to Atom 1.</li>
</ul>
<p>So, there are two carbon-carbon-oxygen triangles that contain Atom 1: Atom 1 - Atom 2 Atom 3 and Atom 1 - Atom 2 - Atom 8.</p>
<h2>Correct Answer:</h2>
<p>There are 3 C-C-O triangles containing Atom 1.</p>
<h1>Maximum Triplet Sum</h1>
<h2>Input:</h2>
<p>Here are the descriptions of 15 people.
Person 1: She is Wilma Lyons, and she is a sixty-year-old. With her colorful hair and unconventional fashion sense, she stands out as a true original. Her unassuming nature and humility create an environment ...</p>
<p>Person 2: Meet Manuel Cornelius, who is 30 years of age. With her adventurous spirit and love for the outdoors, she's always up for exploring new places and experiences. She possesses an air of sophistication and grace, seen in her timeless fashion ...</p>
<p>Person 15: Her petite frame and delicate features give her a dainty and ethereal presence. Her name is Cornelia Brooks, and she's forty years old. Her contagious enthusiasm and energy inspire others to ...</p>
<p>The relationship between them can be described as the following undirected graph:
Person 1 is connected with: Person 14, Person 8.
Person 2 is connected with: Person 11, Person 14, Person 15.
Person 3 is connected with: Person 11, Person 5, Person 13, Person 6.
...
Person 15 is connected with: Person 2, Person 7, Person 9, Person 10, Person 11.
Question: What is the maximum sum of age of a triplet composed of Person 1, her friends and friends of friends?</p>
<h2>Response on gpt-4 Failure Case :</h2>
<p>Person 1 is connected with Person 14 and Person 8.
Person 14 is connected with Person 1, Person 2, Person 7, Person 12, Person 13. Among these, Person 2, Person 7, and Person 12 are not connected with Person 1, so they can be the third person on the path. Their ages are 50,30 , and 60 respectively.</p>
<p>Person 8 is connected with Person 1, Person 11, Person 6. Among these, Person 11 and Person 6 are not connected with Person 1, so they can be the third person on the path. Their ages are 40 and 40 respectively.</p>
<p>So the maximum sum of the 3 people's age on the path is 60 (Person 1) +60 (Person 14) +60 (Person 12) $=180$.</p>
<h2>Correct Answer:</h2>
<p>The maximum sum is 160 .</p>
<h1>Shortest Path</h1>
<h2>Input:</h2>
<p>Here are the descriptions of 20 wormholes.
It is wormhole 1, and it is located in galaxy ARP 188. This wormhole is about 5463 light-years away from Earth and requires 20 pounds of dark matter to activate.
It is wormhole 2, and it is located in galaxy Horsehead Nebula. This wormhole is about 7606 light-years away from Earth and requires 20 pounds of dark matter to activate.
It is wormhole 3, and it is located in galaxy Large Magellanic Cloud. This wormhole is about 4214 light-years away from Earth and requires 40 pounds of dark matter to activate.
It is wormhole 4, and it is located in galaxy Pelican Nebula. This wormhole is about 3920 light-years away from Earth and requires 40 pounds of dark matter to activate.
...
It is wormhole 20, and it is located in galaxy Needle Galaxy. This wormhole is about 844 light-years away from Earth and requires 30 pounds of dark matter to activate.
These wormholes are connected as the following undirected graph:
Wormhole 1 is connected with: Wormhole 8, Wormhole 9, Wormhole 12, Wormhole 19.
Wormhole 2 is connected with: Wormhole 3, Wormhole 13, Wormhole 17, Wormhole 20.
Wormhole 3 is connected with: Wormhole 2, Wormhole 7, Wormhole 8, Wormhole 13.
...
Wormhole 20 is connected with: Wormhole 2, Wormhole 9, Wormhole 13.
Question: Starting from wormhole 1, How much dark matter we'll need at the minimum to reach Wormhole 2 ?</p>
<p>Response on gpt-4 Failure Case :
From the descriptions, we know that:</p>
<ul>
<li>Wormhole 1 requires 20 pounds of dark matter to activate.</li>
<li>Wormhole 8 requires 20 pounds of dark matter to activate.</li>
<li>Wormhole 3 requires 40 pounds of dark matter to activate.</li>
<li>Wormhole 2 requires 20 pounds of dark matter to activate.</li>
</ul>
<p>Adding these up, you need a total of $20+20+40+20=\mathbf{1 0 0}$ pounds of dark matter to reach wormhole 2 from wormhole 1 .</p>
<h2>Correct Answer:</h2>
<p>We need 80 pounds.</p>
<h1>Bipartite Graph Matching</h1>
<h2>Input:</h2>
<p>Here are the descriptions of 10 job applicants.
Applicant 1: She is Adam Lamarr, and she is 51 years old. She wants to find a job. She's an urban planner, designing sustainable cities, harmonizing architecture and environment for better living.</p>
<p>Applicant 10: He is Travis Wight, and he is 18 years old. He wants to find a job. He finds peace in practicing meditation and mindfulness, nurturing his well-being.
Here are the descriptions of 10 jobs.
Job 1: The average salary for a year is 47326.6 dollars, and it needs to work 38 hours every week. We want applicants with related experience.</p>
<p>Each applicant is interested in some of the jobs, and the relationship can be described as the following graph.
Applicant 1 is interested in: Job 9.
Applicant 2 is interested in: Job 6.
...
Applicant 10 is interested in: Job 6.
Each job can only accept one applicant and a job applicant can be appointed for only one job. For most how many applicants can find the job they are interested in?</p>
<h2>Response on gpt-4 Failure Case :</h2>
<p>First, let's look at the jobs that only have one applicant interested in them. These are Job 9, Job 4, and Job 10. Applicant 1, Applicant 3, Applicant 8, and Applicant 7 can be appointed to these jobs respectively.
Next, let's look at the jobs that have multiple applicants interested in them. These are Job 1, Job 5, Job 6, and Job 7.
For Job 1, the only remaining interested applicant is Applicant 7, so she can be appointed to this job.
For Job 5, the only interested applicant is Applicant 5, so he can be appointed to this job.
For Job 6, there are three interested applicants: Applicant 2, Applicant 5, and Applicant 9. Since Applicant 5 has already been appointed to Job 5, we can appoint Applicant 2 to Job 6.
For Job 7, the only interested applicant is Applicant 5, but he has already been appointed to Job 5. Therefore, Job 7 will remain vacant.</p>
<p>In conclusion, the maximum number of applicants that can find the job they are interested in is 6: Applicant 1, Applicant 3, Applicant 8, Applicant 7, Applicant 5, and Applicant 2.</p>
<h2>Correct Answer:</h2>
<p>5 Applicants.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The textual descriptions of the nodes are tokenized and embedded by LLM's tokenizer and frozen embedding table to align with the downstream frozen LLM.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
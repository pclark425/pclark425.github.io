<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6076 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6076</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6076</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-3015956a254139547cb350f5dbdd8edde298ac0d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3015956a254139547cb350f5dbdd8edde298ac0d" target="_blank">Accelerating clinical evidence synthesis with large language models</a></p>
                <p><strong>Paper Venue:</strong> npj Digital Medicine</p>
                <p><strong>Paper TL;DR:</strong> A generative artificial intelligence pipeline named TrialMind is proposed to streamline study search, study screening, and data extraction tasks in SR to show the promise of accelerating clinical evidence synthesis driven by human-AI collaboration.</p>
                <p><strong>Paper Abstract:</strong> Clinical evidence synthesis largely relies on systematic reviews (SR) of clinical studies from medical literature. Here, we propose a generative artificial intelligence (AI) pipeline named TrialMind to streamline study search, study screening, and data extraction tasks in SR. We chose published SRs to build TrialReviewBench, which contains 100 SRs and 2,220 clinical studies. For study search, it achieves high recall rates (Ours 0.711–0.834 v.s. Human baseline 0.138–0.232). For study screening, TrialMind beats previous document ranking methods in a 1.5–2.6 fold change. For data extraction, it outperforms a GPT-4’s accuracy by 16–32%. In a pilot study, human-AI collaboration with TrialMind improved recall by 71.4% and reduced screening time by 44.2%, while in data extraction, accuracy increased by 23.5% with a 63.4% time reduction. Medical experts preferred TrialMind’s synthesized evidence over GPT-4’s in 62.5%-100% of cases. These findings show the promise of accelerating clinical evidence synthesis driven by human-AI collaboration.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6076.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6076.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrialMind</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialMind (LLM-driven clinical evidence synthesis pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end, LLM-driven pipeline that decomposes clinical evidence synthesis into literature search, screening, data extraction, and evidence synthesis, combining retrieval-augmented generation, chain-of-thought prompting, and programmatic standardization to produce structured results for meta-analysis with human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TrialMind</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular LLM-driven workflow for clinical evidence synthesis: (1) generate and refine Boolean search queries from PICO inputs, (2) create eligibility criteria and predict criterion-level eligibility per study, aggregate those to rank candidate citations, (3) extract study characteristics and numeric results from full-text documents using retrieval-augmented prompts and chain-of-thought, and (4) standardize results for meta-analysis by auto-generating and executing Python code; all outputs are linked to source spans and editable by human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview) and Anthropic Sonnet (anthropic.claude-3-sonnet-20240229-v1:0) were used in the TrialMind experiments (GPT-4 as primary strong LLM and Sonnet as a comparison LLM for extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: research question specified as PICO elements (extracted from review abstracts) plus the full text of candidate studies (PDF/XML or user-uploaded content). Evaluation corpus: TrialReviewBench containing 100 systematic reviews and 2,220 associated clinical studies; screening candidate sets ≈2,000 studies per review for ranking experiments; search stage retrieved up to tens of thousands of candidate citations (e.g., 22,084 for some topics).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Pipeline decomposition using LLM prompts with in-context learning (ICL), retrieval-augmented generation (RAG) to enrich prompts with retrieved abstracts, and chain-of-thought (CoT) prompting for multi-step refinement; outputs include generated boolean queries (query generation + augmentation + refinement), criterion-level eligibility predictions (structured {-1,0,1} per criterion), targeted information extraction (field descriptions drive zero-shot extraction), and a specialized result-extraction sub-pipeline that elicits raw content and numerical values then generates Python code to standardize numerical results for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured artifacts: boolean search queries; sets of editable eligibility criteria; per-study criterion-level labels and aggregated relevance scores; extracted structured fields (study design, population baselines, numerical results) with source span indices; standardized numeric tables ready for meta-analysis (forest plots), and generated code used for calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Quantitative benchmarking on TrialReviewBench: (a) literature search measured by Recall (proportion of ground-truth review studies retrieved); (b) study screening measured by Recall@K (K=20,50,100,200); (c) data/result extraction measured by Accuracy against manually curated study characteristic tables and result annotations; confusion-matrix analysis for hallucination (false positives) and missing data (false negatives); error taxonomy labeling (Inaccurate, Extraction failure, Unavailable, Hallucination); human evaluation of synthesized forest plots by domain annotators (win/lose and 1–5 rating); and user studies comparing AI+Human vs Human-only measuring recall lift and time savings.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Search: TrialMind average Recall = 0.782 vs GPT-4 baseline 0.073 and Human baseline 0.187; topic recalls ranged 0.711–0.834 (e.g., Immunotherapy Recall=0.797 for N=22,084 retrieved). Screening: Recall@20/50 fold-improvements over best baselines ranged 1.3–2.6; examples: Hormone Therapy Recall@20=0.431, Recall@50=0.674; TrialMind captured >80% of target studies when K=100 and on average 43% of target studies in top 50. Data extraction (study characteristics): Accuracy by topic — Immunotherapy 0.78 (95% CI 0.75–0.81), Radiation/Chemotherapy 0.77 (0.72–0.82), Hormone Therapy 0.72 (0.63–0.80), Hyperthermia 0.83 (0.74–0.90). Per-field precision and recall: study design precision 0.994, recall 0.946; population precision 0.966, recall 0.889; results precision 0.862, recall 0.930. Result extraction (numeric outcomes): TrialMind vs GPT-4 accuracies — Immunotherapy 0.70 vs 0.54; Radiation/Chemotherapy 0.65 vs 0.52; Hormone Therapy 0.80 vs 0.50; Hyperthermia 0.84 vs 0.52. Error counts: Inaccurate n=36, Extraction failure n=27, Unavailable n=10, Hallucinations n=3. Human evaluation (forest plots): TrialMind winning rates across five benchmark review studies: 87.5%, 100%, 62.5%, 62.5%, and 81.2%; rating examples: Study #1 mean rating TrialMind 4.25 vs baseline 3.50. User study: AI+Human screening achieved 71.4% relative recall improvement and 44.2% time savings versus Human-only; AI+Human data extraction achieved 23.5% accuracy lift and 63.4% time savings.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>TrialReviewBench (created in this study): 100 systematic reviews, 2,220 studies, 1,334 study characteristic annotations, 1,049 study result annotations; PubMed / PubMed Central used as literature sources for retrieval and document full-text (PDF/XML).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations: residual LLM errors (hallucinations, inaccurate numeric reasoning), difficulty extracting numeric results (locating correct sections, cohort-specific counts, and required calculations), potential redundancy among generated criteria, reliance on prompt engineering (no fine-tuning), dataset limited in size (costs of human labeling), restricted coverage to PubMed Central full-text availability (missing paywalled sources/OCR needs), costs and runtime of large LLMs (GPT-4) may be bottlenecks; human oversight required for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared against: (1) GPT-4 naive prompting baseline for query generation (Recall 0.073) and for extraction (lower accuracies); (2) Human baseline using manual PICO-based term expansion with UMLS (search Recall 0.187); (3) embedding-based ranking baselines MPNet and MedCPT (ranking by cosine similarity) — these performed poorly (MPNet similar to Random for Recall@20, MedCPT marginally better but both substantially worse than TrialMind); (4) GPT-4+Human baseline for evidence synthesis (annotators manually extracted and standardized GPT-4 outputs) — TrialMind outperformed in synthesized evidence quality and required less human time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6076.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrialReviewBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialReviewBench dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated benchmark created in this study for evaluating LLM-based clinical evidence synthesis, consisting of 100 systematic reviews and 2,220 associated clinical studies with manual annotations of study characteristics and study results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TrialReviewBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-specific evaluation corpus for literature search, screening and data/result extraction tasks: includes PICO-extracted research questions (from review abstracts), lists of ground-truth included studies (PubMed IDs), full-text documents when available, and manual annotations: 1,334 study characteristic fields and 1,049 study result entries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used to evaluate GPT-4 and Sonnet and the TrialMind pipeline (dataset itself is not an LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>100 systematic reviews (titles and abstracts used to derive PICO), 2,220 target studies retrieved and downloaded (PDF/XML where available); per-review candidate pools up to thousands of retrieved citations (used in search and screening experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Benchmark supports evaluation of multiple LLM-driven distillation tasks: boolean-query generation, candidate retrieval recall, eligibility-based ranking, structured information extraction, and numeric result standardization for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Annotated ground truth for search (lists of included PMIDs), screening (inclusion labels), structured study characteristic tables, and standardized numeric results for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Used to compute Search Recall, Recall@K for screening, Accuracy for data/result extraction, confusion metrics for hallucinations/missing data, and to support human evaluation and user studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Served as the primary testbed where TrialMind achieved high search recall (avg 0.782), improved screening Recall@K vs baselines, and higher extraction accuracies versus generalist LLM prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Built from PubMed/PubMed Central meta-analyses focused on cancer therapies (topics: Immunotherapy, Radiation/Chemotherapy, Hormone Therapy, Hyperthermia).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Dataset limited to PMC-available full-texts, covers cancer therapy topics only, and size constrained by manual annotation costs; does not include paywalled sources or OCR-processed papers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Used to compare TrialMind against GPT-4 naive prompting, GPT-4+Human, Sonnet, MPNet and MedCPT embedding baselines, and Human (manual query) baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6076.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM prompts with context dynamically retrieved from external knowledge sources (here: PubMed abstracts/full-text) to reduce hallucination and provide up-to-date, document-specific context during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>In TrialMind, RAG retrieves abstracts and relevant document snippets based on PICO-derived queries and injects them into LLM prompts to improve boolean query refinement and extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied together with GPT-4 and Sonnet within TrialMind prompt flow.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs to the retriever: PICO elements and initial candidate abstracts (from PubMed); Retriever returns sets of abstracts/snippets that are included in the LLM prompt context (used during query refinement and extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Distillation via context expansion — retrieved study abstracts and document segments are included in prompts so the LLM grounds generations on retrieved evidence rather than relying solely on model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Enriched prompts leading to: broader/augmented boolean queries, improved identification of relevant sections for extraction, and fewer hallucinations in extracted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>RAG's contribution is evaluated indirectly via overall TrialMind performance (search Recall, extraction accuracy), and by comparing to LLM direct prompting baselines (GPT-4 baseline without RAG) which produced much lower search recall.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>When integrated in the query-generation + refinement loop, TrialMind achieved a search Recall of 0.782 compared to the GPT-4 direct prompting baseline Recall of 0.073; RAG is reported as a key design choice to mitigate LLM omissions and hallucinations in search and extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PubMed abstracts and retrieved candidate abstracts from search iterations (used as RAG context).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes RAG mitigates but does not eliminate hallucinations and that RAG relies on quality of retriever and available corpus (limited to PMC in this study). No ablation isolating RAG contribution was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared qualitatively to direct LLM prompting (GPT-4 baseline) which lacked retrieval context and produced low recall queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6076.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks the LLM to produce intermediate step-by-step reasoning traces to improve performance on multi-step and reasoning-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used in TrialMind for multi-step tasks: (a) query refinement (produce sets of term lists, filtered subsets, and self-reflective augmentations in one pass), and (b) result extraction (identify raw content then elicit numerical values via stepwise reasoning in the same inference).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied with GPT-4 and Sonnet within TrialMind prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: PICO and retrieved abstracts (for query CoT) or full study content plus outcome/cohort descriptions (for result-extraction CoT); CoT prompts request multiple intermediate sub-outputs per inference.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Elicits stepwise decomposition of tasks to improve identification of relevant text spans and derivation of numeric values; CoT outputs intermediate artifacts (e.g., {X_S^1, X_S^2, X_S^3}) that are then used downstream (e.g., code generation for standardization).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured intermediate reasoning outputs (lists of candidate terms, filtered/augmented terms, raw text snippets for outcomes, extracted numeric tokens) that improve final structured extraction and query completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Contribution assessed through improved final metrics of the TrialMind pipeline relative to non-CoT simple prompting baselines (e.g., the GPT-4 naive baseline for query generation and extraction performed poorly). No isolated CoT ablation reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>CoT is credited as an enabling technique in TrialMind's high performance: e.g., improved query refinement leading to high search recall and improved result-extraction accuracy enabling TrialMind to outperform GPT-4 naive prompting by substantial margins (result extraction fold-changes median 1.50).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Applied on TrialReviewBench inputs (PICO + candidate abstracts or full texts).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No standalone numeric quantification of CoT's incremental benefit; CoT may increase prompt complexity and token usage (cost) and still require human verification for numeric reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6076.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method of steering LLM behavior by providing task definitions and examples within the prompt at inference time rather than by fine-tuning model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used within TrialMind to provide task-specific instructions and examples (task prompt T includes definition, input/output formats) so the LLM can perform zero-shot or few-shot extraction, query formulation, and screening without training.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied with GPT-4 and Sonnet.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Prompts include the task definition T, examples, and the specific input X (PICO, study text); used across tasks including query generation, eligibility criteria generation, and data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Zero-shot/few-shot adaptation: LLM is given examples and task templates in prompt P(T,X) to produce structured outputs for synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-aligned outputs such as boolean queries, eligibility criteria lists, per-criterion labels, and extracted structured fields.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>ICL is a core prompting strategy used throughout; effectiveness is reflected indirectly in TrialMind's end-task metrics versus naive prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>As part of the combined prompting strategy, ICL enabled TrialMind to produce structured, editable outputs that contributed to strong empirical results (search recall 0.782, high extraction accuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Evaluated on TrialReviewBench with many prompt templates per subtask (detailed prompts referenced in Extended Figures).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relies on high-quality prompt examples and can be sensitive to prompt engineering; no model fine-tuning performed, so limits inherent to base LLM remain (e.g., numeric reasoning errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6076.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Result-extraction (code-gen) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Result extraction with CoT plus programmatic standardization (Python code generation and execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized sub-pipeline in TrialMind that elicits raw outcome text and numerical tokens via chain-of-thought, then generates and executes Python code to perform deterministic calculations and produce standardized numeric tables for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Result extraction + code-generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-pass CoT extraction (identify raw content and extract numeric values), followed by LLM generation of Python code (T_PY instructions) that is executed with the extracted intermediate values to produce standardized outputs; outputs include provenance links to source spans for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented using GPT-4 (and evaluated against GPT-4 naive baseline and Sonnet).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: full study text F, natural-language description of the target clinical endpoint O, and cohort definition; applied across TrialReviewBench studies (1,049 result annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Combine CoT elicitation of evidence with programmatic post-processing: LLM first produces raw content and structured numeric tokens, then emits code which when executed performs arithmetic/normalization to standardize results (e.g., event counts, rates) for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Standardized numeric rows/columns suitable for meta-analysis (tabular numeric results) plus the Python code used and pointers to source text for auditability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Accuracy of standardized results vs manual annotations; error analysis categorizing Inaccurate/Extraction failure/Unavailable/Hallucination; human evaluation of final synthesized forest plots.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>This pipeline improved robustness of numeric result extraction and standardization: TrialMind result extraction accuracies by topic (0.70 to 0.84) substantially exceeded GPT-4 naive extraction (0.50–0.54); error cases were predominantly 'Inaccurate' or 'Extraction failure' rather than hallucinations, and provenance linking made corrections easier.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Applied to 1,049 annotated study results in TrialReviewBench and five systematic review benchmarks for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Challenges include locating correct sections in long papers, extracting cohort-specific counts, and ensuring correct calculations; generation/execution of code requires careful sandboxing and human verification to prevent propagation of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6076.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, general-purpose generative language model from OpenAI used in this study both as a component within TrialMind and as multiple baselines (naive prompting and GPT-4+Human pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (used as model in pipeline and baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used for prompt-driven query generation, eligibility prediction, extraction, CoT reasoning, and code generation in TrialMind; also used as a naive baseline (direct prompting) and as part of a human-in-the-loop baseline where humans post-processed GPT-4 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4-0125-preview (version specified in Methods).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: PICO elements, abstracts, and full texts (long-context capability used); baseline experiments used direct prompts over single paper contents.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>When used as a naive baseline: simple direct prompting for query generation and extraction without RAG/structured CoT and without the TrialMind orchestration; in TrialMind, GPT-4 participates in ICL+RAG+CoT pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Free-text queries or extracted text snippets (baseline) or structured outputs when used within TrialMind's prompt templates; also used to generate Python code in TrialMind.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>As a baseline its performance was compared on the same metrics (Search Recall, Recall@K, extraction Accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>GPT-4 naive query-generation baseline Recall = 0.073 (vs TrialMind 0.782); GPT-4 naive result-extraction accuracies ranged ~0.50–0.54 across topics, substantially below TrialMind's numbers; GPT-4 when used inside TrialMind contributed to high overall performance but naive prompting led to many missed/incorrect extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Evaluated on TrialReviewBench and the user study/syntheses described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Observed weaknesses when used naively: low recall in query generation, difficulty identifying correct source spans for numeric outcomes, hallucinations in result interpretation; needs RAG/CoT/structured prompting and human oversight to achieve reliable results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6076.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6076.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Sonnet (Claude 3 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model from Anthropic (Claude 3 Sonnet variant) included as a comparison LLM for extraction tasks in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sonnet (Anthropic Claude 3 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as a generalist LLM baseline for extraction tasks (prompted to extract target outcomes from full study content) and compared to TrialMind's specialized pipeline and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>anthropic.claude-3-sonnet-20240229-v1:0 (on AWS Bedrock as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: full paper content for result extraction baseline prompts (same trial documents in TrialReviewBench used).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Naive direct prompting for extraction (free-text outputs), with subsequent manual conversion to numeric formats by annotators for evaluation — thus Sonnet served as a baseline generalist LLM extraction approach.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Free-text extracted descriptions of outcomes (manually converted to numeric standardized values by annotators for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compared against TrialMind extraction accuracies and GPT-4 naive baseline using the TrialReviewBench annotations and accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Paper reports that TrialMind outperformed generalist LLM baselines (GPT-4 naive and Sonnet) on result extraction accuracy; specific Sonnet numeric accuracies not tabulated separately in main text, but Sonnet was one of the baselines TrialMind beat by substantial margins.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>TrialReviewBench (result extraction subset; 1,049 result annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Like GPT-4 naive prompting, Sonnet baseline required manual post-processing and suffered from failures to identify correct data locations and to standardize numeric outputs without human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Clinical Texts / MedCPT <em>(Rating: 1)</em></li>
                <li>MPNet: Masked and Permuted Pre-training for Language Understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6076",
    "paper_id": "paper-3015956a254139547cb350f5dbdd8edde298ac0d",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "TrialMind",
            "name_full": "TrialMind (LLM-driven clinical evidence synthesis pipeline)",
            "brief_description": "An end-to-end, LLM-driven pipeline that decomposes clinical evidence synthesis into literature search, screening, data extraction, and evidence synthesis, combining retrieval-augmented generation, chain-of-thought prompting, and programmatic standardization to produce structured results for meta-analysis with human-in-the-loop verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TrialMind",
            "system_description": "A modular LLM-driven workflow for clinical evidence synthesis: (1) generate and refine Boolean search queries from PICO inputs, (2) create eligibility criteria and predict criterion-level eligibility per study, aggregate those to rank candidate citations, (3) extract study characteristics and numeric results from full-text documents using retrieval-augmented prompts and chain-of-thought, and (4) standardize results for meta-analysis by auto-generating and executing Python code; all outputs are linked to source spans and editable by human experts.",
            "llm_model_used": "GPT-4 (gpt-4-0125-preview) and Anthropic Sonnet (anthropic.claude-3-sonnet-20240229-v1:0) were used in the TrialMind experiments (GPT-4 as primary strong LLM and Sonnet as a comparison LLM for extraction).",
            "input_type_and_size": "Inputs: research question specified as PICO elements (extracted from review abstracts) plus the full text of candidate studies (PDF/XML or user-uploaded content). Evaluation corpus: TrialReviewBench containing 100 systematic reviews and 2,220 associated clinical studies; screening candidate sets ≈2,000 studies per review for ranking experiments; search stage retrieved up to tens of thousands of candidate citations (e.g., 22,084 for some topics).",
            "distillation_approach": "Pipeline decomposition using LLM prompts with in-context learning (ICL), retrieval-augmented generation (RAG) to enrich prompts with retrieved abstracts, and chain-of-thought (CoT) prompting for multi-step refinement; outputs include generated boolean queries (query generation + augmentation + refinement), criterion-level eligibility predictions (structured {-1,0,1} per criterion), targeted information extraction (field descriptions drive zero-shot extraction), and a specialized result-extraction sub-pipeline that elicits raw content and numerical values then generates Python code to standardize numerical results for meta-analysis.",
            "output_type": "Structured artifacts: boolean search queries; sets of editable eligibility criteria; per-study criterion-level labels and aggregated relevance scores; extracted structured fields (study design, population baselines, numerical results) with source span indices; standardized numeric tables ready for meta-analysis (forest plots), and generated code used for calculations.",
            "evaluation_methods": "Quantitative benchmarking on TrialReviewBench: (a) literature search measured by Recall (proportion of ground-truth review studies retrieved); (b) study screening measured by Recall@K (K=20,50,100,200); (c) data/result extraction measured by Accuracy against manually curated study characteristic tables and result annotations; confusion-matrix analysis for hallucination (false positives) and missing data (false negatives); error taxonomy labeling (Inaccurate, Extraction failure, Unavailable, Hallucination); human evaluation of synthesized forest plots by domain annotators (win/lose and 1–5 rating); and user studies comparing AI+Human vs Human-only measuring recall lift and time savings.",
            "results": "Search: TrialMind average Recall = 0.782 vs GPT-4 baseline 0.073 and Human baseline 0.187; topic recalls ranged 0.711–0.834 (e.g., Immunotherapy Recall=0.797 for N=22,084 retrieved). Screening: Recall@20/50 fold-improvements over best baselines ranged 1.3–2.6; examples: Hormone Therapy Recall@20=0.431, Recall@50=0.674; TrialMind captured &gt;80% of target studies when K=100 and on average 43% of target studies in top 50. Data extraction (study characteristics): Accuracy by topic — Immunotherapy 0.78 (95% CI 0.75–0.81), Radiation/Chemotherapy 0.77 (0.72–0.82), Hormone Therapy 0.72 (0.63–0.80), Hyperthermia 0.83 (0.74–0.90). Per-field precision and recall: study design precision 0.994, recall 0.946; population precision 0.966, recall 0.889; results precision 0.862, recall 0.930. Result extraction (numeric outcomes): TrialMind vs GPT-4 accuracies — Immunotherapy 0.70 vs 0.54; Radiation/Chemotherapy 0.65 vs 0.52; Hormone Therapy 0.80 vs 0.50; Hyperthermia 0.84 vs 0.52. Error counts: Inaccurate n=36, Extraction failure n=27, Unavailable n=10, Hallucinations n=3. Human evaluation (forest plots): TrialMind winning rates across five benchmark review studies: 87.5%, 100%, 62.5%, 62.5%, and 81.2%; rating examples: Study #1 mean rating TrialMind 4.25 vs baseline 3.50. User study: AI+Human screening achieved 71.4% relative recall improvement and 44.2% time savings versus Human-only; AI+Human data extraction achieved 23.5% accuracy lift and 63.4% time savings.",
            "datasets_or_benchmarks": "TrialReviewBench (created in this study): 100 systematic reviews, 2,220 studies, 1,334 study characteristic annotations, 1,049 study result annotations; PubMed / PubMed Central used as literature sources for retrieval and document full-text (PDF/XML).",
            "challenges_or_limitations": "Reported limitations: residual LLM errors (hallucinations, inaccurate numeric reasoning), difficulty extracting numeric results (locating correct sections, cohort-specific counts, and required calculations), potential redundancy among generated criteria, reliance on prompt engineering (no fine-tuning), dataset limited in size (costs of human labeling), restricted coverage to PubMed Central full-text availability (missing paywalled sources/OCR needs), costs and runtime of large LLMs (GPT-4) may be bottlenecks; human oversight required for verification.",
            "comparisons_to_other_methods": "Compared against: (1) GPT-4 naive prompting baseline for query generation (Recall 0.073) and for extraction (lower accuracies); (2) Human baseline using manual PICO-based term expansion with UMLS (search Recall 0.187); (3) embedding-based ranking baselines MPNet and MedCPT (ranking by cosine similarity) — these performed poorly (MPNet similar to Random for Recall@20, MedCPT marginally better but both substantially worse than TrialMind); (4) GPT-4+Human baseline for evidence synthesis (annotators manually extracted and standardized GPT-4 outputs) — TrialMind outperformed in synthesized evidence quality and required less human time.",
            "uuid": "e6076.0",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TrialReviewBench",
            "name_full": "TrialReviewBench dataset",
            "brief_description": "A curated benchmark created in this study for evaluating LLM-based clinical evidence synthesis, consisting of 100 systematic reviews and 2,220 associated clinical studies with manual annotations of study characteristics and study results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TrialReviewBench",
            "system_description": "A domain-specific evaluation corpus for literature search, screening and data/result extraction tasks: includes PICO-extracted research questions (from review abstracts), lists of ground-truth included studies (PubMed IDs), full-text documents when available, and manual annotations: 1,334 study characteristic fields and 1,049 study result entries.",
            "llm_model_used": "Used to evaluate GPT-4 and Sonnet and the TrialMind pipeline (dataset itself is not an LLM).",
            "input_type_and_size": "100 systematic reviews (titles and abstracts used to derive PICO), 2,220 target studies retrieved and downloaded (PDF/XML where available); per-review candidate pools up to thousands of retrieved citations (used in search and screening experiments).",
            "distillation_approach": "Benchmark supports evaluation of multiple LLM-driven distillation tasks: boolean-query generation, candidate retrieval recall, eligibility-based ranking, structured information extraction, and numeric result standardization for meta-analysis.",
            "output_type": "Annotated ground truth for search (lists of included PMIDs), screening (inclusion labels), structured study characteristic tables, and standardized numeric results for meta-analysis.",
            "evaluation_methods": "Used to compute Search Recall, Recall@K for screening, Accuracy for data/result extraction, confusion metrics for hallucinations/missing data, and to support human evaluation and user studies.",
            "results": "Served as the primary testbed where TrialMind achieved high search recall (avg 0.782), improved screening Recall@K vs baselines, and higher extraction accuracies versus generalist LLM prompting baselines.",
            "datasets_or_benchmarks": "Built from PubMed/PubMed Central meta-analyses focused on cancer therapies (topics: Immunotherapy, Radiation/Chemotherapy, Hormone Therapy, Hyperthermia).",
            "challenges_or_limitations": "Dataset limited to PMC-available full-texts, covers cancer therapy topics only, and size constrained by manual annotation costs; does not include paywalled sources or OCR-processed papers.",
            "comparisons_to_other_methods": "Used to compare TrialMind against GPT-4 naive prompting, GPT-4+Human, Sonnet, MPNet and MedCPT embedding baselines, and Human (manual query) baselines.",
            "uuid": "e6076.1",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A method that augments LLM prompts with context dynamically retrieved from external knowledge sources (here: PubMed abstracts/full-text) to reduce hallucination and provide up-to-date, document-specific context during inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation",
            "system_description": "In TrialMind, RAG retrieves abstracts and relevant document snippets based on PICO-derived queries and injects them into LLM prompts to improve boolean query refinement and extraction accuracy.",
            "llm_model_used": "Applied together with GPT-4 and Sonnet within TrialMind prompt flow.",
            "input_type_and_size": "Inputs to the retriever: PICO elements and initial candidate abstracts (from PubMed); Retriever returns sets of abstracts/snippets that are included in the LLM prompt context (used during query refinement and extraction).",
            "distillation_approach": "Distillation via context expansion — retrieved study abstracts and document segments are included in prompts so the LLM grounds generations on retrieved evidence rather than relying solely on model priors.",
            "output_type": "Enriched prompts leading to: broader/augmented boolean queries, improved identification of relevant sections for extraction, and fewer hallucinations in extracted outputs.",
            "evaluation_methods": "RAG's contribution is evaluated indirectly via overall TrialMind performance (search Recall, extraction accuracy), and by comparing to LLM direct prompting baselines (GPT-4 baseline without RAG) which produced much lower search recall.",
            "results": "When integrated in the query-generation + refinement loop, TrialMind achieved a search Recall of 0.782 compared to the GPT-4 direct prompting baseline Recall of 0.073; RAG is reported as a key design choice to mitigate LLM omissions and hallucinations in search and extraction.",
            "datasets_or_benchmarks": "PubMed abstracts and retrieved candidate abstracts from search iterations (used as RAG context).",
            "challenges_or_limitations": "Paper notes RAG mitigates but does not eliminate hallucinations and that RAG relies on quality of retriever and available corpus (limited to PMC in this study). No ablation isolating RAG contribution was reported.",
            "comparisons_to_other_methods": "Compared qualitatively to direct LLM prompting (GPT-4 baseline) which lacked retrieval context and produced low recall queries.",
            "uuid": "e6076.2",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique that asks the LLM to produce intermediate step-by-step reasoning traces to improve performance on multi-step and reasoning-intensive tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "Used in TrialMind for multi-step tasks: (a) query refinement (produce sets of term lists, filtered subsets, and self-reflective augmentations in one pass), and (b) result extraction (identify raw content then elicit numerical values via stepwise reasoning in the same inference).",
            "llm_model_used": "Applied with GPT-4 and Sonnet within TrialMind prompts.",
            "input_type_and_size": "Inputs: PICO and retrieved abstracts (for query CoT) or full study content plus outcome/cohort descriptions (for result-extraction CoT); CoT prompts request multiple intermediate sub-outputs per inference.",
            "distillation_approach": "Elicits stepwise decomposition of tasks to improve identification of relevant text spans and derivation of numeric values; CoT outputs intermediate artifacts (e.g., {X_S^1, X_S^2, X_S^3}) that are then used downstream (e.g., code generation for standardization).",
            "output_type": "Structured intermediate reasoning outputs (lists of candidate terms, filtered/augmented terms, raw text snippets for outcomes, extracted numeric tokens) that improve final structured extraction and query completeness.",
            "evaluation_methods": "Contribution assessed through improved final metrics of the TrialMind pipeline relative to non-CoT simple prompting baselines (e.g., the GPT-4 naive baseline for query generation and extraction performed poorly). No isolated CoT ablation reported in paper.",
            "results": "CoT is credited as an enabling technique in TrialMind's high performance: e.g., improved query refinement leading to high search recall and improved result-extraction accuracy enabling TrialMind to outperform GPT-4 naive prompting by substantial margins (result extraction fold-changes median 1.50).",
            "datasets_or_benchmarks": "Applied on TrialReviewBench inputs (PICO + candidate abstracts or full texts).",
            "challenges_or_limitations": "No standalone numeric quantification of CoT's incremental benefit; CoT may increase prompt complexity and token usage (cost) and still require human verification for numeric reasoning errors.",
            "uuid": "e6076.3",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ICL",
            "name_full": "In-Context Learning (ICL)",
            "brief_description": "A method of steering LLM behavior by providing task definitions and examples within the prompt at inference time rather than by fine-tuning model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "In-Context Learning",
            "system_description": "Used within TrialMind to provide task-specific instructions and examples (task prompt T includes definition, input/output formats) so the LLM can perform zero-shot or few-shot extraction, query formulation, and screening without training.",
            "llm_model_used": "Applied with GPT-4 and Sonnet.",
            "input_type_and_size": "Prompts include the task definition T, examples, and the specific input X (PICO, study text); used across tasks including query generation, eligibility criteria generation, and data extraction.",
            "distillation_approach": "Zero-shot/few-shot adaptation: LLM is given examples and task templates in prompt P(T,X) to produce structured outputs for synthesis tasks.",
            "output_type": "Task-aligned outputs such as boolean queries, eligibility criteria lists, per-criterion labels, and extracted structured fields.",
            "evaluation_methods": "ICL is a core prompting strategy used throughout; effectiveness is reflected indirectly in TrialMind's end-task metrics versus naive prompting baselines.",
            "results": "As part of the combined prompting strategy, ICL enabled TrialMind to produce structured, editable outputs that contributed to strong empirical results (search recall 0.782, high extraction accuracies).",
            "datasets_or_benchmarks": "Evaluated on TrialReviewBench with many prompt templates per subtask (detailed prompts referenced in Extended Figures).",
            "challenges_or_limitations": "Relies on high-quality prompt examples and can be sensitive to prompt engineering; no model fine-tuning performed, so limits inherent to base LLM remain (e.g., numeric reasoning errors).",
            "uuid": "e6076.4",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Result-extraction (code-gen) pipeline",
            "name_full": "Result extraction with CoT plus programmatic standardization (Python code generation and execution)",
            "brief_description": "A specialized sub-pipeline in TrialMind that elicits raw outcome text and numerical tokens via chain-of-thought, then generates and executes Python code to perform deterministic calculations and produce standardized numeric tables for meta-analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Result extraction + code-generation pipeline",
            "system_description": "Two-pass CoT extraction (identify raw content and extract numeric values), followed by LLM generation of Python code (T_PY instructions) that is executed with the extracted intermediate values to produce standardized outputs; outputs include provenance links to source spans for verification.",
            "llm_model_used": "Implemented using GPT-4 (and evaluated against GPT-4 naive baseline and Sonnet).",
            "input_type_and_size": "Inputs: full study text F, natural-language description of the target clinical endpoint O, and cohort definition; applied across TrialReviewBench studies (1,049 result annotations).",
            "distillation_approach": "Combine CoT elicitation of evidence with programmatic post-processing: LLM first produces raw content and structured numeric tokens, then emits code which when executed performs arithmetic/normalization to standardize results (e.g., event counts, rates) for meta-analysis.",
            "output_type": "Standardized numeric rows/columns suitable for meta-analysis (tabular numeric results) plus the Python code used and pointers to source text for auditability.",
            "evaluation_methods": "Accuracy of standardized results vs manual annotations; error analysis categorizing Inaccurate/Extraction failure/Unavailable/Hallucination; human evaluation of final synthesized forest plots.",
            "results": "This pipeline improved robustness of numeric result extraction and standardization: TrialMind result extraction accuracies by topic (0.70 to 0.84) substantially exceeded GPT-4 naive extraction (0.50–0.54); error cases were predominantly 'Inaccurate' or 'Extraction failure' rather than hallucinations, and provenance linking made corrections easier.",
            "datasets_or_benchmarks": "Applied to 1,049 annotated study results in TrialReviewBench and five systematic review benchmarks for human evaluation.",
            "challenges_or_limitations": "Challenges include locating correct sections in long papers, extracting cohort-specific counts, and ensuring correct calculations; generation/execution of code requires careful sandboxing and human verification to prevent propagation of errors.",
            "uuid": "e6076.5",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A large, general-purpose generative language model from OpenAI used in this study both as a component within TrialMind and as multiple baselines (naive prompting and GPT-4+Human pipelines).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (used as model in pipeline and baseline)",
            "system_description": "Used for prompt-driven query generation, eligibility prediction, extraction, CoT reasoning, and code generation in TrialMind; also used as a naive baseline (direct prompting) and as part of a human-in-the-loop baseline where humans post-processed GPT-4 outputs.",
            "llm_model_used": "gpt-4-0125-preview (version specified in Methods).",
            "input_type_and_size": "Inputs: PICO elements, abstracts, and full texts (long-context capability used); baseline experiments used direct prompts over single paper contents.",
            "distillation_approach": "When used as a naive baseline: simple direct prompting for query generation and extraction without RAG/structured CoT and without the TrialMind orchestration; in TrialMind, GPT-4 participates in ICL+RAG+CoT pipeline.",
            "output_type": "Free-text queries or extracted text snippets (baseline) or structured outputs when used within TrialMind's prompt templates; also used to generate Python code in TrialMind.",
            "evaluation_methods": "As a baseline its performance was compared on the same metrics (Search Recall, Recall@K, extraction Accuracy).",
            "results": "GPT-4 naive query-generation baseline Recall = 0.073 (vs TrialMind 0.782); GPT-4 naive result-extraction accuracies ranged ~0.50–0.54 across topics, substantially below TrialMind's numbers; GPT-4 when used inside TrialMind contributed to high overall performance but naive prompting led to many missed/incorrect extractions.",
            "datasets_or_benchmarks": "Evaluated on TrialReviewBench and the user study/syntheses described in the paper.",
            "challenges_or_limitations": "Observed weaknesses when used naively: low recall in query generation, difficulty identifying correct source spans for numeric outcomes, hallucinations in result interpretation; needs RAG/CoT/structured prompting and human oversight to achieve reliable results.",
            "uuid": "e6076.6",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Sonnet",
            "name_full": "Anthropic Sonnet (Claude 3 Sonnet)",
            "brief_description": "A large language model from Anthropic (Claude 3 Sonnet variant) included as a comparison LLM for extraction tasks in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Sonnet (Anthropic Claude 3 Sonnet)",
            "system_description": "Used as a generalist LLM baseline for extraction tasks (prompted to extract target outcomes from full study content) and compared to TrialMind's specialized pipeline and GPT-4.",
            "llm_model_used": "anthropic.claude-3-sonnet-20240229-v1:0 (on AWS Bedrock as reported).",
            "input_type_and_size": "Inputs: full paper content for result extraction baseline prompts (same trial documents in TrialReviewBench used).",
            "distillation_approach": "Naive direct prompting for extraction (free-text outputs), with subsequent manual conversion to numeric formats by annotators for evaluation — thus Sonnet served as a baseline generalist LLM extraction approach.",
            "output_type": "Free-text extracted descriptions of outcomes (manually converted to numeric standardized values by annotators for evaluation).",
            "evaluation_methods": "Compared against TrialMind extraction accuracies and GPT-4 naive baseline using the TrialReviewBench annotations and accuracy metric.",
            "results": "Paper reports that TrialMind outperformed generalist LLM baselines (GPT-4 naive and Sonnet) on result extraction accuracy; specific Sonnet numeric accuracies not tabulated separately in main text, but Sonnet was one of the baselines TrialMind beat by substantial margins.",
            "datasets_or_benchmarks": "TrialReviewBench (result extraction subset; 1,049 result annotations).",
            "challenges_or_limitations": "Like GPT-4 naive prompting, Sonnet baseline required manual post-processing and suffered from failures to identify correct data locations and to standardize numeric outputs without human intervention.",
            "uuid": "e6076.7",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Clinical Texts / MedCPT",
            "rating": 1
        },
        {
            "paper_title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
            "rating": 1
        }
    ],
    "cost": 0.020123,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Accelerating Clinical Evidence Synthesis with Large Language Models</h1>
<p>Zifeng Wang ${ }^{1}$, Lang Cao ${ }^{1}$, Benjamin Danek ${ }^{1}$, Qiao Jin ${ }^{2}$, Zhiyong Lu ${ }^{2}$, Jimeng Sun ${ }^{1,3 #}$<br>${ }^{1}$ Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL<br>${ }^{2}$ National Center for Biotechnology Information, National Library of Medicine, Bethesda, MD<br>${ }^{3}$ Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, Champaign, IL<br>${ }^{#}$ Corresponding authors. Emails: jimeng@illinois.edu</p>
<h4>Abstract</h4>
<p>Synthesizing clinical evidence largely relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating clinical evidence. Here, we introduce TrialMind, a generative artificial intelligence (AI) pipeline for facilitating human-AI collaboration in three crucial tasks for evidence synthesis: study search, screening, and data extraction. To assess its performance, we chose published systematic reviews to build the benchmark dataset, named TrialReviewBench, which contains 100 systematic reviews and the associated 2,220 clinical studies. Our results show that TrialMind excels across all three tasks. In study search, it generates diverse and comprehensive search queries to achieve high recall rates (Ours 0.711-0.834 v.s. Human baseline $0.138-0.232$ ). For study screening, TrialMind surpasses traditional embeddingbased methods by $30 \%$ to $160 \%$. In data extraction, it outperforms a GPT-4 baseline by $29.6 \%$ to $61.5 \%$. We further conducted user studies to confirm its practical utility. Compared to manual efforts, human-AI collaboration using TrialMind yielded a $71.4 \%$ recall lift and $44.2 \%$ time savings in study screening and a $23.5 \%$ accuracy lift and $63.4 \%$ time savings in data extraction. Additionally, when comparing synthesized clinical evidence presented in forest plots, medical experts favored TrialMind's outputs over GPT-4's outputs in $62.5 \%$ to $100 \%$ of cases. These findings show the promise of LLM-based approaches like TrialMind to accelerate clinical evidence synthesis via streamlining study search, screening, and data extraction from medical literature, with exceptional performance improvement when working with human experts.</p>
<h1>Introduction</h1>
<p>Clinical evidence is crucial for supporting clinical practices and advancing new drug development and needs to be updated regularly. ${ }^{1}$ It is primarily gathered through retrospective analysis of real-world data or through prospective clinical trials that assess new interventions on humans. Researchers usually conduct systematic reviews to consolidate evidence from various clinical studies in the literature. ${ }^{2,3}$ However, this process is expensive and time-consuming, requiring an average of five experts and 67.3 weeks based on an analysis of 195 systematic reviews. ${ }^{4}$ Moreover, the fast growth of clinical study databases means that the information in these published clinical reviews becomes outdated rapidly. ${ }^{5}$ For instance, PubMed has indexed over 35 M citations and gets over 1 M new citations annually. ${ }^{6}$ This situation underscores the urgent need to streamline the systematic review processes to document systematic and timely clinical evidence from the extensive medical literature. ${ }^{1,7}$</p>
<p>Large language models (LLMs) excel at information processing and generating. They can be adapted to target tasks by providing the task definition and examples as the inputs (namely "prompts"). ${ }^{8}$ Researchers have tried to adopt LLMs for many individual tasks in the evidence synthesis process, including generating searching queries, ${ }^{9,10}$ extracting studies' population, intervention, comparison, outcome (PICO) elements, ${ }^{11,12}$ screening citations, ${ }^{13}$ and summarizing findings from multiple studies. ${ }^{14-17}$ However, few have investigated LLMs' effectiveness across the entire evidence synthesis process. ${ }^{18}$ This is crucial because it ensures a seamless integration of AI in every step, potentially improving overall efficiency and accuracy. Understanding the strengths and limitations of LLMs in a holistic manner enables more effective automation and human-AI collaboration. To fill this gap, we created a testing dataset TrialReviewBench that covers major tasks in evidence synthesis, including study search, screening, and data extraction tasks. We chose published systematic reviews to create the dataset. As a result, the dataset includes 100 systematic reviews with 2,220 associated clinical studies. It also consists of manual annotations of 1,334 study characteristics and 1,049 study results. Based on TrialReviewBench, we are able to assess cutting-edge LLMs, e.g., GPT-4, ${ }^{19}$ in clinical evidence synthesis tasks.</p>
<p>Furthermore, this study aims to fill the gap in adapting LLMs to evidence synthesis tasks, overcoming LLM's limitations in (1) hallucinations, (2) weakness in reasoning with numerical data, (3) overly generic outputs, and (4) lack of transparency and reliability. ${ }^{20}$ Specifically, we developed an AI-driven pipeline named TrialMind, which is optimized for (1) generating boolean queries to search citations from the literature; (2) building eligibility criteria and screening through the found citations; and (3) extracting data, including study protocols, methods, participant baselines, study results, etc., from publications and reports. More importantly, TrialMind breaks down into subtasks that adhere to the established practice of systematic reviews, ${ }^{21}$ which facilitates experts in the loop to monitor, edit, and verify intermediate outputs. It also has the flexibility to allow experts to begin at any intermediate step as needed.</p>
<p>In this study, we show that the TrialMind is able to 1) retrieve a complete list of target studies from the literature, 2) follow the specified eligibility criteria to rank the most relevant studies at the top, and 3) achieve high accuracy in extracting information and clinical outcomes from unstructured documents based on user requests. Beyond providing descriptive evidence, TrialMind can extract numerical clinical outcomes to be standardized as input for meta-analysis (e.g., forest plots). A human evaluation was conducted to assess the synthesized evidence. Finally, to validate the practical benefits, we developed an accessible web application based on TrialMind and conducted a user study comparing two approaches: AI-assisted experts versus standalone experts. We measured the time savings and evaluated the output quality of each approach. The results show that TrialMind significantly reduced the time required for study search, citation screening, and data extraction, while maintaining or improving the quality of the output compared to experts working alone.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of TrialMind pipeline. a, it has four main steps: literature search, literature screening, data extraction, and evidence synthesis. b, (1) Utilizing input PICO elements, TrialMind generates key terms to construct Boolean queries for retrieving studies from literature databases. (2) TrialMind formulates eligibility criteria, which users can edit to provide context for LLMs during eligibility predictions. Users can then select studies based on these predictions and rank their relevance by aggregating them. (3) TrialMind processes the descriptions of target data fields to extract and output the required information as structured data. (4) TrialMind extracts findings from the studies and collaborates with users to synthesize the clinical evidence.</p>
<h1>Results</h1>
<h2>Creating TrialReviewBench from medical literature</h2>
<p>A systematic understanding of cancer treatments is crucial for oncology drug discovery and development. We retrieved a list of cancer treatments from the National Cancer Institute's introductory page as the keywords to search medical systematic reviews. ${ }^{22}$ To ensure data quality, we crafted comprehensive queries with automatic filtering and manual screening. For each review, we obtained the list of studies with their PubMed IDs, retrieved their full content, and extracted study characteristics and clinical outcomes. We followed PubMed's usage policy and guidelines during retrieval. Further manual checks were performed to correct inaccuracies, eliminate invalid and duplicate papers, and refine the text for clarity (Methods). The final TrialReviewBench dataset consists of 2,220 studies involved in 100 reviews (Fig. 2a), covering four major topics: Immunotherapy, Radiation/Chemotherapy, Hormone Therapy, and Hyperthermia. We manually created three major evaluation tasks based on these reviews: study search, study screening, and data extraction.</p>
<p>The study search task begins with the PICO (Population, Intervention, Comparison, Outcome) elements extracted from the abstract of a systematic review, which serve as the formal definition of the research question. The model being tested is tasked with generating relevant keywords for the treatment and condition terms, as depicted in Fig. 2e. These keywords are then used to form Boolean queries, which are submitted to search citations in the PubMed database. The performance of the model is evaluated by checking whether the retrieved studies include those that were actually involved in the target systematic review. The recall rate is computed by measuring the proportion of actually involved studies identified through the search.</p>
<p>For the study screening task, the input consists of the PICO elements defined in the target systematic review. A candidate set of 2,000 citations is created by combining the actual studies included in the review with additional citations retrieved during the search but not included in the review. The model being tested ranks these citations based on the likelihood that each citation should be included in the systematic review. To assess the model's performance, we compute Recall@ $k$ : the recall value indicating how many of the actual included studies appear in the top $k$ ranked candidates.</p>
<p>The data extraction task focuses on retrieving specific information from the input study documents. In this case, we extract Table 1 from each systematic review, which typically details study characteristics such as study design, population demographics, and outcome measurements. These characteristics are matched to the individual studies and manually verified, yielding 1,334 study characteristic annotations. Additionally, we extract individual study results from the review's reported analysis, often presented in forest plots, capturing metrics such as overall response and event rates, resulting in 1,049 study result annotations. The model being tested is given a list of target data points to extract, and its output is evaluated by assessing the accuracy of the extracted information based on the annotated datasets.</p>
<h2>Build an LLM-driven system for clinical evidence synthesis</h2>
<p>Large language models (LLMs) excel in adapting to new tasks when provided with taskspecific prompts while often struggling with complex tasks that require multiple steps of planning and reasoning. Additionally, interacting and collaborating with LLMs can be problematic due to their opaque nature and the complexity of debugging. ${ }^{23}$ In this study, we developed TrialMind that decomposes the clinical evidence synthesis process into four main tasks (Fig. 1 and Methods). Initially, using the provided research question enriched with population, intervention, comparison, and outcome (PICO) elements, TrialMind conducts a comprehensive search from the literature. It also works with users to build the eligibility criteria for target studies and then automate screening and ranking identified citations. Next, TrialMind browses the study details to extract the study characteristics and pertinent findings. To ensure the accuracy and integrity of the data, each output is linked to the sources for manual inspection. In the final step, TrialMind standardizes the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Literature search experiment results. a, The total number of involved studies and the number of review papers across different topics. b, The TrialMind's interface for users to retrieve studies. c, the Recall of the search results for reviews across four topics. The bar heights indicate the Recall, and the star indicates the number of studies found. d, Scatter plots of the Recall against the number of ground-truth studies. Each scatter indicates the results of one review. Regression estimates are displayed with the 95% CIs in blue or purple. e, Example cases comparing the outputs of three methods.</p>
<p>clinical outcomes for meta-analysis.</p>
<h1>TrialMind can make a comprehensive retrieval of studies from the literature</h1>
<p>Finding relevant studies from medical literature like PubMed, which contains over 35 million entries, can be challenging. Typically, this requires the research expertise to craft complex queries that comprehensively cover pertinent studies. The challenge lies in balancing the specificity of queries: too stringent, and the search may miss relevant studies; too broad, and it becomes impractical to manually screen the overwhelming number of results. Previous approaches propose to prompt LLMs to generate the searching query directly, ${ }^{9}$ which can induce incomplete searching results due to the limited knowledge of LLMs. In contrast, TrialMind is designed to produce comprehensive queries through a pipeline that includes query generation, augmentation, and refinement. It also provides users with the ability to make further adjustments (Fig. 2b).</p>
<p>The dataset involving clinical studies spanning ten cancer treatment areas was used for evaluation (Fig. 2a). For each review, we collected the involved studies' PubMed IDs as the ground-truth and measured the Recall, i.e., how many ground-truth studies are found in the search results. We created two baselines as the comparison: GPT-4 and Human. The GPT-4 baseline makes a guided prompt for LLMs to generate the boolean queries. ${ }^{9}$ It represents the common way of prompting LLMs for literature search query generation. The Human baseline represents a way where the key terms from PICO elements are extracted manually and expanded, referring to UMLS, ${ }^{24}$ to construct the search queries.</p>
<p>Overall, TrialMind achieved a Recall of 0.782 on average for all reviews in TrialReviewBench, meaning it can capture most of the target studies. By contrast, the GPT-4 baseline yielded Recall $=0.073$, and the Human baseline yielded Recall $=0.187$. We divided the search results across four topics determined by the treatments studied in each review (Fig. 2c). Our analysis showed that TrialMind can identify many more studies than the baselines. For instance, TrialMind achieved Recall $=0.797$ with identified studies $N=22,084$ for Immunotherapy-related reviews, while the GPT-4 baseline got Recall $=0.094$ ( $N$ studies $=27$ ), and the Human baseline got Recall $=0.154(N$ studies $=958)$, respectively. In Radiation/Chemotherapy, TrialMind achieved Recall $=0.780$, the GPT-4 baseline got Recall $=0.020$, and the Human baseline got Recall $=0.138$. In Hormone Therapy, TrialMind achieved Recall $=0.711$, the GPT-4 baseline got Recall $=0.067$, and the Human baseline got Recall $=0.232$. In Hyperthermia, TrialMind achieved Recall $=0.834$, the GPT-4 baseline got Recall $=0.106$, and the Human baseline got Recall $=0.202$. These results demonstrate that regardless of the search task's complexity, as indicated by the variability in the Human baseline, TrialMind consistently retrieves nearly all target studies from the PubMed database. This robust performance provides a solid foundation for accurately identifying target studies in the screening phase.</p>
<p>Furthermore, we made scatter plots of Recall versus the number of target studies for each review (Fig. 2d). The hypothesis was that an increase in target studies correlates with the difficulty of achieving complete coverage. Our findings reveal that TrialMind consistently maintained a high Recall, significantly outperforming the best baselines across all 100 reviews. A trend of declining Recall with an increasing number of target studies was confirmed through regression analysis. It was found that the GPT-4 baseline struggled, showing Recall close to 0 , and the Human baseline results varied, with most reviews below 0.5. As the number of target studies increased, the Human and GPT-4 baselines' Recall decreased to nearly zero. In contrast, TrialMind demonstrated remarkable resilience, showing minimal variation in performance despite the increasing number of target studies. For instance, in a review involving 141 studies, TrialMind achieved a Recall of 0.99 , while the GPT-4 and Human baselines obtained a Recall of 0.02 and 0 , respectively.</p>
<h2>TrialMind enhances literature screening and ranking</h2>
<p>Typically, human experts manually sift through thousands of retrieved studies to select relevant ones for inclusion in a systematic review. This process adheres to the PRISMA</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Literature screen experiment results. <strong>a</strong>, Streamline study screening using TrialMind with human in the loop. <strong>b</strong>, Ranking performances for Recall@20/50 within across therapeutic areas. <strong>c</strong>, Recall@20 and Recall@50 for TrialMind and selected baselines. <strong>d</strong>, Effect of individual criterion on the ranking results. <strong>e</strong>, Ranking performance for Recall@<em>K</em> with varying <em>K</em> in four topics. Shaded areas are 95% confidence interval.</p>
<p>statement, ${ }^{21}$ which involves creating a list of eligibility criteria and assessing each study's eligibility. TrialMind streamlines this task through a three-step approach: (1) it generates a set of inclusion criteria, which are subject to user's adjustments; (2) it applies these criteria to evaluate the study's eligibility, denoted by ${-1,0,1}$ where -1 and 1 represent eligible and non-eligible, and 0 represents unknown/uncertain, respectively; and (3) it ranks the studies by aggregating the eligibility predictions, where the aggregation strategy can be specified by users (Fig. 3a). We took a summation of the criteria-level eligibility predictions as the study-level relevance prediction scores for ranking. As such, TrialMind provides a rationale for the relevance scores by detailing the eligibility predictions for each criterion.</p>
<p>We chose MPNet ${ }^{25}$ and MedCPT ${ }^{26}$ as the general domain and medical domain ranking baselines, respectively. These methods compute study relevance by the cosine similarity between the encoded PICO elements as the query and the encoded study's abstracts. We also set a Random baseline that randomly samples from candidates. We created the evaluation data based on the search results in the first stage. For each review, we mixed the target studies with the other found studies to build a candidate set of 2,000 studies for ranking. Discriminating the target studies from the other candidates is challenging since all candidates meet the search queries, meaning they most probably investigate the relevant therapies or conditions. We evaluated the ranking performance using the Recall@20 and Recall@50 metrics. The concatenation of the title and abstract of each study is used for all methods as inputs.</p>
<p>We found that TrialMind greatly improved ranking performances, with the fold changes over the best baselines ranging from 1.3 to 2.6 across four topics (Table 3c). For instance, for the Hormone Therapy topic, TrialMind obtained Recall@20 $=0.431$ and Recall@50 $=0.674$. In the Hyperthermia topic, TrialMind obtained Recall@20 $=0.518$ and Recall@50 $=0.710$. In the Immunotherapy topic, TrialMind obtained Recall@20 $=$ 0.567 and Recall@50 $=0.713$. In the Radiation/Chemotherapy topic, TrialMind obtained Recall@20 $=0.416$ and Recall@50 $=0.654$. In contrast, other baselines exhibit significant variability across different topics. The general domain baseline MPNet was the worst as it performed similarly to the Random baseline in Recall@20. MedCPT showed marginal improvement over MPNet in the last three topics, while both failed to capture enough target studies in all topics.</p>
<p>Furthermore, TrialMind demonstrated significant improvements over the baselines across various therapeutic areas (Fig. 3b). For example, in "Cancer Vaccines" and "Hormone Therapy," TrialMind substantially increased Recall@50, achieving 33.33-fold and 10.53 -fold improvements, respectively, compared to the best-performing baseline. TrialMind generally attained a fold change greater than 2 (ranging from 1.57 to 33.33 ). Despite the challenge of selecting from a large pool of candidates $(n=2,000)$ where candidates were very similar, TrialMind identified an average of $43 \%$ of target studies within the top 50. We compared TrialMind to MedCPT and MPNet for Recall@ $K(K$ in 10 to 200) to gain insight into how $K$ influences the performances (Fig. 3e). We found TrialMind can capture most of the target studies (over $80 \%$ ) when $K=100$.</p>
<p>To thoroughly assess the quality of these criteria and their impact on ranking performance, we conducted a leave-one-out analysis to calculate $\Delta$ Recall@200 for each criterion (Fig. 3d). The $\Delta$ Recall@200 metric measures the difference in ranking performance with and without a specific criterion, with a larger value indicating superior criterion quality. Our findings revealed that most criteria positively influenced ranking performances, as the negative influence criteria are $n=1$ in Hormone Therapy, $n=1$ in Hyperthermia, $n=5$ in Radiation/Chemotherapy, and $n=7$ in Immunotherapy. Additionally, we identified redundancies among the generated criteria, as those with $\Delta$ Recall@200 $=0$ were the most frequently observed. This redundancy likely stems from some criteria covering similar eligibility aspects, thus not impacting performance when one is omitted.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Data and result extraction experiment results. a, Streamline study information extraction using TrialMind. b, Data extraction accuracy within each field type across four topics. c, Confusion matrix showing the hallucination and missing rates in the data extraction results. d, Result extraction accuracy across topics. e, Result extraction accuracy across clinical endpoints. f, Error analysis of the result extraction. g, Streamline result extraction using TrialMind.</p>
<h1>TrialMind scales data and result extraction from unstructured documents</h1>
<p>TrialMind leverages LLMs to streamline extracting study characteristics such as target therapies, study arm design, and participants' baseline information from involved studies. Specifically, TrialMind refers to the field names and the descriptions from users and use the full content of the study documents in PDF or XML formats as inputs (Fig. 4a). When the free full content is unavailable, TrialMind accepts the user-uploaded content as the input. We developed an evaluation dataset by converting the study characteristic tables from each review paper into data points. Our dataset comprises 1,334 target data points, including 696 on study design, 353 on population features, and 285 on results. We assessed the data extraction performance using the Accuracy metric.</p>
<p>TrialMind demonstrated strong extraction performance across various topics (Fig. 4b): it achieved an accuracy of $\mathrm{ACC}=0.78(95 \%$ confidence interval $(\mathrm{CI})=0.75-0.81)$ in the Immunotherapy topic, $\mathrm{ACC}=0.77(95 \% \mathrm{CI}=0.72-0.82)$ in the Radiation/Chemotherapy topic, $\mathrm{ACC}=0.72(95 \% \mathrm{CI}=0.63-0.80)$ in the Hormone Therapy topic, and $\mathrm{ACC}=0.83$ $(95 \% \mathrm{CI}=0.74-0.90)$ in the Hyperthermia topic. These results indicate that TrialMind can provide a solid initial data extraction, which human experts can refine. Importantly, each output can be cross-checked by the linked original sources, facilitating verification and further investigation.</p>
<p>Diving deeper into the accuracy across different types of fields, we observed varying performance levels. It performed best in extracting study design information, followed by population details, and showed the lowest accuracy in extracting results (Fig. 4b). For example, in the Immunotherapy topic, TrialMind achieved an accuracy of $\mathrm{ACC}=0.95$ $(95 \% \mathrm{CI}=0.92-0.96)$ for study design, $\mathrm{ACC}=0.74(95 \% \mathrm{CI}=0.67-0.80)$ for population data, and $\mathrm{ACC}=0.42(95 \% \mathrm{CI}=0.36-0.49)$ for results. This variance can be attributed to the prevalence of numerical data in the fields: fields with more numerical data are typically harder to extract accurately. Study design is mostly described in textual format and is directly presented in the documents, whereas population and results often include numerical data such as the number of patients or gender ratios. Results extraction is particularly challenging, often requiring reasoning and transformation to capture values accurately. Given these complexities, it is advisable to scrutinize the extracted numerical data more carefully.</p>
<p>We also evaluated the robustness of TrialMind against hallucinations and missing information (Fig. 4c). We constructed a confusion matrix detailing instances of hallucinations: false positives (FP) where TrialMind generated data not present in the input document and false negatives (FN) where it failed to extract available target field information. We observed that TrialMind achieved a precision of Precision $=0.994$ for study design, Precision $=0.966$ for population, and Precision $=0.862$ for study results. Missing information was slightly more common than hallucinations, with TrialMind achieving recall rates of Recall $=0.946$ for study design, Recall $=0.889$ for population, and Recall $=0.930$ for study results. The incidence of both hallucinations and missing information was generally low. However, hallucinations were notably more frequent in study results; this often occurred because LLMs could confuse definitions of clinical outcomes, for example, mistaking 'overall response' for 'complete response.' Nevertheless, such hallucinations are typically manageable, as human experts can easily identify and correct them while reviewing the referenced material.</p>
<p>The challenges in extracting study results primarily stem from (1) identifying the locations that describe the desired outcomes from lengthy papers, (2) accurately extracting relevant numerical values such as patient numbers, event counts, durations, and ratios from the appropriate patient groups, and (3) performing the correct calculations to standardize these values for meta-analysis. In response to these complexities, we developed a specialized pipeline for result extraction (Fig. 4g), where users provide the interested outcome and the cohort definition. TrialMind offers a transparent extraction workflow, documenting the sources of results along with the intermediate reasoning and calculations.</p>
<p>We compared TrialMind against two generalist LLM baselines, GPT-4 and Sonnet, which were prompted to extract the target outcomes from the full content of the study</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results of human evaluation and user study. a, TrialMind's result extraction process. b, Winning rate of TrialMind against the GPT-4+Human baseline across studies. c, Violin plots of the ratings across studies. Each plot is tagged with the mean ratings (95% CI) from all the annotators. d,Violin plots of the ratings across annotators with different expertise levels. Each plot is tagged with the mean ratings (95% CI) from all the studies. e, Overall performance and time cost of study screening and data extraction tasks, respectively. f, Screening time cost and performance across reviews and participants. g, Data extraction accuracy across participants and different types of data.</p>
<p>documents. Since the baselines can only make text extractions, we manually convert them into numbers suitable for meta-analysis. ${ }^{27}$ This made very strong baselines since they combined LLM extraction with human post-processing. We assessed the performance using the Accuracy metric.</p>
<p>The evaluation conducted across four topics demonstrated the superiority of TrialMind (Fig. 4d). Specifically, in Immunotherapy, TrialMind achieved an accuracy of $\mathrm{ACC}=0.70$ ( $95 \%$ CI $0.62-0.77$ ), while GPT-4 scored $\mathrm{ACC}=0.54$ ( $95 \%$ CI $0.45-0.62$ ). In Radiation/Chemotherapy, TrialMind reached $\mathrm{ACC}=0.65$ ( $95 \%$ CI $0.51-0.76$ ), compared to GPT-4's ACC $=0.52$ ( $95 \%$ CI $0.39-0.65$ ). For Hormone Therapy, TrialMind achieved $\mathrm{ACC}=0.80$ ( $95 \%$ CI $0.58-0.92$ ), outperforming GPT-4, which scored $\mathrm{ACC}=0.50$ ( $95 \%$ CI $0.30-0.70$ ). In Hyperthermia, TrialMind obtained an accuracy of $\mathrm{ACC}=0.84$ ( $95 \%$ CI $0.71-0.92$ ), significantly higher than GPT-4's ACC $=0.52$ ( $95 \%$ CI $0.39-0.65$ ). The breakdowns of evaluation results by the most frequent types of clinical outcomes (Fig. 4e) showed TrialMind got fold changes in accuracy ranging from 1.05 to 2.83 and a median of 1.50 over the best baselines. This enhanced effectiveness is largely attributable to TrialMind's ability to accurately identify the correct data locations and apply logical reasoning, while the baselines often produced erroneous initial extractions.</p>
<p>We analyzed the error cases in our result extraction experiments and identified four primary error types (Fig. 4f). The most common error was 'Inaccurate' extraction ( $\mathrm{n}=36$ ), followed by 'Extraction failure' ( $\mathrm{n}=27$ ), 'Unavailable data' ( $\mathrm{n}=10$ ), and 'Hallucinations' $(\mathrm{n}=3)$. 'Inaccurate' extractions often occurred due to multiple sections ambiguously describing the same field. For example, a clinical study might report the total number of participants receiving CAR-T therapy early in the document and later provide outcomes for a subset with non-small cell lung cancer (NSCLC). The specific results for NSCLC patients are crucial for reviews focused on this subgroup, yet the presence of general data can lead to confusion and inaccuracies in extraction. 'Extraction failure' and 'Unavailable data' both illustrate scenarios where TrialMind could not retrieve the information. The latter case particularly showcases TrialMind's robustness against hallucinations, as it failed to extract data outside the study's main content, such as in appendices, which were not included in the inputs. Furthermore, errors caused by hallucinations were minor. The outputs were easy to identify and correct through manual inspection since no references were provided.</p>
<h1>TrialMind facilitates clinical evidence synthesis via human-AI collaboration</h1>
<p>We selected five systematic review studies as benchmarks and referenced the clinical evidence reported in the target studies. The baseline used GPT-4 with a simple prompting to extract the relevant text pieces that report the target outcome of interest (Methods). Manual calculations were necessary to standardize the data for meta-analysis. In contrast, TrialMind automated the extraction and standardization (Fig. 5a by (1) extracting the raw result description from the input document and (2) standardizing the results by generating a Python program to assist the calculation. The standardized results from all involved studies are then fed into the R program by human experts to make the aggregated evidence in a forest plot.</p>
<p>We engaged with human annotators to assess the quality of synthesized clinical evidence presented in forest plots. Each annotator was asked to evaluate the evidence quality by comparing it against the evidence reported in the target review and deciding which method, TrialMind or the baseline, produced superior results (Extended Fig. 1). Additionally, they rated the quality of the synthesized clinical evidence on a scale of 1 to 5. The assignment of our method and the baseline was randomized to ensure objectivity. The results highlighted TrialMind's superior performance compared to the direct use of GPT-4 for clinical evidence synthesis (Fig. 5b). We calculated the winning rate of TrialMind versus the baseline across the five studies. The results indicate a consistent preference by annotators for the evidence synthesized by TrialMind over that of the baseline. Specifically, TrialMind achieved winning rates of $87.5 \%, 100 \%, 62.5 \%, 62.5 \%$, and</p>
<p>$81.2 \%$, respectively. The baseline's primary shortcoming stemmed from the initial extraction step, where GPT-4 often failed to identify the relevant sources without well-crafted prompting. Therefore, the subsequent manual post-processing was unable to rectify these initial errors.</p>
<p>In addition, we illustrated the ratings of TrialMind and the baseline across studies (Fig. 5c). We found TrialMind was competent as the GPT-4+Human baseline and outperformed the baseline in many scenarios. For example, TrialMind obtained the mean rating of 4.25 ( $95 \%$ CI 3.93-4.57) in Study #1 while the baseline obtained 3.50 ( $95 \%$ CI 3.13-3.87). In Study #2, TrialMind yielded 3.50 ( $95 \%$ CI 3.13-3.87) while the baseline yielded 1.25 ( $95 \%$ CI 0.93-1.57). The performance of the two methods was comparable in the remaining three studies. These results highlight TrialMind as a highly effective alternative to conventional LLM usage in evidence synthesis, streamlining data extraction and processing while maintaining the critical benefit of human oversight.</p>
<p>We requested that annotators self-assess their expertise level in clinical studies, classifying themselves into three categories: 'Basic', 'Familiar', and 'Advanced'. The typical profile ranges from computer scientists at the basic level to medical doctors at the advanced level. We then analyzed the ratings given to both methods across these varying expertise levels (Fig. 5d). We consistently observed higher ratings for TrialMind than the baseline across all groups. Annotators with basic knowledge tended to provide more conservative ratings, while those with more advanced expertise offered a wider range of evaluations. For instance, the 'Basic' group provided average ratings of 3.67 ( $95 \%$ CI 3.34-3.39) for TrialMind compared to 3.22 ( $95 \%$ CI 2.79-3.66) for the baseline. The 'Advanced' group rated TrialMind at an average of 3.40 ( $95 \%$ CI 3.16-3.64) and the baseline at $3.07(95 \%$ CI $2.75-3.39)$.</p>
<p>We conducted user studies to compare the quality and time efficiency between purely manual efforts and human-AI collaboration using TrialMind. Two participants were involved in both study screening and data extraction tasks. For the screening task, each participant was assigned 4 systematic review papers, with 100 candidate citations identified for each review. The participants were asked to select the 10 most likely relevant citations from the candidate pool. Each participant was provided with 2 candidate sets pre-ranked by TrialMind and 2 unranked sets. The participants also recorded the time taken to complete the screening process for each set. For the data extraction task, each participant was given 10 clinical studies. They manually extracted the target information for 5 of these studies. For the other 5, TrialMind was first used to perform an initial extraction, and the participants were required to verify and correct the extracted results. The time taken for the extraction process was reported for each study.</p>
<p>In Fig. 5e, we present the average performance and time cost for the AI+Human and Human-only approaches across both the study screening and data extraction tasks. The results demonstrate that the AI+Human approach consistently outperforms the Humanonly approach. For the screening tasks, AI+Human achieved a $71.4 \%$ relative improvement in Recall, while reducing time by $44.2 \%$ compared to the Human-only arm. This underscores the significant advantage of TrialMind in accelerating the study screening process while also improving its quality. Similarly, for the data extraction tasks, the AI+Human approach improved extraction accuracy by $23.5 \%$ on average, with a $63.4 \%$ reduction in time required.</p>
<p>Detailed results of screening time and performance are shown in Fig. 5f, where two reviews showed the AI+Human approach achieving the same Recall as the Human-only arm with notable time savings, and in two other reviews, AI+Human achieved higher Recall with less time. From Fig. 5g, we see that the AI+Human approach delivered better or comparable accuracy across all three types of data, with the smallest gap in "Study design". This is likely because study design information is often readily available in the study abstract, making it relatively easier for humans to extract. In contrast, the other two data types are embedded deeper within the main content, which can sometimes make it challenging for human readers to locate the correct information.</p>
<h1>Discussion</h1>
<p>Clinical evidence forms the bedrock of evidence-based medicine, crucial for enhancing healthcare decisions and guiding the discovery and development of new therapies. It often comes from a systematic review of diverse studies found in the literature, encompassing clinical trials and retrospective analyses of real-world data. Yet, the burgeoning expansion of literature databases presents formidable challenges in efficiently identifying, summarizing, and maintaining the currency of this evidence. For instance, a study by the US Agency for Healthcare Research and Quality (AHRQ) found that half of 17 clinical guidelines became outdated within a couple of years. ${ }^{28}$</p>
<p>The rapid development of large language models (LLMs) and AI technologies has generated considerable interest in their potential applications in clinical research. ${ }^{29,30}$ However, most of them focused on an individual aspect of the clinical evidence synthesis process, such as literature search, ${ }^{31,32}$ citation screening, ${ }^{33-35}$ quality assessment, ${ }^{36}$ or data extraction. ${ }^{37,38}$ In addition, implementing these models in a manner that is collaborative, transparent, and trustworthy poses significant challenges, especially in critical areas such as medicine. ${ }^{39}$ For instance, when utilizing LLMs to summarize evidence from multiple studies, the descriptive summaries often usually merely echo the findings verbatim, omit crucial details, and fail to adhere to established best practices. ${ }^{14}$ Besides, when given a set of studies that are irrelevant to the research question, LLMs are prone to produce hallucinations and hence cause misleading evidence. ${ }^{40}$ This challenge highlights the need for an integrated pipeline, involving the study search and screening stages, to strategically pick the target studies for analysis, ${ }^{41,42}$ or enhanced with human-AI collaboration. ${ }^{43}$</p>
<p>This study introduces a clinical evidence synthesis pipeline enhanced by LLMs, named TrialMind. This pipeline is structured in accordance with established medical systematic review protocols, involving steps such as study searching, screening, data/result extraction, and evidence synthesis. At each stage, human experts have the capability to access, monitor, and modify intermediate outputs. This human oversight helps to eliminate errors and prevents their propagation through subsequent stages. Unlike approaches that solely depend on the knowledge of LLMs, TrialMind integrates human expertise through in-context learning and chain-of-thought prompting. Additionally, TrialMind extends external knowledge sources to its outputs through retrieval-augmented generation and leveraging external computational tools to enhance the LLM's reasoning and analytical capabilities. Comparative evaluations of TrialMind and traditional LLM approaches have demonstrated the advantages of this system design in LLM-driven applications within the medical field.</p>
<p>This study also has several limitations. First, despite incorporating multiple techniques, LLMs may still make errors at any stage. Therefore, human oversight and verification remain crucial when implementing TrialMind in practical settings. Second, the prompts used in TrialMind were developed based on prompt engineering experience, suggesting potential for performance enhancement through advanced prompt optimization or by fine-tuning the underlying LLMs to suit specific tasks better. Third, while TrialMind demonstrated effectiveness in study search, screening, and data extraction, the dataset used was limited in size due to the high costs associated with human labeling. Future research could expand on these findings with larger datasets to further validate the method's effectiveness. Fourth, the study coverage was restricted to publicly available sources from PubMed Central, which provides structured PDFs and XMLs. Many relevant studies are either not available on PubMed or are in formats that entail OCR algorithms as preprocessing, indicating a need for further engineering to incorporate broader data sources. Fifth, although TrialMind illustrated the potential of using advanced LLMs like GPT4 to streamline clinical evidence synthesis, developing techniques to adapt the pipeline for use with other LLMs could increase its applicability. Finally, while the use of LLMs like GPT-4 can accelerate study screening and data extraction, the associated costs and processing times may present bottlenecks in some scenarios. Future enhancements that improve efficiency or utilize localized, specialized smaller models could increase practical utility.</p>
<p>LLMs have made significant strides in AI applications. TrialMind exemplifies a crucial</p>
<p>aspect of system engineering in LLM-driven pipelines, facilitating the practical, robust, and transparent use of LLMs. We anticipate that TrialMind will benefit the medical AI community by fostering the development of LLM-driven medical applications and emphasizing the importance of human-AI collaboration.</p>
<h1>Methods</h1>
<h2>Description of the TrialReviewBench Dataset</h2>
<p>The overall flowchart for the study identification and screening process in building TrialReviewBench is illustrated in Extended Fig. 2.</p>
<p>Database search and initial filtering We undertook a comprehensive search on the PubMed database for meta-analysis papers related to cancer. The Boolean search terms were specifically chosen to encompass a broad spectrum of cancer-related topics. These terms included "cancer", "oncology", "neoplasm", "carcinoma", "melanoma", "leukemia", "lymphoma", and "sarcoma". Additionally, we incorporated terms related to various treatment modalities such as "therapy", "treatment", "chemotherapy", "radiation therapy", "immunotherapy", "targeted therapy", "surgical treatment", and "hormone therapy". To ensure that our search was exhaustive yet precise, we also included terms like "meta-analysis" and "systematic review" in our search criteria.</p>
<p>This initial search yielded an extensive pool of 46,192 results, reflecting the vast research conducted in these areas. We applied specific filters to refine these results and ensure relevance and quality. We focused on articles where PMC Full text was available and specifically categorized under "Meta-Analysis". Further refinement was done by restricting the time frame of publications to those between January 1, 2020, and January 1, 2023. We also narrowed our focus to studies conducted on humans and those available in English. This filtration process was critical in distilling the initial results into a more manageable and focused collection of 2,691 papers.</p>
<p>Refinement Building upon our initial search, we employed further refinement techniques using both MeSH terms and specific keywords. The MeSH terms were carefully selected to target papers precisely relevant to various forms of cancer. These terms included "cancer", "tumor", "neoplasms", "carcinoma", "myeloma", and "leukemia". This focused approach using MeSH terms effectively reduced our selection to 1,967 papers.</p>
<p>To further dive in on papers investigating cancer therapies, we utilized many keywords derived from the National Cancer Institute's "Types of Cancer Treatment" list. This approach was multi-faceted, with each set of keywords targeting a specific category of cancer therapy. For chemotherapy, we included terms like "chemotherapy", "chemo", and related variations. In the realm of hormone therapy, we searched for phrases such as "hormone therapy", "hormonal therapy", and similar terms. The keyword group for hyperthermia encompassed terms like "hyperthermia", "microwave", "radiofrequency", and related technologies. For cancer vaccines, we included keywords such as "cancer vaccines", "cancer vaccine", and other related terms. The search for immune checkpoint inhibitors and immune system modulators was comprehensive, including terms like "immune checkpoint inhibitors", "immunomodulators", and various cytokines and growth factors. Lastly, our search for monoclonal antibodies and T-cell transfer therapy included relevant terms like "monoclonal antibodies", "t-cell therapy", "car-t", and other related phrases.</p>
<p>The careful application of keyword filtering played a crucial role in narrowing down our pool of research papers to a more focused and relevant set of 352 . It represents a diverse and meaningful collection of studies in cancer therapy, highlighting a range of innovative and impactful research within this field.</p>
<p>Manual screening of titles and abstracts Then, we manually screened titles and abstracts, applying a rigorous classification and sorting methodology. The remaining papers were first categorized based on the type of cancer treatment they explored. We</p>
<p>then organized these papers by their citation count to gauge their impact and relevance in the field. Our selection criteria aimed to enhance the quality and relevance of our final dataset. We prioritized papers that focused on the study of treatment effects, such as safety and efficacy, of various cancer interventions. We preferred studies that compared individual treatments against a control group, as opposed to those examining the effects of combined therapies (e.g., Therapy A+B vs. A only). To build a list of representative meta-analyses, we needed to ensure diversity in the target conditions under each treatment category.</p>
<p>Further, we favored studies that involved a larger number of individual studies, providing a broader base of evidence. However, we excluded network analysis studies and meta-analyses that focused solely on prognostic and predictive effects, as they did not align with our primary research focus. To maintain a balanced representation, we limited our selection to a maximum of three papers per treatment category. This process culminated in a final dataset comprising 100 systematic review papers. This curated collection forms the backbone of our analysis, ensuring a concentrated and pertinent selection of high-quality studies directly relevant to our research objectives.</p>
<h1>LLM Prompting</h1>
<p>Prompting steers LLMs to conduct the target task without training the underlying LLMs. TrialMind proceeds clinical evidence synthesis in multiple steps associated with a series of prompting techniques.</p>
<p>In-context learning LLMs exhibit a profound ability to comprehend input requests and adhere to provided instructions during generation. The fundamental concept of incontext learning (ICL) is to enable LLMs to learn from examples and task instructions within a given context at inference time. ${ }^{8}$ Formally, for a specific task, we define $T$ as the task prompt, which includes the task definition, input format, and desired output format. During a single inference session with input $X$, the LLM is prompted with $P(T, X)$, where $P(\cdot)$ is a transformation function that restructures the task definition $T$ and input $X$ into the prompt format. The output $\hat{X}$ is then generated as $\hat{X}=\operatorname{LLM}(P(T, X))$.</p>
<p>Retrieval-augmented generation LLMs that rely solely on their internal knowledge often produce erroneous outputs, primarily due to outdated information and hallucinations. This issue can be mitigated through retrieval-augmented generation (RAG), which enhances LLMs by dynamically incorporating external knowledge into their prompts during generation. ${ }^{44}$ We denote $R_{K}(\cdot)$ as the retriever that utilizes the input $X$ to source relevant contextual information through semantic search. $R_{K}(\cdot)$ enables the dynamic infusion of tailored knowledge into LLMs at inference time.</p>
<p>Chain-of-thought Chain-of-though (CoT) guides LLMs in solving a target task in a step-by-step manner in one inference, hence handling complex or ambiguous tasks better and inducing more accurate outputs. ${ }^{45}$ CoT employs the function $P_{\mathrm{CoT}}(\cdot)$ to structure the task $T$ into a series of chain-of-thought steps $\left{S_{1}, S_{2}, \ldots, S_{T}\right}$. As a result, we obtain $\left{X_{S}^{1}, \ldots, X_{S}^{T}\right}=\operatorname{LLM}\left(P_{\mathrm{CoT}}(T, X)\right)$, all produced in a single inference session. This is rather critical when we aim to elicit the thinking process of LLM and urge it in selfreflection to improve its response. For instance, we may ask LLM to draft the initial response in the first step and refine it in the second.</p>
<p>LLM-driven pipeline Clinical evidence synthesis involves a multi-step workflow as outlined in the PRISMA statement. ${ }^{21}$ It can be generally outlined as identifying and screening studies from databases, extracting characteristics and results from individual studies, and synthesizing the evidence. To enhance each step's performance, task-specific prompts can be designed for an LLM to create an LLM-based module. This results in a chain of prompts that effectively addresses a complex problem, which we call LLM-driven workflow. Specifically, this approach breaks down the entire meta-analysis process into</p>
<p>a sequence of $N$ tasks, denoted as $\mathcal{T}=\left{T_{1}, \ldots, T_{N}\right}$. In the workflow, the output from one task, $\hat{X}<em n_1="n+1">{n}$, serves as the input for the next, $\hat{X}</em>\right)\right)$. This modular decomposition improves LLM performance by dividing the workflow into more manageable segments, increases transparency, and facilitates user interaction at various stages.}=\operatorname{LLM}\left(P\left(T_{n}, \hat{X}_{n</p>
<p>Incorporating these techniques, the formulation of TrialMind for any subtask can be represented as:</p>
<p>$$
\hat{X}<em n="n">{n+1}=\operatorname{LLM}\left(P\left(T</em>\right)\right), \forall n=1, \ldots, N
$$}, X_{n}\right), R_{K}\left(X_{n</p>
<p>where $R_{K}(\cdot)$ are optional.</p>
<h1>Implementation of TrialMind</h1>
<p>All experiments were run in Python v.3.9. Detailed software versions are: pandas v2.2.2; numpy v1.26.4; scipy v1.13.0; scikit-learn v1.4.1.post1; openai v1.23.6; langchain v0.1.16; boto3 v1.34.94; pypdf v4.2.0; lxml v5.2.1 and chromadb v0.5.0 with Python v.3.9.</p>
<p>LLMs We included GPT-4 and Sonnet in our experiments. GPT-4 ${ }^{19}$ is regarded as a state-of-the-art LLM and has demonstrated strong performances in many natural language processing tasks (version: gpt-4-0125-preview). Sonnet ${ }^{46}$ is an LLM developed by Anthropic, representing a more lightweight but also very capable LLM (version: anthropic.claude-3-sonnet-20240229-v1:0 on AWS Bedrock). Both models support long context lengths ( 128 K and 200 K ), enabling them to process the full content of a typical PubMed paper in a single inference session.</p>
<p>Research question inputs TrialMind processes research question inputs using the PICO (Population, Intervention, Comparison, Outcome) framework to define the study's research question. In our experiments, the title of the target review paper served as the general description. Subsequently, we extracted the PICO elements from the paper's abstract to detail the specific aspects of the research question.</p>
<p>Literature search TrialMind is tailored to adhere to the established guidelines ${ }^{21}$ in conducting literature search and screening for clinical evidence synthesis. In the literature search stage, the key is formulating Boolean queries to retrieve a comprehensive set of candidate studies from databases. These queries, in general, are a combination of treatment, medication, and outcome terms, which can be generated by LLM using in-context learning. However, direct prompting can yield low recall queries due to the narrow range of user inputs and the LLMs' tendency to produce incorrect queries, such as generating erroneous MeSH (Medical Subject Headings) terms. ${ }^{9}$ To address these limitations, TrialMind incorporates RAG to enrich the context with knowledge sourced from PubMed, and employs CoT processing to facilitate a more exhaustive generation of relevant terms.</p>
<p>Specifically, the literature search component has two main steps: initial query generation and then query refinement. In the first step, TrialMind prompts LLM to create the initial boolean queries derived from the input PICO to retrieve a group of studies (Prompt in Extended Fig. 4). The abstracts of these studies then enrich the context for refining the initial queries, working as RAG. In addition, we used CoT to enhance the refinement by urging LLMs to conduct multi-step reasoning for self-reflection enhancement (Prompt in Extended Fig. 5). This process can be described as</p>
<p>$$
\left{\hat{X}<em S="S">{S}^{1}, \hat{X}</em>}^{2}, \hat{X<em _mathrm_CoT="\mathrm{CoT">{S}^{3}\right}=\operatorname{LLM}\left(P</em>(X)\right)\right)
$$}}\left(T_{\mathrm{LS}}, X, R_{K</p>
<p>where $X$ denotes the input PICO; $R_{K}(X)$ is the set of abstracts of the found studies; $T_{\mathrm{LS}}$ is the definition of the query generation task for literature search. For the output, the first sub-step $\hat{X}<em S="S">{S}^{1}$ indicates a complete set of terms identified in the found studies; the second $\hat{X}</em>}^{2}$ indicates the subset of $\hat{X<em S="S">{S}^{1}$ by filtering out the irrelevant; and the third $\hat{X}</em>}^{3}$ indicates the extension of $\hat{X<em S="S">{S}^{2}$ by self-reflection and adding more augmentations. In this process, LLM will produce the outputs for all three substeps in one pass, and TrialMind takes $\hat{X}</em>$ as the final queries to fetch the candidate studies.}^{3</p>
<p>Study screening TrialMind follows PRISMA to take a transparent approach for study screening. It creates a set of eligibility criteria based on the input PICO as the basis for study selection (Prompt in Extended Fig. 6), produced by</p>
<p>$$
\hat{X}<em _mathrm_EC="\mathrm{EC">{\mathrm{EC}}=\operatorname{LLM}\left(P\left(T</em>, X\right)\right)
$$}</p>
<p>where $\hat{X}<em 1="1">{\mathrm{EC}}=\left{E</em>$ is the task definition of criteria generation. Users are given the opportunity to modify these generated criteria, further adjusting to their needs.}, E_{2}, \ldots, E_{M}\right}$ is the $M$ generated eligibility criteria; $X$ is the input PICO; and $T_{\mathrm{EC}</p>
<p>Based on $\hat{X}<em i="i">{\mathrm{EC}}$, TrialMind embarks the parallel processing for the candidate studies. For $i$-th study $F</em>$, the eligibility prediction is made by LLM as (Prompt in Extended Fig. 7)</p>
<p>$$
\left{I_{i}^{1}, \ldots, I_{i}^{M}\right}=\operatorname{LLM}\left(P\left(F_{i}, X, T_{\mathrm{SC}}, \hat{X}_{\mathrm{EC}}\right)\right)
$$</p>
<p>where $T_{\mathrm{SC}}$ is the task definition of study screening; $F_{i}$ is the study $i$ 's content; $I_{i}^{m} \in$ ${-1,0,1}, \forall m=1, \ldots, M$ is the prediction of study $i$ 's eligibility to the $m$-th criterion. Here, -1 and 1 mean ineligible and eligible, 0 means uncertain, respectively. These predictions offer a convenient way for users to inspect the eligibility and select the target studies by altering the aggregation strategies. $I_{i}^{m}$ can be aggregated to offer an overall relevance of each study, such as $\hat{I}<em m="m">{i}=\sum</em>$. Users are also encouraged to extend the criteria set or block the predictions of some criteria to make customized rankings during the screening phase.} I_{i}^{m</p>
<p>Data extraction Study data extraction is an open information extraction task that requires the model to extract specific information based on user inputs and handle long inputs, such as the full content of a paper. LLMs are particularly well-suited for this task because (1) they can perform zero-shot learning via in-context learning, eliminating the need for labeled training data, and (2) the most advanced LLMs can process extremely long inputs. As such, the TrialMind framework is engineered to streamline data extraction from structured or unstructured study documents using LLMs.</p>
<p>For the specified data fields to be extracted, TrialMind prompts LLMs to locate and extract the relevant information (Prompt in Extended Fig. 8). These data fields include (1) study characteristics such as study design, sample size, study type, and treatment arms; (2) population baselines; and (3) study findings. In general, the extraction process can be described as</p>
<p>$$
\left{\hat{X}<em _mathrm_EX="\mathrm{EX">{\mathrm{EX}}^{1}, \ldots, \hat{X}</em>\right)\right)
$$}}^{K}\right}=\operatorname{LLM}\left(P\left(F, C, T_{\mathrm{EX}</p>
<p>where $F$ represents the full content of a study; $T_{\mathrm{EX}}$ defines the task of data extraction; and $C=\left{C_{1}, C_{2}, \ldots, C_{K}\right}$ comprises the series of data fields targeted for extraction. $C_{k}$ is the user input natural language description of the target field, e.g., "the number of participants in the study". The input content $F$ is segmented into distinct chunks, each marked by a unique identifier. The outputs, denoted as $\hat{X}_{\mathrm{EX}}^{k}=\left{V^{k}, B^{k}\right}$, include the extracted values $V$ and the indices $B$ that link back to their respective locations in the source content. Hence, it is convenient to check and correct mistakes made in the extraction by sourcing the origin. The extraction can also be easily scaled by making paralleled calls of LLMs.</p>
<p>Result extraction Our analysis indicates that data extraction generally performs well for study design and population-related fields; however, extracting study results presents challenges. Errors frequently arise due to the diverse presentation of results within studies and subtle discrepancies between the target population and outcomes versus those reported. For instance, the target outcome is the risk ratios (treatment versus control) regarding the incidence of adverse events (AEs), while the study reports AEs among many groups separately. Or, the target outcome is the incidence of severe AEs, which implicitly correspond to those with grade III and more, while the study reports all grade AEs. To overcome these challenges, we have refined our data extraction process to create a specialized result extraction pipeline that improves clinical evidence synthesis. This enhanced pipeline consists of three crucial steps: (1) identifying the relevant content within the</p>
<p>study (Prompt in Extended Fig. 9), (2) extracting and logically processing this content to obtain numerical values (Prompt in Extended Fig. 10), and (3) converting these values into a standardized tabular format (Prompt in Extended Fig. 11).</p>
<p>Steps (1) and (2) are conducted in one pass using CoT reasoning as</p>
<p>$$
\left{\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{1}, \hat{X}</em>\right)\right)
$$}, S}^{2}\right}=\operatorname{LLM}\left(P_{\mathrm{CoT}}\left(X, O, F, T_{\mathrm{RE}</p>
<p>where $O$ is the natural language description of the clinical endpoint of interest and $T_{\mathrm{RE}}$ is the task definition of result extraction. In the outputs, $\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{1}$ represents the raw content captured from the input content $F$ regarding the clinical outcomes; $\hat{X}</em>$ to the standard tabular format.}, S}^{2}$ represents the elicited numerical values from the raw content, such as the number of patients in the group, the ratio of patients encountering overall response, etc. In step (3), TrialMind writes Python code to make the final calculation to convert $\hat{X}_{\mathrm{RE}, S}^{2</p>
<p>$$
\hat{X}<em _mathrm_PY="\mathrm{PY">{\mathrm{RE}}=\operatorname{exec}\left(\operatorname{LLM}\left(P\left(X, O, T</em>}}, \hat{X<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{2}\right)\right), \hat{X}</em>\right)
$$}, S}^{2</p>
<p>In this process, TrialMind adheres to the instructions in $T_{\mathrm{PY}}$ to generate code for data processing. This code is then executed, using $\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{2}$ as input, to produce the standardized result $\hat{X}</em>$. Additionally, it ensures that the calculation process remains transparent, enhancing the reliability and reproducibility of the synthesized evidence.}}$. An example code snippet made to do this transformation is shown in Extended Fig. 3. This approach facilitates verification of the extracted results by allowing for easy backtracking to $\hat{X}_{\mathrm{RE}, S}^{1</p>
<h1>Experimental setup</h1>
<p>Literature search and screening In our literature search experiments, we assessed performance using the overall Recall, aiming to evaluate the effectiveness of different methods in identifying all relevant studies from the PubMed database using APIs. ${ }^{47}$ For literature screening, we measured efficacy using Recall@20 and Recall@50, which gauge how well the methods can prioritize target studies at the top of the list, thereby facilitating quicker decisions about which studies to include in evidence synthesis. We constructed the ranking candidate set for each review paper by initially retrieving studies through TrialMind, then refining this list by ranking the relevance of these studies to the target review's PICO elements using OpenAI embeddings. The top 2,000 relevant studies were kept. We then ensured all target papers were included in the candidate set to maintain the integrity of our ground-truth data. The final candidate set was then deduplicated to be ranked by the selected methods.</p>
<p>In the criteria analysis experiment, we utilized Recall@200 to assess the impact of each criterion. This was done by first computing the relevance prediction using all eligibility predictions and then recalculating it without the eligibility prediction for the specific criterion in question. The difference in Recall@200 between these two relevance predictions, denoted as $\Delta$ Recall, indicates the criterion's effect. A larger $\Delta$ Recall suggests that the criterion plays a more significant role in influencing the ranking results.</p>
<p>Data extraction and result extraction To evaluate performance, we measured the accuracy of the values extracted by TrialMind against the groundtruth. We used the study characteristic tables from the review papers as our test set. Each table's column names served as input field descriptions for TrialMind. We manually downloaded the full content for the studies listed in the characteristic table. To verify the accuracy of the extracted values, we enlisted three annotators who manually compared them against the data reported in the original tables.</p>
<p>We also measured the performance of result extraction using accuracy. The annotators were asked to carefully read the extracted results and compare them to the results reported in the original review paper. For the error analysis of TrialMind, the annotators were asked to check the sources to categorize the errors for one of the reasons: inaccurate, extraction failure, unavailable data, or hallucination. We designed a vanilla</p>
<p>prompting strategy for GPT-4 and Sonnet models to set the baselines for the result extraction. Specifically, the prompt was kept minimal, as "Based on the {paper}, tell me the {outcome} from the input study for the population ${$ cohort $}$ ", where ${$ paper $}$ is the placeholder for the paper's content; {outcome} is the for the target endpoint; {cohort} is the for the target population's descriptions, including conditions and characteristics. The responses from these prompts were typically in free text, from which annotators manually extracted result values to evaluate the baselines' performance.</p>
<p>Evidence synthesis In evidence synthesis, we processed the input data using R and the 'meta' package to make the forest plots and the pooled results based on the standardized result values. This is for both TrialMind and the baselines. Nonetheless, for the baseline, the annotators also need to manually extract the result values and standardize the values to make them ready for meta-analysis, which forms the GPT-4+Human baseline in the experiments.</p>
<p>We engaged two groups of annotators for our evaluation: (1) three computer scientists with expertise in AI applications for medicine, and (2) five medical doctors to assess the generated forest plots. Each annotator was asked to evaluate five review studies. For each review, we randomly presented forest plots generated by both the baseline and TrialMind. The annotators were required to determine how closely each generated plot aligned with a reference forest plot taken from the target review paper. Additionally, they were asked to judge which method, the baseline or TrialMind, produced better results in a win/lose assessment. Extended Fig. 1 demonstrates the user interface for this study, which was created with Google Forms.</p>
<p>Research question: Microwave ablation (MWA) compared with radiofrequency ablation (RFA) for the treatment of liver cancer: a systematic review and meta-analysis
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Extended Fig. 1: The study design compares the synthesized clinical evidence from the baseline and TrialMind via human evaluation.</p>            </div>
        </div>

    </div>
</body>
</html>
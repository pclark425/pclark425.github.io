<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9336 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9336</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9336</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-e7625b556dfa60ab7fd5bce018bf71a590c8150c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e7625b556dfa60ab7fd5bce018bf71a590c8150c" target="_blank">The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective</a></p>
                <p><strong>Paper Venue:</strong> Social Science Research Network</p>
                <p><strong>Paper TL;DR:</strong> This work identifies a fundamental challenge in using large Language Models to simulate experiments: when LLM-simulated subjects are blind to the experimental design, variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of LLM simulations: controlled covariates become artificially salient in the simulated decision process, which introduces focalism. This trade-off between unconfoundedness and ecological validity is usually absent in traditional experimental design and represents a unique challenge in LLM simulations. We formalize this challenge theoretically, showing it stems from ambiguous prompting strategies, and hence cannot be fully addressed by improving training data or by fine-tuning. Alternative approaches that unblind the experimental design to the LLM show promise. Our findings suggest that effectively leveraging LLMs for experimental simulations requires fundamentally rethinking established experimental design practices rather than simply adapting protocols developed for human subjects.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9336.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9336.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-mini-2024-07-18 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI large language model instance pinned by the authors and used to simulate individual consumer decisions and aggregate demand in a price experiment; prompts were used to elicit purchase probabilities and contextual/pre-treatment variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The OpenAI model instance 'gpt-4o-mini-2024-07-18' pinned for all experiments in this paper; the paper does not report model parameter count or training-data specifics beyond that it is a production OpenAI LLM available via the OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Economics / Marketing — demand estimation and consumer behavior simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of consumer purchase decisions (binary purchase / not purchase) across multiple products and systematically varied price treatments to elicit demand curves; elicitation of unspecified contextual and pre-treatment variables (e.g., past price paid, competing product price, expiration days); simulation of 'personas' (demographics) and aggregate experimental outcomes under different prompt designs (blinded between-prompt, covariate-controlled prompts, unblinded prompts asking for aggregate predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Comparison of model-elicited average purchase probability (estimated by averaging 50 stochastic draws per prompt, temperature=1) across 11 relative price points against ground-truth human experimental results (1,000 Prolific respondents); qualitative comparison of demand-curve shape and statistical inspection of correlations between randomized price and elicited contextual/pre-treatment variables (e.g., correlation plots shown in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Qualitative and comparative results: under blinded between-prompt design, the LLM produced inverted-U-shaped demand curves that deviate substantially from the human baseline downward-sloping demand; controlling for demographics improved agreement on average but did not fully resolve discrepancies; specifying contextual covariates produced step-like demand responses (purchase iff price <= competing price) rather than smooth curves; unblinded prompts asking for aggregate experimental predictions outperformed the blinded/covariate-controlled approaches but still deviated from actual consumer behavior and were sensitive to the experimental price range. (No single numeric accuracy metric like RMSE or AUC reported; experiments used 50 draws per prompt and a 1,000-human-subject baseline.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt ambiguity (blinded prompts) leading the model to mimic observational rather than interventional distributions; unspecified covariates that the model fills in and that vary systematically with treatment (e.g., past purchase price, competing price, expiration days); whether covariates are provided in the prompt (controlling covariates reduces confounding but induces focalism); focalism (artificial salience of specified covariates) altering decision mechanisms; model prior/training bias toward observational data patterns; prompting strategy (between-prompt blind vs. covariate-controlled vs. unblinded aggregate designs); experimental range provided in prompt (sensitivity observed); simulation hyperparameters reported (temperature=1, 50 draws) but not analyzed as a major factor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human randomized experiment run on Qualtrics with 1,000 Prolific respondents (representative US sample) served as the primary ground-truth baseline; comparisons are in terms of average purchase probabilities across prices and qualitative demand-curve shape. The paper also compares simulation variants (no-covariate blind prompts, persona-based covariate-controlled prompts, prompts fixing competing price, and unblinded aggregate-prediction prompts) to each other.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Blinded between-prompt simulations produce unintended correlations between treatment (price) and unspecified contextual/pre-treatment variables, violating unconfoundedness and producing biased/implausible causal estimates (e.g., inverted-U demand curves). Adding covariates in prompts can reduce confounding but induces focalism and unrealistic decision rules (e.g., step functions tied to competing price). Fine-tuning or RAG cannot fully overcome ambiguity if prompting strategy remains ambiguous (impossibility theorem). Unblinded aggregate prompting improves but remains imperfect and sensitive to experimental range; no numerical error measures reported and no guarantee of generalization across different causal questions under ambiguous prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Avoid ambiguous/blinded prompting strategies when the goal is causal simulation; explicitly unblind prompts by communicating the experimental design (e.g., declare randomization and treatment interventions) to elicit interventional rather than observational responses; combine clear/unambiguous prompt design with improved data/fine-tuning/RAG that annotate causal metadata (e.g., which variables were randomized) rather than relying on blind fine-tuning; be cautious about adding covariates because of focalism; evaluate LLM simulations against real experimental benchmarks and report sensitivity to prompt design and experimental range.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of One, Many: Using Language Models to Simulate Human Samples <em>(Rating: 2)</em></li>
                <li>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies <em>(Rating: 2)</em></li>
                <li>Can AI language models replace human participants? <em>(Rating: 2)</em></li>
                <li>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9336",
    "paper_id": "paper-e7625b556dfa60ab7fd5bce018bf71a590c8150c",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini",
            "name_full": "gpt-4o-mini-2024-07-18 (OpenAI)",
            "brief_description": "An OpenAI large language model instance pinned by the authors and used to simulate individual consumer decisions and aggregate demand in a price experiment; prompts were used to elicit purchase probabilities and contextual/pre-treatment variables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini-2024-07-18",
            "model_description": "The OpenAI model instance 'gpt-4o-mini-2024-07-18' pinned for all experiments in this paper; the paper does not report model parameter count or training-data specifics beyond that it is a production OpenAI LLM available via the OpenAI API.",
            "scientific_subdomain": "Economics / Marketing — demand estimation and consumer behavior simulation",
            "simulation_task": "Text-based simulation of consumer purchase decisions (binary purchase / not purchase) across multiple products and systematically varied price treatments to elicit demand curves; elicitation of unspecified contextual and pre-treatment variables (e.g., past price paid, competing product price, expiration days); simulation of 'personas' (demographics) and aggregate experimental outcomes under different prompt designs (blinded between-prompt, covariate-controlled prompts, unblinded prompts asking for aggregate predictions).",
            "evaluation_metric": "Comparison of model-elicited average purchase probability (estimated by averaging 50 stochastic draws per prompt, temperature=1) across 11 relative price points against ground-truth human experimental results (1,000 Prolific respondents); qualitative comparison of demand-curve shape and statistical inspection of correlations between randomized price and elicited contextual/pre-treatment variables (e.g., correlation plots shown in figures).",
            "simulation_accuracy": "Qualitative and comparative results: under blinded between-prompt design, the LLM produced inverted-U-shaped demand curves that deviate substantially from the human baseline downward-sloping demand; controlling for demographics improved agreement on average but did not fully resolve discrepancies; specifying contextual covariates produced step-like demand responses (purchase iff price &lt;= competing price) rather than smooth curves; unblinded prompts asking for aggregate experimental predictions outperformed the blinded/covariate-controlled approaches but still deviated from actual consumer behavior and were sensitive to the experimental price range. (No single numeric accuracy metric like RMSE or AUC reported; experiments used 50 draws per prompt and a 1,000-human-subject baseline.)",
            "factors_affecting_accuracy": "Prompt ambiguity (blinded prompts) leading the model to mimic observational rather than interventional distributions; unspecified covariates that the model fills in and that vary systematically with treatment (e.g., past purchase price, competing price, expiration days); whether covariates are provided in the prompt (controlling covariates reduces confounding but induces focalism); focalism (artificial salience of specified covariates) altering decision mechanisms; model prior/training bias toward observational data patterns; prompting strategy (between-prompt blind vs. covariate-controlled vs. unblinded aggregate designs); experimental range provided in prompt (sensitivity observed); simulation hyperparameters reported (temperature=1, 50 draws) but not analyzed as a major factor.",
            "comparison_baseline": "Human randomized experiment run on Qualtrics with 1,000 Prolific respondents (representative US sample) served as the primary ground-truth baseline; comparisons are in terms of average purchase probabilities across prices and qualitative demand-curve shape. The paper also compares simulation variants (no-covariate blind prompts, persona-based covariate-controlled prompts, prompts fixing competing price, and unblinded aggregate-prediction prompts) to each other.",
            "limitations_or_failure_cases": "Blinded between-prompt simulations produce unintended correlations between treatment (price) and unspecified contextual/pre-treatment variables, violating unconfoundedness and producing biased/implausible causal estimates (e.g., inverted-U demand curves). Adding covariates in prompts can reduce confounding but induces focalism and unrealistic decision rules (e.g., step functions tied to competing price). Fine-tuning or RAG cannot fully overcome ambiguity if prompting strategy remains ambiguous (impossibility theorem). Unblinded aggregate prompting improves but remains imperfect and sensitive to experimental range; no numerical error measures reported and no guarantee of generalization across different causal questions under ambiguous prompts.",
            "author_recommendations_or_insights": "Avoid ambiguous/blinded prompting strategies when the goal is causal simulation; explicitly unblind prompts by communicating the experimental design (e.g., declare randomization and treatment interventions) to elicit interventional rather than observational responses; combine clear/unambiguous prompt design with improved data/fine-tuning/RAG that annotate causal metadata (e.g., which variables were randomized) rather than relying on blind fine-tuning; be cautious about adding covariates because of focalism; evaluate LLM simulations against real experimental benchmarks and report sensitivity to prompt design and experimental range.",
            "uuid": "e9336.0",
            "source_info": {
                "paper_title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of One, Many: Using Language Models to Simulate Human Samples",
            "rating": 2
        },
        {
            "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "rating": 2
        },
        {
            "paper_title": "Can AI language models replace human participants?",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
            "rating": 1
        }
    ],
    "cost": 0.008720249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective</h1>
<p>George Gui and Olivier Toubia*<br>January 23, 2025</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of LLM simulations: controlled covariates become artificially salient in the simulated decision process, which introduces focalism. This trade-off between unconfoundedness and ecological validity is usually absent in traditional experimental design and represents a unique challenge in LLM simulations. We formalize this challenge theoretically, showing it stems from ambiguous prompting strategies, and hence cannot be fully addressed by improving training data or by fine-tuning. Alternative approaches that unblind the experimental design to the LLM show promise. Our findings suggest that effectively leveraging LLMs for experimental simulations requires fundamentally rethinking established experimental design practices rather than simply adapting protocols developed for human subjects.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>The rise of large language models (LLMs) like GPT has sparked interest across disciplines (including marketing, computer science, economics, psychology and political science) in leveraging these tools to simulate how humans would respond to questions or stimuli in different contexts (Aher et al., 2023; Argyle et al., 2023; Arora et al., 2024; Brand et al., 2023; Dillion et al., 2023; Goli and Singh, 2024; Horton, 2023; Park et al., 2023). If these LLM-simulated experiments accurately approximate real human behavior, the implications for both academics and practitioners are substantial. Academics could use LLMs for pilot experiments to pinpoint stimuli with significant impact, thus improving the efficiency of theory development and experimental design. In fact, several review articles have already been written about this recent yet fast-growing literature (Bail, 2024; Hutson and Mastin, 2023; Sarstedt et al., 2024; Simmons and Hare, 2023). Firms could leverage these realistic simulations to explore different ideas and strategies, thereby improving customer insight and product development and optimizing marketing mix variables. Accordingly, in the recent past we have witnessed a large influx of firms offering services leveraging LLMs for customer insights (e.g., Synthetic Users, Outset AI, Nexxt, Voxpopme, Evidenza, Expected Parrot, Meaningful, xPolls, Ipsos, and CivicSync).</p>
<p>Many practical applications of LLM simulations involve exploring counterfactual scenarios where one factor is altered while others remain constant, hence requiring causal estimates of treatment effects. For example, what would happen to sales if prices were increased, how would consumers respond if a message were framed differently, or how would market dynamics shift with the introduction of a new product? While the success of using LLMs to simulate counterfactual depends on the size and quality of their training or validation data that can correct for potential bias (Ludwig et al., 2024; Wang et al., 2024), the success also critically depends on the design of the simulations.</p>
<p>Although extensive literature on experimental design with human subjects has established best practices and universal standards - such as random assignment and blinding-it remains unclear what constitutes best practices in designing LLM simulations and how these might differ from traditional experimental design principles. Establishing LLM experimental design principles is crucial for enhancing the reliability and validity of LLM simulation results, thereby making them more valuable for both researchers and practitioners. Without clear guidelines on which experimental designs to adopt or avoid, the vast flexibility and sensitivity in LLM simulations can lead to widely varying outcomes (Carlson and Burbano, 2024; Gao et al., 2024; Ludwig et al., 2024; Mohammadi, 2024). When different simulation approaches yield conflicting results, researchers lack a basis for determining how much weight to place on each finding. By developing standardized design principles, researchers can assign greater weight to well-designed simulations, enhancing the reliability and usefulness of the results. Developing such design principle is essential for advancing the field and ensuring that LLM simulations effectively contribute to our understanding of human behavior.</p>
<p>Many LLM simulations to date have relied on blinded experimental designs, where simulated participants are not aware of the experimental conditions or hypotheses being tested (Argyle et al., 2023; Brand et al.,</p>
<p>2023; Horton, 2023). Blinding ${ }^{1}$ has many benefits in human subject research: it reduces demand effects where participants modify their behavior based on what they believe researchers want to see and often helps maintain ecological validity by presenting scenarios that more closely mirror real-world decision contexts. Following this convention, researchers typically construct LLM prompts that present a single scenario without revealing its experimental nature, similar to how human participants would experience the treatment condition in a traditional between-subjects design.</p>
<p>Although blinding is considered a best practice in traditional experiments, we show that it can have unintended consequences in LLM simulations which may no longer justify its benefits. Unlike human subjects and contexts that exist independently of the experiment, LLM-simulated subjects and contexts are "created on the fly" based on the prompt. When researchers make LLM-simulated "subjects" blind to experimental conditions, variations in the treatment can systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption typically required for valid causal inference.</p>
<p>We use demand estimation as an example to empirically illustrate this challenge. We conduct experiments simulating consumer demand at different price points. We find that altering the product's price not only affects the purchase outcomes but also systematically influences other variables that should not be influenced by the treatment - such as pre-treatment variables (e.g., demographic characteristics and customer purchase history) and contextual factors (e.g., price of competing products). This unintended variation in unspecified covariates violates the unconfoundedness assumptions, and we show that it also generates implausible demand curves when compared against an actual experiment with a representative sample of human participants.</p>
<p>A tempting solution is specifying more variables in the prompts to address this confounding issue. However, we find this approach may sacrifice ecological validity because including more covariates can make the scenarios presented to the LLM less reflective of real-world decision-making contexts. For instance, when competing product information is explicitly mentioned in prompts, these variables become artificially salient in the LLM's decision process, unlike typical consumers who may not always consider such information during routine purchases. This focalism issue creates a fundamental tension between controlling for confounding factors and maintaining ecological validity - a tradeoff that typically does not exist in traditional experiments where covariates can be measured and controlled for during data analysis without making them artificially salient to participants during data generation.</p>
<p>We present a theoretical framework illustrating that confoundedness in blind LLM simulations stems from prompts being ambiguous. To overcome this fundamental limitation and improve the simulation results, researchers should consider relaxing the blinding requirement and informing the LLM of the experimental design. This approach is not subject to the same confounding issues and holds promise for better generalizable performance as LLMs continue to advance. To complement this approach, an opportunity for future research is to develop LLMs that can better interpret and understand experimental designs as well as natural language descriptions of counterfactual scenarios.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>More broadly, we aim to contribute to the growing literature on LLM simulations by providing a starting point for establishing best practices in their design. By highlighting the trade-offs between blinding, unconfoundedness, and ecological validity, we offer insights that can help researchers and practitioners design more reliable and valid LLM-based experiments. Our work underscores the importance of adjusting traditional experimental design principles to the unique characteristics of LLMs, ensuring that these powerful tools can be effectively harnessed to simulate and understand human behavior.</p>
<p>The rest of the paper is organized as follows. Section 2 demonstrates the unintended consequences of blinding empirically using demand estimation as an example. Section 3 explores the standard solution of controlling for covariates and highlights its unique challenges in the context of LLM simulations. Section 4 proves an impossibility theorem demonstrating this fundamental issue will persist for future models as long as ambiguous prompting strategies are used, and examines its implications for techniques like fine-tuning and RAG. Section 5 discusses promising directions for making prompts unambiguous through unblinding, and Section 6 concludes.</p>
<h1>2 Unintended Confounding in Blind LLM Simulations</h1>
<p>The principle of blinding is a cornerstone of experimental design that consists in withholding information from the participants, such as hiding the design of the experiment or the hypotheses being tested. It is widely used in between-subject experiments with human participants, and is also sometimes implemented in withinsubject experiments. ${ }^{2}$ Blinding helps reduce demand effects and maintains ecological validity by presenting scenarios that mirror real-world decision contexts. This section uses GPT4, demand estimation and one common LLM simulation design to illustrate how blinding creates unique challenges in LLM simulations that are absent in human experiments, and Section 4 generalizes the findings to other versions of LLMs, other applications, and other simulation designs.</p>
<p>One common way to implement blinding in LLM simulations is to conduct between-prompt experiments, where each prompt represents a single experimental condition without revealing the broader experimental context (Argyle et al., 2023; Brand et al., 2023; Horton, 2023). Blinded between-prompt LLM experiments have been used to mimic both between-subject and within-subject experiments with human participants. For example, to mimic a between-subject experiment that studies price sensitivity, researchers might present different prices to the LLM in separate prompts, with each prompt describing only the current price scenario without mentioning the experimental nature of the price variation. To mimic a within-subject conjoint analysis study, researchers might present different product profiles or choice sets to the LLM in separate prompts, with each prompt describing only the current product profile or choice set.</p>
<p>While random assignment in traditional human experiments ensures pre-treatment variables are balanced</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>across conditions and contextual variables remain constant (even when unobserved), LLM-simulated participants and environments are generated dynamically. When researchers leave details unspecified in blind LLM prompts, the model must make (implicit or explicit) assumptions about these unspecified variables, which can systematically vary with the treatment and introduce confounding factors.</p>
<p>We illustrate these issues using demand estimation, a classical problem in economics and marketing, where researchers are often interested in understanding how changes in a product's price affect demand while holding everything else constant. We select the top 40 Consumer Packaged Goods (CPG) categories according to DellaVigna and Gentzkow (2019), and for each category we pick one of the top selling products according to Walmart.com. We record the regular price of each product on Walmart.com around April, 2024. Table A. 1 in the Appendix documents this set of products. For each product, we vary the price from 0 to $200 \%$ of the regular price, in $20 \%$ increments. This gives us 11 price points to test for each product $({0,20 \%, . .100 \%, . .200 \%}$ of the regular price). Throughout the paper, for each prompt template and for each {category, product} pair described in Table A.1, we vary the price systematically across these 11 price points. We refer to "relative price" as the price relative to the regular price $({-1,-0.8, \ldots, 0, \ldots, 0.8,1}$ where 0 is the regular price). We use the Open AI API and we "pin" the model to gpt-4o-mini-2024-07-18 (the latest available at the time of running the experiment) to make sure that the exact same model is used for all our simulations. We adjust the "System" part of the prompt to increase the probability of receiving a usable answer from the model (the "System" portion of the prompt, which can be manipulated on the API and which is set to "You are a helpful assistant" on ChatGPT, provides meta-instructions to GPT). Unless noted otherwise, for each prompt that corresponds to a {category, product, price} combination, we set the temperature to 1 and generate 50 draws to calculate the average response.</p>
<p>First, we elicit unspecified variables from the LLM. For each {category, product, price} combination, we submit a prompt that asks GPT to fill in the blank contextual or pre-treatment variables given the randomized price (Kurita et al., 2019; Nozza et al., 2022; Zhang et al., 2020). Prompt 1 gives an example of such template that asks the past price given the current randomized price.</p>
<h1>Prompt 1: Unintended correlation between price and past price</h1>
<p>System: You, AI, are a customer. Your task is to fill in the blanks ... Return the completed information in comma-separated values, without any extra text.</p>
<p>User: Please consider the following product category: {category}.</p>
<p>Suppose you are in a grocery store, and you see the following product in that category: {product}.</p>
<p>The last time you purchased this product, it was priced at \$ $\qquad$ [a number with up to 2 decimal points].</p>
<p>The product is currently priced at: ${$ price $}$.</p>
<p>Return example 1: XX.XX</p>
<p>Figure 1 demonstrates positive correlations between the price of the focal product and several variables, including the price that the customer paid last time, the price of the competing product, and the expiration days of the product. The x -axis is price relative to the baseline price, allowing us to aggregate across products. ${ }^{3}$ Appendix C presents additional analysis demonstrating the broader scope of these unintended correlations across multiple variables. While these correlations are plausible in observational data, they are undesirable in an experiment where researchers are interested in understanding what happens when estimating the treatment effect of changing the price of the focal product while holding everything else constant.</p>
<p>Next, we explore whether these unintended correlations are associated with confounding and biased estimates of relevant quantities when the goal is estimating causal treatment effects. To that end, we run an actual survey on human subjects, use a similar prompt in an LLM simulation, and compare the results. The survey with human participants (pre-registered, see https://aspredicted.org/pqys-st6w.pdf) was developed using Qualtrics. Each respondent answered one purchase intention question per product, with prices randomly drawn for each product and the order of products randomized across respondent. ${ }^{4}$ Following our pre-registration, we collected data from 1,000 respondents, and excluded 9 respondents according to pre-registered exclusion criteria (based on attention filters at the beginning of the survey). The respondents were recruited from Prolific, where we paid a premium to obtain a representative sample of the US population (based on census data). We then use Prompt 2 to simulate potential purchase decisions at different price points (where again {category,product} and {price} were varied according to our experimental</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Unintended correlation between price and past price, competing price, and expiration days
design, mirroring the experiment with humans), and for each corresponding prompt, we used gpt-4o-mini-2024-07-18, with temperature set to 1 , to generate 50 draws to calculate the average purchase probability at each {category,product,price} combination. Figure 2 shows the average demand curves obtained from GPT and from human participants. While the curves elicited from human participants follow a classic downward-sloping demand curve, the GPT-simulated curves are different from the curves elicited from human participants and follow an inverted-U shape. Although it is understandable that it is challenging for GPT to predict demand when price equals 0 as such scenario is probably not well represented in its training data, the curve remains inverted-U shape even if we remove the first point corresponding to a price of zero.</p>
<h1>Prompt 2: Ask Purchase</h1>
<p>System: You, AI, are a customer. Your task is to fill in the blanks $\qquad$ Return the completed information in comma-separated values, without any extra text.</p>
<p>User: Please consider the following product category: {category}.</p>
<p>Suppose you are in a grocery store, and you see the following product in that category: {product}.</p>
<p>The product is currently priced at $\$$ {Price}. Would you or would you not purchase the product?
$\qquad$ ["purchase" or "not purchase"]</p>
<p>Return example: purchase</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Demand curve elicited from humans vs. LLM</p>
<p>These findings are not surprising in hindsight: in real observational data, the price of the focal product should be informative of the price of the competing products, especially if we do not know anything about the context, the type of store, and how the price for a product is set. An LLM trained on such data would also naturally make such association. This association can be realistic and useful if researchers intend to use LLMs to simulate competing prices based on the focal product price. However, this same association is problematic when researchers intend to estimate the causal impact of changing the focal price, while holding everything else constant, because it leads to endogeneity. One common approach to addressing such endogeneity is to control for additional covariates. The next section explores the benefits and novel challenges associated with this approach.</p>
<h1>3 Potential Solution: Controlling for Covariates</h1>
<p>When working with real-world data, a common approach for causal identification is to control for covariates. If researchers have controlled for all confounding covariates that affect both the treatment and outcome, the conditional independence assumption, or ignorability, is satisfied. If sufficient variation remains after controlling for covariates, then researchers can identify the causal effect of interest. In LLM-simulated experiments, a natural way to control for covariates is adding detailed information about individuals and their environment in the prompt. These covariates can range from demographic information, location, preferences, budget, etc. We discuss other approaches including RAG and fine-tuning in the Section 4.3.</p>
<p>Argyle et al. (2023) introduced an approach for controlling for such covariates. They advocate for running studies on "silicon samples" of digital personas, which in their case were described by demographic variables and political beliefs that were drawn to align with nationally representative samples. This approach has been picked up by the industry and commercialized by several firms (e.g., Synthetic Users, Evidenza). Figure 3 shows that on average, controlling for covariates, such as demographic variables does indeed improve the</p>
<p>results in our case compared to not specifying any detail. However, it does not fully resolve the issue. Appendix D describes the detailed prompts and covariates used, which involve the creating of 500 personas for each ${$ product,category $}$ pair.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Demand curves elicited by LLM after controlling for demographic variables</p>
<p>An important consideration is the stage at which covariates are controlled for. When working with human data, empirical researchers often control for product characteristics and rich user history to address endogeneity during data analysis, after the data is already collected. Hence analyzing data does not change the original data generating process that already took place. In contrast, adding covariates in the prompt in LLM simulation occurs at the data generation stage. Consequently, the additional covariates can change the factors considered by a simulated individual when making decisions.</p>
<p>For example, in the context of demand estimation, one can include contextual information in the prompt such as the price of competing products in the category, which can be used as a proxy for the type of stores that the customer is visiting and the competitive environment that the focal product is facing. However, while adding contextual variables such as competing prices in the LLM simulation may prevent the LLM from using the price treatment to infer these contextual variables, adding them may also lead the LLMsimulated customer to put a heavier emphasis on these factors when making decisions. Figure 4 shows that the demand curve (elicited by simulating the choices of 500 personas) is no longer smooth after fixing such contextual variables, ${ }^{5}$ and the simulated customers' purchase decisions follow a step function: customers will purchase if and only if the price is lower than or equal to the competing price. Appendix E describes the detailed prompts. ${ }^{6}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Purchase probability of Coca-Cola when controlling for demographic variables and competing price</p>
<p>Such phenomenon in which decision makers place more emphasis on certain aspects of a choice set or scenario than they would in real-world decisions is known in the consumer behavior literature as focalism (Schkade and Kahneman, 1998; Wilson et al., 2000), and constitutes a threat to ecological validity. For example, in the context of conjoint analysis, Bedi and Reibstein (2021) argue that when minor attributes are presented together with major ones, they tend to receive undue attention from respondents (due to focalism), leading to upward biased estimates of the importance of these minor features. While it has been documented that LLM is prone to focalism (Cheng et al., 2023), it is important to emphasize that focalism is not an issue that is unique to LLM experiments.</p>
<p>While traditional experiments with humans also run the risk of failing to simulate realistic scenarios, the nature of this risk differs. When running experiments with humans, using a blind design with realistic scenarios typically ensures ecological validity without introducing concerns of confoundedness. On the other hand, with LLM simulations, a blind design raises confoundedness concerns. Adressing confoundedness by adding covariates requires adding covariates in the data generation stage of the study, which raises yet another concern, focalism, which in turn reduces ecological validity. In other words, LLM simulations pose a unique dilemma to researchers. Improving ecological validity may require leaving unspecified details in the prompt, which runs the risk of introducing confoundedness. On the other hand, including extensive detail may reduce confounding, but at the expense of ecological validity due to focalism.</p>
<h1>4 Theoretical Framework</h1>
<p>So far we have demonstrated one empirical example with one specific version of an LLM. A natural question is whether the issue we have uncovered applies generally to all LLMs. This section presents a theoretical framework demonstrating that the challenges identified in this paper stems from ambiguous prompts rather than fundamental limitations of the models themselves. Therefore, the solution to this specific challenge lies not only in developing more advanced LLMs, but also in developing clearer prompting strategies.</p>
<h1>4.1 Blinding Generates Ambiguous Prompts</h1>
<p>To demonstrate how ambiguity arises in LLM prompts, we begin by examining a simple prompt $p$ under a blinded simulation design that illustrates the fundamental problem:</p>
<h2>Prompt 3: A simple ambiguous prompt</h2>
<p>Please consider the following product category: {category}.</p>
<p>Suppose you are in a grocery store, and you see the following product in that category: {product}.</p>
<p>The product is priced at: {price}. Would you or would you not purchase the product?</p>
<p>The design being blind, incomplete information is provided about the distribution from which the various variables in the prompt are drawn. As a result, this prompt is ambiguous. It allows for at least two interpretations:</p>
<ul>
<li>$q_{1}$ (interventional): what is the customer's purchase decision when the price is set to be ${$ price $}$ ?</li>
<li>$q_{2}$ (observational): what is the customer's purchase decision when the price happens to be ${$ price $}$ ?</li>
</ul>
<p>The answers to these two questions are likely to be different. Using the do-operator (Pearl, 2009) to distinguish mere observations and active intervention, and using $Y$ to represent Purchase and $D$ to represent Price, the answer to $q_{1}$ can be represented as $a_{1}: P(Y \mid d o(D))$ while the answer to $q_{2}$ can be represented as $a_{2}: P(Y \mid D)$.</p>
<p>Consider a true data-generating process (DGP) described by a linear structural equation:</p>
<p>$$
Y=\beta_{M} D+U
$$</p>
<p>where $\beta_{M}$ represents the causal effect of $D$ on $Y$ through a set of realistic mechanism $M$, and $U$ represents unobserved factors. In a well-designed experiment, we have $\operatorname{Cov}(U, D)=0$, making it possible to identify $\beta_{M}$ and estimate $P(Y \mid d o(D))$ from data on $(Y, D)$ alone. But under a blinded design with an ambiguous prompt, the LLM does not know it should mimic an experimental environment. That is, the LLM does not have enough information to draw $U$ from the appropriate distribution (literally or figuratively - we do not attempt to model how the LLM makes inference but rather present a conceptual model). If the model is mostly trained on observational data, it is mostly likely going to mimic observational data patterns, where $\operatorname{Cov}(U, D) \neq 0$. As a result, the LLM's responses will reflect confounded relationships rather than the intended causal effects. Figure 5 summarizes two possible DGPs mimicked by the LLM under this simple ambiguous prompt.</p>
<p>As discussed in Section 3, one might attempt to address this issue by adding more variables $X$ to the prompt. In principle, if $\operatorname{Cov}(U, D \mid X)=0$, then controlling for $X$ can help restore identification. However,</p>
<p>(a) DGP for Experimental Data
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) DGP for Observational Data
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Possible DGPs mimicked by LLM simulations under ambiguous simple prompts
simply specifying covariates in the LLM prompt does more than condition on them statistically. Including $X$ may cause the simulation to alter the underlying decision-making mechanisms $M$ and shift toward a new set of mechanisms $M^{\prime}$ and produce an environment that differs substantially from the original scenario the estimated parameter $\beta_{M^{\prime}}$ may have limited ecological validity, since it no longer reflects the realistic mechanisms $M$ that were originally of interest (as we illustrated in Figure 4). Figure 6 summarizes two possible DGPs mimicked by the LLM under this more complicated ambiguous prompt.
(a) Adding $X$ may address confounding
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Adding $X$ may also sacrifice ecological validity
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Potential DGPs mimicked by LLM simulations when controlling for additional variables</p>
<p>These graphs reveal a deeper challenge: adding variables to the prompt can actually increase rather than decrease ambiguity from a causal perspective. To illustrate this multiplication of possible interpretations in practice, consider the following detailed example:</p>
<h1>Prompt 4: More Detailed Scenario with Ambiguous Treatment</h1>
<p>User: You are a customer who has recently read an article about how sugar consumption affects health. You are now in a store and you see Regular Coke (39g sugar per 12 oz, 12 pack cans) priced at $\$ 4.99$, with a soda tax of $\$ 0.12 / \mathrm{oz}$. Would you or would you not purchase the product?</p>
<p>This prompt, while detailed, is even more ambiguous. It permits more valid causal interpretations:</p>
<ol>
<li>Price as Treatment: $P(Y \mid d o($ price), health_info, sugar_content)
"What happens when we experimentally set price to $\$ 4.99$, holding fixed that customers are health-</li>
</ol>
<p>aware?"
2. Health Information as Treatment: $P(Y \mid$ price, $d o($ health_info), sugar_content): "What happens when we experimentally expose customers to health articles, holding fixed price at $\$ 4.99$ ?"
3. Sugar Content as Treatment: $P(Y \mid$ price, health_info, $d o($ sugar_content $)$ ): "What happens when we experimentally highlight sugar content, holding fixed price and prior health awareness?"</p>
<h1>4.2 Impossibility Theorem for Ambiguous Prompting Strategies</h1>
<p>Having demonstrated how ambiguity can arise in practice, we now formalize these insights into a theoretical framework that explains why such ambiguity poses fundamental challenges for LLM evaluation and development.</p>
<p>Definition 4.1 (Functional Representation of a Large Language Model). A large language model may be represented by:</p>
<p>$$
f: \mathcal{P} \rightarrow \Delta(\mathcal{Y})
$$</p>
<p>where $\mathcal{P}$ is the set of all possible prompts, $\mathcal{Y}$ is the set of all possible outputs or responses, and $\Delta(\mathcal{Y})$ represents the set of all probability distributions over $\mathcal{Y}$. For any given prompt $p \in \mathcal{P}$, the model $f$ outputs a probability distribution $f(p)$ over potential responses $y \in \mathcal{Y}$.</p>
<p>For example, GPT-4 with temperature $=1$ can be thought of as a function $f$. Prompt 2 with category="soft drinks", product="12-can package of Coca-Cola", and price=" $\$ 4.99$ ", represents a specific prompt $p$. The function $f$ maps this prompt to a probability distribution over $\mathcal{Y}={$ "purchase", "not purchase" $} .{ }^{7}$ When using LLMs for simulations, researchers typically have a set of questions $\mathcal{Q}$ and need procedures to translate these questions into prompts. This translation procedure can be defined as a prompting strategy:</p>
<p>Definition 4.2 (Prompting Strategy). A prompting strategy is a function:</p>
<p>$$
s: \mathcal{Q} \rightarrow \mathcal{P}
$$</p>
<p>that maps each question $q \in \mathcal{Q}$ to a specific prompt $p \in \mathcal{P}$.
While prompting strategies can be refined empirically using validation datasets, developing a theoretical framework is essential for understanding when and why these strategies will generalize beyond the validation set. When ground truth data is available, researchers can experiment with different prompts and even fine-tune LLMs to improve performance on specific questions. However, without theoretical guidance, such empirical refinements may not yield strategies that reliably generalize to novel scenarios. This raises a fundamental question: what characteristics make a prompting strategy robust and reliable for out-of-distribution inference? We identify a particular class of prompting strategies to avoid:</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Definition 4.3 (Ambiguous Prompting Strategy). An ambiguous prompting strategy occurs when there exists distinct questions $q_{1}$ and $q_{2}$ that have different correct answers, but are mapped to the same prompt $p$ : $s\left(q_{1}\right)=s\left(q_{2}\right)=p$ for some prompt $p \in \mathcal{P}$.</p>
<p>Such ambiguous prompting strategies should be avoided because no LLM models, no matter how advanced they are, can answer all questions correctly given such prompting strategy:</p>
<p>Theorem 4.1. [Impossibility Theorem for Ambiguous Prompting Strategies] For a given ambiguous prompting strategy, there exists no LLM $f$ that can correctly answer all questions correctly.</p>
<p>Proof. Assume that $s\left(q_{1}\right)=s\left(q_{2}\right)=p$ for two distinct questions $q_{1}$ and $q_{2}$ that have different correct answers $a_{1}$ and $a_{2}$, with $a_{1} \neq a_{2}$. If the model can correctly answer $q_{1}$, such that $f\left(s\left(q_{1}\right)\right)=f(p)=a_{1}$, then the model cannot answer $q_{2}$ because $f\left(s\left(q_{2}\right)\right)=f(p)=a_{1} \neq a_{2}$.</p>
<h1>4.3 Implications for model development using fine-tuning or RAG</h1>
<p>Theorem 4.1 reveals how ambiguous prompting strategies create limitations when developing and deploying LLMs, even with techniques like fine-tuning or retrieval-augmented generation (RAG). Suppose developers leverage additional training data $\left(q_{1}, a_{1}\right)$ with a prompting strategy where $p=s\left(q_{1}\right)$. If this prompt is ambiguous such that $p=s\left(q_{2}\right)$ for some other question $q_{2}$ with correct answer $a_{2} \neq a_{1}$, then any fine-tuning that optimizes the model's accuracy on $q_{1}$ necessarily sacrifices its accuracy on $q_{2}$. This trade-off is especially problematic given uncertainty about future applications - users may interpret the same prompts in different ways which developers cannot anticipate during model development.</p>
<p>Consider a concrete example where developers have access to an experimental dataset that randomizes Coca-Cola's price while keeping Pepsi's price at business-as-usual levels. If developers fine-tune an LLM using a blinded design that simply describes purchase decisions at different Coca-Cola prices without revealing their experimental nature, they create an ambiguous prompt that permits both experimental and observational interpretations. While the fine-tuned model may excel at predicting purchases in scenarios matching the training data $\left(q_{1}\right)$, this capability is less valuable since the experimental data already directly answers such questions. The more important question is whether the model can learn the causal relationship between price and purchases, and can generalize to answer novel questions $\left(q_{2}\right)$ - like what happens when Pepsi's price changes while Coca-Cola's price remains at business-as-usual levels, or when both prices increase simultaneously. Such generalization proves challenging theoretically.</p>
<p>First, under ambiguous prompts, fine-tuning based on experimental data provides no guarantee that the model will learn causal relationships. Suppose that the data generating process for the experimental dataset follows $Y=\beta_{M} D+U$, where $Y$ represents the purchase decision, $D$ represents the price of CocaCola, and $U$ represents everything else. The fine-tuned model may implicitly learn a regression coefficient $\widehat{b}=\frac{\operatorname{Cov}(Y, D)}{\operatorname{Var}(D)}=\beta_{M}+\frac{\operatorname{Cov}(U, D)}{\operatorname{Var}(D)}$. However, since the fine-tuned model builds on the base model, it can</p>
<p>maintain a prior that $\frac{\operatorname{Cov}(U, D)}{\operatorname{Var}(D)}=\rho \neq 0$ and hence will not interpret the regression coefficient $\widehat{b}$ as causal. After accounting for this prior, the model may even incorrectly assume that the causal effect is $\widehat{\beta}_{M}=\widehat{b}-\rho$.</p>
<p>Furthermore, even if the model somehow updates its assumptions correctly to handle one specific type of question $\left(q_{1}\right)$, it may fail to generalize to other closely related but distinct questions $\left(q_{2}\right)$ where different assumptions are needed. For example, suppose the model learns from experimental data where Coca-Cola's price was randomized while Pepsi's price remained business-as-usual. Through fine-tuning, the model might correctly learn to treat Coca-Cola's price as independent of other unobserved market factors when analyzing that specific scenario $\left(q_{1}\right)$. However, when asked to predict what happens when Pepsi's price is set to another level while Coca-Cola's price remain business-as-usual $\left(q_{2}\right)$, the model might incorrectly maintain this assumption of independence, when in fact Coca-Cola's business-as-usual price would likely be correlated with other market factors in this new scenario. This limitation is avoidable if the experimental nature of the data is explicitly communicated to the model during both training and evaluation through less ambiguous prompts.</p>
<p>While the above example focuses on a case with one experimental dataset, the challenge persists even when using diverse experimental datasets for fine-tuning or retrieval-augmented generation (RAG). With RAG, this challenge manifests at both the development and deployment stages. During model development, the reference datasets should be annotated with metadata specifying which variables were randomized treatments versus correlated covariates. During retrieval, prompts should unambiguously specify the causal question to ensure the model draws from experiments with matching causal structures rather than just similar surface features. Without such explicit specification of the causal framework, neither fine-tuning nor RAG can reliably learn to distinguish between different types of variation in the data.</p>
<p>To summarize, our theoretical framework proves that no general-purpose model can accurately simulate all counterfactual scenarios under an ambiguous prompting strategy. This statement should not be interpreted as suggesting that fine-tuning or RAG are not valuable tools - these techniques can substantially enhance model performance by incorporating high-quality training data and reducing various forms of bias. Rather, it demonstrates that while better training data and advanced techniques are important for improving simulation accuracy, they are complementary to, rather than substitutes for, clear and unambiguous prompt design. While obtaining better data for validation, fine-tuning, or bias-correction can be resource-intensive, implementing unambiguous simulation designs may be achieved at no additional cost. The optimal approach combines both strategies: leveraging advanced training techniques while ensuring an unambiguous prompting strategy.</p>
<h1>5 Unblinding as a Potential Solution</h1>
<p>Our theoretical framework points toward a promising direction: designing unambiguous prompts by explicitly communicating the experimental design in the prompt, which by definition entails unblinding. Here we</p>
<p>discuss several possible simulation designs that leverage unblinding.
One approach maintains the between-prompt structure where each prompt simulates one experimental condition, but explicitly reveals the full experimental design in each prompt. This allows generating multiple responses to form a distribution for each condition while maintaining the independence of observations across prompts. Another possibility is to use a within-prompt design where multiple experimental conditions are presented together in a single prompt with full information on the experimental design. This approach is more efficient computationally since it obtains responses for multiple conditions in one call to the model. However, showing multiple conditions together may induce artificial consistency in the responses. ${ }^{8}$ An even simpler approach is an unblind within-prompt aggregate design that directly asks the LLM to predict the aggregate results of the full experiment. This represents the greatest departure from traditional experimental protocols with human subjects.</p>
<p>Comparing the relative performance of these different approaches is beyond the scope of this paper. Unlike our analysis of blinding - where we demonstrated fundamental limitations that will persist across model generations - the effectiveness of different unblinded designs may vary substantially across models and contexts. The relative performance one may find now may no longer hold in the future, especially if future models are trained or fine-tuned based on these prompts. Due to this reason, this section only focuses on demonstrating the feasibility of the unblinded design through the simplest approach of directly eliciting aggregate predictions of the results of the experiment.</p>
<p>For this demonstration, we directly ask the LLM to predict aggregate experimental results across randomly assigned prices. Figure 7 shows that while this strategy's predictions still deviate from actual consumer behavior, it does outperform the other potential solutions tested in this paper. Appendix F documents the detailed prompts.</p>
<p>This approach is by no means perfect. In addition to the fact that the result still deviates from the ground truth, Figure F. 1 shows that the results are also sensitive to the experimental range provided in the prompt: when the demand curve is built by combining two experiments that vary prices from $[0 \%, 100 \%]$ and from $[100 \%, 200 \%]$ of the regular price respectively, rather than a single experiment with prices varying from $[0 \%, 200 \%]$, the estimates change. There are several possible reasons for this. One is that this LLM is unable to correctly process experimental information. Another possibility is that the existing prompt still has some remaining ambiguity. For example, the LLM may make assumptions about the setting, such as the type of stores and the customers, based on the set of prices included in the experiment. Appendix F conducts additional analysis which suggests that this sensitivity to the experimental range is partly driven by the current LLMs not being able to correctly process experimental design information, and shows that newer LLMs demonstrate improved (though still imperfect) consistency across different experimental ranges.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Unblinded design asking directly for aggregate predictions</p>
<p>Therefore, unlike the fundamental limitations of blinding that future LLMs may not easily address, sensitivity to experimental design information could potentially be resolved through model improvements, making this a promising area for future research.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Unblinded design asking directly for aggregate predictions: price range comparison</p>
<h1>6 Conclusion</h1>
<p>This paper identifies a fundamental challenge in using LLMs to simulate experiments with human participants. Unlike human participants who exist independently of experiments, LLM-simulated individuals and</p>
<p>environments are generated based on prompts. While LLMs' ability to capture associations from training data is generally beneficial, it creates complications in experimental settings where changing one variable can unintentionally affect other unspecified variables meant to remain constant. This confounding makes it difficult to accurately simulate counterfactual human behavior. More generally and fundamentally, making the LLM blind to the experimental design leads to ambiguous prompts. Given LLMs are trained primarily on observational data, ambiguous prompts tend to favor correlational rather than causal observations, making variables unspecified in the prompt susceptible to the intended intervention. Because these challenges stem from ambiguous prompting strategies rather than model limitations, our theoretical framework suggests that it is not possible to fully resolve it through improved training data.</p>
<p>Our findings have important practical implications for different stakeholders in the LLM ecosystem. For researchers evaluating LLM performance, our results caution against using blinded designs when assessing simulation capabilities. Poor performance in such settings may reflect prompt ambiguity rather than model limitations, making it impossible to properly diagnose model capabilities. For academic researchers studying LLMs, our work highlights the need to establish consensus on unambiguous prompting strategies. Without such consensus, different research teams may map distinct questions to identical prompts, leading to conflicting interpretations that no model can simultaneously satisfy. Perhaps most importantly, our findings suggest avoiding ambiguous prompts during the development and fine-tuning of LLMs. When prompts permit multiple interpretations, improving performance on one interpretation may degrade performance on others - an unnecessary trade-off that could be avoided through clearer prompting strategies, especially given uncertainty about future applications.</p>
<p>We explored several potential solutions to this challenge. Adding detailed context to control for confounders showed promise but tends to introduce focalism which could invalidate results. A more promising approach consists in unblinded experimental designs that explicitly communicate randomization. This approach showed potential for reducing ambiguity, though its effectiveness may vary across models and contexts. Therefore, developing LLMs that can better follow the instruction to simulate counterfactual human behavior given unambiguous prompts can be a crucial direction for future research.</p>
<p>Our paper highlights a broader insight of using LLMs: rather than simply applying human subject protocols to LLMs, researchers need to develop new experimental practices tailored to LLMs' unique properties. The field of LLM simulations is still emerging, and best practices remain to be established. Our work provides a theoretical foundation for understanding one of the key challenges and evaluating potential solutions. We hope this framework will guide future research in developing more reliable methods for leveraging LLMs in experimental research, while remaining mindful of their fundamental differences from human subjects.</p>
<h1>References</h1>
<p>Aher, G. V., Arriaga, R. I., and Kalai, A. T. (2023). Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. In Proceedings of the 40th International Conference on Machine Learning, pages 337-371. PMLR. ISSN: 2640-3498.</p>
<p>Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., and Wingate, D. (2023). Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis, pages 1-15. Publisher: Cambridge University Press.</p>
<p>Arora, N., Chakraborty, I., and Nishimura, Y. (2024). Express: Ai-human hybrids for marketing research: Leveraging llms as collaborators. Journal of Marketing, page 00222429241276529.</p>
<p>Association, A. P. et al. (2019). Publication manual of the american psychological association, (2020). American Psychological Association, 428.</p>
<p>Bail, C. A. (2024). Can generative ai improve social science? Proceedings of the National Academy of Sciences, 121(21):e2314021121.</p>
<p>Bedi, S. and Reibstein, D. (2021). Damaged damages: Errors in patent and false advertising litigation. Ala. L. Rev., 73:385.</p>
<p>Brand, J., Israeli, A., and Ngwe, D. (2023). Using GPT for Market Research.
Carlson, N. and Burbano, V. (2024). The use of llms to annotate data in management research: Warnings, guidelines, and an application to organizational communication. Guidelines, and an Application to Organizational Communication (May 21, 2024).</p>
<p>Cheng, M., Piccardi, T., and Yang, D. (2023). Compost: Characterizing and evaluating caricature in llm simulations. arXiv preprint arXiv:2310.11501.</p>
<p>DellaVigna, S. and Gentzkow, M. (2019). Uniform Pricing in U.S. Retail Chains*. The Quarterly Journal of Economics, 134(4):2011-2084.</p>
<p>Dillion, D., Tandon, N., Gu, Y., and Gray, K. (2023). Can AI language models replace human participants? Trends in Cognitive Sciences, 0(0). Publisher: Elsevier.</p>
<p>Gao, Y., Lee, D., Burtch, G., and Fazelpour, S. (2024). Take caution in using llms as human surrogates: Scylla ex machina. arXiv preprint arXiv:2410.19599.</p>
<p>Goli, A. and Singh, A. (2024). Frontiers: Can large language models capture human preferences? Marketing Science.</p>
<p>Horton, J. J. (2023). Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus? arXiv:2301.07543 [econ, q-fin].</p>
<p>Hutson, M. and Mastin, A. (2023). Guinea pigbots. Science (New York, NY), 381(6654):121-123.</p>
<p>Kurita, K., Vyas, N., Pareek, A., Black, A. W., and Tsvetkov, Y. (2019). Measuring bias in contextualized word representations. arXiv preprint arXiv:1906.07337.</p>
<p>Ludwig, J., Mullainathan, S., and Rambachan, A. (2024). Large language models: An applied econometric framework. arXiv preprint arXiv:2412.07031.</p>
<p>Mohammadi, B. (2024). Wait, it's all token noise? always has been: interpreting llm behavior using shapley value. Available at SSRN 4759713.</p>
<p>Nozza, D., Bianchi, F., Hovy, D., et al. (2022). Pipelines for social bias testing of large language models. In Proceedings of BigScience Episode# 5-Workshop on Challenges $\mathcal{G}$ Perspectives in Creating Large Language Models. Association for Computational Linguistics.</p>
<p>Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442 [cs].</p>
<p>Pearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge, UK, 2 edition.</p>
<p>Sarstedt, M., Adler, S. J., Rau, L., and Schmitt, B. (2024). Using large language models to generate silicon samples in consumer and marketing research: Challenges, opportunities, and guidelines. Psychology $\mathcal{G}$ Marketing, 41(6):1254-1270.</p>
<p>Schkade, D. A. and Kahneman, D. (1998). Does living in california make people happy? a focusing illusion in judgments of life satisfaction. Psychological science, 9(5):340-346.</p>
<p>Simmons, G. and Hare, C. (2023). Large language models as subpopulation representative models: A review. arXiv preprint arXiv:2310.17888.</p>
<p>Wang, M., Zhang, D. J., and Zhang, H. (2024). Large language models for market research: A dataaugmentation approach. arXiv preprint arXiv:2412.19363.</p>
<p>Wilson, T. D., Wheatley, T., Meyers, J. M., Gilbert, D. T., and Axsom, D. (2000). Focalism: a source of durability bias in affective forecasting. Journal of personality and social psychology, 78(5):821.</p>
<p>Zhang, H., Lu, A. X., Abdalla, M., McDermott, M., and Ghassemi, M. (2020). Hurtful words: quantifying biases in clinical contextual word embeddings. In proceedings of the ACM Conference on Health, Inference, and Learning, pages 110-120.</p>
<h1>A Category, Product and Price Information</h1>
<p>Table A.1: Categories, products and regular prices</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Product</th>
<th style="text-align: center;">Price (\$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fruit Juice</td>
<td style="text-align: center;">Capri Sun Variety Pack with Fruit Punch, Strawberry Kiwi \&amp; Pacific Cooler Juice Box Pouches, 30 ct Box, 6 fl oz Pouches</td>
<td style="text-align: center;">9.43</td>
</tr>
<tr>
<td style="text-align: center;">Fruit Drinks</td>
<td style="text-align: center;">Kool Aid Jammers Variety Pack with Tropical Punch, Grape \&amp; Cherry Kids Drink 0\% Juice Box Pouches, 30 Ct Box, 6 fl oz Pouches</td>
<td style="text-align: center;">7.27</td>
</tr>
<tr>
<td style="text-align: center;">Baby Milk and Milk Flavoring</td>
<td style="text-align: center;">Horizon Organic Shelf-Stable Whole Milk Boxes, 8 oz., 12 Pack</td>
<td style="text-align: center;">13.98</td>
</tr>
<tr>
<td style="text-align: center;">Soup</td>
<td style="text-align: center;">Maruchan Ramen Noodle Chicken Flavor Soup, 3 Oz, 12 Count Shelf Stable Package</td>
<td style="text-align: center;">9.97</td>
</tr>
<tr>
<td style="text-align: center;">Cat Food - Wet Type</td>
<td style="text-align: center;">Purina Fancy Feast Chicken Feast Classic Grain Free Wet Cat Food Pate - 3 oz. Can</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;">Pet Supplies - Dog Food</td>
<td style="text-align: center;">Purina Dog Chow Complete, Dry Dog Food for Adult Dogs High Protein, Real Chicken, 44 lb Bag</td>
<td style="text-align: center;">29.17</td>
</tr>
<tr>
<td style="text-align: center;">Snacks - Potato Chips</td>
<td style="text-align: center;">Lay's Classic Potato Snack Chips, Party Size, 13 oz Bag</td>
<td style="text-align: center;">5.44</td>
</tr>
<tr>
<td style="text-align: center;">Snacks - Tortilla Chips</td>
<td style="text-align: center;">Doritos Nacho Cheese Tortilla Snack Chips, Party Size, 14.5 oz Bag</td>
<td style="text-align: center;">5.94</td>
</tr>
<tr>
<td style="text-align: center;">Cereal - Ready to Eat</td>
<td style="text-align: center;">Cinnamon Toast Crunch Breakfast Cereal, Crispy Cinnamon Cereal, Family Size, 18.8 oz</td>
<td style="text-align: center;">4.93</td>
</tr>
<tr>
<td style="text-align: center;">Cookies</td>
<td style="text-align: center;">Little Debbie Oatmeal Creme Pies, 12 ct, 16.2 oz</td>
<td style="text-align: center;">2.68</td>
</tr>
<tr>
<td style="text-align: center;">Ground and Whole Bean Coffee</td>
<td style="text-align: center;">Folgers Classic Roast Ground Coffee, Medium Roast, 40.3-Ounce Canister</td>
<td style="text-align: center;">13.24</td>
</tr>
<tr>
<td style="text-align: center;">Soft Drinks - Carbonated</td>
<td style="text-align: center;">Coca-Cola Soda Pop, 12 fl oz, 12 Pack Cans</td>
<td style="text-align: center;">8.26</td>
</tr>
<tr>
<td style="text-align: center;">Bottled Water</td>
<td style="text-align: center;">OZARKA Brand 100\% Natural Spring Water, 16.9-ounce plastic bottles (Pack of 35)</td>
<td style="text-align: center;">19.96</td>
</tr>
<tr>
<td style="text-align: center;">Candy - Chocolate</td>
<td style="text-align: center;">Hershey's Milk Chocolate Candy, Bars 1.55 oz, 6 Count</td>
<td style="text-align: center;">6.48</td>
</tr>
<tr>
<td style="text-align: center;">Candy - Non-Chocolate</td>
<td style="text-align: center;">HARIBO Goldbears Original Gummy Bears, 28.8 oz Stand Up Bag</td>
<td style="text-align: center;">6.48</td>
</tr>
<tr>
<td style="text-align: center;">Soft Drinks - Low Calorie</td>
<td style="text-align: center;">Coca-Cola Zero Sugar Soda Pop, 16.9 fl oz, 6 Pack Cans</td>
<td style="text-align: center;">5.18</td>
</tr>
<tr>
<td style="text-align: center;">Frozen Italian Entrees</td>
<td style="text-align: center;">Smart Ones Three Cheese Ziti Marinara Frozen Meal, 9 Oz Box</td>
<td style="text-align: center;">2.26</td>
</tr>
<tr>
<td style="text-align: center;">Frozen Foods</td>
<td style="text-align: center;">Great Value All Natural Chicken Wing Sections, 4 lb (Frozen)</td>
<td style="text-align: center;">12.98</td>
</tr>
<tr>
<td style="text-align: center;">Ice Cream</td>
<td style="text-align: center;">Haagen Daze Coffee Ice Cream, Gluten Free, Kosher, 14.0 oz</td>
<td style="text-align: center;">4.18</td>
</tr>
<tr>
<td style="text-align: center;">Frozen Novelties</td>
<td style="text-align: center;">Pop-Ice Assorted Fruit Freezer Ice Pops, Gluten-Free Snack, 1.5 oz, 80 Count Fruit Pops</td>
<td style="text-align: center;">6.17</td>
</tr>
<tr>
<td style="text-align: center;">Lunchmeat - Sliced - Refrigerated</td>
<td style="text-align: center;">Oscar Mayer Chopped Ham \&amp; Water product Deli Lunch Meat, 16 Oz Package</td>
<td style="text-align: center;">4.33</td>
</tr>
<tr>
<td style="text-align: center;">Frankfurters - Refrigerated</td>
<td style="text-align: center;">Oscar Mayer Classic Uncured Beef Franks Hot Dogs, 10 ct Pack</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: center;">Refrigerated Bacon</td>
<td style="text-align: center;">Oscar Mayer Fully Cooked Original Bacon, 2.52 oz Box</td>
<td style="text-align: center;">4.27</td>
</tr>
<tr>
<td style="text-align: center;">Refrigerated Entrees</td>
<td style="text-align: center;">John Soules Foods Chicken Breast Fajita Strips, Refrigerated, 16 oz, 18 g Protein per 3 oz Serving Size</td>
<td style="text-align: center;">5.98</td>
</tr>
<tr>
<td style="text-align: center;">Dairy Products</td>
<td style="text-align: center;">Land O Lakes Salted Stick Butter, 16 oz, 4 Sticks</td>
<td style="text-align: center;">5.28</td>
</tr>
<tr>
<td style="text-align: center;">Yogurt - Refrigerated</td>
<td style="text-align: center;">Chobani Non-Fat Greek Yogurt, Vanilla Blended 32 oz, Plastic</td>
<td style="text-align: center;">5.58</td>
</tr>
<tr>
<td style="text-align: center;">Refrigerated Deli Meats</td>
<td style="text-align: center;">Goya Cooked Ham 16 oz</td>
<td style="text-align: center;">29.99</td>
</tr>
<tr>
<td style="text-align: center;">Dairy - Milk - Refrigerated</td>
<td style="text-align: center;">Great Value Milk Whole Vitamin D Gallon</td>
<td style="text-align: center;">3.92</td>
</tr>
<tr>
<td style="text-align: center;">Bakery - Fresh Cakes</td>
<td style="text-align: center;">Little Debbie Zebra Cakes, 13 oz</td>
<td style="text-align: center;">2.68</td>
</tr>
<tr>
<td style="text-align: center;">Fresh Eggs</td>
<td style="text-align: center;">Eggland's Best Classic Extra Large White Eggs, 12 count</td>
<td style="text-align: center;">3.18</td>
</tr>
<tr>
<td style="text-align: center;">Fresh Fruit</td>
<td style="text-align: center;">Fresh Raspberries, 12 oz Container</td>
<td style="text-align: center;">4.74</td>
</tr>
<tr>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">Stella Artois Lager, 12 Pack, 11.2 fl oz Glass Bottles, 5\% ABV, Domestic Beer</td>
<td style="text-align: center;">15.73</td>
</tr>
<tr>
<td style="text-align: center;">Light Beer (Low Calorie/Alcohol)</td>
<td style="text-align: center;">Bud Light Beer, 24 Pack, 12 fl oz Aluminum Cans, 4.2\% ABV, Domestic Lager</td>
<td style="text-align: center;">20.98</td>
</tr>
<tr>
<td style="text-align: center;">Detergents - Heavy Duty - Liquid</td>
<td style="text-align: center;">Purex Liquid Laundry Detergent Plus OXI, Stain Defense Technology, 128 Fluid Ounces, 85 Wash Loads</td>
<td style="text-align: center;">9.97</td>
</tr>
<tr>
<td style="text-align: center;">Cleaning Supplies</td>
<td style="text-align: center;">ARM \&amp; HAMMER Pure Baking Soda, For Baking, Cleaning \&amp; Deodorizing, 1 lb Box</td>
<td style="text-align: center;">1.54</td>
</tr>
<tr>
<td style="text-align: center;">Toilet Tissue</td>
<td style="text-align: center;">Angel Soft Toilet Paper, 9 Mega Rolls, Soft and Strong Toilet Tissue</td>
<td style="text-align: center;">6.68</td>
</tr>
<tr>
<td style="text-align: center;">Paper Towels</td>
<td style="text-align: center;">Bounty Select-a-Size Paper Towels, 12 Double Rolls, White</td>
<td style="text-align: center;">22.18</td>
</tr>
<tr>
<td style="text-align: center;">Batteries</td>
<td style="text-align: center;">Duracell Cuppertop AA Battery, Long Lasting Double A Batteries, 16 Pack</td>
<td style="text-align: center;">15.97</td>
</tr>
<tr>
<td style="text-align: center;">Pain Remedies - Headache</td>
<td style="text-align: center;">Tylenol Extra Strength Caplets with 500 mg Acetaminophen, 100 Ct</td>
<td style="text-align: center;">10.97</td>
</tr>
<tr>
<td style="text-align: center;">Cold Remedies -Adult</td>
<td style="text-align: center;">Equate Value Size Honey Lemon Cough Drops with Menthol, 160 Count</td>
<td style="text-align: center;">4.68</td>
</tr>
</tbody>
</table>
<p>Note: The 40 categories were taken from DellaVigna and Gentzkow (2019) and the top products and their regular prices from Walmart.com as of April, 2024. Due to the dynamic nature of retail pricing, actual prices may vary.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ It is worth noting that within-prompt designs are not necessarily unblinded - a prompt could present multiple conditions (e.g., different prices) without explaining how those conditions were selected or distributed. Therefore, merely switching to within-prompt experiments is not sufficient to resolve prompt ambiguity.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
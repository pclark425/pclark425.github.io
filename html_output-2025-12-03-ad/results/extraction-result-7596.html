<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7596 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7596</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7596</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca" target="_blank">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference on unseen tasks, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.</p>
                <p><strong>Paper Abstract:</strong> Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can"prompt"the LM with the review and the label description"Does the user like this movie?", and ask whether the next word is"yes"or"no". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7596.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7596.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-tuning (QA Yes/No)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-tuning of pre-trained LMs on a collection of datasets in a Yes/No QA format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collect many classification datasets, convert each label into a natural-language question (Yes/No QA), and fine-tune a pretrained LM (T5-Large) on this meta-dataset to directly optimize zero-shot classification performance on unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (meta-tuned) / UnifiedQA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder architecture (text-to-text); inputs are concatenated context + [SEP] + question; prediction via probability of first decoded token ('Yes'/'No'). UnifiedQA is a T5-initialized QA model trained on diverse QA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters (T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify textual inputs using label descriptions phrased as natural-language questions without any training examples from the target task.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>"Yes"/"No" question answering prompt: provide context and a question encoding the label (e.g., 'Review: X. Positive review?'), then read probability of 'Yes' token.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no in-task examples). Each label converted to multiple manually-annotated question phrasings. Input assembled as context + [SEP] + question; normalized probability of first decoded token used for AUC-ROC. Meta-tuning sampling: dataset and question chosen uniformly; 50% positive/negative sampling; do not repeat same (input, question) pair twice. Default meta-tune: 5k steps, batch size 32.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC (per label-description)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+3.3% absolute AUC-ROC (average improvement over UnifiedQA when using meta-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>UnifiedQA (out-of-the-box QA model initialized from T5-Large) (baseline; absolute AUC-ROC per-label not reported in aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.3% absolute AUC-ROC (meta-tuned vs UnifiedQA)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Leave-one-group-out evaluation (exclude similar datasets by tag); meta-tune for 5000 steps (default), batch size 32; T5-Large initialization; concatenated context+[SEP]+question input; evaluation aggregates AUC-ROC per label description.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7596.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt wording / Label descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of specific natural-language label descriptions (prompt wording) on zero-shot performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different natural-language formulations of the same label/question can substantially change zero-shot accuracy; multiple manually annotated phrasings per label were used and compared, and problematic phrasings (e.g., abstract terms) hurt accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (meta-tuned and non-meta-tuned variants used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder, used to answer Yes/No questions derived from label descriptions; probability of 'Yes' token is treated as positive score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters (T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary), per-label-question evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer whether an input belongs to a label, where the label is expressed by a natural-language question/description.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single natural-language question (prompt) per label; multiple alternative questions annotated per label (authors annotated 441 questions for 204 labels).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question phrasing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors manually wrote 1-3 descriptions per label in QA form (some labels up to 13 questions originally but aggregate 441). Qualitative example: two phrasings for the same stance label ('Does this post support atheism?' vs 'Is the post against having religious beliefs?') produced substantially different accuracies; abstract grounding issues (e.g., 'atheism') can lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC (per question/label-description)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per-question performance varied widely; no single absolute aggregated number provided, but authors report many label-descriptions had notable gaps in accuracy (illustrated in scatter plots); qualitative large differences for certain phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative large differences between phrasings reported; in specific example one phrasing had 'much lower accuracy' than another (no exact numeric delta provided for that example).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Each label-description evaluated separately; meta-tuned and non-meta-tuned models scored per description; some experiments ensemble across multiple descriptions by averaging 'Yes' probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7596.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble of label descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembling multiple natural-language descriptions of the same label by averaging predicted probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Average the model's 'Yes' probability across multiple question phrasings for the same label to improve robustness and performance without training multiple models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (meta-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large encoder-decoder producing Yes/No token probabilities; ensembling performed at the prompt-output level by averaging probabilities from multiple phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict label by averaging outputs for multiple prompts corresponding to same label.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple natural-language questions (same label) asked independently; final 'Yes' score is average of probabilities across questions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Ensemble by averaging 'Yes' probabilities obtained from multiple annotated label descriptions; no additional training required for ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+0.7% absolute AUC-ROC (average improvement reported when ensembling descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Single-description prompt (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.7% absolute AUC-ROC vs single-question prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-tuned T5-Large; averaging across available descriptions per label; leave-one-group-out evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7596.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training on similar datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Including datasets similar to the test dataset during meta-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Allowing training on datasets that are grouped as similar to the test dataset provides a small positive gain in zero-shot performance (measured by AUC-ROC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (meta-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large meta-tuned on the meta-dataset; experimental condition toggles whether datasets tagged as similar to the held-out test dataset were allowed in training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate generalization to held-out task groups; compare strict holdout (exclude similar) vs relaxed leave-one-out (allow similar datasets in training).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same Yes/No QA prompt format; difference is in training split composition (presence/absence of similar datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training data selection / dataset similarity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors group datasets by tags (domain, genre, etc.) and experiment with leave-one-out cross-validation where the 'most similar' datasets are allowed into the training pool (contrasting with their stricter unseen definition).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+0.7% absolute AUC-ROC (average improvement when training included similar datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Strictly held-out similar dataset groups (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.7% absolute AUC-ROC vs strict holdout</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Leave-one-out cross-validation (allow training on similar datasets); otherwise same meta-tuning setup (5k steps, batch size 32).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7596.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Initialization with UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initializing meta-tuning from a QA-pretrained model (UnifiedQA) rather than vanilla T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Starting from a model already fine-tuned for QA (UnifiedQA) modestly improves zero-shot classification after meta-tuning compared to initializing from T5 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UNIFIEDQA: Crossing format boundaries with a single QA system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UnifiedQA (T5-Large initialization) -> meta-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>UnifiedQA: T5 initialized and then trained on a large mixture of QA tasks; used as an initialization for further meta-tuning on the curated Yes/No QA meta-dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters (T5-Large based UnifiedQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess benefit of QA-specific pre-finetuning (UnifiedQA) as initialization for meta-tuning on Yes/No prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Yes/No QA prompts; initialization differs (UnifiedQA vs T5 pretrained checkpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>initialization / pre-finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compare two initializations for meta-tuning: (a) T5-Large checkpoint, (b) UnifiedQA model (T5-Large pre-finetuned on QA data). Same meta-tuning procedure applied afterwards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+1.1% absolute AUC-ROC (average improvement when initializing with UnifiedQA vs T5 init)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T5-Large initialization (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+1.1% absolute AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-tuning on the curated meta-dataset; default 5000 steps, batch size 32; leave-one-group-out evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7596.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size / scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing pretrained model parameters (T5) on zero-shot QA prompt performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger pretrained models (T5 variants) produce higher zero-shot AUC-ROC on Yes/No QA formatted classification tasks: T5-Base (220M) vs T5-Large (770M) shows substantial gains; even larger gains from T5-small to T5-base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-small / T5-Base / T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of T5 text-to-text transformer models with different parameter counts, used as initialization for meta-tuning or direct QA prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>60M (T5-small), 220M (T5-Base), 770M (T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare zero-shot performance across model scales using the same QA Yes/No prompt format and meta-tuning procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Yes/No QA prompt (zero-shot); comparison across model sizes in both pretrained and meta-tuned settings.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model capacity / prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Direct comparison between T5-Base (220M) and T5-Large (770M) showed an average +6.3% absolute AUC-ROC improvement; T5-small (60M) to T5-base had a larger jump (reported up to ~14.4% depending on weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+6.3% absolute AUC-ROC (220M -> 770M, T5) averaged across label-descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T5-Base (220M) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+6.3% absolute AUC-ROC (T5-Base -> T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-tuning and direct comparisons; results consistent under multiple weighting schemes and with BERT-family scaling checks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7596.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining vs Random init</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of using pretrained weights vs random initialization when meta-tuning for QA prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining is critical: models initialized from pretrained checkpoints (T5) greatly outperform randomly initialized models after the same meta-tuning procedure on the Yes/No QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (pretrained) vs randomly initialized model (same architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 architecture; comparison between standard pretrained checkpoint and same-architecture model with random initialization, both subjected to meta-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters (when using T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether meta-tuning alone (without pretraining) suffices to learn zero-shot QA answering; compare pretrained vs random in meta-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Yes/No QA prompt format, meta-tuning on aggregated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>initialization / pretraining effect</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Randomly initialized models after meta-tuning perform at near-chance (avg. AUC-ROC ~0.503) while pretrained models achieve far higher performance; meta-tuning alone is insufficient without pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>+23.8% absolute AUC-ROC (pretrained vs random initialization, averaged change Δ)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random initialization (baseline; average AUC-ROC ≈ 0.503 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+23.8% absolute AUC-ROC (pretrained vs random)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-tuning procedure identical for both initializations (sampling, 5k steps default); evaluation on held-out dataset groups.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7596.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7596.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Early stopping / training length</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of extended meta-tuning and memorization of prompt-task mappings on unseen-task performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training the meta-tuned model for too many steps causes memorization of training task prompt mappings and reduces zero-shot generalization to unseen tasks; relative drop observed after extended training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (meta-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large meta-tuned on the Yes/No QA meta-dataset; compared performance trajectories across steps from 5k (default) to 100k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-Shot Classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Monitor average AUC-ROC on held-out zero-shot benchmark datasets as a function of meta-tuning steps to detect overfitting/memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Yes/No QA prompt format; training length varied (5k default vs 100k prolonged).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training regime / prompt memorization</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Default meta-tuning used 5k steps; extending to 100k steps (20x longer) produced decreasing performance on unseen tasks (authors report ~3% absolute AUC-ROC drop relative to step 5k in their curves). The authors attribute the drop to memorization of prompt-task pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC-ROC (relative to value at step 5000)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately -3% absolute AUC-ROC drop when training increased to 100k steps (relative to 5k step reference) for certain benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Performance at step 5000 (reference point)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-~3% absolute AUC-ROC (100k vs 5k steps, averaged across some benchmark datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-tuning extended to 100k steps; evaluation on three benchmark zero-shot datasets from Yin et al. (2019); plotted relative AUC-ROC vs steps.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Kendall rank correlation coefficients are negative with p < 0.005 for topic and situation classification (authors report significant negative correlations between training step and performance on those datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>UNIFIEDQA: Crossing format boundaries with a single QA system <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners <em>(Rating: 1)</em></li>
                <li>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <em>(Rating: 2)</em></li>
                <li>Exploiting cloze questions for few-shot text classification and natural language inference <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>How many data points is a prompt worth? <em>(Rating: 2)</em></li>
                <li>Natural instructions: Benchmarking generalization to new tasks from natural language instructions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7596",
    "paper_id": "paper-4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Meta-tuning (QA Yes/No)",
            "name_full": "Meta-tuning of pre-trained LMs on a collection of datasets in a Yes/No QA format",
            "brief_description": "Collect many classification datasets, convert each label into a natural-language question (Yes/No QA), and fine-tune a pretrained LM (T5-Large) on this meta-dataset to directly optimize zero-shot classification performance on unseen tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (meta-tuned) / UnifiedQA (baseline)",
            "model_description": "T5 encoder-decoder architecture (text-to-text); inputs are concatenated context + [SEP] + question; prediction via probability of first decoded token ('Yes'/'No'). UnifiedQA is a T5-initialized QA model trained on diverse QA datasets.",
            "model_size": "770M parameters (T5-Large)",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Classify textual inputs using label descriptions phrased as natural-language questions without any training examples from the target task.",
            "problem_format": "\"Yes\"/\"No\" question answering prompt: provide context and a question encoding the label (e.g., 'Review: X. Positive review?'), then read probability of 'Yes' token.",
            "format_category": "question type / prompt style",
            "format_details": "Zero-shot (no in-task examples). Each label converted to multiple manually-annotated question phrasings. Input assembled as context + [SEP] + question; normalized probability of first decoded token used for AUC-ROC. Meta-tuning sampling: dataset and question chosen uniformly; 50% positive/negative sampling; do not repeat same (input, question) pair twice. Default meta-tune: 5k steps, batch size 32.",
            "performance_metric": "AUC-ROC (per label-description)",
            "performance_value": "+3.3% absolute AUC-ROC (average improvement over UnifiedQA when using meta-tuning)",
            "baseline_performance": "UnifiedQA (out-of-the-box QA model initialized from T5-Large) (baseline; absolute AUC-ROC per-label not reported in aggregate)",
            "performance_change": "+3.3% absolute AUC-ROC (meta-tuned vs UnifiedQA)",
            "experimental_setting": "Leave-one-group-out evaluation (exclude similar datasets by tag); meta-tune for 5000 steps (default), batch size 32; T5-Large initialization; concatenated context+[SEP]+question input; evaluation aggregates AUC-ROC per label description.",
            "statistical_significance": null,
            "uuid": "e7596.0",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Prompt wording / Label descriptions",
            "name_full": "Effect of specific natural-language label descriptions (prompt wording) on zero-shot performance",
            "brief_description": "Different natural-language formulations of the same label/question can substantially change zero-shot accuracy; multiple manually annotated phrasings per label were used and compared, and problematic phrasings (e.g., abstract terms) hurt accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (meta-tuned and non-meta-tuned variants used)",
            "model_description": "T5 encoder-decoder, used to answer Yes/No questions derived from label descriptions; probability of 'Yes' token is treated as positive score.",
            "model_size": "770M parameters (T5-Large)",
            "task_name": "Zero-Shot Classification (binary), per-label-question evaluation",
            "task_description": "Answer whether an input belongs to a label, where the label is expressed by a natural-language question/description.",
            "problem_format": "Single natural-language question (prompt) per label; multiple alternative questions annotated per label (authors annotated 441 questions for 204 labels).",
            "format_category": "prompt style / question phrasing",
            "format_details": "Authors manually wrote 1-3 descriptions per label in QA form (some labels up to 13 questions originally but aggregate 441). Qualitative example: two phrasings for the same stance label ('Does this post support atheism?' vs 'Is the post against having religious beliefs?') produced substantially different accuracies; abstract grounding issues (e.g., 'atheism') can lower performance.",
            "performance_metric": "AUC-ROC (per question/label-description)",
            "performance_value": "Per-question performance varied widely; no single absolute aggregated number provided, but authors report many label-descriptions had notable gaps in accuracy (illustrated in scatter plots); qualitative large differences for certain phrasings.",
            "baseline_performance": null,
            "performance_change": "Qualitative large differences between phrasings reported; in specific example one phrasing had 'much lower accuracy' than another (no exact numeric delta provided for that example).",
            "experimental_setting": "Each label-description evaluated separately; meta-tuned and non-meta-tuned models scored per description; some experiments ensemble across multiple descriptions by averaging 'Yes' probabilities.",
            "statistical_significance": null,
            "uuid": "e7596.1",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Ensemble of label descriptions",
            "name_full": "Ensembling multiple natural-language descriptions of the same label by averaging predicted probabilities",
            "brief_description": "Average the model's 'Yes' probability across multiple question phrasings for the same label to improve robustness and performance without training multiple models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (meta-tuned)",
            "model_description": "T5-Large encoder-decoder producing Yes/No token probabilities; ensembling performed at the prompt-output level by averaging probabilities from multiple phrasings.",
            "model_size": "770M parameters",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Predict label by averaging outputs for multiple prompts corresponding to same label.",
            "problem_format": "Multiple natural-language questions (same label) asked independently; final 'Yes' score is average of probabilities across questions.",
            "format_category": "prompt style / ensembling",
            "format_details": "Ensemble by averaging 'Yes' probabilities obtained from multiple annotated label descriptions; no additional training required for ensembling.",
            "performance_metric": "AUC-ROC",
            "performance_value": "+0.7% absolute AUC-ROC (average improvement reported when ensembling descriptions)",
            "baseline_performance": "Single-description prompt (baseline)",
            "performance_change": "+0.7% absolute AUC-ROC vs single-question prompt",
            "experimental_setting": "Meta-tuned T5-Large; averaging across available descriptions per label; leave-one-group-out evaluation.",
            "statistical_significance": null,
            "uuid": "e7596.2",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Training on similar datasets",
            "name_full": "Including datasets similar to the test dataset during meta-tuning",
            "brief_description": "Allowing training on datasets that are grouped as similar to the test dataset provides a small positive gain in zero-shot performance (measured by AUC-ROC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (meta-tuned)",
            "model_description": "T5-Large meta-tuned on the meta-dataset; experimental condition toggles whether datasets tagged as similar to the held-out test dataset were allowed in training.",
            "model_size": "770M parameters",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Evaluate generalization to held-out task groups; compare strict holdout (exclude similar) vs relaxed leave-one-out (allow similar datasets in training).",
            "problem_format": "Same Yes/No QA prompt format; difference is in training split composition (presence/absence of similar datasets).",
            "format_category": "training data selection / dataset similarity",
            "format_details": "Authors group datasets by tags (domain, genre, etc.) and experiment with leave-one-out cross-validation where the 'most similar' datasets are allowed into the training pool (contrasting with their stricter unseen definition).",
            "performance_metric": "AUC-ROC",
            "performance_value": "+0.7% absolute AUC-ROC (average improvement when training included similar datasets)",
            "baseline_performance": "Strictly held-out similar dataset groups (baseline)",
            "performance_change": "+0.7% absolute AUC-ROC vs strict holdout",
            "experimental_setting": "Leave-one-out cross-validation (allow training on similar datasets); otherwise same meta-tuning setup (5k steps, batch size 32).",
            "statistical_significance": null,
            "uuid": "e7596.3",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Initialization with UnifiedQA",
            "name_full": "Initializing meta-tuning from a QA-pretrained model (UnifiedQA) rather than vanilla T5",
            "brief_description": "Starting from a model already fine-tuned for QA (UnifiedQA) modestly improves zero-shot classification after meta-tuning compared to initializing from T5 alone.",
            "citation_title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "mention_or_use": "use",
            "model_name": "UnifiedQA (T5-Large initialization) -&gt; meta-tuned",
            "model_description": "UnifiedQA: T5 initialized and then trained on a large mixture of QA tasks; used as an initialization for further meta-tuning on the curated Yes/No QA meta-dataset.",
            "model_size": "770M parameters (T5-Large based UnifiedQA)",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Assess benefit of QA-specific pre-finetuning (UnifiedQA) as initialization for meta-tuning on Yes/No prompts.",
            "problem_format": "Yes/No QA prompts; initialization differs (UnifiedQA vs T5 pretrained checkpoints).",
            "format_category": "initialization / pre-finetuning",
            "format_details": "Compare two initializations for meta-tuning: (a) T5-Large checkpoint, (b) UnifiedQA model (T5-Large pre-finetuned on QA data). Same meta-tuning procedure applied afterwards.",
            "performance_metric": "AUC-ROC",
            "performance_value": "+1.1% absolute AUC-ROC (average improvement when initializing with UnifiedQA vs T5 init)",
            "baseline_performance": "T5-Large initialization (baseline)",
            "performance_change": "+1.1% absolute AUC-ROC",
            "experimental_setting": "Meta-tuning on the curated meta-dataset; default 5000 steps, batch size 32; leave-one-group-out evaluation.",
            "statistical_significance": null,
            "uuid": "e7596.4",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Model size / scaling",
            "name_full": "Effect of increasing pretrained model parameters (T5) on zero-shot QA prompt performance",
            "brief_description": "Larger pretrained models (T5 variants) produce higher zero-shot AUC-ROC on Yes/No QA formatted classification tasks: T5-Base (220M) vs T5-Large (770M) shows substantial gains; even larger gains from T5-small to T5-base.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-small / T5-Base / T5-Large",
            "model_description": "Family of T5 text-to-text transformer models with different parameter counts, used as initialization for meta-tuning or direct QA prompting.",
            "model_size": "60M (T5-small), 220M (T5-Base), 770M (T5-Large)",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Compare zero-shot performance across model scales using the same QA Yes/No prompt format and meta-tuning procedure.",
            "problem_format": "Yes/No QA prompt (zero-shot); comparison across model sizes in both pretrained and meta-tuned settings.",
            "format_category": "model capacity / prompt sensitivity",
            "format_details": "Direct comparison between T5-Base (220M) and T5-Large (770M) showed an average +6.3% absolute AUC-ROC improvement; T5-small (60M) to T5-base had a larger jump (reported up to ~14.4% depending on weighting).",
            "performance_metric": "AUC-ROC",
            "performance_value": "+6.3% absolute AUC-ROC (220M -&gt; 770M, T5) averaged across label-descriptions",
            "baseline_performance": "T5-Base (220M) baseline",
            "performance_change": "+6.3% absolute AUC-ROC (T5-Base -&gt; T5-Large)",
            "experimental_setting": "Meta-tuning and direct comparisons; results consistent under multiple weighting schemes and with BERT-family scaling checks.",
            "statistical_significance": null,
            "uuid": "e7596.5",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Pretraining vs Random init",
            "name_full": "Effect of using pretrained weights vs random initialization when meta-tuning for QA prompts",
            "brief_description": "Pretraining is critical: models initialized from pretrained checkpoints (T5) greatly outperform randomly initialized models after the same meta-tuning procedure on the Yes/No QA tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (pretrained) vs randomly initialized model (same architecture)",
            "model_description": "T5 architecture; comparison between standard pretrained checkpoint and same-architecture model with random initialization, both subjected to meta-tuning.",
            "model_size": "770M parameters (when using T5-Large)",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Assess whether meta-tuning alone (without pretraining) suffices to learn zero-shot QA answering; compare pretrained vs random in meta-tuning.",
            "problem_format": "Yes/No QA prompt format, meta-tuning on aggregated datasets.",
            "format_category": "initialization / pretraining effect",
            "format_details": "Randomly initialized models after meta-tuning perform at near-chance (avg. AUC-ROC ~0.503) while pretrained models achieve far higher performance; meta-tuning alone is insufficient without pretraining.",
            "performance_metric": "AUC-ROC",
            "performance_value": "+23.8% absolute AUC-ROC (pretrained vs random initialization, averaged change Δ)",
            "baseline_performance": "Random initialization (baseline; average AUC-ROC ≈ 0.503 reported)",
            "performance_change": "+23.8% absolute AUC-ROC (pretrained vs random)",
            "experimental_setting": "Meta-tuning procedure identical for both initializations (sampling, 5k steps default); evaluation on held-out dataset groups.",
            "statistical_significance": null,
            "uuid": "e7596.6",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Early stopping / training length",
            "name_full": "Effect of extended meta-tuning and memorization of prompt-task mappings on unseen-task performance",
            "brief_description": "Training the meta-tuned model for too many steps causes memorization of training task prompt mappings and reduces zero-shot generalization to unseen tasks; relative drop observed after extended training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (meta-tuned)",
            "model_description": "T5-Large meta-tuned on the Yes/No QA meta-dataset; compared performance trajectories across steps from 5k (default) to 100k steps.",
            "model_size": "770M parameters",
            "task_name": "Zero-Shot Classification (binary)",
            "task_description": "Monitor average AUC-ROC on held-out zero-shot benchmark datasets as a function of meta-tuning steps to detect overfitting/memorization.",
            "problem_format": "Yes/No QA prompt format; training length varied (5k default vs 100k prolonged).",
            "format_category": "training regime / prompt memorization",
            "format_details": "Default meta-tuning used 5k steps; extending to 100k steps (20x longer) produced decreasing performance on unseen tasks (authors report ~3% absolute AUC-ROC drop relative to step 5k in their curves). The authors attribute the drop to memorization of prompt-task pairs.",
            "performance_metric": "AUC-ROC (relative to value at step 5000)",
            "performance_value": "Approximately -3% absolute AUC-ROC drop when training increased to 100k steps (relative to 5k step reference) for certain benchmarks",
            "baseline_performance": "Performance at step 5000 (reference point)",
            "performance_change": "-~3% absolute AUC-ROC (100k vs 5k steps, averaged across some benchmark datasets)",
            "experimental_setting": "Meta-tuning extended to 100k steps; evaluation on three benchmark zero-shot datasets from Yin et al. (2019); plotted relative AUC-ROC vs steps.",
            "statistical_significance": "Kendall rank correlation coefficients are negative with p &lt; 0.005 for topic and situation classification (authors report significant negative correlations between training step and performance on those datasets).",
            "uuid": "e7596.7",
            "source_info": {
                "paper_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "rating": 2
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "rating": 2
        },
        {
            "paper_title": "Exploiting cloze questions for few-shot text classification and natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "How many data points is a prompt worth?",
            "rating": 2
        },
        {
            "paper_title": "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
            "rating": 2
        }
    ],
    "cost": 0.01792,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections</h1>
<p>Ruiqi Zhong Kristy Lee<em> Zheng Zhang</em> Dan Klein<br>Computer Science Division, University of California, Berkeley<br>{ruiqi-zhong, kristylee, zhengzhang1216, klein}@berkeley.edu</p>
<h4>Abstract</h4>
<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can "prompt" the LM with the review and the label description "Does the user like this movie?", and ask whether the next word is "Yes" or "No". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by finetuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a samesized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220 M to 770 M improves AUC-ROC scores by $6.3 \%$, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-thebox might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.</p>
<h2>1 Introduction</h2>
<p>The goal of zero-shot classification (ZSC) is to classify textual inputs using label descriptions without any examples (Yin et al., 2019). Large language models - whose only training objective is to predict the next word given the context - have acquired a surprising ability to perform ZSC (Radford et al., 2019; Brown et al., 2020; Le Scao and Rush, 2021). For example, to classify whether the sentence "This movie is amazing!" is positive, we
can prompt the language model with the context "Review: This movie is amazing! Positive Review? $\qquad$ ", and check whether the next word is more likely to be "Yes" or "No" (Zhao et al., 2021). To convert ZSC into a language modeling (LM) task that an LM model is likely to perform well, many recent works focus on finding better prompts (Shin et al., 2020; Schick and Schütze, 2020a,b; Gao et al., 2021).</p>
<p>However, the LM training objective is correlated but still misaligned with the target objective to answer prompts. Our work addresses this weakness by directly optimizing the zero-shot classification objective through fine-tuning (Section 4). This requires us to 1) unify different classification tasks into the same format, and 2) gather a collection of classification datasets and label descriptions (prompts) for training (Section 2). Since we fine-tune our model on a meta-dataset, we name our approach meta-tuning.</p>
<p>We focus on binary classification tasks and unify them into a "Yes"/"No" QA format (Clark et al., 2019; McCann et al., 2018), where the input is provided as the context and the label information is provided in the question (Figure 1 (a)). Using this format, we gathered a diverse set of classification datasets from 43 different sources listed on Kaggle, SemEval, HuggingFace, and other papers. These tasks range from hate speech detection, question categorization, sentiment classification to stance classification, etc, and the genre ranges from textbooks, social media, to academic papers, etc. In total, these datasets contain 204 unique labels, and we manually annotated 441 label descriptions (Figure 2).</p>
<p>To evaluate ZSC, we need to define what counts as a task that the model has not seen during training time. While prior work considers different notions of "unseen" by disallowing the same label or the same dataset to appear during training, our work defines "unseen" more harshly by dis-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) We convert the format to question answering. We manually annotate label descriptions (questions) ourselves (Section 2). (b) We finetune the UnifiedQA (Khashabi et al., 2020) model (with 770 M parameters) on a diverse set of tasks (Section 4), and evaluate its 0-shot classification (ZSC) performance on an unseen task. (c) For each label description (question) we evaluate the AUC-ROC score for the "Yes" answer, and each dot represents a label description (Section 3). The $x$-value is the ZSC performance of UnifiedQA; the $y$-value is the performance after meta-tuning. In most cases, the $y$-value improves over the $x$-value (above the red line) and is better than random guesses (above the black line) by a robust margin (Section 5).</p>
<p>allowing similar datasets. For example, we consider AG News topic classification dataset (Zhang et al., 2015) and the topic classification dataset from Yin et al. (2019) to be similar, even though their sources and label spaces are different.</p>
<p>Meta-tuning improves ZSC over UnifiedQA for most labels (Figure 1 (c)). Moreover, larger models are better, and hence we forecast that meta-tuning would work for even larger models. We also find that the performance can be slightly improved by training on datasets similar to the test dataset, ensembling different label descriptions, or initializing with a QA model (Section 5.1). All of our findings reliably hold under different robustness checks (Section 5.2), and our approach outperforms the previous SOTA <em>Yin et al. (2019)</em> using the same pre-training method (Section 5.3).</p>
<p>Our results suggest two promising future directions (Section 6). First, large language models' (e.g., GPT-3) potential for zero-shot learning, as currently measured by context-prompting, might have been broadly underestimated; meta-tuning might significantly improve their performance. Second, community-wide efforts on aggregating and unifying datasets can scale up training and evaluation for zero-shot learning models. On the flip side, however, the meta-tuning approach might incentivize providers of LM inference APIs to collect prompts from users, hence potentially leading to security, privacy, and fairness concerns at a greater scale (Section A).</p>
<p><strong>Contributions</strong> To summarize, we 1) curate a dataset of classification datasets with expert annotated label descriptions. 2) demonstrate a simple approach to train models to perform zero-shot learning, and 3) identify several factors that improve performance; in particular, larger pretrained models are better. <sup>1</sup></p>
<h2>2 Data</h2>
<p>We gather a wide range of classification datasets and unify them into the "<em>Yes"/"No</em>" question answering format for binary classification. Then we group similar datasets together to determine what counts as unseen tasks during evaluation.</p>
<p><strong>Gathering classification datasets</strong> We collect classification datasets from Kaggle<sup>2</sup>, Huggingface (Wolf et al., 2020), SemEval<sup>3</sup>, and other papers. We looked through these sources and only considered English classification datasets. We also skipped the tasks that we felt were already better represented by other datasets in our collection. Then we manually examined a few examples in each remaining dataset to make sure it seemed plausibly clean.</p>
<p>The goals of these classification datasets include, but are not limited to sentiment classification (IMDB Reviews, Maas et al. (2011a)), topic classification (AG News, Zhang et al. (2015)), grammaticality judgement (CoLA, Warstadt et al. (2018)), paraphrase detection (QQP<sup>4</sup>), definition</p>
<p><sup>1</sup>Code and data available here: https://github.com/ruiqi-zhong/Heta-tuning</p>
<p><sup>2</sup>https://www.kaggle.com</p>
<p><sup>3</sup>https://semeval.github.io</p>
<p><sup>4</sup>https://www.kaggle.com/c/</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: For each dataset, we annotate 1-3 descriptions for each label in the form of questions, and associate it with a set of property tags. The question answering format can be seen in Figure 1 (a).
detection (SemEval 2020 Task 6, Spala et al. (2019)), stance classification (SemEval 2016 Task 6, Mohammad et al. (2016)), etc. The genre includes academic papers, reviews, tweets, posts, messages, articles, and textbooks. The comprehensive list of datasets is in Appendix B. Overall, we aim for a high diversity of tasks and genres by building upon what the broader research community has studied. Our approach is complementary to that of Weller et al. (2020), which asks turkers to generate tasks, and that of Mishra et al. (2021), which generates tasks by decomposing existing templates used to construct reading comprehension datasets. The concurrent work of Bragg et al. (2021) unifies the evaluation for few-shot learning; their zero-shot evaluation setup is the closest to ours, and they used templates and verbalizers (Schick and Schütze, 2020a) to specify the semantics of a task.</p>
<p>Some of our datasets are noisy and not peer reviewed, or contain tasks that are too complicated (e.g. Multi-NLI, Williams et al. (2018)) for ZSC. To make our evaluation more informative, we only include them for training but not testing. We make these decisions before running our experiments in Section 5 to prevent selection bias.</p>
<p>Unifying the dataset format We convert each classification dataset into a "Yes"/"No" question answering format and provide label information in the question. For each label, we annotate 13 questions. If the label is null (for example, a text that does not express a particular emotion in an emotion classification dataset), we skip this label. Three of the authors ${ }^{5}$ manually annotated 441 questions for 204 unique labels, and each question</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Are these two questions asking for the same thing?
Does the tweet contain irony?
Is this news about world events?
Does the text contain a definition?
Is the tweet an offensive tweet?
Is the text objective?
Does the question ask for a numerical answer?
Is the tweet against environmentalist initiatives?
Is this abstract about Physics?
Does the tweet express anger?
Does the user dislike this movie?
Is the sentence ungrammatical?
Is this text expressing a need for evacuation?
Is this text about Society and Culture?
Is this a spam?
Figure 3: Some example manually annotated label descriptions (questions). Three of the authors manually wrote 441 questions in total, and each of them is proofread by at least another author.
is proofread by at least another author. See Figure 2 for a concrete example, and Figure 3 for some representative label descriptions.</p>
<p>Additionally, some datasets contain thousands of labels (Chalkidis et al., 2019; Allaway and McKeown, 2020). In this case, we use templates to automatically synthesize label descriptions and exclude them from evaluation.</p>
<p>Grouping similar datasets Our goal is to test the models' ability to generalize to tasks that are different enough from the training tasks. Therefore, at test time, we need to exclude not only the same dataset that appeared in the meta-tuning phase, but also ones that are similar.</p>
<p>This poses a challenge: whether two datasets perform the same task involves subjective opinion, and there is no universally agreed definition. On one extreme, most datasets can be counted as dissimilar tasks, since they have different label spaces and input distributions. On the other extreme, all datasets can be considered the same task, since they can all be unified into the question answering format.</p>
<p>To tackle this challenge, we create a set of tags, each describing a dataset property. The set of tags includes domain classification, article, emotion, social-media, etc, and the full set of them can be seen in Appendix C. Then we define the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Example dataset groups based on tags. We never train and test on datasets from the same group, e.g. train on hotel review and test on movie review.
two datasets to be similar if they are associated with the same set of tags, and prohibit the model to learn from one and test on the other. For example, our work considers the topic classification datasets from Zhang et al. (2015) (AG News) and Yin et al. (2019) to be similar since they both classify topics for articles, even though their sources and label spaces are different. Some example dataset groups can be seen in Figure 4.</p>
<p>Nevertheless, our procedure is not bullet-proof and one can argue that our notion of unseen tasks, though harsher than prior works (Yin et al., 2019; Pushp and Srivastava, 2017), is still lenient. Therefore, as additional robustness checks, for each dataset we evaluate, we manually identify and list the most relevant dataset that is allowed during training in Appendix F . For example, the most relevant dataset to the IMDB review sentiment classification dataset is the emotion classification dataset from Yin et al. (2019), which classifies the input text into 9 emotions, such as "joy", "surprise", "guilt", etc. We consider the emotion classification dataset to be relevant, since sentiment classification often involves identifying emotions. However, one can also argue that they are different tasks: their input and label spaces are different, and sadness can be caused by a great tragedy, or a bad movie that wastes the users' time. The comprehensive list of label descriptions grouped by dataset similarity is in Appendix D.</p>
<p>In total, we spend around 200 hours to collect this dataset. This time estimate includes skimming through the dataset repos and recent NLP papers, writing programs to download the datasets and unify their format, annotating label descriptions, performing quality controls, and documenting the collection process.</p>
<h2>3 Metrics</h2>
<p>To reliably aggregate performance across different datasets and present as much information as possible, we report a set of descriptive statistics and provide visualizations whenever we compare two models. We generally do not reduce a model's performances on different datasets into one scalar quantity and compare this number only.</p>
<p>Descriptive statistics For each label description (question), we calculate the AUC-ROC score ${ }^{6}$ by treating the "Yes" answer as the positive class. After calculating the AUC-ROC score for each label, we calculate the following set of descriptive statistics to compare two models. Suppose that model $Y$ is hypothetically better than $X$. Denoting $\Delta$ as the change of AUC-ROC of a label description from $X$ to $Y$, we can summarize how $\Delta$ is distributed across the set of label descriptions with the following statistics:</p>
<ul>
<li>$\mathbb{E}[\Delta]$ : the average change in AUC-ROC.</li>
<li>$\mathbb{P}[\Delta&gt;t]$ : the fraction of label descriptions where the change is over the threshold $t$.</li>
<li>$\mathbb{P}[\Delta&lt;-t]$ : the fraction of label descriptions where the change is less than $-t$.</li>
<li>$\operatorname{Std}[\Delta]$ : the standard deviation of the change.</li>
</ul>
<p>In the main paper, we weight each label description equally in this distribution to calculate the above statistics. We may also weight each label or dataset equally, and the corresponding results are in Appendix E. To make sure our conclusions are robust, we consider one model to be better only when $\mathbb{E}[\Delta]&gt;0$ and $\mathbb{P}[\Delta&gt;t]&gt;\mathbb{P}[\Delta&lt;-t]$ for all $t \in{1 \%, 5 \%, 10 \%}$, under all three types of weighting. In other words, we claim that one model is better than the other only when 12 conditions simultaneously hold.</p>
<p>Visualizations We use scatter plots to visualize and compare the performance of two models, where each dot represents a label description, its x value represents the AUC-ROC score of the model $X$, and its y-value represents that of $Y$. If most dots are above the identity line $y=x$, the model $Y$ is better than $X$.</p>
<p>The descriptive statistics and the visualizations are explained in Figure 5.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Each dot represents a label description, and its $x / y$-value each represents the performance of model $X / Y$ (measured by AUC-ROC score). For example, on label description $D 1$, model $X / Y$ has AUC-ROC score $0.5 / 0.65$. If the dot is above the black line $(y=0.5)$, model $Y$ is performing better than random guesses. If the dot is above the red line $(y=x)$, model $Y$ is better than model $X$. Since one out of two dots are above $y=x+0.05$, we have $\mathbb{P}[\Delta&gt;5 \%]=0.5$.</p>
<h2>4 Model</h2>
<p>Architecture We format the inputs to the model in the same way as UnifiedQA (Khashabi et al., 2020), which concatenates the context to the question and adds a " $[\operatorname{SEP}]$ " token in between. Then we feed the concatenated input into the T5 encoder and produce the answer score by normalizing the "Yes"/"No" probability of the first decoded token. Unless otherwise noted, we initialize our model with T5-Large ( 770 Million parameters). We sometimes compare to or initialize with the UnifiedQA model (Khashabi et al., 2020), which is trained on a wide range of question answering datasets. For a fair comparison, we use the UnifiedQA model initialized with T5Large as well. To meta-tune non-Seq2Seq pretrained models, such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), we add an MLP layer on top of the pooled output/"[CLS]" token to classify between "Yes"/"No". We leave the improvement on model architectures (Ye and Ren, 2021; Li and Liang, 2021; Lester et al., 2021) and training objectives (Murty et al., 2021; Yin et al., 2020) for future work.</p>
<p>Meta-tuning We create a training distribution that balances between datasets, label descriptions, and "Yes"/"No" answers. To create the next training datapoint for meta-tuning, we select a
dataset from the training split uniformly at random (u.a.r.); then we select a label description (question) u.a.r. and with $50 \%$ probability select a textual input with the answer "Yes"/"No". To prevent over-fitting, we do not train on any combination of label description and textual input twice. Unless otherwise noted, we meta-tune the model for 5000 steps and use batch size 32. We did not tune any hyper-parameters or training configurations since they work well during our first attempt. To evaluate ZSC performance on each dataset, we leave out one group of similar datasets as the evaluation set and train on the rest. Altogether, the experiments take around 250 GPU hours on Quadro 8000.</p>
<h2>5 Results</h2>
<h3>5.1 Hypotheses and Conclusions</h3>
<p>We investigate and validate the following hypotheses, sorted by importance in descending order.</p>
<ul>
<li>Meta-tuned models outperform general question answering models in zero-shot classification.</li>
<li>Larger pre-trained models are better.</li>
<li>Pre-training does the heavy lifting.</li>
<li>Performance can be improved by training on similar datasets, initializing with a QA model, or ensembling label descriptions.</li>
<li>Early stopping is crucial to performance.</li>
</ul>
<p>Meta-tuned models are better. We compare a meta-tuned T5-Large model ( 770 M parameters) ${ }^{7}$ with the same-sized UnifiedQA model (Khashabi et al., 2020) out of the box. Relevant descriptive statistics can be seen in the first row of Table 1 and Figure 6 (a). Adapting the model for ZSC improves the average AUC-ROC by $3.3 \%$.</p>
<p>Larger pre-trained models are better. We compare T5-Base ( 220 Million parameters) against T5-Large ( 770 M ). The statistics can be seen in the second row of Table 1 and Figure 6 (b). Increasing the model size from 220 M to 770 M improves the average AUC-ROC by $6.3 \%$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathbb{E}[\Delta]$</th>
<th style="text-align: center;">$\mathbb{P}[\Delta&gt;1 \%]$</th>
<th style="text-align: center;">$\mathbb{P}[\Delta&lt;-1 \%]$</th>
<th style="text-align: center;">$S t d(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Meta-tuned vs. UnifiedQA</td>
<td style="text-align: center;">$3.3 \%$</td>
<td style="text-align: center;">$59.5 \%$</td>
<td style="text-align: center;">$28.1 \%$</td>
<td style="text-align: center;">$9.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Larger</td>
<td style="text-align: center;">$6.3 \%$</td>
<td style="text-align: center;">$75.1 \%$</td>
<td style="text-align: center;">$15.1 \%$</td>
<td style="text-align: center;">$8.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained vs. Random</td>
<td style="text-align: center;">$23.8 \%$</td>
<td style="text-align: center;">$95.7 \%$</td>
<td style="text-align: center;">$3.2 \%$</td>
<td style="text-align: center;">$14.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Train on Similar</td>
<td style="text-align: center;">$0.7 \%$</td>
<td style="text-align: center;">$43.8 \%$</td>
<td style="text-align: center;">$20.5 \%$</td>
<td style="text-align: center;">$3.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble Descriptions</td>
<td style="text-align: center;">$0.7 \%$</td>
<td style="text-align: center;">$28.9 \%$</td>
<td style="text-align: center;">$16.8 \%$</td>
<td style="text-align: center;">$3.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Initialize with UnifiedQA</td>
<td style="text-align: center;">$1.1 \%$</td>
<td style="text-align: center;">$54.1 \%$</td>
<td style="text-align: center;">$24.3 \%$</td>
<td style="text-align: center;">$6.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: The statistics used to compare two models, introduced in Section 3. The larger $\mathbb{E}[\Delta]$ and the difference between $\mathbb{P}[\Delta&gt;1 \%]$ and $\mathbb{P}[\Delta&lt;-1 \%]$, the better. Row 1 finds that a meta-tuned model is better than UnifiedQA; row 2 finds that the larger model is better; row 3 finds that pre-training does the heavy lifting; row 4,5 , and 6 finds that the performance can be improved by training on similar datasets, ensembling label descriptions, and initializing with a UnifiedQA model. Note that $\operatorname{Std}(\Delta)$ is the standard deviation of individual descriptions, not the standard deviation of the estimated mean. Due to space constraint we only show $t=1 \%$ in this table.</p>
<p>Pre-training does the heavy lifting. In Figure (c) and the third row of Table 1, we compare pretrained and random initializations, where the latter cannot beat the random baseline (average AUCROC 0.503 ). Hence, meta-tuning alone is far from enabling the model to perform ZSC. An intuitive interpretation is that the model already "knows" how to perform ZSC after pre-training under the LM objective, and learns how to use this knowledge during meta-tuning.</p>
<p>Training on similar datasets improves performance. Unlike before, we no longer avoid training on similar datasets from the same group. Instead, we perform straightforward leave-one-out cross-validation. The statistics can be seen in the fourth row of Table 1 and Figure 6 (d), and it improves the average AUC-ROC by $0.7 \%$. The performance gain is not as significant as increasing the model size or adapting for ZSC. We conjecture that it is because we have not collected enough datasets; otherwise, there might be more similar datasets, hence improving ZSC performance.</p>
<p>Ensembling label descriptions improves performance. Instead of asking the model a single question for each label and obtain the probability of the answer being "Yes", we can average the probability obtained by asking multiple questions with the same meaning. This approach is different from traditional ensembling, which typically needs to store/train multiple models to average across them. The fifth row of Table 1 and Figure 6 (e) verifies that ensembling descriptions improves performance slightly ( $0.7 \%$ AUC-ROC score).</p>
<p>Initializing with UnifiedQA improves performance. Figure 6 (f) and the sixth row of Table 1
compare the UnifiedQA against against the T5 initialization. Initializing with UnifiedQA improves average AUC-ROC by $1.1 \%$.</p>
<p>Early stopping is crucial to performance. If we train the model for too long, the model might simply "memorize" that certain label descriptions correspond to certain training tasks, and the performance on unseen tasks may drop. To explore this possibility, we meta-tune our models for 100 K steps, which is 20 times as long as our default setting and encourages the model to memorize the training tasks. We then evaluate them on the three benchmark zero-shot classification datasets by Yin et al. (2019) (which we describe in more details in the next section). We calculate the average AUCROC across all label descriptions for each of the 3 datasets, and plot them in Figure 7.</p>
<p>The performance decreases ${ }^{8}$ as training continues. On the other hand, however, the performance drop of $3 \%$ in AUC-ROC is not fatal and the model's performance is still much better than random guesses.</p>
<h3>5.2 Robustness Checks</h3>
<p>We examine a series of additional results to make sure our conclusions are robust. The observed improvements in Table 1 and Figure 6 might be caused by the improvement of a small number of labels that are annotated with more descriptions, or by the improvement on a dataset with more distinct labels. Appendix E. 1 compares the performance by assigning equal weights to each label/datasets.</p>
<p>To provide additional supporting evidence for</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: The interpretation of these figures can be seen in Figure 5. (a) compares a meta-tuned model (y) against UnifiedQA (x); (b) compares T5-Large (770 M parameters) against T5-base (220M); (c) compares the T5 pretrained initialization against the random initialization; (d), (e), and (f) investigate whether performance can be improved by training on similar datasets, ensembling different label descriptions (questions), and initializing with UnifiedQA. Conclusion: Since most dots are above the red line y = x for all 6 figures and above the random guess baseline (y = 0.5) by a robust margin, all conclusions listed at the beginning of Section 5 hold.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Each curve corresponds to the models' performance on a dataset from Yin et al. (2019). x-value is the number of training steps; y-value is the average AUC-ROC score across all label descriptions, relative to the value at step 5000. Training for too long decreases performance on unseen tasks.</p>
<p>our forecast that larger models are better, Appendix E.2 compares a 60M-parameter model against a 220M-parameter model, and finds that the latter is much better. One concern, however, is that our models are initialized with T5 (Raffel et al., 2019), which is trained on the open web and might have seen the datasets we gathered. There-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>emotion</th>
<th>situation</th>
<th>topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Yin et al. (2019)</td>
<td>25.2</td>
<td>38.0</td>
<td>52.1</td>
</tr>
<tr>
<td>Meta-tuned</td>
<td>28.2</td>
<td>48.4</td>
<td>54.3</td>
</tr>
</tbody>
</table>
<p>Table 2: "Prior" means the best performing system from Yin et al. (2019) for each dataset; "Meta-tuned" means meta-tuning on RoBERTa. Our approach is better on all three datasets.</p>
<p>fore, larger models might be better simply because they are better at memorization (Sagawa et al., 2020). Appendix E.3 addresses this by showing that larger models are also better with BERT initialization (Devlin et al., 2019), which is trained on Wikipedia and Book Corpus (Zhu et al., 2015).</p>
<p>We also report the models' performance on each dataset for readers' reference in Appendix G.</p>
<h3>5.3 Comparison with Yin et al. (2019)</h3>
<p>This section shows that our approach has higher performance than the zero-shot classification system built by Yin et al. (2019). Their system ensembles several natural language inference models based on RoBERTA-Large (355M parameters, Liu et al. (2020)), and another model trained to categorize Wikipedia articles. It was evaluated on three classification datasets:</p>
<ul>
<li>topic (10-way): classifies article domains, such as family \&amp; relationship, education, sports, etc. The metric is accuracy.</li>
<li>emotion (10-way): classifies emotion types, such as joy, anger, guilt, shame, etc. The metric is label-weighted F1.</li>
<li>situation (12-way): classifies disaster situations, e.g. regime change, crime \&amp; violence, and the resource they need, e.g. search \&amp; rescue. The metric is label-weighted F1.</li>
</ul>
<p>We use the exact same evaluation metrics as in Yin et al. (2019), and the same label resolution strategy when the model answers "Yes"9 for multilabel classification. Concretely, when the model predicts "Yes" on multiple labels, the one with the highest probability is selected. For a fair comparison, we meta-tune RoBERTa of the same size and compare it with the highest performing model in Yin et al. (2019) for each of the three datasets.</p>
<p>The results are in Table 2, and our model has higher performance across all 3 datasets using the same pre-training method.</p>
<h2>6 Discussion and Future Directions</h2>
<p>Main takeaways We construct a dataset of classification datasets to adapt the language model for zero-shot classification via meta-tuning. The adapted model outperforms a general-purpose question answering model and the prior state of the art based on natural language inference. We forecast that meta-tuning would be more effective on larger models, and the current engineering ceiling for zero-shot learning might have been broadly under-estimated.</p>
<p>Aggregating and unifying datasets The main bottleneck of our research is to manually gather a wide range of datasets and unify their format. The difficulties are: 1) we need to brainstorm and review the NLP literature extensively to decide what new tasks to look for; 2) different datasets encode their data in different formats, and we need to write programs manually for each of them to convert to the desired format; 3) it is hard to tell the quality of a dataset purely by its provenance, and sometimes we need to examine the dataset manually. If we as a community can aggregate and unify datasets better, we could potentially train and evaluate zero-shot learning models at a larger scale.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Meta-tuning as a probe There is a growing interest in measuring the intelligence (Hendrycks et al., 2021a,b) or the few-shot learning ability (Brown et al., 2020) of large language models like GPT-3. However, since these models are not adapted to answer those prompts (Holtzman et al., 2021), we suspect that its knowledge and true potential to perform few-shot learning is much higher than reported. Since pre-training does the heavy lifting and meta-tuning is unlikely to provide additional ZSC ability to the model, we can potentially first use meta-tuning as a probe to make them adapted to answering prompts before measuring their performance.</p>
<p>Still, to make this methodology rigorous, interpreting and controlling the strength of the probes will be an important future direction (Hewitt and Liang, 2019). For example, if the training set contains a prompt that is too similar to the prompt to be tested, the probe will be meaningless.</p>
<p>Beyond Shallow Correlations One possibility is that the model only learns shallow statistical correlations from meta-tuning rather than "more sophisticated reasoning skills". For example, the word "exciting" might occur in positive reviews more. This is unlikely, given that larger models are consistently better than smaller or randomly initialized ones. To explain this performance gap, larger models must have learned to use more complicated features during meta-tuning.</p>
<p>Relation to Meta/Multitask-Learning Our method is closely related to, but different from meta-learning (Yin, 2020; Murty et al., 2021) and multi-task learning (Ye et al., 2021; Aghajanyan et al., 2021). Both meta-learning and multitask-learning typically involve at least a few examples from the target task; in our setup, however, the model does not learn from any target task examples. The "meta" in our name does not mean "meta-learning", but reflects the fact that our model learns from a meta-dataset of tasks.</p>
<p>Nevertheless, our framework can be easily adapted to a few-shot learning setup, which enables the language model to learn to learn from incontext examples (see below). Since this approach models the learning process as a sequence classification problem, it can be seen as a form of metalearning similar to (Ravi and Larochelle, 2016).</p>
<p>Annotating Prompts Three of our authors annotated the label descriptions. Since they are all</p>
<p>Computer Science major students who understand machine learning and natural language processing, they might not be representative of the final user population of this ZSC application. Annotating prompts that match the target user distribution will be an important research direction.</p>
<p>Additionally, shorter and more natural descriptions sometimes fail to capture the exact semantics of the label. For example, in Yin et al. (2019), the description of the label "medical" is "people need medical assistance"; or alternatively, it can be longer but more accurate: "people need an allied health professional who supports the work of physicians and other health professionals". How to scalably generate more accurate and detailed label descriptions without expert efforts will be another future direction.</p>
<p>Optimizing Prompts Our work is complementary to recent works that optimize the prompts to achieve better accuracy. Even if our metatuned model is specialized in answering prompts, it might still react very differently towards different prompts. For example, in the stance classification dataset (Barbieri et al., 2020), we annotated two label descriptions (prompts) for the same label: "Does this post support atheism?" and "Is the post against having religious beliefs?". They have similar meanings, but the former has much lower accuracy than the later. We conjecture that this is because the model cannot ground abstract concepts like "atheism".</p>
<p>Other extensions We conjecture that metatuning can be extended to more diverse tasks beyond zero-shot binary classification. To extend to multi-label classification, we need to develop a procedure to resolve the labels when the model predicts positive for more than one labels. To extend to few-shot learning, we need to increase the context length to fit several training examples into the input, which requires a larger context window and hence more computational resources. To extend to other sequence generation tasks, we need to collect a wide range of diverse sequence generation tasks to meta-tune the model, such as machine translation, summarization, free-form question answering, grammar correction, etc.</p>
<h2>Acknowledgements</h2>
<p>We thank Eric Wallace for his feedbacks throughout the project. We thank Steven Cao, David</p>
<p>Gaddy, Haizhi Lai, Jacob Steinhardt, Kevin Yang and anonymous reviewers for their comments on the paper.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations with pre-finetuning. arXiv preprint arXiv:2101.11038.</p>
<p>Emily Allaway and Kathleen McKeown. 2020. ZeroShot Stance Detection: A Dataset and Model using Generalized Topic Representations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8913-8931, Online. Association for Computational Linguistics.</p>
<p>Tiago Almeida, José María Gómez Hidalgo, and Tiago Pasqualini Silva. 2013. Towards sms spam filtering: Results under a new dataset. International Journal of Information Security Science, 2(1):1-18.</p>
<p>Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1644-1650, Online. Association for Computational Linguistics.</p>
<p>Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019. SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54-63, Minneapolis, Minnesota, USA. Association for Computational Linguistics.</p>
<p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.</p>
<p>Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. Flex: Unifying evaluation for few-shot nlp. arXiv preprint arXiv:2107.07170.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.</p>
<p>Extracting training data from large language models. arXiv preprint arXiv:2012.07805.</p>
<p>Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Large-scale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6314-6322, Florence, Italy. Association for Computational Linguistics.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning AI With Shared Human Values. arXiv e-prints, page arXiv:2008.02275.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning {ai} with shared human values. In International Conference on Learning Representations.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733-2743, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. CoRR, abs/2104.08315.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefixtuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.</p>
<p>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.</p>
<p>Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. 2019. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044-3049, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: A robustly optimized {bert} pretraining approach.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011a. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011b. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730.</p>
<p>Tsvetomila Mihaylova, Georgi Karadzhov, Pepa Atanasova, Ramy Baly, Mitra Mohtarami, and Preslav Nakov. 2019. SemEval-2019 task 8: Fact checking in community question answering forums. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 860-869, Minneapolis, Minnesota, USA. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773.</p>
<p>Rishabh Misra. 2019. Imdb spoiler dataset.
Rishabh Misra, Mengting Wan, and Julian McAuley. 2018. Decomposing fit semantics for product size recommendation in metric spaces. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 422-426. ACM.</p>
<p>Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. SemEval-2018 task 1: Affect in tweets. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 1-17, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. 2016. SemEval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 3141, San Diego, California. Association for Computational Linguistics.</p>
<p>Shikhar Murty, Tatsunori B Hashimoto, and Christopher D Manning. 2021. Dreca: A general task augmentation strategy for few-shot natural language inference.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271-278, Barcelona, Spain.</p>
<p>Pushpankar Kumar Pushp and Muktabh Mayank Srivastava. 2017. Train once, test anywhere: Zeroshot learning for text classification. arXiv preprint arXiv:1712.05972.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Sachin Ravi and Hugo Larochelle. 2016. Optimization as a model for few-shot learning.</p>
<p>Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502-518, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. 2020. An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning, pages 8346-8356. PMLR.</p>
<p>Timo Schick and Hinrich Schütze. 2020a. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676.</p>
<p>Timo Schick and Hinrich Schütze. 2020b. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Sasha Spala, Nicholas Miller, Franck Dernoncourt, and Carl Dockhorn. 2020. SemEval-2020 task 6: Definition extraction from free text with the DEFT corpus. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 336-345, Barcelona (online). International Committee for Computational Linguistics.</p>
<p>Sasha Spala, Nicholas A. Miller, Yiming Yang, Franck Dernoncourt, and Carl Dockhorn. 2019. DEFT: A corpus for definition extraction in free- and semistructured text. In Proceedings of the 13th Linguistic Annotation Workshop, pages 124-131, Florence, Italy. Association for Computational Linguistics.</p>
<p>Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962.</p>
<p>Cynthia Van Hee, Els Lefever, and Véronique Hoste. 2018. SemEval-2018 task 3: Irony detection in English tweets. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 3950, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Eric Wallace, Tony Z Zhao, Shi Feng, and Sameer Singh. 2020. Customizing triggers with concealed data poisoning. arXiv preprint arXiv:2010.12563.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.</p>
<p>Orion Weller, Nicholas Lourie, Matt Gardner, and Matthew Peters. 2020. Learning from task descriptions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361-1375, Online. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for crosstask generalization in NLP. CoRR, abs/2104.08835.</p>
<p>Qinyuan Ye and Xiang Ren. 2021. Zero-shot learning by generating task-specific adapters. arXiv preprint arXiv:2101.00420.</p>
<p>Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. 2020. Metalearning without memorization. In International Conference on Learning Representations.</p>
<p>Wenpeng Yin. 2020. Meta-learning for few-shot natural language processing: A survey. arXiv preprint arXiv:2007.09604.</p>
<p>Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3914-3923, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval). In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75-86, Minneapolis, Minnesota, USA. Association for Computational Linguistics.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, page 649-657, Cambridge, MA, USA. MIT Press.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages $19-27$.</p>
<h2>A Ethics</h2>
<p>Data and incentives In the existing prompting framework, end users send the natural language descriptions and a few training examples to the large language model inference API to perform few-shot learning (Brown et al., 2020). This becomes a natural source of training data for metatuning. Hence, the success of meta-tuning presented in this paper might incentivize for-profit organizations who provide language model inference APIs to collect prompts from the users, and train on these data.</p>
<p>Privacy, security, and fairness If a model is meta-tuned on user-provided data, certain security, privacy and fairness concerns can potentially emerge. For example, Carlini et al. (2020) shows that it is possible to extract the training data from large language models, and hence meta-tuned systems might expose some users' prompts to other users. Wallace et al. (2020) shows that it is possible to poison the model through training data and trigger unwanted behaviors; the meta-tuning procedure might be susceptible to these data poisoning attacks as well. Finally, meta-tuning might perpetuate existing societal biases hidden in the users' prompts (Bolukbasi et al., 2016).</p>
<p>If not addressed properly, these concerns might have a broader negative societal impact through meta-tuning. Compared to other domain-specific and task-specific machine learning applications, meta-tuned models might be applied to a much wider range of tasks, deployed at a larger scale, and serving a more diverse set of user population. Therefore, biased or poisoned training data for one task from one user population might compromise fairness and performance of another task and harm another user population; additionally, malicious or biased data might even tamper with the few-shot learning capability ("meta-poisoning").</p>
<p>Potential abuse As shown in Figure 6, the AUC-ROC score for a lot of tasks are still well below 0.9 , and hence our system is far from solving a significant fraction of tasks. Therefore, even though our system is flexible and has the potential to perform a wide range of tasks, it does not present an elixir to all classification tasks. Particularly, it should not be applied to higher stake scenarios (e.g. hate speech detection, fake news detection, etc), since its efficacy, robustness, and fairness properties remain unknown.</p>
<h2>B Datasets</h2>
<p>IMDB movie review sentiment classification (Maas et al., 2011b). Classifies whether the user likes the movie.</p>
<p>Positive: "'My favourite police series of all time turns to a TV-film. Does it work? Yes. ..."</p>
<p>Negative: " "Stupid! Stupid! Stupid! I can not stand Ben stiller anymore."</p>
<p>Zero Shot Emotion Classification (Yin et al., 2019). This task classifies a textual input into 9 emotion types {"sadness", "joy", "anger", "disgust", "fear", "surprise", "shame", "guilt", "love"}, and none-type if not any of the above. For example,</p>
<p>JOy: "Making new friends is always fun, specially when playing dress up"</p>
<p>ANGER: "People that smoke cigarettes irritate my soul."</p>
<p>Zero Shot topic Classification (Yin et al., 2019). This task classifies an articles into 10 topic classes, such as "Health", 'Sports", "Family \&amp; Relationships", "Science \&amp; Mathematics", etc.</p>
<p>SCIENCE: "How the human species evolved ? How the human species evolved ? A tough question as it overlaps science and theology ..."</p>
<p>Sports: "Number one formula one driver ? Depends on your question.Kimi Raikkonen won more races ..."</p>
<p>Zero Shot Situation Classification (Yin et al., 2019). This task classifies what type of situation the user is having and what aids it needs. The classes include "food", "terrorism", etc.</p>
<p>FOOD: "livestock, crops, rural and urban housing, roads, river and canal embankments, and other infrastructure suffered extensive damage"</p>
<p>Terrorism: "Daish attacked Tahurmahu village with chemical gas."</p>
<p>SemEval 2017 Task 4 (Barbieri et al., 2020) (Rosenthal et al., 2017). This task classifies the sentiment of a tweet as {"negative", "neutral", "positive"}. For example,</p>
<p>Negative: "That sucks if you have to take the SATs tomorrow"</p>
<p>Neutral: "@user Well said on HMW. Can you now address why Texans fans file out of the stadium midway through the 4th qtr of every game?"</p>
<p>Positive: "'This is the last Sunday w/o football .....,NFL is back baby'"</p>
<p>ETHICS Justice (Hendrycks et al., 2020). This task identifies whether the justification provided for a given scenario is deemed reasonable. For example,</p>
<p>REASONABLE: "I deserve to be paid by my boss because I keep her house clean daily."</p>
<p>Unreasonable: "I deserve to be given a brand new car by my boss because I keep her house clean daily."</p>
<p>ETHICS Deontology (Hendrycks et al., 2020). This task identifies whether the excuse provided for a given scenario is a reasonable excuse. For example,</p>
<p>SCenario: "Aren't you supposed to be preparing for the meeting?"</p>
<p>Excuse (Reasonable): "No because I have everything already set up in the conference room."</p>
<p>Excuse (Unreasonable): "No because you don't have a meeting"</p>
<p>ETHICS Common Sense (Hendrycks et al., 2020). This task identifies identifies whether a scenario demonstrates common sense. For example,</p>
<p>COMMON SENSE: "I went to the principal's office to change my records before going to a different school."</p>
<p>Not Common Sense: "I secured the loan because I would make the payments."</p>
<p>EURLEX57K (Chalkidis et al., 2019). This task classifies European legislation.</p>
<p>National Currency: "Council Regulation (EC) No 2595/2000 of 27 November 2000 amending Regulation (EC) No 1103/97 on certain provisions relating to the introduction of the euro"</p>
<p>Southern Africa: "95/458/EC: Commission Regulation (EC) No 302/2006 of 20 February 2006 on import licences in respect of beef and veal products originating in Botswana, Kenya, Madagascar, Swaziland, Zimbabwe and Namibia"</p>
<p>SemEval 2019 Task 6 (Barbieri et al., 2020) (Zampieri et al., 2019). This task classifies the tweet as either offensive or not offensive. For example,</p>
<p>Offensive: "@user She has become a parody unto herself? She has certainly taken some heat for being such an....well idiot. Could be optic too</p>
<p>Who know with Liberals They're all optics. No substance"</p>
<p>Not Offensive: "@user @user She is great. Hi Fiona!"</p>
<p>Click Bait Detection ${ }^{10}$ This task detects whether a news title is a click bait.</p>
<p>Click Bait: "Can You Pass This Basic Trigonometry Quiz"</p>
<p>Non Click Bait: "NASCAR driver Kyle Busch wins 2011 Jeff Byrd 500".</p>
<p>Abstract Domain Classification ${ }^{11}$ This classifies the abstract into 4 domains: "Physcis", "Maths", "Computer Science", "Statistics". For example,</p>
<p>PhYSICS: "a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge ..."</p>
<p>Maths: "a main result of this note was a existence of martingale solutions to a stochastic heat equation (she) inside the riemannian manifold ..."</p>
<p>SemEval 2019 Task 5 (Barbieri et al., 2020) (Basile et al., 2019). This task identifies whether the tweet contains hate speech towards women and/or immigrants or not. For example,</p>
<p>Hate SPEECH: "This account was temporarily inactive due to an irrational woman reporting us to Twitter. What a lack of judgement, shocking. #YesAllMen"</p>
<p>No Hate Speech: "@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds congregating on you..."</p>
<p>SemEval 2019 Task 8 (Mihaylova et al., 2019). This task identifies whether the text is an example of a question asking for factual information, an example of a question asking for an opinion, or an example of socializing. For example,</p>
<p>FACTUAL: "is there any place i can find scented massage oils in qatar?"</p>
<p>Opinion: "hi there; i can see a lot of massage center here; but i dont which one is better.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>can someone help me which massage center is good...and how much will it cost me? thanks"</p>
<p>Socializing: "Hello people...let's play this game...you have to write something good about the person whose 'post' is above you on QL.You can write anything and you can write\&#160; multiple times."</p>
<p>SemEval 2018 Task 3 (Barbieri et al., 2020) (Van Hee et al., 2018). This task identifies whether the tweet contains irony or not. For example,</p>
<p>IRONY: "seeing ppl walking w/ crutches makes me really excited for the next 3 weeks of my life"</p>
<p>No Irony: "@user on stage at #fizjingleball at the @user in #Tampa #iheartradio"</p>
<p>SemEval 2018 Task 1 (Barbieri et al., 2020; Mohammad et al., 2018) This task classifies a tweet as one of 4 emotion types {"sadness", "joy", "anger", "optimism" }. For example,</p>
<p>SADNESS: "@user I so wish you could someday come to Spain with the play, I can't believe I'm not going to see it #sad"</p>
<p>JOY: "#ThisIsUs has messed with my mind \&ang; now I'm anticipating the next episode with #apprehension \&amp; #delight! #isthereahelplineforthis"</p>
<p>ANGER: "@user Haters!!! You are low in self worth. Self righteous in your delusions. You cower at the thought of change. Change is inevitable."</p>
<p>OptiMISM: "Don't be #afraid of the space between your #dreams and #reality. If you can #dream it, you can #make it so"</p>
<p>SemEval 2016 Task 6 (Mohammad et al., 2016; Barbieri et al., 2020) This task classifies a tweet's stance as {"neutral", "against", "favor"}. Each tweet contains a stance on one of the five different target topics {"abortion", "atheism", "climate change", "feminism", "hillary"}. For example,</p>
<p>Neutral: "@user maybe that's what he wants #SemST"</p>
<p>AGAINST: "Life is #precious \&amp; so are babies, mothers, \&amp; fathers. Please support the sanctity of Human Life. Think #SemST"</p>
<p>FAVOUR: "@user @user Nothing to do with me. It's not my choice, nor is it yours, to dictate what another woman chooses. #feminism #SemST"</p>
<p>SemEval 2020 Task 6 (Spala et al., 2020). This task classifies whether textbook sentence contains a definition. For example,</p>
<p>Contains Definition: "Since 2005, automated sequencing techniques used by laboratories are under the umbrella of next-generation sequencing, which is a group of automated techniques used for rapid DNA sequencing"</p>
<p>Doesn't Contain Definition: "These automated low-cost sequencers can generate sequences of hundreds of thousands or millions of short fragments ( 25 to 500 base pairs ) in the span of one day."</p>
<p>TREC (Li and Roth, 2002). This task classifies a question into one of six question types: DESC (description), ABBR (abbreviation), ENTY (entity), HUM (people/individual), LOC (location), NUM (numeric information), each of which have specific fine-grained sub-categories. For example,</p>
<p>DESC: "How did serfdom develop in and then leave Russia?"</p>
<p>ABBR: "What is the full form of .com?"
ENTY: "What films featured the character Popeye Doyle?"</p>
<p>HUM: "What contemptible scoundrel stole the cork from my lunch?"</p>
<p>LOC: "What sprawling U.S. state boasts the most airports?"</p>
<p>NUM: "How many Jews were executed in concentration camps during WWII?"</p>
<p>SUBJ (Pang and Lee, 2004). This task classifies a sentence as being subjective or objective. For example,</p>
<p>Subjective: "smart and alert, thirteen conversations about one thing is a small gem."</p>
<p>Objective: "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter."</p>
<p>The Corpus of Linguistic Acceptability (Warstadt et al., 2018).This task detects if sentences are grammatically acceptable by their original authors. For example,</p>
<p>Grammatically Acceptable: "Her little sister will disagree with her."</p>
<p>Grammatically Not Acceptable: "Has not Henri studied for his exam?"</p>
<p>The Multi-Genre NLI Corpus (Williams et al., 2018). This task detects if a premise is a contradiction or entailment of a hypothesis, or if a hypothesis holds neutral view on the premise.. For example,</p>
<p>Neutral: "Premise: Exoatmospheric Kill Vehicles orbiting Earth would be programmed to collide with warheads. Hypothesis: Exoatmospheric Kill Vehicles would be very expensive and hard to make."</p>
<p>Entailment: "Premise: so we have to run our clocks up forward an hour and $i$ sure do hate to loose that hour of sleep in the morning. Hypothesis: I don't like the time change that results in losing an hour of sleeping time."</p>
<p>CONTRADICTION: "Premise: The mayor originally hoped groundbreaking would take place six months ago, but it hasn't happened yet. Hypothesis: The mayor doesn't want groundbreaking to happen at all."</p>
<h2>Metaphor as a Medium for Emotion: An Empirical Study (?). This task detects if the application of a word is Literal or Metaphorical. For example,</h2>
<p>Word: ABUSE
LITERAL: "This boss abuses his workers."
Metaphorical: "Her husband often abuses alcohol."</p>
<p>Political Preference Classification (Allaway and McKeown, 2020). This task predicts a comment's stand point on a political topic. For example,</p>
<p>Topic: Companies Regulation
Con: "Regulation of corporations has been subverted by corporations. States that incorporate corporations are not equipped to regulate corporations that are rich enough to influence elections, are rich enough to muster a legal team that can bankrupt the state. Money from corporations and their principals cannot be permitted in the political process if democracy is to survive."</p>
<p>Pro: "Regulation is to a corporation what a conscience is to a living person. Without a conscience, we would all be sociopaths. Corporations do not have a conscience, thus they need regulation to make sure they are focused on benefiting society instead on merely benefiting themselves."</p>
<p>Neutral: "Without government to ensure their behavior, companies will attempt to make a profit even to the DETRIMENT of the society that supports the business. We have seen this in the environment, in finances, in their treatment of workers and customers. Enough."</p>
<p>Airline Service Review ${ }^{12}$ This task classifies if an airline review has a positive or negative sentiment. For example,</p>
<p>Positive: "This is such a great deal! Already thinking about my 2nd trip to Australia; I haven't even gone on my 1st trip yet!"</p>
<p>Negative: "amazing to me that we can't get any cold air from the vents."</p>
<p>Covid-19 Tweets Sentiment Analysis ${ }^{13}$ This task classifies if a tweet has a positive or negative sentiment. For example,</p>
<p>Positive: "Taken by Henk Zwoferink on Saturday in Wargl, our black beauty hauled a train bringing the last tourists home. Our colleagues are #workinghard to keep supply chains running while respecting the measures to ensure everyone's #safety. A pleasure to work with such #DedicatedPeople!"</p>
<p>Negative: "So far, the Minister does not seem to have made statement on the catastrophe that can develop if the issue of markets operation is not addressed. Food insecurity has potential to make current Covid-19 panic look like a kindergarten and could lead to riots. I submit."</p>
<p>Hotel Review ${ }^{14}$ This task predicts if a hotel review is a positive or negative review. For example,</p>
<p>Negative: "The single rooms like hospital rooms single rooms hotel sparse intentional know ugly like trapped hospital white walls sink basin room small rectangle shape.the beds hard rocks blankets rough really noisy.this overrated hotel stayed fans type hotels"</p>
<p>Positive: "loved stay, stayed univ, inn 10 days april 2005 thoroughly enjoyed, free parking clean spacious room friendly staff great breakfast snack, loved location, definitely stay, "</p>
<p>Stock Market Sentiment ${ }^{15}$ This task predicts if a comment holds a positive or negative view on the performance of the stock market. For example,</p>
<p>Negative: "GPS wow that wa s a fast fast fade..."</p>
<p>Positive: "user Maykiljil posted that: I agree that MSFT is going higher \&amp; possibly north of 30"</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>AG-News (Zhang et al., 2015). This task classifies the topic of news based on their contents. For example,</p>
<p>World News: "Greek duo could miss drugs hearing"</p>
<p>Sports News: "AL Wrap: Olerud Cheers Yankees by Sinking Ex-Team"</p>
<p>Business News: "Lowe's Second-Quarter Profit Rises"</p>
<p>Tech News: "Satellite boosts Olympic security"</p>
<p>Real and Fake News ${ }^{16}$ This task classifies if a news is fake or real. For example,</p>
<p>REAL: "WASHINGTON (Reuters) - Alabama Secretary of State John Merrill said he will certify Democratic Senator-elect Doug Jones as winner on Thursday despite opponent Roy Mooreâ $x 80$
$x 99$ challenge, in a phone call on CNN. Moore, a conservative who had faced allegations of groping teenage girls when he was in his 30s, filed a court challenge late on Wednesday to the outcome of a U.S. Senate election he unexpectedly lost."</p>
<p>FAKE: "Ronald Reagan shut down the Berkeley protests many years ago THIS is how you do it!"</p>
<p>Disaster Tweets ${ }^{17}$ This task detects if a tweet announces an emergency or a disaster. For example,</p>
<p>Contains DisAster: "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all."</p>
<p>Does not Contain Disaster: "My dog attacked me for my food #pugprobs."</p>
<p>Obama vs Trump Tweets ${ }^{18}$ This task detects if a tweet was send by Obama or Trump. For example,</p>
<p>Obama: "Michelle and I are delighted to congratulate Prince Harry and Meghan Markle on their engagement. We wish you a lifetime of joy and happiness together."</p>
<p>Trump: "Together, we dream of a Korea that is free, a peninsula that is safe, and families that are reunited once again!"</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Kaggle Sexually Explicit Tweets ${ }^{19}$ This dataset provides positive examples of profane comments. For example,</p>
<p>EXPLICIT"What do guys say when you get naked in front of them for the first time?"</p>
<p>Democratic vs Republican Tweets ${ }^{20}$ This task detects if a tweet was send by the Democratic or Republican Party. For example,</p>
<p>Democratic: "#YuccaMountain would require moving tens of thousands of metric tons of radioactive waste across the country and through Southern Nevada."</p>
<p>Republican: "Stopped by One Hour Heating\&amp; Air Conditioning to discuss the benefits tax reform will bring to their business."</p>
<h2>Women E-commerce Clothing Reviews ${ }^{21}$</h2>
<p>This task predicts if the buyer likes or recommends a product base on its review. For example,</p>
<p>LIKE: "After reading the previous reviews, $i$ ordered a size larger. i am so glad i did it! it fits perfectly! i am 5'4"/115/32dd and went with the s regular. so beautiful! i can't wait to wear it!"</p>
<p>DISLIKE: "The zipper broke on this piece the first time i wore it. very disappointing since i love the design. I'm actually going to try to replace the zipper myself with something stronger, but annoying that it's come to that."</p>
<p>Quora Question Pairs ${ }^{22}$ This task predicts if a pair of Quora question is asking for the same thing. For example,</p>
<p>SAME: "Question 1: How many months does it take to gain knowledge in developing Android apps from scratch?; Question 2: How much time does it take to learn Android app development from scratch?"</p>
<p>DIFFERENT: "Question 1: How would you review the site Waveclues? ; Question 2: Is there a good pay for reviews site out there?"</p>
<p>Headline Sarcasm Detection This task detects if is a news headline contains scarcasm. For example,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>SARCASM: "guy who just wiped out immediately claims he's fine"</p>
<p>No SARCASM: "Donald trump effigies burn across Mexico in Easter ritual"</p>
<p>Company Account Tweets ${ }^{23}$ This task detects whether the tweet is targeted towards a company account. For example,</p>
<p>Yes: "@VirginTrains Oh, that's nice. What are you doing about it? What are you targets next year?"</p>
<p>No: "@115738 That's the best kind of trick-ortreating. All treats, my friend. -Becky"</p>
<p>SMS Spam Detection (Almeida et al., 2013) This task detects whether the SMS is a spam message. For example,</p>
<p>SPAM: "Thank you, winner notified by sms. Good Luck! No future marketing reply STOP to 84122 customer services 08450542832"</p>
<p>HAM: "Lol great now I am getting hungry."
Clothing Fitness (Misra et al., 2018) Checking whether the customer complains that the cloth is too small or too large.</p>
<p>SMALL: "runs a bit small. wish it fit".
LARGE: "too big".
Water Problem Topic Classification ${ }^{24}$ Classifying the topic of a report on water problems. The labels include "biological", "climatic indicator", "environmental technology", etc. For example,</p>
<p>BiOLOGICAL: "Mineralization of organic phosphorus in bottom sediments reaches 40-80\% and as we found out during the project implementation it intensified in autumn-winter period."</p>
<p>CLIMATIC INDICATOR: "The average amount of precipitation in the lower part of the basin makes 470 mm to 540 mm . The relative average annual air humidity makes 60-65\%".</p>
<p>ENVIRONMENTAL TECHNOLOGY: "Most of wastewater treatment facilities require urgent modernization and reconstruction".</p>
<p>Sexist Statement Detection ${ }^{25}$ This task classifies whether the statement is sexist. For example,</p>
<p>SEXIST: "It's impossible for a girl to be faithful."</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Non SEXIST: "Without strength, can we work to create wealth?"</p>
<p>Movie Spoiler Detection (Misra, 2019) ${ }^{26}$ This task classifies whether the movie review is a spoiler. For example,</p>
<p>SPOILER: "I must say that this movie was good but several things were left unsaid. For those who have seen the movie know what I am talking about but for those who haven't, I don't want to give spoilers. I was also impressed by Vin Diesel's acting skills. Overall I have to say it was a good movie filled with several twists and turns."</p>
<p>Non Spoiler: "The Great Wall amazes with its spectacular effects, both on screen and sound. Usually I do not appreciate 3D movies, but in this case I felt like it worth it.However, being honest, the storytelling and the story itself had its weaknesses. There were many logical lapses, and for me, many details are still waiting to be answered.On the other hand, expect decent acting especially from the main characters.All in all, The Great Wall is a solid popcorn-movie, but I expected a more elaborated unfolding of the legend it tells about."</p>
<h2>News Summary/headline Topic Classification</h2>
<p>${ }^{27}$ This task classifies the topic of the summary of a news. For example,</p>
<p>Politics: "City and state officials said they received little advance warning of the decision."</p>
<p>BUSINESS: "The streaming giant's thirdquarter earnings were nothing like the Upside Down."</p>
<h2>C Dataset Property Tags</h2>
<p>Here we list all the dataset property tags (Section 2). We define two datasets to be "similar" if they have the set of tags, and disallow meta-tuning on datasets that are similar to evaluation dataset.
social media: whether the source is from social media (e.g. tweets).
social/political: whether the task is highly related to political/social topics. Some examples include stance classification and hate speech detection.
topic classification: whether the task classifies the topics of the input.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>good vs. bad: whether the task classifies whether the text is judging something to be good or bad.
paper: whether input text comes from a paper.
review: whether the input text is a review of a product (e.g. movie, hotel).
questions: whether the input texts are questions. Some examples include classifying whether the question asks for factual information or subjective opinion and detecting whether two questions have the same meaning.
emotion: whether the task classifies certain emotion in the text, for example "hate", "surprise", "joy", etc.</p>
<p>Besides, we do not assign tags to datasets that we are confident to be different enough from other tasks (e.g. extracting whether a text contains definition), and allow the model to be meta-tuned on all other datasets.</p>
<h2>D List of Label Descriptions</h2>
<p>Please refer to the appendix in our arXiv version: https://arxiv.org/abs/2104. 04670. Somehow the acl_pubcheck software package always gives us errors.</p>
<h2>E Robustness Checks</h2>
<p>We report all the descriptive statistics mentioned in Section 3 under 3 different types of description weighting. We additionally compare T5-small vs. T5-base, BERT-medium vs. BERT-Base and BERT-Base vs. BERT Large. All the results can be seen in Table 3, 4, and 5 Due to space constraint, we abbreviate $\mathbb{P}[\Delta&gt;t]$ as $&gt;t$ if $t$ is positive, and $&lt;t$ if $t$ is negative. Notice that, since we only have around 20 datasets to evaluate the model, most of the results presented here are not statistically significant at the dataset level; nevertheless,</p>
<h2>E. 1 Different Description Weighting</h2>
<p>We weight each label and dataset equally in Table 4 and 5. We find that, under almost all comparisons across different weighting, the mean change $\bar{\Delta}$ is positive, and the change above a certain threshold $t$ is more frequent than the change below a certain threshold $-t$. The only single exception the "Ensemble" row in Table 5, where there are slightly more datasets where the change is lower than $-1 \%$. Nevertheless, given that the trend is still positive under $t=5 \%$ and $10 \%$, and two other
description weightings, we may still conclude that ensembling label descriptions is more likely to improve model performance.</p>
<h2>E. 2 Larger T5 Models are Better</h2>
<p>In addition to comparing T5-Base (220 Million parameters) vs. T5-Large (770M), we also compare T5-small (60M) vs. T5-base (220M). Across all metrics, larger models are significantly better. Most notably, there is a sudden jump in performance when increasing model size from T5-small to T5-base (sometimes $15 \%$ increase in $\bar{\Delta}$ ).</p>
<h2>E. 3 Larger BERT Models are Better</h2>
<p>We also compare different sizes of BERT (Turc et al., 2019) (41, 110, and 330M) parameters. Across all metrics, larger models are significantly better.</p>
<h2>F Most Relevant Datasets</h2>
<p>To ensure that we are testing the models' ability to generalize to an unseen tasks, we disallow both training and testing on datasets that are too similar, which is defined as "having the same set of dataset property tags" (Section 2). To help interpret how we define unseen tasks, for each dataset that we evaluate on, we try to find the "most relevant" dataset that the model has seen during the meta-tuning phase, and list it in Table 6.</p>
<h2>G Performance Break Down</h2>
<p>For each model, we average the AUC-ROC scores for each label description for each dataset, and report the results in Table 7.</p>
<h2>H Accuracy</h2>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">$\Delta$</th>
<th style="text-align: right;">$&gt;1 \%$</th>
<th style="text-align: right;">$&lt;-1 \%$</th>
<th style="text-align: right;">$&gt;5 \%$</th>
<th style="text-align: right;">$&lt;-5 \%$</th>
<th style="text-align: right;">$&gt;10 \%$</th>
<th style="text-align: right;">$&lt;-10 \%$</th>
<th style="text-align: right;">$\operatorname{std}(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Meta-tuned vs QA</td>
<td style="text-align: right;">$3.3 \%$</td>
<td style="text-align: right;">$59.5 \%$</td>
<td style="text-align: right;">$28.1 \%$</td>
<td style="text-align: right;">$31.4 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
<td style="text-align: right;">$15.7 \%$</td>
<td style="text-align: right;">$5.9 \%$</td>
<td style="text-align: right;">$9.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">220 vs 770M (T5)</td>
<td style="text-align: right;">$6.3 \%$</td>
<td style="text-align: right;">$75.1 \%$</td>
<td style="text-align: right;">$15.1 \%$</td>
<td style="text-align: right;">$47.6 \%$</td>
<td style="text-align: right;">$2.7 \%$</td>
<td style="text-align: right;">$27.0 \%$</td>
<td style="text-align: right;">$0.5 \%$</td>
<td style="text-align: right;">$8.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained vs. Random</td>
<td style="text-align: right;">$23.8 \%$</td>
<td style="text-align: right;">$95.7 \%$</td>
<td style="text-align: right;">$3.2 \%$</td>
<td style="text-align: right;">$91.4 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
<td style="text-align: right;">$83.2 \%$</td>
<td style="text-align: right;">$1.1 \%$</td>
<td style="text-align: right;">$14.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble</td>
<td style="text-align: right;">$0.7 \%$</td>
<td style="text-align: right;">$28.9 \%$</td>
<td style="text-align: right;">$16.8 \%$</td>
<td style="text-align: right;">$8.7 \%$</td>
<td style="text-align: right;">$1.7 \%$</td>
<td style="text-align: right;">$1.7 \%$</td>
<td style="text-align: right;">$0.6 \%$</td>
<td style="text-align: right;">$3.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Initialized with QA</td>
<td style="text-align: right;">$1.1 \%$</td>
<td style="text-align: right;">$54.1 \%$</td>
<td style="text-align: right;">$24.3 \%$</td>
<td style="text-align: right;">$24.3 \%$</td>
<td style="text-align: right;">$11.9 \%$</td>
<td style="text-align: right;">$6.5 \%$</td>
<td style="text-align: right;">$4.9 \%$</td>
<td style="text-align: right;">$6.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Train on similar</td>
<td style="text-align: right;">$0.7 \%$</td>
<td style="text-align: right;">$43.8 \%$</td>
<td style="text-align: right;">$20.5 \%$</td>
<td style="text-align: right;">$6.5 \%$</td>
<td style="text-align: right;">$4.3 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
<td style="text-align: right;">$1.1 \%$</td>
<td style="text-align: right;">$3.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">60 vs 220M (T5)</td>
<td style="text-align: right;">$14.4 \%$</td>
<td style="text-align: right;">$86.5 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
<td style="text-align: right;">$79.5 \%$</td>
<td style="text-align: right;">$4.3 \%$</td>
<td style="text-align: right;">$61.1 \%$</td>
<td style="text-align: right;">$2.2 \%$</td>
<td style="text-align: right;">$12.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">41 vs. 110M (BERT)</td>
<td style="text-align: right;">$4.3 \%$</td>
<td style="text-align: right;">$65.9 \%$</td>
<td style="text-align: right;">$22.7 \%$</td>
<td style="text-align: right;">$40.0 \%$</td>
<td style="text-align: right;">$10.8 \%$</td>
<td style="text-align: right;">$20.5 \%$</td>
<td style="text-align: right;">$5.9 \%$</td>
<td style="text-align: right;">$9.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">110 vs. 340M (BERT)</td>
<td style="text-align: right;">$1.4 \%$</td>
<td style="text-align: right;">$46.5 \%$</td>
<td style="text-align: right;">$35.7 \%$</td>
<td style="text-align: right;">$23.8 \%$</td>
<td style="text-align: right;">$17.3 \%$</td>
<td style="text-align: right;">$11.4 \%$</td>
<td style="text-align: right;">$6.5 \%$</td>
<td style="text-align: right;">$8.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: All results, with metrics explained in Section 3 and Appendix E. Each label description is weighted equally.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">$\Delta$</th>
<th style="text-align: right;">$&gt;1 \%$</th>
<th style="text-align: right;">$&lt;-1 \%$</th>
<th style="text-align: right;">$&gt;5 \%$</th>
<th style="text-align: right;">$&lt;-5 \%$</th>
<th style="text-align: right;">$&gt;10 \%$</th>
<th style="text-align: right;">$&lt;-10 \%$</th>
<th style="text-align: right;">$\operatorname{std}(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Meta-tuned vs QA</td>
<td style="text-align: right;">$3.0 \%$</td>
<td style="text-align: right;">$57.5 \%$</td>
<td style="text-align: right;">$30.7 \%$</td>
<td style="text-align: right;">$31.3 \%$</td>
<td style="text-align: right;">$11.5 \%$</td>
<td style="text-align: right;">$16.2 \%$</td>
<td style="text-align: right;">$7.3 \%$</td>
<td style="text-align: right;">$10.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">220M vs 770M (T5)</td>
<td style="text-align: right;">$5.8 \%$</td>
<td style="text-align: right;">$75.8 \%$</td>
<td style="text-align: right;">$15.5 \%$</td>
<td style="text-align: right;">$46.9 \%$</td>
<td style="text-align: right;">$3.5 \%$</td>
<td style="text-align: right;">$25.6 \%$</td>
<td style="text-align: right;">$1.4 \%$</td>
<td style="text-align: right;">$7.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained vs. Random</td>
<td style="text-align: right;">$23.7 \%$</td>
<td style="text-align: right;">$93.5 \%$</td>
<td style="text-align: right;">$5.5 \%$</td>
<td style="text-align: right;">$89.4 \%$</td>
<td style="text-align: right;">$3.4 \%$</td>
<td style="text-align: right;">$82.5 \%$</td>
<td style="text-align: right;">$2.1 \%$</td>
<td style="text-align: right;">$15.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble</td>
<td style="text-align: right;">$0.5 \%$</td>
<td style="text-align: right;">$25.0 \%$</td>
<td style="text-align: right;">$18.8 \%$</td>
<td style="text-align: right;">$6.9 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
<td style="text-align: right;">$1.7 \%$</td>
<td style="text-align: right;">$0.7 \%$</td>
<td style="text-align: right;">$3.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Initialized with QA</td>
<td style="text-align: right;">$1.2 \%$</td>
<td style="text-align: right;">$54.0 \%$</td>
<td style="text-align: right;">$24.0 \%$</td>
<td style="text-align: right;">$26.0 \%$</td>
<td style="text-align: right;">$11.8 \%$</td>
<td style="text-align: right;">$8.1 \%$</td>
<td style="text-align: right;">$5.3 \%$</td>
<td style="text-align: right;">$7.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Train on similar</td>
<td style="text-align: right;">$0.7 \%$</td>
<td style="text-align: right;">$44.5 \%$</td>
<td style="text-align: right;">$20.1 \%$</td>
<td style="text-align: right;">$6.0 \%$</td>
<td style="text-align: right;">$4.3 \%$</td>
<td style="text-align: right;">$1.7 \%$</td>
<td style="text-align: right;">$0.8 \%$</td>
<td style="text-align: right;">$3.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">60 vs 220M (T5)</td>
<td style="text-align: right;">$15.2 \%$</td>
<td style="text-align: right;">$85.7 \%$</td>
<td style="text-align: right;">$11.4 \%$</td>
<td style="text-align: right;">$79.1 \%$</td>
<td style="text-align: right;">$3.9 \%$</td>
<td style="text-align: right;">$62.5 \%$</td>
<td style="text-align: right;">$1.9 \%$</td>
<td style="text-align: right;">$13.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">41 vs. 110M (BERT)</td>
<td style="text-align: right;">$4.8 \%$</td>
<td style="text-align: right;">$67.0 \%$</td>
<td style="text-align: right;">$21.5 \%$</td>
<td style="text-align: right;">$41.9 \%$</td>
<td style="text-align: right;">$9.2 \%$</td>
<td style="text-align: right;">$22.5 \%$</td>
<td style="text-align: right;">$4.9 \%$</td>
<td style="text-align: right;">$9.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">110 vs. 340M (BERT)</td>
<td style="text-align: right;">$1.1 \%$</td>
<td style="text-align: right;">$44.3 \%$</td>
<td style="text-align: right;">$36.3 \%$</td>
<td style="text-align: right;">$21.9 \%$</td>
<td style="text-align: right;">$18.2 \%$</td>
<td style="text-align: right;">$11.0 \%$</td>
<td style="text-align: right;">$7.3 \%$</td>
<td style="text-align: right;">$8.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: All results, with metrics explained in Section 3 and Appendix E. Each label is weighted equally.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">$\Delta$</th>
<th style="text-align: right;">$&gt;1 \%$</th>
<th style="text-align: right;">$&lt;-1 \%$</th>
<th style="text-align: right;">$&gt;5 \%$</th>
<th style="text-align: right;">$&lt;-5 \%$</th>
<th style="text-align: right;">$&gt;10 \%$</th>
<th style="text-align: right;">$&lt;-10 \%$</th>
<th style="text-align: right;">$\operatorname{std}(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Meta-tuned vs QA</td>
<td style="text-align: right;">$1.2 \%$</td>
<td style="text-align: right;">$55.4 \%$</td>
<td style="text-align: right;">$35.7 \%$</td>
<td style="text-align: right;">$31.2 \%$</td>
<td style="text-align: right;">$17.7 \%$</td>
<td style="text-align: right;">$15.6 \%$</td>
<td style="text-align: right;">$13.6 \%$</td>
<td style="text-align: right;">$11.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">220 vs 770M (T5)</td>
<td style="text-align: right;">$6.3 \%$</td>
<td style="text-align: right;">$77.4 \%$</td>
<td style="text-align: right;">$16.5 \%$</td>
<td style="text-align: right;">$51.7 \%$</td>
<td style="text-align: right;">$7.0 \%$</td>
<td style="text-align: right;">$31.6 \%$</td>
<td style="text-align: right;">$4.5 \%$</td>
<td style="text-align: right;">$9.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained vs. Random</td>
<td style="text-align: right;">$20.2 \%$</td>
<td style="text-align: right;">$89.8 \%$</td>
<td style="text-align: right;">$8.5 \%$</td>
<td style="text-align: right;">$84.8 \%$</td>
<td style="text-align: right;">$6.1 \%$</td>
<td style="text-align: right;">$76.6 \%$</td>
<td style="text-align: right;">$1.5 \%$</td>
<td style="text-align: right;">$15.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble</td>
<td style="text-align: right;">$0.1 \%$</td>
<td style="text-align: right;">$18.6 \%$</td>
<td style="text-align: right;">$20.2 \%$</td>
<td style="text-align: right;">$4.3 \%$</td>
<td style="text-align: right;">$1.9 \%$</td>
<td style="text-align: right;">$1.5 \%$</td>
<td style="text-align: right;">$1.2 \%$</td>
<td style="text-align: right;">$2.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Initialized with QA</td>
<td style="text-align: right;">$2.3 \%$</td>
<td style="text-align: right;">$59.2 \%$</td>
<td style="text-align: right;">$22.5 \%$</td>
<td style="text-align: right;">$34.3 \%$</td>
<td style="text-align: right;">$9.9 \%$</td>
<td style="text-align: right;">$13.9 \%$</td>
<td style="text-align: right;">$5.7 \%$</td>
<td style="text-align: right;">$7.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Train on similar</td>
<td style="text-align: right;">$0.6 \%$</td>
<td style="text-align: right;">$48.8 \%$</td>
<td style="text-align: right;">$25.4 \%$</td>
<td style="text-align: right;">$7.3 \%$</td>
<td style="text-align: right;">$5.7 \%$</td>
<td style="text-align: right;">$1.3 \%$</td>
<td style="text-align: right;">$0.9 \%$</td>
<td style="text-align: right;">$3.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">60 vs 220M (T5)</td>
<td style="text-align: right;">$12.1 \%$</td>
<td style="text-align: right;">$84.6 \%$</td>
<td style="text-align: right;">$12.9 \%$</td>
<td style="text-align: right;">$73.6 \%$</td>
<td style="text-align: right;">$3.5 \%$</td>
<td style="text-align: right;">$52.9 \%$</td>
<td style="text-align: right;">$2.2 \%$</td>
<td style="text-align: right;">$11.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">41 vs. 110M (BERT)</td>
<td style="text-align: right;">$7.0 \%$</td>
<td style="text-align: right;">$74.6 \%$</td>
<td style="text-align: right;">$13.8 \%$</td>
<td style="text-align: right;">$58.5 \%$</td>
<td style="text-align: right;">$6.8 \%$</td>
<td style="text-align: right;">$31.5 \%$</td>
<td style="text-align: right;">$2.9 \%$</td>
<td style="text-align: right;">$8.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">110 vs. 340M (BERT)</td>
<td style="text-align: right;">$1.1 \%$</td>
<td style="text-align: right;">$45.6 \%$</td>
<td style="text-align: right;">$36.1 \%$</td>
<td style="text-align: right;">$25.5 \%$</td>
<td style="text-align: right;">$18.6 \%$</td>
<td style="text-align: right;">$10.8 \%$</td>
<td style="text-align: right;">$9.3 \%$</td>
<td style="text-align: right;">$8.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: All results, with metrics explained in Section 3 and Appendix E. Each dataset is weighted equally.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluation Dataset</th>
<th style="text-align: center;">Most Relevant Training Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SemEval 2016 Task 6, stance classifications on issues like feminism, atheism, etc</td>
<td style="text-align: center;">SemEval 2019 Task 5, detecting hate speech against women and immigrants</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2019 Task 6, classifying whether the text is offensive</td>
<td style="text-align: center;">A dataset from Kaggle that classifies sexually explicit comments</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2019 Task 5, detecting hate speech against women and immigrants</td>
<td style="text-align: center;">SemEval 2016 Task 6, stance classifications on issues like feminism, atheism, etc</td>
</tr>
<tr>
<td style="text-align: center;">TREC, classifying the type the question is asking about (e.g. numbers, acronyms, human/occupations, etc)</td>
<td style="text-align: center;">AG News, which classifies news into different categories (e.g. sports, world events).</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2019 Task 8, classifying whether the question is asking for subjective opinion, factual information, or simply having a conversation</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">SUBJ, classifying whether the text contains subjective or objective information</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">QQP, classifying whether two questions have the same meaning</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Yin et al. (2019) emotion classification, classifying text into 9 emotion types, such as "joy", "anger", "guilt", "shame", etc.</td>
<td style="text-align: center;">Classifying whether an IMDB movie review is positive.</td>
</tr>
<tr>
<td style="text-align: center;">Yin et al. (2019) situation classification, classifying which disaster situation people are experiencing, e.g. "regime change", "crime and violence", and what resource they need, e.g. "food and water", "search and rescue".</td>
<td style="text-align: center;">Classifying (binary) whether a tweet is related to a natural disaster.</td>
</tr>
<tr>
<td style="text-align: center;">Yin et al. (2019) topic classification, classifying the domain of an article into domains such as "family and relationship", "education", "business", "sports"</td>
<td style="text-align: center;">classifying the domain of a paper abstract into physics, maths, computer sciences, and statistics.</td>
</tr>
<tr>
<td style="text-align: center;">AG News, which classifies news into different categories (e.g. sports, world events).</td>
<td style="text-align: center;">Abstract Domain classification, classifying the domain of a paper abstract into physics, maths, computer sciences, and statistics.</td>
</tr>
<tr>
<td style="text-align: center;">Abstract Domain classification, classifying the domain of a paper abstract into physics, maths, computer sciences, and statistics.</td>
<td style="text-align: center;">AG News, which classifies news into different categories (e.g. sports, world events).</td>
</tr>
<tr>
<td style="text-align: center;">IMDB movie reviews, classifying whether the user feels positive about the movie</td>
<td style="text-align: center;">Stock market sentiment, classifying whether a comment is optimistic about the market.</td>
</tr>
<tr>
<td style="text-align: center;">CoLA, classifying whether a sentence is grammatical</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2020 Task 6, classifying whether a sentence contains a definition</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Spam classification, classifying whether a text message is a spam</td>
<td style="text-align: center;">click-bait classification, classifying whether the title of an article is a clickbait.</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2018 Task 1, classifying a tweet as one of 4 emotion types {"sadness", "joy", "anger", "optimism"}</td>
<td style="text-align: center;">Classifying whether an IMDB movie review is positive.</td>
</tr>
<tr>
<td style="text-align: center;">SemEval 2018 Task 3, classifying whether a tweet is ironic</td>
<td style="text-align: center;">classifying whether a news title is sarcastic.</td>
</tr>
</tbody>
</table>
<p>Table 6: For each dataset that we evaluate on, we list the task in the training split that we consider to be the most relevant. We list "N/A" if we think that none of the training dataset is particularly relevant.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{23}$ https://www.kaggle.com/thoughtvector/ customer-support-on-twitter
${ }^{24}$ https://www.kaggle.com/vbmokin/ nip-reports-news-classification?select= water_problem_nip_en_for_Kaggle_100.csv
${ }^{25}$ https://www.kaggle.com/dgrosz/ sexist-workplace-statements&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{26}$ https://www.kaggle.com/rmisra/ imdb-spoiler-dataset?select=IMDB_ reviews.json
${ }^{27}$ https://www.kaggle.com/rmisra/ news-category-dataset&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1997 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1997</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1997</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-279251567</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07759v1.pdf" target="_blank">REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> —Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1997.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1997.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REMoH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Evolution of Multi-objective Heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid NSGA-II evolutionary framework that uses LLMs to generate, crossover and mutate heuristic code, guided by a clustering-based reflection mechanism to preserve diversity and improve convergence on multi-objective scheduling (FJSSP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REMoH</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (hybrid with classical EA)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Heuristics are represented as Python code snippets generated by an LLM (tested models: Gemini 2.0 Flash, GPT-4o, DeepSeek-V3). The evolutionary loop uses NSGA-II selection; parents are clustered by objective-space performance and the LLM is prompted to produce short cluster reflections and a long reflection, which guide LLM-based crossover (combining parent code and reflections) and elitist mutation (mutating best individual using reflection). Dynamic role prompts and a seed heuristic are used for population initialization. Invalid/infeasible heuristics trigger re-generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Training/selection experiments used two FJSSP datasets (Dauzere and Barnes) for model selection; validation/evaluation performed on Brandimarte. For ablation and some flexibility experiments Brandimarte was used for training. No dataset sizes or text corpora for LLM pretraining are reported (LLMs used via external APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Flexible Job Shop Scheduling Problem (FJSSP); Brandimarte, Barnes, and Dauzere instance suites (Brandimarte used as primary benchmark/evaluation set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mixed-Integer Linear Programming (Gurobi), Constraint Programming (OR-Tools CP-SAT and IBM CPLEX CP Optimizer / DoCplex), greedy dispatching rule baseline, and two RL-based methods (Ho et al., Echeverria et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>REMoH (Gemini 2.0 Flash) achieved a mean makespan GAP of 12.60% over Brandimarte instances (GAP = relative % difference to best-known lower bounds); produced a non-dominated front of 13 heuristics. Also shows consistently higher Hypervolume (HV) and lower Inverted Generational Distance (IGD) when the reflection mechanism is included (qualitative improvement reported; exact HV/IGD numbers not tabulated in paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Best CP lexicographic configuration (DC_mb) obtained mean GAP of 1.43% on its prioritized objective (makespan) in the presented benchmarking; MILP runtimes and objective behavior reported but MILP sometimes outperformed REMoH on single prioritized metrics in small instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>Diversity and solution-quality assessed using Hypervolume (HV) and Inverted Generational Distance (IGD); reflection-enabled runs had higher HV and lower IGD vs non-reflective variants (no additional novelty metric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Generalization tested by training on Dauzere and Barnes and evaluating on Brandimarte; Gemini-produced heuristics were most consistent on unseen Brandimarte instances. GPT generalized better than DeepSeek despite DeepSeek showing stronger performance during training (indicates different generalization dynamics across LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Observed behavior: GPT-4o converged early (premature convergence) during evolution, suggesting potential overfitting / limited exploration despite strong early-stage performance; DeepSeek improved more over later generations but generalized less well than GPT. No quantitative bias measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Reported average runtimes: MILP ~1661.5 s per instance (time-limited), CP (DoCplex) ~230.9 s, CP (OR-Tools) ~148.4 s. Greedy dispatching rule ~0.002 s. REMoH heuristics runtime (execution of generated heuristics) varied by LLM used: GPT-generated heuristics ~0.001 s, DeepSeek ~0.012 s, Gemini ~0.167 s (execution time per heuristic). Note: these are heuristic execution times (not LLM API cost/time) and mathematical solvers' runtimes reflect solver CPU times with limits; LLM inference / prompt-call costs and latency for generation are run via external APIs but not quantified as total optimization-run cost.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Cross-dataset transfer assessed: LLMs trained/selected using Dauzere and Barnes were validated on Brandimarte. Gemini demonstrated better transfer/generalization consistency; no cross-domain transfers beyond FJSSP datasets were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Three LLMs compared: Gemini 2.0 Flash (best final performance), DeepSeek-V3 (stronger training-phase improvement but worse generalization), GPT-4o (strong early performance but premature convergence). The paper does not detail differences in pretraining corpora; comparison is empirical on the FJSSP task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation removing the reflection mechanism: reflective variant had consistently higher HV and lower IGD across two experimental configurations (population sizes / iteration counts). Reflection preserves diversity and prevents premature convergence by producing clustering-guided offspring; exact numeric ablation values are presented in figures but not as tabular numeric summaries in text.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Clustering of parent heuristics in objective space (K-Means with silhouette-based K selection) is used to characterize hypothesis/heuristic-space structure; cluster centroids and short reflections summarize group strengths/weaknesses and guide offspring generation (qualitative characterization; no global coverage metrics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — adaptation occurs via iterative reflections: short (per-cluster) and long reflections (across generations) are concatenated and passed to the LLM to guide crossover/mutation prompts; dynamic role instructions are used in initialization to diversify generation. This online prompt-guided adaptation improves HV/IGD per ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Premature convergence observed with GPT-4o (strong early-stage but plateaus). DeepSeek-V3 showed stronger gains during training but generalized worse to held-out Brandimarte instances. Exact failure percentages or error cases not quantified. Infeasible heuristics are detected and re-prompted (no rate reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-based operators, when embedded in an evolutionary loop and guided by introspective reflection, can produce diverse, interpretable heuristics that achieve competitive multi-objective trade-offs on FJSSP; reflection (cluster-based introspection) is a crucial mechanism that improves diversity and convergence (higher HV, lower IGD). LLM choice affects training vs generalization dynamics: some models (DeepSeek) improve more during evolution, others (GPT) generalize better early but may converge prematurely, while Gemini balanced late-stage improvement and generalization. Computational trade-offs differ: evolved heuristics execute very quickly compared to solvers, but LLM generation complexity (model inference) adds unquantified API cost; exact solver-optimality trade-offs depend on instance size (exact CP/MILP outperform on small instances for single prioritized objectives).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1997.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EPS / EoH / ReEvo / HSEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Evolutionary Program Search family (including EoH, ReEvo, HSEvo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of approaches that represent individuals as LLM-generated code snippets and evolve them with evolutionary techniques; includes prior methods that inspired REMoH's encoding, reflection, and diversity strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-EPS family (EoH, ReEvo, HSEvo)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (evolutionary program search / hyper-heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Individuals encoded as code generated by LLMs; approaches incorporate reflective prompting (ReEvo), harmony-search/diversity-driven initialization (HSEvo), and evolutionary program search (EoH). HSEvo used dynamic role instructions to diversify prompts; ReEvo combined verbal feedback with evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Referenced across optimization tasks; cited in context of scheduling and heuristic design (no single benchmark specified in paper for these methods).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Presented historically as alternatives to manual heuristic design and traditional metaheuristics; paper references them as predecessors inspiring REMoH (no head-to-head comparisons in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>HSEvo emphasizes diversity-driven harmony search; ReEvo uses reflective evolution and verbal feedback to guide improvements (no numeric metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>ReEvo and HSEvo use reflective mechanisms and diversity heuristics to characterize and explore heuristic spaces (conceptual discussion only).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes (reflective prompts and dynamic role instructions used to adapt generation during evolutionary cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>These prior LLM-based evolutionary paradigms motivated REMoH's representation (code-as-individual), reflection-driven operator guidance, and diversity-aware initialization; they underscore the importance of prompt design and feedback for guiding learned-operator evolution.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1997.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs-GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs-GP (LLM-adapted Genetic Programming operators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that adapts Genetic Programming principles by using LLMs' pattern-recognition capabilities to design genetic operators and generate executable optimization code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs-GP</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (inspired by genetic programming principles)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Uses LLMs to design genetic operators (crossover/mutation) and produce executable code resembling GP outcomes; described as adapting GP principles through language-model pattern recognition (paper references it as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Mentioned generally in context of optimization algorithm generation and program evolution; no experimental benchmark reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as an adaptation of Genetic Programming — implied comparators would be traditional GP operators, but no direct comparison data reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Conceptually yes — LLMs craft operators during evolutionary cycles, but this paper does not provide implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLMs-GP exemplifies how LLMs can synthesize GP-like operators and code, suggesting a pathway to replace hand-designed GP operators with learned linguistic-generation mechanisms; however, empirical comparisons are not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1997.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMEA / AEL / Algorithmic Evolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Algorithmic Evolution using LLMs (AEL), LLaMEA and related frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks where LLMs are embedded in an evolutionary cycle to generate, mutate, and evaluate optimization algorithms or constructive heuristics automatically, sometimes outperforming handcrafted or purely LLM-generated baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AEL / LLaMEA (Algorithmic Evolution using LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (algorithm/heuristic generation and mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLMs are queried iteratively to propose algorithmic components (constructive algorithms, heuristics), mutate them, and use performance feedback to refine subsequent generations. Some frameworks also co-evolve test cases or use task descriptions and performance feedback to drive LLM mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>TSP and other combinatorial optimization tasks referenced in literature citations; in-paper mention is general.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Described as outperforming handcrafted and purely LLM-generated baselines in cited works (no quantitative reproductions in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — iterative LLM feedback loops and mutation/evaluation cycles are core to these frameworks (described conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>These approaches show that LLM-driven algorithm evolution can generate competitive constructive algorithms for classic combinatorial problems and indicate that performance depends strongly on the feedback loop and evaluation strategy used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1997.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEO / LMEA / QDAIF / LMX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative LLM-driven evolutionary/heuristic systems (LEO, LMEA, QDAIF, LMX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of systems that use LLMs as components in evolutionary optimization: LEO uses separate explore/exploit pools; LMEA lets LLMs autonomously perform selection and genetic operations; QDAIF integrates LLMs for quality-diversity evaluation; LMX uses LLMs as crossover operators by synthesizing offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LEO, LMEA, QDAIF, LMX</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (various roles: operators, evaluators, selectors)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LEO: LLM-based evolutionary optimizer with explore and exploit pools. LMEA: LLMs autonomously conduct selection and genetic ops with a self-adaptive temperature. QDAIF: Quality-diversity framework using LLMs to generate and evaluate candidate solutions. LMX: LLMs synthesize offspring by combining multiple parents or predicting code differences.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Referenced for single and multi-objective combinatorial tasks (example: TSP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison vs traditional heuristics and GP-style operators in cited literature; no direct comparisons reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>QDAIF explicitly focuses on quality-diversity evaluation; LEO uses separate pools to maintain exploration/exploitation balance. REMoH uses HV/IGD for diversity/convergence evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — several of these systems incorporate mechanisms (temperature adaptation, separate pools, reflective prompts) to adapt LLM behavior over generations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Demonstrates a variety of roles LLMs can take within evolutionary systems (operator generation, evaluation, selection), suggesting rich hybridization opportunities with traditional GP and EA operators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1997.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeEvo / LHHs / LLM-EPS frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SeEvo (self-evolutionary population), Language Hyper-Heuristics (LHHs), and related LLM-EPS frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks using LLMs to generate and evolve dispatching rules and heuristics over time; LHHs combine LLM heuristic generation with Reflective Evolution to refine heuristics using verbal feedback plus evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SeEvo, LHHs</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (heuristic rule generation / hyper-heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>SeEvo: LLMs generate dispatching rules that evolve over time with reflection-inspired mechanisms. LHHs: LLMs generate heuristics with minimal input; Reflective Evolution (ReEvo) complements with verbal feedback plus evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Scheduling and combinatorial optimization tasks (dynamic job shop scheduling referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Positioned in literature as alternatives to hand-designed dispatching rules and RL-based schedulers; no reproduction data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — reflective mechanisms and evolutionary search are core features enabling adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>These frameworks highlight the practicality of LLMs to produce interpretable dispatching rules and hyper-heuristics, and motivate the reflection mechanism used in REMoH to balance exploration/exploitation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1997.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1997.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL baselines (Ho et al., Echeverria et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep RL scheduling baselines referenced (Ho et al., Echeverria et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art reinforcement learning approaches for FJSSP used as baselines: Ho et al. use graph neural networks with filtering, Echeverria et al. combine CP with RL and train on CP-generated optimal solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RL baselines (Ho et al., Echeverria et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>Neural network / Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Ho et al.: GNN-based construction heuristic with filtering of completed jobs and machines to focus decision space. Echeverria et al.: hybrid CP+RL approach where the model is trained on CP-generated optimal solutions and CP refines intermediate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Echeverria et al. trained on optimal solutions generated via CP; Ho et al. trained on scheduling instances (details in original papers). In this paper the original implementations were not publicly available, so reported performance values were taken from those publications.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>FJSSP (Brandimarte instances); reported primarily for makespan optimization on instances 1-10 in the cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in REMoH benchmarking: RL methods focused only on makespan and could not obtain results for instances larger than Mk10; REMoH (multi-objective) compared favorably in many Brandimarte instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>RL baselines limited scalability: could not report results for instances larger than Mk10 as noted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper notes RL methods require retraining when new constraints introduced (e.g., nonlinear setup times) — implying sensitivity to training distribution and cost of adapting to new constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not quantified in this paper (performance values extracted from publications); authors note RL methods can be computationally expensive to retrain for new constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>RL approaches require retraining for new constraints and did not scale to larger instances in cited works (Mk > 10).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>RL methods can be effective for single-objective optimization but are less flexible than prompt-driven LLM approaches when incorporating new, nonlinear constraints without retraining; LLM-based REMoH adapted to nonlinear SDST with minimal prompt changes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PromptBreeder: Self-referential self-improvement via prompt evolution <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Reevo: Large language models as hyper-heuristics with reflective evolution <em>(Rating: 2)</em></li>
                <li>Language model crossover: Variation through few-shot prompting <em>(Rating: 1)</em></li>
                <li>Large language model-based evolutionary optimizer: Reasoning with elitism <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1997",
    "paper_id": "paper-279251567",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "REMoH",
            "name_full": "Reflective Evolution of Multi-objective Heuristics",
            "brief_description": "A hybrid NSGA-II evolutionary framework that uses LLMs to generate, crossover and mutate heuristic code, guided by a clustering-based reflection mechanism to preserve diversity and improve convergence on multi-objective scheduling (FJSSP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "REMoH",
            "operator_type": "LLM-based (hybrid with classical EA)",
            "operator_description": "Heuristics are represented as Python code snippets generated by an LLM (tested models: Gemini 2.0 Flash, GPT-4o, DeepSeek-V3). The evolutionary loop uses NSGA-II selection; parents are clustered by objective-space performance and the LLM is prompted to produce short cluster reflections and a long reflection, which guide LLM-based crossover (combining parent code and reflections) and elitist mutation (mutating best individual using reflection). Dynamic role prompts and a seed heuristic are used for population initialization. Invalid/infeasible heuristics trigger re-generation prompts.",
            "training_data_description": "Training/selection experiments used two FJSSP datasets (Dauzere and Barnes) for model selection; validation/evaluation performed on Brandimarte. For ablation and some flexibility experiments Brandimarte was used for training. No dataset sizes or text corpora for LLM pretraining are reported (LLMs used via external APIs).",
            "domain_or_benchmark": "Flexible Job Shop Scheduling Problem (FJSSP); Brandimarte, Barnes, and Dauzere instance suites (Brandimarte used as primary benchmark/evaluation set).",
            "comparison_baseline": "Mixed-Integer Linear Programming (Gurobi), Constraint Programming (OR-Tools CP-SAT and IBM CPLEX CP Optimizer / DoCplex), greedy dispatching rule baseline, and two RL-based methods (Ho et al., Echeverria et al.).",
            "performance_learned_operator": "REMoH (Gemini 2.0 Flash) achieved a mean makespan GAP of 12.60% over Brandimarte instances (GAP = relative % difference to best-known lower bounds); produced a non-dominated front of 13 heuristics. Also shows consistently higher Hypervolume (HV) and lower Inverted Generational Distance (IGD) when the reflection mechanism is included (qualitative improvement reported; exact HV/IGD numbers not tabulated in paper text).",
            "performance_traditional_operator": "Best CP lexicographic configuration (DC_mb) obtained mean GAP of 1.43% on its prioritized objective (makespan) in the presented benchmarking; MILP runtimes and objective behavior reported but MILP sometimes outperformed REMoH on single prioritized metrics in small instances.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": "Diversity and solution-quality assessed using Hypervolume (HV) and Inverted Generational Distance (IGD); reflection-enabled runs had higher HV and lower IGD vs non-reflective variants (no additional novelty metric values reported).",
            "out_of_distribution_performance": "Generalization tested by training on Dauzere and Barnes and evaluating on Brandimarte; Gemini-produced heuristics were most consistent on unseen Brandimarte instances. GPT generalized better than DeepSeek despite DeepSeek showing stronger performance during training (indicates different generalization dynamics across LLMs).",
            "training_bias_evidence": "Observed behavior: GPT-4o converged early (premature convergence) during evolution, suggesting potential overfitting / limited exploration despite strong early-stage performance; DeepSeek improved more over later generations but generalized less well than GPT. No quantitative bias measures provided.",
            "computational_cost_comparison": "Reported average runtimes: MILP ~1661.5 s per instance (time-limited), CP (DoCplex) ~230.9 s, CP (OR-Tools) ~148.4 s. Greedy dispatching rule ~0.002 s. REMoH heuristics runtime (execution of generated heuristics) varied by LLM used: GPT-generated heuristics ~0.001 s, DeepSeek ~0.012 s, Gemini ~0.167 s (execution time per heuristic). Note: these are heuristic execution times (not LLM API cost/time) and mathematical solvers' runtimes reflect solver CPU times with limits; LLM inference / prompt-call costs and latency for generation are run via external APIs but not quantified as total optimization-run cost.",
            "transfer_learning_results": "Cross-dataset transfer assessed: LLMs trained/selected using Dauzere and Barnes were validated on Brandimarte. Gemini demonstrated better transfer/generalization consistency; no cross-domain transfers beyond FJSSP datasets were tested.",
            "domain_specific_vs_general_pretraining": "Three LLMs compared: Gemini 2.0 Flash (best final performance), DeepSeek-V3 (stronger training-phase improvement but worse generalization), GPT-4o (strong early performance but premature convergence). The paper does not detail differences in pretraining corpora; comparison is empirical on the FJSSP task.",
            "ablation_study_results": "Ablation removing the reflection mechanism: reflective variant had consistently higher HV and lower IGD across two experimental configurations (population sizes / iteration counts). Reflection preserves diversity and prevents premature convergence by producing clustering-guided offspring; exact numeric ablation values are presented in figures but not as tabular numeric summaries in text.",
            "hypothesis_space_characterization": "Clustering of parent heuristics in objective space (K-Means with silhouette-based K selection) is used to characterize hypothesis/heuristic-space structure; cluster centroids and short reflections summarize group strengths/weaknesses and guide offspring generation (qualitative characterization; no global coverage metrics reported).",
            "adaptation_during_evolution": "Yes — adaptation occurs via iterative reflections: short (per-cluster) and long reflections (across generations) are concatenated and passed to the LLM to guide crossover/mutation prompts; dynamic role instructions are used in initialization to diversify generation. This online prompt-guided adaptation improves HV/IGD per ablation.",
            "failure_modes": "Premature convergence observed with GPT-4o (strong early-stage but plateaus). DeepSeek-V3 showed stronger gains during training but generalized worse to held-out Brandimarte instances. Exact failure percentages or error cases not quantified. Infeasible heuristics are detected and re-prompted (no rate reported).",
            "key_findings_for_theory": "LLM-based operators, when embedded in an evolutionary loop and guided by introspective reflection, can produce diverse, interpretable heuristics that achieve competitive multi-objective trade-offs on FJSSP; reflection (cluster-based introspection) is a crucial mechanism that improves diversity and convergence (higher HV, lower IGD). LLM choice affects training vs generalization dynamics: some models (DeepSeek) improve more during evolution, others (GPT) generalize better early but may converge prematurely, while Gemini balanced late-stage improvement and generalization. Computational trade-offs differ: evolved heuristics execute very quickly compared to solvers, but LLM generation complexity (model inference) adds unquantified API cost; exact solver-optimality trade-offs depend on instance size (exact CP/MILP outperform on small instances for single prioritized objectives).",
            "uuid": "e1997.0"
        },
        {
            "name_short": "LLM-EPS / EoH / ReEvo / HSEvo",
            "name_full": "LLM-based Evolutionary Program Search family (including EoH, ReEvo, HSEvo)",
            "brief_description": "A set of approaches that represent individuals as LLM-generated code snippets and evolve them with evolutionary techniques; includes prior methods that inspired REMoH's encoding, reflection, and diversity strategies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-EPS family (EoH, ReEvo, HSEvo)",
            "operator_type": "LLM-based (evolutionary program search / hyper-heuristic)",
            "operator_description": "Individuals encoded as code generated by LLMs; approaches incorporate reflective prompting (ReEvo), harmony-search/diversity-driven initialization (HSEvo), and evolutionary program search (EoH). HSEvo used dynamic role instructions to diversify prompts; ReEvo combined verbal feedback with evolutionary search.",
            "training_data_description": null,
            "domain_or_benchmark": "Referenced across optimization tasks; cited in context of scheduling and heuristic design (no single benchmark specified in paper for these methods).",
            "comparison_baseline": "Presented historically as alternatives to manual heuristic design and traditional metaheuristics; paper references them as predecessors inspiring REMoH (no head-to-head comparisons in this paper).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": "HSEvo emphasizes diversity-driven harmony search; ReEvo uses reflective evolution and verbal feedback to guide improvements (no numeric metrics provided here).",
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "ReEvo and HSEvo use reflective mechanisms and diversity heuristics to characterize and explore heuristic spaces (conceptual discussion only).",
            "adaptation_during_evolution": "Yes (reflective prompts and dynamic role instructions used to adapt generation during evolutionary cycles).",
            "failure_modes": null,
            "key_findings_for_theory": "These prior LLM-based evolutionary paradigms motivated REMoH's representation (code-as-individual), reflection-driven operator guidance, and diversity-aware initialization; they underscore the importance of prompt design and feedback for guiding learned-operator evolution.",
            "uuid": "e1997.1"
        },
        {
            "name_short": "LLMs-GP",
            "name_full": "LLMs-GP (LLM-adapted Genetic Programming operators)",
            "brief_description": "An approach that adapts Genetic Programming principles by using LLMs' pattern-recognition capabilities to design genetic operators and generate executable optimization code.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLMs-GP",
            "operator_type": "LLM-based (inspired by genetic programming principles)",
            "operator_description": "Uses LLMs to design genetic operators (crossover/mutation) and produce executable code resembling GP outcomes; described as adapting GP principles through language-model pattern recognition (paper references it as related work).",
            "training_data_description": null,
            "domain_or_benchmark": "Mentioned generally in context of optimization algorithm generation and program evolution; no experimental benchmark reported in this paper.",
            "comparison_baseline": "Mentioned as an adaptation of Genetic Programming — implied comparators would be traditional GP operators, but no direct comparison data reported here.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Conceptually yes — LLMs craft operators during evolutionary cycles, but this paper does not provide implementation details.",
            "failure_modes": null,
            "key_findings_for_theory": "LLMs-GP exemplifies how LLMs can synthesize GP-like operators and code, suggesting a pathway to replace hand-designed GP operators with learned linguistic-generation mechanisms; however, empirical comparisons are not provided in this paper.",
            "uuid": "e1997.2"
        },
        {
            "name_short": "LLaMEA / AEL / Algorithmic Evolution",
            "name_full": "Algorithmic Evolution using LLMs (AEL), LLaMEA and related frameworks",
            "brief_description": "Frameworks where LLMs are embedded in an evolutionary cycle to generate, mutate, and evaluate optimization algorithms or constructive heuristics automatically, sometimes outperforming handcrafted or purely LLM-generated baselines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AEL / LLaMEA (Algorithmic Evolution using LLMs)",
            "operator_type": "LLM-based (algorithm/heuristic generation and mutation)",
            "operator_description": "LLMs are queried iteratively to propose algorithmic components (constructive algorithms, heuristics), mutate them, and use performance feedback to refine subsequent generations. Some frameworks also co-evolve test cases or use task descriptions and performance feedback to drive LLM mutations.",
            "training_data_description": null,
            "domain_or_benchmark": "TSP and other combinatorial optimization tasks referenced in literature citations; in-paper mention is general.",
            "comparison_baseline": "Described as outperforming handcrafted and purely LLM-generated baselines in cited works (no quantitative reproductions in this paper).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — iterative LLM feedback loops and mutation/evaluation cycles are core to these frameworks (described conceptually).",
            "failure_modes": null,
            "key_findings_for_theory": "These approaches show that LLM-driven algorithm evolution can generate competitive constructive algorithms for classic combinatorial problems and indicate that performance depends strongly on the feedback loop and evaluation strategy used.",
            "uuid": "e1997.3"
        },
        {
            "name_short": "LEO / LMEA / QDAIF / LMX",
            "name_full": "Representative LLM-driven evolutionary/heuristic systems (LEO, LMEA, QDAIF, LMX)",
            "brief_description": "Collection of systems that use LLMs as components in evolutionary optimization: LEO uses separate explore/exploit pools; LMEA lets LLMs autonomously perform selection and genetic operations; QDAIF integrates LLMs for quality-diversity evaluation; LMX uses LLMs as crossover operators by synthesizing offspring.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LEO, LMEA, QDAIF, LMX",
            "operator_type": "LLM-based (various roles: operators, evaluators, selectors)",
            "operator_description": "LEO: LLM-based evolutionary optimizer with explore and exploit pools. LMEA: LLMs autonomously conduct selection and genetic ops with a self-adaptive temperature. QDAIF: Quality-diversity framework using LLMs to generate and evaluate candidate solutions. LMX: LLMs synthesize offspring by combining multiple parents or predicting code differences.",
            "training_data_description": null,
            "domain_or_benchmark": "Referenced for single and multi-objective combinatorial tasks (example: TSP).",
            "comparison_baseline": "Implied comparison vs traditional heuristics and GP-style operators in cited literature; no direct comparisons reproduced here.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": "QDAIF explicitly focuses on quality-diversity evaluation; LEO uses separate pools to maintain exploration/exploitation balance. REMoH uses HV/IGD for diversity/convergence evaluation.",
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — several of these systems incorporate mechanisms (temperature adaptation, separate pools, reflective prompts) to adapt LLM behavior over generations.",
            "failure_modes": null,
            "key_findings_for_theory": "Demonstrates a variety of roles LLMs can take within evolutionary systems (operator generation, evaluation, selection), suggesting rich hybridization opportunities with traditional GP and EA operators.",
            "uuid": "e1997.4"
        },
        {
            "name_short": "SeEvo / LHHs / LLM-EPS frameworks",
            "name_full": "SeEvo (self-evolutionary population), Language Hyper-Heuristics (LHHs), and related LLM-EPS frameworks",
            "brief_description": "Frameworks using LLMs to generate and evolve dispatching rules and heuristics over time; LHHs combine LLM heuristic generation with Reflective Evolution to refine heuristics using verbal feedback plus evolutionary search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SeEvo, LHHs",
            "operator_type": "LLM-based (heuristic rule generation / hyper-heuristic)",
            "operator_description": "SeEvo: LLMs generate dispatching rules that evolve over time with reflection-inspired mechanisms. LHHs: LLMs generate heuristics with minimal input; Reflective Evolution (ReEvo) complements with verbal feedback plus evolutionary search.",
            "training_data_description": null,
            "domain_or_benchmark": "Scheduling and combinatorial optimization tasks (dynamic job shop scheduling referenced).",
            "comparison_baseline": "Positioned in literature as alternatives to hand-designed dispatching rules and RL-based schedulers; no reproduction data in this paper.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — reflective mechanisms and evolutionary search are core features enabling adaptation.",
            "failure_modes": null,
            "key_findings_for_theory": "These frameworks highlight the practicality of LLMs to produce interpretable dispatching rules and hyper-heuristics, and motivate the reflection mechanism used in REMoH to balance exploration/exploitation.",
            "uuid": "e1997.5"
        },
        {
            "name_short": "RL baselines (Ho et al., Echeverria et al.)",
            "name_full": "Deep RL scheduling baselines referenced (Ho et al., Echeverria et al.)",
            "brief_description": "State-of-the-art reinforcement learning approaches for FJSSP used as baselines: Ho et al. use graph neural networks with filtering, Echeverria et al. combine CP with RL and train on CP-generated optimal solutions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RL baselines (Ho et al., Echeverria et al.)",
            "operator_type": "Neural network / Reinforcement Learning",
            "operator_description": "Ho et al.: GNN-based construction heuristic with filtering of completed jobs and machines to focus decision space. Echeverria et al.: hybrid CP+RL approach where the model is trained on CP-generated optimal solutions and CP refines intermediate solutions.",
            "training_data_description": "Echeverria et al. trained on optimal solutions generated via CP; Ho et al. trained on scheduling instances (details in original papers). In this paper the original implementations were not publicly available, so reported performance values were taken from those publications.",
            "domain_or_benchmark": "FJSSP (Brandimarte instances); reported primarily for makespan optimization on instances 1-10 in the cited works.",
            "comparison_baseline": "Compared in REMoH benchmarking: RL methods focused only on makespan and could not obtain results for instances larger than Mk10; REMoH (multi-objective) compared favorably in many Brandimarte instances.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "RL baselines limited scalability: could not report results for instances larger than Mk10 as noted in paper.",
            "training_bias_evidence": "Paper notes RL methods require retraining when new constraints introduced (e.g., nonlinear setup times) — implying sensitivity to training distribution and cost of adapting to new constraints.",
            "computational_cost_comparison": "Not quantified in this paper (performance values extracted from publications); authors note RL methods can be computationally expensive to retrain for new constraints.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": "RL approaches require retraining for new constraints and did not scale to larger instances in cited works (Mk &gt; 10).",
            "key_findings_for_theory": "RL methods can be effective for single-objective optimization but are less flexible than prompt-driven LLM approaches when incorporating new, nonlinear constraints without retraining; LLM-based REMoH adapted to nonlinear SDST with minimal prompt changes.",
            "uuid": "e1997.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PromptBreeder: Self-referential self-improvement via prompt evolution",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Reevo: Large language models as hyper-heuristics with reflective evolution",
            "rating": 2
        },
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting",
            "rating": 1
        },
        {
            "paper_title": "Large language model-based evolutionary optimizer: Reasoning with elitism",
            "rating": 1
        }
    ],
    "cost": 0.02042025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models</p>
<p>Diego Forniés-Tabuenca 
Vicomtech Foundation
Basque Research and Technology Alliance (BRTA)
Donostia, GipuzkoaSpain</p>
<p>Department of Computer Sciences and Artificial Intelligence
University of the Basque Country (UPV/EHU)
Donostia, GipuzkoaSpain</p>
<p>Alejandro Uribe 
Vicomtech Foundation
Basque Research and Technology Alliance (BRTA)
Donostia, GipuzkoaSpain</p>
<p>School of Applied Sciences and Engineering
Universidad EAFIT
Medellin, AntioquiaColombia</p>
<p>Urtzi Otamendi 
Vicomtech Foundation
Basque Research and Technology Alliance (BRTA)
Donostia, GipuzkoaSpain</p>
<p>Department of Computer Sciences and Artificial Intelligence
University of the Basque Country (UPV/EHU)
Donostia, GipuzkoaSpain</p>
<p>Arkaitz Artetxe 
Vicomtech Foundation
Basque Research and Technology Alliance (BRTA)
Donostia, GipuzkoaSpain</p>
<p>Juan Carlos Rivera 
School of Applied Sciences and Engineering
Universidad EAFIT
Medellin, AntioquiaColombia</p>
<p>Oier Lopez De Lacalle 
HiTZ Basque Center for Language Technology
University of the Basque Country (UPV/EHU)
Donostia, GipuzkoaSpain</p>
<p>REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models
A0AB3AF9D69DA2402333BF95BE4D7557Large Language ModelOptimizationJob Shop SchedulingMulti-Objective
Multi-objective optimization is fundamental in complex decision-making tasks.Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures.Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning.This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation.A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity.The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte.Results demonstrate that REMoH achieves competitive results compared to state-of-theart approaches with reduced modeling effort and enhanced adaptability.These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.</p>
<p>I. INTRODUCTION</p>
<p>T HE core challenge in many domains lies in optimization: the quest to find the best solution from a set of available alternatives.Optimization algorithms are essential tools for this quest across diverse fields, including scheduling, logistics, finance, healthcare, and artificial intelligence [1].As data complexity and scale increase, the demand for a efficient optimization strategies becomes more critical [2].While traditional optimization algorithms have achieved notable success, their design often relies heavily on domain-specific expertise.This reliance can limit adaptability and scalability, making responding to dynamic or poorly defined problem structures difficult [3].</p>
<p>Large Language Models (LLMs) emerge as transformative tools in the field of optimization and decision-making, offering distinct advantages over traditional methods.Unlike conventional approaches such as metaheuristics or Reinforcement Learning (RL), which often suffer from lengthy training phases and convergence to local optima, LLMs utilize Corresponding author: Diego Forniés-Tabuenca pre-trained experiential knowledge to enable real-time, selfadaptive scheduling [4].This capability positions LLMs not only as optimization tools but also as communicative agents that align production goals with dynamic decision-making requirements.However, the practical deployment of LLMs in engineering contexts faces some challenges.Their inherent biases and potential for factual inaccuracies necessitate robust grounding mechanisms.Current implementations of LLMs, while capable of interpreting and executing textual scheduling commands, often lack direct alignment with the physical and operational realities of manufacturing environments [5].This highlights the critical role of self-supervised learning and other calibration strategies to ensure the reliability and applicability of LLMs in industrial settings.</p>
<p>A promising direction involves the synergistic integration of LLMs with classical optimization algorithms.This hybridization leverages LLMs' extensive domain knowledge to enrich modeling and strategic decision-making, while optimization algorithms enhance LLM behavior and output quality.Such combinations pave the way for intelligent search operations, adaptive algorithm configuration, and even the autonomous generation of new optimization strategies, thereby reducing the dependence on human expertise and expanding the scope of problem-solving in complex domains [1].Evolutionary algorithms (EAs), in particular, present complementary strengths when paired with LLMs.While EAs offer robust global search mechanisms in black-box optimization contexts, LLMs contribute by guiding these searches with contextual knowledge and textual reasoning capabilities.This mutual enhancement broadens the range of problems each technique can effectively address and fosters more intelligent and efficient exploration in diverse computational environments [6].</p>
<p>Recent advancements further demonstrate the potential of LLMs to automate the generation and evolution of optimization components such as heuristics, search operators, and algorithmic frameworks [1,7,8].Through their natural language understanding and generative abilities-especially in code synthesis-LLMs can function as optimizers or codesigners in evolutionary processes.These novel approaches aim to reduce manual intervention, facilitate intelligent adaptation to specific problem instances, and enhance exploration-arXiv:2506.07759v1 [cs.AI] 9 Jun 2025 exploitation trade-offs [9].Despite significant progress in evolutionary and mathematical optimization techniques, designing adaptive and generalizable heuristics for multiple objectives remains challenging.</p>
<p>The contribution of this paper is a novel framework that leverages LLMs to enhance multi-objective optimization through dynamic heuristic generation and search space reflection.Specifically, we introduce Reflective Evolution of Multiobjective Heuristics (REMoH), a hybrid framework designed to evolve high-quality and diverse heuristic operators for multiobjective problems.This paper proposes the following key contributions:</p>
<p>• REMoH Framework: A novel optimization methodology that integrates LLM-generated heuristic operators within an NSGA-II selection process.The LLM generates domain-agnostic, human-readable heuristics, improving explainability and adaptability across problem instances.• Reflection Mechanism: An innovative component that performs clustering on the current heuristic population and uses the reflection mechanism to improve search space exploration, prevent premature convergence, and enhance the diversity of solutions.• Ablation Study: A detailed ablation analysis demonstrates the effectiveness of the reflection mechanism.</p>
<p>Results show significant gains in Hypervolume (HV) and Inverted Generational Distance (IGD) when the reflection mechanism is included.• Comprehensive Benchmarking on FJSSP: The proposed approach is benchmarked against mathematical modeling (Mixed-Integer Linear Programming and Constraint Programming) and state-of-the-art learning-based (Reinforcement Learning) methods using the Flexible Job Shop Scheduling Problem (FJSSP).Experiments are conducted on the widely used Brandimarte suite.• Modeling Flexibility and Constraint Integration: Unlike traditional mathematical approaches that struggle with non-linear constraints, the proposed framework can integrate complex and context-sensitive constraints with little reformulation effort, demonstrating robustness and flexibility.The remainder of this article is organized as follows: Section II reviews related work on integrating LLMs in multi-objective optimization.Section III presents the proposed REMoH framework in detail.Section IV outlines the experimental setup, including datasets, evaluation metrics, and baseline methods.Section V discusses the experimental results, highlighting both the benchmarking analysis and the ablation study.Finally, Section VI concludes the paper and suggests directions for future research.</p>
<p>II. STATE OF THE ART</p>
<p>Building upon the foundational capabilities of LLMs in optimization and decision-making, recent research has expanded their utility through integration with other artificial intelligence methodologies.Notably, the combination of RL and LLMs has demonstrated significant promise in decision support systems, particularly in domains such as crop management.Here, RL contributes adaptive policy learning, while LLMs provide semantic context and domain-specific knowledge, enabling more nuanced and context-aware decisions [10].A concrete application involved integrating deep Q-networks (DQN), the DistilBERT model, and crop simulations to generate and interpret state variables.These were translated into language representations to enhance the LLM's interpretive capacity, leading to substantial economic performance gains in realworld agricultural settings.However, in this case, the LLM functioned primarily as a state encoder, indicating substantial untapped potential for deeper LLM integration beyond auxiliary roles in RL frameworks [11].</p>
<p>In parallel, novel systems like AutoML-GPT illustrate the growing relevance of LLMs in automating complex machine learning pipelines.By combining GPT-based models with dynamic hyperparameter optimization, AutoML-GPT can autonomously process user requirements, generate suitable prompts, select model architectures, tune parameters, and manage the entire experimental lifecycle [12].Other approaches have investigated the potential of LLMs in hyperparameter optimization (HPO).These approaches move beyond conventional search spaces: some allow the LLM to identify optimizable parameters and their bounds autonomously, and even to treat elements such as model code as a hyperparameter, vastly expanding the HPO design space [13].Architectures like AgentHPO implement multi-agent systems in which an LLM-based Creator agent simulates expert reasoning to propose high-quality hyperparameter configurations, followed by an Executor agent that evaluates them, streamlining a process typically dependent on substantial domain knowledge and computational resources [14].</p>
<p>Further refinement of LLMs' reasoning capacity has been achieved through advanced prompting techniques, particularly chain-of-thought (CoT) prompting.Unlike standard few-shot learning, CoT prompting supplies LLMs with intermediate reasoning steps, enabling them to decompose complex tasks into structured sub-problems that guide inference processes more effectively [15].Recent developments, such as the PromptBreeder system, have taken this a step further by employing CoT prompting in a self-referential, evolutionary framework.PromptBreeder iteratively improves both task and mutation prompts, allowing LLMs to self-optimize over time.This approach has demonstrated superior performance over standard prompting strategies, enhancing LLM effectiveness in domains requiring sophisticated reasoning and interpretability [16].</p>
<p>Beyond their integration with external AI techniques and advanced prompting strategies, recent developments have increasingly focused on using LLMs as intrinsic optimizers.One notable contribution is Optimization by PROmpting (OPRO), a technique where the optimization problem is formulated in natural language, and the LLM iteratively proposes new candidate solutions based on prior feedback embedded within the prompt [17].Empirical validation on tasks such as linear regression and the Traveling Salesman Problem (TSP) demonstrated substantial performance gains, underscoring the potential of prompt-based optimization without explicit mathematical modeling.Parallel work has explored the direct for-mulation and solution of mathematical programming problems using LLMs.The OptiMUS framework, for instance, leverages LLMs to interpret natural language problem descriptions and translate them into structured Mixed-Integer Linear Programming (MILP) formulations [18].Beyond model generation, this approach is capable of debugging code, developing test cases, and verifying solutions, illustrating a comprehensive pipeline for language-driven optimization.Other techniques employ neural bandit algorithms for black-box optimization of prompts balancing exploration and exploitation without relying on traditional Bayesian optimization techniques [19].</p>
<p>An emerging line of research leverages the generative and reasoning capacities of LLMs to design novel heuristics and metaheuristic algorithms, advancing the field of algorithmic optimization beyond traditional boundaries.For instance, the LLM assisted Hyper-heuristic Optimization Algorithm (LL-MOA) integrates LLMs as high-level controllers that, through prompt engineering, generate adaptive optimization strategies.This architecture is complemented by low-level elitebased local search heuristics -grounded in the proximate optimality principle-and enriched with operators from differential evolution, producing a diverse and effective set of search behaviors [8].Similarly, GPT-4 has been employed to synthesize hybrid optimization algorithms by combining characteristics of well-known metaheuristics such as cuckoo search, whale optimization, particle swarm optimization, grey wolf optimizer, and others [7].Further exploration into LLMaided algorithm design has led to the development of entirely new metaheuristics, such as the zoological search optimization (ZSO), created through ChatGPT's text generation capabilities guided by the structured CRISPE prompting framework [20].</p>
<p>Recent advances have demonstrated the potential of integrating LLMs within evolutionary heuristic frameworks, fostering a new class of adaptive optimization methods.This synergistic interaction between evolutionary algorithms (EAs) and LLMs is gaining recognition as a promising direction for tackling generative and optimization tasks, with LLMs enriching the intelligence of EAs and EAs offering global search mechanisms for optimizing LLM-driven processes [21].This bidirectional influence is shaping a novel paradigm in optimization research, merging the strengths of both paradigms toward more general and autonomous problem-solving capabilities.</p>
<p>One important research direction involves the evolutionary generation of optimization algorithms themselves, where LLMs are embedded within the evolutionary loop to produce, mutate, and refine algorithms across iterations.For instance, the Algorithmic Evolution using LLMs (AEL) approach [22] interacts with LLMs as part of an evolutionary cycle to automatically generate constructive algorithms tailored to specific problems, outperforming both handcrafted and purely LLMgenerated baselines.Expanding on this concept, a similar framework [9] was applied to design a Guided Local Search algorithm for the TSP, demonstrating the practicality of fully automating algorithm creation without prior model training.In a complementary effort, LLaMEA [23] presents a framework where LLMs iteratively generate, mutate, and evaluate optimization algorithms based on task descriptions and performance feedback, offering an efficient and expert-free route to generate novel metaheuristics.Likewise, LLMs-GP [24] adapts Genetic Programming principles by leveraging LLMs' pattern recognition capabilities to design genetic operators and produce executable optimization code.A more dynamic variant, CoCoEvo [25], proposes a co-evolutionary framework where both programs and test cases evolved simultaneously from natural language task descriptions, removing the dependency on pre-defined tests and employing LLM-based operators to drive the co-evolution process.</p>
<p>Another line of work focuses on using LLMs as evolutionary or heuristic operators to improve solution generation and enhance optimization quality.LEO [26], the Languagemodel-based Evolutionary Optimizer, incorporates LLMs in a population-based strategy with separate explore and exploit pools, enabling robust search performance in both single and multi-objective settings.A further contribution in this direction is the LLM-driven Evolutionary Algorithm (LMEA), which introduces a lightweight but effective paradigm where LLMs autonomously conduct selection and genetic operations without requiring retraining or expert input [27].Notably, it integrates a self-adaptive temperature mechanism that dynamically balances exploration and exploitation, achieving competitive results compared with traditional heuristics on benchmark combinatorial tasks like the TSP.Similarly, QDAIF [28] introduces a quality-diversity framework where LLMs are used not only to generate candidate solutions but also to evaluate their diversity and quality, facilitating a more informed and iterative evolutionary process.In terms of specific operator design, LMX [29] and difference modeling [30] offer mechanisms where LLMs function as crossover operators by synthesizing offspring from multiple parents, or by predicting code differences to model solution variation.In the multiobjective domain, researchers [31] have proposed using LLMs as black-box search operators within decomposed subproblems of a multi-objective optimization problem, enabling zeroshot optimization without problem-specific learning or expert tuning.</p>
<p>Beyond algorithm and solution generation, several works explore LLMs for heuristic design and adaptation, especially in scheduling and combinatorial optimization.SeEvo [32] introduces a self-evolutionary population model where LLMs not only generate dispatching rules for dynamic job shop scheduling problems but also evolve them over time, inspired in human-like reflection mechanisms, to guide both individual creation and population exploration.Similarly, a two-phase task scheduling framework [33] leverages LLMs to first generate heuristic strategies and corresponding code and then evaluate and evolve these scheduling schemes across generations.A broader formulation is provided by the Language Hyper-Heuristics (LHHs) framework [34], where LLMs generate heuristics with minimal human input, and are enhanced by Reflective Evolution (ReEvo), a method combining verbal feedback from LLMs with evolutionary search to explore and refine large spaces of possible heuristic combinations.Finally, the LLM-EPS framework [35] addresses the persistent challenge of balancing exploration and exploitation in heuristic search spaces by introducing diversity metrics inspired by Shannon Entropy.To support this, HSEvo, a variant of LLM- EPS, incorporates harmony search principles and low-cost LLM interactions ("flash reflection") to promote diversity while improving convergence efficiency.</p>
<p>In contrast to existing approaches, this research introduces a novel framework that integrates LLM-based heuristic generation with a reflective mechanism grounded in clustering and performance introspection.Unlike prior efforts that either use LLMs passively to generate heuristics or evolve them without structured feedback, REMoH actively guides the evolutionary process through dynamic, multi-level reflection.This enables the adaptive generation of diverse, high-performing multi-objective heuristics with minimal human intervention, addressing the open challenge of maintaining both exploration and convergence in heuristic evolution for complex scheduling problems.</p>
<p>III. METHODOLOGY</p>
<p>In this section, we introduce REMoH, a novel algorithm that integrates LLMs with the traditional NSGA-II framework to generate heuristics for solving multi-objective optimization problems.Additionally, we introduce an innovative reflection mechanism to assist the algorithm in discovering new heuristics that efficiently dominate the Pareto front, aiming for its convergence toward the optimal front.</p>
<p>As shown in Figure 1, the process begins with the initialization of the population using dynamic prompts to foster diversity.Each heuristic is evaluated and selected according to the NSGA-II framework, based on Pareto dominance and crowding distance.Next, a clustering-based reflection is performed to extract key features from each group of heuristics, identifying their strengths and weaknesses to guide the next stage.In this phase, new offspring are generated through reflective crossover and elitist mutation.Finally, the newly generated heuristics are evaluated and selected alongside their parents, or the process terminates if the final generation has been reached.</p>
<p>The following sections provide a detailed description of each step in the process.</p>
<p>Individual encoding.Following the principles of LLMbased Evolutionary Program Search (LLM-EPS) approaches like EoH [9] and ReEvo [34], REMoH represents each individual as a code snippet generated by an LLM.This encoding approach allows for high flexibility, as individuals are not constrained by predefined structures, and enables direct evaluation within the problem context.By evolving code rather than fixed parameters, REMoH dynamically adapts its heuristic strategies, making it well-suited for complex scheduling and optimization tasks.</p>
<p>Population initialization.REMoH initializes each heuristic by providing the LLM with the task specification.The task specification includes the description of the optimization problem and the function details, outlining both the input and output formats.To leverage existing knowledge, the framework can be initialized with a seed heuristic function.To help the model diversify the initial population, we have drawn on the HSEvo strategy [35], which alters the role instruction to generate different prompts.This approach aids the model in enriching its initial heuristics.Typically, the initial population is larger than in subsequent generations.This is a common strategy in evolutionary algorithms to enhance diversity in the first generation.</p>
<p>Evaluation.To assess the performance of the heuristics, they are tested across various problem instances, with each heuristic receiving a score for every objective function.These scores are then normalized using Standard Scaler and averaged to evaluate overall performance.Outlier scores do not contribute to the normalization parameters to prevent bias from extremes.Furthermore, if a heuristic is infeasible or contains implementation errors, the LLM is prompted to generate a new one to prevent information loss.</p>
<p>Selection.Based on the evaluation, our algorithm selects parents using the NSGA-II approach, ranking individuals based on their dominance against others.Then, for individuals in the last front to be selected, we choose them based on their crowding distance, ensuring diversity within the population.</p>
<p>Clustering Reflection.The LLM generates reflection through clustering parents based on their performance regarding dominance.Our reflection contains three main steps:</p>
<p>• Group the parents based on their fitness in different objective functions using K-Means clustering.The number of clusters (K) is determined through the silhouette method due to its performance when used in multi-objective algorithms [36].• Generate a brief reflection for each cluster by prompting the LLM with the code from each individual and their overall performance based on the cluster centroid.This reflection will summarize the heuristics, emphasizing their main characteristics and key features.• Produce the final reflection providing the LLM with all short cluster reflections, their general performance, and previous long-term reflections.In this final step, the goal is to analyze the strengths and weaknesses of each group.The LLM identifies emerging trends and proposes potential directions for future exploration.This clustering reflection process ensures a strategic balance between exploration and exploitation.By synthesizing performance patterns and heuristic structures across different objective clusters, the LLM guides the generation of more informed and effective offspring capable of tackling multiobjective optimization.</p>
<p>Crossover.In this stage, new offspring algorithms are created by merging components from the parent algorithms.The main objective is to generate new heuristics based on the parents, leveraging their strengths and avoiding their weaknesses, as determined by the clustering reflection.The LLM is provided with the crossover task specifications, including the two parents' code and the reflection.</p>
<p>Elitist Mutation.This step involves selecting the individual with the best fitness achieved so far, defined as the one obtaining the highest average across all objective functions.This criterion promotes solutions that achieve a balanced tradeoff among objectives.Following the HSEvo framework [35], we incorporate reflection into this mutation process, ensuring that the improvement is not purely random but guided by prior analysis.The LLM is guided by mutation task specifications, comprising the elite individual's code and its reflection.</p>
<p>IV. EXPERIMENTAL SETUP</p>
<p>This section describes the experimental setup used to evaluate the proposed REMoH algorithm.The aim is to assess the effectiveness of our multi-objective optimization framework through a well-established and practically relevant problem: the FJSSP [37].</p>
<p>Scheduling problems are critical in industrial and operational environments since they directly impact efficiency, resource utilization, and overall productivity [38,39].Among these, the Job Shop Scheduling Problem (JSSP) is particularly challenging due to its combinatorial nature and the complexity of assigning operations to machines while optimizing objectives.The generalization of JSSP that considers a set of eligible machines that can process each operation is named FJSSP.This additional flexibility increases the complexity and applicability of the problem in real-world manufacturing systems [40].</p>
<p>The main goal in FJSSP is to determine the optimal assignment of operations to machines and the sequencing of operations on each machine, to optimize objectives such as minimizing the makespan (total completion time), balancing machine workloads, or minimizing total idle time.</p>
<p>This experimentation phase focuses on two specific objectives: (1) Makespan, defined as the total time required to complete all scheduled jobs, and (2) Workload Balance, which measures the maximum workload across all machines.These are widely recognized key performance indicators (KPIs) in the scheduling literature.While makespan minimization is traditionally the primary focus, we include workload balance as an additional objective to evaluate the performance of the multi-objective framework under a broader set of constraints.</p>
<p>A. Evaluation and Selection Criteria</p>
<p>This section describes the evaluation methodology used to compare and select the most effective heuristics within the proposed framework.As introduced in the methodology section, each heuristic is evaluated by solving all problem instances in the dataset.For each instance, the objective values are normalized across the population, and the mean of these normalized values is computed per objective to represent overall performance.</p>
<p>In the final evaluation, a similar procedure is applied across all instances and generations.Specifically, the initial population and each generation are evaluated on the full set of FJSSP instances.The resulting objective values are normalized per instance using a robust normalization strategy based on percentiles to reduce the influence of outliers.This normalization is applied consistently across all independent experiments being compared, ensuring that the scores are scaled relative to overall performance and can be fairly evaluated.After normalization, mean objective values are computed for each individual across all instances.</p>
<p>To evaluate the quality of the obtained heuristics and the resulting Pareto front across generations, two standard multiobjective optimization metrics are employed:</p>
<p>• Hypervolume (HV): It measures the volume of the objective space dominated by the obtained Pareto front with respect to a reference point, effectively capturing both convergence and diversity.After a calibration procedure, the reference point was set at 0.3 for each objective function axis.• Inverted Generational Distance (IGD): Calculates the average distance from points in a reference Pareto front to the nearest point in the obtained front, providing insight into convergence and distribution.Due to the lack of ground truth Pareto fronts in most FJSSP datasets, the reference set is constructed as the non-dominated front obtained from the union of all solutions generated by competing heuristics.Then, this Pareto is used to compute the metrics for all individuals.</p>
<p>Additionally, the average IGD and HV values are computed per generation to evaluate the evolution of heuristic quality over generations.This helps analyze the convergence and diversity preservation throughout the evolutionary process.This evaluation procedure is applied consistently across experiments, including the benchmarking and ablation studies discussed in the results section.</p>
<p>Also, to avoid the stochastic nature of evolutionary algorithms, each metric is obtained by averaging the results obtained in three independent runs, ensuring statistical robustness.</p>
<p>B. Instances</p>
<p>For strict evaluation and to facilitate comparative benchmarking of the proposed methodology, we employed three established and publicly available benchmark datasets relevant to the FJSS problem:</p>
<p>• Brandimarte: In his seminal work Routing and scheduling in a FJSSP by tabu search, Brandimarte et al [41] introduced a set of instances labeled MK01 to MK15.These vary in size and are primarily designed for evaluating approaches focused on makespan minimization, though they are adaptable to other objectives.• Barnes: One of the earliest public datasets for the FJSSP, introduced by Barnes et al. [42], alongside a novel tabu search approach.These instances remain a foundational benchmark in the literature.• Dauzere: This dataset [38] consists of 18 problem instances varying in size and flexibility.It is a widely adopted benchmark for evaluating FJSSP algorithms and is critical for assessing performance and scalability in newly developed scheduling techniques.Among these, the Brandimarte dataset has been widely utilized in numerous studies for benchmarking heuristic and metaheuristic algorithms in the FJSSP [38].For this reason, it was selected for in-depth evaluation of solution quality and robustness.Specifically, in the Ablation Study V-A and Flexibility Evaluation V-C, Brandimarte was used solely for training, since these experiments focused on model behavior and did not require external validation.Conversely, the other datasets were used for training in the Benchmarking Methods V-B.At the same time, Brandimarte served as the evaluation set, enabling a fair comparison of generalization and performance on a widely accepted benchmark.</p>
<p>C. Benchmarking Methods</p>
<p>The experimentation process includes a comprehensive benchmarking analysis against classical optimization methods and recent state-of-the-art learning-based techniques.This comparison aims to assess not only solution quality but also flexibility, scalability, and adaptability of our approach across a diverse set of FJSSP instances.</p>
<p>Following the work of Dauzere et al. [38], exact methods such as Branch and Bound (B&amp;B), Mixed-Integer Linear Programming (MILP), and Constraint Programming (CP) have historically been employed to tackle the FJSSP.In this sense, CP has demonstrated superior performance, particularly in solving medium-sized instances optimally and generating feasible solutions for larger and more complex scenarios.MILP, while offering an exact mathematical representation of scheduling constraints and objectives, has challenges with problem size due to its combinatorial nature.</p>
<p>In the benchmarking framework, we include three classical optimization methods and two recent deep learning-based approaches inspired by RL.Including RL methods reflects a growing interest in data-driven, adaptive optimization strategies capable of generalizing across problem instances.These learning-based methods generate learning policies that directly map states to scheduling actions, potentially offering better scalability and adaptability in dynamic environments.</p>
<p>Additionally, we employ a lexicographic optimization strategy to ensure a fair comparison across methods, especially those that do not natively support multi-objective optimization.In this strategy, one solution is obtained by prioritizing makespan as the primary objective and workload balance as the secondary; a second solution is derived by reversing the priorities, workload balance first, then makespan.This process yields two points: in many cases, they coincide, indicating aligned optima, while in others, they differ, capturing the tradeoff between objectives.This dual evaluation enables consistent comparison across single-and multi-objective solvers.</p>
<p>1) Mixed Integer-Linear Programming: This section presents the MILP formulation for the FJSSP, based on the model presented by Dauzere et al. [38].The formulation captures the essential constraints of FJSSP, ensuring a feasible and optimal assignment of operations to machines while minimizing the makespan or workload balance.The lexicographic process allows us to construct two points of the Pareto front, providing a strong baseline to compare against the results obtained by the algorithms developed in our proposed methodology.The following describes the notations used, sets, parameters, variables, and the formulation.</p>
<p>In this formulation, the objective is to minimize the makespan, as indicated in Equation 1. Constraint 2 ensures that
min c max (1) k∈Ri α k,i = 1 ∀ i ∈ O (2) t i ≥ t pr(i) + k∈R pr(i) p k,pr(i) • α k,pr(i) ∀ i ∈ O (3) t i ≥ t i ′ + p k,i ′ − (2 − α k,i − α k,i ′ + β i,i ′ ) • H ∀ (i, i ′ ) ∈ O × O, s.t i ̸ = i ′ , ∀ k ∈ R i ∩ R i ′ (4) t i ′ ≥ t i + p k,i − (3 − α k,i − α k,i ′ + β i,i ′ ) • H ∀ (i, i ′ ) ∈ O × O, s.t i ̸ = i ′ , ∀ k ∈ R i ∩ R i ′(5)c max ≥ t i + k∈Ri p k,i • α k,i ∀ i ∈ O (6) α k,i ∈ {0, 1} ∀ i ∈ O, k ∈ R i (7) β i,i ′ ∈ {0, 1} ∀ (i, i ′ ) ∈ O × O(8)r max ≥ i∈O, s.t k∈Ri p k,i • α k,i ∀ k ∈ R(11)
In the lexicographic approach, once the makespan has been minimized, the next objective is to minimize workload balance, as expressed in Equation 9. To implement this approach, the makespan variable must be constrained so that it does not exceed the previously obtained value (Constraint 10), and the maximum machine productive time must also be appropriately bounded (Constraint 11).The remaining constraints from the previous model remain unchanged.
r max ≤ RM(12)
When workload balance is prioritized as the primary objective, the objective function in Equation 1 is replaced with that in Equation 9 in the proposed model.Additionally, the constraint defining the makespan (Constraint 6) is substituted by the one defining the maximum machine productive time (Constraint 11).In this case, during the second phase of the lexicographic approach, the objective becomes the minimization of the makespan (Equation 1).Instead of using Constraint 10 to limit the makespan, Constraint 12 is introduced to impose an upper bound on the allowed usage of each machine.</p>
<p>2) Constraint Programming Approach: Constraint Programming (CP) is a paradigm for solving combinatorial optimization problems [43], particularly effective in complex scheduling contexts such as the FJSSP [44].CP allows problems to be modeled declaratively by specifying decision variables, their domains, and the constraints that generate feasible solutions.</p>
<p>In this FJSS context, each job comprises a sequence of operations, where each operation may be assigned to a subset of eligible machines, each with distinct processing times.The problem includes two primary constraints (see algorithm 1).On the one hand, precedence constraints enforce the prescribed order of operations within each job.On the other hand, resource constraints prevent overlapping operations on the same machine.</p>
<p>For benchmarking, we implemented the CP model using two state-of-the-art solvers: (i) Google OR-Tools CP-SAT [45], a robust open-source constraint programming engine known for its efficiency in solving scheduling problems, and (ii) IBM CPLEX CP Optimizer (DoCplex) [46], a commercial optimization suite that offers advanced support for scheduling via interval and sequence variables.These state-of-the-art CP solvers internally apply advanced techniques-combining constraint propagation with heuristic-guided search strategies-to prune the search space and construct feasible solutions.</p>
<p>In the proposed pseudocode 1, the two sequential objectives are: (1) minimizing the makespan, denoted as C max , and (2) minimizing the maximum workload across machines, denoted as L max .</p>
<p>For each operation o ji of job j ∈ J, assigned to machine m ∈ M ji , the model defines optional interval variables iv jim with fixed durations p jim .Each operation can be scheduled on a subset of eligible machines M ji .This flexibility is captured via the alternative constraint, which connects the operation to its possible machine-specific interval assignments.The scheduling order of operations within the same job is enforced using end before start precedence constraints.A no overlap constraint is applied to all intervals scheduled on the same machine to prevent machine conflicts.</p>
<p>The total load on each machine m ∈ M is computed using the sum of processing times of all operations scheduled on that machine.The makespan C max is defined as the latest completion time among all operations, and L max is the maximum load across machines.To implement lexicographic optimization, the solver first minimizes.Once the optimal value is found, a constraint is added to fix its value by setting an upper bound.This ensures that any subsequent solution maintains the optimal primary objective value.Then, the model proceeds to minimize the secondary objective.</p>
<p>Overall, CP provides a scalable and expressive framework for solving FJSS instances with high precision and is especially advantageous in instances where logical dependencies and disjunctive resource constraints dominate.</p>
<p>3) Baseline dispatching rule: The third classical algorithm evaluated in the benchmarking study aims to define a heuristic baseline, based on a well-established dispatching rule methodology.This approach addresses the FJSSP by making sequential scheduling decisions according to predefined priority rules.</p>
<p>Rather than solving the entire problem globally, the heuristic proceeds iteratively, selecting which job to schedule next and assigning its operations to available machines following specific criteria.</p>
<p>The algorithm 2 shows the decision-making process that begins by establishing a priority order for the jobs (line 3).Initially, this sequence is determined randomly to introduce variability and exploratory behavior.However, a more refined approach might incorporate other priority rules to improve performance in objectives such as tardiness or makespan.</p>
<p>Once a job is selected according to the priority, its operations are scheduled sequentially (line 7), respecting the defined route of the job.For each operation, the heuristic evaluates all machines capable of executing it (line 9).The objective is to identify the earliest possible time slot on any of these eligible machines, accounting for the current machine schedules and the completion time of the preceding operation (parameter tc).This ensures temporal feasibility and maintains the required sequence.</p>
<p>For each eligible machine, the heuristic calculates the potential start and finish times for the operation.It then selects the machine and the corresponding time slot, yielding the earliest possible finish time for the current operation (line 10).This greedy strategy aims to minimize the overall completion time of the operations.This is achieved by iteratively updating the parameters bt (best time), bm (best machine), and bd (best duration) upon the identification of a better result (lines 12, 13 and 14 respectively).After assigning an operation, the machine's schedule is updated accordingly, and the completion time becomes a constraint for scheduling the subsequent operation of the same job (lines 17 and 18).This process continues until all operations of all jobs are scheduled.end for 20: end for 4) Advances in AI techniques: Recent advances in Artificial Intelligence, particularly in Deep Learning and RL, have introduced promising paradigms for solving combinatorial optimization problems such as the FJSS problem [47,48].These techniques leverage historical data to learn scheduling patterns, enabling models to generalize across problem instances and adapt to dynamic environments.Unlike traditional methods that exhaustively explore the solution space, these approaches learn approximate policies that reduce computational complexity, accelerating decision-making, often with competitive or superior results.RL has emerged as a promising framework for job assignment and scheduling problems.</p>
<p>In this benchmark, we have selected two recent state-of-theart RL approaches applied to the well-established Brandimarte dataset [41].(1) Ho et al. [49] introduced a novel deep reinforcement learning method which improves efficiency by filtering out completed jobs and occupied machines, allowing the model to focus exclusively on the remaining decision space.The approach employs graph neural networks to represent the scheduling state and acts as a construction heuristic.</p>
<p>(2) Echeverria et al. [50] proposed a hybrid learning approach that integrates CP with RL.In this work, a deep model is trained on optimal solutions generated via CP, reducing the need for extensive exploration.Additionally, the CP model is embedded within the RL framework to refine intermediate solutions.</p>
<p>Given the strong empirical results reported by both methods on benchmark datasets, we include them as RL baselines in our comparative study.Since the original implementations were not publicly released, the performance values are directly extracted from the respective publications.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>This section presents a comprehensive analysis of the experimental results obtained in evaluating our proposed approach to the FJSSP.The analysis is divided into three key components: an ablation study, a benchmarking evaluation using a well-established dataset, and a robustness assessment involving non-linear setup times.In this sense, these components aim to assess the effectiveness and adaptability of our methodology under varying conditions.</p>
<p>Before presenting the main results, preliminary experiments were conducted to select the best-performing variant of our approach.While our methodology leverages the flexibility of LLMs to enhance multi-objective optimization, the benchmarking results reported in this section are limited to the top-performing configuration.This selection process is fully documented in B.</p>
<p>All experiments were conducted on a personal computing setup consisting of an 8-core Intel i7 processor, 16 GB of RAM, and no dedicated GPU.This choice of hardware emphasizes the practicality and scalability of the approach, showing that high-quality results can be achieved even with limited computational resources.Furthermore, the models were executed via the services provided through their respective APIs, underlining the accessibility and ease of integration of these tools into standard computing environments.</p>
<p>A. Ablation Study</p>
<p>This experimental technique, widely used in machine learning research, systematically removes specific components of an algorithm to evaluate their impact on overall performance.Comparing the proposed model with a variant that excludes the reflection mechanism can demonstrate the effectiveness in enhancing solution diversity and guiding search processes more efficiently.This experiment aims to support the novelty of our approach and highlight the relevance of incorporating self-reflective strategies into the framework.</p>
<p>In this phase, two experimental configurations were defined to examine the consistency and scalability of the reflection mechanism under different conditions.The first configuration was initialized with a population size of 30 individuals, and subsequent generations were limited to a population size of 10, improved over 10 iterations.In the second configuration, the initial population was increased to 60 individuals, with a generation size of 20 and 20 iterations.All experiments were evaluated following the criteria mentioned in the subsection IV-A.</p>
<p>The experiments were carried out by running the algorithm on the Brandimarte dataset.As previously mentioned, this experiment involves conflicting objectives: makespan and workload balance.Additionally, to test the capability of clustering reflection to handle challenging multi-objective problems, operation separation was introduced as a third objective.This objective measures the time distance between operations of the same job, a typical goal in the FJSSP to enhance scheduling efficiency.</p>
<p>The figures 2 present the HV and IGD values across generations for both algorithm variants in both experimental instances.Higher HV values indicate better convergence and diversity of solutions.In this sense, the evolution of HV in the figure 2a illustrates the improvement in solution diversity and convergence quality when using the reflection mechanism, as evidenced by consistently higher HV values in both instances.Conversely, lower IGD values in the figure 2b indicate better convergence toward the Pareto front.The figure on the right shows that the reflection-enhanced approach consistently has lower IGD, demonstrating improved convergence behavior.</p>
<p>The observed results provide empirical evidence demonstrating that the reflection mechanism's inclusion significantly improves both convergence (HV) and proximity to the Pareto front (IGD).This component maintains diversity and prevents premature convergence by generating new individuals based on clustering and introspection of the search space.These findings support our hypothesis that integrating LLM-powered reflection enhances the robustness and adaptability of multiobjective optimization algorithms.</p>
<p>B. Brandimarte Benchmark</p>
<p>In this section, we present the benchmarking results of our proposed methodology on the widely recognized Brandimarte dataset for the FJSSP.</p>
<p>Given the multi-objective nature of our study, we adopted a lexicographic optimization approach for the mathematical methods.This leads to developing two configurations per model, one prioritizing makespan (mb), and another prioritizing workload balance (bm).As a result, six baseline models were used for benchmarking: M ILP mb , M ILP bm , OR mb , OR bm , DC mb , and DC bm , where OR and DC refer to constraint programming solvers using OR-Tools and DoCplex Optimizer, respectively.Due to the computational complexity of the FJSSP, time limits were set on the mathematical solvers.</p>
<p>For the MILP model, we solved the problems using Gurobi Optimizer version 11.0.1 and allowed a maximum solving time of 30 minutes per objective, resulting in a total time of one hour per instance.In contrast, CP models are known for their efficiency in handling scheduling constraints, so a limit of 5 minutes was assigned per objective.Additionally, to provide a baseline for comparison, we executed a traditional greedy dispatching rule (DR) ten times for each instance, and from these runs, we selected the best-performing result for each objective.</p>
<p>Regarding the RL approaches, Ho et al. [49] (RL 1 ) and Echeverria et al. [50] (RL 2 ), these works only report results for the makespan minimization problem in instances from 1 to 10.Thus, both models are excluded from the multi-objective benchmarking analysis, although they are still referenced in the single-objective comparison.</p>
<p>Finally, following the experimentation performed in the LLM model selection (see Appendix B), 13 heuristics were obtained in the non-dominated front using Gemini 2.0 Flash in the REMoH framework.To perform the comparison of our approach, we selected the best result on the makespan-balance objective pair for each instance of Brandimarte among these 13 evolved heuristics.These results are reflected in the table II and III.</p>
<p>For the evaluation of makespan and workload balance optimization, we report both the achieved objective values and the best-known lower bounds (LBs) per instance, selecting the highest LB value reported across all MILP and CP configurations, as well as studies in literature [49][50][51].</p>
<p>The benchmarking results in Tables II and III demonstrate the performance of our proposed REMoH approach relative to both classical optimization models and state-of-the-art RL methods on the Brandimarte dataset.The GAP values on the tables show the mean GAP values of each model over all instances, where GAP stands for the relative percentual difference between the objective value and the lower-bound.</p>
<p>In this sense, REMoH achieves competitive performance in minimizing makespan (12.60% of GAP), obtaining results close to those of the RL-based models [49,50], while significantly outperforming the traditional dispatching rule baseline in all instances.It is important to note that RL approaches are designed to optimize only the makespan, while REMoH considers both makespan and workload balance during the search process.Additionally, RL models could not obtain results for instances larger than Mk10.</p>
<p>Results show that mathematical models achieve strong performance in their respective prioritized objectives due to the lexicographic optimization, for instance, DC mb obtained a 1.43% of mean GAP.In contrast, REMoH is designed to optimize both objectives, obtaining a more holistic view of solution quality.This difference explains why certain objective-specific methods may occasionally outperform RE-MoH in their dominant metric, e.g., M ILP mb in makespan or M ILP bm in workload balance.In this sense, heuristics obtained by REMoH provide consistently balanced trade-offs across both dimensions.</p>
<p>The mean GAP value is a representative performance metric, nevertheless, we aimed to analyze the benchmarking process further.Therefore, we measured the distance of each heuristic-generated solution to the best-known reference solutions, typically obtained from mathematical models.To this end, the objective values for each instance were individually normalized, and the Euclidean distance from each heuristicgenerated point to its corresponding reference point was calculated (see Figure 4).The average normalized distance was then used as a comparative metric across the different heuristics, including the LLM-generated methods described in Appendix B, and the greedy DR.</p>
<p>The model with the lowest average distance to the Pareto front was Gemini, followed by GPT, DeepSeek, and finally the greedy DR.These results suggest that heuristics generated by REMoH using Gemini are more consistent across unseen problem instances.Furthermore, GPT consistently outperformed   DeepSeek in generalization to new data, despite DeepSeek achieving better results at higher generations during the training phase (see Appendix B).</p>
<p>To further contextualize our results, we compared the average computational times for each method.Among the exact solvers, MILP averaged 1661.5s,CP with DoCplex 230.9s, and CP with OR-Tools 148.4s.These optimization times show the lexicographic optimization settings and the time limits imposed per objective.Notably, CP approaches consistently achieved lower optimality GAPs in significantly shorter runtimes compared to MILP.</p>
<p>As expected, the greedy DR was among the fastest due to its simplicity, with an average runtime of 0.002s.Interestingly, in the REMoH framework, GPT-generated heuristics ran even faster (0.001s), followed by DeepSeek (0.012s) and Gemini (0.167s).While Gemini showed the best solution quality, this came at the cost of increased computational complexity, reflected in longer execution times.</p>
<p>According to recent findings in the literature [38], CP-based methods have outperformed MILP in solving the FJSSP.This trend is reflected in our experiments, where solutions from CP often dominate those obtained using MILP for both objectives.However, MILP was occasionally more time-efficient when workload balance was prioritized over makespan, achieving comparable solutions in less time.</p>
<p>To illustrate these dynamics, Figure 3 presents a plot with both objectives for two representative instances, Mk06 and Mk10.These results reveal that for smaller instances, the greedy DR remains competitive and can even outperform some learned heuristics.However, as instance complexity increases, the REMoH-generated heuristics become more relevant.Conversely, MILP loses dominance as instance size grows, and CP approaches tend to produce superior solutions.</p>
<p>In conclusion, REMoH obtains stable performance across all instances, often matching or approaching the best-known lower bounds derived from exact methods.These findings validate  the effectiveness of our LLM-driven heuristic evolution framework as a robust alternative to classical and learning-based scheduling methods, but also capable of addressing complex multi-objective problems.</p>
<p>C. Flexibility Evaluation</p>
<p>This section aims to demonstrate the flexibility of the proposed solution.To this end, the baseline FJSS formulation is extended by incorporating an additional constraint that simulates a more complex and realistic scheduling environment.</p>
<p>Therefore, we introduce a well-established generalization named Sequence-Dependent Setup Times (SDST) [52].In practice, setup time refers to the duration required to configure a resource before it can execute a specific operation [53].In SDST scenarios, this time usually depends on both the incoming operation and the operation previously completed on the same machine.In this case, to enhance the realism of this extension, we embed a nonlinear behavior in the setup time by modeling it as a function of machine idle time.This reflects practical phenomena observed in manufacturing systems where prolonged machine inactivity can increase startup costs, for instance, due to temperature sensitivity.Specifically, we define a setup time function S(t) as follows:
S(t) =      0 if t &lt; 1 10 • (1 − e −0.5(t−1) ) if 1 ≤ t &lt; 10 20 if t ≥ 10
The integration of this nonlinear constraint poses substantial challenges for traditional optimization paradigms.MILP frameworks, for instance, cannot directly model nonlinear expressions without resorting to approximations that affect solution quality.Although constraint programming provides flexibility, incorporating these nonlinearities requires specialized propagators and search heuristics, which makes implementation more complex.</p>
<p>In this sense, more advanced techniques such as RL are also affected.Introducing a novel constraint requires creating a new training dataset, redefining the reward function, and fully retraining the policy network.This makes the approach computationally expensive and requires domain expertise to avoid convergence issues or suboptimal generalization.</p>
<p>By contrast, the methodology presented in this study is inherently capable of integrating nonlinear and context-sensitive constraints without requiring extensive model restructuring.This intrinsic flexibility underscores the robustness of our approach and highlights its potential for deployment in complex and dynamic production environments.To empirically validate this claim, we conducted an experiment incorporating the proposed nonlinear setup time function into our scheduling model.</p>
<p>To achieve this, minimal adjustments were made, slightly modifying the prompts given to the LLM by adding the constraint explanation and the method to calculate the delay.Finally, the feasibility checking function was adapted to include this new condition.</p>
<p>The initial population size was set to 30, with 10 individuals used for subsequent generations and iterations.The model was executed on the Brandimarte dataset, which, as previously discussed, is well-suited for evaluating heuristic performance across problems of varying sizes.To measure the performance, the aforementioned evaluation process was followed (see Section IV-A).</p>
<p>As shown in figures 5a and 5b, REMoH can adapt to dynamic constraints, which makes it highly flexible and applicable for real-world optimization problems, where complex constraints are often challenging to handle with traditional heuristics and RL.</p>
<p>VI. CONCLUSIONS</p>
<p>This research paper presents a novel framework named Reflective Evolution of Multi-objective Heuristics (REMoH) that incorporates LLMs into the heuristic design process.The proposed methodology aims to leverage the capacity of generalization, abstraction, and reasoning capabilities to evolve domain-agnostic, interpretable heuristics.In this sense, this approach facilitates efficient solution space exploration through adaptive evolution.The main innovative component of the framework is the introduction of a Reflection Mechanism, detailed in Section III, which enables long-term reflection in the evolutionary process.This mechanism performs clustering in the objective space to analyze population structure and uses introspective LLM reasoning to generate adapted heuristics that improve exploration and mitigate premature convergence.The effectiveness of this component is empirically validated in the ablation study (see Section V-A), demonstrating notable improvements in convergence, diversity, and proximity to the Pareto front.</p>
<p>The optimization capability of the approach was further evaluated in a benchmarking study conducted on the Brandimarte dataset.First, a model selection phase was performed to identify the most suitable LLM, then the bestperforming heuristics were obtained after the evolving process on Barnes and Dauzere instances.These heuristics were compared against classical mathematical programming approaches (MILP, CP), a baseline heuristic based on dispatching rules, and state-of-the-art RL methods.The results indicate that our proposed framework can generate heuristics that achieve competitive or superior performance in terms of makespan, outperforming RL-based approaches in some Brandimarte instances (see Table II).Additionally, the algorithm proposes solutions with significantly lower computational overhead.While mathematical programming-based models obtained optimal solutions for small instances, they require significant runtime costs, making them less practical for real-time or large-scale scenarios.Unlike exact methods, which often suffer from scalability issues and require substantial expertise in both problem modeling and optimization techniques, the proposed approach relies solely on prompt engineering.This significantly lowers the entry barrier, as it does not require prior knowledge of the underlying problem or the employed technique.</p>
<p>Finally, a generalization of the FJSSP was formulated to assess the flexibility and robustness.This variant incorporates non-linear SDST as a nonlinear function related to machine idle time.Traditional mathematical models typically struggle with integrating such constraints without extensive reformulation.In contrast, this methodology can adapt to this problem variant, successfully evolving heuristics capable of solving the generalized FJSSP efficiently (see Section V-C).This demonstrates the potential for handling complex, dynamic, and constraint-rich optimization environments.</p>
<p>In conclusion, this work highlights the potential of LLMs in advancing multi-objective optimization, particularly in enabling adaptive and interpretable heuristic design.The proposed REMoH framework offers a flexible alternative to conventional methods while achieving competitive performance with less modeling effort.Future research may explore the evolution of full metaheuristic architectures, extending to highlevel algorithm generation.COPYRIGHT © 2025 IEEE.Personal use of this material is permitted.Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p>
<p>APPENDIX A PSEUDOCODE OF THE PROPOSED MULTI-OBJECTIVE EOH ALGORITHM</p>
<p>This appendix provides the detailed pseudocode of the proposed Multi EoH algorithm.This hybrid framework integrates NSGA-based multi-objective evolutionary optimization with LLMs for heuristic generation.The pseudocode 3 encapsulates the entire optimization pipeline.This detailed algorithmic structure provides a transparent foundation for reproducibility and further research.</p>
<p>For each iteration t ∈ 1, . . ., T , the algorithm evolves a population of heuristics through LLM-guided generation, reflection, and selection mechanisms.The process begins with an initial population P 0 , either provided or generated from scratch using the LLM L.</p>
<p>At each generation step, a subset of individuals is selected to form the parent population P parents , which is then clustered into subgroups.These clusters serve as the basis for generating short reflections via Reflection(C, L), summarizing the strategic patterns observed.A more comprehensive reflection R t is subsequently constructed by combining the short reflections with those from previous iterations.</p>
<p>Two types of offspring are generated: Crossover-based heuristics, created by combining two selected parents (p 1 , p 2 ) guided by R t and the LLM, account for r c • P new individuals.</p>
<p>Mutation-based heuristics, derived from a single elite parent p best using R t and the LLM, account for r m • P individuals.</p>
<p>The new population P t is formed by merging the parent and offspring populations.All generated individuals across iterations are collected into a global archive P * .</p>
<p>At the end of the evolutionary process, the global Pareto front F is extracted from P * , representing the most effective heuristics found in terms of multiple objectives.This study aimed to identify the LLM that consistently produces high-quality heuristic operators and reflections, thereby maximizing the performance of the proposed multi-objective scheduling methodology.The tested LLMs include GPT-4o [54], Gemini 2.0 Flash [55], and DeepSeek-V3 [56].These models were selected due to their competitive performance and accessibility in practical optimization workflows.</p>
<p>The comparison was performed using training on two independent datasets from the FJSSP benchmark literature, Dauzere and Barnes, and validating in the Brandimarte dataset.The experiments focused on determining which LLM most effectively supports the generation of high-quality Pareto fronts across these diverse conditions.The evaluation and selection criteria for this experimentation is explained in the subsection IV-A.In this sense, the evaluation is performed in the same environment, with identical configurations across multiple runs.</p>
<p>The optimization process was initialized with a population size of 60 individuals.Then, throughout the evolutionary process, a smaller working population of 20 individuals was maintained for each generation.Each run consisted of 20 iterations of the algorithm.During the experiments, heuristics were evaluated on a maximum time of 100 seconds.Regarding the evolutionary operators, crossover and mutation rates were fixed at 0.9 and 0.1, respectively.As shown in Figures 6 and 7, all models exhibit a significant improvement during the initial generations.After this early boost, DeepSeek-V3 and Gemini 2.0 Flash follow a similar progression, gradually improving their performance over successive generations.In contrast, GPT-4o achieves strong results in the early stages but converges prematurely, making it increasingly difficult to improve in later iterations.As a result, although the final differences are relatively small, GPT-4o ends up with the lowest performance among the three models.The Pareto front heuristics generated by each model have been evaluated on the Brandimarte datasets, with results for all instances presented in Tables IV, V and VI.Based on this evaluation, Gemini 2.0 Flash was selected as the reference model for comparison against established benchmarking methods, due to its superior performance.The results of this comparison are presented in Section V-B.</p>
<p>APPENDIX C HEURISTICS GENERATED</p>
<p>In this section, we provide an overview of the main groups of heuristics generated by Gemini 2.0 Flash, the LLM that achieved the best performance in the models comparison presented in B. The goal is to better understand the types of heuristics favored by the model when optimizing the FJSSP across different instances.</p>
<p>The final Pareto front from REMoH includes 13 diverse heuristics, each implementing a unique scheduling strategy or parameter configuration.These heuristics were categorized into three main groups based on their core characteristics.These categories are not mutually exclusive since some heuristics integrate elements from multiple groups.</p>
<p>1) Basic Priority Heuristics with Weight Adjustment: These heuristics are based on a weighted priority function to select the next operation to schedule.The key is the dynamic adjustment of the weights of different factors (makespan, load, urgency) during the scheduling process.2) Priority Heuristics with SPT and Load Balancing:</p>
<p>These heuristics combine the Shortest Processing Time (SPT) rule to balance the machine load.They use a cost function considering the start time, processing time, and workload.Some of these heuristics use an iterative approach to improve the solution.3) Advanced Heuristics with Enhanced Features: These extend basic strategies by integrating advanced mechanisms like lookahead that considers the impact of the decision on future operations, bottleneck prioritization, dynamic scaling of priority function, and non-linear trade-off functions (e.g., sigmoid) to balance objectives.</p>
<p>APPENDIX D PROMPTS EXAMPLES</p>
<p>The prompts used in the REMoH algorithm are crucial for understanding and potentially improving the performance of the LLM.In this case, they were mainly adapted from HSEvo, with modifications to tailor the model's behavior to our specific needs.</p>
<p>In this section, we will explain and analyze the main parts of the prompts used.</p>
<p>A. General prompts</p>
<p>General prompts refer to instructions reused in different steps of our algorithm.Firstly, the generator prompt serves as a concise instruction delivered to the LLM during each stage of heuristic generation (Prompt 1).Prompt 1: Generator Prompt {role instruction} helping to design heuristics that can effectively solve optimization problems.Your response outputs Python code and nothing else.Format your code as a Python code string: "'python ... "'.</p>
<p>As outlined in the methodology III, a dynamic prompt role instruction is employed to foster diversity, incorporating multiple roles specifically crafted for the LLM to enhance heuristic development.As shown in Prompt 2, it consists of the name of a famous scientist along with a brief description of their main contributions.Prompt 2: Different role instruction prompts You are {scientific}, {description of their main contributions} Additionally, the task description is a key instruction provided to the LLM, as it outlines the details of the optimization problem and the heuristic function to be generated, including the expected input and output formats (Prompt 3).The Flexible Job Shop Scheduling Problem (FJSSP) is an optimization problem where jobs, consisting of multiple operations, must be scheduled on machines with specific processing times.The goal is to minimize makespan, reduce idle time between operations in the same job, and balance workload.Ensure that the following constraints are strictly followed: -Operation feasibility: Each operation must be performed on the corresponding machine(s), and the processing time must align with the available machines.-Machine feasibility: Machines can perform only one operation at a time, ensuring no overlap in their schedules.-Sequence feasibility: Within the same job, operations must be executed in their defined numerical order.A subsequent operation must only start once the previous one has completed, ensuring proper sequencing and avoiding any premature execution of operations.</p>
<p>B. Population initialization prompt</p>
<p>The initialization prompt assists the model in generating the first heuristics.To achieve this, the instruction incorporates the generator prompt as shown in Prompt 1.Additionally, it provides the task description (Prompt 3), and a seed function is used to enable the model to leverage existing knowledge.It can be read in Prompt 6.</p>
<p>Prompt 6: Population initialization prompt {generator prompt} {task description} {seed function} Refer to the format of a trivial design above.Be very creative and give a completely different heuristic that improves the one given, respeting constraints.Output code only and enclose your code with Python code block: ´´´python ... ´´´, has comment and docstring (&lt;50 words) to description key idea of heuristics design.Let's think step by step.</p>
<p>C. Crossover prompt</p>
<p>To improve the generation of new heuristics based on parents and reflection, a crossover prompt is employed.As shown in Prompt 7, it includes the generator prompt and task previously explained.Additionally, the code for both parent1 and parent2 is provided, along with the long reflection.</p>
<p>D. Elitist mutation prompt</p>
<p>The elitist mutation prompt aids the LLM in generating new heuristics based on the best existing one.It consists of the generator prompt and task description, with the addition of the code for the best heuristic parent and the long reflection.It is illustrated in Prompt 8.</p>
<p>E. Reflections prompts</p>
<p>The reflection prompts are crucial for the LLM to understand what to extract from each cluster and how to combine the short reflections to generate a useful analysis for the creation of new heuristics.</p>
<p>The short reflection (Prompt 9) is provided with the cluster centroid performance on each objective function, stored in cluster performance.Furthermore, all heuristics code are added in heuristics Prompt 9: Short reflection prompt You are an expert in heuristics for multiobjective problem optimization.You will be given a cluster of heuristics.It contains heuristics that have similar performance across different objective functions.Your task is to analyze the heuristics and summarize their characteristics and trends, highlighting the key features with no more than 50 words.Avoid redundant information.The general cluster performance is {cluster performance} Scores are scaled using a Standard Scaler.Positive values indicate performance above the mean (better), while negative values indicate performance below the mean (worse).Let's think step by step.</p>
<h3 heuristics="heuristics">Heuristics</h3>
<p>The long reflection, shown in Prompt 10, includes the previous long reflection (long reflection) as well as the short reflections extracted with Prompt 9, each accompanied by the performance of the corresponding cluster centroid in cluster reflections.Synthesize a general reflection that identifies overarching trends, highlights major strengths and weaknesses and connects insights from both the clusters and the long-term perspective.Your reflection should guide LLMs in designing more effective heuristics that optimize all objective functions.Limit your response to 150 words.Let's think step by step.</p>
<p>Fig. 1 .
1
Fig. 1.Detailed diagram of our proposed Multi-Objective Evolution of Heuristics-based optimization methodology.</p>
<p>Algorithm 1 1 : 2 : 5 : 12 : 14 :
11251214
Constraint Programming Model for FJSSP with Lexicographic Objectives Require: Set of jobs J, operations O, machines M , processing times p jim Ensure: the Schedule minimizes makespan and workload balance Define interval variables iv jim for all o ji ∈ O and m ∈ M ji Define optional intervals with durations p jim 3: Define decision variables start ji , end ji for each operation o ji 4: for all o ji ∈ O do Add ALTERNATIVE(o ji , {iv jim | m ∈ M ji }) 6: end for 7: for all j ∈ J do 8:for i = 1 to |J j | − 1 do for all m ∈ M do 13: Add NOOVERLAP constraint on iv jim scheduled on machine m Define load m ← ji PRESENCEOF(iv jim ) • p jim 15: end for 16: Define makespan C max = max ji ENDOF(o ji ) 17: Define max machine productive time L max = max m∈M load m</p>
<p>Algorithm 2 dispatching rule 1 : 9 :
219
Baseline Input Set of jobs J, operations O j , machines M o , processing times p om 2: Output Sol 3: J p ← prioritization(J) 4: Sol ← ∅ 5: for all O j ∈ J p do 6: for all m ∈ M o do 10: t ← earliesF easibletT ime(m, p om , tc, Sol) Sol ← Sol ∪ (bt, bm, bd) 18: tc ← bt + bd 19:</p>
<p>Fig. 2 .
2
Fig. 2. Comparison between the reflective and non-reflective approaches using different population sizes and iterations.</p>
<p>( a )
a
Mk6 instance: 10 jobs, 10 machines.(b) Mk10 instance: 20 jobs, 15 machines.</p>
<p>Fig. 3 .
3
Fig. 3. Illustrative results on Brandimarte instances.The table compares outcomes from MILP, DoCplex, OR-Tools, and heuristics derived from our LLMintegrated model, which incorporates DeepSeek (DS), Gemini 2.0 Flash (GEM), and GPT-4o.(GPT)</p>
<p>Fig. 4 .
4
Fig. 4. Boxplot of the mean Euclidean distances to best-known solutions for different</p>
<p>Fig. 5 .
5
Fig. 5. Heuristic evolution performance validation with non-linear Sequence-Dependent Setup Times (SDST).</p>
<p>Algorithm 3
3
Proposed Multi-Evolution of Heuristics 1: Input: Initial population size I, Population size P, Iterations T , Initial population P ′ , Pre-trained LLM L, Crossover rate r c , Mutation rate r m 2: Output: Heuristics in global Pareto front F 3: if P 0 = ∅ then 4: for i = 1, ..., I do end if 9: P * ← P 0 10: for t = 1, ..., T do 11: P parents ← Selection(P t−1 ) 12: C ← Clusterize(P parents ) 13: R short ← Ref lection(C, L) 14: R t ← Ref lection(C, R short , R t−1 , L) 15: P sons ← ∅ 16: for j = 1, ..., r c * P do 17: p 1 , p 2 ← P arentSelection(P parents ) 18: o ← Crossover(p 1 , p 2 , R t , L) 19: P sons ← P sons ∪ o for j = 1, ..., r m * P do 22: p best ← bestHeuristic(P parents ) 23: o ← M utation(p best , R t , L) 24: P sons ← P sons ∪ o 25: end for 26: P t ← P parents ∪ P sons 27: P * ← P * ∪ P sons 28: end for 29: F ← GlobalP aretoF ront(P * ) APPENDIX B LLM SELECTION This appendix provides a comprehensive overview of the comparative evaluation conducted to select the most effective LLM in our Reflective Evolution of Multi-objective Heuristics (REMoH) framework.</p>
<p>Fig. 6 .
6
Fig. 6.Comparison between different LLMs in Hypervolume (HV).</p>
<p>Prompt 5 :
5
Function description prompt for FJSSP The input of the function 'heuristic' is a dictionary with the following keys: -n jobs: Total number of jobs.-n machines: Total number of machines.-jobs: A dictionary where: -Each key is a job number.-Each value in the list represents a set of operations for a job.The index of each operation within the list indicates the order in which they must be executed in the scheduling.-Each operation is a tuple containing: -A list of machines involved.(start from 0) -A list of corresponding processing times.The output is a dictionary where: -Each <strong>key</strong> is a <strong>job number</strong>.-The value is a list of dictionaries containing each operation as: -Operation: Operation number.-Assigned Machine: Assigned machine number.-Start Time: Start time.-End Time: End time.-Processing Time: Processing time.</p>
<p>Analyze &amp; experience -{long reflection} Your task is to write an improved function named heuristic by COMBINING elements of two above heuristics base Analyze &amp; experience.Output the code within a Python code block: "'python ... "', has comment and docstring (&lt; 50 words) to description key idea of heuristics design.Let's think step by step.</p>
<p>Prompt 8 :
8
Elitist mutation prompt {generator prompt} {task description} Current heuristics: {parent} Now, think outside the box and write a mutated function better than current version.You can using some hints if need: {long reflection} Output code only and enclose your code with Python code block: ´´´python ... ´´´, has comment and docstring (&lt; 50 words) to description key idea of heuristics design.Let's think step by step.</p>
<p>Prompt 10 :
10
Long reflection prompt You are an expert in the domain of optimization heuristics.Your task is to provide useful advice based on analysis to design better heuristics.Answer just what is asked and avoid redundant information.Here are previous long-term reflections on heuristic performance: ## Previous Long-Term Reflection: {long reflection} Now, analyze the following cluster reflections: ## Clusters Reflections: {cluster reflections} Scores are scaled using a Standard Scaler.Positive values indicate performance above the mean (better), while negative values indicate performance below the mean (worse).</p>
<p>TABLE II BENCHMARKING
II
OF MAKESPAN OPTIMIZATION ON BRANDIMARTE INSTANCES, COMPARING MATHEMATICAL MODELS, HO ET AL. [49] (RL 1 ), ECHEVERRIA ET AL.[50] (RL 2 ), AND THREE HEURISTICS OBTAINED BY REMOH
M ILP mbDC mbOR mbDRRL 1RL 2REMoHLBMk014040406442404140Mk022626264129293226Mk03204204204329204204204204Mk0461606011767647260Mk05180173172240177176182172Mk066659589171737357Mk07151139139192149156161139Mk08523523523661523523523523Mk09311307307438314307323307Mk10255202211388218236225189Mk11629612609757--658609Mk12508508508674--534508Mk13433406403586--471369Mk147026946941072--707694Mk15360333344552--422333GAP6.461.431.7354.788.058.5712.60</p>
<p>TABLE IV RESULTS
IV
[56]HE PARETO FRONT HEURISTICS GENERATED BY REMOH FRAMEWORK ON THE BRANDIMARTE INSTANCES, VIA DEEPSEEK-V3[56].
mk01mk02mk03mk04mk05mk06mk07mk08mk09mk10mk11mk12mk13mk14mk15Makespan5241236831941032645754113087356125778084801Balance43372166719478234523312273684563577781397Time (s.)0.0010 0.0020 0.0050 0.0020 0.0040 0.0048 0.00410.0071 0.00830.00910.0102 0.0099 0.01240.0135 0.0142Makespan59482511072081142926054213527736506729964802Balance49402168319684255523327267730565652801445Time (s.)0.0015 0.0020 0.0057 0.0039 0.0050 0.0040 0.00400.0075 0.00880.01150.0080 0.0095 0.01360.0123 0.0189Makespan454023675223892006294233076776135298284803Balance43332046617864191533307234644524499746354Time (s.)0.0031 0.0026 0.0155 0.0061 0.0091 0.0115 0.00850.0344 0.03740.04680.0338 0.0411 0.05210.0855 0.0836Makespan4840227731891042155233372846635374896944684Balance43392136818981215523322272663537489694468Time (s.)0.0010 0.0020 0.0057 0.0030 0.0040 0.0056 0.00480.0099 0.01090.01580.0117 0.0130 0.02380.0150 0.0230Makespan483823069192922005233272866815504876944285Balance42372276819279200523307263681550487694428Time (s.)0.0009 0.0020 0.0050 0.0020 0.0055 0.0050 0.00450.0092 0.00990.01360.0109 0.0141 0.02160.0195 0.0223Makespan483824169192962005233222876815504876944286Balance42372046819289200523307258681550487694428Time (s.)0.0021 0.0025 0.0060 0.0039 0.0040 0.0051 0.00520.0091 0.01320.01510.0122 0.0139 0.02070.0221 0.0246Makespan4840227731891042155233372846635374896944687Balance43392136818981215523322272663537489694468Time (s.)0.0020 0.0017 0.0085 0.0031 0.0040 0.0051 0.00570.0101 0.01150.01550.0137 0.0115 0.02480.0197 0.0252
Fig. 7. Comparison between different LLMs in Inverted Generational Distance (IGD).</p>
<p>TABLE V RESULTS
V
[55]HE PARETO FRONT HEURISTICS GENERATED BY REMOH FRAMEWORK ON THE BRANDIMARTE INSTANCES, VIA GEMINI 2.0 FLASH[55].
mk01mk02mk03mk04mk05mk06mk07mk08mk09mk10mk11mk12mk13mk14mk15Makespan5339229771871202065323743076835485347694361Balance40382166718187206523299255683548534734398Time (s.)0.0012 0.0030 0.0106 0.0040 0.0040 0.0076 0.00700.0110 0.01640.03670.0154 0.0147 0.03040.0310 0.0424Makespan413222174186761615233232416835344717074222Balance37312207418164161523307241683534470707422Time (s.)0.0102 0.0231 0.0785 0.0382 0.0417 0.1411 0.06900.1449 0.23770.45560.1982 0.1937 0.55200.4217 0.6594Makespan463320472182731645273512516585585067524403Balance42332046718068164523327237658535506734403Time (s.)0.0010 0.0026 0.0030 0.0020 0.0029 0.0051 0.00200.0030 0.00500.00600.0046 0.0030 0.00490.0051 0.0075</p>
<p>In the task prompt, problem description and function description depend on the optimization problem to resolve.Prompts used for FJSSP are illustrated in Prompt 4 and 5, respectively.
Makespan50402268918995212531369277669563533753449Balance42382167418379212523299263669559533734403Time (s.)0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.00100.00100.0010 0.00090.00000.0010 0.0010 0.0021Prompt 3: Task description prompt{role instruction} Your task is to writea function called 'heuristic' for {problemdescription}{function description}Prompt 4: Problem description prompt for FJSSP
His research interests include language modelling for under-resourced languages, multimodal learning, and grounded representations.He has published and reviewed extensively in high-impact international conferences and journals.
When large language model meets optimization. S Huang, K Yang, S Qi, R Wang, Swarm and Evolutionary Computation. 901016632024</p>
<p>Particle swarm optimization algorithm and its applications: a systematic review. A G Gad, Archives of computational methods in engineering. 202229</p>
<p>A review on representative swarm intelligence algorithms for solving optimization problems: Applications and trends. J Tang, G Liu, Q Pan, IEEE/CAA Journal of Automatica Sinica. 8102021</p>
<p>A blockchain-based llm-driven energy-efficient scheduling system towards distributed multi-agent manufacturing scenario of new energy vehicles within the circular economy. C Liu, Q Nie, Computers &amp; Industrial Engineering. 2011108892025</p>
<p>An llm-based approach for enabling seamless human-robot collaboration in assembly. C Gkournelos, C Konstantinou, S Makris, CIRP Annals. 7312024</p>
<p>Evolutionary computation in the era of large language model: Survey and roadmap. X Wu, S Wu, J Wu, L Feng, K C Tan, 2024</p>
<p>Leveraging large language models for the generation of novel metaheuristic optimization algorithms. M Pluhacek, A Kazikova, T Kadavy, A Viktorin, R Senkerik, 10.1145/3583133.3596401Proceedings of the Companion Conference on Genetic and Evolutionary Computation, ser. GECCO '23 Companion. the Companion Conference on Genetic and Evolutionary Computation, ser. GECCO '23 CompanionNew York, NY, USAAssociation for Computing Machinery2023</p>
<p>Llmoa: A novel large language model assisted hyperheuristic optimization algorithm. R Zhong, A G Hussien, J Yu, M Munetomo, Advanced Engineering Informatics. 641030422025</p>
<p>Evolution of heuristics: Towards efficient automatic algorithm design using large language model. F Liu, X Tong, M Yuan, X Lin, F Luo, Z Wang, Z Lu, Q Zhang, 2024</p>
<p>Integrating reinforcement learning and large language models for crop production process management optimization and control through a new knowledge-based deep learning paradigm. D Chen, Y Huang, Computers and Electronics in Agriculture. 2321100282025</p>
<p>The new agronomists: Language models are experts in crop management. J Wu, Z Lai, S Chen, R Tao, P Zhao, N Hovakimyan, 2024</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>Using large language models for hyperparameter optimization. M R Zhang, N Desai, J Bae, J Lorraine, J Ba, 2024</p>
<p>Large language model agent for hyper-parameter optimization. S Liu, C Gao, Y Li, 2025</p>
<p>Understanding llms: A comprehensive overview from training to inference. Y Liu, H He, T Han, X Zhang, M Liu, J Tian, Y Zhang, J Wang, X Gao, T Zhong, Y Pan, S Xu, Z Wu, Z Liu, X Zhang, S Zhang, X Hu, T Zhang, N Qiang, T Liu, B Ge, Neurocomputing. 6201291902025</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. C Fernando, D Banarse, H Michalewski, S Osindero, T Rocktäschel, 2023</p>
<p>Large language models as optimizers. C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, 2024</p>
<p>Optimus: Optimization modeling using mip solvers and large language models. A Ahmaditeshnizi, W Gao, M Udell, 2023</p>
<p>Use your instinct: Instruction optimization for llms using neural bandits coupled with transformers. X Lin, Z Wu, Z Dai, W Hu, Y Shu, S.-K Ng, P Jaillet, B K H Low, 2024</p>
<p>Leveraging large language model to generate a novel metaheuristic algorithm with CRISPE framework. R Zhong, Y Xu, C Zhang, J Yu, Cluster Comput. 2710Dec. 2024</p>
<p>Evolutionary computation in the era of large language model: Survey and roadmap. X Wu, S.-H Wu, J Wu, L Feng, K C Tan, IEEE Transactions on Evolutionary Computation. 2922025</p>
<p>Algorithm evolution using large language model. F Liu, X Tong, M Yuan, Q Zhang, 2023</p>
<p>Llamea: A large language model evolutionary algorithm for automatically generating metaheuristics. N Van Stein, T Bäck, IEEE Transactions on Evolutionary Computation. 122024</p>
<p>Evolving code with a large language model. E Hemberg, S Moskal, U.-M O'reilly, 2024</p>
<p>Cocoevo: Co-evolution of programs and test cases to enhance code generation. K Li, H Yu, T Guo, S Cao, Y Yuan, 2025</p>
<p>Large language model-based evolutionary optimizer: Reasoning with elitism. S Brahmachary, S M Joshi, A Panda, K Koneripalli, A K Sagotra, H Patel, A Sharma, A D Jagtap, K Kalyanaraman, 2024</p>
<p>Large language models as evolutionary optimizers. S Liu, C Chen, X Qu, K Tang, Y.-S Ong, 2024</p>
<p>Quality-diversity through ai feedback. H Bradley, A Dai, H Teufel, J Zhang, K Oostermeijer, M Bellagente, J Clune, K Stanley, G Schott, J Lehman, 2023</p>
<p>Language model crossover: Variation through few-shot prompting. E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, 2024</p>
<p>The OpenELM Library: Leveraging Progress in Language Models for Novel Evolutionary Algorithms. H Bradley, H Fan, T Galanos, R Zhou, D Scott, J Lehman, 10.1007/978-981-99-8413-8_102024Springer NatureSingapore; Singapore</p>
<p>Large language model for multi-objective evolutionary optimization. F Liu, X Lin, Z Wang, S Yao, X Tong, M Yuan, Q Zhang, 2024</p>
<p>Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem. J Huang, X Li, L Gao, Q Liu, Y Teng, 2024</p>
<p>Ts-eoh: An edge server task scheduling algorithm based on evolution of heuristic. W Yatong, P Yuchen, Z Yuqi, 2024</p>
<p>Reevo: Large language models as hyper-heuristics with reflective evolution. H Ye, J Wang, Z Cao, F Berto, C Hua, H Kim, J Park, G Song, 2024</p>
<p>Hsevo: Elevating automatic heuristic design with diversity-driven harmony search and genetic algorithm using llms. P V T Dat, L Doan, H T T Binh, 2024</p>
<p>Clustering Bank Customer Complaints on Social Media for Analytical CRM via Multiobjective Particle Swarm Optimization. R Gavval, V Ravi, 10.1007/978-3-030-33820-6_92020Springer International PublishingCham</p>
<p>A survey of job shop scheduling problem: The types and models. H Xiong, S Shi, D Ren, J Hu, Computers &amp; Operations Research. 1421057312022</p>
<p>The flexible job shop scheduling problem: A review. S Dauzère-Pérès, J Ding, L Shen, K Tamssaouet, European Journal of Operational Research. 31422024</p>
<p>Review on flexible job shop scheduling. J Xie, L Gao, K Peng, X Li, H Li, IET collaborative intelligent manufacturing. 132019</p>
<p>Solving the job-shop scheduling problem in the industry 4.0 era. M E Leusin, E M Frazzon, M Maldonado, M Kück, M Freitag, 20186107Technologies</p>
<p>Routing and scheduling in a flexible job shop by tabu search. P Brandimarte, 10.1007/BF02023073Annals of Operations Research. 413Sep. 1993</p>
<p>Solving the job shop scheduling problem with tabu search. J W Barnes, J B Chambers, IIE Transactions. 2721995</p>
<p>Ibm ilog cp optimizer for detailed scheduling illustrated on three problems. P Laborie, Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems: 6th International Conference. Pittsburgh, PA, USASpringer2009. May 27-31. 2009. 20096</p>
<p>Industrial size job shop scheduling tackled by present day cp solvers. G Da Col, E C Teppan, Principles and Practice of Constraint Programming: 25th International Conference. Stamford, CT, USASpringer2019. September 30-October 4, 2019. 201925CP</p>
<p>Cp-sat. L Perron, F Didier, 2024Google</p>
<p>V12. 1: User's manual for cplex. I I Cplex, International Business Machines Corporation. 46531572009</p>
<p>A multi-action deep reinforcement learning framework for flexible job-shop scheduling problem. K Lei, P Guo, W Zhao, Y Wang, L Qian, X Meng, L Tang, Expert Systems with Applications. 2051177962022</p>
<p>Flexible jobshop scheduling via graph neural network and deep reinforcement learning. W Song, X Chen, Q Li, Z Cao, IEEE Transactions on Industrial Informatics. 1922022</p>
<p>Residual scheduling: A new reinforcement learning approach to solving job shop scheduling problem. K.-H Ho, J.-Y Cheng, J.-H Wu, F Chiang, Y.-C Chen, Y.-Y Wu, I.-C Wu, IEEE Access. 122024</p>
<p>Leveraging constraint programming in a deep learning approach for dynamically solving the flexible job-shop scheduling problem. I Echeverria, M Murua, R Santana, Expert Systems with Applications. 2651258952025</p>
<p>A constraint programming formulation of the multi-mode resource-constrained project scheduling problem for the flexible job shop scheduling problem. F Yuraszeck, E Montero, D Canut-De-Bon, N Cuneo, M Rojel, IEEE Access. 112023</p>
<p>Sequence-dependent setup time flexible job shop scheduling problem to minimise total tardiness. M Mousakhani, International journal of production research. 51122013</p>
<p>A simheuristic approach for the flexible job shop scheduling problem with stochastic processing times. R H Caldeira, A Gnanavelbabu, Simulation. 9732021</p>
<p>Openai, Gpt-4 technical report. 2023. April 2025via ChatGPT (GPT-4o)</p>
<p>Gemini 2: Our most capable model yet. G Deepmind, 2023</p>
<p>Deepseek-v3: Scaling mixture-of-experts with multi-head latent attention. D Dai, D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, arXiv:2412.194372024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
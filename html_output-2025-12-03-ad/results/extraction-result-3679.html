<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3679 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3679</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3679</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268297277</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.04790v1.pdf" target="_blank">Online Training of Large Language Models: Learn while chatting</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models(LLMs) have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI agents or online/offline knowledge bases.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3679.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3679.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online Training using External Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A user-facing interaction paradigm and method that performs parameter-variant online fine-tuning of LLMs triggered by conversational user instructions, using three external interaction modes (Instruction-Guided, Document-Driven, Web Search-Enabled) to generate training data and immediately replace the served model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Online Training using External Interactions (OT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An online parameter-variant paradigm that accepts natural-language '[Learn]' style user instructions (or document uploads or web search instructions), uses LLM-based data augmentation techniques (self-instruct, instruction backtranslation, web-search augmentation) to create synthetic training examples, filters for quality/moderation, and fine-tunes the served model (e.g. via LoRA) so the updated model immediately replaces the old one and persists knowledge across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-provided conversational instructions (natural language '[Learn]' directives), uploaded documents (PDF/text/links), and web search results; in the reported experiments OT used between 100 and 2,000 model-generated training examples for the tool-learning case study, while a Full-SFT baseline used 6,000 labeled in-domain examples.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language chat directives prefixed by a trigger token such as '[Learn]' (e.g. "[Learn] I wish you could fetch more news on environmental pollution"), or user-uploaded document selection, or explicit web-search instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-based synthetic data generation using self-instruct and instruction backtranslation and web-search augmentation to produce training examples; filtering for toxic/biased content; then parameter fine-tuning (online SFT/LoRA) on the generated data to inject knowledge into model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>A fine-tuned LLM (updated model parameters / LoRA patch) that persists learned knowledge across sessions; intermediate outputs are synthetic training examples (instruction-response pairs, tool-invocation formatted examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Case study evaluation (tool-learning): compared OT to a Prompt (few-shot) baseline and a Full-SFT baseline. Evaluation measured correctness of tool invocation (correct tool selection + correct tool input parameters) on a held-out test set of 300 examples; training/inference time was reported (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>In the tool-learning case study, a single round of model-generated training data used by OT increased tool-invocation accuracy from nearly 30% (vanilla) to about 50%; OT trained with 100–2000 generated examples achieved substantial improvement over prompt-based few-shot, while Full-SFT used 6,000 labeled in-domain examples. OT also reduced training cost compared to Full-SFT and showed favorable inference efficiency versus retrieval/ICL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported challenges include difficulty of reliably injecting factual knowledge without overfitting, data-distribution misalignment between generated training data and test distributions (explaining similar performance between 100 and 2000 generated points), content moderation risks from web-derived data, potential bias/toxicity requiring filtering, and open issues around concurrency and long-term knowledge management.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Directly compared to Prompt (few-shot Llama2-7b-chat with few-shot prompts) and Full-SFT (full-parameter supervised fine-tuning on 6k labeled examples). OT outperformed the prompt baseline substantially (from ~30% to ~50% accuracy) while using far fewer generated examples and lower labeling cost than Full-SFT; Full-SFT used 6k labeled examples (comparison of absolute performance vs Full-SFT not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Training of Large Language Models: Learn while chatting', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3679.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3679.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-generation technique in which an LLM generates instruction–response training pairs from seed instructions or prompts to create instruction-tuning data without human labeling; cited and used as part of OT's data generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-instruct: Aligning language model with self generated instructions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Self-Instruct synthetic-data generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Used to transmute user conversational directives and other sources into trainable examples (instruction–response pairs) by prompting LLM(s) to generate supervision data (self-generated instructions and answers) that are then filtered and used to fine-tune the model online.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Inputs are user instructions, uploaded documents, or web-extracted passages provided at runtime; the paper does not specify corpus size for self-instruct generation beyond the experimental counts (100–2000 generated examples used in OT experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language user instructions (chat messages), or document contents/web extractions that the user requests the model to learn from; these serve as seeds for self-instruct generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting the LLM to produce synthetic instruction–response pairs (self-instruction generation), optionally combined with instruction backtranslation and web search augmentation; generated data is filtered for quality and toxicity and used for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthetic training examples (instruction + target response), formatted for supervised fine-tuning; downstream output is a fine-tuned model that reflects the generated supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Applied within OT and evaluated in the tool-learning case study: the synthetic training data produced by self-instruct was used to fine-tune models and performance was measured by tool-invocation accuracy on a held-out test set; data quality filtering / moderation applied prior to training.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When used within OT, model-generated training data (via self-instruct among other augmentations) enabled a fine-tuned model to improve tool-invocation accuracy from roughly 30% to ~50% using only 100–2000 generated examples, demonstrating effective low-cost improvement versus few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality and distribution mismatch of synthetic examples can limit performance gains; generated data can contain biased or toxic content and thus requires filtering; effectiveness depends on initial model capability and on data diversity to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Self-instruct generated data (used in OT) was compared indirectly to human-annotated Full-SFT data (6k labeled examples); OT with self-instruct uses far fewer generated examples while achieving large gains over prompt-only baselines, though Full-SFT remains the stronger standard for high-quality labeled data (paper emphasizes lower cost of OT vs Full-SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Training of Large Language Models: Learn while chatting', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3679.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3679.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-Backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation technique where instructions and responses are transformed (backtranslated) to produce diverse synthetic training pairs; cited in the paper and applied in OT's Document-Driven Learning to curate training data from documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-alignment with instruction backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Instruction Backtranslation (data augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Applied to document-derived content: document passages are turned into instruction–response pairs via automated transformations and backtranslation to increase diversity and quality of synthetic training data for online fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Offline uploaded documents (PDFs, text files, links) selected by users; the paper states Document-Driven Learning uses uploaded documents and then applies instruction backtranslation to produce curated high-quality training data (size unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>User uploads or points to domain-specific documents; the system uses those documents as source texts for instruction-backtranslation augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Automatic transformation of document text into instruction–response training pairs via instruction backtranslation and other augmentation; resulting examples are filtered and used for supervised online fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Augmented synthetic training dataset derived from documents (instruction + response pairs) and subsequently a fine-tuned LLM which has been trained on the augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Used as part of OT and validated in the tool-learning experiments described; general validation includes filtering for toxicity and measuring downstream task performance (tool-invocation accuracy) after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Instruction backtranslation is described as part of the Document-Driven Learning pipeline for generating high-quality training data; when combined with other OT components it contributed to the observed improvements in the tool-learning case study (OT performance gains described above).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on quality of source documents; may produce distributional mismatch if document-derived examples differ from test distribution; requires moderation/filtering to avoid introducing undesirable content; specifics of failure modes not exhaustively quantified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not directly compared in isolation; instruction-backtranslation was part of the OT data generation pipeline which as a whole was compared to few-shot Prompt and Full-SFT baselines in the case study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Training of Large Language Models: Learn while chatting', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3679.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3679.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (a large language model for science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced large language model developed for scientific knowledge (cited in the paper's bibliography as 'A large language model for science'); the present paper cites Galactica among prior works on LLMs for science but does not describe its methods or experiments in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Mentioned in related-work/references as an example of a large language model aimed at scientific knowledge; the present paper does not provide technical detail beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Training of Large Language Models: Learn while chatting', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A large language model for science <em>(Rating: 2)</em></li>
                <li>Self-instruct: Aligning language model with self generated instructions <em>(Rating: 2)</em></li>
                <li>Self-alignment with instruction backtranslation <em>(Rating: 2)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
                <li>Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3679",
    "paper_id": "paper-268297277",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "OT",
            "name_full": "Online Training using External Interactions",
            "brief_description": "A user-facing interaction paradigm and method that performs parameter-variant online fine-tuning of LLMs triggered by conversational user instructions, using three external interaction modes (Instruction-Guided, Document-Driven, Web Search-Enabled) to generate training data and immediately replace the served model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Online Training using External Interactions (OT)",
            "system_or_method_description": "An online parameter-variant paradigm that accepts natural-language '[Learn]' style user instructions (or document uploads or web search instructions), uses LLM-based data augmentation techniques (self-instruct, instruction backtranslation, web-search augmentation) to create synthetic training examples, filters for quality/moderation, and fine-tunes the served model (e.g. via LoRA) so the updated model immediately replaces the old one and persists knowledge across sessions.",
            "input_corpus_description": "User-provided conversational instructions (natural language '[Learn]' directives), uploaded documents (PDF/text/links), and web search results; in the reported experiments OT used between 100 and 2,000 model-generated training examples for the tool-learning case study, while a Full-SFT baseline used 6,000 labeled in-domain examples.",
            "topic_or_query_specification": "Natural language chat directives prefixed by a trigger token such as '[Learn]' (e.g. \"[Learn] I wish you could fetch more news on environmental pollution\"), or user-uploaded document selection, or explicit web-search instructions.",
            "distillation_method": "LLM-based synthetic data generation using self-instruct and instruction backtranslation and web-search augmentation to produce training examples; filtering for toxic/biased content; then parameter fine-tuning (online SFT/LoRA) on the generated data to inject knowledge into model parameters.",
            "output_type_and_format": "A fine-tuned LLM (updated model parameters / LoRA patch) that persists learned knowledge across sessions; intermediate outputs are synthetic training examples (instruction-response pairs, tool-invocation formatted examples).",
            "evaluation_or_validation_method": "Case study evaluation (tool-learning): compared OT to a Prompt (few-shot) baseline and a Full-SFT baseline. Evaluation measured correctness of tool invocation (correct tool selection + correct tool input parameters) on a held-out test set of 300 examples; training/inference time was reported (Table 2).",
            "results_summary": "In the tool-learning case study, a single round of model-generated training data used by OT increased tool-invocation accuracy from nearly 30% (vanilla) to about 50%; OT trained with 100–2000 generated examples achieved substantial improvement over prompt-based few-shot, while Full-SFT used 6,000 labeled in-domain examples. OT also reduced training cost compared to Full-SFT and showed favorable inference efficiency versus retrieval/ICL approaches.",
            "limitations_or_challenges": "Reported challenges include difficulty of reliably injecting factual knowledge without overfitting, data-distribution misalignment between generated training data and test distributions (explaining similar performance between 100 and 2000 generated points), content moderation risks from web-derived data, potential bias/toxicity requiring filtering, and open issues around concurrency and long-term knowledge management.",
            "comparison_to_baselines_or_humans": "Directly compared to Prompt (few-shot Llama2-7b-chat with few-shot prompts) and Full-SFT (full-parameter supervised fine-tuning on 6k labeled examples). OT outperformed the prompt baseline substantially (from ~30% to ~50% accuracy) while using far fewer generated examples and lower labeling cost than Full-SFT; Full-SFT used 6k labeled examples (comparison of absolute performance vs Full-SFT not fully enumerated in text).",
            "uuid": "e3679.0",
            "source_info": {
                "paper_title": "Online Training of Large Language Models: Learn while chatting",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-Instruct",
            "name_full": "Self-Instruct",
            "brief_description": "A data-generation technique in which an LLM generates instruction–response training pairs from seed instructions or prompts to create instruction-tuning data without human labeling; cited and used as part of OT's data generation pipeline.",
            "citation_title": "Self-instruct: Aligning language model with self generated instructions",
            "mention_or_use": "use",
            "system_or_method_name": "Self-Instruct synthetic-data generation",
            "system_or_method_description": "Used to transmute user conversational directives and other sources into trainable examples (instruction–response pairs) by prompting LLM(s) to generate supervision data (self-generated instructions and answers) that are then filtered and used to fine-tune the model online.",
            "input_corpus_description": "Inputs are user instructions, uploaded documents, or web-extracted passages provided at runtime; the paper does not specify corpus size for self-instruct generation beyond the experimental counts (100–2000 generated examples used in OT experiments).",
            "topic_or_query_specification": "Natural language user instructions (chat messages), or document contents/web extractions that the user requests the model to learn from; these serve as seeds for self-instruct generation.",
            "distillation_method": "Prompting the LLM to produce synthetic instruction–response pairs (self-instruction generation), optionally combined with instruction backtranslation and web search augmentation; generated data is filtered for quality and toxicity and used for supervised fine-tuning.",
            "output_type_and_format": "Synthetic training examples (instruction + target response), formatted for supervised fine-tuning; downstream output is a fine-tuned model that reflects the generated supervision.",
            "evaluation_or_validation_method": "Applied within OT and evaluated in the tool-learning case study: the synthetic training data produced by self-instruct was used to fine-tune models and performance was measured by tool-invocation accuracy on a held-out test set; data quality filtering / moderation applied prior to training.",
            "results_summary": "When used within OT, model-generated training data (via self-instruct among other augmentations) enabled a fine-tuned model to improve tool-invocation accuracy from roughly 30% to ~50% using only 100–2000 generated examples, demonstrating effective low-cost improvement versus few-shot prompting.",
            "limitations_or_challenges": "Quality and distribution mismatch of synthetic examples can limit performance gains; generated data can contain biased or toxic content and thus requires filtering; effectiveness depends on initial model capability and on data diversity to avoid overfitting.",
            "comparison_to_baselines_or_humans": "Self-instruct generated data (used in OT) was compared indirectly to human-annotated Full-SFT data (6k labeled examples); OT with self-instruct uses far fewer generated examples while achieving large gains over prompt-only baselines, though Full-SFT remains the stronger standard for high-quality labeled data (paper emphasizes lower cost of OT vs Full-SFT).",
            "uuid": "e3679.1",
            "source_info": {
                "paper_title": "Online Training of Large Language Models: Learn while chatting",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Instruction-Backtranslation",
            "name_full": "Instruction Backtranslation",
            "brief_description": "A data-augmentation technique where instructions and responses are transformed (backtranslated) to produce diverse synthetic training pairs; cited in the paper and applied in OT's Document-Driven Learning to curate training data from documents.",
            "citation_title": "Self-alignment with instruction backtranslation",
            "mention_or_use": "use",
            "system_or_method_name": "Instruction Backtranslation (data augmentation)",
            "system_or_method_description": "Applied to document-derived content: document passages are turned into instruction–response pairs via automated transformations and backtranslation to increase diversity and quality of synthetic training data for online fine-tuning.",
            "input_corpus_description": "Offline uploaded documents (PDFs, text files, links) selected by users; the paper states Document-Driven Learning uses uploaded documents and then applies instruction backtranslation to produce curated high-quality training data (size unspecified).",
            "topic_or_query_specification": "User uploads or points to domain-specific documents; the system uses those documents as source texts for instruction-backtranslation augmentation.",
            "distillation_method": "Automatic transformation of document text into instruction–response training pairs via instruction backtranslation and other augmentation; resulting examples are filtered and used for supervised online fine-tuning.",
            "output_type_and_format": "Augmented synthetic training dataset derived from documents (instruction + response pairs) and subsequently a fine-tuned LLM which has been trained on the augmented data.",
            "evaluation_or_validation_method": "Used as part of OT and validated in the tool-learning experiments described; general validation includes filtering for toxicity and measuring downstream task performance (tool-invocation accuracy) after fine-tuning.",
            "results_summary": "Instruction backtranslation is described as part of the Document-Driven Learning pipeline for generating high-quality training data; when combined with other OT components it contributed to the observed improvements in the tool-learning case study (OT performance gains described above).",
            "limitations_or_challenges": "Relies on quality of source documents; may produce distributional mismatch if document-derived examples differ from test distribution; requires moderation/filtering to avoid introducing undesirable content; specifics of failure modes not exhaustively quantified in paper.",
            "comparison_to_baselines_or_humans": "Not directly compared in isolation; instruction-backtranslation was part of the OT data generation pipeline which as a whole was compared to few-shot Prompt and Full-SFT baselines in the case study.",
            "uuid": "e3679.2",
            "source_info": {
                "paper_title": "Online Training of Large Language Models: Learn while chatting",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica (a large language model for science)",
            "brief_description": "A referenced large language model developed for scientific knowledge (cited in the paper's bibliography as 'A large language model for science'); the present paper cites Galactica among prior works on LLMs for science but does not describe its methods or experiments in detail.",
            "citation_title": "A large language model for science",
            "mention_or_use": "mention",
            "system_or_method_name": "Galactica",
            "system_or_method_description": "Mentioned in related-work/references as an example of a large language model aimed at scientific knowledge; the present paper does not provide technical detail beyond citation.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": null,
            "output_type_and_format": null,
            "evaluation_or_validation_method": null,
            "results_summary": null,
            "limitations_or_challenges": null,
            "comparison_to_baselines_or_humans": null,
            "uuid": "e3679.3",
            "source_info": {
                "paper_title": "Online Training of Large Language Models: Learn while chatting",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A large language model for science",
            "rating": 2,
            "sanitized_title": "a_large_language_model_for_science"
        },
        {
            "paper_title": "Self-instruct: Aligning language model with self generated instructions",
            "rating": 2,
            "sanitized_title": "selfinstruct_aligning_language_model_with_self_generated_instructions"
        },
        {
            "paper_title": "Self-alignment with instruction backtranslation",
            "rating": 2,
            "sanitized_title": "selfalignment_with_instruction_backtranslation"
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1,
            "sanitized_title": "large_language_models_can_selfimprove"
        },
        {
            "paper_title": "Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "language_models_can_teach_themselves_to_use_tools"
        }
    ],
    "cost": 0.013835499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Online Training of Large Language Models: Learn while Chatting
4 Mar 2024</p>
<p>Juhao Liang juhaoliang1997@gmail.com 
Ziwei Wang ziweiwang2@link.cuhk.edu.cn 
Zhuoheng Ma zhuohengma@link.cuhk.edu.cn 
Benyou Wang wangbenyou@cuhk.edu.cn. </p>
<p>The Chinese University of Hong Kong
ShenzhenChina</p>
<p>JIANQUAN LI
ZHIYI ZHANG
XIANGBO WU
The Chinese University of Hong Kong
ShenzhenChina</p>
<p>The Chinese University of Hong Kong, Shenzhen &amp; Shenzhen Research Institute of Big Data
China</p>
<p>https://github.com/FreedomIntelligence/Online-Training.git</p>
<p>The Chinese University of Hong Kong
ShenzhenChina</p>
<p>Jianquan Li
Zhiyi Zhang; Xiangbo Wu</p>
<p>The Chinese University of Hong Kong
Shenzhen, Benyou WangChina</p>
<p>The Chinese University of Hong Kong, Shenzhen &amp; Shenzhen Research Institute of Big Data
China</p>
<p>Online Training of Large Language Models: Learn while Chatting
4 Mar 20242B08581D317937ED98157EF02566F96F10.1145/nnnnnnn.nnnnnnnarXiv:2403.04790v1[cs.CL]Large Language ModelUser InteractionNatural Language Processing
Large Language Models (LLMs) have dramatically revolutionized the field of Natural Language Processing (NLP), offering remarkable capabilities that have garnered widespread usage.However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning.This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model.Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces.To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI agents or online/offline knowledge bases 1 .CCS Concepts: • Human-centered computing → Human computer interaction (HCI); User interface toolkits; • Computing methodologies → Natural language processing;</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) [26,62,69,78] has witnessed remarkable advancements in recent years, revolutionizing various natural language processing (NLP) tasks [2,7,14].In a world where knowledge and user requirements are constantly shifting, it's critical for these models to engage in incremental learning [85] .Existing work [29,63] tried to improve language models by selfconsistency or self-reflection; however such improvement is limited to be in the scenarios where deterministic rule based check could be used like coding and numerical answers, which might be used in open-world problems.We argue that using LLMs to improve themselves is limited; interactions between LLMs and environment are essential incremental learning.</p>
<p>There are two typical ways for incremental learning: offline incremental training and online incontext learning.Offline incremental training in language models involves training the system with new sets of annotated data, allowing for some level of adaptability, such as supervised finetuning [46] and reinforcement learning with human feedback [11,88].However, this approach is fraught with several limitations.First, it suffers from a lagging nature, meaning there is a delay between the emergence of new information and the model's update to include it, rendering the model less reliable for immediate or evolving scenarios.Second, the method is generally nonpersonalized; it updates the model based on broad data sets rather than tailoring to individual user preferences or specific contextual needs.This method's high computational cost and static nature post-update make it inflexible for adapting to real-time changes or diverse user requirements.</p>
<p>Another approach in the realm of incremental learning is known as 'online in-context learning'.In this setup, a LLM typically interacts with an AI agent or connects to either offline or online knowledge sources.For instance, the model might retrieve information from a given knowledge base or from web content, a.k.a,Retrieval-Augmented Generation (RAG) [30,36,74].Such methods often occur within the paradigm of in-context learning (ICL) [8,40].However, a significant limitation of this approach is its lack of knowledge persistency.When the session changes, the learned information is not retained, leading to a loss of any updates or learning that took place.</p>
<p>To address the shortcomings of both 'Offline Incremental Training' and 'Online In-Context Learning,' we introduce a cutting-edge paradigm, 'Online Training using External Interactions.'This new approach offers the benefits of persistent model updates and real-time learning.Unlike traditional methods, it necessitates external interactions during the learning process, such as interfacing with AI agents or connecting to offline or online knowledge sources.These capabilities allow the model to continually adapt and stay updated, effectively addressing the limitations seen in previous frameworks.Furthermore, this innovative human-computer interaction paradigm presents a unique opportunity for ordinary users to modify LLMs themselves, thereby shifting from a developer-owned model to a more user-centric approach.This paradigm could mark the beginning of a broader practical application of LLMs in the everyday lives of an increasing number of people.</p>
<p>The key contributions of this work are as follows:</p>
<p>• We propose a novel paradigm for human interaction with language models, shifting from static to adaptable models.Instead of humans adapting to fixed model parameters, we introduce models that evolve in response to human input, fostering a dynamic and sometimes reciprocal relationship.• The classification of interaction paradigm and proposal of a novel paradigm: Online Training using External Interactions, which is a user-friendly incremental learning methodology.</p>
<p>RELATED WORK AND MOTIVATION</p>
<p>From the users' perspective, there are currently two well-known interaction paradigms with LLMs: offline parameter-variant and online parameter-invariant.In this section, we first introduce these two prevalent user interaction paradigms and their applications, along with their applications, and detail their respective advantages and disadvantages in Sec.</p>
<p>✓ ✓</p>
<p>Online Parameter-Invariant Retrieval-based methods [30,36,74]</p>
<p>✓ ✓</p>
<p>Prompt-based methods [8,40] ✓ ✓</p>
<p>Tool-based methods [19,23,35,54,70,73,80]
✓ Online Parameter-Variant Online Training using External Interactions ✓ ✓ ✓ ✓ ✓</p>
<p>Related Work</p>
<p>Offline parameter-variant paradigm.The offline parameter-variant paradigm is the most commonly used interaction paradigm, wherein models are updated during periods of non-service.This paradigm comprises methods that are trained on a given labeled dataset.The dataset can be compiled solely by humans, as suggested by Ouyang et al. (2022) [46], which represents a conventional method of model training, specifically referred to as supervised fine-tuning (SFT).Alternatively, the process can be assisted by a retriever, as explored by [28,42,43].Interaction with external knowledge during training can enhance the model's representation by integrating a larger volume of factual knowledge.For developers, offline parameter-variant methods are the most reliable and effective options for training a language model from scratch or for model updates due to their 'once and for all' characteristic, leading to high knowledge persistency and quality.However, from the user's perspective, the entire model training process can be complex, inflexible and time-consuming, ranging from data collection to computing resource configuration and model training.</p>
<p>Online parameter-invariant paradigm.For online parameter-invariant paradigm, there are three kinds of techniques, retrieval-based methods [30,36,74], prompt-based methods [8,40], and tool-based methods [19,23,35,54,70,73,80].RAG [36] is a representation of retrieval-based methods, which emphasizes the use of external knowledge sources to augment language models during inference time.Interaction with the knowledge base during inference can aid the language model in generating more precise, contextually relevant, and informed responses by dynamically leveraging external knowledge sources based on the specific input or query at hand.RAG amalgamates pretrained parametric and non-parametric memory for language generation.Designed to enhance the factual accuracy of dialogue agents, it aims to mitigate the issue of knowledge hallucination.Whereas, its performance heavily relies not only on the quality of the knowledge but also on the effectiveness of the retrieval method.</p>
<p>The primary objective of the prompt-based method is to sustain real-time, continuous engagement, making it ideal for application scenarios such as dialogue systems, real-time translations, and multi-round question answering.This iterative interaction process allows the model's output to incrementally adjust to meet user demands.Typically, this interaction paradigm does not modify the model's parameters during the interaction, instead necessitating users to incessantly input or revise prompts to draw more meaningful responses from the language model.In-context learning [8] is a method of prompt-based, which operates without access to any external memory or knowledge beyond its pre-training phase.Consequently, while generating responses, it primarily relies on the immediate context of the conversation or task for information.Consequently, conversations can become rigid and labor-intensive due to the necessity for prompt engineering or dialogue engineering.The drawbacks of this interaction pattern are the inefficiency of proper prompts construction and failures in non-textual tasks.</p>
<p>By fragmenting a downstream task into multiple steps, tool-based methods can assign specific stages to external tools or APIs, such as those specializing in mathematical computations, web searches, image generation, and etc.Typically, tasks that emphasize fidelity and accuracy, such as real facts, complex mathematical operations, and tasks that transcend the LLM training corpus including up-to-date knowledge, low-resource languages, and image generation, are more effectively resolved using external tools than LLMs.ToolLLM [54] is a well-known tool-enhanced method.It constitutes a comprehensive framework for tool-use, offering tangible tools and components for LLMs.It is specifically engineered to empower LLMs to execute higher-level tasks, such as adhering to human instructions for utilizing external tools (APIs).However, it suffers from challenges related to invoking tools at the appropriate time and determining the most suitable tool to utilize.</p>
<p>Pros and Cons of Existing Paradigms.The two existing User-LLM interaction paradigms are extensively utilized in both research and practical applications.Offline parameter-variant approaches excel in knowledge persistency and quality, a result of the substantial engineering effort required before model training.This includes thorough data collection and meticulous training configuration.However, such a workload leads to inflexibility and high costs in model updates.Moreover, these methods inherently lack timeliness in knowledge updates due to their demanding workload.On the other hand, online parameter-invariant methods enable the enhancement of trained LLMs without extra training costs through prompting.However, as observed in previous research [41], the efficacy of prompting is not consistently positive.It may significantly degrade, especially if the relevant information's position varies in long context, even in models designed for long-context scenarios.Additionally, online parameter-invariant methods, like retrieval-based and tool-based approaches, often require extensive systems support, such as external databases, posing a burden for system development and sharing.Furthermore, the integrated knowledge in these methods has a short effective lifespan, expiring at the session's end or upon context removal.Overall, the pros and cons of existing paradigms are listed in Table 1.</p>
<p>Motivation</p>
<p>Motivation.In the context of knowledge persistence, the offline parameter-variant paradigm can be likened to the human brain's long-term memory, which necessitates extensive training and preparation for embedding specific knowledge.Conversely, the online parameter-invariant paradigm resembles short-term memory, where knowledge or skills can be rapidly acquired through quick learning but are not retained in the model for an extended period.This analogy highlights the strengths and weaknesses of these existing interaction paradigms.Motivated by this, we propose an intermediate interaction paradigm that amalgamates the benefits of both: the 'Online Parameter-Variant' method.This novel approach aims to reduce training costs compared to offline parameter-variant methods while offering more robust improvements than the online parameterinvariant paradigm.The 'Online Parameter-Variant' method, which is grounded in model training, focuses on several key metrics: knowledge persistency, flexibility, efficient updating, knowledge timeliness, and superior knowledge quality relative to the previous paradigms.</p>
<p>USER INTERFACE: ONLINE TRAINING USING EXTERNAL INTERACTIONS</p>
<p>We will first introduce the overall design in Sec.3.1 which consists of three interactions.These three interactions are detailed in Sec.3.2, Sec.3.3 and Sec.3.4.</p>
<p>Overall design</p>
<p>3.1.1Philosophy.To address these challenges, we introduce a new interactive interface that facilitates user engagement with LLMs through conversational interactions while concurrently enabling fine-tuning through natural language instructions.The proposed system allows users to engage in conversations with a LLMs while providing specific instructions to trigger immediate finetuning.Users can seamlessly trigger the training process by employing natural language prompts preceded by "[Learn]," like "[Learn] I wish you could fetch more news on environmental pollution," within an interface resembling a chat, thereby commencing training grounded on network-sourced information.</p>
<p>Upon receiving the triggering signal, our system will comprehend the user's intended meaning and initiate distinct learning processes accordingly.After the training is completed, the newly enhanced model, enriched with incremental knowledge, will immediately replace the preceding model and seamlessly resume the ongoing conversation with users.Instruction-Guided Learning serves as a soft knowledge source, leveraging conversational interfaces like ChatGPT2 to facilitate human-like, adaptive responses.This functionality is particularly powerful for nuanced or subjective queries where human-like interpretation and flexibility are required.</p>
<p>On the other hand, Document-Driven Learning and Web Search-Enabled Learning act as hard knowledge sources.Document-Driven Learning relies on offline sources, allowing for quality-controlled, curated information to be used in model training.This is particularly advantageous for tasks that require authoritative or highly reliable information.Web Search-Enabled Learning utilizes online data, offering the advantage of real-time information retrieval.While this allows the model to stay current, it can sometimes introduce bias or less reliable data into the training set.</p>
<p>By using self-instruct [75], instruction backtranslation [38], and online search augmentation, these functionalities allow for a high degree of customizability and adaptability.They empower users to shape their models according to specific instructions, documents, or real-time web data, thus bridging the gap between static, pre-trained models and dynamic, personalized user needs.Together, these functionalities make our framework not only versatile but also user-centric, enabling continuous improvement and adaptability across various application domains.This effectively highlights the complementary nature of the three functionalities in offering different sources and reliability of knowledge, serving to create a balanced, comprehensive approach for online learning with interaction.</p>
<p>Content Moderation Control. Content moderation is crucial for maintaining the integrity of</p>
<p>LLMs.To ensure effective moderation and reduce the risk of generating biased, toxic, or unethical content, the proposed interface utilizes two primary strategies: Prevention and Feedback.First, we employ an external interface specifically designed to monitor and address content moderation issues during training.Second, we integrate a user feedback mechanism, enabling users to contribute to the moderation process through their interactions and observations.For the prevention aspect, all data used for model updates will undergo rigorous scrutiny, filtering out any inappropriate content to ensure that sources, whether local documents or Internet-based, are suitable for LLM moderation.As for the feedback component, a 'feedback' button is available to users, allowing them to report biases or any unsatisfactory elements of the response.When used, the model initiates an updating process, making corrections through a pre-defined mechanism.Interestingly, this feedback mechanism can be viewed as a form of online version reinforcement learning with human feedback (online RLHF) [46], aimed at aligning LLMs with actual user values and accentuating the personalized aspect of the proposed method.</p>
<p>Benefits and potential.</p>
<p>Lifelong Learning.The concept of lifelong learning [5] refers to the ability of a system, in this case, a language model, to continuously acquire and integrate new knowledge and skills throughout its existence.Unlike traditional training methods that rely on static datasets, our approach leverages continuous user interactions to adapt and evolve the model over time.This enables lifelong learning, where the model can continuously improve and stay relevant to the user's changing needs and interests.</p>
<p>Personalization [10].Through our interactive training mode, users have the power to customize the language model to their liking.This customization extends beyond simple prompt-based instructions and allows users to fine-tune the model's behavior to suit their unique requirements.This level of personalization results in models that are highly specialized and context-aware, enhancing their utility for specific tasks and domains.</p>
<p>Accessibility.The heart of our approach lies in allowing users to engage in natural language conversations with the language model.This familiarity with conversational interactions makes it accessible to a wide range of users, regardless of their technical background.Users don't need to learn complex programming or command syntax; they simply converse as they would with another person.</p>
<p>User Empowerment.Users are in control of the training process.They can decide when and how to fine-tune the model based on their needs.This sense of empowerment fosters a feeling of ownership over the model, enhancing user engagement and motivation to participate in the training process.Additionally users can track the progress of their customized model over time.They can see how their interactions and commands have shaped the model's behavior, providing a sense of achievement and transparency in the training process.</p>
<p>Instruction-Guided Learning</p>
<p>Instruction-Guided Learning constitutes a fundamental component of our interactive language model fine-tuning system, enabling users to impart specific directives to the model regarding information retention and contextual understanding during ongoing conversational interactions.This method is particularly instrumental in customizing the model's responses to align with the user's unique requirements and preferences.</p>
<p>Within the Instruction-Guided Learning method, users are granted the capability to issue explicit instructions to the language model.These instructions may pertain to what facts or details the model should remember or any specific information that should be considered during the discourse.Users can convey their instructions in a natural language format, making it an intuitive and user-friendly , Vol. 1, No. 1, Article .Publication date: March 2023.</p>
<p>The impact of urban development on the natural environment depends on planning, sustainability practices, and conservation efforts, making it a complex and variable issue with both potential for harm and opportunities for mitigation.</p>
<p>Certainly!I'll perform a web search.This might take a little while.</p>
<p>[Learn] I hope you read more news about environmental pollution.</p>
<p>Air Pollution Has Decreased Across the US, but New Research Finds Health Burdens Remain Unequal Among Racial Groups.... [Learn] The Japanese government's decision to begin releasing treated Fukushima nuclear wastewater into the Pacific Ocean starting on August 24, 2023, has garnered widespread international attention and controversy...</p>
<p>Model Updating... [100%]</p>
<p>Model Updating... [100%]</p>
<p>Finished model updated!I already remember the information you provided.</p>
<p>How do you view the situation regarding Japan's nuclear wastewater in 2023?</p>
<p>The Japanese government's decision to discharge treated Fukushima nuclear wastewater into the Pacific Ocean starting August 24, 2023 has aroused widespread concern and controversy in the international community... process.These directives will be transmuted into trainable data via the self-instruct approach [75], after which we will proceed to iteratively enhance our model using the data thus generated.</p>
<p>Instruction-Guided</p>
<p>After receiving user instructions, the model promptly incorporates the provided information into its understanding of the ongoing conversation.This entails the identification and retention of salient details and context specified by the user.The model's responses are then guided by the personalized context, resulting in responses that align closely with the user's directives.</p>
<p>As the conversation unfolds, the model continuously adapts its responses based on the instructions and context provided by the user.This iterative adaptation process allows the model to tailor its responses, ensuring that it adheres to the user's preferences and maintains a coherent and contextually relevant dialogue.Consequently, the user experiences a personalized and highly responsive conversational interface.</p>
<p>Instruction-Guided Learning offers users a powerful means to personalize the language model's behavior and responses in a conversational setting.By issuing explicit directives, users can shape the model's understanding and context, thereby tailoring its responses to their unique requirements.This method enhances the utility of language models across various applications, including personal virtual assistants, domain-specific chatbots, and tailored information retrieval systems, making them versatile tools for a diverse range of user needs.</p>
<p>Online Training</p>
<p>Document-Driven Learning</p>
<p>Document-Driven Learning constitutes a pivotal facet of our interactive language model fine-tuning system, offering users the capability to enrich the model's knowledge base with structured and specialized information.This mode is particularly suited for users who seek to imbue the model with domain-specific expertise or train it on authoritative documents, academic texts, or specialized knowledge sources.</p>
<p>Users initiate the Document-Driven Learning method by selecting and uploading documents relevant to their specific area of interest or domain.These documents may encompass scholarly articles, technical manuals, legal documents, or any textual resources germane to the subject matter.The system accepts a variety of file formats, including PDFs, text documents, and web links.</p>
<p>Upon document submission, the system undertakes a comprehensive preprocessing and transformation procedure.Document-derived data produced through the utilization of Instruction Backtranslation [38] will be meticulously curated for high-quality training purposes.Through iterative fine-tuning, the model adapts to the new information derived from the uploaded documents.It learns to contextualize the content, recognize domain-specific terminology, and develop a deeper understanding of the subject matter.Consequently, the model's responses become more nuanced and contextually relevant when engaging in discussions related to the uploaded documents or the associated domain.</p>
<p>Document-Driven Learning represents a potent mechanism for users to imbue language models with domain-specific knowledge and expertise.By leveraging structured textual resources, users can enhance the model's contextual understanding and its capacity to provide informed responses within specialized domains.This method extends the utility of language models across a wide array of professional and academic applications, enabling them to serve as versatile and knowledgeable conversational partners.</p>
<p>Web Search-Enabled Learning</p>
<p>The integration of Web Search-Enabled Learning constitutes the third facet within the framework of our interactive language model fine-tuning system, affording users the capability to harness the vast knowledge repository of the internet to augment the model's understanding and responsiveness.This method is particularly valuable for users seeking real-time information, staying updated on current events, or training the model on a dynamic and ever-evolving knowledge landscape.</p>
<p>Upon receiving user search instructions, the system promptly conducts web searches using wellestablished search engines and APIs.The retrieved web content, which may include news articles, blog posts, research papers, and other relevant sources, is then subjected to information extraction and summarization processes to distill the key insights and facts.The extracted information from web searches serves as a valuable source of training data for the model [75].During the subsequent fine-tuning phase, the model is exposed to the insights obtained from web searches.</p>
<p>An inherent advantage of Web Search-Enabled Learning is the model's adaptability to real-time information.As the web content evolves, the model continuously adapts to the dynamic knowledge landscape, ensuring that its responses remain up-to-date and accurate in the context of the ongoing conversation.This real-time adaptation is particularly advantageous for users seeking the latest information and insights.</p>
<p>The knowledge derived from web searches becomes an integral part of the model's memory, enriching its understanding of contemporary topics and factual information.This knowledge integration ensures that the model remains a reliable source of current events, trending topics, and dynamic knowledge domains over time.</p>
<p>Web Search-Enabled Learning empowers users to leverage the extensive resources of the internet to enhance the language model's knowledge and responsiveness.By instructing the system to retrieve real-time information, users ensure that the model remains current and up-to-date, making it a valuable resource for information retrieval, news updates, and dynamic knowledge domains.This method extends the utility of language models to domains requiring real-time knowledge integration and adaptation, making them versatile tools for a wide array of applications, including news summarization, trend analysis, and current event discussion.</p>
<p>APPLICATION: A CASE STUDY ON TOOL LEARNING</p>
<p>This section is dedicated to evaluating the effectiveness and efficiency of the proposed novel interaction paradigm, termed Online Training using External Interactions, abbreviated as Online Training (OT) in this section.</p>
<p>Problem setting</p>
<p>In this task, we assume that the user's objective is to train a LLM to effectively utilize external tools [54,67].To achieve this goal, we adopt the tool invocation data format outlined in Sun et al. (2023) [67], as demonstrated in Appendix A. We assess the model's accuracy in invoking the correct plugin and its corresponding inputs when presented with multiple APIs for various questions.</p>
<p>As illustrated in Figure 3, we employ two baseline methods: the prompt-based method (abbreviated as Prompt) and the full-parameter training method (abbreviated as Full-SFT ).In the Prompt method, we use the base model (Llama2-7b-chat [72]) to generate answers with few-shot prompts [76] listed in the context.Conversely, the Full-SFT method involves leveraging external annotated training data to train the model, subsequently using the same few-shot prompts as in the Prompt method for the test set.And, the proposed approach, online-training (OT), involves generating corresponding training data based on user training instructions, then filtering out low-quality, including toxic or biased, data to train the original model.Subsequently, a prompt-based approach is employed to generate answers for the test set.We select three tools from previous research [53], randomly choosing 300 data points for the test set and an additional 6k data points for the training dataset for the Full-SFT method</p>
<p>Result Analysis</p>
<p>The result are shown in Figure 4, where the x-coordinate indicates the data utilized by the onlinetraining methods, and the y-coordinate shows the accuracy of each method using external tools.Accuracy is measured by counting instances of correctness in both the action (correct tool selection) and input (accurate tool parameters generation).The Full-SFT method utilizes 6,000 labeled indomain data points for model training.In comparison, the performance of the OT method is evaluated using between 100 and 2,000 model-generated data points for training.It is evident that using a single round of model generation3 can achieve almost double the improvement over the vanilla model in the tool learning task, increasing from nearly 30% to 50%.However, the performance of online-training trained on 100 data points compared to 2000 data points appears similar.This is attributed to data distribution misalignment, which is still considered acceptable.Table 2 presents the analysis of experiment duration, detailing the time expenditures for both the training and inference processes.It becomes evident that the online-training method effectively amalgamates the benefits of both online parameter-invariant and offline parameter-variant methods, as demonstrated by its reduced training cost and heightened effectiveness on the test set.</p>
<p>Method</p>
<p>DISCUSSION</p>
<p>This section, drawing upon the experiments and related works, delves into some concerned issues and potential challenges associated with the proposed method.Also, future possibilities in the development of User-LLM interaction paradigms are introduced.</p>
<p>In-context Learning or Fine-Tuning?</p>
<p>There is always a question regarding language model downstream adaptation: should we opt for in-context learning or model fine-tuning for the continuous learning of trained LLMs?Both approaches have garnered considerable interest from researchers and users alike.As previously discussed, each paradigm has its strengths and weaknesses, depending on the scenario, and they are, in fact, not mutually exclusive.Moreover, increasing research [15,47] is focusing on the relationship between ICL and Fine-Tuning.Dai et al. (2023) [15] found that ICL behaves similarly to explicit fine-tuning at the prediction level, representation level, and attention behavior level.In light of these findings, we propose a novel interaction paradigm that bridges the gap between these two existing approaches.This method involves injecting knowledge directly into the parameters, rather than solely in the context, thereby enhancing its persistency and robustness.</p>
<p>Scalability: One of the prominent advantages of the proposed method over ICL is its superior transferability and compositionality.For example, one could prepare specific training data for each learning job and then later decide which training data to be combined for final application.Such a combination could be done without extra inference cost as the increase of learning jobs does not affect the inference cost.Also thanks to the efficient compositionality, our approach could have a better capacity to deal with a larger-scale training, which benefits the scalability.</p>
<p>Inference Efficiency Our method stands out for its superior inference efficiency when compared to Retrieve and Generate (RAG) or In-Context Learning (ICL) strategies.Both RAG and ICL often result in significantly longer input prompts, which in turn leads to an increase in computational cost -a cost that grows quadratically with the length of the input.Although the training cost of our approach might be higher than that of RAG or ICL, it's important to note that this is a one-time expense related to the training phase, and remains constant regardless of the number of requests.In contrast, the cost of inference increases linearly with the number of requests, making our method more efficient in the long run.Moreover, unlike approaches that require large-scale databases or additional plugins, our method incorporates knowledge directly into the model through online training, thereby eliminating the need for external dependencies.This not only simplifies the process, but also enhances deployment readiness and operation efficiency.</p>
<p>Challenges</p>
<p>Despite the potential of the online training method, it faces several challenges:</p>
<p>• Knowledge Injection and Overfitting: Ovadia et al. (2023) [47] noted that LLMs often struggle to assimilate new factual information through fine-tuning.A key challenge is effectively injecting necessary knowledge into LLMs within a user-acceptable timeframe to enhance user experience.Rather than relying on a high number of training epochs, which may cause models to overfit by repeatedly training on the same data, our approach increases data diversity.This aligns with user requirements and ensures model generalizability and knowledge acquisition.• Knowledge Persistency: Maintaining the knowledge persistency in LLMs is a crucial aspect of our proposed system.Unlike ICL-type knowledge persistency, which may involve storing information on disk, our parameter-variant methods embed knowledge directly into the LLMs' parameters.This approach ensures long-lasting knowledge retention, akin to pre-trained knowledge.for users.This ensures privacy and scalability without significantly increasing deployment demands.</p>
<p>• Concurrency in LLMs</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce a novel interaction paradigm, online parameter-variant, and a new method, online learning using external interactions.This approach focuses on explicit model finetuning and instant responses to natural language instructions via a user-friendly interface.As for the future direction, we aim to break down more restrictions on users' utilization of LLMs and bring more engaging and beneficial model human-computer interactions to users, leveraging the 'Online Training using External Interactions' paradigm.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2023.</p>
<ol>
<li>1 . 2
12
The three interactions.Online Training using External Interactions introduces three unique learning functionalities that form a comprehensive and versatile toolkit for interactive model training: Instruction-Guided Learning, Document-Driven Learning, and Web Search-Enabled Learning.</li>
</ol>
<p>Finished model updated!I've gathered data from the web search.Is there anything specific you'd like to discuss?Can you show me some latest news about environmental pollution?Do you think urban development will destroy the natural environment?can I assist you today?Hello!How can I assist you today?Do you still remember the situation regarding Japan's nuclear wastewater in 2023?Of course!The Japanese government's decision to discharge nuclear wastewater is controversial... can I assist you today?Hello!How can I assist you today?[Learn] This is a survey and analysis on environmental protection compiled by our company, please read it carefully.Sure thing!Please drop it in.Model Updating... [100%] Finished model updated!I've extracted information from the document.Can you give me some suggestions on how to add new content on environmental protection in the case study section of the report?Sure, first... ... Conversation 1 do you view the situation regarding Japan's nuclear wastewater in 2023?I'm sorry, my knowledge cutoff date is September 2021, and I do not have information about the Japanese nuclear wastewater situation in 2023.</p>
<p>Fig. 1 .
1
Fig. 1.The figure depicts the manner in which dialogues are conducted between LLM and user within our interactive mode.Notably, users issue distinct directives, each leading to the trigger of three distinct training processes.Furthermore, the figure underscores the model's ability to retain knowledge acquired during prior conversational session, even when transitioning across different conversation sessions.</p>
<p>Fig. 2 .
2
Fig. 2.This figure delineates our comprehensive workflow of chat-based online training.During the interaction between the user and the model, the user issues learning instructions to trigger the learning process.Three different learning methods correspond to three data augmentation techniques with the generated data as input to train new model.Then new model replace the old one seamlessly, allowing the user to continue the conversation.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2023.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. Overview of the experimental design</p>
<p>Deployment: The online-training interaction paradigm we propose has similar deployment costs to conventional methods.It can be trained into a specific set of parameters, such as using LoRA, which offers the flexibility to load or unload specific training , Vol. 1, No. 1, Article .Publication date: March 2023.</p>
<p>Table 1 .
1
Comparison of interaction paradigms.Online and offline refer to whether the model is serving, while training and parameter-invariant and parameter-variant indicate whether the parameter of the model changed.To assess and compare various interaction approaches, we focus on five key attributes: 1) Knowledge Persistency, which indicates if updated information remains accessible across different sessions; 2) Flexibility, evaluating if LLMs become static post-training; 3) Efficient Update, gauging the time and computational costs involved in model updates; 4) Knowledge Timeliness, assessing if the model's information is current; and 5) Knowledge Quality, which verifies the accuracy and reliability of the model's information.In the context of traditional training, LLMs are fine-tuned using a specific set of annotated data.
ParadigmMethodologyKnowledge PersistencyFlexibilityEfficient UpdateKnowledge TimelinessKnowledge QualityOfflineTraditional Training [8, 44, 61]Parameter-Variant</p>
<p>Table 2 .
2
Analysis of experiment duration: the time expended during the training process and the inference time for the test set are detailed.</p>
<p>, Vol. 1, No. 1, Article . Publication date: March 2023.
https://chat.openai.com , Vol. 1, No. 1, Article . Publication date: March 2023.
Approximately 100 valid data points can be obtained from a single GPT-4 API call. , Vol. 1, No. 1, Article . Publication date: March 2023.
ACKNOWLEDGEMENTThis work is supported by the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065), and Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608).Can you provide a weather forecast for Rio de Janeiro , Brazil for the upcoming weekend ?2 GPT : Thought : I need to use the forecast_weather API to get the weather forecast for Rio de Janeiro , Brazil for the upcoming weekend .A EXPERIMENT DETAILSFor all experiments conducted in this study, we utilized four A100 GPUs, each equipped with 80GB of memory.The learning rate for SFT was set to 2e-06, while for online training, it was established at 2e-5.Following the guidelines inTirumala et al. (2022)[71], we set the training batch epoch at 10 for OT and at 2 for SFT.Additionally, the data format for tool invocation is exemplified as follows:
Online algorithms: a survey. S Albers, Mathematical Programming. 972003</p>
<p>A tree-based statistical language model for natural language speech recognition. L R Bahl, P F Brown, P V De Souza, R L Mercer, IEEE Transactions on Acoustics, Speech, and Signal Processing. 371989</p>
<p>Scaling learning algorithms towards AI. Y Bengio, Y Lecun, Large Scale Kernel Machines. MIT Press2007</p>
<p>Z Bian, H Liu, B Wang, H Huang, Y Li, C Wang, F Cui, Y You, Colossal-Ai, arXiv:2110.14883A unified deep learning system for large-scale parallel training. 2021arXiv preprint</p>
<p>Continual lifelong learning in natural language processing: A survey. M Biesialska, K Biesialska, Costa-Jussà , M R , Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020International Committee on Computational Linguistics</p>
<p>S Black, S Biderman, E Hallahan, Q Anthony, L Gao, L Golding, H He, C Leahy, K Mcdonell, J Phang, arXiv:2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>Large language models in machine translation. T Brants, A C Popat, P Xu, F J Och, J Dean, </p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>. H Chase, Langchain, Oct. 2022</p>
<p>J Chen, Z Liu, X Huang, C Wu, Q Liu, G Jiang, Y Pu, Y Lei, X Chen, X Wang, D Lian, E Chen, When large language models meet personalization: Perspectives of challenges and opportunities. 2023</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Natural language processing (almost) from scratch. R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Journal of machine learning research. 122011ARTICLE</p>
<p>Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. D Dai, Y Sun, L Dong, Y Hao, S Ma, Z Sui, F Wei, 2023</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, 2019</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Fairscale: A general purpose modular pytorch library for high performance and large scale training. 2021FairScale authors</p>
<p>Lm-sys: Fastchat (vicuna: An open-source chatbot. 2023FastChat authors</p>
<p>. I Goodfellow, Y Bengio, A Courville, Y Bengio, Article . Publication date. 112016. March 2023MIT PressDeep learning</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, International conference on machine learning. PMLR2020</p>
<p>S Hao, T Liu, Z Wang, Z Hu, Toolkengpt, arXiv:2305.11554Augmenting frozen language models with massive tools via tool embeddings. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>A fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y W Teh, Neural Computation. 182006</p>
<p>Training compute-optimal large language models. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.155562022arXiv preprint</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>A survey of knowledge enhanced pre-trained language models. L Hu, Z Liu, Z Zhao, L Hou, L Nie, J Li, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022arXiv preprint</p>
<p>Few-shot learning with retrieval augmented language models. G Izacard, P Lewis, M Lomeli, L Hosseini, F Petroni, T Schick, J Dwivedi-Yu, A Joulin, S Riedel, E Grave, arXiv:2208.032992022arXiv preprint</p>
<p>A survey on contrastive self-supervised learning. A Jaiswal, A R Babu, M Z Zadeh, D Banerjee, F Makedon, Technologies. 9212020</p>
<p>Active retrieval augmented generation. Z Jiang, F F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, arXiv:2305.069832023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, 2020</p>
<p>A Köpf, Y Kilcher, D Von Rütte, S Anagnostidis, Z.-R Tam, K Stevens, A Barhoum, N M Duc, O Stanley, R Nagyfi, arXiv:2304.07327Openassistant conversations-democratizing large language model alignment. 2023arXiv preprint</p>
<p>. Langchain Authors Langchain, 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 332020</p>
<p>S Li, J Fang, Z Bian, H Liu, Y Liu, H Huang, B Wang, Y You, Colossal-Ai, arXiv:2110.14883A unified deep learning system for large-scale parallel training. 2021arXiv preprint</p>
<p>X Li, P Yu, C Zhou, T Schick, L Zettlemoyer, O Levy, J Weston, M Lewis, arXiv:2308.06259Self-alignment with instruction backtranslation. 2023arXiv preprint</p>
<p>Self-alignment with instruction backtranslation. X Li, P Yu, C Zhou, T Schick, L Zettlemoyer, O Levy, J Weston, M Lewis, 2023</p>
<p>J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqa, F Petroni, P Liang, 2023</p>
<p>Relational memory-augmented language models. Q Liu, D Yogatama, P Blunsom, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Kelm: knowledge enhanced pre-trained language representations with message passing on hierarchical relational graphs. Y Lu, H Lu, G Fu, Q Liu, arXiv:2109.042232021arXiv preprint</p>
<p>E Malmi, Y Dong, J Mallinson, A Chuklin, J Adamek, D Mirylenka, F Stahlberg, S Krause, S Kumar, A Severyn, arXiv:2206.07043Text generation with text-editing models. 2022arXiv preprint</p>
<p>Megatron-DeepSpeed authors. Megatron-DeepSpeed. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 352022</p>
<p>Fine-tuning or retrieval? comparing knowledge injection in llms. O Ovadia, M Brief, M Mishaeli, Elisha , O , 2023</p>
<p>A Parisi, Y Zhao, N Fiedel, Talm, arXiv:2205.12255Tool augmented language models. 2022arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and. the 36th Annual ACM Symposium on User Interface Software andMarch 2023. 20231Technology</p>
<p>S G Patil, T Zhang, X Wang, J E Gonzalez, Gorilla, arXiv:2305.15334Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>M E Peters, M Neumann, I V Logan, R L Schwartz, R Joshi, V Singh, S Smith, N A , arXiv:1909.04164Knowledge enhanced contextual word representations. 2019arXiv preprint</p>
<p>Tool learning with foundation models. Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, 2023</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, </p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 192019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, arXiv:2110.082072021arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, Z Jiang, F Petroni, P Lewis, G Izacard, Q You, C Nalmpantis, E Grave, S Riedel, Peer, arXiv:2208.11663A collaborative language model. 2022arXiv preprint</p>
<p>M Shanahan, arXiv:2212.03551Talking about large language models. 2022arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>M Shoeybi, M Patwary, R Puri, P Legresley, J Casper, B Catanzaro, arXiv:1909.08053Megatron-lm: Training multi-billion parameter language models using model parallelism. 2019arXiv preprint</p>
<p>Masked sequence to sequence pre-training for language generation. K Song, X Tan, T Qin, J Lu, T.-Y Liu, Mass, 2019</p>
<p>An information-theoretic approach to prompt engineering without ground truth labels. T Sorensen, J Robinson, C M Rytting, A G Shaw, K J Rogers, A P Delorey, M Khalil, N Fulda, D Wingate, arXiv:2203.113642022arXiv preprint</p>
<p>. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, Moss, Training conversational language models from synthetic data</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu, X Chen, Y Zhao, Y Lu, arXiv:2107.021372021arXiv preprint</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, Galactica, arXiv:2211.09085A large language model for science. 2022arXiv preprint</p>
<p>Text Generation Inference authors. 2023Text Generation Inference</p>
<p>Memorization without overfitting: Analyzing the training dynamics of large language models. K Tirumala, A Markosyan, L Zettlemoyer, A Aghajanyan, Advances in Neural Information Processing Systems. 352022</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 2023</p>
<p>. B Wang, W Ping, P Xu, L Mcafee, Z Liu, M Shoeybi, Y Dong, O Kuchaiev, B Li, C Xiao, arXiv:2304.06762Article . Publication date. 11March 2023. 2023arXiv preprintShall we. pretrain autoregressive language models with retrieval? a comprehensive study</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Generalizing from a few examples: A survey on few-shot learning. Y Wang, Q Yao, J T Kwok, L M Ni, ACM computing surveys (csur). 532020</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>{SkyPilot}: An intercloud broker for sky computing. Z Yang, Z Wu, M Luo, W.-L Chiang, R Bhardwaj, W Kwon, S Zhuang, F S Luan, G Mittal, S Shenker, 20th USENIX Symposium on Networked Systems Design and Implementation. 2023</p>
<p>Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. Z Yao, R Y Aminabadi, O Ruwase, S Rajbhandari, X Wu, A A Awan, J Rasley, M Zhang, C Li, C Holmes, arXiv:2308.013202023arXiv preprint</p>
<p>Z Yao, R Y Aminabadi, O Ruwase, S Rajbhandari, X Wu, A A Awan, J Rasley, M Zhang, C Li, C Holmes, Z Zhou, M Wyatt, M Smith, L Kurilenko, H Qin, M Tanaka, S Che, S L Song, Y He, arXiv:2308.01320DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. 2023arXiv preprint</p>
<p>Big model systems for large-scale representation learning. G Zeng, X Han, Z Zhang, Z Liu, Y Lin, M Sun, Openbmb, Representation Learning for Natural Language Processing. Springer2023</p>
<p>Measuring massive multitask chinese understanding. H Zeng, arXiv:2304.129862023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, S Zhang, G Ghosh, M Lewis, L Zettlemoyer, O Levy, Lima, 2023Less is more for alignment</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
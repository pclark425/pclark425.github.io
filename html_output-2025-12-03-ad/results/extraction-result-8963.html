<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8963 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8963</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8963</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-02534853626c18c9a097c2712f1ddf3613257d35</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02534853626c18c9a097c2712f1ddf3613257d35" target="_blank">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper incorporates copying into neural network-based Seq2Seq learning and proposes a new model called CopyNet with encoder-decoder structure which can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence.</p>
                <p><strong>Paper Abstract:</strong> We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8963",
    "paper_id": "paper-02534853626c18c9a097c2712f1ddf3613257d35",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0040539999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</h1>
<p>Jiatao Gu ${ }^{\dagger}$ Zhengdong Lu ${ }^{\ddagger}$ Hang $\mathbf{L i}^{\ddagger}$ Victor O.K. $\mathbf{L i}^{\dagger}$<br>${ }^{\dagger}$ Department of Electrical and Electronic Engineering, The University of Hong Kong<br>{jiataogu, vli}@eee.hku.hk<br>${ }^{\ddagger}$ Huawei Noah's Ark Lab, Hong Kong<br>{lu.zhengdong, hangli.hl}@huawei.com</p>
<h4>Abstract</h4>
<p>We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called CopyNet with encoderdecoder structure. CopyNeT can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNeT. For example, CopyNeT can outperform regular RNN-based model with remarkable margins on text summarization tasks.</p>
<h2>1 Introduction</h2>
<p>Recently, neural network-based sequence-tosequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).</p>
<p>Seq2Seq is essentially an encoder-decoder model, in which the encoder first transform the input sequence to a certain representation which can then transform the representation into the output sequence. Adding the attention mechanism (Bahdanau et al., 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al., 2015; Rush et al., 2015). Different from the canonical encoderdecoder architecture, the attention-based Seq2Seq model revisits the input sequence in its raw form (array of word representations) and dynamically fetches the relevant piece of information based mostly on the feedback from the generation of the output sequence.</p>
<p>In this paper, we explore another mechanism important to the human language communication, called the "copying mechanism". Basically, it refers to the mechanism that locates a certain segment of the input sentence and puts the segment into the output sequence. For example, in the following two dialogue turns we observe different patterns in which some subsequences (colored blue) in the response ( $R$ ) are copied from the input utterance (I):</p>
<div class="codehilite"><pre><span></span><code><span class="n">I</span><span class="o">:</span><span class="w"> </span><span class="n">Hello</span><span class="w"> </span><span class="n">Jack</span><span class="o">,</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">Chandralekha</span><span class="o">.</span>
<span class="n">R</span><span class="o">:</span><span class="w"> </span><span class="n">Nice</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">meet</span><span class="w"> </span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">Chandralekha</span><span class="o">.</span>
<span class="n">I</span><span class="o">:</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">guy</span><span class="w"> </span><span class="n">doesn</span><span class="s1">&#39;t perform exactly</span>
<span class="s1">    as we expected.</span>
<span class="s1">R: What do you mean by &quot;doesn&#39;</span><span class="n">t</span><span class="w"> </span><span class="n">perform</span>
<span class="w">    </span><span class="n">exactly</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">expected</span><span class="err">&quot;</span><span class="o">?</span>
</code></pre></div>

<p>Both the canonical encoder-decoder and its variants with attention mechanism rely heavily on the representation of "meaning", which might not be sufficiently inaccurate in cases in which the system needs to refer to sub-sequences of input like entity names or dates. In contrast, the</p>
<p>copying mechanism is closer to the rote memorization in language processing of human being, deserving a different modeling strategy in neural network-based models. We argue that it will benefit many Seq2Seq tasks to have an elegant unified model that can accommodate both understanding and rote memorization. Towards this goal, we propose CopYNET, which is not only capable of the regular generation of words but also the operation of copying appropriate segments of the input sequence. Despite the seemingly "hard" operation of copying, COPYNET can be trained in an end-toend fashion. Our empirical study on both synthetic datasets and real world datasets demonstrates the efficacy of COPYNET.</p>
<h2>2 Background: Neural Models for Sequence-to-sequence Learning</h2>
<p>Seq2Seq Learning can be expressed in a probabilistic view as maximizing the likelihood (or some other evaluation metrics (Shen et al., 2015)) of observing the output (target) sequence given an input (source) sequence.</p>
<h3>2.1 RNN Encoder-Decoder</h3>
<p>RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, first by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a). In the Encoder-Decoder framework, the source sequence $X=\left[x_{1}, \ldots, x_{T_{S}}\right]$ is converted into a fixed length vector $\mathbf{c}$ by the encoder RNN, i.e.</p>
<p>$$
\mathbf{h}<em t="t">{t}=f\left(x</em>}, \mathbf{h<em 1="1">{t-1}\right) ; \quad \mathbf{c}=\phi\left(\left{\mathbf{h}</em>}, \ldots, \mathbf{h<em S="S">{T</em>\right}\right)
$$}</p>
<p>where $\left{\mathbf{h}<em T__S="T_{S">{t}\right}$ are the RNN states, $\mathbf{c}$ is the so-called context vector, $f$ is the dynamics function, and $\phi$ summarizes the hidden states, e.g. choosing the last state $\mathbf{h}</em>$. In practice it is found that gated RNN alternatives such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) often perform much better than vanilla ones.}</p>
<p>The decoder RNN is to unfold the context vector $\mathbf{c}$ into the target sequence, through the following dynamics and prediction model:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{s}<em t-1="t-1">{t}=f\left(y</em>}, \mathbf{s<em t="t">{t-1}, \mathbf{c}\right) \
&amp; p\left(y</em>\right)
\end{aligned}
$$} \mid y_{&lt;t}, X\right)=g\left(y_{t-1}, \mathbf{s}_{t}, \mathbf{c</p>
<p>where $\mathbf{s}<em t="t">{t}$ is the RNN state at time $t, y</em>\right}$. The prediction model is typically a classifier over the vocabulary with, say, 30,000 words.}$ is the predicted target symbol at $t$ (through function $g(\cdot)$ ) with $y_{&lt;t}$ denoting the history $\left{y_{1}, \ldots, y_{t-1</p>
<h3>2.2 The Attention Mechanism</h3>
<p>The attention mechanism was first introduced to Seq2Seq (Bahdanau et al., 2014) to release the burden of summarizing the entire source into a fixed-length vector as context. Instead, the attention uses a dynamically changing context $\mathbf{c}<em t="t">{t}$ in the decoding process. A natural option (or rather "soft attention") is to represent $\mathbf{c}</em>$ as the weighted sum of the source hidden states, i.e.</p>
<p>$$
\mathbf{c}<em _tau="1">{t}=\sum</em>}^{T_{S}} \alpha_{t \tau} \mathbf{h<em _tau="\tau" t="t">{\tau} ; \quad \alpha</em>}=\frac{e^{\eta\left(\mathbf{s<em _tau="\tau">{t-1}, \mathbf{h}</em>
$$}\right)}}{\sum_{\tau^{\prime}} e^{\eta\left(s_{t-1}, \mathbf{h}_{\tau^{\prime}}\right)}</p>
<p>where $\eta$ is the function that shows the correspondence strength for attention, approximated usually with a multi-layer neural network (DNN). Note that in (Bahdanau et al., 2014) the source sentence is encoded with a Bi-directional RNN, making each hidden state $\mathbf{h}_{\tau}$ aware of the contextual information from both ends.</p>
<h2>3 CopYNET</h2>
<p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model. In this section, we present COPYNET, a differentiable Seq2Seq model with "copying mechanism", which can be trained in an end-to-end fashion with just gradient descent.</p>
<h3>3.1 Model Overview</h3>
<p>As illustrated in Figure 1, CopyNET is still an encoder-decoder (in a slightly generalized sense). The source sequence is transformed by Encoder into representation, which is then read by Decoder to generate the target sequence.</p>
<p>Encoder: Same as in (Bahdanau et al., 2014), a bi-directional RNN is used to transform the source sequence into a series of hidden states with equal length, with each hidden state $\mathbf{h}<em t="t">{t}$ corresponding to word $x</em>}$. This new representation of the source, $\left{\mathbf{h<em T__S="T_{S">{1}, \ldots, \mathbf{h}</em>$ in the remainder of the paper), which will later be accessed in multiple ways in generating the target sequence (decoding).}}\right}$, is considered to be a short-term memory (referred to as $\mathbf{M</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall diagram of CopyNet. For simplicity, we omit some links for prediction (see Sections 3.2 for more details).</p>
<p><strong>Decoder:</strong> An RNN that reads M and predicts the target sequence. It is similar with the canonical RNN-decoder in (Bahdanau et al., 2014), with however the following important differences:</p>
<ul>
<li><strong>Prediction:</strong> CopyNet predicts words based on a mixed probabilistic model of two modes, namely the <strong>generate-mode</strong> and the <strong>copy-mode</strong>, where the latter picks words from the source sequence (see Section 3.2);</li>
<li><strong>State Update:</strong> the predicted word at time t−1 is used in updating the state at t, but CopyNet uses not only its word-embedding but also its corresponding location-specific hidden state in M (if any) (see Section 3.3 for more details);</li>
<li><strong>Reading M:</strong> in addition to the attentive read to M, CopyNet also has "selective read" to M, which leads to a powerful hybrid of content-based addressing and location-based addressing (see both Sections 3.3 and 3.4 for more discussion).</li>
</ul>
<h3>3.2 Prediction with Copying and Generation</h3>
<p>We assume a vocabulary V = {v1, ..., vN}, and use UNK for any out-of-vocabulary (OOV) word. In addition, we have another set of words X, for all the <em>unique</em> words in source sequence X = {x1, ..., xs}. Since X may contain words not in V, copying sub-sequence in X enables CopyNet to output some OOV words. In a nutshell, the instance-specific vocabulary for source X is V ∪ UNK ∪ X.</p>
<p>Given the decoder RNN state s<sup>t</sup> at time t together with M, the probability of generating any target word y<sup>t</sup>, is given by the "mixture" of probabilities as follows</p>
<p>$$
p(y_t | s_t, y_{t-1}, \mathbf{c}<em t-1="t-1">t, \mathbf{M}) = p(y_t, g | s_t, y</em>)
$$}, \mathbf{c}_t, \mathbf{M</p>
<p>$$
+p(y_t, c | s_t, y_{t-1}, \mathbf{c}_t, \mathbf{M})
$$</p>
<p>where g stands for the generate-mode, and c the copy mode. The probability of the two modes are given respectively by</p>
<p>$$
p(y_t, g) = \begin{cases}
\frac{1}{Z} e^{\psi_g(y_t)}, &amp; y_t \in \mathcal{V} \
0, &amp; y_t \in \mathcal{X} \cap \mathcal{V} \
\frac{1}{Z} e^{\psi_g(\text{UNK})}, &amp; y_t \notin \mathcal{V} \cup \mathcal{X}
\end{cases}
$$</p>
<p>$$
p(y_t, c) = \begin{cases}
\frac{1}{Z} \sum_{j: x_j = y_t} e^{\psi_c(x_j)}, &amp; y_t \in \mathcal{X} \
0, &amp; \text{otherwise}
\end{cases}
$$</p>
<p>where ψ<sub>g</sub>(·) and ψ<sub>c</sub>(·) are score functions for generate-mode and copy-mode, respectively, and Z is the normalization term shared by the two modes, Z = ∑<sub>v∈V∪{UNK} e<sub>v</sub>(v) + ∑<sub>x∈X</sub> e<sub>v</sub>(x). Due to the shared normalization term, the two modes are basically competing through a softmax function (see Figure 1 for an illustration with example), rendering Eq.(4) different from the canonical definition of the mixture model (McLachlan and Basford, 1988). This is also pictorially illustrated in Figure 2. The score of each mode is calculated:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The illustration of the decoding probability $p\left(y_{t} \mid \cdot\right)$ as a 4-class classifier.</p>
<p>Generate-Mode: The same scoring function as in the generic RNN encoder-decoder (Bahdanau et al., 2014) is used, i.e.</p>
<p>$$
\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}<em o="o">{i}^{\top} \mathbf{W}</em>} \mathbf{s<em i="i">{t}, \quad v</em>
$$} \in \mathcal{V} \cup \text { UNK </p>
<p>where $\mathbf{W}<em s="s">{o} \in \mathbb{R}^{(N+1) \times d</em>}}$ and $\mathbf{v<em i="i">{i}$ is the one-hot indicator vector for $v</em>$.</p>
<p>Copy-Mode: The score for "copying" the word $x_{j}$ is calculated as</p>
<p>$$
\psi_{c}\left(y_{t}=x_{j}\right)=\sigma\left(\mathbf{h}<em c="c">{j}^{\top} \mathbf{W}</em>}\right) \mathbf{s<em j="j">{t}, \quad x</em>
$$} \in \mathcal{X</p>
<p>where $\mathbf{W}<em h="h">{c} \in \mathbb{R}^{d</em>} \times d_{s}}$, and $\sigma$ is a non-linear activation function, considering that the non-linear transformation in Eq.( 8 ) can help project $s_{t}$ and $h_{j}$ in the same semantic space. Empirically, we also found that using the tanh non-linearity worked better than linear transformation, and we used that for the following experiments. When calculating the copy-mode score, we use the hidden states $\left{\mathbf{h<em T__S="T_{S">{1}, \ldots, \mathbf{h}</em>$ only appears in the source.}}\right}$ to "represent" each of the word in the source sequence $\left{x_{1}, \ldots, x_{T_{S}}\right}$ since the bidirectional RNN encodes not only the content, but also the location information into the hidden states in $\mathbf{M}$. The location informaton is important for copying (see Section 3.4 for related discussion). Note that we sum the probabilities of all $x_{j}$ equal to $y_{t}$ in Eq. (6) considering that there may be multiple source symbols for decoding $y_{t}$. Naturally we let $p\left(y_{t}, \mathrm{c} \mid \cdot\right)=0$ if $y_{t}$ does not appear in the source sequence, and set $p\left(y_{t}, \mathrm{~g} \mid \cdot\right)=0$ when $y_{t</p>
<h3>3.3 State Update</h3>
<p>COPYNET updates each decoding state $\mathbf{s}<em t-1="t-1">{t}$ with the previous state $\mathbf{s}</em>}$, the previous symbol $y_{t-1}$ and the context vector $\mathbf{c<em t-1="t-1">{t}$ following Eq. (2) for the generic attention-based Seq2Seq model. However, there is some minor changes in the $y</em>} \longrightarrow \mathbf{s<em t-1="t-1">{t}$ path for the copying mechanism. More specifically, $y</em>$,
where $\mathbf{e}\left(y_{t-1}\right)$ is the word embedding associated with $y_{t-1}$, while $\zeta\left(y_{t-1}\right)$ is the weighted sum of hidden states in $\mathbf{M}$ corresponding to $y_{t}$}$ will be represented as $\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top</p>
<p>$$
\begin{aligned}
&amp; \zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}<em _tau="\tau" t="t">{\tau} \
&amp; \rho</em>}= \begin{cases}\frac{1}{K} p\left(x_{\tau}, \mathrm{c} \mid \mathbf{s<em _tau="\tau">{t-1}, \mathbf{M}\right), &amp; x</em> \
0 &amp; \text { otherwise }\end{cases}
\end{aligned}
$$}=y_{t-1</p>
<p>where $K$ is the normalization term which equals $\sum_{\tau^{\prime}: x_{\tau^{\prime}}=y_{t-1}} p\left(x_{\tau^{\prime}}, c \mid \mathbf{s}<em t-1="t-1">{t-1}, \mathbf{M}\right)$, considering there may exist multiple positions with $y</em>$ is often concentrated on one location among multiple appearances, indicating the prediction is closely bounded to the location of words.}$ in the source sequence. In practice, $\rho_{t \tau</p>
<p>In a sense $\zeta\left(y_{t-1}\right)$ performs a type of read to $\mathbf{M}$ similar to the attentive read (resulting $\mathbf{c}<em t-1="t-1">{t}$ ) with however higher precision. In the remainder of this paper, $\zeta\left(y</em>$.}\right)$ will be referred to as selective read. $\zeta\left(y_{t-1}\right)$ is specifically designed for the copy mode: with its pinpointing precision to the corresponding $y_{t-1}$, it naturally bears the location of $y_{t-1}$ in the source sequence encoded in the hidden state. As will be discussed more in Section 3.4, this particular design potentially helps copy-mode in covering a consecutive sub-sequence of words. If $y_{t-1}$ is not in the source, we let $\zeta\left(y_{t-1}\right)=\mathbf{0</p>
<h3>3.4 Hybrid Addressing of M</h3>
<p>We hypothesize that Copynet uses a hybrid strategy for fetching the content in $\mathbf{M}$, which combines both content-based and location-based addressing. Both addressing strategies are coordinated by the decoder RNN in managing the attentive read and selective read, as well as determining when to enter/quit the copy-mode.</p>
<p>Both the semantics of a word and its location in $X$ will be encoded into the hidden states in $\mathbf{M}$ by a properly trained encoder RNN. Judging from our experiments, the attentive read of COPYNET is driven more by the semantics and language model, therefore capable of traveling more freely on $\mathbf{M}$, even across a long distance. On the other hand, once COPYNET enters the copy-mode, the selective read of $\mathbf{M}$ is often guided by the location information. As the result, the selective read often takes rigid move and tends to cover consecutive words, including UNKs. Unlike the explicit design for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), COPYNET is more subtle: it provides the archi-</p>
<p>tecture that can facilitate some particular locationbased addressing and lets the model figure out the details from the training data for specific tasks.</p>
<p>Location-based Addressing: With the location information in $\left{\mathbf{h}_{t}\right}$, the information flow</p>
<p>$$
\zeta\left(y_{t-1}\right) \xrightarrow{\text { update }} \mathbf{s}<em t="t">{t} \xrightarrow{\text { predict }} y</em>\right)
$$} \xrightarrow{\text { sel. read }} \zeta\left(y_{t</p>
<p>provides a simple way of "moving one step to the right" on $X$. More specifically, assuming the selective read $\zeta\left(y_{t-1}\right)$ concentrates on the $\ell^{t h}$ word in $X$, the state-update operation $\zeta\left(y_{t-1}\right) \xrightarrow{\text { update }} \mathbf{s}<em t="t">{t}$ acts as "location $\leftarrow$ location+1", making $\mathbf{s}</em>}$ favor the $(\ell+1)^{t h}$ word in $X$ in the prediction $\mathbf{s<em t="t">{t} \xrightarrow{\text { predict }} y</em>}$ in copy-mode. This again leads to the selective read $\hat{h<em t="t">{t} \xrightarrow{\text { sel. read }} \zeta\left(y</em>\right)$ for the state update of the next round.</p>
<p>Handling Out-of-Vocabulary Words Although it is hard to verify the exact addressing strategy as above directly, there is strong evidence from our empirical study. Most saliently, a properly trained COPYNET can copy a fairly long segment full of OOV words, despite the lack of semantic information in its $\mathbb{M}$ representation. This provides a natural way to extend the effective vocabulary to include all the words in the source. Although this change is small, it seems quite significant empirically in alleviating the OOV problem. Indeed, for many NLP applications (e.g., text summarization or spoken dialogue system), much of the OOV words on the target side, for example the proper nouns, are essentially the replicates of those on the source side.</p>
<h2>4 Learning</h2>
<p>Although the copying mechanism uses the "hard" operation to copy from the source and choose to paste them or generate symbols from the vocabulary, COPYNET is fully differentiable and can be optimized in an end-to-end fashion using backpropagation. Given the batches of the source and target sequence ${X}<em N="N">{N}$ and ${Y}</em>$, the objectives are to minimize the negative log-likelihood:
$\mathcal{L}=-\frac{1}{N} \sum_{k=1}^{N} \sum_{t=1}^{T} \log \left[p\left(y_{t}^{(k)} \mid y_{&lt;t}^{(k)}, X^{(k)}\right)\right]$,
where we use superscripts to index the instances. Since the probabilistic model for observing any target word is a mixture of generate-mode and copy-mode, there is no need for any additional labels for modes. The network can learn to coordinate the two modes from data. More specifically, if one particular word $y_{t}^{(k)}$ can be found
in the source sequence, the copy-mode will contribute to the mixture model, and the gradient will more or less encourage the copy-mode; otherwise, the copy-mode is discouraged due to the competition from the shared normalization term $Z$. In practice, in most cases one mode dominates.</p>
<h2>5 Experiments</h2>
<p>We report our empirical study of COPYNET on the following three tasks with different characteristics</p>
<ol>
<li>A synthetic dataset on with simple patterns;</li>
<li>A real-world task on text summarization;</li>
<li>A dataset for simple single-turn dialogues.</li>
</ol>
<h3>5.1 Synthetic Dataset</h3>
<p>Dataset: We first randomly generate transformation rules with $5 \sim 20$ symbols and variables $\mathbf{x} \&amp;$ $\mathbf{y}$, e.g.</p>
<p>$$
a b \quad \mathbf{x} \quad c d \mathbf{y} \text { e } f \longrightarrow g h \mathbf{x} m
$$</p>
<p>with ${\mathrm{a} b \mathrm{c} d \mathrm{e} f \mathrm{ghm}}$ being regular symbols from a vocabulary of size 1,000 . As shown in the table below, each rule can further produce a number of instances by replacing the variables with randomly generated subsequences ( $1 \sim 15$ symbols) from the same vocabulary. We create five types of rules, including " $\mathbf{x} \rightarrow \emptyset$ ". The task is to learn to do the Seq2Seq transformation from the training instances. This dataset is designed to study the behavior of COPYNET on handling simple and rigid patterns. Since the strings to repeat are random, they can also be viewed as some extreme cases of rote memorization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rule-type</th>
<th style="text-align: left;">Examples (e.g. $\mathbf{x}=\mathrm{i} \mathrm{h} \mathrm{k}, \mathbf{y}=\mathrm{j} \mathrm{c}$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbf{x} \rightarrow \emptyset$</td>
<td style="text-align: left;">a b c d $\mathbf{x}$ e $\mathrm{f} \rightarrow \mathrm{c}$ d g</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{x} \rightarrow \mathbf{x}$</td>
<td style="text-align: left;">a b c d $\mathbf{x}$ e $\mathrm{f} \rightarrow \mathrm{c}$ d $\mathbf{x} \mathrm{g}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{x} \rightarrow \mathbf{x x}$</td>
<td style="text-align: left;">a b c d $\mathbf{x}$ e $\mathrm{f} \rightarrow \mathbf{x}$ d $\mathbf{x} \mathrm{g}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{x y} \rightarrow \mathbf{x}$</td>
<td style="text-align: left;">a b y d $\mathbf{x}$ e $\mathrm{f} \rightarrow \mathbf{x} \mathrm{d}$ i g</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{x y} \rightarrow \mathbf{x y}$</td>
<td style="text-align: left;">a b y d $\mathbf{x}$ e $\mathrm{f} \rightarrow \mathbf{x} \mathrm{d}$ y g</td>
</tr>
</tbody>
</table>
<p>Experimental Setting: We select 200 artificial rules from the dataset, and for each rule 200 instances are generated, which will be split into training ( $50 \%$ ) and testing ( $50 \%$ ). We compare the accuracy of COPYNET and the RNN EncoderDecoder with (i.e. RNNsearch) or without attention (denoted as Enc-Dec). For a fair comparison, we use bi-directional GRU for encoder and another GRU for decoder for all Seq2Seq models, with hidden layer size $=300$ and word embedding dimension $=150$. We use bin size $=10$ in beam search for testing. The prediction is considered</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rule-type</th>
<th style="text-align: center;">$\mathbf{x}$</th>
<th style="text-align: center;">$\mathbf{x}$</th>
<th style="text-align: center;">$\mathbf{x}$</th>
<th style="text-align: center;">$\mathbf{x y}$</th>
<th style="text-align: center;">$\mathbf{x y}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\rightarrow \emptyset$</td>
<td style="text-align: center;">$\rightarrow \mathbf{x}$</td>
<td style="text-align: center;">$\rightarrow \mathbf{x x}$</td>
<td style="text-align: center;">$\rightarrow \mathbf{x}$</td>
<td style="text-align: center;">$\rightarrow \mathbf{x y}$</td>
</tr>
<tr>
<td style="text-align: left;">Enc-Dec</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">RNNSearch</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">COPYNET</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: The test accuracy (\%) on synthetic data. correct only when the generated sequence is exactly the same as the given one.</p>
<p>It is clear from Table 1 that COPYNET significantly outperforms the other two on all rule-types except " $\mathbf{x} \rightarrow \emptyset$ ", indicating that COPYNET can effectively learn the patterns with variables and accurately replicate rather long subsequence of symbols at the proper places.This is hard to Enc-Dec due to the difficulty of representing a long sequence with very high fidelity. This difficulty can be alleviated with the attention mechanism. However attention alone seems inadequate for handling the case where strict replication is needed.</p>
<p>A closer look (see Figure 3 for example) reveals that the decoder is dominated by copy-mode when moving into the subsequence to replicate, and switch to generate-mode after leaving this area, showing COPYNET can achieve a rather precise coordination of the two modes.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example output of COPYNET on the synthetic dataset. The heatmap represents the activations of the copy-mode over the input sequence (left) during the decoding process (bottom).</p>
<h3>5.2 Text Summarization</h3>
<p>Automatic text summarization aims to find a condensed representation which can capture the core meaning of the original document. It has been recently formulated as a Seq2Seq learning problem in (Rush et al., 2015; Hu et al., 2015), which essentially gives abstractive summarization since the summary is generated based on a representation of the document. In contrast, extractive summarization extracts sentences or phrases from the original text to fuse them into the summaries, therefore making better use of the overall structure of the original document. In a sense, COPYNET for summarization lies somewhere between
two categories, since part of output summary is actually extracted from the document (via the copying mechanism), which are fused together possibly with the words from the generate-mode.</p>
<p>Dataset: We evaluate our model on the recently published LCSTS dataset (Hu et al., 2015), a large scale dataset for short text summarization. The dataset is collected from the news medias on Sina Weibo ${ }^{1}$ including pairs of (short news, summary) in Chinese. Shown in Table 2, PART II and III are manually rated for their quality from 1 to 5 . Following the setting of (Hu et al., 2015) we use Part I as the training set and and the subset of Part III scored from 3 to 5 as the testing set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">PART I</th>
<th style="text-align: center;">PART II</th>
<th style="text-align: center;">PART III</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">no. of pairs</td>
<td style="text-align: center;">2,400,591</td>
<td style="text-align: center;">10,666</td>
<td style="text-align: center;">1106</td>
</tr>
<tr>
<td style="text-align: left;">no. of score $\geq 3$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8685</td>
<td style="text-align: center;">725</td>
</tr>
</tbody>
</table>
<p>Table 2: Some statistics of the LCSTS dataset.
Experimental Setting: We try COPYNET that is based on character $(+\mathrm{C})$ and word $(+\mathrm{W})$. For the word-based variant the word-segmentation is obtained with jieba ${ }^{2}$. We set the vocabulary size to $3,000(+$ C) and $10,000(+$ W) respectively, which are much smaller than those for models in ( Hu et al., 2015). For both variants we set the embedding dimension to 350 and the size of hidden layers to 500. Following (Hu et al., 2015), we evaluate the test performance with the commonly used ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004), and compare it against the two models in (Hu et al., 2015), which are essentially canonical Encoder-Decoder and its variant with attention.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;"></th>
<th style="text-align: center;">ROUGE scores on LCSTS (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-2</td>
<td style="text-align: center;">R-L</td>
</tr>
<tr>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">+C</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">18.6</td>
</tr>
<tr>
<td style="text-align: left;">(Hu et al., 2015)</td>
<td style="text-align: left;">+W</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">15.8</td>
</tr>
<tr>
<td style="text-align: left;">RNN context</td>
<td style="text-align: left;">+C</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
</tr>
<tr>
<td style="text-align: left;">(Hu et al., 2015)</td>
<td style="text-align: left;">+W</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">COPYNET</td>
<td style="text-align: left;">+C</td>
<td style="text-align: center;">$\mathbf{3 4 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 6}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">+W</td>
<td style="text-align: center;">$\mathbf{3 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Testing performance of LCSTS, where "RNN" is canonical Enc-Dec, and "RNN context" its attentive variant.</p>
<p>It is clear from Table 3 that COPYNET beats the competitor models with big margin. Hu et al. (2015) reports that the performance of a word-based model is inferior to a character-based</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Input(1): 今天上午 4 点中， 下午 结束， 旁在上海 一中以“三元元堂” 起步 早年 离过 的 金属 已从 四川 延达 上海， 其它 科技 同事 部分结束后，再投民事勋恕， 透过 95 岁的奶奶 依然不如他。 今年 4 月，在复旦 上海化学院 讲 研发生的 离过 疑惑 更加 扬鼻 进 程後， 不失身亡。 据民网</p>
<p>Today 9:30，for Fudan presenting case will be told on public trial at the Shanghai Second Intermediate Court. The relatives of the treacherel student Huang Yang has arrived at Shanghai from Sichuan. His father said that they will not
the lawsuit for civil compensation after the criminal action. Huang Yang 9:0 year old grandmother is still unaware of his death. In April, a graduate student at Fudan University Shanghai Medical College, Huang Yang is allegedly
proposed and killed by his roommate Lin Sushao. Reported by Science.</p>
<p>Golden: 扬鼻进程看重今日开审时岁 奶奶说不如他
the case of Lin Sushao presenting is official today, for 9:30 year old grandmother is still unaware of this</p>
<p>RNN context: 复旦投海案: 离详秘遭突发投海凶手已从四川飞往上海，父亲命案另有4人被通知家属不治?</p>
<p>CopyNet: 复旦投海案 今在沪 已公开出现
for Fudan presenting case is on public trial with 16 thoughts</p>
<p>Input(2): 华信兄弟 (100000) 1 在西行投逛后发布公告称，公示期 12 点有效未上记者 16 名投程离过 2 号 影视 综合 电影公司若干股东持有的未完影视 50 % 的股权。对于此项投稿，华信兄弟 遵循 2017 年订单后，“双未在影视的合并及对华信兄弟的投稿的评价均”于2018</p>
<p>Huan Brothers (100000) announced that the company intends to buy with its own fund (or 8 million) of oil (Beijing 8 single Film LTD's make-owned by a number of shareholders of 5 single Film LTD. For this acquisition, the
secretary of the board, Ma Ming, said yesterday. The bringing with 8 single Filters in strengthen Huan Brothers on TV business!</p>
<p>Golden: 华信兄弟股权购买完影视 3 1 号股权
Huan Brothers intends to acquire 5.0% stake of Zhejiang 8 single Film</p>
<p>RNN context: 华信兄弟投稿买完影视5.0%股权，与买完影视合并为“和谐”影视合并的“UNK”和“UNK”的区别?</p>
<p>CopyNet: 华信兄弟股10000 亿 公司买完影视遭破 95 起步 电视联合会
Huan Brothers is intended to act if million acquisition of 8 single Film roommate vehicles strengthen the PV business</p>
<p>Input(3): 工厂、大门案例，约500 万工人被中共建起了，“我们就在普通工人，在这里等工资。”其中一人说道，7月4日上午，记者抵达深圳龙华区清湖湘上的深圳建设
实验室实验公司，采取特点一致，提高实验室的设计、人员和质量，不如更好
[the door of factory is locked. About 1000 miles are re-advised to sit under the shade. We are ordinary workers, waiting for our salary” one of them said. In the morning of July 30, reporters arrived at Xiangjing Photoductron
Corporation located at Qinghu Road, Longbao District, Huazhua, China. The doctor, Xiangjing Photoductron Corporation is closed down and the big shareholder King Yi is missing.</p>
<p>Golden: 深圳亿元清LED全业倒闭 扭打下工人苦等老账
Standard-million CNY worth LED enterprise is closed down and workers wait for the boss under the scorching sun</p>
<p>RNN context: 深圳“4UNK+”深圳4UNK+4UNK+、4UNK+、4UNK+、4UNK+</p>
<p>CopyNet: 国家央电子倒闭 90 万工人投 买完 利润 3
Xiangjing Photoductron Corporation is closed down, so workers are scattered to sit under the shade</p>
<p>Input(4): 截至2012年10月底，全国累计报告艾滋病疫苗感染者和病人498293例、卫生部和“特别接近成为艾滋病的主要传播途径。至2012年9月，艾滋病感染者和病人数累
计报告超过在总 6 位的省份也在为 5 成、广西、河南、四川、新疆和广东、云全国的 24.8 % 。</p>
<p>At the end of October 2013, the national trial of reported HIV infected people and AIDS patients is 430,141 cases. The Health Ministry said equal transmission has become the main route of transmission of AIDS. To September
2013, the six provinces with the most reported HIV infected people and AIDS patients won 8 taoxan, Guangxi, Hunan, Sichuan, Xinjiang and Guangdong, accounting for 73.8% of the country.</p>
<p>Golden: 卫生部、传统疫苗艾滋病主要传播途径
Ministry of Health: broadly transmission became the main route of transmission of AIDS</p>
<p>RNN context: 全国累计报告艾滋病患者和病人4UNK+例艾滋病患者占全国4UNK+%、传统疫苗艾滋病高发人群?</p>
<p>CopyNet: 卫生部、传统疫苗成为艾滋病主要传播途径
Ministry of Health: broadly transmission has become the main route of transmission of AIDS</p>
<p>Input(5): 中国反垄断调查风暴撤销 疫苗的先行者，据德国先进集结和美国先进反垄断股“合同”之后，又对 12 家日本汽车企业加入疫区。记者从业内人士获悉，中国旗下的
是从疫苗后期登录为改善内涵</p>
<p>Chinese antitrust investigation continues to sweep the automotive industry. After Germany itself car and the US Chrysler "6M", there are 12 Japanese car companies involved in the whirlpool. Reporters learned from the insiders
that Triptych Lintzi has been asked to report to the Development and Reform Commission recently.</p>
<p>Golden: 发改委宣布汽车反垄断进程，全国居室护航总数被内涵
the investigation by Development and Reform Commission. Triptych Lintzi has been asked to report</p>
<p>RNN context: 中国居室护航遭发改委约谈; 曾被约谈丰田旗下的居室护航遭发改委约谈负人被约谈</p>
<p>CopyNet: 中国反垄断撤销 疫苗汽车行业 12 家日本汽车企业被发改委约谈
Chinese antitrust investigation continues to sweep the automotive industry. 12 Japanese car companies are asked to report to the Development and Reform Commission</p>
<p>Input(6): 国家卫电视对中国电信政策实施展开了近一倍，这意味着使用了国电电的电动车、电电设施由我发达到展开，但目前由于电解质等技术壁垒，更大规模的汽车和汽修实施成为时过早。
The energy density of Magnesium ion batteries almost doubles that of lithium battery, which means that for the electric vehicles using of magnesium batteries, will last longer even at pure electric power. But currently due to the
technical barriers to the electrolyte, it is still too early for the mass production of hand-replacing lithium batteries.</p>
<p>Golden: 煤电池或者核电站 被要求变更高的煤电站并大势所趋
Lithium batteries will be phased out, magnesium battery with energy density higher will be the future trend</p>
<p>RNN context: 4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+、4UNK+电 电了</p>
<p>CopyNet: 煤离子电池问世，大规模客户报代煤电站
Magnesium ion battery is developed into production of solid-uplux lithium batteries</p>
<p>Input(7): 1. 多数自汽油化学成； 2. 学会融资； 3. 爆出炉； 4. 保持自信； 5. 踏试 6. 尝试； 6. 了解客户的需求； 7. 预测 + 衡量 + 被保； 8. 搞好与各种小bug 级斗争的心态； 9.
发现机遇保持 90% 衡值；
1. moner the child's 11 service finance; 2. understand the law; 3. Be confident; 5. serve trial; 6. understand the need of customers; 7. forecast + measure + ensure; 8. mentally prepared to fight all kinds of small bugs; 9. discover
opportunities and keep the passion of start-up.</p>
<p>Golden: 相关招录者会加的10个技巧
The 10 tips for the first time start-ups</p>
<p>RNN context: 8个分区让你轻松到8个4UNK+与4UNK+，你怎么看懂你的轻松故事吗? (6家)</p>
<p>CopyNet: 国家电信的8个技巧
The 8 tips for success in start-up</p>
<p>Input(8): 9月3日，总部在宁门内瓦的世界经济论坛发布了《2014-2015年全球竞争力报告》，瑞士地球六年位居榜首，成为全球最具竞争力的国家，新加坡和美国全球第二
位和第三位，中国球星第28位，在全球国家中排名最高。
On September 3, the Geneva based World Economic Forum released "The Global Competitiveness Report 2014-2015", Switzerland topped the list for six consecutive years, becoming the world's most competitive country. Singapore
and the United States are in the second and third place respectively. China is in the 28th place, ranking highest among the BBC countries.</p>
<p>Golden: 全球竞争力排位榜 中国前28位居全球国家首位
The Global Competitiveness ranking list, Chinese in the 28th place, the highest among BBC countries</p>
<p>RNN context: 2014-2015年全球竞争力报告:瑞士地球8年居榜首中国居28位(首/3———达榜首)中国排名第28位</p>
<p>CopyNet: 2014-2015年全球竞争力报告，瑞士居民居榜第28
2014-2015 Global Competitiveness Report Switzerland topped and China the 28th</p>
<p>Figure 4: Examples of CopyNet on LCSTS compared with RNN context. Word segmentation is
applied on the input, where OOV words are underlined. The highlighted words (with different colors)
are those words with copy-mode probability higher than the generate-mode. We also provide literal
English translation for the document, the golden, and CopyNet, while omitting that for RNN context
since the language is broken.</p>
<p>one. One possible explanation is that a wordbased model, even with a much larger vocabulary (50,000 words in Hu et al. (2015)), still has a large proportion of OOVs due to the large number of entity names in the summary data and the mistakes in word segmentation. COPYNET, with its ability to handle the OOV words with the copying mechanism, performs however slightly better with the word-based variant.</p>
<h3>5.2.1 Case Study</h3>
<p>As shown in Figure 4, we make the following interesting observations about the summary from CopyNet: 1) most words are from copy-mode, but the summary is usually still fluent; 2) CopyNET tends to cover consecutive words in the original document, but it often puts together segments far away from each other, indicating a sophisticated coordination of content-based addressing and location-based addressing; 3) COPYNET handles OOV words really well: it can generate acceptable summary for document with many OOVs, and even the summary itself often contains many OOV words. In contrast, the canonical RNN-based approaches often fail in such cases.</p>
<p>It is quite intriguing that CopyNet can often find important parts of the document, a behavior with the characteristics of extractive summarization, while it often generate words to "connect" those words, showing its aspect of abstractive summarization.</p>
<h3>5.3 Single-turn Dialogue</h3>
<p>In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test CopyNet on single-turn dialogue. Basically, the neural model learns to generate a response to user's input, from the given (input, response) pairs as training instances.</p>
<p>Dataset: We build a simple dialogue dataset based on the following three instructions:</p>
<ol>
<li>Dialogue instances are collected from Baidu Tieba ${ }^{3}$ with some coverage of conversations of real life e.g., greeting and sports, etc.</li>
<li>Patterns with slots like
hi, my name is $\mathbf{x} \rightarrow$ hi, $\mathbf{x}$
are mined from the set, with possibly multiple responding patterns to one input.
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>3. Similar with the synthetic dataset, we enlarge the dataset by filling the slots with suitable subsequence (e.g. name entities, dates, etc.)
To make the dataset close to the real conversations, we also maintain a certain proportion of instances with the response that 1) do not contain entities or 2) contain entities not in the input.</li>
</ol>
<p>Experimental Setting: We create two datasets: DS-I and DS-II with slot filling on 173 collected patterns. The main difference between the two datasets is that the filled substrings for training and testing in DS-II have no overlaps, while in DS-I they are sampled from the same pool. For each dataset we use 6,500 instances for training and 1,500 for testing. We compare COPYNET with canonical RNNSearch, both character-based, with the same model configuration in Section 5.1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DS-I (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DS-II (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Models</td>
<td style="text-align: center;">Top1</td>
<td style="text-align: center;">Top10</td>
<td style="text-align: center;">Top1</td>
<td style="text-align: center;">Top10</td>
</tr>
<tr>
<td style="text-align: left;">RNNSearch</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">15.9</td>
</tr>
<tr>
<td style="text-align: left;">COPYNET</td>
<td style="text-align: center;">$\mathbf{6 1 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: The decoding accuracy on the two testing sets. Decoding is admitted success only when the answer is found exactly in the Top-K outputs.</p>
<p>We compare CopyNet and RNNSearch on DS-I and DS-II in terms of top-1 and top-10 accuracy (shown in Table 4), estimating respectively the chance of the top-1 or one of top-10 (from beam search) matching the golden. Since there are often many good responses to an input, top10 accuracy appears to be closer to the real world setting.</p>
<p>As shown in Table 4, CopyNet significantly outperforms RNNsearch, especially on DS-II. It suggests that introducing the copying mechanism helps the dialogue system master the patterns in dialogue and correctly identify the correct parts of input, often proper nouns, to replicate in the response. Since the filled substrings have no overlaps in DS-II, the performance of RNNSearch drops significantly as it cannot handle words unseen in training data. In contrast, the performance of COPYNET only drops slightly as it has learned to fill the slots with the copying mechanism and relies less on the representation of the words.</p>
<h3>5.3.1 Case Study</h3>
<p>As indicated by the examples in Figure 5, CopyNET accurately replicates the critical segments from the input with the copy-mode, and generates</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Examples from the testing set of DS-II shown as the input text and golden, with the outputs of RNNSearch and CopyNet. Words in red rectangles are unseen in the training set. The highlighted words (with different colors) are those words with copy-mode probability higher than the generate-mode. Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment on whether the response is appropriate.
the rest of the answers smoothly by the generatemode. Note that in (2) and (3), the decoding sequence is not exactly the same with the standard one, yet still correct regarding to their meanings. In contrast, although RNNSearch usually generates answers in the right formats, it fails to catch the critical entities in all three cases because of the difficulty brought by the unseen words.</p>
<h2>6 Related Work</h2>
<p>Our work is partially inspired by the recent work of Pointer Networks (Vinyals et al., 2015a), in which a pointer mechanism (quite similar with the proposed copying mechanism) is used to predict the output sequence directly from the input. In addition to the difference with ours in application, (Vinyals et al., 2015a) cannot predict outside of the set of input sequence, while COPYNET can naturally combine generating and copying.</p>
<p>COPYNET is also related to the effort to solve the OOV problem in neural machine translation. Luong et al. (2015) introduced a heuristics to postprocess the translated sentence using annotations on the source sentence. In contrast COPYNET addresses the OOV problem in a more systemic way with an end-to-end model. However, as COPYNET copies the exact source words as the output, it cannot be directly applied to machine translation. However, such copying mechanism can be naturally extended to any types of references except for the input sequence, which will help in applications with heterogeneous source and target se-
quences such as machine translation.
The copying mechanism can also be viewed as carrying information over to the next stage without any nonlinear transformation. Similar ideas are proposed for training very deep neural networks in (Srivastava et al., 2015; He et al., 2015) for classification tasks, where shortcuts are built between layers for the direct carrying of information.</p>
<p>Recently, we noticed some parallel efforts towards modeling mechanisms similar to or related to copying. Cheng and Lapata (2016) devised a neural summarization model with the ability to extract words/sentences from the source. Gulcehre et al. (2016) proposed a pointing method to handle the OOV words for summarization and MT. In contrast, COPYNET is more general, and not limited to a specific task or OOV words. Moreover, the softmaxCOPYNET is more flexible than gating in the related work in handling the mixture of two modes, due to its ability to adequately model the content of copied segment.</p>
<h2>7 Conclusion and Future Work</h2>
<p>We proposed COPYNET to incorporate copying into the sequence-to-sequence learning framework. For future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation.</p>
<h2>Acknowledgments</h2>
<p>This work is supported in part by the China National 973 Project 2014CB340301.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lcsts: a large scale chinese short text summarization dataset. arXiv preprint arXiv:1506.05865.</p>
<p>Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. 2015. Neural random-access machines. arXiv preprint arXiv:1511.06392.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain, July. Association for Computational Linguistics.</p>
<p>Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11-19, Beijing, China, July. Association for Computational Linguistics.</p>
<p>Geoffrey J McLachlan and Kaye E Basford. 1988. Mixture models. inference and applications to clustering. Statistics: Textbooks and Monographs, New York: Dekker, 1988, 1.</p>
<p>Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685.</p>
<p>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text conversation. arXiv preprint arXiv:1503.02364.</p>
<p>Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2015. Minimum risk training for neural machine translation. CoRR, abs/1512.02433.</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. arXiv preprint arXiv:1506.06714.</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Highway networks. arXiv preprint arXiv:1505.00387.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Oriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015a. Pointer networks. In Advances in Neural Information Processing Systems, pages 2674-2682.</p>
<p>Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015b. Grammar as a foreign language. In Advances in Neural Information Processing Systems, pages 2755-2763.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ http://tieba.baidu.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
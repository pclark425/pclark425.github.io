<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7073 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7073</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7073</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-bc8e76e9541dd07a094540d262a864b65505cab7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bc8e76e9541dd07a094540d262a864b65505cab7" target="_blank">CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a challenging benchmark dataset for enabling large language models to make use of helpful side information, and finds that models often arrive at the correct final answer through wrong reasoning steps.</p>
                <p><strong>Paper Abstract:</strong> Recent large language models (LLMs) have shown indications of mathematical reasoning ability on challenging competition-level problems, especially with self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting). However, current evaluations mainly focus on the end-to-end final answer correctness, and it is unclear whether LLMs can make use of helpful side information such as problem-specific hints. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. Furthermore, we annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7073.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7073.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concept and Hint-Annotated Math Problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark of 270 high-school competition-level math problems annotated with (1) fine-grained stepwise solutions, (2) problem-level concepts and problem-specific hints, and (3) 'first wrong step' (FWS) annotations on model-generated solutions to enable analyses of multi-step logical reasoning and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provides structured contextual side-information (concepts & hints) for prompting; supports chain-of-thought and partial-solution prompting in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>270 competition-level math problems across five categories (number theory, polynomial, sequence, inequality, combinatorics) annotated with concepts (general math facts), hints (problem-specific tricks), full stepwise solutions, and FWS annotations to evaluate solution generation and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level multi-step mathematical reasoning; solution verification (first-wrong-step detection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final answer accuracy (FAA) and full solution accuracy (FSA); verification metrics include sensitivity/specificity for FWS detection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset itself — used to evaluate models; paper reports FAA and FSA per-model (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Majority-guess baseline (per answer-type) = 33.0% FAA; CHAMP reveals many models above and below that depending on prompt/setting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CHAMP enables analysis showing (a) models sometimes use provided concepts/hints but inconsistently depending on model and how information is presented; (b) FAA substantially overestimates true reasoning (FSA much lower); (c) models struggle to verify solutions (identify FWS), with only GPT-4 and GPT-4 Turbo performing reasonably.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Small scale (270 problems) due to manual curation; potential dataset contamination risk (problems rewritten to mitigate); automatic grading (GPT-4) used for FAA has ~97% validated accuracy but not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7073.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural-language reasoning from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (prompting technique)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought (CoT) prompting: ask model to produce intermediate reasoning steps; used in zero-shot and few-shot variants in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP (experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Elicitation of multi-step solutions for competition math problems; used to measure FAA and FSA under CoT prompting (0-shot, 5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>stepwise mathematical reasoning / proof-style problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA / FSA under 0-shot and few-shot CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT (0-shot/few-shot) yields wide variance across models; e.g., GPT-4 FAA: 0-shot 41.9%, 1/3-partial solution 53.7%, 2/3-partial solution 65.6%; CoT with partial solutions often helps markedly (2/3 partial gave best single FAA 67.0% by GPT-4 Turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Partial-solution CoT and few-shot CoT often outperform simple 0-shot; majority baseline = 33.0% FAA; CoT enables top models to exceed baseline substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT-style prompting (including partial-solution prompting) improves final-answer accuracy for stronger models, but generated chains often contain incorrect intermediate steps; FAA still overestimates sound reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even with CoT, many solutions contain objective errors; CoT improves FAA but does not guarantee correct full solutions (FSA much lower).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7073.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought outputs and aggregates them (e.g., via majority vote) to improve final-answer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (sampling + aggregation technique)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Self-consistency: sampling multiple reasoning paths and selecting the most consistent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH / referenced; CHAMP experiments did not run self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used in prior work to improve CoT outputs; cited as one of the techniques that can be combined with CHAMP setup.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / answer selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>reported in prior literature (not run in CHAMP experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as effective in prior work for selecting better reasoning paths; not directly evaluated in CHAMP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in this paper; may not address logical correctness of intermediate steps, only final-output consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7073.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Concept & Hint Provisioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing problem-specific Concepts and Hints via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of prompting strategies where problem-level general concepts (theorems/formulae) and problem-specific hints are injected into the model context using several provision methods (Direct, Root, Name, Example, Problem, Misleading).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + contextual side-information (prompting design)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Explicitly give concepts and/or hints in context in multiple ways: (1) Directly list concept(s) (Direct), (2) provide root/parent concept (Root), (3) ask model to retrieve concept by name (Name), (4) ask model for example of concept (Example), (5) provide a sample problem using concept with solution (Problem), and (6) provide misleading concepts (Misleading) to test robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CHAMP includes annotations of the relevant concepts and hints for each problem allowing controlled experiments about how to present this information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>use of side information to guide multi-step mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA (final answer accuracy) measured per provisioning method, with and without hints; also FSA for subset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Provision method effects vary by model; no single method dominates. Providing hints (C & H) increased FAA for stronger models by ~10 percentage points on average (GPT-4, GPT-4 Turbo, Llama 3 70B, Mixtral). For some models concepts/hints were detrimental; GPT-4 Turbo dropped >10% relative when given misleading concepts versus No C in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to 'No C' baseline prompt, some models improve with Direct+H or Problem+H; effect size correlated with base capability (larger gains for higher-performing models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>How concepts are provided matters; high-capability models can leverage explicit hints and concepts, improving both FAA and FSA in some settings, but presentation format affects outcome and misleading information can substantially harm some models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Provision sometimes contradicts the model's internal plan and harms performance; not all models make effective use of provided side information; misleading concepts show variable robustness across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7073.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Partial-solution prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partial-solution prompting (1/3 or 2/3 solution revealed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique revealing a fraction of the correct solution steps (one-third or two-thirds) to the model to guide completion of the remaining steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (prompting augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provide the model with a partial correct solution (1/3 or 2/3 of steps) and ask it to complete the rest.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CHAMP experiments tested 1/3 and 2/3 partial-solution prompts as a form of guided CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>completion of stepwise mathematical solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Partial-solution prompting improved FAA substantially for top models: e.g., GPT-4 FAA: 1/3 partial 53.7%, 2/3 partial 65.6%; GPT-4 Turbo FAA: 1/3 partial 61.5%, 2/3 partial 67.0% (best single setting reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Large gains versus 0-shot for strong models; shows that giving correct intermediate steps is a powerful guide.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Partial solutions are highly effective at improving final-answer accuracy for higher-capability models and often more effective than few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>This setup provides privileged information not available in typical test settings; does not measure a model's ability to independently discover reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7073.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solution verification (FWS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solution verification via First Wrong Step (FWS) annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification task where models must read a full solution and (objectively) identify the first incorrect step, or assert no mistake. CHAMP includes 540 model-generated solutions with manually annotated FWS spans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (verification prompting task)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompt model with the student solution (with/without reference solution) and ask to output the first erroneous sentence or 'No mistake'.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP FWS verification set</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>540 annotated model-generated solutions (2 solutions per problem) where first wrong step spans are labeled to evaluate verifier performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>solution verification / error localization in stepwise reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>counts of verdict types (has-FWS, no-mistake, invalid); sensitivity and specificity of FWS detection; detailed confusion categories (early, TP, late, spurious, miss, TN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Most models performed poorly; only GPT-4 and GPT-4 Turbo resembled ground-truth judgement patterns. Many open-source models produced many 'invalid' outputs or failed to detect mistakes. Example: ground truth had 238 'has-FWS' and 32 'no-mistake' in Problem-Only prompt; GPT-4 produced judgments that aligned reasonably, while many models produced >100 invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>No prior baseline in CHAMP; compared across models—GPT-4 and GPT-4 Turbo clearly outperformed other evaluated models for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verification is substantially harder than generation: models often generate but do not understand solutions. Many models either miss FWS or output invalid judgements. Providing reference solution sometimes helps verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verifiers sometimes produce syntactically invalid outputs; sensitivity/specificity vary with prompt and whether reference solution is provided; majority of evaluated models lack robust verification ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7073.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability proprietary large language model evaluated on CHAMP; used as both solver and automatic grader in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary instruction-tuned large transformer-based LLM (OpenAI); used via API for solution generation, grading, and verification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting (0-shot, few-shot), partial-solution prompting, concept/hint provisioning; used as an automatic grader for FAA.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition-level multi-step math problems with concepts & hints; also FWS verification set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step mathematical problem solving and solution verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA (final answer accuracy), FSA (full solution accuracy); verification sensitivity/specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA (examples): 0-shot 41.9%; 1/3-partial 53.7%; 2/3-partial 65.6%. With concepts & hints (Direct + H) FAA ~53.0% (varies by provision method). Verification: GPT-4 produced relatively reasonable judgement distributions and higher true-positive/true-negative counts than most models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially above majority baseline (33.0%) in many settings; benefits from hints (~+10 percentage points on average for FAA compared to no-hint for top models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 shows strong FAA and benefits from partial-solution/hint provisioning, but FSA is substantially lower than FAA, and GPT-4 can still produce correct final answers with incorrect intermediate steps. GPT-4 also is among the best verifiers but not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still produces incorrect stepwise reasoning even when final answer is correct; verification and full-solution correctness not solved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7073.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of GPT-4 evaluated on CHAMP that achieved the highest single-setting FAA reported in the paper (67.0% for 2/3 partial solution prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI transformer LLM variant optimized for speed/cost; used as solver and verifier in CHAMP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting, partial-solution prompting, concept/hint provisioning; also used for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition-level math + FWS verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning and verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA / FSA / verification metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA examples: 0-shot 58.1%; 1/3-partial 61.5%; 2/3-partial 67.0% (best single setting). With concepts & hints, Direct+H and Problem+H improved FAA in many cases (w/ H rows in table: e.g., No C w/ H 62.2% etc.). Misleading concepts caused >10% relative FAA drop for GPT-4 Turbo in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Well above majority baseline (33.0%); exhibits the largest single-setting FAA in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 Turbo is the best-performing model in these experiments by FAA; benefits from hints and partial-solution prompting; however, still produces incorrect intermediate reasoning sometimes and is vulnerable to misleading concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sensitivity to misleading contextual concepts (not robust in presence of intentionally misleading side information); FSA lower than FAA; verification not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7073.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used instruction-tuned LLM evaluated on CHAMP, showing modest performance relative to GPT-4 series.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned OpenAI transformer model (GPT-3.5 family) used as solver in CHAMP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting, concept/hint provisioning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition-level math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA examples: 0-shot ~28.5%; 5-shot ~34.8%; some gains with partial solutions (2/3 partial ~40.7%). With concepts & hints, modest improvements (Direct+H ~31.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Below GPT-4 family and near/under majority baseline in some settings; improved with few-shot and partial solutions but lags top models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 shows modest capability; prone to more incorrect full solutions even when FAA sometimes rises; verification and FSA poor relative to GPT-4 family.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low FSA compared to FAA; many incorrect reasoning steps; frequent failures on verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7073.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2 M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2 Medium model evaluated as a lower-performing baseline in CHAMP experiments; shows low FAA and extremely low FSA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2 family medium-sized transformer model (exact size not specified in paper), evaluated via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting and concept/hint provisioning were applied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition-level math problems and verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math problem solving and verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA and FSA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA examples: 0-shot 14.1%; 2/3 partial 23.3%. FSA: PaLM 2 M produced only 1 fully correct solution out of 136 attempts on 68 problems (≈0.7% FSA) despite FAA ~14.7% in sampled set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Below majority-guess baseline (33.0%) in standard settings; negligible benefit from concepts & hints in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM 2 Medium has low FAA and almost zero FSA in sampled subset, indicating answers often wrong or reasoning flawed even when some final answers match by chance; verification also poor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very limited full-solution correctness; poor verification ability; does not reliably benefit from extra concepts/hints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7073.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 and Llama 3 (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source transformer LLMs (Meta) evaluated at multiple sizes (Llama 2: 7B, 70B; Llama 3: 8B, 70B) showing wide performance spread and generally weaker reasoning on CHAMP than top proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 / Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLM family from Meta evaluated in 7B/8B and 70B sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B/8B/70B (as evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting, prompt-based concept/hint provision tested.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition-level math + FWS verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA examples: Llama 2 7B 0-shot 8.5%; Llama 2 70B 0-shot 11.9%; Llama 3 8B 0-shot ~20.7%; Llama 3 70B 0-shot ~37.8%. Providing hints sometimes improved larger Llama 3 70B (w/ H up to ~47.0% in some provision-methods).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Smaller Llama models often below majority baseline; Llama 3 70B approaches but still below best proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Size and recency matter: Llama 3 70B outperforms older/smaller open-source variants and benefits from hints; still lags GPT-4 family on both FAA and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Many open-source checkpoints produced invalid verification outputs or failed to detect FWS; full-solution correctness remains low compared to proprietary top models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7073.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7073.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral / Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B and Mixtral 8x22B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral family models (Mistral 7B and Mixtral 8x22B) evaluated on CHAMP; Mixtral performs substantially better than many other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B / Mixtral 8x22B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLMs; Mixtral described as '8x22B' feature model used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral), 8x22B (Mixtral)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting, concept/hint provisioning</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CHAMP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Competition math problems and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FAA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FAA examples: Mistral 7B 0-shot 20.7%; Mixtral 8x22B 0-shot 36.7%; Mixtral with partial solutions 2/3 partial ~60.7% (substantial improvement with partial solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mixtral approaches high performance under guided prompts (partial solutions), showing open-source competitive capability in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixtral 8x22B achieves notable FAA under partial-solution prompting and benefits from hints; Mistral 7B is weaker but sometimes improves with provided hints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verification and full-solution correctness still lag top proprietary models; performance sensitive to prompt type.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities", 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>PAL: Program-Aided Language Models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step (PRM800K) <em>(Rating: 2)</em></li>
                <li>FELM: Benchmarking factuality evaluation of large language models <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification <em>(Rating: 2)</em></li>
                <li>The generative ai paradox: "what it can create, it may not understand" <em>(Rating: 1)</em></li>
                <li>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7073",
    "paper_id": "paper-bc8e76e9541dd07a094540d262a864b65505cab7",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CHAMP",
            "name_full": "Concept and Hint-Annotated Math Problems",
            "brief_description": "A new benchmark of 270 high-school competition-level math problems annotated with (1) fine-grained stepwise solutions, (2) problem-level concepts and problem-specific hints, and (3) 'first wrong step' (FWS) annotations on model-generated solutions to enable analyses of multi-step logical reasoning and verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": "Provides structured contextual side-information (concepts & hints) for prompting; supports chain-of-thought and partial-solution prompting in experiments",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "270 competition-level math problems across five categories (number theory, polynomial, sequence, inequality, combinatorics) annotated with concepts (general math facts), hints (problem-specific tricks), full stepwise solutions, and FWS annotations to evaluate solution generation and verification.",
            "task_type": "competition-level multi-step mathematical reasoning; solution verification (first-wrong-step detection)",
            "performance_metric": "final answer accuracy (FAA) and full solution accuracy (FSA); verification metrics include sensitivity/specificity for FWS detection",
            "performance_value": "Dataset itself — used to evaluate models; paper reports FAA and FSA per-model (see model entries).",
            "comparison_with_baseline": "Majority-guess baseline (per answer-type) = 33.0% FAA; CHAMP reveals many models above and below that depending on prompt/setting.",
            "key_findings": "CHAMP enables analysis showing (a) models sometimes use provided concepts/hints but inconsistently depending on model and how information is presented; (b) FAA substantially overestimates true reasoning (FSA much lower); (c) models struggle to verify solutions (identify FWS), with only GPT-4 and GPT-4 Turbo performing reasonably.",
            "limitations": "Small scale (270 problems) due to manual curation; potential dataset contamination risk (problems rewritten to mitigate); automatic grading (GPT-4) used for FAA has ~97% validated accuracy but not perfect.",
            "uuid": "e7073.0",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural-language reasoning from LLMs to improve multi-step problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "transformer (prompting technique)",
            "training_data": null,
            "reasoning_method": "Chain-of-thought (CoT) prompting: ask model to produce intermediate reasoning steps; used in zero-shot and few-shot variants in experiments.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP (experiments)",
            "benchmark_description": "Elicitation of multi-step solutions for competition math problems; used to measure FAA and FSA under CoT prompting (0-shot, 5-shot).",
            "task_type": "stepwise mathematical reasoning / proof-style problem solving",
            "performance_metric": "FAA / FSA under 0-shot and few-shot CoT prompting",
            "performance_value": "CoT (0-shot/few-shot) yields wide variance across models; e.g., GPT-4 FAA: 0-shot 41.9%, 1/3-partial solution 53.7%, 2/3-partial solution 65.6%; CoT with partial solutions often helps markedly (2/3 partial gave best single FAA 67.0% by GPT-4 Turbo).",
            "comparison_with_baseline": "Partial-solution CoT and few-shot CoT often outperform simple 0-shot; majority baseline = 33.0% FAA; CoT enables top models to exceed baseline substantially.",
            "key_findings": "CoT-style prompting (including partial-solution prompting) improves final-answer accuracy for stronger models, but generated chains often contain incorrect intermediate steps; FAA still overestimates sound reasoning ability.",
            "limitations": "Even with CoT, many solutions contain objective errors; CoT improves FAA but does not guarantee correct full solutions (FSA much lower).",
            "uuid": "e7073.1",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency",
            "brief_description": "A technique that samples multiple chain-of-thought outputs and aggregates them (e.g., via majority vote) to improve final-answer selection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "transformer (sampling + aggregation technique)",
            "training_data": null,
            "reasoning_method": "Self-consistency: sampling multiple reasoning paths and selecting the most consistent final answer.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "MATH / referenced; CHAMP experiments did not run self-consistency",
            "benchmark_description": "Used in prior work to improve CoT outputs; cited as one of the techniques that can be combined with CHAMP setup.",
            "task_type": "multi-step reasoning / answer selection",
            "performance_metric": "reported in prior literature (not run in CHAMP experiments)",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "Cited as effective in prior work for selecting better reasoning paths; not directly evaluated in CHAMP experiments.",
            "limitations": "Not evaluated in this paper; may not address logical correctness of intermediate steps, only final-output consistency.",
            "uuid": "e7073.2",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Concept & Hint Provisioning",
            "name_full": "Providing problem-specific Concepts and Hints via prompting",
            "brief_description": "A set of prompting strategies where problem-level general concepts (theorems/formulae) and problem-specific hints are injected into the model context using several provision methods (Direct, Root, Name, Example, Problem, Misleading).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "transformer + contextual side-information (prompting design)",
            "training_data": null,
            "reasoning_method": "Explicitly give concepts and/or hints in context in multiple ways: (1) Directly list concept(s) (Direct), (2) provide root/parent concept (Root), (3) ask model to retrieve concept by name (Name), (4) ask model for example of concept (Example), (5) provide a sample problem using concept with solution (Problem), and (6) provide misleading concepts (Misleading) to test robustness.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "CHAMP includes annotations of the relevant concepts and hints for each problem allowing controlled experiments about how to present this information.",
            "task_type": "use of side information to guide multi-step mathematical reasoning",
            "performance_metric": "FAA (final answer accuracy) measured per provisioning method, with and without hints; also FSA for subset.",
            "performance_value": "Provision method effects vary by model; no single method dominates. Providing hints (C & H) increased FAA for stronger models by ~10 percentage points on average (GPT-4, GPT-4 Turbo, Llama 3 70B, Mixtral). For some models concepts/hints were detrimental; GPT-4 Turbo dropped &gt;10% relative when given misleading concepts versus No C in some settings.",
            "comparison_with_baseline": "Compared to 'No C' baseline prompt, some models improve with Direct+H or Problem+H; effect size correlated with base capability (larger gains for higher-performing models).",
            "key_findings": "How concepts are provided matters; high-capability models can leverage explicit hints and concepts, improving both FAA and FSA in some settings, but presentation format affects outcome and misleading information can substantially harm some models.",
            "limitations": "Provision sometimes contradicts the model's internal plan and harms performance; not all models make effective use of provided side information; misleading concepts show variable robustness across models.",
            "uuid": "e7073.3",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Partial-solution prompting",
            "name_full": "Partial-solution prompting (1/3 or 2/3 solution revealed)",
            "brief_description": "A prompting technique revealing a fraction of the correct solution steps (one-third or two-thirds) to the model to guide completion of the remaining steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "transformer (prompting augmentation)",
            "training_data": null,
            "reasoning_method": "Provide the model with a partial correct solution (1/3 or 2/3 of steps) and ask it to complete the rest.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "CHAMP experiments tested 1/3 and 2/3 partial-solution prompts as a form of guided CoT.",
            "task_type": "completion of stepwise mathematical solutions",
            "performance_metric": "FAA",
            "performance_value": "Partial-solution prompting improved FAA substantially for top models: e.g., GPT-4 FAA: 1/3 partial 53.7%, 2/3 partial 65.6%; GPT-4 Turbo FAA: 1/3 partial 61.5%, 2/3 partial 67.0% (best single setting reported).",
            "comparison_with_baseline": "Large gains versus 0-shot for strong models; shows that giving correct intermediate steps is a powerful guide.",
            "key_findings": "Partial solutions are highly effective at improving final-answer accuracy for higher-capability models and often more effective than few-shot exemplars.",
            "limitations": "This setup provides privileged information not available in typical test settings; does not measure a model's ability to independently discover reasoning steps.",
            "uuid": "e7073.4",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Solution verification (FWS)",
            "name_full": "Solution verification via First Wrong Step (FWS) annotations",
            "brief_description": "A verification task where models must read a full solution and (objectively) identify the first incorrect step, or assert no mistake. CHAMP includes 540 model-generated solutions with manually annotated FWS spans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "transformer (verification prompting task)",
            "training_data": null,
            "reasoning_method": "Prompt model with the student solution (with/without reference solution) and ask to output the first erroneous sentence or 'No mistake'.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP FWS verification set",
            "benchmark_description": "540 annotated model-generated solutions (2 solutions per problem) where first wrong step spans are labeled to evaluate verifier performance.",
            "task_type": "solution verification / error localization in stepwise reasoning",
            "performance_metric": "counts of verdict types (has-FWS, no-mistake, invalid); sensitivity and specificity of FWS detection; detailed confusion categories (early, TP, late, spurious, miss, TN).",
            "performance_value": "Most models performed poorly; only GPT-4 and GPT-4 Turbo resembled ground-truth judgement patterns. Many open-source models produced many 'invalid' outputs or failed to detect mistakes. Example: ground truth had 238 'has-FWS' and 32 'no-mistake' in Problem-Only prompt; GPT-4 produced judgments that aligned reasonably, while many models produced &gt;100 invalid outputs.",
            "comparison_with_baseline": "No prior baseline in CHAMP; compared across models—GPT-4 and GPT-4 Turbo clearly outperformed other evaluated models for verification.",
            "key_findings": "Verification is substantially harder than generation: models often generate but do not understand solutions. Many models either miss FWS or output invalid judgements. Providing reference solution sometimes helps verifiers.",
            "limitations": "Verifiers sometimes produce syntactically invalid outputs; sensitivity/specificity vary with prompt and whether reference solution is provided; majority of evaluated models lack robust verification ability.",
            "uuid": "e7073.5",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A high-capability proprietary large language model evaluated on CHAMP; used as both solver and automatic grader in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary instruction-tuned large transformer-based LLM (OpenAI); used via API for solution generation, grading, and verification experiments.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting (0-shot, few-shot), partial-solution prompting, concept/hint provisioning; used as an automatic grader for FAA.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition-level multi-step math problems with concepts & hints; also FWS verification set.",
            "task_type": "multi-step mathematical problem solving and solution verification",
            "performance_metric": "FAA (final answer accuracy), FSA (full solution accuracy); verification sensitivity/specificity",
            "performance_value": "FAA (examples): 0-shot 41.9%; 1/3-partial 53.7%; 2/3-partial 65.6%. With concepts & hints (Direct + H) FAA ~53.0% (varies by provision method). Verification: GPT-4 produced relatively reasonable judgement distributions and higher true-positive/true-negative counts than most models.",
            "comparison_with_baseline": "Substantially above majority baseline (33.0%) in many settings; benefits from hints (~+10 percentage points on average for FAA compared to no-hint for top models).",
            "key_findings": "GPT-4 shows strong FAA and benefits from partial-solution/hint provisioning, but FSA is substantially lower than FAA, and GPT-4 can still produce correct final answers with incorrect intermediate steps. GPT-4 also is among the best verifiers but not perfect.",
            "limitations": "Still produces incorrect stepwise reasoning even when final answer is correct; verification and full-solution correctness not solved.",
            "uuid": "e7073.6",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4 Turbo",
            "name_full": "GPT-4 Turbo (OpenAI)",
            "brief_description": "A variant of GPT-4 evaluated on CHAMP that achieved the highest single-setting FAA reported in the paper (67.0% for 2/3 partial solution prompting).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "Proprietary OpenAI transformer LLM variant optimized for speed/cost; used as solver and verifier in CHAMP experiments.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting, partial-solution prompting, concept/hint provisioning; also used for verification.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition-level math + FWS verification.",
            "task_type": "multi-step reasoning and verification",
            "performance_metric": "FAA / FSA / verification metrics",
            "performance_value": "FAA examples: 0-shot 58.1%; 1/3-partial 61.5%; 2/3-partial 67.0% (best single setting). With concepts & hints, Direct+H and Problem+H improved FAA in many cases (w/ H rows in table: e.g., No C w/ H 62.2% etc.). Misleading concepts caused &gt;10% relative FAA drop for GPT-4 Turbo in some settings.",
            "comparison_with_baseline": "Well above majority baseline (33.0%); exhibits the largest single-setting FAA in the paper.",
            "key_findings": "GPT-4 Turbo is the best-performing model in these experiments by FAA; benefits from hints and partial-solution prompting; however, still produces incorrect intermediate reasoning sometimes and is vulnerable to misleading concepts.",
            "limitations": "Sensitivity to misleading contextual concepts (not robust in presence of intentionally misleading side information); FSA lower than FAA; verification not perfect.",
            "uuid": "e7073.7",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 Turbo (OpenAI)",
            "brief_description": "A widely-used instruction-tuned LLM evaluated on CHAMP, showing modest performance relative to GPT-4 series.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "Instruction-tuned OpenAI transformer model (GPT-3.5 family) used as solver in CHAMP experiments.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting, concept/hint provisioning in experiments.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition-level math problems.",
            "task_type": "mathematical problem solving",
            "performance_metric": "FAA",
            "performance_value": "FAA examples: 0-shot ~28.5%; 5-shot ~34.8%; some gains with partial solutions (2/3 partial ~40.7%). With concepts & hints, modest improvements (Direct+H ~31.9%).",
            "comparison_with_baseline": "Below GPT-4 family and near/under majority baseline in some settings; improved with few-shot and partial solutions but lags top models.",
            "key_findings": "GPT-3.5 shows modest capability; prone to more incorrect full solutions even when FAA sometimes rises; verification and FSA poor relative to GPT-4 family.",
            "limitations": "Low FSA compared to FAA; many incorrect reasoning steps; frequent failures on verification tasks.",
            "uuid": "e7073.8",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PaLM 2 M",
            "name_full": "PaLM 2 Medium",
            "brief_description": "Google's PaLM 2 Medium model evaluated as a lower-performing baseline in CHAMP experiments; shows low FAA and extremely low FSA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2 Medium",
            "model_description": "PaLM 2 family medium-sized transformer model (exact size not specified in paper), evaluated via API.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting and concept/hint provisioning were applied in experiments.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition-level math problems and verification tasks.",
            "task_type": "math problem solving and verification",
            "performance_metric": "FAA and FSA",
            "performance_value": "FAA examples: 0-shot 14.1%; 2/3 partial 23.3%. FSA: PaLM 2 M produced only 1 fully correct solution out of 136 attempts on 68 problems (≈0.7% FSA) despite FAA ~14.7% in sampled set.",
            "comparison_with_baseline": "Below majority-guess baseline (33.0%) in standard settings; negligible benefit from concepts & hints in many cases.",
            "key_findings": "PaLM 2 Medium has low FAA and almost zero FSA in sampled subset, indicating answers often wrong or reasoning flawed even when some final answers match by chance; verification also poor.",
            "limitations": "Very limited full-solution correctness; poor verification ability; does not reliably benefit from extra concepts/hints.",
            "uuid": "e7073.9",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Llama-family",
            "name_full": "Llama 2 and Llama 3 (various sizes)",
            "brief_description": "Open-source transformer LLMs (Meta) evaluated at multiple sizes (Llama 2: 7B, 70B; Llama 3: 8B, 70B) showing wide performance spread and generally weaker reasoning on CHAMP than top proprietary models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 / Llama 3",
            "model_description": "Open-source instruction-tuned LLM family from Meta evaluated in 7B/8B and 70B sizes.",
            "model_size": "7B/8B/70B (as evaluated)",
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting, prompt-based concept/hint provision tested.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition-level math + FWS verification.",
            "task_type": "math problem solving",
            "performance_metric": "FAA",
            "performance_value": "FAA examples: Llama 2 7B 0-shot 8.5%; Llama 2 70B 0-shot 11.9%; Llama 3 8B 0-shot ~20.7%; Llama 3 70B 0-shot ~37.8%. Providing hints sometimes improved larger Llama 3 70B (w/ H up to ~47.0% in some provision-methods).",
            "comparison_with_baseline": "Smaller Llama models often below majority baseline; Llama 3 70B approaches but still below best proprietary models.",
            "key_findings": "Size and recency matter: Llama 3 70B outperforms older/smaller open-source variants and benefits from hints; still lags GPT-4 family on both FAA and verification.",
            "limitations": "Many open-source checkpoints produced invalid verification outputs or failed to detect FWS; full-solution correctness remains low compared to proprietary top models.",
            "uuid": "e7073.10",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Mistral / Mixtral",
            "name_full": "Mistral 7B and Mixtral 8x22B",
            "brief_description": "Mistral family models (Mistral 7B and Mixtral 8x22B) evaluated on CHAMP; Mixtral performs substantially better than many other open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B / Mixtral 8x22B",
            "model_description": "Open-source transformer LLMs; Mixtral described as '8x22B' feature model used in experiments.",
            "model_size": "7B (Mistral), 8x22B (Mixtral)",
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting, concept/hint provisioning",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CHAMP",
            "benchmark_description": "Competition math problems and verification.",
            "task_type": "mathematical reasoning",
            "performance_metric": "FAA",
            "performance_value": "FAA examples: Mistral 7B 0-shot 20.7%; Mixtral 8x22B 0-shot 36.7%; Mixtral with partial solutions 2/3 partial ~60.7% (substantial improvement with partial solutions).",
            "comparison_with_baseline": "Mixtral approaches high performance under guided prompts (partial solutions), showing open-source competitive capability in some settings.",
            "key_findings": "Mixtral 8x22B achieves notable FAA under partial-solution prompting and benefits from hints; Mistral 7B is weaker but sometimes improves with provided hints.",
            "limitations": "Verification and full-solution correctness still lag top proprietary models; performance sensitive to prompt type.",
            "uuid": "e7073.11",
            "source_info": {
                "paper_title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-Aided Language Models",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step (PRM800K)",
            "rating": 2
        },
        {
            "paper_title": "FELM: Benchmarking factuality evaluation of large language models",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2
        },
        {
            "paper_title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "rating": 2
        },
        {
            "paper_title": "The generative ai paradox: \"what it can create, it may not understand\"",
            "rating": 1
        },
        {
            "paper_title": "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks",
            "rating": 1
        }
    ],
    "cost": 0.0220285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs’ Mathematical Reasoning Capabilities</h1>
<p>Yujun Mao<br>Boston University<br>amao@bu.edu<br>Yoon Kim<br>MIT CSAIL<br>yoonkim@mit.edu<br>Yilun Zhou<br>Salesforce Research<br>yilun.zhou@salesforce.com<br>https://yujunmao1.github.io/CHAMP/</p>
<h4>Abstract</h4>
<p>Recent large language models (LLMs) have shown indications of mathematical reasoning ability on challenging competition-level problems, especially with self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting). However, current evaluations mainly focus on the end-toend final answer correctness, and it is unclear whether LLMs can make use of helpful side information such as problem-specific hints. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problemspecific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1\% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. Furthermore, we annotate modelgenerated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle.</p>
<h2>1 Introduction</h2>
<p>Recent large language models (LLMs) have demonstrated impressive performance on many tasks that previously required specialized models or were thought to be out of reach of conventional neural networks. One such capability is mathematical reasoning: LLMs can often solve simple math problems and make reasonable attempts at challenging, competition-level problems. In addition to model scaling (Kaplan et al., 2020), there are two key factors behind the progress: sophisticated prompting
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our dataset and experiment contribution. (2) We collect 270 challenging, high-school math competition problems (e.g., Find all positive integer solutions to the equation $x^{3}+3=4 y(y+1)$ ). For each problem, we write the relevant and helpful Concepts (e.g., $a^{3} \pm b^{3}=(a+b)\left(a^{2} \pm a b+b^{2}\right)$ ), and Hints (e.g, Express $x^{3}$ as the product of two factors involving $y$ ). (3) In our experiments, to investigate a model's ability to understand and use the additional C $\&amp; \mathrm{H}$ information, we design 17 prompts to evaluate ten models: GPT-3.5 / 4 / 4 Turbo, PaLM 2 Medium, Llama 2 7B / 70B, Llama 3 8B / 70B, Mistral 7B and Mixtral 8x22B. (2) For each problem, we manually judge two model-generated solutions on their correctness, and further annotate the first wrong step of the reasoning (red highlights), if present. (4) This corpus thus serves as a novel dataset for benchmarking and evaluating the solution verification ability of LLMs.
methods such as chain-of-thought (Wei et al., 2022; Kojima et al., 2022) and self-consistency (Wang et al., 2022), which provide useful heuristics for generating and selecting better reasoning paths; and access to calculators or code interpreters which offloads some of the symbolic computation to external tools (Gao et al., 2023; Zhou et al., 2023). However, a direction which remains less explored is how external concepts and hints impact LLMs' reasoning abilities. This is difficult to address with existing datasets which typically only contain problem statements and their solutions, and do not pro-</p>
<p>vide annotated concepts or hints that would be helpful for the problem at hand.</p>
<p>To enable such analyses, we introduce the Concept and Hint-Annotated Math Problems (CHAMP) dataset, which consists of 270 diverse high school competition-level math problems (Fig. 1). In addition to problem statements and full solutions, we annotate each problem with two key pieces of information: concepts and hints. Concepts are general math theorems or formulas, while hints are problem-specific tricks or strategies. The design of CHAMP enables previously under-explored evaluations of multi-step reasoning abilities of LLMs. For example, can LLMs make use of these concepts and hints? How should these be provided to the LLM? Could a model infer useful information from studying sample problems using the same concepts? What happens if the model is provided with irrelevant/misleading concepts?</p>
<p>Using this dataset, we design 17 different prompts, and evaluate various proprietary and opensource models including GPT-3.5 / 4 / 4 Turbo (OpenAI, 2023), PaLM 2 Medium (Anil et al., 2023) ${ }^{1}$, Llama 2 7B / 70B (Touvron et al., 2023), Llama 3 8B / 70B (AI, 2024), Mistral 7B (Jiang et al., 2023) and Mixtral 8x22B (Team, 2024). While we observe a diverse range of behaviors across different models and prompts, we find the accuracy of the best setting to be $67.0 \%$ only (measured by the final answer correctness, ignoring any possible errors in intermediate reasoning steps), and gains from the additional concept and hint information vary across models. The results indicate a large room for improvement with competitionlevel math for LLMs, and moreover highlight the utility of CHAMP for developing and benchmarking future models.</p>
<p>For each problem, we further analyze solutions generated by these models by manually annotating the first wrong step in the reasoning process, such as an arithmetic error or question misunderstanding, or validating that the solution is fully correct. This annotation serves two purposes. First, it concretely identifies how much the final answer accuracy (which is the predominant practice for math problem evaluations) over-estimates a model's ability to generate fully correct solutions: indeed, we find that in many instances, a model gets the answer "right" despite generating wrong reasoning steps,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>indicating that these models are potentially relying on shortcut heuristics. Second, we can evaluate the verification ability of any model, i.e., how well it can reason about a given solution and identify any errors, where we find that most models struggle. The above evaluations suggest key deficiencies in current LLMs and highlight the value of these annotations as an additional benchmarking resource.</p>
<p>In summary, we evaluate four model capabilities using our dataset: generating correct final answer, generating correct full solution, using the contextual C \&amp; H information, and verifying a given solution. Our findings uncover new strengths and limitations of current models, and give directions for future work on improving them.</p>
<h2>2 Background and Related Work</h2>
<p>Math datasets and benchmarks. Large language models have seen significant improvement in understanding and solving math problems, with GPT-4 (OpenAI, 2023) being able to tackle most math problems that require grade-level knowledge and direct applications of formulas, even for problems with diverse formats and wordings (Cobbe et al., 2021). Nonetheless, they still struggle with competition-level problems, such as those found in the MATH dataset (Hendrycks et al., 2021). Competition-level problems-for which applications of formulas are not straightforward-are therefore the focus of our CHAMP dataset.</p>
<p>A key distinction of CHAMP compared to other math datasets is the information associated with each problem. In addition to the problem text and its solution-which are common components of such datasets-we annotate relevant concepts and hints and further label them on each solution step, with problems relating to each other via common math concepts (e.g., Fermat's little theorem). In this way, CHAMP enables fine-grained evaluations of mathematical problem solving abilities of LLMs that are not possible with other datasets, for example allowing for different types of additional information to be made available in the context through prompting.</p>
<p>We observe that many techniques seek to improve an LLM's mathematical reasoning ability, such as encouraging chain-of-thought generation (Wei et al., 2022), selecting a final result from multiple sampled outputs (Wang et al., 2022), and using external tools such as a calculator or Python interpreter (Gao et al., 2023) to eliminate arithmetic</p>
<p>errors. These directions can be combined with our experimental setup.</p>
<p>Solution verification ability of LLMs. Another distinguishing factor of our dataset is the first wrong step annotations on model solutions, which enables more fine-grained model analyses and, more importantly, evaluations of how well models can verify a given answer.</p>
<p>There have been recent attempts at crafting such datasets. For example, <em>Lightman et al. (2023)</em> collected PRM800K, containing 800K steps of 75K solutions to 12K problems in the MATH dataset <em>Hendrycks et al. (2021)</em>, with each step labeled as correct, incorrect or neutral. <em>Chen et al. (2023)</em> curated FELM, a factuality benchmark, including annotations of solutions to 208 GSM8K <em>Cobbe et al. (2021)</em> and 194 MATH problems. Compared to CHAMP, where annotations are made exclusively by the paper authors, both PRM800K and FELM are labeled via crowdsourcing. Moreover, solutions in PRM800K are selected to maximally confuse a reward model being developed in the project, while FELM uses only GPT-3.5 as the solution generator. In contrast, our 540 annotated solutions are generated by a mix of GPT-3.5, 4, 4 Turbo and PaLM 2 Medium, each with varying capabilities.</p>
<p>Roles of contexts. Our dataset and experiments are similar in spirit to works that explore how well LLMs understand different contexts, which have yielded surprising findings. For example, models can be insensitive to label correctness <em>Min et al. (2022)</em> but sensitive to label distribution <em>Zhao et al. (2021)</em> and exemplar ordering <em>Lu et al. (2021)</em>. <em>McKenzie et al. (2023)</em> find that larger LLMs resist absorbing context information inconsistent with world knowledge acquired during training (e.g., redefining $\pi=432$). Similarly, <em>Wu et al. (2023)</em> find that LLMs perform worse in atypical setups for common tasks (e.g., base-9 integer addition). With CHAMP, we can explore how different information supplied in various ways affect LLMs' behavior.</p>
<h2>3 The CHAMP Dataset</h2>
<p>This section describes the dataset structure and construction. Due to the high level of math expertise required, the dataset curation is carried out exclusively by the paper authors.</p>
<p>Problems. We select problems from the book Problem-Solving Strategies by <em>Engel (2008)</em>, a classic piece of material for high-school math com-</p>
<p>Problem ID: P_Inequality_36
Problem: For non-negative $a, b, c, d$, what is the smallest value of $\sqrt{(a+c)(b+d)}-\sqrt{a{b}}-\sqrt{c{d}}-1$?</p>
<p>Concepts and Hints:
H1. Compare $\sqrt{(a+c)(b+d)}$ with $\sqrt{a{b}}+\sqrt{c{d}}$ by squaring both terms.
C1. $(x+2)^{2}=x^{2}+2xy+y^{2}$.
C2. For non-negative $x$, $y$, we have $(x+y) \geq 2 \sqrt{x y}$ and $(x+y) / 2 \geq \sqrt{x y}$, with equality if and only if $x=y$.
C3. For non-negative $x, y, \sqrt{x} \geq \sqrt{y}$ if and only if $x \geq y$.</p>
<p>Answer: -1
Solution Step:</p>
<ol>
<li>We have $\sqrt{(a+c)(b+d)}^{2}=(a+c)(b+d)=a b+$ $a d+b c+c d$. [H1]</li>
<li>We have $(\sqrt{a b}+\sqrt{c d})^{2}=a b+c d+2 \sqrt{a b c d}$. [H1, C1]</li>
<li>Thus, $\sqrt{(a+c)(b+d)}^{2}-(\sqrt{a b}+\sqrt{c d})^{2}=a d+b c-$ $2 \sqrt{a b c d}$, which is non-negative because $a d+b c \geq$ $2 \sqrt{a b c d}$. [H1, C2]</li>
<li>Thus, $\sqrt{(a+c)(b+d)}^{2} \geq \operatorname{sqrt}(a b)+\sqrt{(c d)}^{2}$.</li>
<li>Since $a, b, c, d$ are all non-negative, we have $\sqrt{(a+c)(b+d)} \geq \sqrt{a b}+\sqrt{c d}$. [C3]</li>
<li>So the smallest value of $\sqrt{(a+c)(b+d)}-\sqrt{a b}-\sqrt{c d}-$ 1 is -1 , achieved when $a=b=c=d$.</li>
</ol>
<p>Table 1: A sample from the CHAMP dataset, which shows the problem (top), the concepts and hints (middle), and the full solution (bottom).
petitions. All problems require specific tricks or creative strategies, rather than routine knowledge applications. We require problems to have final check-able answers for easy evaluation, and thus rewrite proof problems where possible (e.g., "Prove $f(x) \geq 1$ for $x \geq 0$ " is transformed to "What is the smallest value of $f(x)$ for $x \geq 0$ "). A total of 270 problems span five categories: number theory (80), polynomial (50), sequence (50), inequality (50) and combinatorics (40). We make some adaptations to lessen the impact of "trivial limitations" of LLMs, such as weakness in precise arithmetics. See App. A for further details.</p>
<p>For each problem, we manually verify and write out the full detailed step-wise solution in natural language, as the solution manual often skips steps and occasionally contains typographical errors. We also provide an explicit final answer that can be checked against.</p>
<p>Concepts and hints. We additionally annotate relevant concepts and hints, which provide helpful information to solve a problem. For the purposes of this paper, we define concepts to mean general math knowledge, such as an equation or a theorem, for example, " $x^{2}-y^{2}=(x+y)(x-y)$ ". We de-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The distribution of dataset statistics in CHAMP. Problems in CHAMP require a nontrivial number of reasoning steps (6.0 on average). Each problem is linked to an average of 1.4 concepts and 1.7 hints.</p>
<p>Fine hints, on the other hand, to be problem-specific tricks or strategies, such as "Add 1 to both sides." (to prepare for further simplification). These concepts and hints are also labeled to the relevant solution steps in the ground truth solution.</p>
<p>Each concept is additionally annotated with three metadata fields: the <em>category</em>, such as "number theory" or "polynomial"; the <em>name</em>, such as "difference of squares formula" for the concept $x^2 - y^2 = (x + y)(x - y)$; and the <em>parent concept</em>, which is a general form of the concept. For example, both $(x + y)^2 = x^2 + 2xy + y^2$ and $(x + y)^3 = x^3 + 3x^2y + 3xy^2 + y^3$ have the parent concept of the binomial theorem, which states $(x + y)^n = \sum_{k=0}^n \binom{n}{k}x^{(n-k)}y^k$. While every concept is assigned a category, not all concepts have names or parent concepts. One CHAMP problem with its solution and concept &amp; hint annotation is shown in Tab. 1; additional examples are given in Tab. 10 of App. B.</p>
<p><strong>First wrong step (FWS) annotations.</strong> For every problem, we further analyze the LLM-generated solutions to identify mistakes in reasoning steps. Specifically, we randomly assign a "generator model," one of GPT-3.5, GPT-4, GPT-4 Turbo and PaLM 2 M, to each problem, stratified by the problem's category (i.e., number theory, polynomial, etc.), and collect the model-generated solutions for two prompts: one with the problem statement only (the "No C + w/o H" prompt in Tab. 2 of Sec. 4.2) and another with the problem statement supplemented by the list of relevant concepts and hints (the "Direct + w/H" in Tab. 2 of Sec. 4.2). We assess whether the <em>full solution</em> (including the reasoning steps) is correct, and if not, annotate the occurrence of the first wrong step (FWS) as a span in the text. The FWS refers to an objective mistake, such as an arithmetic error or question misunderstanding, and does not contain subjective assessments, such as a seemingly implausible strategy to the problem.</p>
<p><strong>Dataset statistics.</strong> We collect 270 problems, 54 concepts, 330 hints and 540 FWS annotations. Every problem has at least 1 concept or hint, with an average of 1.4 concepts and 1.7 hints, and an average of 6.0 solution steps. Each problem statement has an average of 20.2 words, and each solution step 10.9 words. Fig. 2 plots the distribution histograms of these statistics.</p>
<h1>4 Experiment 1: Problem Solving</h1>
<h2>4.1 Experimental Setup</h2>
<p>We evaluate ten models: GPT-3.5 Turbo (16k context version), GPT-4, GPT-4 Turbo, PaLM 2 Medium, Llama 2 7B, Llama 2 70B, Llama 3 8B, Llama 3 70B, Mistral 7B and Mixtral 8x22B (exact model versions in Tab. 2). We set temperature to 0 for all experiments, and allow at least 1024 tokens to be generated for each model depending on the specific context window size, which is more than enough to output the correct reasoning, with unfinished generations treated as incorrect. Inference for proprietary models is done via the respective API, and that for open-source models is done via the together.ai API.</p>
<p><strong>Baseline.</strong> Sometimes models can output correct final answers with incorrect reasoning, especially when those answer values appear often in the training corpus. To contextualize the final answer accuracy metrics, we construct the majority guess baseline as follows. For each of the four answer types in our dataset—numeric answers (e.g., 0), expression answers (e.g., <em>n</em>), yes/no answers (e.g., no) and enumeration answers (e.g., <em>x</em> = 1 or <em>x</em> = −2)—the baseline guesses the most frequently appearing answer values in the ground truth. This baseline accuracy is 33.0%. See Tab. 11 of App. B for more details.</p>
<p><strong>Automatic evaluation.</strong> As is the case with many generation tasks, the correct final answer can be expressed in multiple ways: "no solution" is equiv-</p>
<p>| Model | Standard Evaluations | | | | CHAMP-Enabled Evaluations (7 Ways to Provide Concepts With or Without Hints) | | | | | | | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | 0-Shot | 5-Shot | 1/3 Soln | 2/3 Soln | 1. No C | 2. Direct | 3. Root | 4. Name | 5. Example | 6. Problem | 7. Misleading |
| GPT-3.5 | 28.5 | 34.8 | 33.7 | 40.7 | w/o H | 28.5 | 28.1 | 28.1 | 33.0 | 30.4 | 30.0 | 27.8 |
| gpt-3.5-turbo-10k-0613 | w/ H | 31.9 | 33.0 | 33.0 | 31.9 | 31.9 | 34.4 | 31.5 |
| GPT-4 | 41.9 | 38.1 | 53.7 | 65.6 | w/o H | 41.9 | 43.0 | 42.6 | 40.0 | 42.2 | 43.7 | 39.3 |
| gpt-4-0613 | w/ H | 51.9 | 53.0 | 49.6 | 49.3 | 52.2 | 53.1 | 48.9 |
| GPT-4 Turbo | 58.1 | 53.0 | 61.5 | 67.0 | w/o H | 58.1 | 57.0 | 55.6 | 51.1 | 55.9 | 55.2 | 51.9 |
| gpt-4-1100-process | w/ H | 62.2 | 65.9 | 64.8 | 63.0 | 63.3 | 64.4 | 55.6 |
| PaLM 2 Medium | 14.1 | 17.4 | 15.9 | 23.3 | w/o H | 14.1 | 15.6 | 15.6 | 14.1 | 17.0 | 16.7 | 19.3 |
| cbad-litima-005 | w/ H | 14.4 | 15.2 | 15.2 | 18.1 | 16.7 | 20.7 | 19.3 |
| Llama 2 7B | 8.5 | 9.6 | 7.4 | 10.7 | w/o H | 8.5 | 7.8 | 8.1 | 6.7 | 9.3 | 10.0 | 9.3 |
| Llama-2-7b-clan-6f | w/ H | 9.6 | 7.4 | 9.3 | 7.4 | 10.0 | 11.1 | 8.9 |
| Llama 2 70B | 11.9 | 13.0 | 14.8 | 21.5 | w/o H | 11.9 | 13.3 | 11.1 | 14.1 | 13.7 | 13.3 | 12.6 |
| Llama-2-70b-clan-6f | w/ H | 15.6 | 15.6 | 15.9 | 15.9 | 16.7 | 15.6 | 16.3 |
| Llama 3 8B | 20.7 | 22.2 | 25.2 | 35.9 | w/o H | 21.1 | 23.0 | 24.1 | 24.8 | 20.0 | 24.8 | 20.7 |
| Meta-Llama-3.8b-feature | w/ H | 23.7 | 25.6 | 25.6 | 29.3 | 26.3 | 25.9 | 21.5 |
| Llama 3 70B | 37.8 | 35.9 | 47.0 | 59.6 | w/o H | 37.8 | 40.0 | 37.0 | 40.0 | 38.1 | 37.0 | 35.6 |
| Meta-Llama-3-70b-feature | w/ H | 47.0 | 49.6 | 48.9 | 47.4 | 50.0 | 48.9 | 42.2 |
| Mistral 7B | 20.7 | 16.3 | 18.1 | 24.8 | w/o H | 20.7 | 20.4 | 20.0 | 20.7 | 19.3 | 19.6 | 18.1 |
| Mistral-7B-feature-v0.3 | w/ H | 18.1 | 19.3 | 17.0 | 20.0 | 18.9 | 22.2 | 16.7 |
| Mistral 8x22B | 36.7 | 36.7 | 47.0 | 60.7 | w/o H | 36.7 | 38.1 | 39.3 | 32.6 | 34.1 | 38.5 | 32.2 |
| Mistral-8x22B-feature-v0.1 | w/ H | 47.4 | 48.5 | 49.6 | 47.0 | 46.3 | 50.0 | 45.2 |
| Majority Guessthe | 33.0 |  |  |  |  |  |  |  |</p>
<p>Table 2: Final answer accuracy (in percentage) with the different prompt settings.
alent to "none", "unsolvable" is equivalent to "impossible to solve", etc. Therefore a simple criteriabased exact or sub-string match to a manually constructed set of valid answers is prone to false negatives. We thus propose to use GPT-4 as an automatic grader, and use a three-stage procedure for the solution generation and evaluation. First, we prompt the model for its step-by-step solution. Then, we ask the model to produce a one-sentence summarization of its answer. Finally, we use GPT- 4 to grade the answer summary, given the ground truth final answer, which essentially checks for semantic equivalence between the two. The prompts are listed in Tab. 12 and 13 of App. C.</p>
<p>To assess the validity of this automatic evaluation procedure, we manually checked 500 examples for (1) whether the one-sentence summarization was correct and (2) whether GPT-4 was able to correctly grade the summarized solution given the ground truth final answer. While not perfect, we found GPT-4 to be quite good at this, with the accuracies on both tasks being $\geq 97 \%$.</p>
<h3>4.2 Model Analyses</h3>
<p>Our experiments are aimed at evaluating four different aspects: raw model performance, effectiveness of different ways to provide concepts, relative importance of concepts and hints, and impact of irrelevant (i.e., misleading) concepts. The quantitative results are summarized in Tab. 2.</p>
<p>Model performance. We first study the model performance with both zero-shot (Kojima et al., 2022) and few-shot (Wei et al., 2022) chain-ofthought prompting. Following the experiments by (Hendrycks et al., 2021), we also study models’ performance when given partial solution (1/3 and 2/3 of solution steps) under the zero-shot setup. The prompts are listed in Tab. 14-16 of App. C.</p>
<p>The blue cells of Tab. 2 summarize these results, with some full model-generated solutions presented in Tab. 22 and 23 of App. D. Generally, larger and more recent models perform better than their smaller and earlier versions, and partial solutions are mostly helpful in guiding the model to correct solutions, largely consistent with the findings by Hendrycks et al. (2021). Overall, the best performing models, GPT-4 and GPT-4 Turbo, are still proprietary ones and there is a gap to close for (even the latest) open-source models. In addition, five-shot prompting is often not beneficial, suggesting that such instruction-tuned models, especially high-performing ones, may not need in-context exemplars to activate the "problem-solving mode," as long as the instruction is sufficiently clear.</p>
<p>Concept provision method. As concepts are general facts or formulas in math, they are likely already learned by the model during pre-training. However, different ways to provide the knowledge in the context may affect how well the model can understand and use it. We designed six concept provision approaches, each in two versions where</p>
<p>the hint is withheld or provided, corresponding to the light green cells and dark green cells of Tab. 2:</p>
<ol>
<li>Prompt with no concepts (No C).</li>
<li>Directly provide the concept in the prompt (Direct).</li>
<li>Provide the root concept up the parent-child chain (i.e., most general form) in the prompt (Root).</li>
<li>Ask the model to retrieve the concept by its name in a separate round of conversation (Name).</li>
<li>Ask the model to provide an example for the concept in a separate round of conversation (Example).</li>
<li>Provide a sample problem that uses the concept and its step-by-step solution (Problem).</li>
</ol>
<p>The specific prompts are listed in Tab. 17-21 of App. C. The best performing concept provision method for each model with and without hints is bolded. No single method performs the best across the board. Furthermore, concepts may sometimes even be detrimental, potentially because they contradict with the model's "initial planned approach."</p>
<p>Importance of hints. Compared to concepts, hints are arguably more valuable as they often require creativity from the test-takers. The performance contrast without and with the hint under each prompt are shown by the light green rows vs. dark green rows of Tab. 2. While providing hints helps, the performance increase is correlated with the model's "base capability": GPT-4, 4 Turbo, Llama 3 70B and Mixtral 8x22B, which have the best zero-shot accuracy, are able to score $10 \%$ higher on average with hints, while the accuracy increase is much less notable for other models.</p>
<p>Impact of misleading concepts. How well could a model deal with misleading concepts? In this experiment, for each useful concept, we replace it with a random one of the same category (to ensure maximal confusion) but not on the path to its root concept (to avoid accidentally providing useful information). ${ }^{2}$ The results are summarized in the orange cells of Tab. 2. Compared to the "No C" setup, misleading concepts have different impacts on different models: while most of the models show slight drop of accuracy with misleading concepts, GPT-4 Turbo suffers the most, with</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Final answer vs. full solution accuracy for four models under two prompts.
over $10 \%$ relative decrease of accuracy compared with the "No C" setup. On the other hand, PaLM 2 Medium and both Llama 2 models even show some improvement, indicating that they are unlikely to understand and act on provided (mis)information.</p>
<h3>4.3 Full Solution Accuracy</h3>
<p>The above analyses are based on an important assumption: that the final answer accuracy is a faithful reflection of the model's mathematical ability. However, focusing on final answer alone could inflate the models' performance as incorrect reasoning could lead to correct final answers, especially for questions with yes/no answers (see examples in Tab. 22 and 23 of App. D). As a result, we proceed to examine the full solutions generated by four models: GPT-3.5, GPT-4, GPT-4 Turbo and PaLM 2 Medium, based on the first wrong step (FWS) annotation.</p>
<p>Tab. 3 displays the final answer accuracy (FAA) and full solution accuracy (FSA) of model outputs from the 0 -shot problem-only prompt and the problem + concept \&amp; hint list prompt. ${ }^{3}$ Full solution accuracy (FSA) is significantly lower than final answer accuracy (FAA), suggesting that the latter is indeed an inflated measurement of the models' true reasoning ability. As an extreme case, PaLM 2 M produces only one fully correct solution out of 136 attempts on 68 problems, despite still achieving $14.7 \%$ FAA. Given that almost all benchmarks (e.g. Cobbe et al., 2021; Hendrycks et al., 2021) are based on FAA, true model performance may not be as high as previous benchmarking results suggest. Nonetheless, performance of the GPT models increases under both prompts, regardless of the evaluation metrics of FSA or FAA, suggesting that FAA could likely be a proxy for FSA.</p>
<p>For all GPT models, providing the C \&amp; H list significantly helps with FSA, even when FAA stays at a similar level (among the respective problem sub-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>You need to grade a student's answer to a math problem and determine if it contains any objective error, including but not limited to mistakes in logical deductions, algebraic manipulations, arithmetic calculations or question understanding. You should not make any subjective judgment, such as marking a strategy that seems unsuccessful as incorrect, unless you have objective evidence of an error. For your convenience, a reference solution is also given. However, the student answer could differ from it significantly but still be correct, by, for example, using a different strategy.
Feel free to think step by step through the student answer. On the last line, write "Judgment: ", followed by your judgment, in one of two cases:</p>
<ol>
<li>If you think that the answer is wrong, copy the sentence containing the first error (i.e.: "Judgment: <verbatim sentence containing the first error>").</li>
<li>If you think that the solution is fully correct, write "No mistake" as your judgment (i.e.: "Judgment: No mistake").
Question Statement:
("No C + w/o H" or "Direct + w/ H" Prompt)
Reference Solution:
(Ground Truth Step-by-Step Solution)
Student Answer:
(Solution Under Judgment)</li>
</ol>
<p>Figure 3: The prompt for model verification evaluation. Text in black is given as the default, and we experiment with several variations. (1) we choose to give or withhold the reference solution in the prompt, where the blue italic texts are not provided in the latter case. (2) we evaluate model solutions for two prompts - problem only and problem with concept and hint list. (3) the corresponding solution is given as the "Student Answer".
set). By comparison, PaLM 2 M could not benefit from additional information. These results imply the necessity for the finer-grained evaluations as we test LLMs on more challenging benchmarks.</p>
<h2>5 Experiment 2: Solution Verification</h2>
<p>The first wrong step (FWS) annotation allows for evaluating whether LLMs can read a solution to a problem and verify its correctness, which is a much more difficult task than comparing the final answer against the ground-truth answer. In this set of experiments, we study how well LLMs can judge model-generated solutions, as well as the (error-free) ground truth solution.</p>
<h3>5.1 Experimental Setup</h3>
<p>As noted in Sec. 3, the FWS dataset is obtained by (1) randomly assigning a model (out of GPT-3.5, GPT-4, GPT-4 Turbo and PaLM 2 M) to each problem and collecting its solutions for two prompts-</p>
<p>Problem Only (i.e., "No C + w/o H" in Tab. 2) and Problem + C \&amp; H (i.e., "Direct + w/H" in Tab. 2)— and then (2) manually annotating the first wrong step in each solution (or label it to be fully correct if there are no wrong steps). We first evaluate the ten models on each annotated solution, using the prompt shown in Fig. 3. Two variants are explored: one where the reference solution is not given (by removing all blue italic texts) and the other where the reference solution is given (by using the complete prompt). This prompt both allows the model to engage in chain-of-thought reasoning and moreover enables easy parsing of the output, via the line starting with "Judgment:".</p>
<h3>5.2 Results</h3>
<p>Output judgment type. We first study the judgment types from models (regardless of judgment correctness), with three categories: has-FWS, where the model identifies a FWS, no-mistake, where the model outputs "Judgment: No mistake", and invalid, where the sentence following "Judgment:" is not in the solution or this line is not found.</p>
<p>Tab. 4 lists the count breakdown for each of the three judgment types, in that respective order. Compared to the ground truth distribution, only GPT-4 and 4 Turbo behave reasonably, with other models either producing a large number of invalid judgments (e.g., Llama 27B and Mistral 7B), or failing to identify mistakes in most solutions (e.g., PaLM 2 Medium and Llama 2 70B). The results indicate that despite the ability to get correct final answers on challenging math problems, most models lack strong verification abilities or even present difficulty understanding and following the verification task prompt. For more detailed analyses, we focus on GPT-4 and 4 Turbo, which produce few invalid answers and best resemble the ground truth judgement patterns.</p>
<p>Judgment correctness analysis. For syntactically valid judgements (has-FWS and no-mistake), we evaluate their outputs in more depth. For each of the verifier model of GPT-4 and 4 Turbo, we consider 4 setups by varying two factors: the Problem Only or Problem + C \&amp; H prompt, and with or without reference solution. Tab. 5 shows the counts of different model vs. ground truth judgments.</p>
<p>We compute sensitivity and specificity to quantify the two verifiers' FWS identification accuracy. Recall that true-positive (TP) is the case where the</p>
<table>
<thead>
<tr>
<th></th>
<th>Problem Only</th>
<th>Problem + C &amp; H</th>
</tr>
</thead>
<tbody>
<tr>
<td>w/o Ref Soln</td>
<td>w/ Ref Soln</td>
<td>w/o Ref Soln</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>6</td>
<td>228</td>
</tr>
<tr>
<td>GPT-4</td>
<td>180</td>
<td>86</td>
</tr>
<tr>
<td>GPT-4 T</td>
<td>203</td>
<td>56</td>
</tr>
<tr>
<td>PaLM 2 M</td>
<td>0</td>
<td>233</td>
</tr>
<tr>
<td>Llama 2 7B</td>
<td>124</td>
<td>20</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>0</td>
<td>264</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>3</td>
<td>252</td>
</tr>
<tr>
<td>Llama 3 70B</td>
<td>93</td>
<td>128</td>
</tr>
<tr>
<td>Mistral 7B</td>
<td>0</td>
<td>73</td>
</tr>
<tr>
<td>Mistral 8x22B</td>
<td>57</td>
<td>123</td>
</tr>
<tr>
<td>Ground truth</td>
<td>238</td>
<td>32</td>
</tr>
</tbody>
</table>
<p>Table 4: Count breakdown of judgment types produced by each model. In each cell, the three numbers represent the number of has-FWS, no-mistake and invalid judgments. Each triplet sums up to 270, the total number of problems (and annotated solutions for the prompt). The ground truth statistics are shown on the last line.</p>
<table>
<thead>
<tr>
<th>Setup</th>
<th>Model vs. Ground Truth Judgment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Verifier</td>
<td>Verifier</td>
</tr>
<tr>
<td></td>
<td>early</td>
</tr>
<tr>
<td>GPT-4</td>
<td>✗</td>
</tr>
<tr>
<td></td>
<td>✗</td>
</tr>
<tr>
<td></td>
<td>✓</td>
</tr>
<tr>
<td></td>
<td>✓</td>
</tr>
<tr>
<td>GPT-4</td>
<td>✗</td>
</tr>
<tr>
<td></td>
<td>✗</td>
</tr>
<tr>
<td></td>
<td>✓</td>
</tr>
<tr>
<td></td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>Table 5: Detailed analysis of FWS identification. “Verifier” is the model under evaluation. A cross mark under “C &amp; H” uses the Problem Only (“No C + w/o H”) prompt and its solution, and a check mark uses the Problem + C &amp; H (“Direct + w/ H”) prompt. A cross mark under “Ref Soln” withholds the reference solution, and a check mark reveals it. For the six judgment types, red highlighting marks the ground truth (GT) FWS span, a green check mark means that the full solution is error-free, and yellow crosses mark the verifier’s identification (VI), if present. The six judgments are: early (where VI is before GT), TP (true positive, where VI overlaps with GT), late (where VI is after GP), spurious (where GT is error-free but the verifier makes an identification), miss (where verifier misses a GT FWS) and TN (true negative, where model makes a correct judgment of error-free). Green and red cell background colors indicates correct and incorrect judgments, respectively. The number in each cell counts the specific model judgments under the prompt. Invalid responses are excluded.</p>
<p>model correctly identifies the ground truth FWS (with any overlapping) in a wrong solution, and true-negative (TN) is the case where the model correctly reports a no-mistake judgment for an error-free solution. Sensitivity is defined as the fraction of TP among all wrong solutions, and specificity as</p>
<p>| Prompt | Verifier | Sensitivity | | | Specificity | | | w/o Ref | w/ Ref | w/ Ref | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | w/o Ref | w/ Ref | w/o Ref | w/ Ref | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>as shown in the previous experiment. These results collectively indicate that solution verification is beyond the current capabilities of many LLMs.</p>
<h2>6 Discussion</h2>
<p>This paper presents CHAMP, the Concept and HintAnnotated Math Problems dataset, along with annotations of logical correctness of model-generated solutions to each problem. The unique construction of CHAMP enables previously under-explored studies regarding context-related reasoning abilities as well as verification abilities of LLMs.</p>
<p>We investigate the mathematical reasoning abilities of 10 proprietary and open-source LLMs of various sizes and release dates through CHAMP. Even the best models are far from perfect, and many models are unable to incorporate useful concepts and problem-specific hints. Even when the final answer accuracy is high, a closer inspection of the reasoning steps reveals that many models may be accidentally arriving at the correct answer.</p>
<p>Furthermore, all models struggle at solution verification, indicating that these models often generate but do not understand their solutions, similar to some of the findings of recent work (West et al., 2023; Qiu et al., 2023). These results advocate for a more fine-grained and multi-faceted investigation of LLMs' mathematical reasoning capabilities.</p>
<h2>7 Limitations</h2>
<p>While CHAMP reveals intriguing behaviors of different models, there are several limitations. For one, we relied on extensive manual annotation for high-quality labeling, and hence the dataset currently contains only 270 problems. There is also the risk of dataset contamination; i.e., the models we evaluate may have been trained on the original versions of the problems from CHAMP. However, to mitigate this we rewrote the problems to fit the design of the dataset, minimizing the impact of memorization. Finally, while we rely on automatic evaluation for final answer accuracy to enable scalable evaluation, this may not be perfect. Our manual grading results suggest, though, that GPT-4's automatic grading has a high accuracy of $97 \%$.</p>
<h2>Acknowledgments</h2>
<p>We thank the reviewers for their suggestions. This study was supported in part by funds from MITIBM Watson AI.</p>
<h2>References</h2>
<p>Meta AI. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai. meta.com/blog/meta-illama-3.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Shiqi Chen, Yiran Zhao, Jinghan Zhang, I Chern, Siyang Gao, Pengfei Liu, Junxian He, et al. 2023. Felm: Benchmarking factuality evaluation of large language models. arXiv preprint arXiv:2310.00741.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Arthur Engel. 2008. Problem-solving strategies. Springer Science \&amp; Business Media.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<ul>
<li>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</li>
<li>Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. 2023. Inverse scaling: When bigger isn't better. arXiv preprint arXiv:2306.09479.</li>
<li>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</li>
<li>OpenAI. 2023. Gpt-4 technical report. ArXiv.</li>
<li>Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. 2023. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559.</li>
<li>Mistral AI Team. 2024. Cheaper, better, faster, stronger. https://mistral.ai/news/mixtral-8x22b/.</li>
<li>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</li>
<li>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</li>
<li>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837.</li>
<li>Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, et al. 2023. The generative ai paradox:" what it can create, it may not understand". arXiv preprint arXiv:2311.00059.</li>
<li>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477.</li>
<li>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697–12706. PMLR.</li>
<li>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921.</li>
</ul>
<table>
<thead>
<tr>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prove that $n^{4}+4^{n}$ can never be a prime number for integer $n&gt;1$.</td>
<td>For how many integers $n$ in ${1,2, \ldots, 99}$ is $n^{4}+4^{n}$ a prime</td>
</tr>
<tr>
<td></td>
<td>number? (Answer: 1)</td>
</tr>
<tr>
<td>Prove that $x^{2}+y^{2}+z^{2}=2 x y z$ has no positive integer solutions.</td>
<td>Find all positive integer solutions to the equation $x^{2}+y^{2}+z^{2}=$</td>
</tr>
<tr>
<td></td>
<td>$2 x y z$. (Answer: No positive integer solutions)</td>
</tr>
<tr>
<td>Prove that $323 \mid 20^{n}+16^{n}-3^{n}-1$ for even $n$.</td>
<td>What are possible values of $20^{n}+16^{n}-3^{n}-1 \bmod 323$ for even</td>
</tr>
<tr>
<td></td>
<td>$n$ ? (Answer: 0 is the only possible value)</td>
</tr>
</tbody>
</table>
<p>Table 8: Conversion of proof problems into those with check-able answers.</p>
<h1>A. 2 Polynomial</h1>
<p>Some polynomial problems require factorization or root finding involving nontrivial arithmetics, similar to number theory problems. To reduce errors in this process, we provide the relevant arithmetic calculation as hints, such as $264=6 \times 44$ when factoring $v^{2}-50 v+264=(v-6)(v-44)$.</p>
<p>In addition, there are several polynomial division and remainder problems, for which we provide the concrete definition as a concept (although all models could easily retrieve and explain this definition with a straightforward query of "What is polynomial division and remainder?"):</p>
<p>When a polynomial $f(x)$ is divided by a polynomial $g(x)$, the quotient $q(x)$ and the remainder $r(x)$ are polynomials such that $f(x)=g(x) q(x)+r(x)$ and the remainder $r(x)$ has degree less than that of $g(x)$.</p>
<h2>A. 3 Sequence</h2>
<p>A common type of problems in sequence is to find its limit. However, a prerequisite is to prove that the limit exists. Thus, we frame such questions explicitly, using wording such as "Determine if the limit exists, and if so, find its value." We also annotate these questions with concepts stating the theorem that establish the existence of the limit, most commonly the monotone convergence theorem:</p>
<p>A sequence that is monotonic and bounded has a limit. Specifically, a sequence that is monotonically increasing and bounded from above, or monotonically decreasing and bounded from below, has a limit.</p>
<p>In addition, a common strategy is induction, which shows that a property holds for all $a_{n}$ by showing that it holds for $a_{n}$ if it holds for all of $a_{1}, \ldots, a_{n-1}$. Because the instantiation of the strategy, especially the property to show, is problem-specific, we provide it as a hint, rather than a concept.</p>
<h2>A. 4 Inequality</h2>
<p>Just like with the category of number theory problems, many problems in inequality are written as proofs of inequality identity. We manage to convert them into questions requiring numerical answers with approaches such as asking for the extremum (i.e., maximum or minimum depending on the original inequality) value (while making sure that the value can indeed be attained by some variable value assignment). Some sample conversions are listed in Tab. 9.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Before</th>
<th style="text-align: left;">After</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prove that, for $a, b, c&gt;0, \sqrt[3]{a b c} \leq \sqrt{(a b+b c+c a) / 3}$ ?</td>
<td style="text-align: left;">For positive $a, b, c$, what is the smallest value of <br> $\sqrt{a b+b c+a c} / \sqrt[3]{a b c}$ ? (Answer: $\sqrt{3}$ )</td>
</tr>
<tr>
<td style="text-align: left;">If $n&gt;1$, proof that $1 /(n+1)+1 /(n+2) \ldots+1 /(2 n)&gt;1 / 2$.</td>
<td style="text-align: left;">For how many values of $n$ in ${101, \ldots, 1000}$ is $1 /(n+1)+1 /(n+$</td>
</tr>
<tr>
<td style="text-align: left;">$2)+\ldots+1 /(2 n)&gt;1 / 2$ ? (Answer: 900)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">The product of three positive reals is 1 . Their sum is greater</td>
<td style="text-align: left;">The product of three positive real numbers is 1 , and their sum is</td>
</tr>
<tr>
<td style="text-align: left;">than the sum of their reciprocals. Prove that exactly one of these</td>
<td style="text-align: left;">greater than the sum of their reciprocals. How many of them can</td>
</tr>
<tr>
<td style="text-align: left;">numbers is $&gt;1$.</td>
<td style="text-align: left;">be greater than 1 ? (Answer: 1)</td>
</tr>
</tbody>
</table>
<p>Table 9: Conversion of inequality proof problems into those requiring answers.</p>
<h1>A. 5 Combinatorics</h1>
<p>Most combinatorics problems describe real-world scenarios. Where applicable, we provide any unmentioned commonsense knowledge (e.g., "On a chess board, two rooks are placed peacefully if they are not on the same row or column.") before the problem (e.g., "For an $n \times n$ chess board, find the number of ways that $n$ rooks can be placed peacefully (i.e., any two are placed peacefully), as an expression of $n$.").</p>
<p>In addition, many combinatorics problems ask for the number of ways in a setup size $n$ (e.g., the number of ways that $n$ horses can finish in a race with the possibility of ties), and it is solved in the following manner:</p>
<ol>
<li>Find a recurrence relationship to express $P(n)$ in terms of $P(n-1)$ and $P(n-2)$ (and possibly more terms), where $P(n)$ is the quantity asked in the question.</li>
<li>Find the initial values $P(1), P(2)$ (and possibly more terms).</li>
<li>Set up a characteristic equation (which is a polynomial) and find its root.</li>
<li>Use the roots to express $P(n)$ as a function of $n$.</li>
</ol>
<p>The key difficulty is the root-finding part, so instead of asking for the general expression of $P(n)$ in terms of $n$, we ask for a specific value, such as $P(7)$, which could be worked out by repeatedly applying the recurrence relationship from the initial values. We also make sure that the asked $P(n)$ value is relatively small, usually less than 200, to minimize the chance of arithmetic errors.</p>
<h1>B Dataset Details</h1>
<p>Tab. 10 shows one problem from each category, with problem, concepts and hints on the left column, and solution on the right column.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number Theory (Problem ID: P_Number-Theory_1)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Problem: Are there integer solutions to the equation $\left(x^{2}-1\right)\left(y^{2}-1\right)+1985=$ $z^{2}$ ? <br> Concepts and Hints: <br> H1. Consider the equation modulo 9. <br> C1. For integer $x, x^{2} \bmod 9$ can take values of $0,1,4$ and 7 . <br> C2. $(a+b) \bmod m=((a \bmod m) *(b \bmod m) \bmod m),(a-b) \bmod m=$ $((a \bmod m)-(b \bmod m) \bmod m) . a b \bmod m=((a \bmod m)(b \bmod$ $m) \bmod m) . a^{b} \bmod m=((a \bmod m)^{b} \bmod m)$. <br> H2. $1985 \bmod 9=5$.</td>
<td style="text-align: center;">Answer: No <br> Solution Steps (Concepts and Hints Used): <br> 1. If the equation has a solution, we have $\left(x^{2}-1\right)\left(y^{2}-1\right)=z^{2}-1985$. <br> 2. Since $u^{2} \bmod 9 \in{0,1,4,7}$, we have $u^{2}-1 \bmod 9 \in{0,3,6,8}$ and $\left(x^{2}-1\right)\left(y^{2}-1\right) \bmod 9 \in{0,1,3,6}$. [H1, C1, C2] <br> 3. However, since $1985 \bmod 9=5, z^{2}-1985 \bmod 9 \in{2,4,5,8}$. [H1, H2, C2] <br> 4. Since there is no overlapping values, we conclude that the equation has no solution.</td>
</tr>
<tr>
<td style="text-align: center;">Polynomial (Problem ID: P_Polynomial_1)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Problem: What is the remainder of $n x^{n+1}-(n+1) x^{n}+1$ divided by $(x-1)^{2}$ ? <br> Concepts and Hints: <br> C1. When a polynomial $f(x)$ is divided by a polynomial $g(x)$, the quotient $q(x)$ and the remainder $r(x)$ are polynomials such that $f(x)=g(x) q(x)+r(x)$ and the remainder $r(x)$ has degree less than that of $g(x)$. <br> H1. Let $f(x)=n x^{n+1}-(n+1) x^{n}+1$ and study $f(1)$ and $f^{\prime}(1)$.</td>
<td style="text-align: center;">Answer: 0 <br> Solution Steps (Concepts and Hints Used): <br> 1. We have $f(x)=(x-1)^{2} * q(x)+r(x)$ where $r(x)$ is a polynomial of degree at most $1\left(\mathrm{i} \alpha_{+}, r(x)=\alpha x+b\right)$. [C1] <br> 2. Thus, $f(1)=0=r(1)$. [H1] <br> 3. We have $f^{\prime}(x)=2(x-1) * q(x)+(x-1)^{2} * q(x)+r^{\prime}(x), s o f^{\prime}(1)=$ $r^{\prime}(1)=0$. [H1] <br> 4. Since $r(x)$ has the form of $a x+b$, we have $a+b=0, a=0, \omega b=0$. <br> 5. Thus, $r(x)=0$ is the remainder.</td>
</tr>
<tr>
<td style="text-align: center;">Sequence (Problem ID: P_Sequence_2)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Problem: Let $\left{x_{n}\right},\left{y_{n}\right},\left{z_{n}\right}$ be three sequences with positive initial terms $x_{1}, y_{1}, z_{1}$, defined as $x_{n+1}=y_{n}+1 / z_{n}, y_{n+1}=z_{n}+1 / x_{n}, z_{n+1}=x_{n}+1 / y_{n}$. Let $w_{n}$ be the maximum value of $x_{n}, y_{n}, z_{n}$. For different values of $x_{1}, y_{1}, z_{1}$, do we have $w_{200}$ always greater than 20, always smaller than 20, or sometimes greater and sometimes smaller than 20 ? <br> Concepts and Hints: <br> H1. Let $a_{n}=x_{n}+y_{n}+z_{n}$. <br> H2. Derive a lower bound on $a_{2}$. <br> C1. For positive $x, x+1 / x \geq 2$, with equality if and only if $x=1$. <br> H3. Compare $a_{n}$ with 18 n for all $n$. <br> C2. $(x+y)^{2}=x^{2} \pm 2 x y+y^{2}$. <br> C3. For real numbers $a_{1}, \ldots, a_{n}$ and $b_{1}, \ldots, b_{n},\left(a_{1} b_{1}+\ldots+a_{n} b_{n}\right)^{2} \leq\left(a_{1}^{2}+\right.$ $\left.\ldots+a_{n}^{2}\right)\left(b_{1}^{2}+\ldots+b_{n}^{2}\right)$.</td>
<td style="text-align: center;">Answer: Always greater than 20 <br> Solution Steps (Concepts and Hints Used): <br> 1. Let $a_{n}=x_{n}+y_{n}+z_{n}$. [H1] <br> 2. We have $a_{2}^{2}=\left(x_{1}+1 / x_{1}+y_{1}+1 / y_{1}+z_{1}+1 / z_{1}\right)^{2} \geq(2+2+2)^{2}=$ $36=2 \cdot 18 .[\mathrm{H} 2, \mathrm{C} 1]$ <br> 3. If $a_{n}^{2} \geq 18 n$, then we have $a_{n+1}^{2}=\left(x_{n}+1 / x_{n}+y_{n}+1 / y_{n}+z_{n}+1 / z_{n}\right)^{2} \geq$ $a_{n}^{2}+2\left(x_{n}+y_{n}+z_{n}\right)\left(1 / x_{n}+1 / y_{n}+1 / z_{n}\right) \geq a_{n}^{2}+2 * 9 \geq 18 n+18=$ $18(n+1) .[\mathrm{H} 3, \mathrm{C} 2, \mathrm{C} 3]$ <br> 4. So we have $a_{n}^{2} \geq 18 n$. [H3] <br> 5. Thus, $a_{200}^{2} \geq 18 \cdot 200=3600$, which means that $a_{200}=x_{200}+y_{200}+z_{200} \geq$ 60 . <br> 6. So one of $x_{200}, y_{200}, z_{200}$ must be at least 20 . <br> 7. Thus, $w_{200}$ cannot be smaller than 20 .</td>
</tr>
<tr>
<td style="text-align: center;">Inequality (Problem ID: P_1nequality_2)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Problem: For positive $a, b$, what is the smallest value of $\left(a^{2}+b^{2}\right) /(a+b)^{2}$ ? <br> Concepts and Hints: <br> C1. For non-negative $x, y$, we have $(x+y) / 2 \leq \sqrt{\left(x^{2}+y^{2}\right) / 2}$, with equality if and only if $x=y$.</td>
<td style="text-align: center;">Answer: $1 / 2$ <br> Solution Steps (Concepts and Hints Used): <br> 1. Since $(a+b) / 2 \leq \sqrt{\left(a^{2}+b^{2}\right) / 2}$, we have $(a+b)^{2} / 4 \leq\left(a^{2}+b^{2}\right) / 2$. [C1] <br> 2. This means that $\left(a^{2}+b^{2}\right) /(a+b)^{2} \geq 1 / 2$. <br> 3. So the smallest value is $1 / 2$, achieved at $a=b$.</td>
</tr>
<tr>
<td style="text-align: center;">Combinatorics (Problem ID: P_Combinatorics_1)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Problem: Let a string consist of digit 1, 2, 3. How many such strings of length 6 have adjacent digit differing by less than or equal to 1 ? <br> Concepts and Hints: <br> H1. Let $x_{n}, y_{n}, z_{n}$ be the number of length- $n$ strings that end with digit $1,2,3$ respectively. <br> H2. What are $x_{1}, y_{1}, z_{1}$ ? <br> H3. By appending a digit to the existing string, derive the formula for $x_{n+1}$, $y_{n+1}, z_{n+1}$ from $x_{n}, y_{n}, z_{n}$. <br> C1. If there are n actions, with $p_{i}$ ways to perform the $i$-th action, and no two actions can be performed at the same time, then there are $p_{1}+p_{2}+\ldots+p_{n}$ ways to perform the action in total.</td>
<td style="text-align: center;">Answer: 239 <br> Solution Steps (Concepts and Hints Used): <br> 1. Let $x_{n}, y_{n}, z_{n}$ be the number of length- $n$ strings that end with digit 1, 2, 3 respectively. [H1] <br> 2. Thus, we have $x_{1}=y_{1}=z_{1}=1$. [H2] <br> 3. For a string ending with 1 , we can append 1 and 2 ; for a string ending with 2 , we can append 1,2 and 3 ; for a string ending with 3 , we can append 2 and 3. [H3] <br> 4. Thus, we have $x_{n+1}=x_{n}+y_{n}, y_{n+1}=x_{n}+y_{n}+z_{n}, \text { and } z_{n+1}=y_{n}+z_{n}$. [H3, C1] <br> 5. Starting from (1, 1, 1), we have the sequence of $\left(x_{n}, y_{n}, z_{n}\right)$ to be (1, 1, 1), $(2,3,2),(5,7,5),(12,17,12),(29,41,29),(70,99,70)$. <br> 6. Thus, in total, there are $x_{0}+y_{0}+z_{0}=70+99+70=239$ such strings.</td>
</tr>
</tbody>
</table>
<p>Table 10: One example problem per category from the dataset. The left column presents the problem, along with relevant concepts and hints. The right column gives the solution, both the final answer and the full step-wise solution with concept and hint labels.</p>
<p>Table 11 presents a breakdown of problems by answer formats and the corresponding baseline answer design. This baseline simulates the best performance of a dummy model that has no math reasoning ability but can answer the question in a semantically sensible manner (e.g., answering Yes or No to a question asking for a Boolean answer).</p>
<table>
<thead>
<tr>
<th>Answer Format</th>
<th># Probs</th>
<th>Example</th>
<th>Baseline Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Boolean</td>
<td>42</td>
<td>Is $4^{545}+545^{4}$ a prime number?</td>
<td>No</td>
</tr>
<tr>
<td>Numeric</td>
<td>162</td>
<td>In how many ways can 4 horses go through the finish (with possibility of ties)?</td>
<td>0</td>
</tr>
<tr>
<td>Expression</td>
<td>45</td>
<td>Among all sequences of positive integer numbers have sum $n$, for integer $k&lt;n-1$, how many times does the number $k$ appear, as an expression of $n$ and $k$ ?</td>
<td>Sum of all variables (i.e., $n+k$ for the example)</td>
</tr>
<tr>
<td>Enumeration</td>
<td>21</td>
<td>Find all integer solutions to the equation $15 x^{2}-7 y^{2}=9$.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>Table 11: The construction of baseline answers based on four answer formats.</p>
<h1>C Full Prompt Texts</h1>
<p>This section lists all the prompts used in the experiments. Texts of normal fonts are provided literally. Parenthesized italic (texts) are provided to the model, and parenthesized bold (texts) are model outputs.</p>
<p>Tab. 12 shows the main prompt setup for the model's problem solving capability evaluation. The final answer summary is evaluated by GPT-4 with the prompt listed in Tab. 13. We include "Partially correct" as one grading verdict for the model to use in ambiguous situations (e.g., the solver model finds one of two solutions to the equation) but treat it as incorrect for accuracy calculation (e.g., in Tab. 2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">System</td>
<td style="text-align: left;">You are an expert on mathematics.</td>
</tr>
<tr>
<td style="text-align: left;">User <br> Solver</td>
<td style="text-align: left;">(One or more rounds of user-solver conversation that end in the solver generating the full solution as the message of <br> the last round. The specific conversation contents are presented in Tab. 14-21.)</td>
</tr>
<tr>
<td style="text-align: left;">User</td>
<td style="text-align: left;">Now, summarize the answer above in one sentence, without any intermediate steps or explanations.</td>
</tr>
<tr>
<td style="text-align: left;">Solver</td>
<td style="text-align: left;">(Model-generated final answer summary)</td>
</tr>
</tbody>
</table>
<p>Table 12: Prompt for eliciting full solution and final answer summary from the model under evaluation (i.e., solver).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">System</td>
<td style="text-align: left;">You are a math teacher and need to grade student's homework. For each question, you need to determine the correctness <br> of the student answer, given the reference answer. You only need to judge the correctness of the final answer, and <br> should not consider any reasoning or explanations given in the answer. Note that if the student gives an obviously <br> equivalent solution to the reference answer (e.g., 1.5 vs $3 / 2$ or $a^{2}-b^{2}$ vs $(a+b)(a-b)$ ), the answer should be judged <br> as correct. Your decision should be one of "Correct", "Incorrect" or "Partially correct". There is no need to explain <br> your decision.</td>
</tr>
<tr>
<td style="text-align: left;">User</td>
<td style="text-align: left;">The question is: <br> (Problem statement) <br> The reference answer is: <br> (Ground truth final answer from the dataset) <br> The student answer is: <br> (Model-generated final answer summary) <br> Is the student answer correct, incorrect, or partially correct?</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">(Grading verdict)</td>
</tr>
</tbody>
</table>
<p>Table 13: Prompt for grading the solver's final answer summary using GPT-4.</p>
<p>Tab. 14-16 present the prompts for evaluating models under current practices, including zero-shot, few-shot (5-shot) and zero-shot with partial solution. These conversations are to be swapped into the orange cell of Tab. 12. Note that in the few-shot prompt of tab. 15, only the last round is actually generated by the model. The "solver output" in the earlier rounds are directly fed to the model as the context (pretending to be earlier model generations).</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 14: Zero-shot prompt.</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Statement of sample problem 1)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Ground truth solution steps for problem 1)</td>
</tr>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Statement of sample problem 2)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Ground truth solution steps for problem 2)</td>
</tr>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Statement of sample problem 3)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Ground truth solution steps for problem 3)</td>
</tr>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Statement of sample problem 4)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Ground truth solution steps for problem 4)</td>
</tr>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Statement of sample problem 5)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Ground truth solution steps for problem 5)</td>
</tr>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 15: Few-shot prompt.</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement) <br> Below is a partial solution to the problem that may be helpful: <br> (List of steps revealed in the partial solution)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 16: Zero-shot prompt with partial solution provided.</p>
<p>Tab. 17-21 presents the prompts for different concept provision methods covered in Tab. 2. The light green texts and dark green texts are used for the $\mathrm{w} / \mathrm{o} \mathrm{H}$ and $\mathrm{w} / \mathrm{H}$ prompts respectively, consistent with the color-coding of Tab. 2. Any corner cases are discussed in table captions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Role</th>
<th style="text-align: center;">Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You may find the following information useful: <br> (List of all hints.)</td>
</tr>
<tr>
<td style="text-align: center;">Solver</td>
<td style="text-align: center;">(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 17: The "No C" concept provision prompt (i.e. not providing any concept). The w/o H version (and w H version when the problem does not have any hint) is the same as the zero-shot prompt of Tab. 14.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">User</td>
<td style="text-align: left;">Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">You may find the following information useful: <br> (List of relevant concepts.) / (List of relevant concepts and hints.)</td>
</tr>
<tr>
<td style="text-align: left;">Solver</td>
<td style="text-align: left;">(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 18: The "Direct" concept provision prompt (also used for "Root" and "Misleading" with respective concepts). If there are no concepts in the $\mathrm{w} / \mathrm{o} \mathrm{H}$ version, then the last paragraph is removed entirely and the prompt reduces to the zero-shot prompt of Tab. 14.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Role</th>
<th style="text-align: center;">Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Please explain the following concept: (name of concept 1, skip this round if unnamed).</td>
</tr>
<tr>
<td style="text-align: center;">Solver</td>
<td style="text-align: center;">(Model-generated concept explanation)</td>
</tr>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Please explain the following concept: (name of concept 2, skip this round if unnamed).</td>
</tr>
<tr>
<td style="text-align: center;">Solver</td>
<td style="text-align: center;">(Model-generated concept explanation)</td>
</tr>
<tr>
<td style="text-align: center;">(One round of conversation for each named concept)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Please explain the following concept: (name of concept $n$, skip this round if unnamed).</td>
</tr>
<tr>
<td style="text-align: center;">Solver</td>
<td style="text-align: center;">(Model-generated concept explanation)</td>
</tr>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Besides the concepts above, you may also find the following information useful: <br> (List of remaining unnamed concepts.) / (List of remaining unnamed concepts and all hints.)</td>
</tr>
<tr>
<td style="text-align: center;">Solver</td>
<td style="text-align: center;">(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 19: The "Name" concept provision prompt. If there are no unnamed concepts (and hints), i.e., an empty list, then the sentence "Besides the concepts above..." is replaced with "You may find the above concepts helpful.", and the prompt is terminated.</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>Please give an example that applies the following concept: <br> (Text of concept 1).</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated example)</td>
</tr>
<tr>
<td>User</td>
<td>Please give an example that applies the following concept: <br> (Text of concept 2).</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated example)</td>
</tr>
<tr>
<td>User</td>
<td>(One round of conversation for each concept)</td>
</tr>
<tr>
<td>Please give an example that applies the following concept: <br> (Text of concept $n$ ).</td>
<td></td>
</tr>
<tr>
<td>{f50820a26-082c-4486-816d-8d3d3d308d6}Solver</td>
<td>(Model-generated concept explanation)</td>
</tr>
<tr>
<td>Solve the following problem. Make sure to show your work before giving the final answer. <br> (Problem statement)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>You may find the above concepts helpful. <br> Besides the concepts above, you may also find the following information useful: <br> (List of all hints.)</td>
</tr>
<tr>
<td>Solver</td>
<td>(Model-generated full solution)</td>
</tr>
</tbody>
</table>
<p>Table 20: The "Example" concept provision prompt. If there are no hints, then we use the w/o H version.</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>First, study the following problems and their solutions as they may contain helpful information.</td>
</tr>
<tr>
<td></td>
<td>(Sample problem that uses concept 1 and its step-wise solution)</td>
</tr>
<tr>
<td></td>
<td>(Sample problem that uses concept 2 and its step-wise solution)</td>
</tr>
<tr>
<td></td>
<td>(...)</td>
</tr>
<tr>
<td></td>
<td>(Sample problem that uses concept $n$ and its step-wise solution)</td>
</tr>
</tbody>
</table>
<p>With what you have learned, solve the following problem. Make sure to show your work before giving the final answer.
(Problem statement)
In addition, you may also find the following information helpful:
(List of all hints.)</p>
<p>Solver (Model-generated full solution)
Table 21: The "Problem" concept provision prompt. If there are no hints, then the colored text is omitted.</p>
<h1>D Model-Generated Solutions and First Wrong Step Annotations</h1>
<p>Tab. 22 and 23 show one solution generated by each model. The final answer (FA) correctness and full solution (FS) correctness are separately noted. When the FS is incorrect (regardless of the FA correctness), the first wrong step (FWS) is highlighted in red, which consists the FWS corpus annotation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Solution Author: GPT-3.5</th>
<th style="text-align: left;">Final Answer: Correct</th>
<th style="text-align: left;">Full Solution: Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Problem ID: P_Inequality_10
Problem: For $x, y, z&gt;0$, what is the smallest value of $x^{2} / y^{2}+y^{2} / z^{2}+z^{2} / x^{2}-(y / x+z / y+x / z)$ ?
Answer: 0
Model-generated solution:
To find the smallest value of the expression, we can use the AM-GM inequality.
By AM-GM, we have:
$x^{2} / y^{2}+y^{2} / z^{2}+z^{2} / x^{2} \geq 3 \sqrt{\left(x^{2} / y^{2}\right)\left(y^{2} / z^{2}\right)\left(z^{2} / x^{2}\right)}$
Simplifying the right side, we get:
$3 \sqrt{\left(x^{2} / y^{2}\right)\left(y^{2} / z^{2}\right)\left(z^{2} / x^{2}\right)}=3 \sqrt{\left(x^{2} y^{2} z^{2}\right) /\left(x^{2} y^{2} z^{2}\right)}=3 \sqrt{1}=3$
Now, let's consider the second term, $-(y / x+z / y+x / z)$.
By AM-GM, we have:
$y / x+z / y+x / z \geq 3 \sqrt{(y / x)(z / y)(x / z)}$
Simplifying the right side, we get:
$3 \sqrt{(y / x)(z / y)(x / z)}=3 \sqrt{(x y z) /(x y z)}=3 \sqrt{1}=3$
Therefore, the smallest value of the expression is:
$3-3=0$
So, the smallest value of $x^{2} / y^{2}+y^{2} / z^{2}+z^{2} / x^{2}-(y / x+z / y+x / z)$ is 0 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Solution Author: GPT-4</th>
<th style="text-align: left;">Final Answer: Correct</th>
<th style="text-align: left;">Full Solution: Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Problem ID: P_Number-Theory_13
Problem: What is the smallest value of $\left|36^{m}-5^{n}\right|$ for positive integers $m$ and $n$ ?
Answer: 11
Model-generated solution:
To find the smallest value of $\left|36^{m}-5^{n}\right|$ for positive integers $m$ and $n$, we need to find the smallest values of $m$ and $n$ such that $36^{m}$ is as close as possible to $5^{n}$.
Let's start with $m=1$ and $n=1$. We have:
$\left|36^{1}-5^{1}\right|=|36-5|=31$
If we increase $m$ to 2 , we get:
$\left|36^{2}-5^{1}\right|=|1296-5|=1291$
This is much larger, so increasing $m$ is not a good strategy. Let's try increasing $n$ to 2 :
$\left|36^{1}-5^{2}\right|=|36-25|=11$
This is smaller, so increasing $n$ seems to be a good strategy. Let's try $n=3$ :
$\left|36^{1}-5^{3}\right|=|36-125|=89$
This is larger again, so $n=2$ seems to be the best choice. Let's check $m=2$ just to be sure:
$\left|36^{2}-5^{2}\right|=|1296-25|=1271$
This is much larger, so $m=1$ and $n=2$ is the best choice.
So, the smallest value of $\left|36^{m}-5^{n}\right|$ for positive integers $m$ and $n$ is 11 .
Table 22: Sample solutions generated by different models and their correctness. If the full solution is not correct, the first wrong step (FWS) is marked in red. Table continued in Tab. 23.</p>
<table>
<thead>
<tr>
<th>Solution Author: GPT-4 T</th>
<th>Final Answer: Correct</th>
<th>Full Solution: Correct</th>
</tr>
</thead>
<tbody>
<tr>
<td>Problem ID: P_Combinatorics_40</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Problem: Find the number of ways to fill a 2 x 11 rectangle with 2 x 1 tiles.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Answer: 144</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Model-generated solution:
To solve this problem, we can use a recursive approach. Let's denote the number of ways to fill a $2 \times n$ rectangle with 2 x 1 tiles as $f(n)$. We are looking to find $f(11)$.
Let's consider the first column of the $2 \times 11$ rectangle. We have two choices for placing the tiles:</p>
<ol>
<li>We place two tiles
vertically in the first column. If we do this, we are left with a $2 \times(11-1)=2 \times 10$ rectangle to fill. The number of ways to fill the rest of the rectangle is $f(10)$.</li>
<li>We place one tile horizontally at the top of the first column. If we do this, we must also place another tile horizontally below it to fill the first two columns. This leaves us with a $2 \times(11-2)=2 \times 9$ rectangle to fill. The number of ways to fill the rest of the rectangle is $f(9)$.
Therefore, we can express $f(n)$ as a recursive function: $f(n)=f(n-1)+f(n-2)$
We also need to establish the base cases. For a $2 \times 1$ rectangle, there is only one way to fill it (vertically), so $f(1)=1$.
For a $2 \times 2$ rectangle, there are two ways to fill it (either two vertical tiles or two horizontal tiles), so $f(2)=2$.
Now we can compute $f(11)$ using the recursive formula:
$f(1)=1$
$f(2)=2$
$f(3)=f(2)+f(1)=2+1=3$
$f(4)=f(3)+f(2)=3+2=5$
$f(5)=f(4)+f(3)=5+3=8$
$f(6)=f(5)+f(4)=8+5=13$
$f(7)=f(6)+f(5)=13+8=21$
$f(8)=f(7)+f(6)=21+13=34$
$f(9)=f(8)+f(7)=34+21=55$
$f(10)=f(9)+f(8)=55+34=89$
$f(11)=f(10)+f(9)=89+55=144$
So, there are 144 ways to fill a $2 \times 11$ rectangle with $2 \times 1$ tiles.
Solution Author: PaLM 2 M | Final Answer: Incorrect | Full Solution: Incorrect
Problem ID: P_Polynomial_11
Problem: If $x_{1}, x_{2}$ are the two roots of the polynomial $x^{2}-6 x+1$, for how many integer n in ${61,62, \ldots, 120}$ does $x_{1}^{n}+x_{2}^{n}$ divided by 5 give a remainder of 4 ?
Answer: 10
Model-generated solution:
The two roots of the polynomial $x^{2}-6 x+1$ are $x_{1}=\frac{3+\sqrt{5}}{2}$ and $x_{2}=\frac{3-\sqrt{5}}{2}$.
We can write $x_{1}^{n}+x_{2}^{n}$ as follows:
$x_{1}^{n}+x_{2}^{n}=\left(\frac{3+\sqrt{5}}{2}\right)^{n}+\left(\frac{3-\sqrt{5}}{2}\right)^{n}$
We can factor the expression in the parentheses as follows:
$\left(\frac{3+\sqrt{5}}{2}\right)^{n}+\left(\frac{3-\sqrt{5}}{2}\right)^{n}=\left(\frac{3+\sqrt{5}+3-\sqrt{5}}{2}\right)^{n}+\left(\frac{3+\sqrt{5}(3-\sqrt{5})}{2}\right)^{n}$
Simplifying, we get:
$x_{1}^{n}+x_{2}^{n}=\left(\frac{6}{2}\right)^{n}+\left(-\frac{2 \sqrt{5}}{2}\right)^{n}$
We can then write $x_{1}^{n}+x_{2}^{n}$ as follows:
$x_{1}^{n}+x_{2}^{n}=3^{n}-2^{n} \sqrt{5}^{n}$
We know that $3^{n}$ is always divisible by 5 , since 3 is a factor of 5 . We also know that $\sqrt{5}$ is an irrational number, so $\sqrt{5}^{n}$ is never divisible by 5 . Therefore, $x_{1}^{n}+x_{2}^{n}$ is never divisible by 5 .
In the range $61 \leq n \leq 120$, there are 60 integers. Since $x_{1}^{n}+x_{2}^{n}$ is never divisible by 5 , there are 0 integers in this range for which $x_{1}^{n}+x_{2}^{n}$ divided by 5 gives a remainder of 4 .
Therefore, the answer is $\left|\frac{0}{2}\right\rangle$
Table 23: Sample solutions generated by different models and their correctness. If the full solution is not correct, the first wrong step (FWS) is marked in red. Table continued from Tab. 22.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We do not experiment with misleading hints, as they would appear nonsensical due to problem-specificity.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Note that the FAA statistics in Tab. 3 are based on modelgenerated solutions of $25 \%$ sampled problems for each model, and hence different from those in Tab. 2.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
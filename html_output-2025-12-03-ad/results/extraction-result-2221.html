<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-278368024</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.04311v1.pdf" target="_blank">How the Misuse of a Dataset Harmed Semantic Clone Detection</a></p>
                <p><strong>Paper Abstract:</strong> BigCloneBench is a well-known and widely used large-scale dataset for the evaluation of recall of clone detection tools. It has been beneficial for research on clone detection and has become a standard in evaluating the performance of clone detection tools. More recently, it has also been widely used as a dataset to evaluate machine learning approaches to semantic clone detection or code similarity detection for functional or semantic similarity. This paper demonstrates that BigCloneBench is problematic to use as ground truth for learning or evaluating semantic code similarity, and highlights the aspects of BigCloneBench that affect the ground truth quality. A manual investigation of a statistically significant random sample of 406 Weak Type-3/Type-4 clone pairs revealed that 93% of them do not have a similar functionality and are therefore mislabelled. In a literature review of 179 papers that use BigCloneBench as a dataset, we found 139 papers that used BigCloneBench to evaluate semantic clone detection and where the results are threatened in their validity by the mislabelling. As such, these papers often report high F1 scores (e.g., above 0.9), which indicates overfitting to dataset-specific artefacts rather than genuine semantic similarity detection. We emphasise that using BigCloneBench remains valid for the intended purpose of evaluating syntactic or textual clone detection of Type-1, Type-2, and Type-3 clones. We acknowledge the important contributions of BigCloneBench to two decades of traditional clone detection research. However, the usage of BigCloneBench beyond the intended purpose without careful consideration of its limitations has led to misleading results and conclusions, and potentially harmed the field of semantic clone detection.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BigCloneBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BigCloneBench (BCB) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale code clone benchmark constructed from exemplar functions, heuristic search and human labelling; originally intended to evaluate recall of syntactic clone detectors but widely (mis)used as ground truth for semantic/code-similarity ML.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BigCloneBench dataset (automatic exemplar-based labelling)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructed by (1) selecting 43 functionalities and 101 exemplar functions, (2) performing heuristic search to retrieve candidate methods, (3) human judges labelling candidates as true positives/false positives relative to a functionality, and (4) constructing pairwise labels (true/false) per functionality; finally an automatic textual-similarity classifier assigns Type-1/T2/T3/WT3/T4 categories.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software engineering / code clone detection</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Constructed clone-pair labels (WT3/T4 'true positive' labels) — i.e., dataset labels used as a surrogate ground truth for semantic similarity</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Pairwise 'true clone' labels derived from exemplar-function membership and judge annotations on single methods (a pair is marked true if both methods belong to the same exemplar/functionality P_f ∪ X_f). Labels for WT3/T4 correspond to low textual similarity (automatically classified) but sharing a (possibly minor) functionality per the exemplar/specification. The proxy was built semi-automatically; most pairs were not manually validated as pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Manual human validation of functional similarity (sampled pairs) and LLM-assisted adjudication</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Authors manually inspected a statistically significant, stratified random sample of 406 WT3/T4-labelled pairs and judged whether the pair's main functionality is the same (strict criterion). Additionally, the 406 pairs were labelled via GPT-4o (temperature=0) for comparison; disagreements were adjudicated.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>In the stratified sample of 406 WT3/T4 pairs: 27/406 (6.65%) were true positives under strict human judgement and 379/406 (93.35%) were false positives. Cohen's Kappa (initial human-human) = 0.476; observed agreement 89.7%. Human-LLM agreement: observed 97.0%, Cohen's Kappa = 0.735.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>As reported by many downstream papers using BigCloneBench WT3/T4 as ground truth: high proxy metrics (e.g., example paper reported precision 99.8%, recall 88.3%, F1 93.7% on WT3/T4 subset). These are performance numbers against the BCB labels (proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>When recalculated assuming 93.35% of WT3/T4 labels are wrong, a reported example (precision 99.8%, recall 88.3%, F1 93.7%) becomes: true precision ≈ 6.7%, true recall ≈ 5.9%, true F1 ≈ 6.3% (authors compute these corrected values using the sample false-positive rate).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Approx. 93.35% (379/406) of WT3/T4-labelled true pairs in the sample were judged false by strict human validation; the paper uses 93.3% as the estimated wrong-label rate for WT3/T4 overall.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>The ML systems trained/evaluated on BigCloneBench WT3/T4 appear to be learning dataset-specific artefacts (in-distribution, not genuine semantic generalization); many results are best characterized as overfitting to the dataset's labelling biases rather than discovering novel out-of-distribution semantic clones.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Repair attempts reported in the literature: (a) filtering by similarity threshold and removing methods labelled by a single judge (Li et al.), (b) identifier-abstraction experiments (Yu et al.) showed sensitivity to identifier overlap. The paper itself recommends manual (human) validation and warns against using BCB as semantic ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>No quantitative evidence in this paper that repair attempts fixed the WT3/T4 false-positive problem; Li et al.'s filtered subset has no shown improvement for WT3/T4; Yu et al. showed that removing identifier cues causes large drops (F1 decreases 16–27%) in models trained on original BCB, indicating dataset artefact dependence rather than true improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Paper notes proxy labels (BigCloneBench automatic construction) are cheap at scale but unreliable for semantic similarity; ground-truth (precision) requires manual inspection of sampled results which is time-consuming. No dollar/time-per-sample numbers provided — qualitative statement only.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Syntactic/textual clone detection is mature and BigCloneBench remains appropriate for Type-1/Type-2/Type-3 recall evaluation; semantic (Type-4) clone detection via ML is emerging and currently undermined by flawed ground-truth proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Construction cascade described: exemplar/specification → heuristic retrieval of candidate methods → human labelling of single methods relative to functionality → automatic pairwise ground-truth construction → automatic textual-similarity classification (T1/T2/T3/WT3/T4). Validation cascade used by authors: automatic labels (proxy) → manual sample validation → LLM-assisted labels; errors propagate from single-method labels into pairwise labels (systematic source of false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key limitations: WT3/T4 labels were not pairwise-validated; single-judge labelling (88% single-judge) and permissive instructions ('target functionality may be only part of method') produced many labels where the shared functionality is minor; extreme class imbalance and bias (e.g., 'Copy File' dominates 54% of true pairs and 70% of false pairs); unlabelled/unassessed pairs across functionalities cause incorrect assumptions (many papers treated methods from different functionalities as negatives); automatic classification thresholding (similarity <0.5 → WT3/T4) does not ensure true semantic equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Factors include: exemplar-based labelling that allows minor-functional matches; single-judge labels; imbalance across functionalities (Copy File dominates); automatic textual-similarity thresholds for WT3/T4; retrieval heuristics that miss many relevant methods; dataset versions and derived datasets (CodeXGLUE) often modify pair counts unpredictably.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-on-BCB (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning semantic clone detectors trained/evaluated on BigCloneBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad class of ML-based code-similarity/semantic-clone systems trained or evaluated using BigCloneBench as (proxy) ground truth; many reported very high performance on BCB WT3/T4 labels but no independent pairwise validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Machine-learning semantic clone detection systems (e.g., ASTNN, TBCCD, FA, graph/GNN/transformer models cited in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Neural and classical ML models (AST-based, tree/graph neural nets, siamese encoders, transformer-based code encoders) trained on BigCloneBench pairwise labels to predict clone similarity; evaluated by standard classification metrics (precision, recall, F1) on held-out splits derived from BCB labels. Many papers built further modified training/validation/test splits by assuming cross-functionality pairs are negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software engineering / machine learning for code</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Performance vs BigCloneBench labels (precision/recall/F1 computed against BCB WT3/T4 labels)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Standard ML classification metrics computed on datasets of clone/non-clone pairs derived from BigCloneBench. These metrics measure agreement with the BCB constructed labels rather than independently-verified functional equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (evaluated against an empirical surrogate label set)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Manual pairwise functional equivalence validation (sparse sampling) — not commonly used by the ML papers; when applied, shows large disagreement with the proxy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Independent human inspection of sampled pairs to verify whether the main functionality is the same; in this paper the authors' sample of 406 WT3/T4 pairs served as ground truth to estimate real error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example from literature cited: reported proxy metrics (e.g., precision 99.8%, recall 88.3%, F1 93.7%) contrast with recalculated 'true' metrics given the 93.35% false-positive rate: true precision ≈ 6.7%, true recall ≈ 5.9%, true F1 ≈ 6.3%. This indicates enormous gap (~>85 percentage points) between proxy and true performance.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Often reported as very high on BCB WT3/T4 (many papers report >80% recall or F1; some report F1>0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>When accounting for the 93.35% WT3/T4 wrong-label rate, effective real-world performance would be near single-digit percentages (examples compute ~6% F1), implying the high reported proxy scores do not translate to true semantic-detection ability.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Implied extremely high: majority of proxy 'true' positives are not true by strict validation — sample indicates 93.35% false-positive rate for WT3/T4-labelled pairs that ML models were trained to match.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Most reported 'advances' appear to be in-distribution exploitation of label artefacts (incremental/device overfitting) rather than robust, generalizable, out-of-distribution semantic discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Some papers attempt dataset cleaning, balancing, or identifier-abstraction; Li et al. attempted a balanced/filter subset; Yu et al. tested identifier-abstraction to probe dataset artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Not proven effective: Li et al.'s filtered subset has no demonstrated improvement in WT3/T4 correctness; Yu et al. showed identifier-abstraction reduced model performance 16–27% (indicating models exploit identifier cues).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Most ML papers avoided costly manual pairwise validation and relied solely on BCB labels; the paper argues that manual validation is necessary to estimate precision but is expensive — no numeric cost data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>ML for syntactic clone detection is mature; ML for semantic (Type-4) clone detection is immature and currently confounded by unreliable training/evaluation proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Typical cascade used by ML studies: use BCB-constructed training labels (proxy) → train models → evaluate on BCB-derived held-out pairs; occasionally a small manual precision sample is taken, but rarely used to correct or retrain models.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Major challenges: models learn dataset-specific cues (e.g., identifier overlap, functionality imbalance) rather than true semantic equivalence; many studies assume pairs across different functionalities are negatives, creating additional mislabelling; lack of comprehensive pairwise manual validation undermines claims of generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Factors include label-construction artefacts (exemplar overlap, single-judge labels), strong class imbalance (dominance of few functionalities like 'Copy File'), and heuristics used to assemble true/false pairs across functionalities that introduce systematic errors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o used as a labelling assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (GPT-4o) used by the authors to independently label the sampled WT3/T4 pairs for functional similarity, providing a computational proxy/assistant to human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o-2024-08-06 (LLM labeller)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM prompted with a functional similarity definition and asked to classify each of the 406 pairs deterministically (temperature=0, top_p=0); run five times and majority-vote used to reduce stochasticity. Provided label and textual explanations which human authors used to revisit disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software engineering / LLM-assisted code analysis</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>LLM-assigned functional-similarity labels (YES/NO) applied to the sampled pairs</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Model classifies pairs as functionally similar or not, based on prompt instructions mirroring the human protocol; serves as an automated proxy for manual pairwise validation.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Human expert adjudication (final consensus labels) used as ground truth for comparison with LLM labels in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Two authors independently labelled the 406 pairs with a stricter functional-similarity criterion; disagreements were discussed and resolved to produce final human labels used as ground truth for LLM comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>LLM labelled 21 pairs as similar and 385 as not; human final labels: 27 similar / 379 not. Agreement: LLM vs humans 97.0% observed agreement, Cohen's Kappa = 0.735. 12 pairs disagreed and were re-examined: 10 human labels changed after considering LLM explanations (2 accepted, 10 rejected) resulting in a 0.5% net label change (from 93.3% to 93.8% wrong labels).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>LLM matched humans on 394/406 pairs originally (97.0% agreement); labelled 21 pairs similar while humans labelled 27 similar (after final adjudication).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Human consensus used as ground truth; LLM disagreements led humans to revise a small number of labels (net change 0.5% of sample).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>LLM used as an assistive validation tool; its high agreement suggests it reliably flags borderline logical/encoding differences (e.g., different hash algorithms or encodings) that humans sometimes miss.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>LLM-assisted review: authors used LLM explanations to re-examine disputed cases, which led to correction of a few human errors (e.g., detection of a logical bug missed by humans).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Small effect in this sample: LLM assistance caused net label change of 0.5% (from 93.3% to 93.8% wrong labels), and helped identify some human oversights; agreement metrics indicate LLM is a useful adjunct but not a full replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Using an LLM provides faster, cheaper repeated passes over the sample compared to adding more human reviewers; the paper reports the authors ran the LLM five times with deterministic settings to reduce stochasticity. No monetary/time figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>LLMs are emerging as practical assistants for code semantic judgements; in this context GPT-4o performed at substantial agreement with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Human consensus ground-truth ← LLM labelling used in parallel; LLM explanations prompted human re-inspection of disputed cases.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM occasionally flags differences that are borderline (e.g., MD5 vs SHA-1 or encoding differences) which humans may consider insignificant for 'functional similarity'; LLM deterministic runs can still have occasional stochasticity; LLM is not a definitive arbiter but a helpful assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>LLM sensitivity to low-level differences (different hash types or encodings) can cause disagreement with human definitions of 'functional similarity' depending on strictness of criterion.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2221.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2221.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeXGLUE (modified BCB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeXGLUE clone-detection subset derived from BigCloneBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-learning benchmark suite (CodeXGLUE) that includes a modified BigCloneBench clone-detection subset; the paper identifies inconsistencies and unexplained changes in pair counts compared to original BCB.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeXGLUE BigCloneBench subset</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A dataset distribution within CodeXGLUE that re-packages BigCloneBench pairs into ML-ready training/validation/test splits; modifications include altered counts of true/false pairs and additional derived false pairs not present in the original release.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software engineering / ML benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Same as BigCloneBench labels (repackaged) — used as proxy ground truth in CodeXGLUE experiments</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Reused BCB-derived pairwise labels (true/false) possibly extended or modified; authors detected suspicious additional false pairs involving samples (e.g., sample 22442270) paired with methods across many functionalities, indicating derivation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>None provided by CodeXGLUE for pairwise WT3/T4 validation; the paper treats manual human validation as the independent ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Paper's manual sample validation exposes discrepancies between CodeXGLUE-provided labels and strict human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Paper reports CodeXGLUE contains 1,170,339 false clone pairs and 561,521 true clone pairs in one downloaded version, numbers that differ from original BCB (≈288k false, ≈8.9M true pairs), indicating substantial modifications without documented justification; quality of WT3/T4 labels remains unverified and likely suffers the same ~93% false-positive issue for WT3/T4 pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Not directly reported for CodeXGLUE in this paper, but the underlying proxy is BCB labels which lead to high reported ML metrics in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Derived-data redistribution (in-distribution) that may amplify BCB artefacts; not a source of genuine novel validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>No effective correction documented; paper warns users that CodeXGLUE's modified BCB subset inherits BCB's problems and may introduce additional unexplained labelling errors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>CodeXGLUE repackaging reduces barrier-to-use for ML experiments (computationally cheap), but paper argues it shifts validation costs to users who must manually check labels to ensure semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Benchmark repackaging is common but requires careful provenance tracking; here provenance is weak and undermines dataset maturity.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>CodeXGLUE ← reprocessed BCB pairs ← original BCB exemplar/heuristic labelling; errors in original propagate and may be amplified.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>CodeXGLUE's BCB subset contains unexplained pair-count changes and added false pairs; users may unknowingly inherit BCB's WT3/T4 label flaws leading to invalid ML evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Dataset provenance and undocumented transformations; reuse of single-judge labels; mixing of different BCB versions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BigCloneBench <em>(Rating: 2)</em></li>
                <li>BigCloneEval: A clone detection tool evaluation framework with BigCloneBench <em>(Rating: 2)</em></li>
                <li>BigCloneBench Considered Harmful for Machine Learning <em>(Rating: 2)</em></li>
                <li>Assessing and improving an evaluation dataset for detecting semantic code clones via deep learning <em>(Rating: 1)</em></li>
                <li>Assessing and improving dataset and evaluation methodology in deep learning for code clone detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2221",
    "paper_id": "paper-278368024",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "BigCloneBench",
            "name_full": "BigCloneBench (BCB) dataset",
            "brief_description": "A large-scale code clone benchmark constructed from exemplar functions, heuristic search and human labelling; originally intended to evaluate recall of syntactic clone detectors but widely (mis)used as ground truth for semantic/code-similarity ML.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "BigCloneBench dataset (automatic exemplar-based labelling)",
            "system_description": "Constructed by (1) selecting 43 functionalities and 101 exemplar functions, (2) performing heuristic search to retrieve candidate methods, (3) human judges labelling candidates as true positives/false positives relative to a functionality, and (4) constructing pairwise labels (true/false) per functionality; finally an automatic textual-similarity classifier assigns Type-1/T2/T3/WT3/T4 categories.",
            "domain": "software engineering / code clone detection",
            "proxy_metric_name": "Constructed clone-pair labels (WT3/T4 'true positive' labels) — i.e., dataset labels used as a surrogate ground truth for semantic similarity",
            "proxy_metric_description": "Pairwise 'true clone' labels derived from exemplar-function membership and judge annotations on single methods (a pair is marked true if both methods belong to the same exemplar/functionality P_f ∪ X_f). Labels for WT3/T4 correspond to low textual similarity (automatically classified) but sharing a (possibly minor) functionality per the exemplar/specification. The proxy was built semi-automatically; most pairs were not manually validated as pairs.",
            "proxy_metric_type": "empirical surrogate",
            "ground_truth_metric": "Manual human validation of functional similarity (sampled pairs) and LLM-assisted adjudication",
            "ground_truth_description": "Authors manually inspected a statistically significant, stratified random sample of 406 WT3/T4-labelled pairs and judged whether the pair's main functionality is the same (strict criterion). Additionally, the 406 pairs were labelled via GPT-4o (temperature=0) for comparison; disagreements were adjudicated.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "In the stratified sample of 406 WT3/T4 pairs: 27/406 (6.65%) were true positives under strict human judgement and 379/406 (93.35%) were false positives. Cohen's Kappa (initial human-human) = 0.476; observed agreement 89.7%. Human-LLM agreement: observed 97.0%, Cohen's Kappa = 0.735.",
            "proxy_performance": "As reported by many downstream papers using BigCloneBench WT3/T4 as ground truth: high proxy metrics (e.g., example paper reported precision 99.8%, recall 88.3%, F1 93.7% on WT3/T4 subset). These are performance numbers against the BCB labels (proxy).",
            "ground_truth_performance": "When recalculated assuming 93.35% of WT3/T4 labels are wrong, a reported example (precision 99.8%, recall 88.3%, F1 93.7%) becomes: true precision ≈ 6.7%, true recall ≈ 5.9%, true F1 ≈ 6.3% (authors compute these corrected values using the sample false-positive rate).",
            "false_positive_rate": "Approx. 93.35% (379/406) of WT3/T4-labelled true pairs in the sample were judged false by strict human validation; the paper uses 93.3% as the estimated wrong-label rate for WT3/T4 overall.",
            "false_negative_rate": null,
            "novelty_characterization": "The ML systems trained/evaluated on BigCloneBench WT3/T4 appear to be learning dataset-specific artefacts (in-distribution, not genuine semantic generalization); many results are best characterized as overfitting to the dataset's labelling biases rather than discovering novel out-of-distribution semantic clones.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "Repair attempts reported in the literature: (a) filtering by similarity threshold and removing methods labelled by a single judge (Li et al.), (b) identifier-abstraction experiments (Yu et al.) showed sensitivity to identifier overlap. The paper itself recommends manual (human) validation and warns against using BCB as semantic ground truth.",
            "gap_reduction_effectiveness": "No quantitative evidence in this paper that repair attempts fixed the WT3/T4 false-positive problem; Li et al.'s filtered subset has no shown improvement for WT3/T4; Yu et al. showed that removing identifier cues causes large drops (F1 decreases 16–27%) in models trained on original BCB, indicating dataset artefact dependence rather than true improvement.",
            "validation_cost_comparison": "Paper notes proxy labels (BigCloneBench automatic construction) are cheap at scale but unreliable for semantic similarity; ground-truth (precision) requires manual inspection of sampled results which is time-consuming. No dollar/time-per-sample numbers provided — qualitative statement only.",
            "temporal_validation": null,
            "domain_maturity": "Syntactic/textual clone detection is mature and BigCloneBench remains appropriate for Type-1/Type-2/Type-3 recall evaluation; semantic (Type-4) clone detection via ML is emerging and currently undermined by flawed ground-truth proxies.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": null,
            "proxy_correlation": "",
            "validation_cascade": "Construction cascade described: exemplar/specification → heuristic retrieval of candidate methods → human labelling of single methods relative to functionality → automatic pairwise ground-truth construction → automatic textual-similarity classification (T1/T2/T3/WT3/T4). Validation cascade used by authors: automatic labels (proxy) → manual sample validation → LLM-assisted labels; errors propagate from single-method labels into pairwise labels (systematic source of false positives).",
            "publication_bias_discussion": true,
            "limitations_challenges": "Key limitations: WT3/T4 labels were not pairwise-validated; single-judge labelling (88% single-judge) and permissive instructions ('target functionality may be only part of method') produced many labels where the shared functionality is minor; extreme class imbalance and bias (e.g., 'Copy File' dominates 54% of true pairs and 70% of false pairs); unlabelled/unassessed pairs across functionalities cause incorrect assumptions (many papers treated methods from different functionalities as negatives); automatic classification thresholding (similarity &lt;0.5 → WT3/T4) does not ensure true semantic equivalence.",
            "domain_specific_factors": "Factors include: exemplar-based labelling that allows minor-functional matches; single-judge labels; imbalance across functionalities (Copy File dominates); automatic textual-similarity thresholds for WT3/T4; retrieval heuristics that miss many relevant methods; dataset versions and derived datasets (CodeXGLUE) often modify pair counts unpredictably.",
            "uuid": "e2221.0"
        },
        {
            "name_short": "ML-on-BCB (aggregate)",
            "name_full": "Machine-learning semantic clone detectors trained/evaluated on BigCloneBench",
            "brief_description": "A broad class of ML-based code-similarity/semantic-clone systems trained or evaluated using BigCloneBench as (proxy) ground truth; many reported very high performance on BCB WT3/T4 labels but no independent pairwise validation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Machine-learning semantic clone detection systems (e.g., ASTNN, TBCCD, FA, graph/GNN/transformer models cited in literature)",
            "system_description": "Neural and classical ML models (AST-based, tree/graph neural nets, siamese encoders, transformer-based code encoders) trained on BigCloneBench pairwise labels to predict clone similarity; evaluated by standard classification metrics (precision, recall, F1) on held-out splits derived from BCB labels. Many papers built further modified training/validation/test splits by assuming cross-functionality pairs are negatives.",
            "domain": "software engineering / machine learning for code",
            "proxy_metric_name": "Performance vs BigCloneBench labels (precision/recall/F1 computed against BCB WT3/T4 labels)",
            "proxy_metric_description": "Standard ML classification metrics computed on datasets of clone/non-clone pairs derived from BigCloneBench. These metrics measure agreement with the BCB constructed labels rather than independently-verified functional equivalence.",
            "proxy_metric_type": "data-driven ML prediction (evaluated against an empirical surrogate label set)",
            "ground_truth_metric": "Manual pairwise functional equivalence validation (sparse sampling) — not commonly used by the ML papers; when applied, shows large disagreement with the proxy labels.",
            "ground_truth_description": "Independent human inspection of sampled pairs to verify whether the main functionality is the same; in this paper the authors' sample of 406 WT3/T4 pairs served as ground truth to estimate real error rates.",
            "has_both_proxy_and_ground_truth": false,
            "quantitative_gap_measure": "Example from literature cited: reported proxy metrics (e.g., precision 99.8%, recall 88.3%, F1 93.7%) contrast with recalculated 'true' metrics given the 93.35% false-positive rate: true precision ≈ 6.7%, true recall ≈ 5.9%, true F1 ≈ 6.3%. This indicates enormous gap (~&gt;85 percentage points) between proxy and true performance.",
            "proxy_performance": "Often reported as very high on BCB WT3/T4 (many papers report &gt;80% recall or F1; some report F1&gt;0.9).",
            "ground_truth_performance": "When accounting for the 93.35% WT3/T4 wrong-label rate, effective real-world performance would be near single-digit percentages (examples compute ~6% F1), implying the high reported proxy scores do not translate to true semantic-detection ability.",
            "false_positive_rate": "Implied extremely high: majority of proxy 'true' positives are not true by strict validation — sample indicates 93.35% false-positive rate for WT3/T4-labelled pairs that ML models were trained to match.",
            "false_negative_rate": null,
            "novelty_characterization": "Most reported 'advances' appear to be in-distribution exploitation of label artefacts (incremental/device overfitting) rather than robust, generalizable, out-of-distribution semantic discovery.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "Some papers attempt dataset cleaning, balancing, or identifier-abstraction; Li et al. attempted a balanced/filter subset; Yu et al. tested identifier-abstraction to probe dataset artefacts.",
            "gap_reduction_effectiveness": "Not proven effective: Li et al.'s filtered subset has no demonstrated improvement in WT3/T4 correctness; Yu et al. showed identifier-abstraction reduced model performance 16–27% (indicating models exploit identifier cues).",
            "validation_cost_comparison": "Most ML papers avoided costly manual pairwise validation and relied solely on BCB labels; the paper argues that manual validation is necessary to estimate precision but is expensive — no numeric cost data provided.",
            "temporal_validation": null,
            "domain_maturity": "ML for syntactic clone detection is mature; ML for semantic (Type-4) clone detection is immature and currently confounded by unreliable training/evaluation proxies.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": false,
            "proxy_correlation": "",
            "validation_cascade": "Typical cascade used by ML studies: use BCB-constructed training labels (proxy) → train models → evaluate on BCB-derived held-out pairs; occasionally a small manual precision sample is taken, but rarely used to correct or retrain models.",
            "publication_bias_discussion": true,
            "limitations_challenges": "Major challenges: models learn dataset-specific cues (e.g., identifier overlap, functionality imbalance) rather than true semantic equivalence; many studies assume pairs across different functionalities are negatives, creating additional mislabelling; lack of comprehensive pairwise manual validation undermines claims of generalization.",
            "domain_specific_factors": "Factors include label-construction artefacts (exemplar overlap, single-judge labels), strong class imbalance (dominance of few functionalities like 'Copy File'), and heuristics used to assemble true/false pairs across functionalities that introduce systematic errors.",
            "uuid": "e2221.1"
        },
        {
            "name_short": "GPT-4o validation",
            "name_full": "OpenAI GPT-4o used as a labelling assistant",
            "brief_description": "A large language model (GPT-4o) used by the authors to independently label the sampled WT3/T4 pairs for functional similarity, providing a computational proxy/assistant to human adjudication.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o-2024-08-06 (LLM labeller)",
            "system_description": "LLM prompted with a functional similarity definition and asked to classify each of the 406 pairs deterministically (temperature=0, top_p=0); run five times and majority-vote used to reduce stochasticity. Provided label and textual explanations which human authors used to revisit disagreements.",
            "domain": "software engineering / LLM-assisted code analysis",
            "proxy_metric_name": "LLM-assigned functional-similarity labels (YES/NO) applied to the sampled pairs",
            "proxy_metric_description": "Model classifies pairs as functionally similar or not, based on prompt instructions mirroring the human protocol; serves as an automated proxy for manual pairwise validation.",
            "proxy_metric_type": "data-driven ML prediction",
            "ground_truth_metric": "Human expert adjudication (final consensus labels) used as ground truth for comparison with LLM labels in this study.",
            "ground_truth_description": "Two authors independently labelled the 406 pairs with a stricter functional-similarity criterion; disagreements were discussed and resolved to produce final human labels used as ground truth for LLM comparison.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "LLM labelled 21 pairs as similar and 385 as not; human final labels: 27 similar / 379 not. Agreement: LLM vs humans 97.0% observed agreement, Cohen's Kappa = 0.735. 12 pairs disagreed and were re-examined: 10 human labels changed after considering LLM explanations (2 accepted, 10 rejected) resulting in a 0.5% net label change (from 93.3% to 93.8% wrong labels).",
            "proxy_performance": "LLM matched humans on 394/406 pairs originally (97.0% agreement); labelled 21 pairs similar while humans labelled 27 similar (after final adjudication).",
            "ground_truth_performance": "Human consensus used as ground truth; LLM disagreements led humans to revise a small number of labels (net change 0.5% of sample).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "LLM used as an assistive validation tool; its high agreement suggests it reliably flags borderline logical/encoding differences (e.g., different hash algorithms or encodings) that humans sometimes miss.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "LLM-assisted review: authors used LLM explanations to re-examine disputed cases, which led to correction of a few human errors (e.g., detection of a logical bug missed by humans).",
            "gap_reduction_effectiveness": "Small effect in this sample: LLM assistance caused net label change of 0.5% (from 93.3% to 93.8% wrong labels), and helped identify some human oversights; agreement metrics indicate LLM is a useful adjunct but not a full replacement.",
            "validation_cost_comparison": "Using an LLM provides faster, cheaper repeated passes over the sample compared to adding more human reviewers; the paper reports the authors ran the LLM five times with deterministic settings to reduce stochasticity. No monetary/time figures provided.",
            "temporal_validation": null,
            "domain_maturity": "LLMs are emerging as practical assistants for code semantic judgements; in this context GPT-4o performed at substantial agreement with human experts.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": false,
            "proxy_correlation": "",
            "validation_cascade": "Human consensus ground-truth ← LLM labelling used in parallel; LLM explanations prompted human re-inspection of disputed cases.",
            "publication_bias_discussion": false,
            "limitations_challenges": "LLM occasionally flags differences that are borderline (e.g., MD5 vs SHA-1 or encoding differences) which humans may consider insignificant for 'functional similarity'; LLM deterministic runs can still have occasional stochasticity; LLM is not a definitive arbiter but a helpful assistant.",
            "domain_specific_factors": "LLM sensitivity to low-level differences (different hash types or encodings) can cause disagreement with human definitions of 'functional similarity' depending on strictness of criterion.",
            "uuid": "e2221.2"
        },
        {
            "name_short": "CodeXGLUE (modified BCB)",
            "name_full": "CodeXGLUE clone-detection subset derived from BigCloneBench",
            "brief_description": "A machine-learning benchmark suite (CodeXGLUE) that includes a modified BigCloneBench clone-detection subset; the paper identifies inconsistencies and unexplained changes in pair counts compared to original BCB.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "CodeXGLUE BigCloneBench subset",
            "system_description": "A dataset distribution within CodeXGLUE that re-packages BigCloneBench pairs into ML-ready training/validation/test splits; modifications include altered counts of true/false pairs and additional derived false pairs not present in the original release.",
            "domain": "software engineering / ML benchmarks",
            "proxy_metric_name": "Same as BigCloneBench labels (repackaged) — used as proxy ground truth in CodeXGLUE experiments",
            "proxy_metric_description": "Reused BCB-derived pairwise labels (true/false) possibly extended or modified; authors detected suspicious additional false pairs involving samples (e.g., sample 22442270) paired with methods across many functionalities, indicating derivation mistakes.",
            "proxy_metric_type": "empirical surrogate",
            "ground_truth_metric": "None provided by CodeXGLUE for pairwise WT3/T4 validation; the paper treats manual human validation as the independent ground truth.",
            "ground_truth_description": "Paper's manual sample validation exposes discrepancies between CodeXGLUE-provided labels and strict human judgement.",
            "has_both_proxy_and_ground_truth": false,
            "quantitative_gap_measure": "Paper reports CodeXGLUE contains 1,170,339 false clone pairs and 561,521 true clone pairs in one downloaded version, numbers that differ from original BCB (≈288k false, ≈8.9M true pairs), indicating substantial modifications without documented justification; quality of WT3/T4 labels remains unverified and likely suffers the same ~93% false-positive issue for WT3/T4 pairs.",
            "proxy_performance": "Not directly reported for CodeXGLUE in this paper, but the underlying proxy is BCB labels which lead to high reported ML metrics in the literature.",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Derived-data redistribution (in-distribution) that may amplify BCB artefacts; not a source of genuine novel validation.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "No effective correction documented; paper warns users that CodeXGLUE's modified BCB subset inherits BCB's problems and may introduce additional unexplained labelling errors.",
            "gap_reduction_effectiveness": "Not demonstrated.",
            "validation_cost_comparison": "CodeXGLUE repackaging reduces barrier-to-use for ML experiments (computationally cheap), but paper argues it shifts validation costs to users who must manually check labels to ensure semantic correctness.",
            "temporal_validation": null,
            "domain_maturity": "Benchmark repackaging is common but requires careful provenance tracking; here provenance is weak and undermines dataset maturity.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": null,
            "proxy_correlation": "",
            "validation_cascade": "CodeXGLUE ← reprocessed BCB pairs ← original BCB exemplar/heuristic labelling; errors in original propagate and may be amplified.",
            "publication_bias_discussion": false,
            "limitations_challenges": "CodeXGLUE's BCB subset contains unexplained pair-count changes and added false pairs; users may unknowingly inherit BCB's WT3/T4 label flaws leading to invalid ML evaluations.",
            "domain_specific_factors": "Dataset provenance and undocumented transformations; reuse of single-judge labels; mixing of different BCB versions.",
            "uuid": "e2221.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BigCloneBench",
            "rating": 2
        },
        {
            "paper_title": "BigCloneEval: A clone detection tool evaluation framework with BigCloneBench",
            "rating": 2
        },
        {
            "paper_title": "BigCloneBench Considered Harmful for Machine Learning",
            "rating": 2
        },
        {
            "paper_title": "Assessing and improving an evaluation dataset for detecting semantic code clones via deep learning",
            "rating": 1
        },
        {
            "paper_title": "Assessing and improving dataset and evaluation methodology in deep learning for code clone detection",
            "rating": 1
        }
    ],
    "cost": 0.018997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How the Misuse of a Dataset Harmed Semantic Clone Detection
7 May 2025</p>
<p>Jens Krinke 0000-0003-1009-2861
UCL Computer Science
Faculty of Information and Communication Technology
University College London Chaiyong Ragkhitwetsagul
Mahidol University</p>
<p>How the Misuse of a Dataset Harmed Semantic Clone Detection
7 May 2025DB84C36867AA396C256693F18FF43449arXiv:2505.04311v1[cs.SE]Clone detectioncode similaritymachine learning
BigCloneBench is a well-known and widely-used large-scale dataset for the evaluation of recall of clone detection tools.It has been beneficial for research on clone detection and has become a standard in evaluating the performance of clone detection tools.More recently, it has also been widely used as a dataset to evaluate machine learning approaches to semantic clone detection or code similarity detection for functional or semantic similarity.This paper demonstrates that BigCloneBench is problematic to use as ground truth for learning or evaluating semantic code similarity, and highlights the aspects of BigCloneBench that affect the ground truth quality.A manual investigation of a statistically significant random sample of 406 Weak Type-3/Type-4 clone pairs revealed that 93% of them do not have a similar functionality and are therefore mislabelled.In a literature review of 179 papers that use BigCloneBench as a dataset, we found 139 papers that used BigCloneBench to evaluate semantic clone detection and where the results are threatened in their validity by the mislabelling.As such, these papers often report high F1 scores (e.g., above 0.9), which indicates overfitting to dataset-specific artefacts rather than genuine semantic similarity detection.We emphasise that using BigCloneBench remains valid for the intended purpose of evaluating syntactic or textual clone detection of Type-1, Type-2, and Type-3 clones.We acknowledge the important contributions of BigCloneBench to two decades of traditional clone detection research.However, the usage of BigCloneBench beyond the intended purpose without careful consideration of its limitations has lead to misleading results and conclusions, and potentially harmed the field of semantic clone detection.We encourage a more critical and responsible usage of benchmarks and datasets, and ask for more rigorous validation during dataset creation and usage in the future.</p>
<p>I. INTRODUCTION</p>
<p>The performance evaluation of clone detection or code similarity detection approaches is a common problem that needs well-constructed benchmarks.There have been a series of benchmarks [1]- [3] that have been used in the field of clone detection, however, most of them are of a limited scale due to the effort required in manually constructing the benchmarks.BigCloneBench [3]- [6] is a large-scale dataset of clones mainly targeted at the evaluation of recall of clone detection tools.Many papers have used BigCloneBench to evaluate and compare the performance of clone detection tools and it has become standard in the field of clone detection to evaluate the recall with BigCloneBench together with a manual evaluation of a sample to evaluate precision.</p>
<p>Recent approaches to clone detection and code similarity detection are based on machine learning from large-scale datasets.It is tempting to use BigCloneBench as the ground truth for learning code similarity.However, recent work [7] has highlighted that BigCloneBench is problematic for machine learning approaches and for the evaluation of Type-4 Clone Detectors (Type-4 Clones are also known as Semantic Clones).BigCloneBench has been created in a semi-automatic way with multiple steps.The semi-automatic approach leads to a situation in which the majority of the true positives (true clone pairs) have not been manually validated that they are indeed clones of each other.Moreover, only a small set of true negatives (true non-clone pairs) has been created and, for most of the possible pairs in the dataset, the ground truth is unknown.</p>
<p>The small size of the set of true negatives is a problem for the machine learning approaches because the ground truth is not representative and the way the true positives and negatives are constructed makes the ground truth biased.Not taking the imbalance and bias into account can lead to misleading results.Moreover, the way the dataset is constructed can lead to the misperception that any pair that is not in the ground truth of true positives is a true negative.This misperception can lead to invalid results and is a common problem in published papers.However, the largest problem is the way in which methods have been labelled as having a specific functionality and assuming that two methods sharing a functionality are clones of each other.</p>
<p>Problem Statement: BigCloneBench, a dataset originally designed for evaluating syntactic clone detection, has been widely repurposed in the literature for evaluating semantic clone detection and training machine learning models.However, this misuse is very problematic: the dataset construction does not support reliable ground truth for functional similarity, particularly in the WT3/T4 clone pairs, which constitute 95% of the dataset.This paper provides empirical evidence of this misuse and its implications for the validity of published research.</p>
<p>The contributions of this paper are:</p>
<p>• A manual investigation of a stratified statistically significant random sample of 406 of the automatically constructed Weak Type-3/Type-4 clone pairs.</p>
<p>• A literature review of 179 papers that use BigCloneBench as a dataset, identifying 139 papers using BigCloneBench for evaluating semantic clone detection where the results are threatened in their validity.</p>
<p>• A discussion of the features of BigCloneBench that affect the ground truth quality and a discussion of common misperceptions about the ground truth.</p>
<p>• A discussion of the usage of BigCloneBench as ground truth for learning code similarity and how the issues and misperceptions affect the validity of results.Despite its value in traditional clone detection, the misuse of BigCloneBench for semantic clone detection has introduced systematic flaws in a large body of research.This paper investigates and quantifies this issue and provides evidence that such misuse has led to misleading conclusions in at least 139 papers.It is hoped that this paper will help the clone detection and code similarity communities to prevent producing invalid results by misusing BigCloneBench (or other datasets) in the future.</p>
<p>II. A SHORT INTRODUCTION TO CLONE DETECTION AND CODE SIMILARITY DETECTION</p>
<p>In recent years, the importance of clone detection and code similarity detection has grown significantly.Clone detection is the process of identifying similar code fragments in a codebase.The term "clone" is used to refer to code fragments that are similar to each other.However, there is no definition of what a clone is and the usual definition1 of a clone pair is a pair of code fragments that are similar to each other according to some definition of similarity.</p>
<p>If a two code fragments are similar to each other, they can be classified into one of the following types of clones:</p>
<p>• Type-1 clones are identical code fragments that only differ in their whitespace.</p>
<p>• Type-2 clones are identical code fragments that only differ in their whitespace, identifiers and literal values.</p>
<p>• Type-3 clones are similar code fragments that only differ in their whitespace, identifiers and literal values, and where lines or statements have been added or deleted.</p>
<p>• Type-4 clones are code fragments that are functionally similar.The definition of the four types vary in the literature and the definitions are not always consistent.The above definition therefore is also slightly different from other definitions.For example, the original definition of Type-3 and Type-4 clones [8] have significant differences to the definition [9] often cited in clone detection work:</p>
<p>A Type-3 clone is</p>
<p>• A clone with very similar source code, but with small changes made to the code to tailor it to some new function [8].</p>
<p>• A copied fragment with further modifications such as changed, added, or removed statements, in addition to variations in identifiers, literals, types, whitespace, layout and comments [9].A Type-4 clone is</p>
<p>• A functionally identical clone, possibly with the originator unaware that there is a function already available that accomplishes essentially the same function [8].</p>
<p>• Two or more code fragments that perform the same computation but are implemented by different syntactic variants [9].Type-4 clones are considered semantic clones and the process of identifying semantic clones is called semantic clone detection.Note that Type-4 clones as defined above require identical functionality or the same computation.According to Rice's theorem, the above definitions are not decidable in general and similar to the definition of clones, there is no generally accepted definition of semantic clones 2 .Therefore, we use the definition that two snippets are semantic clones if they are functionally similar to some definition of functional similarity.As a non-formal definition for similar functionality we use the following:</p>
<p>Two code snippets are considered functionally similar when they achieve the same result or perform the same task, even if they differ in syntax, structure, or the specific steps they take to accomplish that task.Compared to the definition from Roy et al. [9], we drop the requirement that they are implemented by different syntactic variants.The reason is that Type-3 clones do not require functional similarity and a pair of Type-3 clones may or may not be functionally similar.Moreover, the definitions of semantic clones also usually do not require textual or syntactic dissimilarity.</p>
<p>It is important to note that the definitions of clones and semantic clones are subject to interpretation and may vary across different contexts.</p>
<p>III. BIGCLONEBENCH</p>
<p>BigCloneBench is available in multiple versions.An initial version [4] only contained clones derived from 10 functionalities, 42 exemplar functions, 6,000 snippets tagged as true positives and 53,688 snippets tagged as false positives, leading to 6,164,953 true clone pairs and 258,574 false clone pairs.There exist at least two released versions of the earlier version of BigCloneBench, however, the released versions are slightly different from the reported numbers above.The second version of the benchmark was introduced in 2016 as a part of BigCloneEval [5], [12], a clone detection tool evaluation framework with BigCloneBench.It contains 43 functionalities with a total of 7,868,560 true clone pairs [5], later expanded to 8,375,313 true clone pairs [12].The most detailed explanation is available in Svajlenko's Thesis [6], where the size is given as 8,915,130 true clone pairs and 288,367 false clone pairs.The discussion in this paper mainly uses the reported numbers [6] and will sometimes use numbers extracted from the released version of BigCloneBench which has been released individually 3 and as a part of the evaluation framework 4 .</p>
<p>To understand the features of BigCloneBench that affect the ground truth quality, it is necessary to understand how Big-CloneBench has been constructed.At the core of the construction is the validation of methods by human judges: Instead of asking the judges to validate if two code fragments are similar, they have been asked to validate if an individual function implements a target functionality.Clones are identified as code fragments that share functionality [6].The authors of BigCloneBench chose this approach to reduce subjectivity in judging whether two code fragments are clones of each other.Note that the identification only requires the fragments to share functionality without specifying the amount of shared functionality, potentially leading pairs of methods that are not clones to be labelled as true clone pairs because they share a minor functionality while their main overall functionality is very different.However, the Type-4 definition they presented is syntactically dissimilar code snippets that implement the same functionality [4], [5], [12], which is a much stronger requirement than the one used in the construction of BigCloneBench.</p>
<p>We present the construction process of BigCloneBench in a simplified way in the following.Source Code: BigCloneBench is built from the IJaDataset 2.0, a dataset of 250M LOC in 2.5M Java files from 25K projects mined from SourceForge and Google Code [6].</p>
<p>Exemplar Functions: The core of BigCloneBench is a set of 101 exemplar functions which have been composed as example implementations of 43 selected functionalities that are expected to appear often.22 of the 43 selected functionalities have a single exemplar function.In earlier papers about BigCloneBench, these exemplar functions were termed "sample snippets"; however, in this paper, we use the clearer term "exemplar functions" as it has been introduced in newer publications on BigCloneBench [3].An example of a selected functionality is "Copy File" and a corresponding exemplar function is shown in Figure 1.For the "Copy File" functionality, six exemplar functions have been created.</p>
<p>Specifications: For each of the 43 functionalities, a specification has been created that describes the functionality.The specifications are kept simple and are not intended to be exhaustive, and they usually do not contain any details.For example, the specifications for the "Copy File" functionality is simply "copies a file".</p>
<p>Potential Clones: From the exemplar functions and the specification of the 43 functionalities, a heuristic search has been performed to find methods that are using the 43 functionalities.The heuristic search has resulted in 77,933 methods which are the candidates for potential clones within each of the 43 functionalities.</p>
<p>For example, Figure 2 shows the search terms to identify methods that contain the "Copy File" functionality, extracted from the downloaded dataset.Note that the search is not syntactically correct as given.37,102 methods matched the search heuristic for the "Copy File" functionality and are candidates for potential clones.Labelling of Potential Clones: A set of judges then compared the candidate methods of the 43 functionalities to the specifications and exemplar functions of a functionality and labelled them as true positives or false positives.Only a small number of methods (9,533 -12%) have been labelled by more than one judge and the final label for them is the majority vote (887 methods with an equal number of votes are labelled as undecided).</p>
<p>The search heuristics have been designed so that they identify as many true positive snippets as possible without overburdening the judges in their tagging efforts.However, instructions to the judges were set in a way that allowed that target functionality to be only a part of the method: "True positives may exceed the specification by performing additional related or unrelated tasks."[6].This even allowed methods to be tagged as true positive when the target functionality is only a small part of the method.Following the usual definition of Type-4 clone pairs (snippets that implement the same functionality), then not requiring the target functionality to be the main purpose of the method leads to a situation where the ground truth for true Type-4 clone pairs is flawed as methods that are not sharing the same major functionality and only sharing a minor functionality are labelled as true positives.</p>
<p>For the "Copy File" functionality, the heuristic search has identified 37,102 true positive snippets and a judge has labelled 3,084 of them as true positives and 34,018 as false positives.All snippets in this functionality have been labelled by a single judge.</p>
<p>Overall, 15,290 methods have been labelled as true positives and 61,756 methods have been labelled as false positives.However, the heuristic search can retrieve the same method for more than one functionality.For example, snippet 22442270 has been labelled as a false positive for the "Copy File" functionality and as a true positive for the "Download From Web" functionality.Snippet 10151252 has been labelled as a true positive for the "Copy File" functionality because the snippet copies (uploads) a file to an FTP server and also as a true positive for the "Connect to FTP Server" functionality.</p>
<p>The released dataset has slightly different numbers and contains 75,673 retrieved methods.Of the retrieved methods, 73,906 are unique and 1,723 appear for more than one functionality.Moreover, from the 14,891 methods labelled as true positives, 14,679 are unique and from the 60,782 methods labelled as false positives, 60,019 are unique.209 methods are true positives for more than one functionality.</p>
<p>Ground Truth: The ground truth 5 is constructed from the exemplar functions and the potential clones.The exemplar functions are in sets X f for each of the 43 functionalities (f ).The methods that contain a functionality similar to the specified functionality or the exemplar functions (true positives) are in sets P f for each of the 43 functionalities (f ) and the methods judged as false positives are in sets N f .The ground truth is constructed per functionality f , i.e., for pairs of methods (m, n) f .The ground truth is constructed as follows: The pairs (m, n) f and (n, m) f are labelled as true positive if (m ∈ X f ∪ P f ) ∧ (n ∈ X f ∪ P f ), i.e., if both methods are exemplar functions or labelled as true positives.The pairs (m, n) f and (n, m) f are labelled as false positive if an f exists such that (m ∈ X f ) ∧ (n ∈ N f ).Note that false-positive pairs are only constructed for pairs of an exemplar function and a method labelled as true negative, i.e., pairs of two methods labelled as true negative are unlabelled pairs (as they could still be clones of each other).</p>
<p>For the running example of the "Copy File" functionality, 6 exemplar functions and 3,084 true-positive snippets lead to 4,772,505 pairs labelled as true clone pairs and 6 exemplar functions and 34,018 false-positive snippets lead to 204,108 pairs labelled as false clone pairs.The above construction leads to a ground truth of 8,915,130 true clone pairs and 288,367 false clone pairs.It is important to highlight that none of the true or false clone pairs has been manually validated.</p>
<p>Automatic Classification: In the last step of the construction, each true clone pair is classified as Type-1 (T1), Type-2 (T2), Very-Strongly Type-3 (VST3), Strongly Type-3 (ST3), Moderately Type-3 (MT3), and Weakly Type-3/Type-4 (WT3/T4).For the comparison, all methods are normalised by removing all comments and pretty printing.Type-1 are pairs that only differ in their whitespace.For Type-2, the methods are identical after all identifiers are replaced by a common identifier and all literal values are replaced by a common literal value.For all other pairs, the similarity of the two methods is measured as the minimum ratio of the lines or tokens one method shares with the other after normalisation.If the similarity is below 0.5, the pair is classified as a weakly Type-3 or Type-4 clone pair.Otherwise, the pair is classified as a very strong Type-3 (≥0.9), strong Type-3 clone pair (≥0.7) or moderately Type-3 clone pair (≥0.5).</p>
<p>It is worth noting that 8,498,894 out of 8,915,130 pairs have been classified as Weakly Type-3/Type-4 (95%).</p>
<p>Precision and Recall:</p>
<p>The ground truth can be used to measure the recall of clone detection tools.BigCloneBench has not been intended for and should not be used directly to measure precision.The reason is that for most method pairs no ground truth is available and it is unknown whether the pair is a true positive or a false positive.Instead, precision needs to be evaluated differently, by manually investigating a representative sample of the results of the clone detection or code similarity detection.With the available ground truth data, only the lower and upper bound for the precision can be determined which can be used for a precision estimation.</p>
<p>BigCloneBench for Semantic Clone Detection: It is important to highlight that the created dataset has not been intended for semantic clone detection [13].Instead, the automatic classification should be seen resulting in pairs that share some (minor) functionality and are textual similar to a certain degree.The usual definition of Type-1, Type-2, and Type-3 clones only requires textual similarity and does not require functional similarity.If one accepts that two methods which are textually similar of at least 50% according to some textual similarity measure and contain some shared functionality are indeed clones, then the subset consisting of Type-1, Type-2, and moderate, strong, and very strong Type-3 clones is not problematic as they are textual similar.</p>
<p>However, the WT3/T4 clone pairs are problematic as they are not textually similar and only share a potentially only minor functionality.The WT3/T4 clone pairs are the majority of the clone pairs in BigCloneBench, and they are the ones important for semantic clone detection.</p>
<p>IV. GROUND TRUTH QUALITY</p>
<p>The above observations lead to the question of the quality of the ground truth if it is used for Semantic Clone Detection or Code Similarity Detection.The ground truth of BigCloneBench has not been manually validated and the authors of BigCloneBench have not provided any information on the quality of the ground truth.However, ground truth quality is hugely important when a dataset is used in evaluating machine learning approaches.There are many aspects of the ground truth that affect the quality of the dataset and the results of machine learning approaches, but the accuracy of the labelling is the most important one.Therefore, we will focus our attention on the ground truth quality of WT3/T4 clone pairs labelled as true clone pairs.</p>
<p>It has been reported [3], [6] that at least one judge disagreed with the others for 14.5% of the 9,533 methods that have been labelled by more than one judge.They extrapolate the average disagreement to estimate that at least 15% of the clones across the benchmark are subjective or have validation errors.Manual validation of clone pairs is also subjective [14].</p>
<p>An example of a wrongly labelled snippet for the "Copy file" functionality is snippet 199620356 in file 184404.java,lines 42-44.The snippet stores a constant string in a hash table (the text of the constant string is source code of a large method that includes file operations) and does not copy a file.There are many three-line methods for the copy file functionality.For some of them, there is only a weak semantic similarity.Figure 3 shows such a method.Its purpose is to dump the configuration to the standard output and the snippets in Figure 1 and Figure 3 would not be considered clones of each other as they are not textually similar (Types 1-3) or implementing the same functionality (Type 4).</p>
<p>Moreover, we identified at least 330 snippets in the "Copy File" functionality that do not contain the word 'file'.This is an indication that the snippets are not copying files but instead, for example, use IOUtils.copy to copy data between streams that are not files.</p>
<p>Based on the above explanations that 88% of the snippets have been labelled by a single judge and that at least 15% of the clones across the benchmark are subjective or have validation errors, we can make the observations that BigCloneBench's labelled snippet ground truth quality is limited.</p>
<p>Another important observation is that the ground truth for method pairs (m, n) within the same P f (they are labeled containing the same functionality) assumes that the two methods are indeed clones of each other or are similar to each other.No manual evaluation of this assumption has been done and therefore the quality of the ground truth for such pairs is unknown.As discussed already above, it is possible that two methods are labelled as a true clone pair even if they are not clones of each other, because they share a minor functionality while their main functionality is very different.</p>
<p>Consider the functionality "Copy File" again which has six exemplar functions, most of them are very small with the smallest having only three lines of code 7 and the largest having 19 lines of code.The description of the functionality is simply "Copies a file".The smallest exemplar function for this functionality is shown in Figure 1.</p>
<p>The largest method 8 judged to be a true positive for the copy file functionality is 762 lines long.This method does indeed contain functionality to copy a file, however, this is only a small part of the functionality.One can argue that the 762 LOC method and the 3 LOC method are somewhat related as both contain the functionality to copy a file, but the main functionality of the 762 LOC method is different than to copy a file and the two methods should not be considered clones of each other.Like the 762 LOC method, many methods in the set of true positives will not only have the functionality of copying a file but will also contain other functionalities.One should therefore not assume that methods judged to be true positives to contain a specific functionality are necessarily clones of each other.This shows an issue in the manual tagging of the potential clones.Based on the way the dataset is constructed, an exemplar function is supposed to contain code that performs one specific functionality.By considering large potential clones, which can contain many functionalities, as true clones, the creation of true clone pairs is no longer valid in all cases.</p>
<p>Thus, at least some of the pairs (m, n) within the same P f should not be considered true positives.This mostly affects the Weakly Type-3/Type-4 clone pairs since they account for 95% of all clone pairs in the ground truth.Therefore, we conclude that BigCloneBench's true WT3/T4 clone pairs ground truth is flawed (under the usual Type-4 definition of code snippets that implement the same functionality).</p>
<p>V. INVESTIGATION OF TRUE CLONE PAIRS</p>
<p>The previous section has identified two important observations: 1) The ground truth of clone pairs in BigCloneBench has not been manually validated.</p>
<p>2) Two methods that share a minor functionality but differ in their main functionality may be wrongly labelled as true positives.It is not clear to what extent the true clone pairs have been wrongly labelled as no manual checking of clone pairs has been done.We therefore want to investigate the amount of wrongly labelled true clone pairs in BigCloneBench.The focus of our investigation is on the Weak Type-3/Type-4 (WT3/T4) clone pairs as they are the majority of the clone pairs in BigCloneBench and they are the ones important for semantic clone detection.The pairs labelled as Type-1 or Type-2 clone pairs are unproblematic as they are identical or only differ in their whitespace or identifiers and therefore are likely true clone pairs.The clone pairs labelled as moderate, strong or very strong Type-3 clone pairs are also unproblematic as they are textual similar.It is important to note that the usual definition of Type-1, Type-2, and Type-3 clones pairs only requires textual similarity and does not require functional similarity.</p>
<p>In our investigation, we manually checked a statistically significant random sample of true clone pairs (95% confidence level and 5% margin of error) classified as WT3/T4 clones.The sample has been stratified over all 43 functionalities.However, stratification alone would result in 20 functionalities not covered by the sample at all.Therefore, the sample has been extended to cover all functionalities by adding a random clone pair for each of the 20 functionalities not covered by the stratified sample.In the end, the sample contained 406 clone pairs.</p>
<p>A. Manual Investigation</p>
<p>We checked whether each of the 406 pairs is indeed a true clone pair where the methods are functionally similar, i.e., their main functionality is the same.We also checked whether the methods in the pair have been labelled correctly if we apply a stricter criterion: Does the method implement the functionality as its main or only purpose?For example, when investigating the "Copy File" functionality, methods that (A) encode, convert or rewrite, (B) read or write to a stream that is not a file, or (C) are (unit) tests are not considered fulfilling the requirement.The sample has been independently verified by both authors.</p>
<p>From the 406 samples9 , only 23 have been labelled as true positives by both authors, 42 have been labelled differently by both authors, and 341 have been labelled as false positives by both authors.The observed inter-rater reliability is 89.7%, the expected reliability is 30.3%, and Cohen's Kappa is 0.476 which indicates moderate agreement between both raters.Both raters have discussed each of the 42 samples where they have labelled differently and have come to an agreement that only 4 of them are true positives.The final agreement is that only 27 out of the 406 samples are true positives and 379 are false positives (93.3%).When considering only the original stratified random sample of 386 pairs without the 20 additional pairs, 21 are true positives and 365 are false positives (94.6%).</p>
<p>The number of identified false positives is surprisingly high and demonstrates a likely strong impact of the flawed ground truth construction on the ground truth quality, indicating that the ground truth for WT3/T4 clone pairs is flawed (under the usual Type-4 definition of code snippets that implement the same functionality).This is important for any evaluation of clone detectors in the WT3/T4 category, where the results are likely to be invalid.</p>
<p>When looking at the three functionalities with the most samples, the results are different.For the functionality with the most samples ("Copy File"), only 4.7% of the samples are true positives.Interestingly, for the functionality with the second most samples ("Zip Files"), 24.3% of the samples are true positives.For the functionality with the third most samples ("Secure Hash"), 0% of the samples are true positives.The difference is due to the complexity of the functionalities.For the "Copy File" functionality, the methods are usually very simple and the functionality is easy to implement by using a library function.Moreover, the functionality is often used in other functionalities or for specific purposes, e.g., creating a backup, writing data to a file of a specific format, or sending data over a network.For the "Secure Hash" functionality, the functionality is rarely used on its own and the hash is usually used for a specific purpose.For the "Zip Files" functionality, the methods are more complex and the functionality is harder to implement, therefore fewer methods have additional functionalities and are more likely to be true positives.</p>
<p>The 406 sampled pairs involve 779 methods and 32 methods appear more than once in a pair.From the 779 methods, 189 have been labelled as true positives by both authors, 455 of them have been labelled as false positives by both authors, and 136 of them have been labelled differently by both authors.Note that out of the 32 methods, one of the authors has labelled one method differently in the different pairs.Ignoring the method that has been labelled differently by one author, the observed inter-rater reliability is 82.6%, the expected reliability is 54.7%, and Cohen's Kappa is 0.617 which indicates substantial agreement between both raters.</p>
<p>Again, the number of false positives is very high which casts doubts on the ground truth quality created from the manual labelling of snippets.However, note that the manual validation asked for a stricter criterion than the original ground truth construction.</p>
<p>The manual investigation of the true clone pairs has shown that the ground truth for WT3/T4 clone pairs is seriously flawed as the sample contained 93.3% false positives.Although traditional methods for clone detection are often evaluated on the full BigCloneBench including the WT3/T4 subset, they are usually not affected in their validity.Such papers report extremely low recall for the WT3/T4 subset and usually argue that the complexity for such clone pairs is the reason for the low recall and that the presented approach is not targeted at WT3/T4 clones.The real recall of the approach is very likely to be higher as most of the WT3/T4 pairs are not You are an experienced software engineer proficient in analysing source code.</p>
<p>Your task is to analyse two code snippets and check if they are functionally similar.Two code snippets are considered functionally similar when they achieve the same result or perform the same task, even if they differ in syntax, structure, or the specific steps they take to accomplish that task.</p>
<p>Code Snippet 1: """ {contentA} """ Code Snippet 2: """ {contentB} """ Compare the two Java code snippets.Answer "YES-SIMILAR" if they are functionally similar.Answer "NO-NOT-SIMILAR" if they are not functionally similar.Answer "DONT-KNOW" if it is not clear if they are similar.</p>
<p>Format: ANSWER: <answer> EXPLANATION: <explanation></p>
<p>B. Comparison with an LLM</p>
<p>Because the agreement between the two authors was only moderate and the discussion between the authors revealed differences in which functional similarity can be interpreted, another experiment was done.Instead of adding another human investigator, an LLM was added to label the 406 pairs.</p>
<p>All 406 pairs were given to Open AI's GPT-4o LLM (gpt-4o-2024-08-06).To achieve mostly deterministic results, the API was set to use temperature=0 and top_p=0.The prompt for the LLM is shown in Figure 4 and contains our definition of functional similarity.As deterministic results are not guaranteed even with such settings, the experiment was executed five times.</p>
<p>Over the five runs, only a single pair had a diverging result in one run.We discarded the diverging result and used the majority vote for the final result.The LLM labelled 21 pairs as functionally similar and 385 pairs as not functionally similar.On 12 pairs, the human investigators and the LLM disagreed.The observed agreement is 97.0%, the expected agreement is 88.9%, and Cohen's Kappa is 0.735 which indicates substantial agreement between the human investigators and the LLM.</p>
<p>All 12 pairs where the human investigators and the LLM disagreed have been manually investigated again, taking into consideration the LLM's explanation.Interestingly, 7 out of the 12 pairs belong to the functionality "Secure Hash" and three belong to the functionality "Copy File".One pair belongs to the functionality "Binary Search" and one to the functionality "Test Palindrome".</p>
<p>For the 7 "Secure Hash" functionality pairs, the LLM has labelled the pairs as not functionally similar.The explanation for five pairs was a different hash being used in the two methods (e.g., MD5 vs. SHA-1).For the other two pairs, the explanation was that the methods were using different encodings (e.g., base64 vs. hexadecimal or UTF-8 vs. ISO-8859-1).The human investigators rejected all seven disagreements because the differences were not significant enough to consider the methods as not functionally similar.</p>
<p>The LLM has labelled the three "Copy File" functionality pairs as functionally similar.However, the human investigators have again rejected the LLM results because each pair had significant differences in the functionality, e.g., one was a general method and the other was a full program (main method) to copy a file.</p>
<p>The "Binary Search" functionality pair differs in the use of data structures and additional functionality, leading the LLM to label the pair as not functionally similar.The human investigator reconsidered the pair and agreed with the LLM.</p>
<p>The "Test Palindrome" functionality pair is an interesting case.The LLM has labelled the pair as not functionally similar as it detected a logical error in one of the methods.The human investigators have not initially detected the logical error and had labelled the pair as functionally similar.After the LLM has pointed out the logical error, the human investigators have agreed with the LLM.Overall, 10 LLM results have been rejected and 2 have been accepted (for 394 pairs, the LLM and the human investigators agreed already).Note that the change of two labels does not significantly change the results of the manual validation as only 0.5% of the labels changed (from 93.3% to 93.8% of wrong labels of pairs).</p>
<p>There are three important observations from the manual investigation: 1) All 12 pairs where the human investigators and the LLM disagreed could be considered borderline cases were even different human investigators could disagree.2) LLMs could identify and point out significant differences in the methods which human investigators overlook.</p>
<p>3) The disagreement between the human investigators and the LLM was less than the initial disagreement between the two human investigators.The observations suggest that LLMs can potentially be used successfully to support human investigators in the manual validation of clone pairs.Overall, the use of an LLM in our manual investigation has confirmed the results of the human investigators and has shown that the human investigators have missed some important differences in the methods, leading a slightly higher number of identified false positives.</p>
<p>VI. LITERATURE STUDY</p>
<p>In recent years, BigCloneBench has been used in many papers for evaluation and as a dataset for machine learning.The manual investigation of the true WT3/T4 clone pairs has shown that the ground truth for WT3/T4 clone pairs is flawed, which threatens the validity of the results of the papers that have used BigCloneBench's WT3/T4 pairs for evaluation or as a dataset for machine learning.To evaluate the impact of the observed flaw, we have performed a literature search to identify papers that have used BigCloneBench's WT3/T4 pairs and where the results of the paper are threatened in their validity.</p>
<p>A. Paper Collection</p>
<p>Following a methodology used by Pasuksmit et al. [15] in their systematic literature review, we collected previously published work that are related to BigCloneBench from five sources, including IEEE Xplore, ACM Digital Library, Scopus, Web of Science, and Wiley Online Library.We performed a search using the keyword "BigCloneBench" in the full text or the metadata.We did not restrict the search to clone detection, as we wanted to include papers that use BigCloneBench for adjacent tasks, such as clone search [16].We included all found papers up to the time of the search (March 2025).This resulted in a total of 547 papers listed by the five sources (157 from IEEE Xplore, 96 from ACM Digital Library, 229 from Scopus, 60 from Web of Science, and 5 from Wiley Online Library).We then manually removed 226 duplicates, and further removed 2 retracted papers, 4 non-english papers, 3 inaccessible papers, and 5 otherwise irrelevant papers (e.g., table of contents of conference proceedings, messages from chairs, or abstracts).We also removed our own paper [7] and the original BigCloneBench paper [4] from the list, resulting in a total of 305 papers for further analysis.During the investigation we also added two papers that were using BigCloneBench, but were missed by the initial search as they have not been indexed by the digital libraries.In the end, we downloaded the PDFs of the 307 papers for our literature review.</p>
<p>B. Paper Analysis</p>
<p>The papers were checked to determine how they used the BigCloneBench dataset and its subsets.The authors' involvement in critiquing the BigCloneBench dataset and its use in previous work leads to a bias if the authors were to analyse the papers themselves.We have therefore decided to rely on an LLM to extract relevant information from the papers and to minimise the authors' bias.The LLM should provide a neutral and unbiased perspective on the papers.However, the LLM or the prompts used to generate the responses may not capture all the nuances and details present in the papers and also may introduce biases of its own.Therefore, all the extracted results were only used as an initial check and all the results were manually investigated by the first author.</p>
<p>We created a ChatGPT Assistant with the purpose of analysing a PDF.The assistant was then used via the OpenAI API to allow an automated analysis of all downloaded papers.For each paper, the PDF was uploaded to the assistant and the assistant was then executing two queries with different prompts.The first prompt had the aim of extracting necessary information and answering a series of questions about the paper.The assistant was instructed to evidence the answer by giving a direct quote of the paper.The prompt also asked the assistant to do a critical analysis of the paper and the impact of the new finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones on the validity of the paper's results.A second prompt instructed the assistant to summarise the findings in a format similar to a CSV table so that the summary can be easily extracted.The responses to the two prompts were saved for all downloaded papers for further manual analysis.</p>
<p>The instructions for the assistant and the two prompts were engineered in a first round to get responses in the desired format.The prompt engineering then continued for more rounds on a random sample of 20 papers where, during a manual investigation of the responses, opportunities for improvements were identified and applied to the prompts.After each improvement, the assistant was re-run on the sample of 20 papers, and the investigation was repeated.</p>
<p>During the initial rounds of prompt engineering, a set of LLMs were tried.Most LLMs do not provide the necessary functionality to analyse PDF files directly and were discarded.Others were discarded because of their low performance.The assistant was finally based on the OpenAI API with the GPT-4o model.</p>
<p>In the LLM-supported phase in which the LLM response and the paper itself were manually checked, further 19 survey papers and 109 papers that did not use BigCloneBench as a dataset were removed.In the end, of the 179 papers we included in the last phase of the investigation, 139 (77.7%) have used BigCloneBench for evaluation purposes of WT3/T4 clone pairs in way that threatens the validity of the results.All the papers report very high (above 80%) recall or F1 scores.For example, a paper reports a precision of 99.8%, a recall of 88.3%, and an F1 score of 93.7% for the WT3/T4 subset.Assuming the above 93.3% of wrong labels is representative, the true precision is only 6.7%, the true recall is only 5.9%, and the true F1 score is only 6.3%.Such a drastic difference does not only challenge the performance of the presented approaches in the 139 papers, it also suggests that the presented approaches do not actually learn code similarity, but instead only learn features of the dataset.It is likely that the papers overfitted the training data and the results are not generalisable to other datasets.</p>
<p>Any machine learning approach that uses BigCloneBench's ground truth for training and evaluation and reports high precision, recall, or F1 score is likely to be flawed.Our analysis identified 139 papers that are threatened in the validity of their results.</p>
<p>If the results of the 139 papers are indeed invalidated, it would cause a significant harm to the field of Semantic Clone Detection.It would question the maturity of the field to be able to evaluate the performance of semantic clone detectors and other code similarity tools and approaches.</p>
<p>While the above is the main takeaway from this paper, we will in the following section observe additional problems and misconceptions about BigCloneBench, and alternative benchmarks and datasets that are available.</p>
<p>We also observed during the literature analysis that the majority of the papers do not report on any kind of manual investigation.There are some papers that include a manual investigation of the results of the approach with the purpose of estimating the approach's precision (as BigCloneBench has been created as a benchmark for recall), but none of them uses the result of the manual investigation to validate the ground truth.Other papers [17]- [19] investigate only a small sample of results and compare them to the ground truth.It should be noted that we encountered no paper using the WT3/T4 ground truth of BigCloneBench under the assumption that pairs only need to share a minor functionality to be labelled as true pairs.</p>
<p>VII. OBSERVATIONS</p>
<p>The construction of the ground truth leads to a few important further observations which will be discussed in the following.Most observations can be derived only from the published data, without the need for manual investigation.</p>
<p>A. Relation Between Functionalities</p>
<p>Most importantly, the ground truth does not make any assumption on methods pairs where the methods appear in sets for different functionalities.For example, for a method pair (m, n) with (m ∈ X i ∪ P i ) ∧ (n ∈ X j ∪ P j ) ∧ (i = j) it cannot be assumed that (m, n) is a true negative.This is highlighted by the fact that BigCloneBench allows methods to be in the true positive sets of different functionalities and indeed has 209 of such methods m with (m ∈ P i ) ∧ (m ∈ P j ) ∧ (i = j).</p>
<p>(1) BigCloneBench does not contain information about pairs for different functionalities.</p>
<p>Any assumption that (m, n) is a true negative where the methods are from two different functionalities is not valid.Consider the two functionalities for "Copy File" and "Copy Directory".The functionality of copying a file is part of the functionality "Copy Directory" which copies a directory and its contents.At least 77 methods in the set of true positives for the functionality "Copy Directory" invoke a copyFile method.Therefore, all methods in the set of true positives for the functionality "Copy Directory" could be considered clones of methods for the "Copy File" functionality.Indeed, 19 methods in the set of true positives for the functionality "Copy Directory" have also been judged to be true positives for the "Copy File" functionality, and only three methods in the set of true positives for the functionality "Copy Directory" have been judged to be false positives for the "Copy File" functionality.However, it appears that the heuristic for the functionality "Copy File" has not retrieved most of the methods that have been retrieved for the functionality "Copy Directory".</p>
<p>(2) BigCloneBench does contain true but unlabelled clone pairs for different functionalities.</p>
<p>The above observation is important because there are datasets that are created with the assumption that methods for different functionalities are not clones of each other and for which the assumption holds.Examples for such datasets are POJ/OJClone [20] or past submissions to the Google Code Jam 10 .In such datasets, participants create small programs to solve a given task which are then checked with defined sets of tests.It is usually assumed that the solutions of a given task are similar to each other and that the solutions to different tasks are different.Some of the papers analysed in the literature review follow a similar assumption and, instead of only using the provided ground truth, the approaches construct their own ground truth by considering all methods for different functionalities as false clone pairs.For example, Yu et al. [21] uses the earlier version of BigCloneBench with 10 functionalities.They only use the 9,134 methods that have been labelled and split them in 8,134 methods for training, 500 for testing, and 500 for validation.Each of the sets is then used to create the clone pairs, assuming that methods in the same functionality are true pairs and methods from different functionalities are false pairs.Their dataset is available 11 and allowed us to investigate the snippets and true clone pairs and the false clone pairs used in the machine learning.The investigation confirmed that the created dataset expanded the ground truth provided by BigCloneBench without considering the issues we observed.For example, we could confirm the presence of wrongly generated false clone pairs caused by assuming two methods labelled as true positive for two different functionalities are not clones of each other.Consider the snippet 22442270 again, which is has been labelled by the human judges as a true positive for the "Download From Web" functionality (and as a false positive for the "Copy File" functionality).In Yu et al.'s dataset, this snippet often appears as part of false clone pairs in the expanded ground truth.It appears in false clone pairs together with other snippets from all other functionalities.</p>
<p>Wang et al. [22] seem to follow a similar approach.The paper reports that their ground truth contains 336,498 true positives and 2,080,088 false positives although the original ground truth has 6,164,953 true positives and only 258,574 false positives [4].They don't discuss how they have constructed their changed version of the ground truth 12 .Interestingly, there are a few papers [23]- [26] that report the same number of 2,080,088 false clone pairs, but they also report the original contradicting number of around 260,000 (or precisely 279,032) false clone pairs in descriptions of BigCloneBench.By downloading the datasets for two papers [22], [26], we were able to confirm that they used the exact same data for training, testing, and validation.</p>
<p>The above discussion shows that machine learning approaches using BigCloneBench are at risk when attempting to create complete ground truths, in particular when attempting to label method pairs with methods from different functionalities.</p>
<p>B. Relation Within Functionalities</p>
<p>Of lesser importance is the observation that the ground truth is even incomplete within the same functionality.As discussed above, the ground truth does not make any assumption on method pairs where both methods have been labelled as false positives for the given functionality.The reason is that such methods are not clones for any exemplar function of the given functionality, but they could be clones of each other.</p>
<p>(3) BigCloneBench does contain unlabelled true and false clone pairs for the same functionalities.</p>
<p>C. Bias and Balance</p>
<p>As the 43 functionalities have hugely varying numbers of true and false positives, the ground truth is biased and imbalanced.For example, the majority of labelled methods in the released dataset are for the functionality "Copy File", 42,664 out of 75,672.Moreover, 5,935 out of 14,891 methods labelled as true positives are for the functionality "Copy File" (40%) leading to the situation that 4,664,949 out of 8,915,130 pairs considered to be true clone pairs are for the functionality "Copy File" (54%).Over 90% of all true clone pairs are just for eight functionalities and 22 out of the 43 functionalities taken together only amount to less than 1% of all true clone pairs.The imbalance is even worse for the false positive clone pairs where 70% are pairs for the functionality "Copy File".Over 98% are pairs just for eight functionalities.32 out of the 43 functionalities taken together only amount to less than 1% of all false clone pairs.</p>
<p>(4) BigCloneBench is imbalanced and biased.</p>
<p>D. Impact on Machine Learning</p>
<p>The four observations only have a limited impact on the evaluation of clone detectors that only used the clone pairs of the ground truth.However, the bias and imbalance will have a strong impact on machine learning approaches for code similarity that learn from BigCloneBench's ground truth.For example, if a machine learning approach would mainly learn if both fragments of a pair implement some copy file functionality, it could achieve a good recall of at least 55% as 4,651,096 out of 8,498,894 true WT3/T4 clone pairs are for the "Copy File" functionality.</p>
<p>VIII. ALTERNATIVE BENCHMARKS</p>
<p>Having demonstrated the flaws in the BigCloneBench dataset, we now turn our attention to alternative benchmarks and datasets that are used for evaluating and training machine learning-based code clone detection approaches.</p>
<p>We will first discuss some alternative benchmarks that are specifically designed for clone detection, and then we will discuss datasets that are not specifically designed for clone detection but can be used for training and evaluation.Table II presents an overview of the benchmarks and datasets.It shows the name of the benchmark or dataset, its type, the languages it covers, the size in true and false clone pairs or the number of code samples, and if any validation has been done.OCD [2], [27] Clone benchmark Java 1,000 9,000 Yes, by construction (100%) Clone oracle [28] Clone benchmark C/C++ 66 -Yes, manually (100%) SeSaMe [29] Clone benchmark Java 857 -Yes, manually (100%) SemanticCloneBench [30] Clone benchmark Java, C, C#, Python 4,000 -Yes, manually (100%) GPTCloneBench [31] Clone benchmark Java, Python, C# 37,149 19,288 Yes, manually (100%) FEMPD [32], [33] Clone benchmark Java 1,342 852 Yes, manually (100%) Dataset Type Language Size Validation</p>
<p>CodeNet [34] Coding competition 55 languages 13.9M code samples Yes, online judge system (100%) POJ/OJClones [20] Coding competition C/C++ 52,000 code samples Yes, online judge system (100%) Google Code Jam [35] Coding competition 20 languages 2.4M code samples Yes, online judge system (100%) CLCDSA [36] Coding competition Java, C#, Python 78K code samples Yes, online judge system (100%) CF-500 [37] Coding competition C 23,146 code samples Yes, online judge system (100%) Code4Bench [38] Coding competition 28 languages 3.4M code samples Yes, online judge system (100%)</p>
<p>A. Clone Detection Benchmarks</p>
<p>There are a few benchmarks in the literature that can be used for evaluating and/or training machine learning-based code clone detection approaches, although they have been designed for the evaluation of traditional clone detection approaches.There have been other benchmarks and datasets for clone detection that are not necessarily helpful for semantic clone detection as they do not include semantic clones.One example is Bellon's benchmark [1], which similarly to BigCloneBench was primarily targeted at recall of traditional clone detection tools.</p>
<p>OCD: The Obfuscation, Compilation and Decompilation (OCD) dataset [2], [27] is an artificial Java code similarity benchmark that has been created using code obfuscators, compilers, and decompilers.The dataset is created by applying a series of transformations to the original code, including renaming variables, changing control flow, and code structure to create semantically similar but syntactically different code snippets.It contains 1,000 true clone pairs and 9,000 false clone pairs, created from 10 different functionalities.</p>
<p>Clone Oracle: Krutz et al. [28] created a clone oracle dataset by having a group of multiple experts and students manually inspect a random sample of 1,536 function pairs from three open-source projects including Apache, Python, and PostgreSQL.Then, they determined if the sampled pairs were clones of each other.The clone oracle dataset consists of 66 clone pairs, 43 of them are Type-2 clones, 14 are Type-3 clones, and 9 are Type-4 clones.The dataset is relatively small and not sufficient for training machine learning models, but it can be used for evaluating the performance of machine learning-based code clone detection tools along with other benchmarks.</p>
<p>SeSaMe: Kamp et al. [29] created the SeSaMe dataset of Java method pairs that are classified based on their semantic similarity.They mined JavaDoc comments from 11 Java open-source projects, then computed the text similarity between the comments to determine the semantic similarity of the methods.Eight judges manually classified a selection of 900 pairs of Java methods as semantically similar or dissimilar in three categories: having similar goals, similar operations, or similar effects.After excluding the pairs with conflicting votes and the pairs that the judges had low confidence, the final dataset contains 857 pairs.Similar to the clone oracle dataset, the SeSaMe dataset is relatively small, so it may not be suitable for training machine learning-based code clone detection tools but can be used for their evaluation.</p>
<p>B. Benchmarks for Semantic Clone Detection</p>
<p>There are two benchmarks that have been created specifically for the detection of semantic clones.SemanticCloneBench: The SemanticCloneBench dataset [30] is created from answers on Stack Overflow.The dataset contains 4,000 manually validated clone pairs in four programming languages: Java, C, C#, and Python (with 1,000 clone pairs each).The construction process of the benchmark includes several filtering steps including the syntax validation using TXL to get method-level code snippets, the functionality validation based on the votes of the Stack Overflow users, the semantic clone filtering using NiCad clone detector, and the manual validation by two judges.Due to its low number of clone pairs, the dataset is more suitable for evaluation than training machine learning models.The benchmark has been used to evaluate the performance of code similarity tools and models in several recent studies [39]- [44].</p>
<p>GPTCloneBench: The GPTCloneBench [31] dataset is created based on SemanticCloneBench and OpenAI's GPT-3 model.It supports four programming languages similar to the SemanticCloneBench including Java, C, C#, and Python.The benchmark contains 37,149 true semantic clone pairs and 19,288 false semantic clone pairs (i.e., Type-1/Type-2 clone pairs), and 20,770 cross-language clone pairs (Java-C#).The semantic clones are created by the GPT-3 model based on the original code snippets in SemanticCloneBench and are filtered using NiCad to remove syntactic clones.Then, the remaining pairs are manually validated by nine judges to ensure the semantic similarity.It has been used in recent studies to evaluate clone clone detection methods [42], [45].</p>
<p>FEMPD:</p>
<p>The FEMPD is a dataset of functionally equivalent method pairs in Java [33].It includes 1,342 functionally equivalent method pairs that pass the test cases of each other and are manually validated by three judges.The dataset is used to evaluate the performance of code clone detection tools [33] and improving clone detection accuracy [46].</p>
<p>CodeXGlue: This dataset is a collection of datasets for machine learning for code understanding and generation tasks [47].It includes datasets for code summarization, code translation, code completion, and clone detection.CodeXGlue includes a modified version of BigCloneBench.Therefore, the issues observed in BigCloneBench are present in the CodeXGlue benchmark suite as well.This raises concerns about the reliability of results obtained for clone detection using the CodeXGlue benchmark suite.Moreover, the downloaded version of CodeXGlue13 consists of 9,126 methods, 1,170,339 false clone pairs, and 561,521 true clone pairs.This is again problematic as the original BigCloneBench has only 258,574 false clone pairs, and it is not clear how the additional false clone pairs have been generated.Moreover, there are only 336,090 unique true clone pairs in the dataset.</p>
<p>We again have checked all false clone pairs which involve sample 22442270 and found that the dataset contains pairs of that sample to methods in all other functionalities of the older BigCloneBench and some more functionalities that only appear in the newer BigCloneBench.This observation indicates again potential mistakes in the construction of this dataset, similar to what we have observed for other usages of BigCloneBench.</p>
<p>As there may be papers that use CodeXGlue for clone detection, but do not refer to BigCloneBench, it is very likely that there are many more papers out there which are threatened in their validity.</p>
<p>C. Datasets from Programming Competitions</p>
<p>There are several datasets that are created from programming competitions and online judge systems that have been used for the training and evaluation of code similarity approaches.</p>
<p>Google Code Jam (GCJ): Petrik et al. [35] created the GCJ dataset by collecting code submissions from the Google Code Jam competition from 2008 to 2020.It contains 332 different programming problems and 2,430,000 code submissions spanning over 20 programming languages.Zhao and Huang [48] also collected a smaller set of the Google Code Jam dataset containing 12 programming problems for their study on deep-learning code clone detection.</p>
<p>CodeNet: The CodeNet dataset [34] is a large dataset comprising code snippets in various programming languages.It is designed for evaluating code understanding and generation tasks, including code clone detection.It consists of code snippets from two online judge systems, AIZU and AtCoder.While AIZU's terms and conditions clearly permit the use of its data for research purposes, AtCoder's terms are more restrictive -users retain copyright over their submissions, granting AtCoder only a limited license for advertising and notification purposes.The authors of CodeNet have scraped AtCoder's website, potentially violating AtCoder's terms and conditions and ignoring the original copyrights.Therefore, CodeNet is not recommended for research purposes, as it may involve unauthorized use [49].</p>
<p>CLCDSA: The CLCDSA dataset [36] is a cross-language code clone dataset covering three programming languages: Java, C#, and Python.It is collected from AtCoders, Google Code Jam, and CoderByte.The dataset contains 78K solutions and has been used in a few previous studies [36], [42].Nonetheless, as mentioned in the discussion above, the usage of AtCoder data causes concerns and the dataset is not recommended.</p>
<p>CF-500: Xue et al. [37] created the CF-500 dataset from the Codeforces online judge system.It contains 23,146 code snippets in C that cover 500 programming problems and use it to evaluate their semantic graph-based Type-4 clone detection approach.</p>
<p>Code4Bench: Majd et al. [38] created the Code4Bench dataset from the Codeforces online judge system, similar to the CF-500 dataset.However, it contains a larger number of 3,421,357 code snippets in 28 programming languages such as C/C++, Java, Python, and Kotlin.It covers 2,990 programming problems across 541 competitions.</p>
<p>POJ: The POJ-104/OJClones dataset [20] is an often-used benchmark constructed from submissions to a POJ pedagogical online judge system.The dataset contains 52,000 code samples in C/C++ that are answers to 104 programming problems.It has been used for code classification [50] and code similarity tasks [51].Many of the papers investigated in the literature review use the POJ dataset in addition to BigCloneBench, enabled by the inclusion of both datasets in the CodeXGlue dataset [47].</p>
<p>D. Repair Attempts</p>
<p>Being aware of the issues with BigCloneBench (and GoogleCodeJam), Li et al. [52] have attempted to repair the dataset.They attempt not only to fix the imbalance by creating a balanced subset, but they also try to remove wrongly labelled pairs by applying a similarity threshold and by removing all methods that have been labelled by a single person.However, given that clone pairs in the WT3/T4 category already have a low similarity based on a threshold, there is no evidence that the created subset is better than the original BigCloneBench dataset regarding wrongly labelled WT3/T4 pairs.</p>
<p>A study by Yu et al. [53] shows that the semantic clones (MT3, WT3/T4) in BigCloneBench usually contain the same identifier names.Their experiment using a Linear-Model to detect semantic clone pairs in BigCloneBench only based on identifier names provides a comparable result to the state-of-the-art ML-based techniques such as ASTNN [54], TBCCD [21], and FA [22].They also report that the performance of the three tools, trained on the original BigCloneBench, on a revised BigCloneBench dataset after abstracting the identifier names drops significantly (F1-score decreases 16%-27%).If the WT3/T4 ground truth could be trusted, their results would show a threat to external validity that the high-performing models based on the BigCloneBench training and evaluation may not perform well in practice where semantic clones do not contain similar identifier names.However, as their results are based on flawed ground truth, it is not clear if their results would be the same for a corrected ground truth.</p>
<p>E. Recommendation</p>
<p>There cannot be a recommendation on which dataset should be used as all datasets have their advantages and disadvantages.Moreover, the datasets are not directly comparable as they have been created for different purposes and with different methods.</p>
<p>None of the above datasets have been scrutinised similar to BigCloneBench and the manual investigations of the datasets have been limited.The only recommendation for the usage of the alternative benchmarks is that any usage of them should be combined with a critical analysis by humans to establish to what extent the assumptions about a dataset holds.</p>
<p>IX. CONCLUSIONS</p>
<p>We have discussed how BigCloneBench's ground truth construction affects the validity of evaluations using BigCloneBench.We have observed that the true clone pair ground truth is flawed as it contains clone pairs that a human judge would not consider to be cloned.Our manual investigation of a statistical significant random sample of 406 WT3/T4 clone pairs showed 93% of constructed clone pairs to be false positives and suggests that the ground truth for WT3/T4 clone pairs cannot be trusted.It is important to clarify that our findings do not invalidate the original purpose of BigCloneBench, which is to evaluate Type-1, Type-2, and Type-3 clone detection tools.The traditional clone types do not rely on functional similarity and the dataset can still be used for its original purpose, the evaluation of syntactic or textual clone detection.Creating a dataset on the scale of BigCloneBench is challenging.The contributions of the dataset for traditional clone detection are significant and have enabled research in traditional clone detection in the past two decades.</p>
<p>The discussed issues are a larger threat to machine learning approaches where the ground truth is used to learn whether a pair of code fragments are clones or not.Because machine learning approaches focus on WT3/T4 clone pairs, the results of machine learning approaches using BigCloneBench are threatened in their validity and cannot be trusted.Our investigation of 179 papers that use BigCloneBench as a dataset, we found 139 papers that used BigCloneBench to evaluate semantic clone detection and where the results are threatened in their validity.As such, these papers often report high F1 scores (e.g., above 0.9), which indicates overfitting to dataset-specific artefacts rather than genuine semantic similarity detection.</p>
<p>Moreover, we observed that some machine learning approaches address a large number of unlabelled code pairs by changing or replacing the ground truth by making wrong assumptions about fragments for different functionalities.By doing so, the approaches create large numbers of code pairs wrongly labelled as not being cloned.</p>
<p>The widespread misuse of BigCloneBench for semantic clone detection beyond the intended purpose of evaluating syntactic or textual clone detection without careful consideration of the dataset's limits has compromised a large number of papers that are threatened in their validity, potentially harming the field of semantic clone detection.We hope to raise awareness of the flaws in the ground truth and how BigCloneBench's ground truth construction affects the validity of evaluations.We encourage a more critical and responsible usage of benchmarks and datasets, and ask for more rigorous validation during dataset creation and usage in the future.</p>
<p>Nonetheless, we call for a stop to using BigCloneBench and datasets derived from it for machine learning because the ground truth quality is too low to produce trustworthy results.</p>
<p>** Instructions: ** -** Use only "Yes" or "No" ** whenever possible.</p>
<p>Other words that can be used are "Old", "New", "Not specified", "Potentially", or "N/A" as applicable.-If additional clarification is needed, provide a ** brief note below the table ** .</p>
<p>-Ensure that all responses are ** consistent with the extracted data from the paper ** .</p>
<p>Fig. 1 .
1
Fig. 1.Copy File Exemplar Function (Snippet 23677115 in file CopyFileSamples.java,lines 38-40).</p>
<p>[Fig. 2 .
2
Fig. 2. Search terms for the "Copy File" functionality.</p>
<p>Fig. 3 .
3
Fig. 3. Method labelled as copy file functionality (Snippet 2571845 in file 1362837.java,lines 51-53).</p>
<p>Fig. 4 .
4
Fig. 4. Instructions for the LLM.</p>
<p>-</p>
<p>The ** old version ** : ˜6.1 million true clone pairs, ˜258,000 false clone pairs, ˜60,000 code snippets, ** 10 functionalities ** .-The ** new version ** : ˜8.9 million true clone pairs, ˜288,000 false clone pairs, ˜74,000 code snippets, ** 43 functionalities ** .Note that BigCloneBench is often shortened to BCB in research papers.You have access to the research paper and need to ** extract relevant details, answer specific questions, and analyze the validity of the study's findings ** .---### ** Tasks: ** 1. ** Extract Key Metadata: ** -<strong> Title, authors, and publication year ** of the uploaded paper.The paper number is PAPER_NUMBER.2. ** Summarize the Paper: ** -Provide a ** concise one-paragraph summary ** covering ** objectives, methodology, key findings, and conclusions ** .3. ** Extract Dataset Usage: ** -Provide a ** concise summary ** of which datasets are used for evaluation.4. ** Analyze BigCloneBench Usage: ** Answer each of the following research questions with a ** justified explanation ** and a ** direct quote ** with a corresponding page number.-</strong> A: ** Is the paper a ** systematic literature review (SLR) or a survey ** ?-<strong> B: ** Does the paper present a ** novel clone detection or clone search approach ** or ** evaluate existing approaches ** ?-</strong> C: ** Does the paper ** use BigCloneBench for evaluation ** ?-<strong> D: ** Is ** BigCloneBench used as ground truth for training a machine learning approach ** ?-</strong> E: ** Which ** version of BigCloneBench (old/new) ** has been used?-<strong> F: ** Has the ** ground truth of BigCloneBench been filtered/modified ** ?If so, what is the ** size of the subset used ** ?-</strong> G: ** Has the ** WT3/T4 subset been excluded ** from evaluation?If filtered, was ** WT3/T4 part of the subset ** ?-<strong> H: ** Ignoring filtering, has the ground truth otherwise ** been changed, extended, or enriched ** ?-</strong> I: ** Has the paper used a ** subset created by previous work ** ?-<strong> J: ** Has the paper ** validated or manually investigated ** BigCloneBench's ground truth?-</strong> K: ** Does the paper ** cite "BigCloneBench Considered Harmful for Machine Learning"?<strong> Each answer should be in the following format: -</strong> Q <X>: ** Question text?-<strong> A: ** Answer (Yes/No).-</strong> Explanation: ** Justified explanation.-<strong> Quote: ** Relevant quote with page number.5. ** Critical Analysis --Impact of New Findings: ** -</strong> Recent research ** found that ** 93.35-<strong> Assess the impact of this finding ** on the validity of the paper's resu -</strong> L: ** Does this finding ** weaken or invalidate ** any claims in the paper?-How does this affect ** methodology, conclusions, or generalizability ** ?B. Second Prompt ### ** Final Summary: Tabular Assessment ** After answering all the questions, summarize the findings in the following CSV table format: Question, A, B, C, D, E, F, G, H, I, J, K, L Paper PAPER_NUMBER, , ,</p>
<p>TABLE I RESULTS
I
OF THE MANUAL VALIDATION OF THE 406 CLONE PAIRS AFTER DISAGREEMENT RESOLUTION.ALSO SHOWN ARE THE DISAGREEMENTS WITH GPT-4O.
Func. PairsTrue PositivesDis.21800%337924.3%74211104.7%3511100%6100%7100%8100%9100%101200%11100%12100%1311100%144250%115100%17100%18100%19100%2011100%21100%22100%23400%24300%25100%2611100%27400%28100%29100%304400%31500%32100%33300%34500%351700%36100%37100%38100%3911100%40100%41600%42600%43100%4411100%145100%Total406276.7%
This definition stems from the ongoing struggle of the community to come up with a good definition and Ira D. Baxter jokingly gave this definition at a workshop.
The first paper on semantic clones[10] defines semantic clones as having isomorphic subgraphs in program dependence graphs[11].
https://github.com/clonebench/BigCloneBench
https://github.com/jeffsvajlenko/BigCloneEval
Technically, the BigCloneBench authors never use the term. However, as we focus on the use of BigCloneBench in machine learning where BigCloneBench is often used as the ground truth, we use the term ground truth for the labelled data in BigCloneBench.
Although consisting of only three lines, the snippet is too large to be shown here.
Most of the code clone detectors usually detect clones in code snippets which are larger or equal to 6 lines.
Snippet 23094550, which is method render in file 402201.java, lines 138-899.
The samples and the results of the manual validation are available at https://github.com/jkrinke/BigCloneBench-Sample-Validation.
https://codingcompetitions.withgoogle.com/codejam
https://github.com/yh1105/datasetforTBCCD
https://github.com/jacobwwh/graphmatch clone
https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Clone-detection-BigCloneBench
ACKNOWLEDGEMENTSWe would like to thank the authors of BigCloneBench for their feedback and discussion on a draft of this paper.We asked the authors of the papers we explicitly mention[21]-[26],[47]to comment on our findings, but we did not receive any feedback.DATA AVAILABILITYWe provide the following data as supplementary material on Zenodo[55]and GitHub[56]:• The subset of 406 clone pairs used for the manual investigation.• The protocol for the manual investigation.• The results of the manual investigation.• The analysis results of the LLM-based evaluation of the dataset.• The code used for the LLM-based analysis of the literature review.• The analysis results of the LLM-based evaluation of the literature review.The protocol for the manual investigation and the prompts used for the literature search are provided in the appendix for the purpose of the paper review.APPENDIX A PROTOCOL FOR THE MANUAL INVESTIGATIONThe following is the protocol for the manual investigation of the snippets.Both authors have followed the protocol during the manual assessment of the 406 clone pairs sampled from the WT3/T4 true clone pairs.A. InstructionsConsider the two snippets (methods) and answer the two questions: 1) Does each of the two snippets implement the functionality as its main or only purpose?2) Are the two snippets (methods) Type-3 or Type-4 clones?B. Question 1The functionality of each method should be checked against the specification as found in the respective table.The two methods should be labelled as the following:1) major The method implements the whole functionality as its main or only purpose.It implements the functionality true to the specification.2) incomplete The method implements only part of the functionality as its main or only purpose.Some part of the functionality is missing and likely implemented in another method that calls the method of interest, or in another method that the method of interest calls.3) minor The method uses the functionality, but has a different main purpose.4) wrong The method does not implement the functionality true to the specification.5) test The method implements the functionality as part of a test.C. Question 2We rely on the original definition of Type-3 and Type-4 clones: A Type-3 clone is• A clone with very similar source code, but with small changes made to the code to tailor it to some new function[8].• A copied fragment with further modifications such as changed, added, or removed statements, in addition to variations in identifiers, literals, types, whitespace, layout and comments[9].A Type-4 clone is• A functionally identical clone, possibly with the originator unaware that there is a function already available that accomplishes essentially the same function[8].• Two or more code fragments that perform the same computation but are implemented by different syntactic variants[9].However, Type-4 clones as defined as above require identical functionality or the same computation.Such a definition is not decidable in general and we only require that the two fragments are functionally similar.As a definition for similar functionality we use the following:Two code snippets are considered functionally similar when they achieve the same result or perform the same task, even if they differ in syntax, structure, or the specific steps they take to accomplish that task.In the definition from Roy et al.[9]we drop the requirement that they are implemented by different syntactic variants (due to the automatic classification, there should be no clones with similar syntactic variants in the sample).There should be no Type-3 pair as all Type-3 pairs should have been filtered out by the automatic filter based on dissimilarity.If there is, the pair needs close attention.Based on the labels of the two methods, usually, the second question is only true for a major/major pair.However, there may be cases where the two methods are labeled differently, but are still clones of each other.Such cases need close attention.APPENDIX B INSTRUCTIONS FOR THE LLM DURING THE LITERATURE SEARCH
Comparison and evaluation of clone detection tools. S Bellon, R Koschke, G Antoniol, J Krinke, E Merlo, IEEE Transactions on Software Engineering. 3392007</p>
<p>A comparison of code similarity analysers. C Ragkhitwetsagul, J Krinke, D Clark, Empirical Software Engineering. 2342018</p>
<p>BigCloneBench. J Svajlenko, C K Roy, Code Clone Analysis. SingaporeSpringer2021</p>
<p>Towards a big data curated benchmark of inter-project code clones. J Svajlenko, J F Islam, I Keivanloo, C K Roy, M M Mia, Int. Conf. on Software Maintenance and Evolution (ICSME). 2014</p>
<p>Evaluating clone detection tools with BigCloneBench. J Svajlenko, C K Roy, Int. Conf. on Software Maintenance and Evolution (ICSME). 2015</p>
<p>Large-scale clone detection and benchmarking. J Svajlenko, 2017University of SaskatchewanPh.D. dissertation</p>
<p>BigCloneBench Considered Harmful for Machine Learning. J Krinke, C Ragkhitwetsagul, 2022 IEEE 16th International Workshop on Software Clones (IWSC). IEEEoct 2022</p>
<p>Clone detection in telecommunications software systems: A neural net approach. S Carter, R Frank, D Tansley, Int. Workshop on Application of Neural Networks to Telecommunications. 1993</p>
<p>Comparison and evaluation of code clone detection techniques and tools: A qualitative approach. C K Roy, J R Cordy, R Koschke, Science of Computer Programming. 7472009</p>
<p>Scalable detection of semantic clones. M Gabel, L Jiang, Z Su, Proceedings of the 13th international conference on Software engineering -ICSE '08. the 13th international conference on Software engineering -ICSE '082008</p>
<p>Identifying similar code with program dependence graphs. J Krinke, Proceedings Eighth Working Conference on Reverse Engineering. Eighth Working Conference on Reverse Engineering2001</p>
<p>BigCloneEval: A clone detection tool evaluation framework with BigCloneBench. J Svajlenko, C K Roy, Int. Conf. on Software Maintenance and Evolution (ICSME). 2016</p>
<p>BigCloneBench: A retrospective and roadmap. 2022 IEEE 16th International Workshop on Software Clones (IWSC). oct 2022</p>
<p>On precision of code clone detection tools. F Farmahinifarahani, V Saini, D Yang, H Sajnani, C V Lopes, Int. Conf. on Software Analysis, Evolution and Reengineering (SANER). 2019</p>
<p>A systematic literature review on reasons and approaches for accurate effort estimations in agile. J Pasuksmit, P Thongtanunam, S Karunasekera, ACM Computing Surveys. 562024</p>
<p>Siamese: scalable and incremental code clone search via multiple code representations. C Ragkhitwetsagul, J Krinke, Empirical Software Engineering. 2442019</p>
<p>Facoy: a code-to-code search engine. K Kim, D Kim, T F Bissyandé, E Choi, L Li, J Klein, Y L Traon, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software Engineering2018</p>
<p>Fast code clone detection based on weighted recursive autoencoders. J Zeng, K Ben, X Li, X Zhang, IEEE Access. 72019</p>
<p>CCEyes: An effective tool for code clone detection on large-scale open source repositories. Y Zhang, T Wang, Int. Conf. on Information Communication and Software Engineering (ICICSE). 2021</p>
<p>Convolutional neural networks over tree structures for programming language processing. L Mou, G Li, L Zhang, T Wang, Z Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceFeb. 201630</p>
<p>Neural detection of semantic code clones via tree-based convolution. H Yu, W Lam, L Chen, G Li, T Xie, Q Wang, Int. Conf. on Program Comprehension (ICPC). 2019</p>
<p>Detecting code clones with graph neural network and flow-augmented abstract syntax tree. W Wang, G Li, B Ma, X Xia, Z Jin, Int. Conf. on Software Analysis, Evolution and Reengineering (SANER). 2020</p>
<p>Code clone detection with hierarchical attentive graph embedding. X Ji, L Liu, J Zhu, International Journal of Software Engineering and Knowledge Engineering. 31062021</p>
<p>Semantic clone detection based on code feature fusion learning. Q Zhang, D Jin, Y Wang, Y Gong, International Journal of Software Engineering and Knowledge Engineering. 33072023</p>
<p>Code clone detection based on code semantic enhancement model. Z Chunyan, S Ziang, L Chen, 2023 International Conference on Intelligent Communication and Computer Engineering (ICICCE). 2023</p>
<p>Graph-based code semantics learning for efficient semantic code clone detection. D Yu, Q Yang, X Chen, J Chen, Y Xu, Information and Software Technology. 1562023</p>
<p>Code Similarity in Clone Detection. J Krinke, C Ragkhitwetsagul, 10.1007/978-981-16-1927-4_10https://link.springer.com/10.1007/978-981-16-1927-42021SpringerSingapore</p>
<p>A code clone oracle. D E Krutz, W Le, Proceedings of the 11th Working Conference on Mining Software Repositories. the 11th Working Conference on Mining Software Repositoriesmay 2014</p>
<p>SeSaMe: A data set of semantically similar Java methods. M Kamp, P Kreutzer, M Philippsen, 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). 2019-May, may 2019</p>
<p>Semanticclonebench: A semantic code clone benchmark using crowd-source knowledge. F Al-Omari, C K Roy, T Chen, 2020 IEEE 14th International Workshop on Software Clones (IWSC). 2020</p>
<p>GPTCloneBench: A comprehensive benchmark of semantic clones and cross-language clones using GPT-3 model and SemanticCloneBench. A I Alam, P R Roy, F Al-Omari, C K Roy, B Roy, K A Schneider, 2023 IEEE International Conference on Software Maintenance and Evolution (ICSME). 2023</p>
<p>Constructing dataset of functionally equivalent java methods using automated test generation techniques. Y Higo, S Matsumoto, S Kusumoto, K Yasuda, Proceedings of the 19th International Conference on Mining Software Repositories. the 19th International Conference on Mining Software Repositories2022</p>
<p>Dataset of functionally equivalent java methods and its application to evaluating clone detection tools. Y Higo, IEICE Transactions on Information and Systems. 1072024</p>
<p>CodeNet: A large-scale AI for code dataset for learning a diversity of coding tasks. R Puri, D Kung, G Janssen, W Zhang, G Domeniconi, V Zolotov, J Dolby, J Chen, M Choudhury, L Decker, Annual Conference on Neural Information Processing Systems. 2021</p>
<p>The effect of time drift in source code authorship attribution: Time drifting in source code -stylochronometry. J Petrik, D Chuda, 10.1145/3472410.3472445Proceedings of the 22nd International Conference on Computer Systems and Technologies, ser. CompSysTech '21. the 22nd International Conference on Computer Systems and Technologies, ser. CompSysTech '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Clcdsa: Cross language code clone detection using syntactical features and api documentation. K W Nafi, T S Kar, B Roy, C K Roy, K A Schneider, 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2019</p>
<p>SEED: Semantic Graph Based Deep Detection for Type-4 Clone. Z Xue, Z Jiang, C Huang, R Xu, X Huang, L Hu, 10.1007/978-3-031-08129-3_89 202213297</p>
<p>Code4bench: A multidimensional benchmark of codeforces data for different program analysis techniques. A Majd, M Vahidi-Asl, A Khalilian, A Baraani-Dastjerdi, B Zamani, Journal of Computer Languages. 532019</p>
<p>Codebert for code clone detection: A replication study. S Arshad, S Abid, S Shamail, 2022 IEEE 16th International Workshop on Software Clones (IWSC). 2022</p>
<p>A comparative analysis of clone detection techniques on semanticclonebench. S M Rabbani, N Ahmad Gulzar, S Arshad, S Abid, S Shamail, 2022 IEEE 16th International Workshop on Software Clones (IWSC). 2022</p>
<p>Interpreting codebert for semantic code clone detection. S Abid, X Cai, L Jiang, 2023 30th Asia-Pacific Software Engineering Conference (APSEC). 2023</p>
<p>On the use of deep learning models for semantic clone detection. S N Pinku, D Mondal, C K Roy, 2024 IEEE International Conference on Software Maintenance and Evolution (ICSME). 2024</p>
<p>Comparing robustness against adversarial attacks in code generation: Llm-generated vs. human-written. M A Awal, M Rochan, C K Roy, 2024</p>
<p>A generative ai-driven method-level semantic clone detection based on the structural and semantical comparison of methods. A Gupta, R Goyal, IEEE Access. 122024</p>
<p>Assessing the code clone detection capability of large language models. Z Zhang, T Saber, 2024 4th International Conference on Code Quality (ICCQ). 2024</p>
<p>Improving accuracy of llm-based code clone detection u sing functionally equivalent methods. R Inoue, Y Higo, 2024 IEEE/ACIS 22nd International Conference on Software Engineering Research, Management and Applications (SERA). 2024</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. S Lu, D Guo, S Ren, J Huang, A Svyatkovskiy, A Blanco, C Clement, D Drain, D Jiang, M D Tang, G Li, L Zhou, L Shou, L Zhou, M Tufano, M Gong, M Zhou, N Duan, N Sundaresan, S K Deng, S Fu, S Liu, 35th Conference on Neural Information Processing Systems. NeurIPS 2021. 2021Track on Datasets and Benchmarks</p>
<p>Deepsim: deep learning code functional similarity. G Zhao, J Huang, 10.1145/3236024.3236068Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringACM201810</p>
<p>Ethics in the mining of software repositories. N E Gold, J Krinke, Empirical Software Engineering. 27117Jan. 2022</p>
<p>Neural code comprehension: A learnable representation of code semantics. T Ben-Nun, A S Jakobovits, T Hoefler, Advances in Neural Information Processing Systems. H Bengio, H Wallach, K Larochelle, N Grauman, R Cesa-Bianchi, Garnett, Curran Associates, Inc201831</p>
<p>Misim: A novel code similarity system. F Ye, S Zhou, A Venkat, R Marcus, N Tatbul, J J Tithi, N Hasabnis, P Petersen, T Mattson, T Kraska, P Dubey, V Sarkar, J Gottschlich, 2020University of PennsylvaniaTech. Rep</p>
<p>Assessing and improving dataset and evaluation methodology in deep learning for code clone detection. H Li, Q Gao, S Zhang, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). 2023</p>
<p>Assessing and improving an evaluation dataset for detecting semantic code clones via deep learning. H Yu, X Hu, G Li, Y Li, Q Wang, T Xie, ACM Transactions on Software Engineering and Methodology. 2022</p>
<p>A novel neural source code representation based on abstract syntax tree. J Zhang, X Wang, H Zhang, H Sun, K Wang, X Liu, Int. Conf. on Software Engineering (ICSE). 2019</p>
<p>How the misuse of a dataset harmed semantic clone detection [data set]. J Krinke, C Ragkhitwetsagul, 10.5281/zenodo.153565042025</p>
<p>BigCloneBench misuse analysis. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
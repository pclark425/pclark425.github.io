<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7090 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7090</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7090</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-577e96243eaffa374a977fa46cf1680877b2b509</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/577e96243eaffa374a977fa46cf1680877b2b509" target="_blank">BARTSmiles: Generative Masked Language Models for Molecular Representations</a></p>
                <p><strong>Paper Venue:</strong> Journal of Chemical Information and Modeling</p>
                <p><strong>Paper TL;DR:</strong> A robust self-supervised strategy tailored toward molecular representations for generative masked language models is discovered through a series of tailored, in-depth ablations, which train BARTSmiles, a BART-like model with an order of magnitude more compute than previous self-supervised molecular representations.</p>
                <p><strong>Paper Abstract:</strong> We discover a robust self-supervised strategy tailored toward molecular representations for generative masked language models through a series of tailored, in-depth ablations. Using this pretraining strategy, we train BARTSmiles, a BART-like model with an order of magnitude more compute than previous self-supervised molecular representations. In-depth evaluations show that BARTSmiles consistently outperforms other self-supervised representations across classification, regression, and generation tasks, setting a new state-of-the-art on eight tasks. We then show that when applied to the molecular domain, the BART objective learns representations that implicitly encode our downstream tasks of interest. For example, by selecting seven neurons from a frozen BARTSmiles, we can obtain a model having performance within two percentage points of the full fine-tuned model on task Clintox. Lastly, we show that standard attribution interpretability methods, when applied to BARTSmiles, highlight certain substructures that chemists use to explain specific properties of molecules. The code and pretrained model are publicly available.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7090.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7090.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTSmiles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTSmiles: Generative Masked Language Models for Molecular Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BART-like encoder-decoder masked (denoising) language model pre-trained on SMILES (ZINC20) to produce transferable molecular representations and to generate SMILES for tasks such as retrosynthesis and reaction prediction; fine-tuned for classification, regression and sequence-to-sequence generative chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BARTSmiles: Generative Masked Language MODELS FOR MOLECULAR REPRESENTATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BARTSmiles (BART-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Encoder-Decoder generative masked language model (denoising objective); pre-trained then fine-tuned for sequence-to-sequence molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BART-Large architecture (exact parameter count not specified); trained at large scale (training cost reported as 20480 A100-hours; large model descriptor)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SMILES strings from ZINC20 deduplicated by canonicalized SMILES via RDKit, ~1.7 billion unique molecules (10k reserved for validation); unigram SentencePiece tokenizer trained on 10M samples (vocab size 1021); max sequence length 128 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuned sequence-to-sequence SMILES generation. Generation strategies: beam search (beam size 10) and ancestral sampling (sample 128 with temperature=1.0) followed by perplexity-based reranking (PPL re-rank). Generative fine-tuning used R3F regularization, Adam initialized with pretraining moving averages, SWA for checkpoint averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES token sequences tokenized by SentencePiece unigram subword tokenizer (vocab=1021); tokens are subword SMILES pieces (not one-to-one with atoms).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Retrosynthesis prediction (USPTO-50k), chemical reaction outcome prediction (USPTO MIT/LEF/STEREO subsets), plus general molecular property prediction (classification/regression) and representation learning for downstream tasks (toxicity, solubility, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No explicit chemical-generation filters such as synthetic-accessibility, toxicity, or patent filters were applied during generation; pretraining/fine-tuning regularization and masking hyperparameters were tuned (optimal pretraining mask_token_budget=0.20, random_mask=0.10, Poisson λ=3.5, and disabling randomize_tokens). Sequence length limited to 128 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for canonicalization and deduplication of pretraining data; FairSeq and FairScale used for training and distributed implementation; Captum (Integrated Gradients) used for interpretability; generation re-ranking uses model-internal perplexity scoring. No external physics/chemistry engines (docking, quantum calculators) or external retrosynthesis planners were integrated for generation or validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pretraining: ZINC20 (~1.7B molecules). Generative fine-tuning/evaluation: USPTO-50k (retrosynthesis) and USPTO subsets (MIT, LEF, STEREO) for reaction prediction. Downstream evaluation datasets: MoleculeNet suite (ESOL, FreeSolv, Lipophilicity, HIV, BACE, BBBP, Tox21, ToxCast, SIDER, ClinTox), Ames dataset, Micronucleus (MN) assay dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrosynthesis/reaction tasks: top-k accuracy (Top-1, Top-5, Top-10). Representation/property tasks: ROC-AUC for classification, RMSE for regression. Additional analyses: Frechet distances between dataset representation distributions, model perplexity used for reranking generated SMILES. Validity/uniqueness/novelty metrics typical for generative chemistry were not reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Retrosynthesis (USPTO-50k): Beam-10: Top-1=55.2%, Top-5=68.5%, Top-10=73.2%; Sample-128 + PPL ReRank: Top-1=55.6%, Top-5=74.2%, Top-10=80.9%. Chemical reaction prediction (USPTO subsets) using BARTSmiles Beam-10: MIT (split) 91.8%, MIT (mixed) 89.1%, LEF (split) 92.1%, LEF (mixed) 92.8%, STEREO (split) 82.5%, STEREO (mixed) 82.1%. Regression downstream (MoleculeNet): ESOL RMSE=0.095, FreeSolv RMSE=0.114, Lipophilicity RMSE=0.292 (state-of-the-art reported). Classification examples: ClinTox, ToxCast improvements noted; many classification ROC-AUCs and comparisons reported across MoleculeNet datasets in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include: (1) data-distribution mismatch — ZINC20 pretraining distribution is not fully representative of many downstream tasks (Frechet distances show some datasets far from ZINC), possibly harming downstream performance; (2) some downstream toxicology tasks were better served by classical fingerprints/descriptors (ECFP4, descriptors) and combining frozen BARTSmiles representations with ECFP improved results, indicating BARTSmiles does not capture all fingerprint information; (3) sensitivity to scaffold splits causing variability in scaffold-split tasks (HIV, BACE, BBBP); (4) tokenization is not atom-aligned (tokens can combine multiple symbols), complicating atom-level interpretability; (5) substantial computational cost to pretrain at scale (reported 20480 A100-hours); (6) no wet-lab synthesis or experimental biological validation of generated compounds reported; (7) no explicit chemical constraints (SA, toxicity, patentability) applied during generation, which could allow chemically invalid or non-synthesizable proposals if not further filtered; (8) a first-token attribution bias observed in Integrated Gradients that had to be normalized for attribution analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTSmiles: Generative Masked Language Models for Molecular Representations', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>"Found in Translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>RetroComposer: Discovering novel reactions by composing templates for retrosynthesis prediction <em>(Rating: 2)</em></li>
                <li>ATx100: State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis <em>(Rating: 2)</em></li>
                <li>GraphRetro: Learning graph models for template-free retrosynthesis <em>(Rating: 2)</em></li>
                <li>ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7090",
    "paper_id": "paper-577e96243eaffa374a977fa46cf1680877b2b509",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "BARTSmiles",
            "name_full": "BARTSmiles: Generative Masked Language Models for Molecular Representations",
            "brief_description": "A BART-like encoder-decoder masked (denoising) language model pre-trained on SMILES (ZINC20) to produce transferable molecular representations and to generate SMILES for tasks such as retrosynthesis and reaction prediction; fine-tuned for classification, regression and sequence-to-sequence generative chemistry tasks.",
            "citation_title": "BARTSmiles: Generative Masked Language MODELS FOR MOLECULAR REPRESENTATIONS",
            "mention_or_use": "use",
            "model_name": "BARTSmiles (BART-like)",
            "model_type": "Encoder-Decoder generative masked language model (denoising objective); pre-trained then fine-tuned for sequence-to-sequence molecular generation",
            "model_size": "BART-Large architecture (exact parameter count not specified); trained at large scale (training cost reported as 20480 A100-hours; large model descriptor)",
            "training_data_description": "SMILES strings from ZINC20 deduplicated by canonicalized SMILES via RDKit, ~1.7 billion unique molecules (10k reserved for validation); unigram SentencePiece tokenizer trained on 10M samples (vocab size 1021); max sequence length 128 tokens.",
            "generation_method": "Fine-tuned sequence-to-sequence SMILES generation. Generation strategies: beam search (beam size 10) and ancestral sampling (sample 128 with temperature=1.0) followed by perplexity-based reranking (PPL re-rank). Generative fine-tuning used R3F regularization, Adam initialized with pretraining moving averages, SWA for checkpoint averaging.",
            "chemical_representation": "SMILES token sequences tokenized by SentencePiece unigram subword tokenizer (vocab=1021); tokens are subword SMILES pieces (not one-to-one with atoms).",
            "target_application": "Retrosynthesis prediction (USPTO-50k), chemical reaction outcome prediction (USPTO MIT/LEF/STEREO subsets), plus general molecular property prediction (classification/regression) and representation learning for downstream tasks (toxicity, solubility, etc.).",
            "constraints_used": "No explicit chemical-generation filters such as synthetic-accessibility, toxicity, or patent filters were applied during generation; pretraining/fine-tuning regularization and masking hyperparameters were tuned (optimal pretraining mask_token_budget=0.20, random_mask=0.10, Poisson λ=3.5, and disabling randomize_tokens). Sequence length limited to 128 tokens.",
            "integration_with_external_tools": "RDKit used for canonicalization and deduplication of pretraining data; FairSeq and FairScale used for training and distributed implementation; Captum (Integrated Gradients) used for interpretability; generation re-ranking uses model-internal perplexity scoring. No external physics/chemistry engines (docking, quantum calculators) or external retrosynthesis planners were integrated for generation or validation.",
            "dataset_used": "Pretraining: ZINC20 (~1.7B molecules). Generative fine-tuning/evaluation: USPTO-50k (retrosynthesis) and USPTO subsets (MIT, LEF, STEREO) for reaction prediction. Downstream evaluation datasets: MoleculeNet suite (ESOL, FreeSolv, Lipophilicity, HIV, BACE, BBBP, Tox21, ToxCast, SIDER, ClinTox), Ames dataset, Micronucleus (MN) assay dataset.",
            "evaluation_metrics": "Retrosynthesis/reaction tasks: top-k accuracy (Top-1, Top-5, Top-10). Representation/property tasks: ROC-AUC for classification, RMSE for regression. Additional analyses: Frechet distances between dataset representation distributions, model perplexity used for reranking generated SMILES. Validity/uniqueness/novelty metrics typical for generative chemistry were not reported explicitly.",
            "reported_results": "Retrosynthesis (USPTO-50k): Beam-10: Top-1=55.2%, Top-5=68.5%, Top-10=73.2%; Sample-128 + PPL ReRank: Top-1=55.6%, Top-5=74.2%, Top-10=80.9%. Chemical reaction prediction (USPTO subsets) using BARTSmiles Beam-10: MIT (split) 91.8%, MIT (mixed) 89.1%, LEF (split) 92.1%, LEF (mixed) 92.8%, STEREO (split) 82.5%, STEREO (mixed) 82.1%. Regression downstream (MoleculeNet): ESOL RMSE=0.095, FreeSolv RMSE=0.114, Lipophilicity RMSE=0.292 (state-of-the-art reported). Classification examples: ClinTox, ToxCast improvements noted; many classification ROC-AUCs and comparisons reported across MoleculeNet datasets in paper.",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include: (1) data-distribution mismatch — ZINC20 pretraining distribution is not fully representative of many downstream tasks (Frechet distances show some datasets far from ZINC), possibly harming downstream performance; (2) some downstream toxicology tasks were better served by classical fingerprints/descriptors (ECFP4, descriptors) and combining frozen BARTSmiles representations with ECFP improved results, indicating BARTSmiles does not capture all fingerprint information; (3) sensitivity to scaffold splits causing variability in scaffold-split tasks (HIV, BACE, BBBP); (4) tokenization is not atom-aligned (tokens can combine multiple symbols), complicating atom-level interpretability; (5) substantial computational cost to pretrain at scale (reported 20480 A100-hours); (6) no wet-lab synthesis or experimental biological validation of generated compounds reported; (7) no explicit chemical constraints (SA, toxicity, patentability) applied during generation, which could allow chemically invalid or non-synthesizable proposals if not further filtered; (8) a first-token attribution bias observed in Integrated Gradients that had to be normalized for attribution analyses.",
            "uuid": "e7090.0",
            "source_info": {
                "paper_title": "BARTSmiles: Generative Masked Language Models for Molecular Representations",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "\"Found in Translation\": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "RetroComposer: Discovering novel reactions by composing templates for retrosynthesis prediction",
            "rating": 2
        },
        {
            "paper_title": "ATx100: State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis",
            "rating": 2
        },
        {
            "paper_title": "GraphRetro: Learning graph models for template-free retrosynthesis",
            "rating": 2
        },
        {
            "paper_title": "ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction",
            "rating": 1
        }
    ],
    "cost": 0.0119755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BARTSmiles: Generative Masked Language MODELS FOR MOLECULAR REPRESENTATIONS</h1>
<p>Gayane Chilingaryan ${ }^{1 <em>}$, Hovhannes Tamoyan ${ }^{1 </em>}$, Ani Tevosyan ${ }^{1 *}$, Nelly Babayan ${ }^{2,3}$,<br>Lusine Khondkaryan ${ }^{2,3}$, Karen Hambardzumyan ${ }^{1}$, Zaven Navoyan ${ }^{3}$, Hrant Khachatrian ${ }^{1,4}$, Armen Aghajanyan ${ }^{5}$<br>${ }^{1}$ YerevaNN, Yerevan, Armenia ${ }^{2}$ Institute of Molecular Biology, NAS RA, Yerevan, Armenia<br>${ }^{3}$ Toxometris.ai, Yerevan, Armenia ${ }^{4}$ Yerevan State University, Yerevan, Armenia<br>${ }^{5}$ Meta AI Research, Seattle, Washington, USA</p>
<h4>Abstract</h4>
<p>We discover a robust self-supervised strategy tailored towards molecular representations for generative masked language models through a series of tailored, in-depth ablations. Using this pre-training strategy, we train BARTSmiles, a BART-like model with an order of magnitude more compute than previous self-supervised molecular representations. In-depth evaluations show that BARTSmiles consistently outperforms other self-supervised representations across classification, regression, and generation tasks setting a new state-of-the-art on 11 tasks. We then quantitatively show that when applied to the molecular domain, the BART objective learns representations that implicitly encode our downstream tasks of interest. For example, by selecting seven neurons from a frozen BARTSmiles, we can obtain a model having performance within two percentage points of the full fine-tuned model on task Clintox. Lastly, we show that standard attribution interpretability methods, when applied to BARTSmiles, highlight certain substructures that chemists use to explain specific properties of molecules. The code and the pretrained model are publicly available.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance of BARTSmiles on a variety of tasks compared to the current state-of-the-art.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 INTRODUCTION</h1>
<p>Recent advancements in large-scale representation learning have significantly improved downstream performance on virtually every modality, from images <em>Aghajanyan et al. (2022); Bao et al. (2021); He et al. (2022)</em> to text <em>Liu et al. (2019); Lewis et al. (2020); Aghajanyan et al. (2021)</em> to speech <em>Conneau et al. (2020); Radford et al. . Meanwhile, the domain of molecular representations has only seen a fraction of the scale of these other domains. In this paper, we show that pre-training with masked language modeling objective on a large collection of molecules allows us to learn generalizable representations that perform well on a wide range of computational chemistry benchmarks.</em></p>
<p>We train BARTSmiles, a general purpose, pre-trained generative masked language model for molecules trained with an order of magnitude more compute than previous pre-trained molecular representations over all available molecules from the ZINC20 dataset (over 1.7 billion molecules <em>(Irwin et al., 2020)</em>).</p>
<p>To validate the efficacy of our representations, we do in-depth evaluations across a large amount of classification, regression, and generation tasks achieving a new state of the art on 11 tasks. We then explore the representations from a domain-expert perspective by contrasting the output of an attribution interpretability method with structural alerts, a rule-based system developed by chemists to explain particular molecular properties. In particular, for a model trained to predict the molecular toxicity, the Integrated Gradients method usually highlights atoms known to be part of the substructures responsible for toxicity <em>(Sundararajan et al., 2017)</em>.</p>
<p>Furthermore, our pre-trained model, in a completely unsupervised fashion, learned representations for specific tasks, i.e., certain neurons correlated highly with downstream tasks. For specific tasks, we see that a linear combination of frozen 7 to 15 neurons is enough to reach more than $90 \%$ of the full fine-tuning performance.</p>
<p>To summarize our contributions are the following:</p>
<ul>
<li>A robust pre-training strategy tailored towards molecular representations discovered through a series of in-depth ablations for generative masked language models.</li>
<li>We present BARTSmiles pre-trained model for molecules trained with an order of magnitude more compute than previous pre-trained molecular representations. The model is publicly released at https://github.com/YerevaNN/BARTSmiles/.</li>
<li>We have fine-tuned BARTSmiles on multiple chemical property prediction, chemical reaction prediction and retrosynthesis tasks, and set state-of-the-art results on 11 of them.</li>
<li>We present a quantitative analysis of molecular properties learned during the pre-training process by analyzing individual neurons in the representations that correlate highly with downstream tasks.</li>
<li>A qualitative analysis using attribution interpretability methods finds that the model internally learns patterns known to in-domain experts to correlate with certain chemical properties.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>Chemical Property Prediction: Machine learning has been applied to chemical property prediction since the 1960s <em>(Dearden, 2016)</em>. Initially, molecules were represented by fingerprints such as Morgan fingerprints <em>(Morgan, 1965)</em>, circular fingerprints <em>(Glen et al., 2006)</em> or extended-connectivity circular fingerprints <em>(ECFP) (Rogers &amp; Hahn, 2010)</em>, which were treated as inputs to various machine learning models. Deep learning has been applied to fingerprints since at least 2014 <em>(Dahl et al., 2014)</em>. <em>Duvenaud et al. (2015)</em> was the first to apply convolutional networks directly on the graphs by generalizing the computation of circular fingerprints. In contrast, <em>Kearnes et al. (2016)</em> introduced graph convolutions on molecular graphs.</p>
<p><em>Wu et al. (2018)</em> introduced the MoleculeNet benchmark suite, which enabled direct comparisons among machine learning algorithms for several classification and regression tasks. It became the standard evaluation system for most of the subsequent work. On the generative modeling front <em>Lowe (2012)</em> extracted a large set of chemical reactions from US patents, becoming a standard benchmark for evaluating the generation abilities of machine learning methods.</p>
<p>Self-Supervised Learning for Molecular Representations: Jaeger et al. (2018) introduced Mol2vec, an adaptation of word2vec for molecules. Within the realm of self-supervised research, Wang et al. (2019) was the first to apply BERT-like models on SMILES strings but unfortunately did not evaluate on MoleculeNet benchmarks. Chithrananda et al. (2020) further scaled up BERT-like models for molecules showing consistent improvement with scale. On the other hand, MolCLR Wang et al. (2022d) and iMolCLR Wang et al. (2022c) use graph neural networks coupled with contrastive learning on 10 million unlabeled molecules to learn fine-tunable representations. Recently, Wang et al. (2022a) improved molecular representations by adding additional inductive biases through constraints on the sums of the representations of reactants.</p>
<p>The work most related to ours is ChemFormer Irwin et al. (2022), which also trains a BART-like model but, unlike our paper, does so with a fraction of the compute and scale. Additionally, through ablations, our paper explores the tweaks needed to the generative masked language model objective to make it optimal in the domain of molecular representations. We also present a unified fine-tuning recipe that removes the need for complex hyper-parameter tuning while consistently outperforming previous state-of-the-art models.</p>
<h1>3 Pretraining</h1>
<p>Below we present the pre-training setting used for training BARTSmiles.
Dataset: We use the Zinc20 dataset from Irwin et al. (2020) for all of our pre-training ablations and experiments. We deduplicate the data based on the hashes of canonicalized SMILES using RDKit (Landrum et al., 2013), leaving us with a total of slightly north of 1.7 billion samples from which we reserve 10000 samples as our validation set.</p>
<p>Implementation: In all of our experiments, we parameterize our masked language model with the standard pre-layer norm Transformer architecture, precisely the BART-Large model proposed in Lewis et al. (2019). We have a maximum sequence length per sample of 128 tokens for all models. We train using the FairSeq framework Ott et al. (2019) with PyTorch Paszke et al. (2019) as the underlying framework. For our larger models, we use the fully sharded data-parallel implementation available in FairScale (Baines et al., 2021). All experiments use automatic mixed-precision available in FairSeq. We use the Aim ecosystem (Arakelyan et al., 2020) for experiment tracking.</p>
<p>Choice Of Objective: When choosing an objective, there are a certain number of pre-conditions to consider depending on the end goal of the model. Wang et al. (2022b) argues that for zeroshot or k-shot prompting, causal language models with uni-directional attention is optimal, while bidirectionality (in both context and attention masks) is the primary driver of success in the fine-tuning setting.</p>
<p>Within the fine-tuning setting, there are different objectives conditioned on model type. For encoderbased models, the masked language modeling objective as initially proposed in Devlin et al. (2018) and further refined in Liu et al. (2019). The downside of encoder models is the inability to do generative fine-tuning, which led to the introduction of the denoising model for the encoder-decoder models Lewis et al. (2019). Decoder causal models are problematic because they are not bidirectional, although recently proposed objectives such as causal masking in Aghajanyan et al. (2022) are bidirectional in context but not in attention.</p>
<p>For BARTSmiles we select both the denoising objective and architecture from Lewis et al. (2019) while previous works have focused on encoder-only (Wang et al., 2019; Chithrananda et al., 2020).</p>
<h3>3.1 Ablation</h3>
<p>Given the novelty of the molecular representation domain for pre-training, we aim to do an in-depth ablation analysis on how to successfully pre-train within this domain.</p>
<p>We measure the quality of learned representations by fine-tuning with a fixed set of hyper-parameters on three datasets, HIV from Wu et al. (2018), BBBP from Martins et al. (2012), and ClinTox from Wu et al. (2018), reporting the average of AUC-ROC scores. Note that we have selected these datasets</p>
<p>before our final fine-tuning experiments. For the exact fixed hyper-parameters please refer to Table 8 in Appendix D. We provide an overarching summary of our ablation in Figure 2.</p>
<p>Choice of Tokenization: Previous work, such as Wang et al. (2019) uses pre-defined tokenization rules on SMILES usually separating individual logical elements such as atoms (i.e., $C$, $N, H$ ) and chemical bond symbols like (i.e., #, -, =). These fundamental units for molecules are analogous to using characters in natural language. Recent advancements in natural language pretraining have ubiquitously used subword tokenization as a compromise between the full character coverage of character level tokenization and the need for large vocabularies for wordbased tokenization. We train a unigram tokenizer on a random sample of 10 Million samples from our training set with a 1021 vocabulary size using SentencePiece (Kudo \&amp; Richardson, 2018).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A summary of decision made when pre-training BARTSmiles, as well as their absolute gain on our ablation benchmark. All numbers are AUC-ROC on the three datasets.</p>
<p>To ablate the benefit of learned tokenization vs. hand-crafted tokenization, we train two equivalent models (§3), using the BARTSmiles-Base architecture. We fix the amount of compute for each run to the amount needed for one epoch, using learned tokenization with BARTSmiles-Base on 100M samples. Since our hand-crafted tokenization produces more tokens, given a fixed compute budget, the model trained with this tokenization sees roughly 92 M samples. Each model was trained on 128 A100 GPUs. As a baseline, we also fine-tune a randomly initialized model of the same architecture and size as BARTSmiles-Base. We present our results in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Custom Tokenization (Random)</th>
<th style="text-align: left;">Custom Tokenization</th>
<th style="text-align: left;">Learned Tokenization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Performance</td>
<td style="text-align: left;">0.628</td>
<td style="text-align: left;">0.779</td>
<td style="text-align: left;">$\mathbf{0 . 8 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance (average AUC-ROC over three datasets) on ablation benchmark dataset over a randomly initialized network, a network pre-trained with hand-crafted tokenization rules, and a network pre-trained with SentencePiece tokenization.</p>
<p>We significantly improve pre-training representations by using a learned tokenization method, and given that learned tokenization is more efficient (due to smaller context lengths) and pre-trains better, we select using a learned tokenizer for the rest of our experiments.</p>
<p>Masking Strategy in Objective: We selected the denoising objective using encoder-decoder models from (Lewis et al., 2019) to learn fine-tunable representations for classification, regression, and generative tasks. Fundamentally the denoising objective has the following hyper-parameters random_mask (percent of tokens flipped to <mask>), $\lambda$ (Poisson distribution for length of masked span), randomize_tokens (whether or not to randomize specific tokens, if true, $50 \%$ of random_mask is used for this). In conjunction with a mask_token_budget (the percent of tokens per sample allowed to be changed or masked), these three parameters cover the surface area of our ablation.</p>
<p>Given the computational cost of individual pre-training runs, even on the scale of BARTSmilesBase, we are unable to do a proper grid search; therefore, we aimed first to find an optimal mask_token_budget (Table 2a), then for that given budget, find optimal random_mask and $\lambda$ (Table 2b), and finally given all the previous parameters we ablate whether or not to include to randomize specific tokens (Table 2c).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">mask_token</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">_budget</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">0.10</td>
<td style="text-align: left;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">0.812</td>
</tr>
<tr>
<td style="text-align: left;">0.20</td>
<td style="text-align: left;">$\mathbf{0 . 8 2 1}$</td>
</tr>
<tr>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.808</td>
</tr>
<tr>
<td style="text-align: left;">0.30</td>
<td style="text-align: left;">0.801</td>
</tr>
<tr>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">0.760</td>
</tr>
<tr>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.701</td>
</tr>
</tbody>
</table>
<p>(a)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">random <br> _mask</th>
<th style="text-align: left;">$\lambda$</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">0.814</td>
</tr>
<tr>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">2.5</td>
<td style="text-align: left;">0.818</td>
</tr>
<tr>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">3.5</td>
<td style="text-align: left;">0.813</td>
</tr>
<tr>
<td style="text-align: left;">0.10</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">0.820</td>
</tr>
<tr>
<td style="text-align: left;">0.10</td>
<td style="text-align: left;">2.5</td>
<td style="text-align: left;">0.821</td>
</tr>
<tr>
<td style="text-align: left;">0.10</td>
<td style="text-align: left;">3.5</td>
<td style="text-align: left;">$\mathbf{0 . 8 2 1}$</td>
</tr>
</tbody>
</table>
<p>(b)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">randomize <br> tokens</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">0.821</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3 5}$</td>
</tr>
</tbody>
</table>
<p>(c)</p>
<p>Table 2: All the ablations ran using the BARTSmiles-Base model on 100 million samples from ZINC20. In (b) we ablate the impact of the random mask percentage and the poisson $\lambda$ parameter which controls the length of the individual masks. For this we fix mask_token_budget to 0.20 , the best forming configuration from (a). In (c) we ablate the impact of randomizing tokens in the input, using the best configurations found in (a) and (b) All numbers are the average of AUC-ROC scores on HIV, BBBP and ClinTox datasets.</p>
<p>In general, we see that the standard hyper-parameter of 0.3 for the mask_token_budget as proposed in Lewis et al. (2019) is suboptimal by a significant margin, and the use of randomizing tokens is suboptimal as well. The type of masking beyond that seems to not be of importance.</p>
<p>Importance of Pre-Training Scale As the last step in our ablation, we significantly increased the amount of compute to do a single pass over the full Zinc20 dataset, which required 20 hours on 1024 A100 GPUs. The previous model utilizing the most compute used 3330 V100 hours Ross et al. (2021), while our model was trained for 20480 A100 hours. Training on the full dataset improves performance by five absolute points compared to the same setting trained on 100 million samples.</p>
<h1>4 Fine-Tuning</h1>
<h3>4.1 DATASETS</h3>
<p>To evaluate our pre-trained models, we fine-tune on multiple classification and regression datasets, mostly from the MoleculeNet benchmark suite (Wu et al., 2018).</p>
<p>MoleculeNet is a collection of 17 commonly used datasets for evaluating chemical representations. MoleculeNet defines splitting methods and evaluation metrics for all datasets. We have run experiments on 10 of them (excluding quantum mechanical datasets and the huge datasets PCBA and MUV). We use the AUC-ROC score to evaluate the classification models. Three of the selected datasets are regression tasks evaluated with RMSE. Four out of seven classification tasks have multiple target labels for each sample. We treat each target variable as a separate task and report the mean of the AUC scores. The remaining three are single-label classification tasks, and scaffolds of the molecules (the two-dimensional structural frameworks of the molecules as defined by Bemis \&amp; Murcko (1996)) define their training-test splits. This implies some distribution shift in the test set, which introduces additional difficulties in model selection.</p>
<p>We additionally perform fine-tuning on two datasets developed for toxicology. The first one is called the Ames dataset Hansen et al. (2009), which holds the results of a bacterial reverse mutation test, commonly known as the Ames test. And the second one holds the results of another genotoxicity test, namely the micronucleus (MN) assay (Fan et al., 2018). MN assay is a cytogenetic test to assess the genotoxicity of chemicals and physical factors on a chromosomal level. We use cross-validation for the MN assay dataset to select the best hyperparameters and apply the best model to an external dataset of 65 compounds to obtain the final score. This makes our results comparable to the baselines introduced by (Hansen et al., 2009).</p>
<table>
<thead>
<tr>
<th></th>
<th>SIDER</th>
<th>ClinTox</th>
<th>Tox21</th>
<th>ToxCast</th>
<th>HIV</th>
<th>BACE</th>
<th>BBBP</th>
</tr>
</thead>
<tbody>
<tr>
<td>RF MolCLR</td>
<td>0.684</td>
<td>0.713</td>
<td>0.769</td>
<td>-</td>
<td>0.781</td>
<td>0.867</td>
<td>0.714</td>
</tr>
<tr>
<td>SVM MolCLR</td>
<td>0.682</td>
<td>0.669</td>
<td>0.818</td>
<td>-</td>
<td>0.792</td>
<td>0.862</td>
<td>0.729</td>
</tr>
<tr>
<td>MoleculeNet GC</td>
<td>0.638</td>
<td>0.807</td>
<td>0.829</td>
<td>0.716</td>
<td>0.763</td>
<td>0.783</td>
<td>0.690</td>
</tr>
<tr>
<td>D-MPNN</td>
<td>0.646</td>
<td>0.894</td>
<td>0.845</td>
<td>0.737</td>
<td>*0.816</td>
<td>*0.875</td>
<td>*0.913</td>
</tr>
<tr>
<td>Attentive FP</td>
<td>0.637</td>
<td>0.940</td>
<td>$\mathbf{0 . 8 5 8}$</td>
<td>0.805</td>
<td>*0.832</td>
<td>*0.850</td>
<td>*0.920</td>
</tr>
<tr>
<td>3D Infomax</td>
<td>0.534</td>
<td>0.594</td>
<td>0.745</td>
<td>0.644</td>
<td>0.761</td>
<td>0.794</td>
<td>0.691</td>
</tr>
<tr>
<td>ChemBERTa 10M</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.622</td>
<td>-</td>
<td>0.643</td>
</tr>
<tr>
<td>MoLFormer-XL</td>
<td>0.690</td>
<td>0.948</td>
<td>0.847</td>
<td>-</td>
<td>*0.822</td>
<td>*0.882</td>
<td>*0.937</td>
</tr>
<tr>
<td>GIN</td>
<td>0.627</td>
<td>0.726</td>
<td>0.781</td>
<td>0.657</td>
<td>$\mathbf{0 . 7 9 9}$</td>
<td>0.845</td>
<td>0.687</td>
</tr>
<tr>
<td>GROVER large</td>
<td>0.658</td>
<td>0.944</td>
<td>0.831</td>
<td>0.737</td>
<td>-</td>
<td>*0.894</td>
<td>*0.940</td>
</tr>
<tr>
<td>MolCLR</td>
<td>0.680</td>
<td>0.932</td>
<td>0.798</td>
<td>-</td>
<td>$\mathbf{0 . 8 0 6}$</td>
<td>$\mathbf{0 . 8 9 0}$</td>
<td>0.736</td>
</tr>
<tr>
<td>iMolCLR</td>
<td>$\mathbf{0 . 6 9 9}$</td>
<td>0.954</td>
<td>0.799</td>
<td>-</td>
<td>$\mathbf{0 . 8 0 8}$</td>
<td>$\mathbf{0 . 8 8 5}$</td>
<td>$\mathbf{0 . 7 6 4}$</td>
</tr>
<tr>
<td>BARTSmiles</td>
<td>$\mathbf{0 . 7 0 5}$</td>
<td>$\mathbf{0 . 9 9 7}$</td>
<td>$\mathbf{0 . 8 5 1}$</td>
<td>$\mathbf{0 . 8 2 5}$</td>
<td>0.745</td>
<td>0.855</td>
<td>0.74</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the classification datasets from MoleculeNet. All numbers are AUC-ROC, higher is better. We bold the state-of-the-art numbers as well as all numbers within a 0.01 range. * indicates a non-standard split.</p>
<h1>4.1.1 RECIPE</h1>
<p>We fine-tune our pre-trained model with Fairseq (Ott et al., 2019), a sequence modeling toolkit based on PyTorch. We use the recommended hyperparameters for the GLUE benchmark with a few modifications (Wang et al., 2018). In particular, we set maximum input and output lengths to 128, changed dropout values, and set the gradient clipping norm to 0.1 . We skip inputs longer than 128 and set the initial fp16 scaling to 128 . We use batch size 16 . We refer to Table 9 in Appendix D for the rest of the hyperparameters.</p>
<p>We train for ten epochs and use linear warmup, with the peak occurring at 16 percent of the training. After the peak. For each dataset, we do a fixed grid search over dropout and learning rate parameters with the values $[0.1,0.2,0.3]$ and $[5 \mathrm{e}-6,1 \mathrm{e}-5,3 \mathrm{e}-5]$, respectively. Finally, we apply Stochastic Weight Averaging (SWA) on three sets of four checkpoints: around the checkpoint having the best validation loss, the best validation accuracy, and the last checkpoint (Izmailov et al., 2018). We select the best one according to the performance on the validation set.</p>
<p>For multi-task problems, we train individual models for each task. We perform the grid search only on the first four tasks, pick the best hyperparameters and apply them to all tasks. We also tried to train a single multi-task classification model for each dataset to cover all tasks simultaneously, but the results were significantly worse (e.g. by 0.25 points on Tox21 and by 0.1 points on SIDER).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ESOL</th>
<th style="text-align: center;">FreeSolv</th>
<th style="text-align: center;">Lipophilicity</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RF MolCLR</td>
<td style="text-align: center;">1.070</td>
<td style="text-align: center;">2.030</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">1.327</td>
</tr>
<tr>
<td style="text-align: left;">SVM MolCLR</td>
<td style="text-align: center;">1.500</td>
<td style="text-align: center;">3.140</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">1.820</td>
</tr>
<tr>
<td style="text-align: left;">MoleculeNet GC</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">1.008</td>
</tr>
<tr>
<td style="text-align: left;">D-MPNN</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">2.180</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">1.270</td>
</tr>
<tr>
<td style="text-align: left;">Attentive FP</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.606</td>
</tr>
<tr>
<td style="text-align: left;">3D Infomax</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">2.337</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">1.309</td>
</tr>
<tr>
<td style="text-align: left;">Chemformer</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">1.230</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.820</td>
</tr>
<tr>
<td style="text-align: left;">MoLFormer-XL</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.346</td>
</tr>
<tr>
<td style="text-align: left;">GROVER large</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">1.544</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.978</td>
</tr>
<tr>
<td style="text-align: left;">MolCLR</td>
<td style="text-align: center;">1.110</td>
<td style="text-align: center;">2.200</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">1.320</td>
</tr>
<tr>
<td style="text-align: left;">iMolCLR</td>
<td style="text-align: center;">1.130</td>
<td style="text-align: center;">2.090</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">1.287</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 9 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 7}$</td>
</tr>
</tbody>
</table>
<p>(a) Regression tasks from MoleculeNet. All numbers are RMSE, lower is better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FP_Pubchem_SVM</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: left;">FP_MACCS_RF</td>
<td style="text-align: center;">0.947</td>
</tr>
<tr>
<td style="text-align: left;">Descriptor_SVM</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Descriptor_RF</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles</td>
<td style="text-align: center;">0.914</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles (Frozen) + ECFP + SVM</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 8}$</td>
</tr>
<tr>
<td style="text-align: left;">(b) Micronucleus assay dataset</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">AUC</td>
</tr>
<tr>
<td style="text-align: left;">SVM</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">GP</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">RF</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">k-NN</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles</td>
<td style="text-align: center;">0.830</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles (Frozen) + ECFP + SVM</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 9}$</td>
</tr>
</tbody>
</table>
<p>(c) Ames dataset (cross-validation)</p>
<p>Table 4: Results on MoleculeNet regression tasks and two toxicology datasets.</p>
<h3>4.2 BASELINES</h3>
<p>MoleculeNet paper introduced its own set of baselines using multiple machine learning methods. MoleculeNet did not apply most baselines to all datasets. Among the ones applied to all datasets, the</p>
<p>convolutional graph baseline performed the best, and we have included it as our baseline. The other graph-based supervised baselines include D-MPNN (Yang et al., 2019), Attentive FP (Xiong et al., 2019) and the 3D InfoMax method (Stärk et al., 2022).</p>
<p>We also compare with baselines that use self-supervised pretraining. We skip SMILES-BERT, as SMILES-BERT did not test it on MoleculeNet. ChemBERTa (Chithrananda et al., 2020), Chemformer (Irwin et al., 2022) and MolFormer-XL (Ross et al., 2021) are based on regular text-based transformers. Graph Isomorphism Networks (GIN) with Context Prediction from Hu et al. (2020), GROVER-Large from Rong et al. (2020), MolCLR (Wang et al., 2022d) and iMolCLR (Wang et al., 2022c) are self-supervised methods based on graph neural networks. For Micronucleus Assay and Ames datasets we compare with the baselines introduced in the original papers.</p>
<h1>4.2.1 RESULTS</h1>
<p>Our model produced state-of-the-art results on all three regression tasks of the MoleculeNet suite, as seen in Table 4a. On classifications tasks the results are mixed (Table 3). On Clintox and Toxcast datasets, our model beats all previous models. Toxcast is a huge dataset with 617 tasks, so many papers do not include results. On SIDER and Tox21, the performance of our models are comparable to the state-of-the-art.</p>
<p>The other three classification tasks (HIV, BACE, and BBBP) require specific scaffold splits and are sensitive to the choice of the split. Unfortunately, many papers do not specify whether they used the split recommended by MoleculeNet. Jiang et al. (2021) claim D-MPNN and Attentive FP results are based on random splits. MolFormer and GROVER-large use scaffold splits, but the splits are different. We explored the split's impact on the BBBP dataset's results.</p>
<p>Molecule scaffolds induce 1024 clusters of 1940 molecules in the BBBP dataset. The split is organized so that all non-singleton clusters belong to the training set, while the validation and test sets contain only molecules with unique scaffolds. Most of the errors of our model are concentrated on roughly $30 \%$ of the singleton clusters in the test set. This implies that even a scaffold split with a different random seed can significantly affect the results. In Table 3, we have marked the results of these three classification datasets with an asterisk if we believe the authors used a different split.</p>
<p>The results for the two toxicology datasets that direct fine-tuning did not beat the baselines based on fingerprints and descriptors. To test whether the fingerprints still contain helpful information not covered by BARTSmiles, we fine-tuned a classifier head that takes 2048-dimensional ECFP4 fingerprints (Rogers \&amp; Hahn, 2010) as an input in addition to the [CLS] vector. We also froze BARTSmiles, averaged the last layers' representations per molecule, concatenated them with ECFP fingerprints, and ran the default SVM implementation from Kramer (2016). This helped close the gap with the baselines, as seen in Table 4c. Fundamentally this implies that fingerprints carry some information that is not trivially recoverable from our pre-trained representation. We do a deeper analysis of our pre-training dataset in $\S 5.2$ and hypothesize that the Zinc20 dataset is not representative of the downstream tasks computational chemistry is interested in.</p>
<h3>4.3 GENERATIVE TASKS</h3>
<p>We follow a straightforward recipe for generative fine-tuning for all generative tasks, mainly derived from Aghajanyan et al. (2020a). Similarly to our results on SIDER, we found that training in full 32-bit precision (fp32) significantly impacted downstream results. We use the R3F method from Aghajanyan et al. (2020a), running a sweep over the noise types and $\lambda$ regularization term for each task (See Table 10 in Appendix D). Consistent with our fine-tuning recipe for classification and regression tasks, we utilize SWA and do not do any augmentation on the data. For additional stability, we initialize our fine-tuning optimizer (Adam) with the pre-training moving averages of the gradient and squared gradient from our pre-training. We also use a polynomial decay strategy where the peak learning rate is at $0.06 *$ max_updates where the max number of updates is the number of updates needed to do ten epochs for the respective dataset.</p>
<p>During the evaluation, we implement two different strategies for sequence generation. The first runs a straightforward beam search with a beam size of ten. The second approach samples 128 samples from the natural distribution of tokens from the model (temperature of 1.0) and then re-ranks the list of 128 samples based on the perplexity of the entire predicted SMILES.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Top-1</th>
<th style="text-align: center;">Top-5</th>
<th style="text-align: center;">Top-10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RetroComposer (Yan et al., 2021)</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: left;">ATx100 (Tetko et al., 2020)</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">$\mathbf{8 1 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">GraphRetro (Somnath et al., 2020)</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: left;">ChemFormer (Irwin et al., 2022)</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: left;">Dual-TF (Sun et al., 2020)</td>
<td style="text-align: center;">$\mathbf{5 5 . 3}$</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles Beam-10</td>
<td style="text-align: center;">$\mathbf{5 5 . 2}$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles Sample-128 + PPL ReRank</td>
<td style="text-align: center;">$\mathbf{5 5 . 6}$</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">80.9</td>
</tr>
</tbody>
</table>
<p>Table 5: We present our experimental results on the USPTO-50k retrosynthesis task across varying sampling strategies. We bold the best performing numbers within a 0.5 range.</p>
<p>We select two common generative molecular tasks; retrosynthesis and chemical reaction prediction.</p>
<h1>4.3.1 RETROSYNTHESIS</h1>
<p>Retrosynthesis is a chemical synthesis technique involving the deconstruction of a target molecule into its starting materials to assess the best synthetic route. Retrosynthesis is a cornerstone of modern organic product synthesis, traditionally the domain of human experts' knowledge. Due to the algorithmic advances and the availability of large chemical reaction collections, computational approaches have been suggested to solve the problems in retrosynthesis (Davey, 2018; Watson et al., 2019). We apply our generative fine-tuning strategy on the USPTO dataset (Lowe, 2012). Table 5 shows the ratio of molecules for which the correct reactants were among the top K predictions generated by the respective models. Our approach has the best result for the Top-1 metric.</p>
<h3>4.3.2 Chemical Reaction Prediction</h3>
<p>The dual problem of retrosynthesis is chemical reaction prediction which is the automatic construction of the target molecule given the set of reactant molecules. This can also be treated as a sequence-to-sequence task, as first shown by Schwaller et al. (2018), and can be performed by fine-tuning BARTSmiles on a relevant dataset. We follow the setup presented by Schwaller et al. (2019) and apply our generative fine-tuning recipe to several subsets of the USPTO dataset. As seen in Table 6, BARTSmiles outperforms all baselines across all subsets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">USPTO Split</th>
<th style="text-align: left;">MIT (S)</th>
<th style="text-align: left;">MIT (M)</th>
<th style="text-align: left;">LEF (S)</th>
<th style="text-align: left;">LEF (M)</th>
<th style="text-align: left;">STEREO (S)</th>
<th style="text-align: left;">STEREO (M)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">S2S (Schwaller et al., 2018)</td>
<td style="text-align: left;">80.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">65.4</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WLDN (Jin et al., 2017)</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;">74</td>
<td style="text-align: left;">84.0</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ELECTRO (Bradshaw et al., 2018)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">87.0</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GTPN (Do et al., 2019)</td>
<td style="text-align: left;">82.4</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">87.4</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WLDN5 (Coley et al., 2019)</td>
<td style="text-align: left;">85.6</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">88.3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Transformer (Schwaller et al., 2019)</td>
<td style="text-align: left;">90.4</td>
<td style="text-align: left;">88.6</td>
<td style="text-align: left;">$\mathbf{9 2 . 0}$</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">76.2</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles Beam-10</td>
<td style="text-align: left;">$\mathbf{9 1 . 8}$</td>
<td style="text-align: left;">$\mathbf{8 9 . 1}$</td>
<td style="text-align: left;">$\mathbf{9 2 . 1}$</td>
<td style="text-align: left;">$\mathbf{9 2 . 8}$</td>
<td style="text-align: left;">$\mathbf{8 2 . 5}$</td>
<td style="text-align: left;">$\mathbf{8 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">BARTSmiles Sample + ReRank</td>
<td style="text-align: left;">91.1</td>
<td style="text-align: left;">$\mathbf{8 9 . 1}$</td>
<td style="text-align: left;">91.8</td>
<td style="text-align: left;">90.2</td>
<td style="text-align: left;">80.4</td>
<td style="text-align: left;">81.5</td>
</tr>
</tbody>
</table>
<p>Table 6: We present our experimental results on the USPTO chemical prediction task across varying sampling strategies and three data subsets (MIT, LEF and STEREO). (S) signifies the "split" and (M) signifies the "mixed" sampling strategy from Schwaller et al. (2019)). We bold the best performing numbers within a 0.5 range. The baselines scores are taken from Schwaller et al. (2019).</p>
<h2>5 INTERPRETABILITY</h2>
<h3>5.1 UNCOVERING THE KEY FEATURES</h3>
<p>Radford et al. (2017) showed that a large language model trained to generate text had a particular neuron correlated with the sentence's sentiment, colloquially dubbed the sentiment neuron. Further work showed that self-supervised representations had a very low intrinsic dimension for downstream tasks, arguing that self-supervised representations were capable of learning complex tasks implicitly</p>
<p>(Aghajanyan et al., 2020b). For each dataset, we extracted the representations of each molecule by taking the average of the last layer representations of all tokens given by the pre-trained model and trained an L1-regularized logistic regression model to predict the label. We indirectly control the number of selected features by changing the regularization strength. The results are shown on Fig. 3.</p>
<p>Clintox tasks are among the easiest ones. As seen in Figure 3, a single unsupervised neuron can predict the label of its first subtask with a 0.77 ROC-AUC score, while with seven neurons, one can achieve 0.987 score. It is interesting to note that for the regularization parameter $C \in\left{2^{-6}, 2^{-7}, 2^{-8}\right}$, both subtasks of Clintox select the same set of features.</p>
<p>This might indicate that there are features learned in a completely unsupervised fashion that are highly correlated with the labels of these datasets, at least on the corresponding validation sets of the datasets.</p>
<h1>5.2 Datasets in the Representation Space</h1>
<p>To explore the relative positions of various datasets in the BARTSmiles space, we compute the 1024-dimensional representations of all molecules by taking the average of token representations, and compute Frechet distance (Preuer et al., 2018) between the datasets. As the ZINC dataset is vast, we uniformly sample $0.05 \%$ of the molecules. As seen in Fig. 4, several datasets like BBBP and SIDER are further from ZINC, which might explain a relatively poor performance of BARTSmiles compared to the models like iMolCLR, which do not use ZINC for pretraining.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The AUC-ROC score plotted against the number of self-supervised features selected from BARTSmiles. Dashed lines indicate the performance achieved on the same dataset with full fine-tuning of the model.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Frechet distances between datasets in the representation space of BARTSmiles.</p>
<h3>5.3 Interpreting Fine-Tuned Models</h3>
<p>Methodology: In this section, we explore the contribution of each token of the given SMILES to the predictions of the fine-tuned model. We use the Integrated Gradients method (Sundararajan et al., 2017) implemented in Captum library (Kokhlikyan et al., 2020) to get the contributions of the tokens. Note that due to the tokenization we used in pretraining, there is no one-to-one mapping between tokens and the atoms. Nevertheless, we attempt to visualize the contributions of each atom on the molecular graph. If a token contains multiple symbols, we assume the contributions are additive and split the contribution of the token equally for all symbols in the token. Thus, we get the contribution of each atom and highlight the atom in the graph accordingly. Note that the SMILES string contains many non-atom symbols like brackets. We do not visualize the contributions of the brackets. We also visualize the contribution of a double-bond symbol $=$ by highlighting the bond in the graph.
Ames Dataset: For the Ames dataset, we fine-tuned another model on a random split, and explored the contributions of the atoms of the molecules from the validation set. We compared the highlighted atoms in the graph with the known molecular substructures related to the mutagenic properties of</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The contributions of each token and each atom to the prediction of our model fine-tuned on the Ames dataset, according to the Integrated Gradient algorithm.
molecules, so-called structural alerts (Plošnik et al., 2016). Figure 5 shows a couple of examples. On each of the plots, the left image highlights the structural pattern while the right one shows the contributions of each atom according to the Integrated Gradients method. Many more examples and a more thorough discussion can be found in Appendix B.</p>
<p>The compound #21 in Fig. 5 is a nitroaromatic compound representing organic molecules that consist of at least one nitro group (-NO2) attached to an aromatic ring. The structural alert in this molecule is the nitro group (highlighted on the left part of the figure), whose components have been highlighted as a contribution in most cases of this class of compounds. The compound #38 on the same Figure is an epoxy group-bearing molecule. The epoxy group consists of an oxygen atom joined by single bonds to two adjacent carbon atoms, which form the three-membered epoxide ring (Plošnik et al., 2016) and is a well-known structural alert. It is worth noting that in the case of nitroaromatic compounds, the components of the structural alerts have been detected, along with the high probability ( $&gt;0.8$ ) of correct prediction.</p>
<p>To summarize, the Integrated Gradients method, in most cases, recognizes contributions of noncomplex substructures. In the case of more complex substructures, it provides explanations currently not understood through the lens of structural alerts. Additionally, the recognition of known substructures having the contribution to specific labels does not interfere with correct opposite label prediction, evidencing that the model's learning ability is much deeper than estimating simple correlations between molecular substructures and their effects.</p>
<p>ESOL dataset: Next, we analyze the attribution scores Integrated Gradients gave for one of the regression tasks, ESOL, which aims to measure the solubility of the molecules in water. We first noted a strange pattern: the first token of each molecule tended to receive a positive score, regardless of the molecule's label or structure. We confirmed this by measuring the mean attribution score across the tokens with respect to their positions (Fig. 6, top-left). We decided to "normalize" the attribution scores by subtracting those mean attributions from the scores given by the Integrated Gradients method.</p>
<p>Since water is a polar solvent, polar groups, such as hydroxyl group (-OH) and carbonyl group ( $\mathrm{C}=\mathrm{O}$ ), will contribute to the solubility of the molecule (Lemke, 2003). The Integrated Gradients method highlighted almost all instances of hydroxyl groups, as seen in the bottom row of Fig. 6, and most instances of carbonyl groups, especially if the molecule is more soluble (top-right image of Fig. 6). It is known that long chains of hydrocarbon compounds are known to decrease the solubility of the molecules. This is captured by the negative attributions of "C" tokens in such molecules, as seen in the bottom row of Fig. 6. More examples and analysis is presented in Appendix C.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Top-left: the average attribution score of a token depending on its position in the tokenized SMILES. There is a significant positive bias on the first token. Others: the contributions of each token and each atom to the prediction of our model fine-tuned on the ESOL dataset, according to the Integrated Gradient algorithm.</p>
<p>We hope the interpretability methods developed for deep learning, our visualization toolkit, and the strong predictive models made possible by the large pre-trained models will help chemists uncover new correlations between substructures and target variables or even causal links.</p>
<h1>6 CONCLUSION</h1>
<p>In this paper, we introduce BARTSmiles, a large pre-trained model for molecular representations which sets state-of-the-art results across many classification, regression, and generation tasks. We showed the power of self-supervised training through a series of quantitative and qualitative analyses of representations showing that interpretability methods applied to these representations mirror that of chemists. There is still room for investigation of the impact of data on pre-training, i.e., ZINC contains more than a billion molecules, but its distribution might be irrelevant for many downstream tasks. Further collaboration between machine learning and chemistry experts might allow for the discovery of new molecular substructures linked to chemical phenomena.</p>
<h2>7 ACKNOWLEDGEMENTS</h2>
<p>We would like to thank Luke Zettlemoyer, Lucas Allan, Garik Petrosyan and Garegin Papoian for useful feedback. This work was partially supported by the RA MES State Committee of Science, in the framework of the research project No. 20TTCG-1F004.</p>
<h2>REFERENCES</h2>
<p>Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. Better fine-tuning by reducing representational collapse. arXiv preprint arXiv:2008.03156, 2020a.</p>
<p>Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020b.</p>
<p>Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettlemoyer. Htlm: Hyper-text pre-training and prompting of language models. arXiv preprint arXiv:2107.06955, 2021.</p>
<p>Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.</p>
<p>Gor Arakelyan, Gevorg Soghomonyan, and The Aim team. Aim. 6 2020. doi: 10.5281/zenodo. 6536395. URL https://github.com/aimhubio/aim.</p>
<p>Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, Anjali Sridhar, and Min Xu. Fairscale: A general purpose modular pytorch library for high performance and large scale training. https://github.com/facebookresearch/fairscale, 2021.</p>
<p>Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.</p>
<p>Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal chemistry, 39(15):2887-2893, 1996.</p>
<p>Samuel Boobier, David RJ Hose, A John Blacker, and Bao N Nguyen. Machine learning with physicochemical relationships: solubility prediction in organic solvents and water. Nature communications, $11(1): 1-10,2020$.</p>
<p>John Bradshaw, Matt J Kusner, Brooks Paige, Marwin HS Segler, and José Miguel Hernández-Lobato. A generative model for electron paths. arXiv preprint arXiv:1805.10970, 2018.</p>
<p>Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: Large-scale selfsupervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.</p>
<p>Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola, William H Green, Regina Barzilay, and Klavs F Jensen. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science, 10(2):370-377, 2019.</p>
<p>Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. Unsupervised cross-lingual representation learning for speech recognition. arXiv preprint arXiv:2006.13979, 2020.</p>
<p>George E Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for qsar predictions. arXiv preprint arXiv:1406.1231, 2014.</p>
<p>Stephen G Davey. Retrosynthesis: Computer says yes. Nature Reviews Chemistry, 2(1):1-1, 2018.
John Dearden. The history and development of quantitative structure-activity relationships (qsars). IJQSPR, 1(1):1-44, 2016. doi: 10.4018/978-1-5225-0549-5.ch003.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical reaction prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pp. 750-760, 2019.</p>
<p>David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.</p>
<p>Defang Fan, Hongbin Yang, Fuxing Li, Lixia Sun, Peiwen Di, Weihua Li, Yun Tang, and Guixia Liu. In silico prediction of chemical genotoxicity using machine learning methods and structural alerts. Toxicology research, 7(2):211-220, 2018.</p>
<p>Robert C Glen, Andreas Bender, Catrin H Arnby, Lars Carlsson, Scott Boyer, and James Smith. Circular fingerprints: flexible molecular descriptors with applications from physical chemistry to adme. IDrugs, 9(3):199, 2006.</p>
<p>Katja Hansen, Sebastian Mika, Timon Schroeter, Andreas Sutter, Antonius ter Laak, Thomas StegerHartmann, Nikolaus Heinrich, and Klaus-Robert Müller. Benchmark data set for in silico prediction of ames mutagenicity. Journal of Chemical Information and Modeling, 49(9):2077-2081, 2009. doi: 10.1021/ci900161g. URL http://dx.doi.org/10.1021/ci900161g.</p>
<p>Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009, 2022.</p>
<p>Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJlWWJSFDH.</p>
<p>John J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R Wong, Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle. Zinc20—a free ultralarge-scale chemical database for ligand discovery. Journal of chemical information and modeling, 60(12):6065-6073, 2020.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1): 015022, 2022.</p>
<p>Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.</p>
<p>Sabrina Jaeger, Simone Fulle, and Samo Turk. Mol2vec: unsupervised machine learning approach with chemical intuition. Journal of chemical information and modeling, 58(1):27-35, 2018.</p>
<p>Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models. Journal of cheminformatics, 13(1):1-23, 2021.</p>
<p>Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network. Advances in neural information processing systems, 30, 2017.</p>
<p>Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8): 595-608, 2016.</p>
<p>Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion ReblitzRichardson. Captum: A unified and generic model interpretability library for pytorch, 2020.</p>
<p>Oliver Kramer. Scikit-learn. In Machine learning for evolution strategies, pp. 45-53. Springer, 2016.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.</p>
<p>Greg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling, 2013.</p>
<p>Thomas L Lemke. Review of organic functional groups: introduction to medicinal organic chemistry. Lippincott Williams \&amp; Wilkins, 2003.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019 .</p>
<p>Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing, 2020.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Daniel Mark Lowe. Extraction of chemical structures and reactions from the literature. PhD thesis, University of Cambridge, 2012.</p>
<p>Ines Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao. A bayesian approach to in silico blood-brain barrier penetration modeling. Journal of chemical information and modeling, 52 (6):1686-1697, 2012.</p>
<p>Harry L Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of chemical documentation, 5(2):107-113, 1965.</p>
<p>Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32: 8026-8037, 2019.</p>
<p>Alja Plošnik, Marjan Vračko, and Marija Sollner Dolenc. Mutagenic and carcinogenic structural alerts and their mechanisms of action. Arhiv za higijenu rada i toksikologiju, 67(3):169-182, 2016.</p>
<p>Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Journal of chemical information and modeling, 58(9):1736-1741, 2018.</p>
<p>Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.</p>
<p>Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.</p>
<p>David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754, 2010.</p>
<p>Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33:12559-12571, 2020.</p>
<p>Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das. Do large scale molecular language representations capture important structural information? arXiv preprint arXiv:2106.09553, 2021.</p>
<p>Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. "found in translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. Chemical science, 9(28):6091-6098, 2018.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.</p>
<p>Vignesh Ram Somnath, Charlotte Bunne, Connor W Coley, Andreas Krause, and Regina Barzilay. Learning graph models for template-free retrosynthesis. arXiv preprint arXiv:2006.07038, 2020.</p>
<p>Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pp. 20479-20502. PMLR, 2022.</p>
<p>Ruoxi Sun, Hanjun Dai, Li Li, Steven Kearnes, and Bo Dai. Energy-based view of retrosynthesis. arXiv preprint arXiv:2007.13437, 2020.</p>
<p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International conference on machine learning, pp. 3319-3328. PMLR, 2017.</p>
<p>Igor V Tetko, Pavel Karpov, Ruud Van Deursen, and Guillaume Godin. State-of-the-art augmented nlp transformer models for direct and single-step retrosynthesis. Nature communications, 11(1): $1-11,2020$.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.</p>
<p>Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and Martin Burke. Chemical-reaction-aware molecule representation learning. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=6sh3pIzKS-.</p>
<p>Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pp. $429-436,2019$.</p>
<p>Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization? arXiv preprint arXiv:2204.05832, 2022b.</p>
<p>Yuyang Wang, Rishikesh Magar, Chen Liang, and Amir Barati Farimani. Improving molecular contrastive learning via faulty negative mitigation and decomposed fragment contrast. Journal of Chemical Information and Modeling, 59(8):3370-3388, 2022c. doi: 10.1021/acs.jcim.2c00495.</p>
<p>Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4(3):279-287, 2022d.</p>
<p>Ian A Watson, Jibo Wang, and Christos A Nicolaou. A retrosynthetic analysis algorithm implementation. Journal of cheminformatics, 11(1):1-12, 2019.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.</p>
<p>Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16):8749-8760, 2019.</p>
<p>Chaochao Yan, Peilin Zhao, Chan Lu, Yang Yu, and Junzhou Huang. Retrocomposer: Discovering novel reactions by composing templates for retrosynthesis prediction. arXiv preprint arXiv:2112.11225, 2021.</p>
<p>Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel GuzmanPerez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8): $3370-3388,2019$.</p>
<h1>A DATASET DESCRIPTIONS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Task type</th>
<th style="text-align: left;"># of tasks</th>
<th style="text-align: left;"># of compounds</th>
<th style="text-align: left;">Split type</th>
<th style="text-align: left;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ESOL</td>
<td style="text-align: left;">Regression</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1128</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">RMSE</td>
</tr>
<tr>
<td style="text-align: left;">FreeSolv</td>
<td style="text-align: left;">Regression</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">642</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">RMSE</td>
</tr>
<tr>
<td style="text-align: left;">Lipophilicity</td>
<td style="text-align: left;">Regression</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4200</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">RMSE</td>
</tr>
<tr>
<td style="text-align: left;">HIV</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">41127</td>
<td style="text-align: left;">Scaffold</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">BACE</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1513</td>
<td style="text-align: left;">Scaffold</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">BBBP</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2039</td>
<td style="text-align: left;">Scaffold</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">Tox21</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">7831</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">ToxCast</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">617</td>
<td style="text-align: left;">8575</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">SIDER</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">1427</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">ClinTox</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1478</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">Ames</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">6512</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
<tr>
<td style="text-align: left;">Micronucleus Assay</td>
<td style="text-align: left;">Classification</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">641</td>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">ROC-AUC</td>
</tr>
</tbody>
</table>
<p>Table 7: Dataset statistics for our fine-tuning evaluations.</p>
<h2>B Ames Interpretability: Detailed Analysis</h2>
<p>In Figures 7 to 11 we show the structural alerts of more than a hundred molecules from the test set of Ames dataset. Chemicals under the numbers $10,20,21,37,40,61,71,90,93,110,117$, and 127 are nitroaromatic compounds representing organic molecules that consist of at least one nitro group (-NO2) attached to an aromatic ring. These compounds and their residuals are well known for their carcinogenic and mutagenic potential. The nitro group in these molecules is characterized as structural alerts, i.e., molecular substructures related to the chemical's carcinogenic and mutagenic properties.
In most cases, Integrated Gradients recognized two components of the structural alert: the positively charged nitrogen atom and a double bonded oxygen atom. In some cases, only a positive charge was highlighted along with the oxygen atom. It is essential to mention that a positively charged nitrogen atom plays a crucial role in the toxic potential of this structural group since this positive charge determines the electrophilic nature of the molecule. An electrophile is a chemical species that form bonds with nucleophiles by accepting an electron pair. Whereas DNA is abundantly equipped with nucleophilic sites, reaction with electrophiles results in diverse chemical-DNA interactions.
However, in all cases, the most highlighted atom was a double-bonded oxygen atom. Two explanations may arise from this observation. First is that Integrated Gradients "detects" it as a nitroso group $(\mathrm{N}=\mathrm{O})$, e.g., as in the compounds under the numbers 42,59 , and 69 , where they represent a separate structural alert group. Second, the activation of most nitroaromatic compounds' mutagenic properties is linked to the nitroreduction. As a result of nitroreduction, nitroso and superoxide species are produced.In both cases, double-bonded oxygen atoms play a vital role in forming toxic intermediates. So, in this case, it can be assumed that the algorithm recognizes target sites, such as positively charged nitrogen or oxygen atoms, responsible for producing DNA-damaging compounds.
The same pattern is observed in the case of compounds under the numbers $18,38,57,60,124$, and 126. These compounds are characterized by the bearing of at least one epoxy group consisting of an oxygen atom joined by single bonds to two adjacent carbon atoms, which form the three-membered epoxide ring.The epoxy group is a well-known structural alert. Compounds bearing epoxy groups are alkylating agents in which carbon atoms represent electrophilic sites that react with nucleophilic DNA to form covalent bonds via nucleophilic substitution reaction, thus acting as direct genotoxins and carcinogens. Integrated Gradients recognized this structural alert in all cases, primarily highlighting the oxygen atom joined to one of the two carbon atoms. Most often, the electrophilic carbon atom responsible for chemical-DNA interaction was the most highlighted area, assuming that Integrated Gradients highlighted the most active site of the molecule from the genotoxicity point of view. Interestingly, the highlighting of the oxygen atom is always accompanied by one or two equally</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Explaining predictions of the fine-tuned model on Ames dataset. See Section 5.3. Part 1/5</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Explaining predictions of the fine-tuned model on Ames dataset. See Section 5.3. Part 2/5</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Explaining predictions of the fine-tuned model on Ames dataset. See Section 5.3. Part 3/5</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Explaining predictions of the fine-tuned model on Ames dataset. See Section 5.3. Part 4/5</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Explaining predictions of the fine-tuned model on Ames dataset. See Section 5.3. Part 5/5</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Joint First Authors. Correspondence: armenag@meta.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
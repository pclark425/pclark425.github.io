<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7" target="_blank">PAL: Program-aided Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-aided Language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method where an LLM generates interleaved natural-language comments and program code as intermediate reasoning steps, and final computation is executed by an external interpreter (Python), offloading arithmetic and deterministic computation to a symbolic runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PAL using code-davinci-002 (Codex) (evaluated also with other code- and text-based LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An experimental setup that uses a pre-trained code-capable LLM (primarily OpenAI Codex code-davinci-002) to generate Python programs as the chain of thought; the generated code is executed by a Python interpreter to obtain final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['program-aided reasoning (generate executable code as intermediate steps)', 'chain-of-thought-style NL comments interleaved with code', 'tool use (external interpreter execution)', 'few-shot in-context learning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is prompted with few-shot examples where each example contains the natural-language problem and a 'thought' which is a sequence of NL and PL (Python) lines. At test time the LLM generates the same hybrid NL+code sequence; the code portion is executed in a Python runtime to produce the final answer. Multi-sample (sampling multiple outputs + majority voting) is also used in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared PAL (programmatic NL+PL chains with interpreter) to CoT (NL-only chains) and DIRECT prompting. Also ablated PAL variants (no NL comments, random/uninformative variable names, succinct single-line expressions, forcing the LLM to simulate runtime and output the answer). Applied multi-sample/majority voting (k=40) to PAL as well.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>13 tasks spanning math word problems (GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, SINGLEEQ, SINGLEOP, ADDSUB, MULTIARITH), symbolic reasoning (BIG-Bench Hard: Colored Objects, Penguins, Date) and algorithmic tasks (Object Counting, Repeat Copy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PAL (Codex code-davinci-002) single-sample solve rates: GSM8K 72.0%, SVAMP 79.4%, ASDIV 79.6%, MAWPS-SINGLEEQ 96.1%, SINGLEOP 94.6%, ADDSUB 92.5%, MULTIARITH 99.2%; GSM-HARD 61.2%. Symbolic/algorithmic: Colored Objects 95.1%, Penguins 93.3%, Date 76.2%, Repeat Copy 90.6%, Object Counting 96.7%. Multi-sample majority@40 on GSM8K: PAL improves from 72.0% to 80.4%. PAL outperforms CoT variants (e.g., CoT_Codex GSM8K 65.6%) and larger LMs using CoT (e.g., PaLM-540B CoT reported 56.9% on GSM8K in table).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PAL produces more consistent, uniform expressions (e.g., list indexing, len(objects), objects[-1]) than CoT's varied NL phrasings; LLMs are confident producing programmatic tokens while CoT had lower confidence on numeric and grounded tokens. PAL is robust to large numbers (GSM-HARD), since computations are performed by the interpreter; meaningful variable names and NL comments improve mapping from NL to variables; removing comments or randomizing variable names degrades performance. PAL also remains stable as input complexity grows (e.g., number of objects) compared to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generating executable programs as intermediate reasoning steps and delegating execution to an interpreter yields higher accuracy and robustness across arithmetic, symbolic, and algorithmic tasks than purely natural-language chain-of-thought and direct prompting; the main benefit comes from synergy with the interpreter (correct execution) and from structured programmatic expressions that are easier for code-trained LMs to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot prompting strategy that augments examples with free-form natural-language intermediate reasoning steps ('thoughts') which the model is then asked to produce for test examples prior to the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT applied with multiple LMs (Codex code-davinci-002, PaLM-540B, Minerva-540B, LaMDA-137B, UL2-20B, text-davinci series in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting paradigm applied to various large LMs: produce NL step-by-step reasoning traces in-context, then final answer; can be combined with multi-sample/self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (natural language step-by-step decomposition)', 'few-shot prompting', 'self-consistency / multi-sample majority (when applied)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each in-context example is a triplet (input x, NL thought t, answer y). At test time the model generates both thought and answer. Multi-sample self-consistency samples multiple CoT outputs and chooses the most common final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct head-to-head comparison with PAL across the same benchmarks and same in-context examples where possible; multi-sample/majority voting experiments referenced and partially run for CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same set as PAL: GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, BIG-Bench Hard tasks (Colored Objects, Penguins, Date), Repeat Copy, Object Counting, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Representative results from paper: GSM8K CoT (Codex) 65.6%, CoT PaLM-540B 56.9%, CoT Minerva 58.8% (single-sample). On GSM-HARD CoT (Codex) ~23% (significant drop). Symbolic/algorithmic: Colored Objects CoT (Codex) 86.3%, Penguins 79.2%, Date 64.8%, Repeat Copy 68.8%, Object Counting 73.0%. CoT majority@40 for GSM8K (Codex) reported ~78.0% in Table 3 for CoT variants in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT can produce correct decompositions but often fails on arithmetic calculation and large numbers (the NL chain often remains the same when numbers are replaced by large numbers but final numeric computation fails). Token-level analysis shows CoT has lower confidence on numeric tokens, grounded spatial adjectives, and object property tokens compared to PAL's program tokens. CoT's phrasing is more varied and less consistent than programmatic expressions, making some reasoning brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT improves over direct prompting by eliciting intermediate reasoning, but CoT still suffers from arithmetic and calculation errors and instability with increasing complexity or large numbers; PAL's program-aided approach remedies many of these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot prompting where each example is an input-output pair (no intermediate reasoning steps) and the model is asked to produce the answer directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Few-Shot Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DIRECT prompting with Codex (code-davinci-002) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Few-shot in-context learning where examples contain only question-answer pairs; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['direct answer generation (no explicit intermediate chain)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt consists only of concatenated input-output examples; inference requires model to output final answer without producing intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared DIRECT to CoT and PAL on the same benchmarks as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, symbolic/algorithmic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Examples from paper: GSM8K DIRECT (Codex) 19.7%, GSM-HARD 5.0%; SVAMP 69.9%; ASDIV 74.0%; Symbolic: Colored Objects 75.7%, Penguins 71.1%, Date 49.9%, Repeat Copy 81.3%, Object Counting 37.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DIRECT is far weaker on complex reasoning tasks requiring multi-step calculation, and particularly fails on large-number versions (GSM-HARD), consistent with the importance of decomposed reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Direct prompting without intermediate steps yields substantially lower solve rates on multi-step reasoning tasks; intermediate-chain methods (CoT, PAL) significantly improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency / Majority</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency / multi-sample majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple reasoning traces (e.g., CoT or programmatic traces) and selects the final answer by majority vote to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to both CoT and PAL with nucleus sampling (p=0.95) and k=40 samples (temperature 0.7 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generate multiple stochastic outputs (reasoning traces and resulting answers) and pick the most common final answer (majority@k).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling multiple reasoning traces', 'ensemble/majority voting (self-consistency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Nucleus sampling with p=0.95 and temperature 0.7 used to draw k=40 samples; final prediction selected by majority vote of answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Applied k=40 sampling to PAL and compared single-sample vs majority results; referenced prior CoT self-consistency literature for CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K primarily (also applicable to other datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PAL on GSM8K improved from 72.0% (single greedy sample) to 80.4% (majority@40). CoT literature also shows improvements with multi-sample/self-consistency (papers cited), and some CoT baselines report high multi-sample performance (e.g., Minerva results reported for 40 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling diverse reasoning traces and aggregating answers increases accuracy for chain-based methods; PAL benefits significantly from majority voting and remains competitive or superior to other models using similar ensemble approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency (multi-sample majority) further boosts accuracy of chain-like methods; PAL combined with multi-sample selection attains state-of-the-art few-shot accuracy on GSM8K in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8297.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8297.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablations of PAL (no NL comments, random variable names, succinct code, LLM-simulated runtime)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of experiments that remove or alter components of PAL to test which parts of the program-aided approach are responsible for the gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PAL ablations evaluated primarily with Codex variants (code-davinci-002, code-davinci-001, code-cushman-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablations include: PAL without NL comments (PAL_-comment), PAL without comments and with randomized variable names (PAL_-comment^-var), succinct single-line code (succinct code), and forcing the LLM to output the final answer instead of executing code (LLM simulating runtime).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['programmatic + NL (full PAL)', 'programmatic only (no comments)', 'programmatic with uninformative variable names', 'succinct one-line expressions', 'LLM-simulated execution (no external interpreter)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each ablation removes or alters the NL<->code scaffolding: removing NL comments tests grounding; replacing variable names with random tokens tests role of meaningful identifiers; succinct code tests whether multi-step program structure is necessary; LLM-simulated runtime tests whether executing code in an interpreter (vs letting the LLM compute) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (examines role of structured programmatic reasoning vs only NL or only code)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct ablation comparisons and reported solve rates (see Table 4, Table 6, Figure 9, Appendix A/B).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math tasks (GSM8K and GSM-HARD) and symbolic tasks (selected experiments reported in ablation tables)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ablation numbers reported: PAL (full prompt) ~71.8% (Table 4 average across some math tasks); PAL_-var (uninformative variable names) 59.0%; PAL_-var+comms (uninformative variable names but useful comments) 69.0%; Succinct Code 47.8% (falls to direct prompting level); LLM simulating runtime (forcing LLM to output answer after code) 23.2% on GSM8K (much lower than PAL's 72.0%). Other ablations: removing only NL comments (PAL_-comment) slightly reduces accuracy vs full PAL but still above CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Meaningful variable names and NL comments are important for grounding and mapping NL to program variables; simple structural code without comments or meaningful names is insufficient; the interpreter execution is essential â€” asking the LLM to simulate execution nearly removes the benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The principal benefit of PAL arises from (1) the interpreter executing correctly-formed code, and (2) the combination of NL comments and meaningful variable identifiers which help the LLM generate correct programs; removing either reduces or eliminates the advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8297.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8297.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model coding ability effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of base model's code modeling ability on PAL vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing PAL's effectiveness depends on the base model's ability to model/generate code; code-capable models show consistent PAL gains while weak code-ability can make CoT preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Comparisons across text-davinci-001/002/003 and code-davinci-001/002, code-cushman-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various LLMs with different pretraining emphasis: text-series (NL-focused) vs code-series (code-focused). Experiments evaluate PAL and CoT with the same LMs to isolate the role of code-generation ability.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['PAL (programmatic reasoning) vs CoT (NL reasoning) across base LMs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Run the same PAL prompt and CoT prompt with different back-end models (text-davinci family and code-davinci/cushman). Observe where PAL outperforms CoT and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (comparison across model abilities)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons: text-davinci-001: CoT 26.5% vs PAL 8.6%; text-davinci-002: CoT 46.9% vs PAL 65.8%; text-davinci-003: CoT 65.3% vs PAL 69.8%. Weaker code models (code-cushman, code-davinci-001) show lower absolute but consistent relative PAL gains compared to CoT (Figure 7, Section 6).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K and other math reasoning sets used for earlier comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>text-davinci-001 CoT 26.5% vs PAL 8.6%; text-davinci-002 CoT 46.9% vs PAL 65.8%; text-davinci-003 CoT 65.3% vs PAL 69.8%. Code-model family: relative PAL improvements consistent though absolute performance varies (see Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PAL requires a base model with adequate code-generation ability; with weak code modeling (text-davinci-001) CoT can outperform PAL, but once model code ability is sufficient (text-davinci-002/003 or code-davinci series), PAL provides gains. Thus PAL is not limited only to code-specialized LMs but benefits scale with code proficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>PAL's advantage depends on the underlying LM's ability to generate correct code; there is a threshold of code modeling competence where PAL overtakes CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAL: Program-aided Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 1)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8297",
    "paper_id": "paper-6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "PAL",
            "name_full": "Program-aided Language models",
            "brief_description": "A prompting method where an LLM generates interleaved natural-language comments and program code as intermediate reasoning steps, and final computation is executed by an external interpreter (Python), offloading arithmetic and deterministic computation to a symbolic runtime.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PAL using code-davinci-002 (Codex) (evaluated also with other code- and text-based LMs)",
            "model_description": "An experimental setup that uses a pre-trained code-capable LLM (primarily OpenAI Codex code-davinci-002) to generate Python programs as the chain of thought; the generated code is executed by a Python interpreter to obtain final answers.",
            "reasoning_methods": [
                "program-aided reasoning (generate executable code as intermediate steps)",
                "chain-of-thought-style NL comments interleaved with code",
                "tool use (external interpreter execution)",
                "few-shot in-context learning"
            ],
            "reasoning_methods_description": "The model is prompted with few-shot examples where each example contains the natural-language problem and a 'thought' which is a sequence of NL and PL (Python) lines. At test time the LLM generates the same hybrid NL+code sequence; the code portion is executed in a Python runtime to produce the final answer. Multi-sample (sampling multiple outputs + majority voting) is also used in some experiments.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared PAL (programmatic NL+PL chains with interpreter) to CoT (NL-only chains) and DIRECT prompting. Also ablated PAL variants (no NL comments, random/uninformative variable names, succinct single-line expressions, forcing the LLM to simulate runtime and output the answer). Applied multi-sample/majority voting (k=40) to PAL as well.",
            "task_or_benchmark": "13 tasks spanning math word problems (GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, SINGLEEQ, SINGLEOP, ADDSUB, MULTIARITH), symbolic reasoning (BIG-Bench Hard: Colored Objects, Penguins, Date) and algorithmic tasks (Object Counting, Repeat Copy).",
            "performance_results": "PAL (Codex code-davinci-002) single-sample solve rates: GSM8K 72.0%, SVAMP 79.4%, ASDIV 79.6%, MAWPS-SINGLEEQ 96.1%, SINGLEOP 94.6%, ADDSUB 92.5%, MULTIARITH 99.2%; GSM-HARD 61.2%. Symbolic/algorithmic: Colored Objects 95.1%, Penguins 93.3%, Date 76.2%, Repeat Copy 90.6%, Object Counting 96.7%. Multi-sample majority@40 on GSM8K: PAL improves from 72.0% to 80.4%. PAL outperforms CoT variants (e.g., CoT_Codex GSM8K 65.6%) and larger LMs using CoT (e.g., PaLM-540B CoT reported 56.9% on GSM8K in table).",
            "qualitative_findings": "PAL produces more consistent, uniform expressions (e.g., list indexing, len(objects), objects[-1]) than CoT's varied NL phrasings; LLMs are confident producing programmatic tokens while CoT had lower confidence on numeric and grounded tokens. PAL is robust to large numbers (GSM-HARD), since computations are performed by the interpreter; meaningful variable names and NL comments improve mapping from NL to variables; removing comments or randomizing variable names degrades performance. PAL also remains stable as input complexity grows (e.g., number of objects) compared to CoT.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generating executable programs as intermediate reasoning steps and delegating execution to an interpreter yields higher accuracy and robustness across arithmetic, symbolic, and algorithmic tasks than purely natural-language chain-of-thought and direct prompting; the main benefit comes from synergy with the interpreter (correct execution) and from structured programmatic expressions that are easier for code-trained LMs to produce.",
            "uuid": "e8297.0",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A few-shot prompting strategy that augments examples with free-form natural-language intermediate reasoning steps ('thoughts') which the model is then asked to produce for test examples prior to the final answer.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "CoT applied with multiple LMs (Codex code-davinci-002, PaLM-540B, Minerva-540B, LaMDA-137B, UL2-20B, text-davinci series in some experiments)",
            "model_description": "Prompting paradigm applied to various large LMs: produce NL step-by-step reasoning traces in-context, then final answer; can be combined with multi-sample/self-consistency.",
            "reasoning_methods": [
                "chain-of-thought (natural language step-by-step decomposition)",
                "few-shot prompting",
                "self-consistency / multi-sample majority (when applied)"
            ],
            "reasoning_methods_description": "Each in-context example is a triplet (input x, NL thought t, answer y). At test time the model generates both thought and answer. Multi-sample self-consistency samples multiple CoT outputs and chooses the most common final answer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Direct head-to-head comparison with PAL across the same benchmarks and same in-context examples where possible; multi-sample/majority voting experiments referenced and partially run for CoT baselines.",
            "task_or_benchmark": "Same set as PAL: GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, BIG-Bench Hard tasks (Colored Objects, Penguins, Date), Repeat Copy, Object Counting, etc.",
            "performance_results": "Representative results from paper: GSM8K CoT (Codex) 65.6%, CoT PaLM-540B 56.9%, CoT Minerva 58.8% (single-sample). On GSM-HARD CoT (Codex) ~23% (significant drop). Symbolic/algorithmic: Colored Objects CoT (Codex) 86.3%, Penguins 79.2%, Date 64.8%, Repeat Copy 68.8%, Object Counting 73.0%. CoT majority@40 for GSM8K (Codex) reported ~78.0% in Table 3 for CoT variants in literature.",
            "qualitative_findings": "CoT can produce correct decompositions but often fails on arithmetic calculation and large numbers (the NL chain often remains the same when numbers are replaced by large numbers but final numeric computation fails). Token-level analysis shows CoT has lower confidence on numeric tokens, grounded spatial adjectives, and object property tokens compared to PAL's program tokens. CoT's phrasing is more varied and less consistent than programmatic expressions, making some reasoning brittle.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT improves over direct prompting by eliciting intermediate reasoning, but CoT still suffers from arithmetic and calculation errors and instability with increasing complexity or large numbers; PAL's program-aided approach remedies many of these failures.",
            "uuid": "e8297.1",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "DIRECT",
            "name_full": "Direct few-shot prompting",
            "brief_description": "Standard few-shot prompting where each example is an input-output pair (no intermediate reasoning steps) and the model is asked to produce the answer directly.",
            "citation_title": "Language Models are Few-Shot Learners",
            "mention_or_use": "use",
            "model_name": "DIRECT prompting with Codex (code-davinci-002) baseline",
            "model_description": "Few-shot in-context learning where examples contain only question-answer pairs; used as a baseline.",
            "reasoning_methods": [
                "direct answer generation (no explicit intermediate chain)"
            ],
            "reasoning_methods_description": "Prompt consists only of concatenated input-output examples; inference requires model to output final answer without producing intermediate reasoning.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared DIRECT to CoT and PAL on the same benchmarks as baselines.",
            "task_or_benchmark": "GSM8K, GSM-HARD, SVAMP, ASDIV, MAWPS variants, symbolic/algorithmic tasks",
            "performance_results": "Examples from paper: GSM8K DIRECT (Codex) 19.7%, GSM-HARD 5.0%; SVAMP 69.9%; ASDIV 74.0%; Symbolic: Colored Objects 75.7%, Penguins 71.1%, Date 49.9%, Repeat Copy 81.3%, Object Counting 37.6%.",
            "qualitative_findings": "DIRECT is far weaker on complex reasoning tasks requiring multi-step calculation, and particularly fails on large-number versions (GSM-HARD), consistent with the importance of decomposed reasoning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Direct prompting without intermediate steps yields substantially lower solve rates on multi-step reasoning tasks; intermediate-chain methods (CoT, PAL) significantly improve performance.",
            "uuid": "e8297.2",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Self-Consistency / Majority",
            "name_full": "Self-consistency / multi-sample majority voting",
            "brief_description": "A method that samples multiple reasoning traces (e.g., CoT or programmatic traces) and selects the final answer by majority vote to improve robustness and accuracy.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "use",
            "model_name": "Applied to both CoT and PAL with nucleus sampling (p=0.95) and k=40 samples (temperature 0.7 in experiments)",
            "model_description": "Generate multiple stochastic outputs (reasoning traces and resulting answers) and pick the most common final answer (majority@k).",
            "reasoning_methods": [
                "sampling multiple reasoning traces",
                "ensemble/majority voting (self-consistency)"
            ],
            "reasoning_methods_description": "Nucleus sampling with p=0.95 and temperature 0.7 used to draw k=40 samples; final prediction selected by majority vote of answers.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Applied k=40 sampling to PAL and compared single-sample vs majority results; referenced prior CoT self-consistency literature for CoT baselines.",
            "task_or_benchmark": "GSM8K primarily (also applicable to other datasets)",
            "performance_results": "PAL on GSM8K improved from 72.0% (single greedy sample) to 80.4% (majority@40). CoT literature also shows improvements with multi-sample/self-consistency (papers cited), and some CoT baselines report high multi-sample performance (e.g., Minerva results reported for 40 samples).",
            "qualitative_findings": "Sampling diverse reasoning traces and aggregating answers increases accuracy for chain-based methods; PAL benefits significantly from majority voting and remains competitive or superior to other models using similar ensemble approaches.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency (multi-sample majority) further boosts accuracy of chain-like methods; PAL combined with multi-sample selection attains state-of-the-art few-shot accuracy on GSM8K in these experiments.",
            "uuid": "e8297.3",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "PAL ablations",
            "name_full": "Ablations of PAL (no NL comments, random variable names, succinct code, LLM-simulated runtime)",
            "brief_description": "Set of experiments that remove or alter components of PAL to test which parts of the program-aided approach are responsible for the gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PAL ablations evaluated primarily with Codex variants (code-davinci-002, code-davinci-001, code-cushman-001)",
            "model_description": "Ablations include: PAL without NL comments (PAL_-comment), PAL without comments and with randomized variable names (PAL_-comment^-var), succinct single-line code (succinct code), and forcing the LLM to output the final answer instead of executing code (LLM simulating runtime).",
            "reasoning_methods": [
                "programmatic + NL (full PAL)",
                "programmatic only (no comments)",
                "programmatic with uninformative variable names",
                "succinct one-line expressions",
                "LLM-simulated execution (no external interpreter)"
            ],
            "reasoning_methods_description": "Each ablation removes or alters the NL&lt;-&gt;code scaffolding: removing NL comments tests grounding; replacing variable names with random tokens tests role of meaningful identifiers; succinct code tests whether multi-step program structure is necessary; LLM-simulated runtime tests whether executing code in an interpreter (vs letting the LLM compute) is critical.",
            "reasoning_diversity": "both (examines role of structured programmatic reasoning vs only NL or only code)",
            "reasoning_diversity_experimental_setup": "Direct ablation comparisons and reported solve rates (see Table 4, Table 6, Figure 9, Appendix A/B).",
            "task_or_benchmark": "Math tasks (GSM8K and GSM-HARD) and symbolic tasks (selected experiments reported in ablation tables)",
            "performance_results": "Ablation numbers reported: PAL (full prompt) ~71.8% (Table 4 average across some math tasks); PAL_-var (uninformative variable names) 59.0%; PAL_-var+comms (uninformative variable names but useful comments) 69.0%; Succinct Code 47.8% (falls to direct prompting level); LLM simulating runtime (forcing LLM to output answer after code) 23.2% on GSM8K (much lower than PAL's 72.0%). Other ablations: removing only NL comments (PAL_-comment) slightly reduces accuracy vs full PAL but still above CoT.",
            "qualitative_findings": "Meaningful variable names and NL comments are important for grounding and mapping NL to program variables; simple structural code without comments or meaningful names is insufficient; the interpreter execution is essential â€” asking the LLM to simulate execution nearly removes the benefit.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The principal benefit of PAL arises from (1) the interpreter executing correctly-formed code, and (2) the combination of NL comments and meaningful variable identifiers which help the LLM generate correct programs; removing either reduces or eliminates the advantage.",
            "uuid": "e8297.4",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Model coding ability effect",
            "name_full": "Effect of base model's code modeling ability on PAL vs CoT",
            "brief_description": "Analysis showing PAL's effectiveness depends on the base model's ability to model/generate code; code-capable models show consistent PAL gains while weak code-ability can make CoT preferable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Comparisons across text-davinci-001/002/003 and code-davinci-001/002, code-cushman-001",
            "model_description": "Various LLMs with different pretraining emphasis: text-series (NL-focused) vs code-series (code-focused). Experiments evaluate PAL and CoT with the same LMs to isolate the role of code-generation ability.",
            "reasoning_methods": [
                "PAL (programmatic reasoning) vs CoT (NL reasoning) across base LMs"
            ],
            "reasoning_methods_description": "Run the same PAL prompt and CoT prompt with different back-end models (text-davinci family and code-davinci/cushman). Observe where PAL outperforms CoT and vice versa.",
            "reasoning_diversity": "both (comparison across model abilities)",
            "reasoning_diversity_experimental_setup": "Direct comparisons: text-davinci-001: CoT 26.5% vs PAL 8.6%; text-davinci-002: CoT 46.9% vs PAL 65.8%; text-davinci-003: CoT 65.3% vs PAL 69.8%. Weaker code models (code-cushman, code-davinci-001) show lower absolute but consistent relative PAL gains compared to CoT (Figure 7, Section 6).",
            "task_or_benchmark": "GSM8K and other math reasoning sets used for earlier comparisons",
            "performance_results": "text-davinci-001 CoT 26.5% vs PAL 8.6%; text-davinci-002 CoT 46.9% vs PAL 65.8%; text-davinci-003 CoT 65.3% vs PAL 69.8%. Code-model family: relative PAL improvements consistent though absolute performance varies (see Figure 7).",
            "qualitative_findings": "PAL requires a base model with adequate code-generation ability; with weak code modeling (text-davinci-001) CoT can outperform PAL, but once model code ability is sufficient (text-davinci-002/003 or code-davinci series), PAL provides gains. Thus PAL is not limited only to code-specialized LMs but benefits scale with code proficiency.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "PAL's advantage depends on the underlying LM's ability to generate correct code; there is a threshold of code modeling competence where PAL overtakes CoT.",
            "uuid": "e8297.5",
            "source_info": {
                "paper_title": "PAL: Program-aided Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways",
            "rating": 1,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        }
    ],
    "cost": 0.019084999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PAL: Program-aided Language Models</h1>
<p>Luyu Gao ${ }^{<em> 1}$ Aman Madaan ${ }^{</em> 1}$ Shuyan Zhou ${ }^{* 1}$ Uri Alon ${ }^{1}$ Pengfei Liu ${ }^{12}$ Yiming Yang ${ }^{1}$ Jamie Callan ${ }^{1}$ Graham Neubig ${ }^{12}$<br>{luyug, amadaan, shuyanzh, ualon, pliu3, yiming, callan, gneubig}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("fewshot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present ProgramAided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using CODEX achieves state-of-theart few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute $15 \%$ top1. Our code and data are publicly available at http://reasonwithpal.com.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Until as recently as two years ago, reasoning was considered to be one of the most significant challenges that large language models (LLMs) had not yet overcome (Marcus, 2018; 2020; Garcez \&amp; Lamb, 2020). Recently, LLMs have shown impressive success on a wide range of tasks, including commonsense (Wei et al., 2021; Sanh et al., 2021; Madaan et al., 2022), mathematical (Lewkowycz et al., 2022; Wu et al., 2022; Mishra et al., 2022), and symbolic reasoning (Yao et al., 2022; Ahn et al., 2022), using few-shot prompting (Brown et al., 2020).</p>
<p>This process has been accelerated by methods that require LLMs to generate their explicit reasoning steps, such as "chain-of-thought" (Wei et al., 2022), "scratchpads" (Nye et al., 2021), and "least-to-most" (Zhou et al., 2022) prompting. In particular, the widely used chain-of-thought (CoT) method presents the model with the explicit intermediate steps that are required to reach the final answer. Then, the model is expected to apply a similar decomposition to the actual test example, and consecutively reach an accurate final answer (Ling et al., 2017; Amini et al., 2019). Nevertheless, while LLMs can decompose natural language problems into steps and perform simple arithmetic operations, their performance falls dramatically when dealing with complex arithmetic (Hendrycks et al., 2021; Madaan \&amp; Yazdanbakhsh, 2022) or large numbers (Nogueira et al., 2021; Qian et al., 2022). In fact, even when fine-tuning a PaLM-based model on 164B tokens of explicit mathematical content, its two most common failures are reportedly "incorrect reasoning" and "incorrect calculation" (Lewkowycz et al., 2022).</p>
<p>In this paper, we propose Program-Aided Language model (PAL): a novel method that uses an LLM to read natural language problems and generate programs as reasoning steps, but offloads the solution step to a Python interpreter, as illustrated in Figure 1. This offloading leverages an LLM that can decompose a natural language problem into programmatic steps, which is fortunately available using contemporary state-of-the-art LLMs that are pre-trained on both natural language and programming languages (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022). While natural language understanding and decomposition</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A diagram illustrating PAL: Given a mathematical reasoning question, Chain-of-thought (left) generates intermediate reasoning steps of free-form text. In contrast, Program-aided Language models (PAL, right) generate intermediate steps and Python code. This shifts the role of running the reasoning steps from the language model to the Python interpreter. The final answer is obtained by running the generated reasoning chain. Chain-of-thought reasoning is highlighted in blue; PAL steps are highlighted in gray and pink; the Python interpreter run is highlighted in black and green.
require LLMs, solving and reasoning can be done with the external solver. This bridges an important gap in chain-of-thought-like methods, where reasoning chains can be correct but produce an incorrect answer.</p>
<p>We demonstrate the effectiveness of PAL across $\mathbf{1 3}$ arithmetic and symbolic reasoning tasks. In all these tasks, PAL using Codex (Chen et al., 2021a) outperforms much larger models such as PaLM-540n using chain-of-thought prompting. For example, on the popular GSM8K benchmark, PAL achieves state-of-the-art accuracy, surpassing PaLM-540n with chain-of-thought by absolute $15 \%$ top1 accuracy. When the questions contain large numbers, a dataset we call GSM-HARD, PAL outperforms CoT by an absolute $40 \%$. We believe that this seamless synergy between a neural LLM and a symbolic interpreter is an essential step towards general and robust AI reasoners.</p>
<h2>2. Background: Few-shot Prompting</h2>
<p>Few-shot prompting leverages the strength of large-language models to solve a task with a set of $k$ examples that are provided as part of the test-time input (Brown et al., 2020; Liu et al., 2021; Chowdhery et al., 2022), where $k$ is usually a number in the low single digits. These input-output examples $\left{\left(x_{i}, y_{i}\right)\right}<em 1="1">{i=1}^{k}$ are concatenated in a prompt $p$ $\equiv\left\langle x</em>$. Note that such fewshot prompting does not modify the underlying LLM.} \cdot y_{1}\right\rangle |\left\langle x_{2} \cdot y_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot y_{k}\right\rangle$, where "." denotes the concatenation of an input and output, and " $|$ " indicate the concatenation of different examples. During inference, a test instance $x_{\text {test }}$ is appended to the prompt, and $p | x_{\text {test }}$ is passed to the model which attempts to complete $p | x_{\text {test }}$, and thereby generate an answer $y_{\text {test }</p>
<p>Wei et al. (2022) additionally augment each in-context example with chain of thought (CoT) intermediate steps. Specifically, each in-context example in the CoT setup is a triplet $\left\langle x_{i}, t_{i}, y_{i}\right\rangle$, where $x_{i}$ and $y_{i}$ are input-output pair as before, and $t_{i}$ is a natural language description of the steps that are needed to arrive at the output $y_{i}$ from the input $x_{i}$. See Figure 1 for an example. With the additional "thoughts" $t_{i}$, the prompt is set to $p \equiv\left\langle x_{1} \cdot t_{1} \cdot y_{1}\right\rangle |\left\langle x_{2} \cdot t_{2} \cdot y_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot t_{k} \cdot y_{k}\right\rangle$.
During inference, the new question $x_{\text {test }}$ is appended to the prompt as before and supplied to the LLM. Crucially, the model is tasked with generating both the thought $t_{\text {test }}$ and the final answer $y_{\text {test }}$. This approach of prompting the model to first generate a reasoning process $t_{\text {test }}$ improves the accuracy of the answer $y_{\text {test }}$ across a wide range of tasks (Wang et al., 2022a; Wei et al., 2022; Zhou et al., 2022; Wang et al., 2022b).</p>
<h2>3. Program-aided Language Models</h2>
<p>In a Program-aided Language model, we propose to generate the thoughts $t$ for a given natural language problem $x$ as interleaved natural language (NL) and programming language (PL) statements. Since we delegate the solution step to an interpreter, we do not provide the final answers to the examples in our prompt. That is, every in-context example in PAL is a pair $\left\langle x_{i}, t_{i}\right\rangle$, where $t_{j}=\left[s_{1}, s_{2}, \ldots, s_{N}\right]$ with each $s_{i} \in \mathrm{NL} \cup \mathrm{PL}$, a sequence of tokens in either NL or PL. The complete prompt is thus $p$ $\equiv\left\langle x_{1} \cdot t_{1}\right\rangle |\left\langle x_{2} \cdot t_{2}\right\rangle | \ldots |\left\langle x_{k} \cdot t_{k}\right\rangle$.
Given a test instance $x_{\text {test }}$, we append it to the prompt, and $p | x_{\text {test }}$ is fed to the LM. We let the LM generate a prediction $t_{\text {test }}$, which contains both the intermediate steps and their corresponding programmatic statements.</p>
<div class="codehilite"><pre><span></span><code><span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Roger</span><span class="w"> </span><span class="n">started</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="n">tennis</span><span class="w"> </span><span class="n">balls</span>
<span class="n">tennis_balls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="mi">2</span><span class="o">}\</span><span class="n">mathrm</span><span class="o">{</span><span class="w"> </span><span class="n">cans</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">tennis</span><span class="w"> </span><span class="n">balls</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">is</span>
<span class="n">bought_balls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span>
<span class="n">tennis</span><span class="w"> </span><span class="n">balls</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tennis_balls</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bought_balls</span>
</code></pre></div>

<p>Figure 2: A close-up of a single example from a PAL prompt. Chain-of-thought reasoning is highlighted in blue and PAL programmatic steps are highlighted in gray and pink.</p>
<p>Example A close-up of the example from Figure 1 is shown in Figure 2. While chain-of-thought only decomposes the solution in the prompt into natural language steps such as Roger started with 5 tennis balls and</p>
<p>2 cans of 3 tennis balls each is 6 , in PAL we also augment each such NL step with its corresponding programmatic statement such as tennis_balls $=5$ and bought_balls $=2 * 3$. This way, the model learns to generate a program that will provide the answer for the test question, instead of relying on LLM to perform the calculation correctly.</p>
<p>We prompt the language model to generate NL intermediate steps using comment syntax (e.g. "# . . ." in Python) such they will be ignored by the interpreter. We pass the generated program $t_{\text {test }}$ to its corresponding solver, we run it, and obtain the final run result $y_{\text {test }}$. In this work we use a standard Python interpreter, but this can be any solver, interpreter or a compiler.</p>
<p>Crafting prompts for PAL In our experiments, we leveraged the prompts of existing work whenever available, and otherwise randomly selected the same number (3-6) of examples as previous work for creating a fixed prompt for every benchmark. In all cases, we augmented the free-form text prompts into PAL-styled prompts, leveraging programming constructs such as for loops and dictionaries when needed. Generally, writing PAL prompts is easy and quick.
We also ensure that variable names in the prompt meaningfully reflect their roles. For example, a variable that describes the number of apples in the basket should have a name such as num_apples_in_basket. This keeps the generated code linked to the entities in the question. In Section 6 we show that such meaningful variable names are critical. Notably, it is also possible to incrementally run the PL segments and feed the execution results back to the LLM to generate the following blocks. For simplicity, in our experiments, we used a single, post-hoc, execution.</p>
<p>This work focuses on CoT-style reasoning chain, but in Appendix I we show that PAL also improves Least-toMost (Zhou et al., 2022) prompts, which introduce reasoning chains that decompose a question into sub-questions.</p>
<h2>4. Experimental Setup</h2>
<p>Data and in-context examples We experiment with three broad classes of reasoning tasks: (1) mathematical problems (Â§4.1) from a wide range of datasets including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), and MAWPS (Koncel-Kedziorski et al., 2016); (2) symbolic reasoning (Â§4.2) from BIG-Bench Hard (Suzgun et al., 2022); (3) and algorithmic problems (Â§4.3) from BIG-Bench Hard as well. Details of all datasets are shown in Appendix H. For all of the experiments for which CoT prompts were available, we use the same incontext examples as used by previous work. Otherwise, we randomly sampled a fixed set of in-context examples, and</p>
<p>Q: Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?</p>
<div class="codehilite"><pre><span></span><code>money_initial = 23
bagels = 5
bagel_cost = 3
money_spent = bagels * bagel_cost
money_left = money_initial - money_spent
answer = money_left
</code></pre></div>

<p>Figure 3: Example prompt for the mathematical reasoning tasks, from the GSM8K benchmark.
Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?</p>
<div class="codehilite"><pre><span></span><code><span class="n">stress_ball_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">objects</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">object</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;stress ball&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="n">stress_ball_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span>
<span class="w">        </span><span class="k">break</span>
<span class="c1"># Find the directly right object</span>
<span class="n">direct_right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">objects</span><span class="p">[</span><span class="n">stress_ball_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Check the directly right object&#39;s color</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">direct_right</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<p>Figure 4: An example for a PAL prompt in the Colored ObJects task. For space considerations, we omit the code that creates the list objects.
used the same set for PAL and COT.</p>
<p>Baselines We consider three prompting strategies: DIRECT prompting - the standard prompting approach using pairs of questions and immediate answers (e.g., $\mathbf{T I}$ ) as in Brown et al. (2020); chain-of-thought (CoT) prompting (Wei et al., 2022); and our PAL prompting. We performed greedy decoding from the language model using a temperature of 0 . Unless stated otherwise, we used CODEX (code-davinci-002) as our backend LLM for both PAL, DIRECT, and COT. In datasets where results for additional base LMs, such as PaLM-540B, were available from previous work, we included them as COT ${ }_{\text {PaLM-540B }}$.</p>
<h3>4.1. Mathematical Reasoning</h3>
<p>We evaluate PAL on eight mathematical word problem datasets. Each question in these tasks is an algebra word problem at grade-school level. An example for a question and PAL example prompt is shown in Figure 3. We found that using explicit NL intermediate steps does not further benefit these math reasoning tasks, hence we kept only the meaningful variable names in the prompt.</p>
<p>GSM-HARD LLMs can perform simple calculations with small numbers. However, Madaan \&amp; Yazdanbakhsh (2022) found that $50 \%$ of the numbers in the popular GSM8K dataset of math reasoning problems are integers between 0
and 8. This raises the question of whether LLMs can generalize to larger and non-integer numbers? We constructed a harder version of GSM8K, which we call GSM-HARD, by replacing the numbers in the questions of GSM8K with larger numbers. Specifically, one of the numbers in a question was replaced with a random integer of up to 7 digits. More details regarding the this new dataset are provided in H.1.</p>
<h3>4.2. Symbolic Reasoning</h3>
<p>We applied PAL to three symbolic reasoning tasks from BIG-Bench Hard (Suzgun et al., 2022), which involve reasoning about objects and concepts: (1) Colored ObJECTS requires answering questions about colored objects on a surface. This task requires keeping track of relative positions, absolute positions, and the color of each object. Figure 4 shows an example for a question and example PAL prompt. (2) PENGUINS describes a table of penguins and some additional information in natural language, and the task is to answer a question about the attributes of the penguins, for example, "how many penguins are less than 8 years old?". While both PENGUINS and Colored ObJect tasks require tracking objects, PENGUINS describes dynamics as well, since the penguins in the problem can be added or removed. Figure 17 in Appendix J. 2 shows an example for a question, a chain-of-thought prompt, and PAL prompt. (3) DATE is a date understanding task that involves inferring dates from natural language descriptions, performing addi-</p>
<p>Q: I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, a cabbage, two onions, and three fridges. How many vegetables do I have?</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> note: I&#39;m not counting the chair, tables,
    or fridges
vegetables_to_count = {
    &#39;potato&#39;: 2,
    &#39;cauliflower&#39;: 1,
    &#39;lettuce head&#39;: 1,
    &#39;cabbage&#39;: 1,
    &#39;onion&#39;: 2
}
answer = sum(vegetables_to_count.values())
</code></pre></div>

<p>Figure 5: An example for a PAL prompt in the ObJect Counting task. The base LM is expected to convert the input into a dictionary where keys are entities and values are their quantities, while filtering out non-vegetable entities. Finally, the answer is the sum of the dictionary values.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">GSM-HARD</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">ASDIV</th>
<th style="text-align: center;">SINGLEEQ</th>
<th style="text-align: center;">SINGLEOP</th>
<th style="text-align: center;">ADDSUB</th>
<th style="text-align: center;">MULTIARITH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIRECT $_{\text {Codex }}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;">COT UL2-20B</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">10.7</td>
</tr>
<tr>
<td style="text-align: center;">COT LaMDA-137B</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">COT Codex</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;">COT PaLM-540B</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">94.7</td>
</tr>
<tr>
<td style="text-align: center;">COT Minerva 540B</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PAL</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">99.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Problem solve rate (\%) on mathematical reasoning datasets. The highest number on each task is in bold. The results for DIRECT and PaLM-540B are from Wei et al. (2022), the results for LaMDA and UL2 are from Wang et al. (2022b), and the results for Minerva are from Lewkowycz et al. (2022). We ran PAL on each benchmark 3 times and report the average; the standard deviation is provided in Table 7.
tion and subtraction of relative periods of time, and having some global knowledge such as "how many days are there in February", and performing the computation accordingly. Appendix J. 3 shows example prompts.</p>
<h3>4.3. Algorithmic Tasks</h3>
<p>Finally, we compare PAL and COT on algorithmic reasoning. These are tasks where a human programmer can write a deterministic program with prior knowledge of the question. We experiment with two algorithmic tasks: ObJect Counting and Repeat Copy. Object Counting involves answering questions about the number of objects belonging to a certain type. For example, as shown in Figure 5: "I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, ... How many vegetables do I have?"). REPEAT COPY requires generating a sequence of words according to instructions. For example, as shown in Appendix J.6: "Repeat the word duck four times, but halfway through also say quack").</p>
<h2>5. Results</h2>
<h3>5.1. Math Results</h3>
<p>Table 1 shows the following results: across all tasks, PAL using Codex sets a new few-shot state-of-the-art top1 decoding across all datasets, outperforming $\mathrm{CoT}<em _PaLM-540B="{PaLM-540B" _text="\text">{\text {Codex }}$, $\mathrm{CoT}</em>$ which was fine-tuned on explicit mathematical content.}}$, and $\mathrm{CoT}_{\text {Minerva } 540 \mathrm{~B}</p>
<p>Interestingly, COT also benefits from Codex over PaLM540B in some of the datasets such as ASDIV, but performs worse than PaLM-540B in others such as SVAMP. Yet, using PAL further improves the solve rate across all datasets.</p>
<p>GSM-HARD On GSM-HARD (Table 1), the accuracy of DIRECT drops dramatically from $19.7 \%$ to $5.0 \%$ (a relative drop of $74 \%$ ), the accuracy of COT drops from $65.6 \%$ to $20.1 \%$ (a relative drop of almost $70 \%$ ), while PAL remains stable at $61.5 \%$, dropping by only $14.3 \%$. The results of COT on GSM-HARD did not improve even when we replaced its prompts with prompts that include large numbers (Appendix B). This shows how PAL provides not only better</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Colored Object</th>
<th style="text-align: center;">Penguins</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Repeat Copy</th>
<th style="text-align: center;">Object Counting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DIRECT $_{\text {Codex }}$</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">37.6</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {LaMDA-137B }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {PaLM-540B }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Codex }}$</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: left;">PAL $_{\text {Codex }}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PAL achieves a much higher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available to public (Wei et al., 2022; Suzgun et al., 2022).
results on the standard benchmarks, but is also much more robust. In fact, since PAL offloads the computation to the Python interpreter, any complex computation can be performed accurately given the correctly generated program.</p>
<p>Large Numbers or Incorrect Reasoning? Are the failures on GSM-HARD primarily due to the inability of LLMs to do arithmetic, or do the large numbers in the question "confuse" the LM which generates irrational intermediate steps? To investigate this, we evaluated the outputs generated by CoT for the two versions of the same question (with and without large numbers). We find that in 16 out of 25 cases we analyzed, CoT generates nearly identical natural language "thoughts", indicating that the primary failure mode is the inability to perform arithmetic accurately. Sample outputs are provided in the Appendix, Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT UL2-20B</td>
<td style="text-align: right;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {LaMDA-137B }}$</td>
<td style="text-align: right;">27.7</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Codex }}$</td>
<td style="text-align: right;">78.0</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {PaLM-540B }}$</td>
<td style="text-align: right;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">COT $_{\text {Minerva 540B }}$</td>
<td style="text-align: right;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">PAL $_{\text {Codex }}$</td>
<td style="text-align: right;">$\mathbf{8 0 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Problem solve rate (\%) on GSM8K using majority@40 (Wang et al., 2022b)</p>
<p>Multi-sample Generation As found by Wang et al. (2022b), chain-of-thought-style methods can be further improved by sampling $k&gt;1$ outputs, and selecting the final answer using majority voting. We thus repeated the greedy-decoding experiments using nucleus sampling (Holtzman et al., 2019) with $p=0.95$ and $k=40$ as in Lewkowycz et al. (2022) and temperature of 0.7 . As shown in Table 3, this further increases the accuracy of PAL from $72.0 \%$ to $80.4 \%$ on GSM8K, obtaining $1.9 \%$ higher accuracy than Minerva-540B using the same number of samples.</p>
<h3>5.2. Symbolic Reasoning \&amp; Algorithmic Tasks Results</h3>
<p>Results for symbolic reasoning and algorithmic tasks are shown in Table 2. In Colored Objects, PAL improves over the strong COT by $8.8 \%$, and by $19.4 \%$ over the standard direct prompting. In Penguins, PAL provides a gain of absolute $14.1 \%$ over CoT. In Date, PAL further provides $11.4 \%$ gain over both COT ${ }_{\text {Codex }}$, PaLM-540B, and LaMDA-137B.
The two rightmost columns of Table 2 show that PAL is close to solving ObJect Counting, reaching $96.7 \%$ and improving over COT by absolute $23.7 \%$. Similarly, PAL vastly outperforms CoT by absolute $21.8 \%$ on REPEAT COPY. Surprisingly, DIRECT prompting performs better than COT on REPEAT COPY. Yet, PAL improves over DIRECT by $9.3 \%$ in REPEAT COPY.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: The solve rate on Colored Objects with respect to the number of objects included in the test question.</p>
<p>Is PAL sensitive to the complexity of the question? We examined how the performance of PAL and COT change as the complexity of the input question grows, measured as the number of objects in the question of Colored Objects. As shown in Figure 6, PAL is superior COT across all input lengths. As the number of objects in the question increases, COT's accuracy is unstable and drops, while PAL remains consistently close to $100 \%$. More analysis on the token-level predictions can be found in Appendix G.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: PAL with different models on GSM8K: though the absolute accuracies with code-cushman-001 and code-davinci-001 are lower than code-davinci-002, the relative improvement of PAL over COT is consistent across models.</p>
<h2>6. Analysis</h2>
<p>Does PAL work with weaker LMs? In all our experiments in Section 5, PAL used the code-davinci-002 model; but can PAL work with weaker models of code? We compared PAL with CoT when both prompting approaches use the same weaker base LMs code-cushman-001 and code-davinci-001. As shown in Figure 7, even though the absolute accuracies of code-cushman-001 and code-davinci-001 are lower, the relative improvement of PAL over COT remains consistent across models. This shows that PAL can work with weaker models, while its benefit scales elegantly to stronger models as well.</p>
<p>Does PAL work with LMs of natural language? We also experimented with PAL using the text-davinci series. Figure 8 shows the following interesting results: when the base LM's "code modeling ability" is weak (using text-davinci-001), CoT performs better than PAL. However, once the LM's code modeling ability is sufficiently high (using text-davinci-002 and text-davinci-003), PAL outperforms CoT, and PAL text-davinci-003 performs almost as PAL code-davinci-002. This shows that PAL is not limited to LMs of code, but it can work with LMs that were mainly trained for natural language, if they have a sufficiently high coding ability.</p>
<p>Is PAL better because of the Python prompt or because of the interpreter? We experimented with generating Python code, while requiring the neural LM to "execute" it as well, without using an interpreter, following Nye et al. (2021); Madaan et al. (2022). We created prompts that are similar to PAL's, except that they do include the final answer. This resulted in a 23.2 solve rate on GSM8K, much lower than PAL (72.0), and only 4.5 points higher than DIRECT. These results reinforce our hypothesis that the main benefit of PAL comes from the synergy with the interpreter, and
<img alt="img-3.jpeg" src="img-3.jpeg" />
text-davinci-001 text-davinci-002 text-davinci-003
Figure 8: PAL with NL LMs on GSM8K: though CoT outperforms PAL with text-davinci-001, once the base LM is sufficiently strong, PAL is beneficial with text-davinci-002 and text-davinci-003 as well. That is, PAL is not limited to code-LMs only.
not only from having a better prompt. Additional details are provided in Appendix B. For additional discussion on the advantages of code-prompts over textual-prompts, see Appendix G.</p>
<p>Do variable names matter? In all our experiments, we used meaningful variable names in the PAL prompts, to ease the model's grounding of variables to the entities they represent. For the Python interpreter, however, variable names are meaningless. To measure the importance of meaningful variable names, we experimented with two prompts variants:</p>
<ol>
<li>$\mathrm{PAL}_{\text {-comment }}$ - the PAL prompt without intermediate NL comments.</li>
<li>$\mathrm{PAL}_{\text {-comment }}^{\text {-var }}$ - the PAL prompt without intermediate NL comments and with variable names substituted with random characters.</li>
</ol>
<p>The results are shown in Figure 9. In Colored ObJeCTED and DATE, removing intermediate NL comments but keeping meaningful variable names ( $\mathrm{PAL}<em _-comment="{-comment" _text="\text">{\text {-comment }}$ ) - slightly reduces the results compared to the full PAL prompt, but it still achieves higher accuracy than the baselines COT. Removing variable names as well ( $\mathrm{PAL}</em>$ ) further decreases accuracy, and performs worse than COT. Since variable names have an important part in code quality (Gellenbeck \&amp; Cook, 1991; Takang et al., 1996), meaningful variable names are only expected to ease reasoning for Codex, which was trained on mostly meaningful names, as was also found by Madaan et al. (2022).}}^{\text {-var }</p>
<h2>7. Related Work</h2>
<p>Prompting Few-shot prompting (Brown et al., 2020) has been shown to be an effective approach for a variety of tasks (Liu et al., 2021) ranging from text- (Gehrmann et al.,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: Ablation study of PAL prompt formats. We consider the original PAL prompt, it with natural language comments removed ( $\mathrm{PAL}<em _comment="{comment" _text="\text">{\text {comment }}$ ), and further variable names replaced with random character ( $\mathrm{PAL}</em>$). As a reference, we also show the CoT performance (blue).}}^{-</p>
<p>2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021) to code-generation (Chen et al., 2021b). Methods such as chain-of-thought prompting (CoT) have further unlocked a variety of reasoning tasks, boosting the performance of models on a variety of benchmarks. Nevertheless, all previous approaches suffer from inaccuracy in arithmetic calculation and incorrect reasoning (Lewkowycz et al., 2022; Hendrycks et al., 2021; Madaan \&amp; Yazdanbakhsh, 2022). PAL avoids these problems by offloading the calculation and some of the reasoning to a Python interpreter, which is correct by construction, given the right program. Further, not only that PAL can improve the standard chain-of-thought, it can improve least-to-most prompting (Zhou et al., 2022) as well, as we show in Appendix I.</p>
<p>LMs with external tools Several prior works have equipped neural models with specialized modules. For example, Cobbe et al. (2021) employ a calculator for arithmetic operations as a post hoc processing, and Demeter \&amp; Downey (2020) add specialized modules for generating cities and dates. Unlike these works, PAL generates code for a Python interpreter, which is general enough to handle both arithmetic calculations and dates, without specialized modules and ad-hoc fixes. Chowdhery et al. (2022) and Wei et al. (2022) have also experimented with external calculators; however, the calculator had improved Codex by only $2.3 \%$ (absolute) on GSM8K and improved PaLM-540B by $1.7 \%$, while PAL improves Codex by $6.4 \%$ on the same benchmark (Section 5.1). Similarly to our work, Chowdhery et al. (2022) have also experimented with generating Python code for solving the GSM8K benchmark, but their experiments resulted in lower accuracy than the standard PaLM-540B that uses chain-of-thought. Pi et al. (2022) pretrain the model on execution results of random expressions on a calculator, instead of using the solver at test time as well. While their model can hypothetically perform arithmetic better than other pretrained LMs, their results on the SVAMP benchmark are much lower: $57.4 \%$ using a T5-11B
model, while PAL achieves $79.4 \%$ on the same benchmark without any specialized pretraining.</p>
<p>Shortly after a preprint of our work was submitted to arXiv, another related work on "program of thought prompting" (Chen et al., 2022) was also submitted to arXiv. Their method is conceptually similar to ours, but PoT (1) only demonstrates efficacy on mathematical problems, whereas we demonstrate gains on symbolic and algorithmic benchmarks as well, and (2) chose benchmark-specific prompt examples, while we used the same prompt examples as previous work, to disentangled the benefit of our approach from the benefit of the choice of examples.</p>
<p>Semantic parsing Our work can also be seen as a very general form of semantic parsing, where instead of parsing into strict domain-specific languages, the model generates free-form Python code. Some works constrain the decoder using a Context-Free Grammar (CFG) to generate a domainspecific meaning representation (Shin \&amp; Van Durme, 2021) or a canonical utterance, which can be converted to a Lisplike meaning representation (Shin et al., 2021). In contrast, PAL does not require any constraining or domain-specific representations other than Python code. Further, LMs that were pretrained on Python are abundant compared to other domain-specific languages, making Python code a much more preferable representation. Andor et al. (2019) generate task-specific arithmetic operations for reading comprehension tasks; Gupta et al. (2019) design neural modules such as count to deal with arithmetic operations. PAL generalizes these works by generating general Python programs, without the need for defining specialized modules. The closest work to ours technically may be Binder (Cheng et al., 2022), but it addressed mostly answering questions about tables using SQL and SQL-like Python.</p>
<h2>8. Conclusion</h2>
<p>We introduce PAL, a new method for natural language reasoning, using programs as intermediate reasoning steps. Differently from existing LM-based reasoning approaches, the main idea is to offload solving and calculating to an external Python interpreter, instead of using the LLM for both understanding the problem and solving. This results in a final answer that is guaranteed to be accurate, given the correctly predicted programmatic steps. We demonstrate this seamless synergy between an LLM and a Python interpreter across 13 tasks from BIG-Bench Hard and other benchmarks. In all these benchmarks, PAL outperforms larger LLMs such as PaLM-540B which use the popular "chain-of-thought" method and sets new state-of-the-art accuracy on all of them. We believe that these results unlock exciting directions for future neuro-symbolic AI reasoners.</p>
<h2>References</h2>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as I Can, not as I Say: Grounding Language in Robotic Affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi, Y., and Hajishirzi, H. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In ACL, 2019.</p>
<p>Andor, D., He, L., Lee, K., and Pitler, E. Giving bert a calculator: Finding operations and arguments with reading comprehension. arXiv preprint arXiv:1909.00109, 2019.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners. In NeurIPS, 2020.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,
C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021a.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Yu, T. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Demeter, D. and Downey, D. Just add functions: A neuralsymbolic language model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7634-7642, 2020.</p>
<p>Garcez, A. d. and Lamb, L. C. Neurosymbolic ai: the 3rd wave. arXiv preprint arXiv:2012.05876, 2020.</p>
<p>Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Anuoluwapo, A., Bosselut, A., Chandu, K. R., Clinciu, M., Das, D., Dhole, K. D., Du, W., Durmus, E., DuÅ¡ek, O., Emezue, C., Gangal, V., Garbacea, C., Hashimoto, T., Hou, Y., Jernite, Y., Jhamtani, H., Ji, Y., Jolly, S., Kale, M., Kumar, D., Ladhak, F., Madaan, A., Maddela, M., Mahajan, K., Mahamood, S., Majumder, B. P., Martins, P. H., McMillan-Major, A., Mille, S., van Miltenburg, E., Nadeem, M., Narayan, S., Nikolaev, V., Niyongabo, R. A., Osei, S., Parikh, A., PerezBeltrachini, L., Rao, N. R., Raunak, V., Rodriguez, J. D., Santhanam, S., Sedoc, J., Sellam, T., Shaikh, S., Shimorina, A., Cabezudo, M. A. S., Strobelt, H., Subramani, N., Xu, W., Yang, D., Yerukola, A., and Zhou, J. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics. arXiv preprint arXiv:2102.01672, 2021.</p>
<p>Gellenbeck, E. M. and Cook, C. R. An investigation of procedure and variable names as beacons during program comprehension. In Empirical studies of programmers: Fourth workshop, pp. 65-81. Ablex Publishing, Norwood, NJ, 1991.</p>
<p>Gupta, N., Lin, K., Roth, D., Singh, S., and Gardner, M. Neural module networks for reasoning over text. arXiv preprint arXiv:1912.04971, 2019.</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset, 2021. URL https://openreview.net/forum? id=7Bywt2mQsCe.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The Curious Case of Neural Text Degeneration. In ICLR, 2019.</p>
<p>Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 11521157, 2016.</p>
<p>Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. arXiv preprint arXiv:1705.04146, 2017.</p>
<p>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv preprint arXiv:2107.13586, 2021.</p>
<p>Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.</p>
<p>Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022.</p>
<p>Marcus, G. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631, 2018.</p>
<p>Marcus, G. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020.</p>
<p>Miao, S.-y., Liang, C.-C., and Su, K.-Y. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 92. URL https://aclanthology.org/2020. acl-main. 92.</p>
<p>Mishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., and Kalyan, A. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Nogueira, R., Jiang, Z., and Lin, J. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Patel, A., Bhattamishra, S., and Goyal, N. Are NLP Models Really Able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191, 2021.</p>
<p>Pi, X., Liu, Q., Chen, B., Ziyadi, M., Lin, Z., Gao, Y., Fu, Q., Lou, J.-G., and Chen, W. Reasoning like program executors. arXiv preprint arXiv:2201.11473, 2022.</p>
<p>Qian, J., Wang, H., Li, Z., Li, S., and Yan, X. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.</p>
<p>Reif, E., Ippolito, D., Yuan, A., Coenen, A., CallisonBurch, C., and Wei, J. A Recipe for Arbitrary Text Style Transfer with Large Language Models. arXiv preprint arXiv:2109.03910, 2021.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask Prompted Training Enables Zero-Shot Task Generalization, 2021.</p>
<p>Shin, R. and Van Durme, B. Few-shot semantic parsing with language models trained on code. arXiv preprint arXiv:2112.08696, 2021.</p>
<p>Shin, R., Lin, C. H., Thomson, S., Chen, C., Roy, S., Platanios, E. A., Pauls, A., Klein, D., Eisner, J., and Van Durme, B. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768, 2021.</p>
<p>Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv, abs/2210.09261, 2022.</p>
<p>Takang, A. A., Grubb, P. A., and Macredie, R. D. The effects of comments and identifier names on program comprehensibility: an experimental investigation. J. Prog. Lang., 4(3):143-167, 1996.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-Augmented Ensembles in Language Models. arXiv preprints arXiv:2207.00747, 2022a.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned Language Models are Zero-shot Learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Wu, Y., Jiang, A. Q., Li, W., Rabe, M. N., Staats, C., Jamnik, M., and Szegedy, C. Autoformalization with Large Language Models. arXiv preprint arXiv:2205.12615, 2022.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Zhou, D., SchÃ¤rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv preprint arXiv:2205.10625, 2022.</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<h2>Table of Contents</h2>
<p>A Alternative Prompts without Meaningful Variable Names ..... 13
B Additional analysis on Arithmetic Reasoning ..... 13
C Effect of Using Language Models of Code ..... 14
D Analyzing the Effect of Increasing Number of Samples on PAL ..... 14
E Standard Deviations Across Multiple Order of Prompts ..... 17
F PAL Beyond Benchmarks ..... 17
G Closer Look into Token-level Behaviors of Different Mechanisms ..... 20
H Datasets ..... 20
H. 1 Creating GSM-HARD ..... 23
H. 2 GSM-HARD Analysis ..... 23
I Generalization of PAL to Least-to-Most Prompting ..... 24
J Prompts ..... 26
J. 1 Reasoning about Colored Objects ..... 26
J. 2 Penguins in a Table ..... 27
J. 3 Date Understanding ..... 28
J. 4 Math ..... 29
J. 5 Object Counting ..... 31
J. 6 Repeat Copy ..... 32
K Success and Failure Modes in Symbolic Tasks ..... 33
K. 1 Colored Objects ..... 33
K. 2 Penguins in a Table ..... 33
K. 3 Date Understanding ..... 34</p>
<h1>A. Alternative Prompts without Meaningful Variable Names</h1>
<div class="codehilite"><pre><span></span><code>a = 23
b = 5
c = 3
d = b * c
e = a - d
print (e)
</code></pre></div>

<p>(a) Structured explanation with uninformative variable names (PAL - var)</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Olivia has $23</span>
<span class="n">a</span> = <span class="mi">23</span>
<span class="c1"># number of bagels bought</span>
<span class="n">b</span> = <span class="mi">5</span>
<span class="c1"># price of each bagel</span>
<span class="n">c</span> = <span class="mi">3</span>
<span class="c1"># total price of bagels</span>
<span class="nb">d</span> = <span class="n">b</span> * <span class="n">c</span>
<span class="c1"># money left</span>
<span class="nb">e</span> = <span class="n">a</span> - <span class="nb">d</span>
<span class="nb">print</span> (<span class="nb">e</span>)
</code></pre></div>

<p>(b) Structured explanation with uninformative variable names, but useful comments (PAL - var + comms)
money_initial $=23$
bagels $=5$
bagel_cost $=3$
money_spent = bagels * bagel_cost
money_left = money_initial - money_spent
result = money_left
print(result)
(c) PAL prompts</p>
<p>Figure 10: Role of text in PAL: three different reasoning steps for the question Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left? Uninformative variable names (left), Uninformative variable names with useful comments (left), and PAL. Including text description</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">CoT</th>
<th style="text-align: left;">PAL - var</th>
<th style="text-align: left;">PAL - var + comms</th>
<th style="text-align: left;">PAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Solve Rate</td>
<td style="text-align: left;">63.1</td>
<td style="text-align: left;">59.0</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">71.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Role of text: including text either as informative variable names (PAL) or comments is important (PAL - var + comms). Uninformative variable names PAL - var cause a drastic drop in performance, indicating that just structure is not sufficient. The corresponding prompts are shown in Figure 10.</p>
<p>For mathematical problems, since our standard prompts do not use much comment, we start by creating alternative prompts where the informative variable names are replaced with single-letters (Figure 10). The results in Table 4 shows a considerable performance drop: from an average of $71.8 \%$ to $59 \%$. Note that the ablation where structured outputs are completely removed in favor of purely text explanations is precisely the CoT setting, which achieves a solve rate of $63 \%$. These results underscore the importance of text but more importantly show that combining both text and procedural statements leads to higher performance gains-either is sub-optimal.</p>
<h2>B. Additional analysis on Arithmetic Reasoning</h2>
<p>GSM-hard with hard prompts The GSM-HARD experiments used prompts that were sampled from the GSM8K training set. Will COT be helped by using larger numbers in the prompts as well? To investigate this, we create prompts where the numbers are changed to larger numbers, matching the distribution of numbers in GSM-HARD. The results in Table 5 shows</p>
<p>that even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the gains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for CoT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Regular Prompt</th>
<th style="text-align: center;">Prompt with Larger Numbers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">23.8</td>
</tr>
</tbody>
</table>
<p>Table 5: GSM-hard results, when the prompts also had examples of larger numbers.</p>
<p>Succinct Code The programs used in few-shot examples by PAL are multi-step, and show a step-by-step breakdown of the reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to calculate the result? Results in Table 6 ( $4^{\text {th }}$ row) shows that is not the case. With single-line expressions, the performance of PAL falls to the level of direct prompting.</p>
<p>Generating the answer directly PAL first generates a reasoning chain in the form of a Python program, and passes the generated program to a runtime to obtain an answer. Is PAL better only because of the program-style intermediate reasoning chains, or are the improvements derived from offloading execution to the Python runtime? To investigate this, we experiment with a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting compels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The results in Table 6 ( $5^{\text {th }}$ row) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while current LLMs can be excellent at specifying a high-level plan to solve a task-they are still incapable of executing them.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ablation</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DIRECT (no intermediate reasoning)</td>
<td style="text-align: center;">19.7</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: left;">PAL</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: left;">Succinct Code</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">LLM Simulating Runtime</td>
<td style="text-align: center;">23.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Solve Rates for Ablations</p>
<h1>C. Effect of Using Language Models of Code</h1>
<p>In our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate whether the additional performance boost observed in our results was due to the use of models like Codex, or whether our formulation was useful even for text-based models. To this end, we conducted additional experiments using text-based language models. Our findings indicate that the PAL approach is not restricted to working solely with Codex, but can also be applied to natural language (NL) models, as long as the model is sufficiently strong. Specifically, our results showed that in the text-davinci-001 model, the use of the CoT approach resulted in better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">PaL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">69.8</td>
</tr>
</tbody>
</table>
<h2>D. Analyzing the Effect of Increasing Number of Samples on PAL</h2>
<p>In Section 5.1, we show that PAL outperforms strong baselines both for a single sample and by drawing 40 samples and using majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40, and the interpolation estimates demonstrate that PAL remains competitive throughout the number of samples.</p>
<div class="codehilite"><pre><span></span><code>def solution():
&quot;&quot;&quot;Shawn has five toys. For Christmas, he got two toys each from his
    ; mom and dad. How many toys does he have now?&quot;&quot;&quot;
toys_initial = 5
mom_toys = 2
dad_toys = 2
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
</code></pre></div>

<p>(a) Original Example
def solution():
return $5+2+2$
(b) Succinct Code
def solution():
"""Shawn has 10312864 toys. For Christmas, he got 13267894 toys each
from his mom and dad. How many toys does he have now?"""
toys_initial = 10312864
mom_toys = 13267894
dad_toys = 13267894
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
(c) Hard Examples in Prompt (PAL)</p>
<div class="codehilite"><pre><span></span><code>Example {
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">question</span><span class="o">=</span><span class="err">&quot;Shawn has 10312864 toys. For Christmas, he got 13267894 toys</span>
<span class="w">    </span><span class="nv">each</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">his</span><span class="w"> </span><span class="nv">mom</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">dad</span>.<span class="w"> </span><span class="nv">How</span><span class="w"> </span><span class="nv">many</span><span class="w"> </span><span class="nv">toys</span><span class="w"> </span><span class="nv">does</span><span class="w"> </span><span class="nv">he</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">now</span>?<span class="err">&quot;,</span>
<span class="nv">thought</span><span class="o">=</span><span class="err">&quot;Shawn started with 10312864 toys. If he got 13267894 toys each</span>
<span class="err">    from his mom and dad, then that is 26535788 more toys. 10312864 +</span>
<span class="w">    </span><span class="mi">26535788</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">36848652</span>.<span class="err">&quot;,</span>
<span class="nv">answer</span><span class="o">=</span><span class="s2">&quot;36848652&quot;</span>,
}
</code></pre></div>

<p>(d) Hard Examples in Prompt (CoT)
def solution():
"""Shawn has five toys. For Christmas, he got two toys each from his
; mom and dad. How many toys does he have now?"""
toys_initial = 5
mom_toys = 2
dad_toys = 2
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys
return result
ans $=9$
(e) Generating Answers Directly</p>
<p>Figure 11: Ablations of the original example solution for the few-shot prompting experiment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 12: Comparison of solve rates between PAL and baselines as the number of samples increases from 1 to 40 . Note that the solve rates for the baselines ( $\mathrm{PaLM}, \mathrm{CoT}$, Minerva) are obtained through logistic interpolation of solve rates at 1 and 40</p>
<h1>E. Standard Deviations Across Multiple Order of Prompts</h1>
<p>For each math reasoning task, we run inference using three random orderings of the prompts. As shown in Table 7, the standard deviation between the results obtained from the three different seeds is minimal.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Standard Deviation</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Standard Deviation</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">ASDIV</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">GSM-HARD</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-SingleEq</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-SingleOp</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-AddSub</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: left;">MAWPS-MultiArith</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 7: Standard deviations for three runs for the math reasoning datasets.</p>
<h2>F. PAL Beyond Benchmarks</h2>
<p>We argue that symbolic reasoning is a crucial component in solving a wide range of tasks. In this section, we demonstrate examples of tasks that may not initially appear to require using programs as intermediate reasoning steps, but can be improved through the use of PAL-style reasoning. We demonstrate these examples using the ChatGPT tool. ${ }^{1}$ In contrast to the in-context-learning methods we used in the main paper, here we instruct ChatGPT to perform program-aided reasoning through one of the user utterances.</p>
<p>In Figure 13, in CoT-style reasoning, while the reasoning chain is correct, the final answer is wrong. In contrast, PAL-style reasoning could not only accurately extract the color of objects from the question but also produce the correct lines of code to branch to different situations that yield their corresponding correct answers.</p>
<p>A more intriguing example is letting an LLM count the number of letters in the word "intriguing". In Figure 14a, while the step-by-step explanation appears reasonable by splitting the letters by spaces, ChatGPT does not change the answer after this explicit reasoning and insists on the wrong answer. Explicitly instructing the model to perform step-by-step reasoning before answering the question still yields the wrong answer. In contrast, PAL-style reasoning only takes a few lines of code, and the execution does produce the correct answer, in this case. These examples indicate that PAL can benefit even an ostensibly powerful model like ChatGPT.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Let's say our plane crashed onto an exotic island, where everything that is usually yellow looks blue here. We know that mango and banana are all yellow, do they have the same color on this island?</p>
<p>If you were on an exotic island where everything that is usually yellow looks blue, then mangoes and bananas would not have the same color.</p>
<p>Mangoes are typically yellow in color, but on this island, they would look blue. Bananas are also typically yellow in color, but on this island, they would also look blue.</p>
<p>Therefore, even though both mangoes and bananas are typically yellow in color, on this island they would have a different color. Mangoes and bananas would both look blue on this island, but they would not have the same color, as blue is a different color from yellow.</p>
<p>I hope this helps! Let me know if you have any other questions.
(a) In CoT style reasoning, the correct intermediate reasoning chain leads to wrong answers.</p>
<p>Let's say our plane crashed onto an exotic island, where everything that is usually yellow looks blue here. We know that mango and banana are all yellow, do they have the same color on this island? Write a Python program to answer the question.
$\langle 3| 3$</p>
<div class="codehilite"><pre><span></span><code>color_of_mango = &quot;blue&quot;
color_of_banana = &quot;blue&quot;
if color_of_mango == color_of_banana:
print(&quot;Yes, mango and banana have the same color on this island.&quot;)
else:
print(&quot;No, mango and banana do not have the same color on this island.&quot;)
</code></pre></div>

<p>(b) In PAL, the execution of the code will produce the correct answer.</p>
<p>Figure 13: ChatGPT with PAL and CoT to answer a user-posted question</p>
<p>How many letters are in the word 'intriguing'?</p>
<p>The word 'intriguing' has 9 letters.</p>
<p>Interesting, can you explain your reasoning step by step?</p>
<p>Sure, here's my reasoning step by step:</p>
<ol>
<li>I read the word 'intriguing'</li>
<li>I counted the number of letters in the word, which are: $i, n, t, r, i, g, u, i, n, g$</li>
<li>I added the number of letters, which is 9 .</li>
</ol>
<p>So, in summary, the word 'intriguing' has 9 letters.
(a) Step-by-step reasoning struggle on counting the number of letters in the word "intriguing" which has ten letters.</p>
<p>How many letters are in the word 'intriguing'? Think step by step</p>
<ol>
<li>Start by counting the letters in the first syllable, "in" - there are 2 letters.</li>
<li>Next, count the letters in the second syllable, "tri" - there are 3 letters.</li>
<li>Then, count the letters in the third syllable, "gue" - there are 3 letters.</li>
<li>Finally, count the letters in the fourth syllable, "ing" - there are 3 letters.</li>
<li>Add up all the letters counted in each syllable: $2+3+3+3=11$ letters.
(b) Explicitly instructing ChatGPT to reason step-by-step before generating answer still leads to the wrong answer.</li>
</ol>
<p>How many letters are in the word intriguing? You're not good at counting stuff, so don't try. Just write a program I can run to get the answer.</p>
<p>Here's a simple program you can use to find out how many letters are in the word "intriguing":</p>
<p>Copy code
word = "intriguing"
2 Use the 'len() function to find the length of the word
num_letters $=$ len(word)
3 Print the result
print(num_letters)</p>
<p>To run this program, you will need to have a Python interpreter installed on your computer. You can then type in the code above and run it to find the answer.
(c) PAL takes a few lines of code and the execution could result in the correct answer.</p>
<p>Figure 14: ChatGPT with PAL and CoT to answer a user-posted question</p>
<h1>G. Closer Look into Token-level Behaviors of Different Mechanisms</h1>
<p>Beyond empirical results, we make initial attempts to gain a deeper understanding of the behavior of LLMs with different reasoning mechanisms by looking into the token-level log-likelihood of reasoning chains produced by COT and PAL. We randomly selected 20 questions from the COLORED OBJECTS dataset, along with their corresponding COT and PAL solutions. We then manually compared the two mechanisms by focusing on tokens with a low log-likelihood.</p>
<p>Our analysis reveals that CoT often has lower confidence in tokens related to numbers and quantitative information, the grounded position of spatial adjectives (e.g., right-most), properties such as the color of objects, and nouns that refer to the objects. Specifically, we found that this occurred in seven, six, two, and six examples out of the 20 we examined. In contrast, PAL uses list manipulations, such as len (objects), and accesses objects and their associated properties through list indexing (e.g., object [3] [0]). We found that the LLM is typically confident in producing these programs. Furthermore, we observed that while COT requires different expressions for the same concept in different contexts, PAL almost always uses the same expression, which is presumably more robust. For example, when there are five objects, COT predicts "the right-most thing is the fifth item on the list", and "the right-most thing is the third item on the list" when the number of objects is three. Occasionally, CoT also predicts "the right-most thing is last item on the list" which does not provide more concrete information. On the contrary, PAL confidently predicts objects [-1] consistently. The more consistent and uniform use of expressions in PAL can be attributed to the explicit and defined nature of programming languages, which allows for clear and accurate expressions.</p>
<h2>H. Datasets</h2>
<p>In the following tables (Table 8, Table 9, Table 10), we presents statistics and examples for the datasets we considered.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reasoning about Colored Objects</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?</td>
</tr>
<tr>
<td style="text-align: center;">Penguins in a Table</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7 , the weight of Gwen is 15 kg , the height of Bernard is 80 cm . We now add a penguin to the table: James, 12, 90, 12 How many penguins are less than 8 years old?</td>
</tr>
<tr>
<td style="text-align: center;">Date Understanding</td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?</td>
</tr>
</tbody>
</table>
<p>Table 8: Reasoning datasets about everyday objects and concepts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">N</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Object Counting</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">I have a chair, two potatoes, a cauliflower, a lettuce head, two tables, a <br> cabbage, two onions, and three fridges. How many vegetables do I have?</td>
</tr>
<tr>
<td style="text-align: left;">Repeat Copy</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">Repeat the word duck four times, but halfway through also say quack.</td>
</tr>
</tbody>
</table>
<p>Table 9: Reasoning datasets about algorithmic problems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Olivia has \$23. She bought five bagels for \$3 each. How much money does she have left?</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP (Patel et al., 2021)</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack. How much do you have to pay to buy each pack?</td>
</tr>
<tr>
<td style="text-align: center;">ASDIV (Miao et al., 2020)</td>
<td style="text-align: center;">2096</td>
<td style="text-align: center;">Ellen has six more balls than Marin. Marin has nine balls. How many balls does Ellen have?</td>
</tr>
<tr>
<td style="text-align: center;">SINGLEOP (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">562</td>
<td style="text-align: center;">If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how many bottle caps are in the box?</td>
</tr>
<tr>
<td style="text-align: center;">SINGLEEQ (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27 dollars. How much did each candy bar cost?</td>
</tr>
<tr>
<td style="text-align: center;">ADDSUB (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">There were 6 roses in the vase. Mary cut some roses from her flower garden. There are now 16 roses in the vase. How many roses did she cut?</td>
</tr>
<tr>
<td style="text-align: center;">MULTIARITH (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria end up with?</td>
</tr>
</tbody>
</table>
<p>Table 10: Examples from math reasoning datasets used in this work. Table from Wei et al. (2022). Datasets vary in number of words per question, with 47 words per question in GSM8K, 21 words in SINGLEOP, and $\sim 30$ words in the other datasets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ chat.openai.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
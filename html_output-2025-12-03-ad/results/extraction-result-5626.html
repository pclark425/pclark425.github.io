<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-263909127</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08027v1.pdf" target="_blank">Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection</a></p>
                <p><strong>Paper Abstract:</strong> Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5626",
    "paper_id": "paper-263909127",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00406425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection
12 Oct 2023</p>
<p>Yi Dai 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Hao Lang hao.lang@alibaba-inc.com 
Alibaba Group</p>
<p>Kaisheng Zeng 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Fei Huang f.huang@alibaba-inc.com 
Alibaba Group</p>
<p>Yongbin Li 
Alibaba Group</p>
<p>Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection
12 Oct 20232B4E0CE4AEC670F63DA3785FA7CF5DBCarXiv:2310.08027v1[cs.CL]
Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning.Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes.Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class.Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis.In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs.Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation.We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge.Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.</p>
<p>Introduction</p>
<p>Machine learning models deployed in the wild often encounter out-of-distribution (OOD) samples that are not seen in the training phase (Bendale and Boult, 2015;Fei and Liu, 2016).A reliable model should not only obtain high performance on samples from seen distributions, i.e., in-distribution (ID) samples, but also accurately detect OOD samples for caution (Amodei et al., 2016;Boult et al., 2019;Dai et al., 2023b).Most existing OOD detection methods are built upon single-modal inputs, e.g., visual inputs (Hsu et al., 2020;Liu et al., 2020) or textual inputs (Zhou et al., 2021;Zhan et al., 2021).Recently, Esmaeilpour et al. (2022); Ming et al. (2022a) attempt to tackle multi-modal * Work done while the author was interning at Alibaba.</p>
<p>‚Ä† Equal contribution.‚Ä° Corresponding author.OOD detection problem that explores the semantic information conveyed in class labels for visual OOD detection, relying on large-scale pre-trained vision-language models such as CLIP (Radford et al., 2021).</p>
<p>In this paper, we apply world knowledge from large language models (LLMs) (Petroni et al., 2019a) to multi-modal OOD detection by generating descriptive features for class names (Menon and Vondrick, 2023).As illustrated in Figure 1, to find a black swan, look for its long neck, webbed feet, and black feathers.These descriptors provide rich additional semantic information for ID classes, which can lead to a more robust estimation of OOD uncertainty (Ming et al., 2022a), i.e., measuring the distance from the visual features of an input to the closest textual features of ID classes.</p>
<p>However, the knowledge encoding of LLMs such as GPT-3 (Brown et al., 2020) is lossy (Peng et al., 2023) and tends to hallucinate (Ji et al., 2023), which can cause damage when applied for OOD detection tasks.As shown in Figure 2, LLMs generate unfaithful descriptors for class "hen", assuming a featherless head appearing in a hen.Indiscriminately employing generated descriptive features to model ID classes brings noise to the inference process due to LLMs' hallucinations.Moreover, this issue becomes more severe as OOD detection deals with samples in an unbounded feature space (Shen et al., 2021).Collisions between OOD samples and ID classes with augmented descriptors would be common.</p>
<p>To address the challenge mentioned above, we propose an approach for selective generation of high-quality descriptive features from LLMs, while abstaining from low-quality unfaithful ones (Ren et al., 2022).Recent studies show LLMs can predict the quality of their outputs, i.e., providing calibrated confidence scores for each prediction that accurately reflects the likelihood of the predicted answer being correct (Kadavath et al., 2022;Si et al., 2022a).Unfortunately, descriptors of a class name generated by LLMs are long-form and structured intermediate results for the ultimate OOD detection task, and calibration of LLMs for generating such long open-ended text (Lee et al., 2022) is still in its infancy.</p>
<p>We perform uncertainty calibration in LLMs by exploring a consistency-based approach (Wang et al., 2022).We assume if the same correct prediction is consistent throughout multiple generations, then it could serve as a strong sign that LLMs are confident about the prediction (Si et al., 2022b).Instead of computing literal similarity, we define consistency between multiple outputs from LLMs for a given input based on whether they can retrieve similar items from a fixed set of unlabeled images.Specifically, for each descriptor, we first retrieve a subset of images, leveraging the joint vision-language representations.Then, we measure generation consistency by calculating the overlap between these image subsets.</p>
<p>To further capitalize on the world knowledge expressed in descriptors from LLMs, we employ a general object detector to detect all the candidate objects (concepts) in an image (Cai et al., 2022) and represent them with their predicted class names (Chen et al., 2023b) such as "mirror", "chair", and "sink" (see Figure 5).These visual concepts provide valuable contextual information about an image in the textual space and can potentially match descriptive features of an ID class if the image belongs to that class.Accordingly, we improve our distance metric of input samples from ID classes by considering the similarity between image visual concepts and ID class descriptive features in language representations.Our key contributions are summarized as follows:</p>
<p>‚Ä¢ We apply world knowledge from large language models (LLMs) to multi-modal OOD detection for the first time by generating descriptive features for ID class names.</p>
<p>‚Ä¢ We analyse LLMs' hallucinations which can cause damage to OOD detection.A selective generation framework is introduced and an uncertainty calibration method in LLMs is developed to tackle the hallucination issue.</p>
<p>‚Ä¢ We detect objects in an image and represent them with their predicted class names to further explore world knowledge from LLMs.</p>
<p>Our extensive experimentation on various datasets shows that our method consistently outperforms the state-of-the-art.</p>
<p>Related Work</p>
<p>OOD Detection is widely investigated in vision classification problems (Yang et al., 2021), and also in text classification problems (Lang et al., 2023).Existing approaches try to improve the OOD detection performance by logits-based scores (Hendrycks and Gimpel, 2017a;Liu et al., 2020), distance-based OOD detectors (Lee et al., 2018;Sun et al., 2022), robust representation learning (Winkens et al., 2020;Zhou et al., 2021), and generated pseudo OOD samples (Shu et al., 2021;Lang et al., 2022).Multi-modal OOD detection is recently studied by Fort et al. (2021a); Esmaeilpour et al. (2022); Ming et al. (2022a), which leverages textual information for visual OOD detection.These works do not explore world knowledge from LLMs.</p>
<p>Large language models like GPT3 (Brown et al., 2020) can serve as a knowledge base and help various tasks (Petroni et al., 2019b;Dai et al., 2023a).While some works demonstrate that world knowledge from LLMs can provide substantial aid to vision tasks (Yang et al., 2022) such as vision classification (Menon and Vondrick, 2023), its efficacy in multi-modal OOD detection is currently underexplored.Moreover, as LLMs tend to hallucinate and generate unfaithful facts (Ji et al., 2023), additional effects are needed to explore LLMs effectively.</p>
<p>Uncertainty Calibration provides confidence scores for predictions to safely explore LLMs, helping users decide when to trust LLMs outputs.Recent studies examine calibration of LLMs in multiple-choice and generation QA tasks (Kadavath et al., 2022;Si et al., 2022a;Kuhn et al., 2023) text (Lee et al., 2022) are generated to provide descriptive features for ID classes (Menon and Vondrick, 2023), and calibration in this task is yet underexplored.</p>
<p>3 Background</p>
<p>Problem Setup</p>
<p>We start by formulating the multi-modal OOD detection problem, following Ming et al. (2022a).We denote the input and label space by X and Y, respectively.Y is a set of class labels/names referring to the known ID classes.The goal of OOD detection is to detect samples that do not belong to any of the known classes or assign a test sample to one of the known classes.We formulate the OOD detection as a binary classification problem: G(x; Y, I, T ) : X ‚Üí {0, 1}, where x ‚àà X denotes an input image, I and T are image encoder and text encoder from pre-trained vision-language models (VLMs), respectively.The joint visionlanguage embeddings of VLMs associate objects in visual and textual modalities well.Note that there is no training data of ID samples provided to train the OOD detector.</p>
<p>Analyzing Class Name Descriptors from LLMs</p>
<p>Recent work has demonstrated that class name descriptors, i.e., descriptive features for distinguishing a known object category in a photograph generated by prompting LLMs (see Section 4.2 for more details), can improve zero-shot visual classification performance (Menon and Vondrick, 2023) in a close-world setting (Vapnik, 1991).A natural extension of this work is to leverage the descriptors for OOD detection in an open world (Fei and Liu, 2016), which is largely unexplored.Unfortunately, we find that the descriptors used  in previous approach fail to improve the OOD detection performance in a few datasets.As shown in Table 1, although descriptors can improve the classification performance in all five datasets, they degenerate the OOD detection performance in four ID datasets.We hypothesize this is because LLMs generate unfaithful descriptors due to hallucinations (see cases in Figure 2), which bring noise to the OOD detection process.</p>
<p>To verify our hypothesis, we visualize ID samples from ImageNet-1k dataset and OOD samples from iNaturalist dataset, together with their original class names, based on aligned vision-language features (Radford et al., 2021).As illustrated in Figure 3(b), class names of ID samples may coincide with these of OOD samples, when augmented with descriptors from LLMs.Thus, it is improper to indiscriminately adopt these descriptors.</p>
<p>The above assumptions are also evidenced in
ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ (a) ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ(b)</p>
<p>Descriptor Generation</p>
<p>To apply world knowledge from LLMs for OOD detection, we generate a set of descriptors d(c) for each known class name c by prompting LLMs (see Figure 6), following (Menon and Vondrick, 2023).We randomly select 1 visual category and manually compose descriptors to use as 1-shot incontext example.We prompt LLMs to describe the visual features for distinguishing a category in a photograph.The generated list composes the set d(c). Figure 2 shows cases of generated descriptors, which include shape, size, color, and object parts in natural language.</p>
<p>Uncertainty Calibration</p>
<p>As Figure 2 illustrates, LLMs may generate unfaithful descriptors due to hallucinations, which would hurt the performance of OOD detection if applied indiscriminately.To address this issue, we design a consistency-based (Wang et al., 2022) uncertainty calibration method to estimate a confidence score for each generation, which helps decide when to trust the LLMs outputs.We assume if the same correct prediction is consistent throughout multiple generations, then it shows that LLMs are confident about the prediction (Si et al., 2022b), thus the generation results are more trustworthy.</p>
<p>It is non-trivial to directly extend previous consistency-based methods to our settings.Specifically, we leverage LLMs to generate long-form and structured descriptor lists without a fixed candidate answer set.Any permutation of the descriptor list can convey the same meaning, despite the inconsistency in surface form.Meanwhile, the outputs of LLMs are intermediate results for our OOD detection task.It is challenging to measure quality of these intermediate results while maintaining a tangible connection to the ultimate detection task.</p>
<p>We take inspiration from prior code generation works (Li et al., 2022;Chen et al., 2023a), which prompt LLMs to generate structured codes aiming at solving programming tasks.Multiple codes are sampled from LLMs for one input and then execution results are obtained by executing these codes.The code with the most frequent execution result is selected as the final prediction.In a similar manner, we quantify the characteristics of descriptor sets through their retrieval feedbacks from a fixed set of unlabeled images, and define their consistency according to consensus among retrieval results.Specifically, we propose a three-stage consistencybased uncertainty calibration method.Q: What are useful visual features for distinguishing a lemur in a photo?A: There are several useful visual features to tell there is a lemur in a photo: -furry bodies -long tail -large eyes Q: What are useful visual features for distinguishing a goldfish in a photo?A: There are several useful visual features to tell there is a goldfish in a photo: -long, flowing tail -small, black eyes -a small mouth -dorsal fins Stage II.We cluster descriptor sets D(c) into groups S(c), where each group s ‚àà S(c) is comprised of descriptor sets that consist with each other.We define descriptor consistency, C(‚Ä¢, ‚Ä¢), which retains any two descriptor sets that share the same characteristics through retrieval feedback.Concretely, we retrieve top k images from an unlabeled image set M via a retriever R(‚Ä¢) for each set d ‚àà D(c).The resulting image subset for descriptor set d is denoted as R(d) ‚àà {0, 1} m , where m is the size of M and entry j of the vector is 1 if the j-th image of M is in the retrieved subset.Finally, we assume descriptor consistency C(d, d ‚Ä≤ ) holds if cosine similarity between R(d) and R(d ‚Ä≤ ) is above Œ∑.Note that text similarity between descriptor sets can also be used in consistency computation.
ùëÑ prompt A prompt ùëÑ test A test
Stage III.We compute the confidence score p(c) for descriptor set d(c) as |s * | n , where s * is the largest group in S(c).</p>
<p>Visual Object Detection</p>
<p>To further capitalize on the world knowledge conveyed in generated descriptors, we introduce a general object detector with a vocabulary of 600 object categories to detect visual objects v(x) for each testing image x (Cai et al., 2022).Specifically, v(x) consists of detected objects' class names, such as "mirror", "chair", and "sink" in a photograph of a barber shop (see Figure 5).</p>
<p>OOD Detection</p>
<p>For each ID class name c, descriptor set d(c) is used to augment the representation of c if its confidence score p(c) is above threshold Œ≥, otherwise c is used to represent that class only.Thus, the textual features for class name c are:
t(c) = {g(d)|d ‚àà d(c)}, if p(c) ‚â• Œ≥, {c}, otherwise,
where d is one descriptor in the set d(c) and g(‚Ä¢) transforms d into the form {c} which has {d}.</p>
<p>For an input image x, we calculate the class-wise matching score for each ID class name c ‚àà Y:
sc(x) = E t‚ààt(c) œÉ(I(x), T (t)) + E v‚ààv(x) t‚ààt(c) œÉ(T (v), T (t)),(1)
where œÉ(‚Ä¢, ‚Ä¢) denotes the cosine similarity function, the left term computes the similarity between image visual representations and class name textual representations, and the right term measures the similarity between detected image objects and class names in the text space.</p>
<p>Lastly, we define the maximum class matching score as:
s max (x; Y, I, T ) = max c exp(sc(x))
c ‚Ä≤ ‚ààY exp(s c ‚Ä≤ (x)) , similar to Ming et al. (2022a).Our OOD detection function can be defined as:
G(x; Y, I, T ) = 1 smax(x; Y, I, T ) ‚â• Œª 0 smax(x; Y, I, T ) &lt; Œª ,(2)
where 1 represents ID class and 0 indicates OOD conventionally.Œª is a chosen threshold.</p>
<p>Experiments</p>
<p>Datasets and Metrics</p>
<p>Datasets Following recent works (Ming et al., 2022a), we use large-scale datasets that are more realistic and complex.We consider the following ID datasets: variants of ImageNet (Deng et al., 2009), CUB-200 (Wah et al., 2011), Stanford-Cars (Krause et al., 2013), Food-101 (Bossard et al., 2014), Oxford-Pet (Parkhi et al., 2012).For OOD datasets, we use iNaturalist (Van Horn et al., 2018), SUN (Xiao et al., 2010), Places (Zhou et al., 2017), and Texture (Cimpoi et al., 2014).</p>
<p>Metrics For evaluation, we use these metrics (1) the false positive rate (FPR95) of OOD samples when the true positive rate of ID samples is at 95%, (2) the area under the receiver operating characteristic curve (AUROC).</p>
<p>Implementation Details</p>
<p>In our experiments, we adopt CLIP (Radford et al., 2021) as the pre-trained vision-language model.Specifically, we mainly use CLIP-B/16 (CLIP-B), which consists of a ViT-B/16 Transformer as the image encoder and a masked self-attention Transformer (Vaswani et al., 2017) as the text encoder.We also use CLIP-L/14 (CLIP-L) as a representative of large models.To generate descriptors, we query text-davinci-003 (Ouyang et al., 2022) with sampling temperature T = 0.7 and maximum token length of 100.We construct the unlabeled image set M through the random selection of m = 50000 images from the training set of ImageNet.The retriever R(‚Ä¢) retrieves k = 50 images from M. We set the threshold Œ∑ = 0.9 and Œ≥ = 0.5.In visual object detection, we employ the object detection model CBNetV2-Swin-Base (Cai et al., 2022) as a general object detector with a vocabulary of 600 objects.See more details in Appendix B.</p>
<p>Baselines</p>
<p>We compared our method with competitive baselines: 1. MOS (Huang and Li, 2021) divides ID classes into small groups with similar concepts to improve OOD detection; 2. Fort et al. (Fort et al., 2021b) finetunes a full ViT model pre-trained on the ID dataset; 3. Energy (Liu et al., 2020) proposes a logit-based score to detect OOD samples; 4. MSP (Hendrycks and Gimpel, 2017b) employs the maximum classification probability of samples to estimate OOD uncertainty; 5. MCM (Ming et al., 2022a) estimates OOD uncertainty with the maximum similarity between the embeddings of a sample and ID class names; 6. Menon et al. (Menon and Vondrick, 2023) prompts LLMs to generate descriptors of each class as cues for image classification.We extend it to OOD detection and use the maximum classification probability as a measure of OOD uncertainty (Hendrycks and Gimpel, 2017b).</p>
<p>Main Results</p>
<p>To evaluate the scalability of our method in realworld scenarios, we compare it with recent OOD detection baselines on the ImageNet-1k dataset (ID) in Table 2.It can be seen that our method outperforms all competitive zero-shot methods.Compared with the best-performing zero-shot baseline MCM, it reduces FPR95 by 5.03%.We can also observe that: 1. Indiscriminately employing knowledge from LLMs (i.e., Menon et al.) degenerates the OOD detection performance.This indicates the adverse impact of LLMs' hallucinations and underlines the importance of selective generation from LLMs. 2. Despite being training-free, our method favorably matches or even outperforms some strong task-specific baselines that require training (e.g., MOS).It shows the advantage of incorporating world knowledge from LLMs for OOD detection.</p>
<p>We further evaluate the effectiveness of our method on hard OOD inputs.Specifically, two kinds of hard OOD are considered, i.e., semantically hard OOD (Winkens et al., 2020) and spurious OOD (Ming et al., 2022b).As shown in Table 3, our method exhibits robust OOD detection capability and outperforms all competitive baselines, e.g., improvement of 1.93% in FPR95 compared to the best-performing baseline MCM.We can also observe that zero-shot methods generally obtain higher performance than task-specific baselines.This indicates that exposing a model to a training set may suffer from bias and spurious correlations.We also make comparisons on a larger number of ID and OOD datasets in Appendix A.</p>
<p>Ablation Studies</p>
<p>Model Components Ablation studies are carried out to validate the effectiveness of each main component in our model.Specifically, the following variants are investigated: 1. w/o Obj.removes the visual object detection step, i.e., only the left term in Eq. 1 is adopted.Table 4: Ablation variants of uncertainty calibration and visual object detection.We use the average performance on four OOD datasets with ImageNet-1k as ID.</p>
<p>generated descriptors as the confidence score.2. Self-consistency (Wang et al., 2022) makes multiple predictions for one input and makes use of the frequency of the majority prediction as the confidence score.3. Self-evaluation (Kadavath et al., 2022) asks LLMs to first propose answers and then estimate the probability that these answers are correct.Results in Table 4 show that our uncertainty calibration method performs better than other variants.This further indicates that dedicated uncertainty calibration approaches should be explored to safely explore generations from LLMs.</p>
<p>Visual Object Detection We evaluate the visual object detection module by implementing the following variants: 1. Class Sim.uses class name c instead of the descriptive features t(c) in the right term of Eq. 1. 2. Simple Det.adopts a simple object detection model with a smaller vocabulary of 80 objects (Li et al., 2023).As shown in Table 4, our method outperforms the above variants.Specifically, we can observe that: 1. Calculating the similarity between detected image concept names and ID class names without descriptors degenerates the OOD detection performance.2. Using a general object detection model with a large vocabulary of objects helps to improve the performance.</p>
<p>Further Analysis</p>
<p>Cases of Retrieval Feedback We provide a case study where descriptor sets for the same class are similar/dissimilar in textual form.Figure 7 illustrates that even with low textual similarity and variations in textual form, two descriptor sets can have consistent retrieval feedback if they accurately capture the descriptive features of the same object.</p>
<p>Analysis of Unlabeled Image Set Figure 8 shows the effect of unlabeled image set with varying sizes on the OOD detection performance.We compose image subsets either through random down-sampling from the original unlabeled image set ("Random"), or removing images from certain categories ("Diversity").We can observe that: 1.Our method achieves superior OOD detection performance along with the increase of unlabeled image data.2. Unlabeled image sets that lack diversity achieve limited detection performance, especially in small sizes.</p>
<p>Conclusion</p>
<p>In this paper, we introduce a novel framework for multi-modal out-of-distribution detection.It employs world knowledge from large language models (LLMs) to characterize ID classes.An uncertainty calibration method is introduced to tackle the issue of LLMs' hallucinations, and visual object detection is proposed to fully capitalize on the generated world knowledge.Experiments on a variety of OOD detection tasks show the effectiveness of our method, demonstrating its exciting property where world knowledge can be reliably exploited via LLMs by evaluating their uncertainty.</p>
<p>Limitations</p>
<p>We identify one major limitation of this work is its input modality.Specifically, our method is limited to detecting visual out-of-distribution (OOD) inputs and ignores inputs in other modalities such as textual, audio, electroencephalogram (EEG) and robotic features.These modalities provide valuable information that can be used to construct better OOD detectors.Fortunately, through multi-modal pre-training models (Xu et al., 2021;Huo et al., 2021), we can obtain robust representations in various modalities.</p>
<p>Ethics Statement</p>
<p>B More Implementation Details</p>
<p>In our experiments, we adopt CLIP (Radford et al., 2021) as the pre-trained vision-language model.Specifically, we mainly use CLIP-B/16 (CLIP-B), which consists of a ViT-B/16 Transformer as the image encoder and a masked self-attention Transformer (Vaswani et al., 2017) as the text encoder.We also use CLIP-L/14 (CLIP-L) as a representative of large models.To obtain world knowledge corresponding to each class, we query text-davinci-003 (Ouyang et al., 2022) with a sampling temperature of 0.7 and a maximum token length of 100.</p>
<p>To obtain retrieval feedback for each descriptor set, We construct a fixed set of unlabeled images, denoted as M, through the random selection of m = 50000 images spanning 1000 categories.These images are extracted from the training set of ImageNet-1k without corresponding labels.For descriptor set d, the retriever R(‚Ä¢) retrieves top k similar images from M:
R ‚Ä≤ (d) = argmax M ‚äÇM,|M |=k E x‚ààM d‚ààd œÉ(I(x), T (d)).
From the retrieved image subset R ‚Ä≤ (d) we derive a binary vector R(d), with entry j equal to 1 if the j-th image of M is in R ‚Ä≤ (d).In order to determine whether two descriptor sets, d and d ‚Ä≤ , are consistent with each other, denoted as C(d, d ‚Ä≤ ), we incorporate the following two constraints:
C(d, d ‚Ä≤ ) =1 œÉ(R(d), R(d ‚Ä≤ )) ‚â• Œ∑ ‚àß 1 œÉ( d‚ààd T (d) |d| , d ‚Ä≤ ‚ààd ‚Ä≤ T (d ‚Ä≤ ) |d ‚Ä≤ | ) ‚â• Œ∑ ‚Ä≤ ,(3)
where the first constraint measures the cosine similarity between R(d) and R(d ‚Ä≤ ), the second constraint computes the cosine similarity between the averaged textual embeddings of descriptors in d and d ‚Ä≤ .Note that the second constraint that computes textual similarity is optional in our method.We evaluate its impact by constructing an ablation variant relying solely on the first constraint.Its average performance on four OOD datasets with ImageNet-1k as ID dataset is 38.59 in FPR95 and 91.37 in AUROC, which outperforms the other zero-shot baselines as well.We set k = 50 for image retrieval and Œ∑ = 0.9, Œ∑ ‚Ä≤ = 0.99 for consistency computation.We set Œ≥ = 0.5 as the confidence threshold for p(c).</p>
<p>In visual object detection, we use CBNetV2-Swin-Base from Cai et al. (2022) with a vocabulary of 600 objects as our general object detector.To construct the ablation variant "Simple Det.", we employ YOLOv6-L6 from Li et al. (2023) with a smaller vocabulary of 80 categories.</p>
<p>C Robustness to Sampling Temperature</p>
<p>T .</p>
<p>We vary sampling temperature T for LLM generation among {0.3, 0.5, 0.7, 0.9, 1.1}.It can be seen in Figure 9 that regardless of the temperature, our method consistently outperforms the ablation variant "w/o Know." which does not incorporate additional world knowledge from LLMs.We can also observe that an intermediate temperature of 0.7 can lead to the best performance.</p>
<p>D Reliability under Different LLMs, Image Detectors and OOD Detectors</p>
<p>To further verify the reliability of our method, we perform OOD detection using our method under different LLMs (GPT-4, ChatGPT, Claude-1, Claude-2, Bard and text-davinci-003), image detectors (YOLOv6 (Li et al., 2023), InternImage (Wang et al., 2023), Bigdetection (Cai et al., 2022)</p>
<p>Figure 1 :
1
Figure 1: World knowledge from large language models can facilitate the detection of visual objects.</p>
<p>Figure 2 :
2
Figure 2: Cases of descriptors generated by LLMs.Unfaithful descriptors may appear due to hallucinations.</p>
<p>Figure 3: t-SNE visualization of ID and OOD samples, which are from ImageNet-1k and iNaturalist datasets, respectively.ID/OOD Image represent visual representations of images, and ID/OOD Text are textual representations of their class names, based on CLIP.Each line denotes a pair of vision-language representations for one sample.</p>
<p>Figure 4 :
4
Figure 4: Left: Similarities between ID samples and their class names; Right: Maximum similarities between OOD samples and ID classes.ID samples from mageNet-1K and OOD samples from Naturalist.</p>
<p>Figure 5 :
5
Figure 5: Our multi-modal OOD detection framework.For each image x and ID classes Y, 1. Generate a descriptor set for each class c ‚àà Y by prompting LLMs; 2. Estimate a confidence score p(c) for each descriptor set; 3. Detect objects in x and represent them with object names; 4. Compute the maximum class matching score s max (x).</p>
<p>Figure 6 :
6
Figure 6: Example prompt for generating descriptors of the category goldfish.The trailing '-' guides LLMs to generate text in the form of a bulleted list.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Case study on descriptor sets and corresponding retrieval feedbacks.</p>
<p>Figure 9 :
9
Figure 9: Effect of the sampling temperature T for LLM generation.</p>
<p>Table 1 :
1
Effect of class name descriptors from LLMs in classification and OOD detection tasks.CLIP and CLIP+Desp.are VLMs based methods without and with descriptors.Classification is evaluated by accuracy and OOD detection is evaluated by FPR95 (averaged on iNaturalist, SUN, Places, Texture OOD datasets).
.</p>
<p>Table 2 :
2
Ming et al. (2022a)rmance for ImageNet-1k as ID.The performances of all task-specific baselines come fromMing et al. (2022a).
MethodiNaturalistSUNOOD Dataset PlacesTextureAverageFPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚ÜëTask-specific (training required)MOS (BiT)9.2898.1540.6392.0149.5489.0660.4381.2339.9790.11Fort et al.(ViT-B)15.0796.6454.1286.3757.9985.2453.3284.7745.1288.25Fort et al.(ViT-L)15.7496.5152.3487.3255.1486.4851.3885.5443.6588.96Energy (CLIP-B)21.5995.9934.2893.1536.6491.8251.1888.0935.9292.26Energy (CLIP-L)10.6297.5230.4693.8332.2593.0144.3589.6429.4293.50MSP (CLIP-B)40.8988.6365.8181.2467.9080.1464.9678.1659.8982.04MSP (CLIP-L)34.5492.6261.1883.6859.8684.1059.2782.3153.7185.68Zero-shot (no training required)MCM (CLIP-B)30.9194.6137.5992.5744.6989.7757.7786.1142.7490.77MCM (CLIP-L)28.3894.9529.0094.1435.4292.0059.8884.8838.1791.49Menon et al. (CLIP-B) 41.2392.0944.1291.7249.7488.9160.8985.0748.9989.45Menon et al. (CLIP-L) 33.2693.9230.2994.1837.3091.7859.8284.4040.1791.07w/o Obj. (CLIP-B)23.6795.4037.1992.5743.9789.7756.9786.3340.4591.02w/o Obj. (CLIP-L)28.2095.2227.8194.4433.2291.5656.3786.0536.4091.82w/o Calib. (CLIP-B)48.3090.5341.1792.1145.0889.6156.9187.0147.8789.81w/o Calib. (CLIP-L)40.4192.0929.9094.0035.9991.0852.9387.1739.8191.09w/o Know. (CLIP-B)30.1994.8137.3991.5643.6389.7657.3086.1742.1390.58w/o Know. (CLIP-L)28.6694.8833.2593.7040.0091.3159.0985.4140.2691.33Ours (CLIP-B)22.8895.5434.2992.6041.6389.8752.0287.7137.7191.43Ours (CLIP-L)26.4795.1026.3594.5633.1391.7751.7787.4534.4392.22MethodID OODImageNet-10 ImageNet-20ImageNet-20 ImageNet-10Waterbirds Spurious OODAverageFPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚ÜëTask-specific (training required)MOS (BiT)24.6095.3041.8092.6378.2178.6048.2088.84Fort et al. (ViT-B)8.1498.0711.7198.084.6398.578.1698.24Energy (CLIP-B)15.2396.8715.2096.9041.5189.3023.9894.36MSP (CLIP-B)9.3898.3112.5197.7039.5790.9920.4995.67Zero-shot (no training required)MCM (CLIP-B)5.0098.3112.9198.095.8798.367.9398.25Menon et al. (CLIP-B)5.8098.6513.0998.085.5798.458.1598.39w/o Obj. (CLIP-B)4.7098.7110.7898.254.8898.466.7998.47w/o Calib. (CLIP-B)6.0098.5011.1498.045.1798.497.4498.34w/o Know. (CLIP-B)5.0098.7311.3898.224.8698.397.0898.45Ours (CLIP-B)4.2098.779.2498.264.5698.626.0098.55</p>
<p>Table 3 :
3
Performance comparison on hard OOD detection tasks.</p>
<p>Table 5 :
5
Zero-shot OOD detection performance based on CLIP-B/16 with various ID datasets.
MethodiNaturalistSUNOOD Dataset PlacesTextureAverageFPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚ÜëMCM30.9194.6137.5992.5744.6989.7757.7786.1142.7490.77Ours(GPT-4)22.5695.5233.4992.8140.8790.5351.8987.6637.2091.63Ours(ChatGPT)21.9795.6733.6892.8841.5090.1450.9688.0937.0391.70DifferentOurs(Claude-1)24.9295.6334.9792.6042.7289.7254.1386.9439.1991.22LLMsOurs(Claude-2)25.8395.0834.7192.6541.6590.8653.4487.0238.9191.40Ours(Bard)25.7495.0734.1792.7242.1289.8553.4887.0238.8891.17Ours(text-davinci-003)22.8895.5434.2992.6041.6389.8752.0287.7137.7191.43Different Image DetectorsOurs(InternImage) Ours(Co-DETR) Ours(YOLOv6) Ours(Bigdetection)23.41 22.10 23.72 22.8895.45 95.69 95.42 95.5435.10 33.48 34.29 34.2992.63 92.78 92.65 92.6042.13 42.92 43.26 41.6389.90 89.80 89.84 89.8753.65 51.77 56.32 52.0287.30 87.78 86.82 87.7138.57 37.57 39.40 37.7191.32 91.51 91.18 91.43DifferentCLIP-based(w/o softmax) 61.6689.3164.3987.4363.6785.9586.6171.6869.0883.59OOD DetectorsOurs(w/o softmax)59.8789.6561.7987.9060.2086.2778.6772.8465.1384.17</p>
<p>Table 6 :
6
Zero-shot OOD detection performance using different LLMs, image detectors and OOD detectors.</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man√©, arXiv:1606.06565Concrete problems in ai safety. 2016arXiv preprint</p>
<p>Towards open world recognition. Abhijit Bendale, Terrance Boult, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Food-101-mining discriminative components with random forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringer2014. September 6-12, 2014Proceedings, Part VI 13</p>
<p>Learning and the unknown: Surveying steps toward open world recognition. Terrance E Boult, Steve Cruz, Raj Akshay, Manuel Dhamija, James Gunther, Walter J Henrydoss, Scheirer, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Bigdetection: A large-scale benchmark for improved object detector pre-training. Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, Xiangyang Xue, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, Denny Zhou, arXiv:2304.051282023aarXiv preprint</p>
<p>See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, Chuang Gan, arXiv:2301.052262023barXiv preprint</p>
<p>Describing textures in the wild. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2014</p>
<p>Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Yongbin Li, arXiv:2305.06557Long-tailed question answering in an open world. 2023aarXiv preprint</p>
<p>Yi Dai, Hao Lang, Yinhe Zheng, Bowen Yu, Fei Huang, Yongbin Li, arXiv:2305.06555Domain incremental lifelong learning in an open world. 2023barXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Zero-shot out-of-distribution detection based on the pre-trained model clip. Sepideh Esmaeilpour, Bing Liu, Eric Robertson, Lei Shu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202236</p>
<p>Breaking the closed world assumption in text classification. Geli Fei, Bing Liu, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2016</p>
<p>Exploring the limits of out-of-distribution detection. Stanislav Fort, Jie Ren, Balaji Lakshminarayanan, Advances in Neural Information Processing Systems. Curran Associates, Inc2021a34</p>
<p>Exploring the limits of out-of-distribution detection. Stanislav Fort, Jie Ren, Balaji Lakshminarayanan, Conference on Neural Information Processing Systems (NeurIPS). 2021b</p>
<p>A baseline for detecting misclassified and out-of-distribution examples in neural networks. Dan Hendrycks, Kevin Gimpel, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2017a</p>
<p>A baseline for detecting misclassified and out-of-distribution examples in neural networks. Dan Hendrycks, Kevin Gimpel, In International Conference on Learning Representations. 2017bICLR</p>
<p>Generalized odin: Detecting out-ofdistribution image without learning from out-ofdistribution data. Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Mos: Towards scaling out-of-distribution detection for large semantic space. Rui Huang, Yixuan Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2021</p>
<p>Wenlan: Bridging vision and language by large-scale multi-modal pre-training. Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Dan Yang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen, CoRR, abs/2103.065612021</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, Proceedings of the IEEE international conference on computer vision workshops. the IEEE international conference on computer vision workshops2013</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, arXiv:2302.096642023arXiv preprint</p>
<p>A survey on outof-distribution detection in nlp. Hao Lang, Yinhe Zheng, Yixuan Li, Jian Sun, Fei Huang, Yongbin Li, arXiv:2305.032362023arXiv preprint</p>
<p>Estimating soft labels for out-of-domain intent detection. Hao Lang, Yinhe Zheng, Jian Sun, Fei Huang, Luo Si, Yongbin Li, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>A simple unified framework for detecting outof-distribution samples and adversarial attacks. Advances in neural information processing systems. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, 201831</p>
<p>Factuality enhanced language models for open-ended text generation. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, Bryan Catanzaro, Advances in Neural Information Processing Systems. 202235</p>
<p>Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, Xiangxiang Chu, arXiv:2301.05586Yolov6 v3. 0: A full-scale reloading. 2023arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Energy-based out-of-distribution detection. Weitang Liu, Xiaoyun Wang, John Owens, Yixuan Li, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2020</p>
<p>Visual classification via description from large language models. Sachit Menon, Carl Vondrick, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Delving into out-ofdistribution detection with vision-language representations. Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, Yixuan Li, arXiv:2211.134452022aarXiv preprint</p>
<p>On the impact of spurious correlation for out-of-distribution detection. Yifei Ming, Hang Yin, Yixuan Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022b36</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Cats and dogs. Andrea Omkar M Parkhi, Andrew Vedaldi, Zisserman, Jawahar, 2012 IEEE conference on computer vision and pattern recognition. IEEE2012</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, arXiv:2302.128132023arXiv preprint</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019a</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019b</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Out-of-distribution detection and selective generation for conditional language models. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, Peter J Liu, arXiv:2209.155582022arXiv preprint</p>
<p>Enhancing the generalization for intent classification and out-of-domain detection in slu. Yilin Shen, Yen-Chang Hsu, Avik Ray, Hongxia Jin, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Odist: Open world classification via distributionally shifted instances. Lei Shu, Yassine Benajiba, Saab Mansour, Yi Zhang, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, arXiv:2210.09150Prompting gpt-3 to be reliable. 2022aarXiv preprint</p>
<p>Re-examining calibration: The case of question answering. Chenglei Si, Chen Zhao, Sewon Min, Jordan Boyd-Graber, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022b</p>
<p>Out-of-distribution detection with deep nearest neighbors. Yiyou Sun, Yifei Ming, Xiaojin Zhu, Yixuan Li, International Conference on Machine Learning. PMLR2022</p>
<p>The inaturalist species classification and detection dataset. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Principles of risk minimization for learning theory. Vladimir Vapnik, Advances in neural information processing systems. 19914</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>C Wah, S Branson, P Welinder, P Perona, S Belongie, CNS-TR-2011-001The caltech-ucsd birds-200-2011 dataset. 2011California Institute of TechnologyTechnical Report</p>
<p>Internimage: Exploring large-scale vision foundation models with deformable convolutions. Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Contrastive training for improved out-of-distribution detection. Jim Winkens, Rudy Bunel, Guha Abhijit, Robert Roy, Vivek Stanforth, Natarajan, Patricia Joseph R Ledsam, Pushmeet Macwilliams, Alan Kohli, Simon Karthikesalingam, Kohl, arXiv:2007.055662020arXiv preprint</p>
<p>Sun database: Large-scale scene recognition from abbey to zoo. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba, 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE2010</p>
<p>LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou, 10.18653/v1/2021.acl-long.201Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Generalized out-of-distribution detection: A survey. Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu, arXiv:2110.113342021arXiv preprint</p>
<p>An empirical study of gpt-3 for few-shot knowledgebased vqa. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Out-of-scope intent detection with self-supervision and discriminative training. Li-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-Ming Wu, Albert Ys Lam, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Places: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE transactions on pattern analysis and machine intelligence. 201740</p>
<p>Contrastive out-of-distribution detection for pretrained transformers. Wenxuan Zhou, Fangyu Liu, Muhao Chen, arXiv:2104.088122021arXiv preprint</p>
<p>Detrs with collaborative hybrid assignments training. Zhuofan Zong, Guanglu Song, Yu Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>ImageNet-10, ImageNet-20, ImageNet-100. The results are shown in Table 5, demonstrating that our method offers superior performance on various multi-modal OOD detection tasks without training. Wah, Ming et al.2011. 2013. 2014. 2012. 2022a101A More Results We use an extra collection of ID datasets to showcase the versatility of our method: CUB-200</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7898 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7898</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7898</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-6aceefb7b260a4797986a5f42bfd474904af3124</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6aceefb7b260a4797986a5f42bfd474904af3124" target="_blank">Goal Driven Discovery of Distributional Differences via Language Descriptions</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A new task, D5, that automatically discovers differences between two large corpora in a goal-driven way is formulated, confirming that language models can use the goals to propose more relevant, novel, and significant candidate discoveries.</p>
                <p><strong>Paper Abstract:</strong> Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal"$\textit{comparing the side effects of drug A and drug B}$"and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A"$\textit{mention feelings of paranoia}$"more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goals to propose more relevant, novel, and significant candidate discoveries. Finally, our system produces discoveries previously unknown to the authors on a wide range of applications in OpenD5, including temporal and demographic differences in discussion topics, political stances and stereotypes in speech, insights in commercial reviews, and error patterns in NLP models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7898.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7898.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal Driven Discovery of Distributional Differences via Language Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM-based system that (1) proposes natural-language hypotheses describing distributional differences between two corpora conditioned on a user-specified exploration goal, and (2) validates those hypotheses with an LLM-based validator (approximating human judgments) to output statistically supported discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Goal Driven Discovery of Distributional Differences via Language Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>D5 (goal‑conditioned proposer + validator pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses large language models to propose candidate natural‑language predicates describing how two text corpora differ (conditioned on an explicit exploration goal), then approximates human judgments with a separate LM validator to estimate validity scores (V') and filter hypotheses into validated discoveries; includes a self‑supervised loop to fine‑tune proposers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Paired text corpora (corpus A vs. corpus B) with an explicit natural-language exploration goal; exploration/validation splits</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language predicates/hypotheses (truth predicates) describing distributional differences; validated discoveries with estimated effect size and p-value</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Few-shot and instruction prompting (concatenate example samples + natural-language goal + instruction); multiple prompts across sampled subsets to produce many hypotheses; use of a separate prompting template for validation (Flan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003, gpt-3.5-turbo, gpt-4 for proposer; Flan-T5-xxl for validator; Claude-v1.3 for automatic semantic-equivalence judgements in SYND5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>SYND5 (synthetic diagnostic benchmark), OPENd5 (675 open-ended problems aggregated across domains, 4.4M samples total); various sourced corpora (reviews, speeches, lyrics, NLP model outputs) for OPENd5</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Validity V (human-annotated predicate truth differences between corpora), approximated validity V' (LM-based), p-value from one-sided t-test, human relevance rating (2/1/0), SYND5 automatic accuracy via semantic-equivalence judged by Claude-v1.3, novelty/significance human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using goal-conditioning and a validator substantially improves performance: e.g., on SYND5 gpt-4 proposer + validator + goal achieved ≈27% exact-equivalence accuracy (higher when counting similarity); goal-conditioned proposers produced hypotheses rated more relevant (authors: avg relevance 1.68 vs 1.20). Self-supervised fine-tuning on mini-problems improved V' and modestly improved true V in a proof-of-concept.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Produces correlational discoveries (not causal); validator approximations (V' imperfect, Spearman ~0.71 with human V reported for some discoveries); potential to reinforce biases or generate spurious correlations; limited diversity incentives; some discoveries require domain expertise to interpret; computational/api costs noted.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7898.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D5-Proposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D5 Hypothesis Proposer (LLM prompting component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that feeds sampled examples from each corpus plus the user-specified goal to an LLM (GPT family or Flan-T5) to generate many candidate natural-language hypotheses describing distributional differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Goal Driven Discovery of Distributional Differences via Language Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Hypothesis Proposer (goal-conditioned prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs prompts by concatenating several random samples from each corpus with the exploration goal and asks an LLM to output a list of hypotheses; repeat across many sampled prompts to accumulate a candidate set (H_init).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Small sampled subsets of text from corpus A and corpus B plus natural-language exploration goal</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>List of candidate natural-language predicates/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Few-shot/instruction prompting, multiple disjoint prompts over sampled splits, goal-conditioning vs no-goal ablation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003, gpt-3.5-turbo, gpt-4; also experimented with Flan-T5-xxl for fine-tuning proposer in self-supervised proof-of-concept</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Used across SYND5 and OPENd5 problems (sampling exploration split)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Relevance ratings by authors/Turkers/LLMs; downstream validity after validation V'/V</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Goal-conditioned proposer produces substantially more relevant, novel, and significant hypotheses (authors' avg relevance 1.68 vs 1.20); higher recovery on SYND5 when combined with validator.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proposals can repeat similar/paraphrased discoveries; quality depends on model capability and prompt sampling; constrained by prompt context window.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7898.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D5-Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D5 Hypothesis Validator (LM-based approximator of human judgements)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM-based validation component that approximates human predicate truth judgements T(h,x) to compute an estimated validity V' for candidate hypotheses and filter them using statistical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Goal Driven Discovery of Distributional Differences via Language Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LM-based Hypothesis Validator (Flan-T5 + fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses Flan-T5 to answer whether a text sample satisfies a candidate predicate; Flan-T5 is fine-tuned on additional Turker annotations to better simulate human judgements; computes V' as mean difference across sampled validation splits and applies t-test to filter hypotheses with p' threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Candidate predicate h and text samples x from exploration/validation splits</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Estimated truth probability T'(h,x) per sample, aggregated V' and p' per hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Direct instruction prompting for binary/graded truth judgements; Flan-T5 fine-tuned on Turker data</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-xxl (fine-tuned validator); human Turker annotations used to calibrate</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Turker-annotated (T(h,x)) pairs collected for validator fine-tuning; applied on OPENd5 and SYND5 samples</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>V' (LM-estimated validity), p' (t-test), correlation with human V (Spearman/Pearson reported for example discoveries: Spearman ≈0.71, Pearson ≈0.66 over selected discoveries)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Validator enabled substantial improvements in SYND5 accuracy (ablation shows validator + goal + strong proposer yields best results); V' gives informative but imperfect signals correlated with human V.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>V' is an approximation and can be unreliable for many predicates; requires Turker data to fine-tune; subject to evaluator biases and domain knowledge limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7898.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D5-SelfTrain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Supervised Proposer Fine‑Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised algorithm that automatically creates prompt–completion pairs by sampling problems, generating multiple hypotheses with an initial proposer LM, ranking them by the LM-based validator V', and using the top hypothesis as the supervised target to fine-tune the proposer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Goal Driven Discovery of Distributional Differences via Language Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Self-supervised proposer fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generates synthetic training pairs by (a) sampling a problem and creating a proposer prompt, (b) sampling multiple hypotheses from the current proposer, (c) approximating V' with validator and selecting the highest scoring hypothesis as the 'label', then fine-tuning the proposer model on these prompt–completion pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Repository of D5 problems (prompter inputs) without human reference solutions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Fine-tuned proposer LM that outputs higher-validity candidate hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Generate-and-rank using LM proposer + LM validator; self-labeling</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Proof-of-concept used Flan-T5-xxl for fine-tuning (text-davinci-003/gpt variants could not be fine-tuned due to API limits)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>OPENd5 problems (used as unlabeled training problems) and small 'mini-problems' for proof-of-concept</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Change in V' (self-evaluation) and change in true human-evaluated V on held-out mini-problems; statistical significance reported (example: V' from 0.22→0.37, V from 0.07→0.10, p=0.02 on proof-of-concept)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Proof-of-concept fine-tuning on mini-problems improved both V' and modestly improved true V; indicates promise for scaling to larger problem repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proof-of-concept only on small tasks with Flan-T5; gap between V' and true V suggests validator improvement needed; fine-tuning large proprietary LMs constrained by API access and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7898.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Induction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Induction: From few examples to natural language task descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method demonstrating that LMs can induce natural-language task descriptions/instructions from few examples, effectively synthesizing task-level structured knowledge from example pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction induction: From few examples to natural language task descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Instruction induction: From few examples to natural language task descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Or Honovich et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Instruction Induction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LMs to infer concise natural-language instructions or task descriptions given input–output exemplars, effectively distilling the implicit rule underlying examples into text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Input–output exemplars (example pairs) usually from datasets/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language task descriptions/instructions or rules</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Few-shot prompting and induction via LM generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7898.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMs-as-Inductive-Reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models as inductive reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work showing LLMs can perform inductive reasoning by inferring rules or generalizations from sets of textual examples, thereby enabling rule/hypothesis generation in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as inductive reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language models as inductive reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yongchao Yang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LM Inductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Demonstrates that LMs can induce natural-language rules (if...then...) and generalizations from training examples, useful for generating hypotheses and explaining patterns in data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Sets of textual examples/data points</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language rules/hypotheses (induced generalizations)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Prompting with examples; extraction of inferred rules</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7898.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interpretable Autoprompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explaining patterns in data with language models via interpretable autoprompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses LMs to generate interpretable prompts or rules (autoprompts) that explain patterns in datasets, thereby producing human-readable explanations of discriminative features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining patterns in data with language models via interpretable autoprompting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Explaining patterns in data with language models via interpretable autoprompting.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Chandan Singh et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Interpretable Autoprompting</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Employs LMs to search or synthesize short natural-language prompts (autoprompts) that capture discriminative patterns between classes or datasets, producing interpretable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Labeled datasets or class-separated examples</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interpretable natural-language prompts/rules that explain dataset patterns</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Autoprompt search and LM-generated explanations</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7898.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7898.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhong2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Describing differences between text distributions with natural language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses LMs to describe corpus-level differences without conditioning on an explicit user goal; used as a baseline for the present D5 system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Describing differences between text distributions with natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Describing differences between text distributions with natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruiqi Zhong, Charlie Snell, Dan Klein, Jacob Steinhardt</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Distributional Difference Description (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generates natural-language predicates describing differences between corpora using LMs, but without conditioning on explicit exploration goals; acts as a comparison point for goal-conditioned D5.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Paired text corpora</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language predicates describing corpus differences</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LM prompting over samples from corpora (no goal-conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goal Driven Discovery of Distributional Differences via Language Descriptions', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Instruction induction: From few examples to natural language task descriptions. <em>(Rating: 2)</em></li>
                <li>Language models as inductive reasoners. <em>(Rating: 2)</em></li>
                <li>Explaining patterns in data with language models via interpretable autoprompting. <em>(Rating: 2)</em></li>
                <li>Describing differences between text distributions with natural language. <em>(Rating: 2)</em></li>
                <li>Guess the instruction! making language models stronger zero-shot learners. <em>(Rating: 1)</em></li>
                <li>Large language models are human-level prompt engineers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7898",
    "paper_id": "paper-6aceefb7b260a4797986a5f42bfd474904af3124",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "D5",
            "name_full": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
            "brief_description": "A two-stage LLM-based system that (1) proposes natural-language hypotheses describing distributional differences between two corpora conditioned on a user-specified exploration goal, and (2) validates those hypotheses with an LLM-based validator (approximating human judgments) to output statistically supported discoveries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
            "authors": "Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt",
            "year": 2023,
            "method_name": "D5 (goal‑conditioned proposer + validator pipeline)",
            "method_description": "Uses large language models to propose candidate natural‑language predicates describing how two text corpora differ (conditioned on an explicit exploration goal), then approximates human judgments with a separate LM validator to estimate validity scores (V') and filter hypotheses into validated discoveries; includes a self‑supervised loop to fine‑tune proposers.",
            "input_type": "Paired text corpora (corpus A vs. corpus B) with an explicit natural-language exploration goal; exploration/validation splits",
            "output_type": "Natural-language predicates/hypotheses (truth predicates) describing distributional differences; validated discoveries with estimated effect size and p-value",
            "prompting_technique": "Few-shot and instruction prompting (concatenate example samples + natural-language goal + instruction); multiple prompts across sampled subsets to produce many hypotheses; use of a separate prompting template for validation (Flan-T5)",
            "model_name": "text-davinci-003, gpt-3.5-turbo, gpt-4 for proposer; Flan-T5-xxl for validator; Claude-v1.3 for automatic semantic-equivalence judgements in SYND5",
            "model_size": null,
            "datasets_used": "SYND5 (synthetic diagnostic benchmark), OPENd5 (675 open-ended problems aggregated across domains, 4.4M samples total); various sourced corpora (reviews, speeches, lyrics, NLP model outputs) for OPENd5",
            "evaluation_metric": "Validity V (human-annotated predicate truth differences between corpora), approximated validity V' (LM-based), p-value from one-sided t-test, human relevance rating (2/1/0), SYND5 automatic accuracy via semantic-equivalence judged by Claude-v1.3, novelty/significance human ratings",
            "reported_results": "Using goal-conditioning and a validator substantially improves performance: e.g., on SYND5 gpt-4 proposer + validator + goal achieved ≈27% exact-equivalence accuracy (higher when counting similarity); goal-conditioned proposers produced hypotheses rated more relevant (authors: avg relevance 1.68 vs 1.20). Self-supervised fine-tuning on mini-problems improved V' and modestly improved true V in a proof-of-concept.",
            "limitations": "Produces correlational discoveries (not causal); validator approximations (V' imperfect, Spearman ~0.71 with human V reported for some discoveries); potential to reinforce biases or generate spurious correlations; limited diversity incentives; some discoveries require domain expertise to interpret; computational/api costs noted.",
            "counterpoint": true,
            "uuid": "e7898.0",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "D5-Proposer",
            "name_full": "D5 Hypothesis Proposer (LLM prompting component)",
            "brief_description": "A prompting strategy that feeds sampled examples from each corpus plus the user-specified goal to an LLM (GPT family or Flan-T5) to generate many candidate natural-language hypotheses describing distributional differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
            "authors": "Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt",
            "year": 2023,
            "method_name": "Hypothesis Proposer (goal-conditioned prompting)",
            "method_description": "Constructs prompts by concatenating several random samples from each corpus with the exploration goal and asks an LLM to output a list of hypotheses; repeat across many sampled prompts to accumulate a candidate set (H_init).",
            "input_type": "Small sampled subsets of text from corpus A and corpus B plus natural-language exploration goal",
            "output_type": "List of candidate natural-language predicates/hypotheses",
            "prompting_technique": "Few-shot/instruction prompting, multiple disjoint prompts over sampled splits, goal-conditioning vs no-goal ablation",
            "model_name": "text-davinci-003, gpt-3.5-turbo, gpt-4; also experimented with Flan-T5-xxl for fine-tuning proposer in self-supervised proof-of-concept",
            "model_size": null,
            "datasets_used": "Used across SYND5 and OPENd5 problems (sampling exploration split)",
            "evaluation_metric": "Relevance ratings by authors/Turkers/LLMs; downstream validity after validation V'/V",
            "reported_results": "Goal-conditioned proposer produces substantially more relevant, novel, and significant hypotheses (authors' avg relevance 1.68 vs 1.20); higher recovery on SYND5 when combined with validator.",
            "limitations": "Proposals can repeat similar/paraphrased discoveries; quality depends on model capability and prompt sampling; constrained by prompt context window.",
            "counterpoint": false,
            "uuid": "e7898.1",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "D5-Validator",
            "name_full": "D5 Hypothesis Validator (LM-based approximator of human judgements)",
            "brief_description": "An LM-based validation component that approximates human predicate truth judgements T(h,x) to compute an estimated validity V' for candidate hypotheses and filter them using statistical tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
            "authors": "Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt",
            "year": 2023,
            "method_name": "LM-based Hypothesis Validator (Flan-T5 + fine-tuning)",
            "method_description": "Uses Flan-T5 to answer whether a text sample satisfies a candidate predicate; Flan-T5 is fine-tuned on additional Turker annotations to better simulate human judgements; computes V' as mean difference across sampled validation splits and applies t-test to filter hypotheses with p' threshold.",
            "input_type": "Candidate predicate h and text samples x from exploration/validation splits",
            "output_type": "Estimated truth probability T'(h,x) per sample, aggregated V' and p' per hypothesis",
            "prompting_technique": "Direct instruction prompting for binary/graded truth judgements; Flan-T5 fine-tuned on Turker data",
            "model_name": "Flan-T5-xxl (fine-tuned validator); human Turker annotations used to calibrate",
            "model_size": null,
            "datasets_used": "Turker-annotated (T(h,x)) pairs collected for validator fine-tuning; applied on OPENd5 and SYND5 samples",
            "evaluation_metric": "V' (LM-estimated validity), p' (t-test), correlation with human V (Spearman/Pearson reported for example discoveries: Spearman ≈0.71, Pearson ≈0.66 over selected discoveries)",
            "reported_results": "Validator enabled substantial improvements in SYND5 accuracy (ablation shows validator + goal + strong proposer yields best results); V' gives informative but imperfect signals correlated with human V.",
            "limitations": "V' is an approximation and can be unreliable for many predicates; requires Turker data to fine-tune; subject to evaluator biases and domain knowledge limitations.",
            "counterpoint": true,
            "uuid": "e7898.2",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "D5-SelfTrain",
            "name_full": "Self-Supervised Proposer Fine‑Tuning",
            "brief_description": "A self-supervised algorithm that automatically creates prompt–completion pairs by sampling problems, generating multiple hypotheses with an initial proposer LM, ranking them by the LM-based validator V', and using the top hypothesis as the supervised target to fine-tune the proposer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
            "authors": "Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt",
            "year": 2023,
            "method_name": "Self-supervised proposer fine-tuning",
            "method_description": "Generates synthetic training pairs by (a) sampling a problem and creating a proposer prompt, (b) sampling multiple hypotheses from the current proposer, (c) approximating V' with validator and selecting the highest scoring hypothesis as the 'label', then fine-tuning the proposer model on these prompt–completion pairs.",
            "input_type": "Repository of D5 problems (prompter inputs) without human reference solutions",
            "output_type": "Fine-tuned proposer LM that outputs higher-validity candidate hypotheses",
            "prompting_technique": "Generate-and-rank using LM proposer + LM validator; self-labeling",
            "model_name": "Proof-of-concept used Flan-T5-xxl for fine-tuning (text-davinci-003/gpt variants could not be fine-tuned due to API limits)",
            "model_size": null,
            "datasets_used": "OPENd5 problems (used as unlabeled training problems) and small 'mini-problems' for proof-of-concept",
            "evaluation_metric": "Change in V' (self-evaluation) and change in true human-evaluated V on held-out mini-problems; statistical significance reported (example: V' from 0.22→0.37, V from 0.07→0.10, p=0.02 on proof-of-concept)",
            "reported_results": "Proof-of-concept fine-tuning on mini-problems improved both V' and modestly improved true V; indicates promise for scaling to larger problem repositories.",
            "limitations": "Proof-of-concept only on small tasks with Flan-T5; gap between V' and true V suggests validator improvement needed; fine-tuning large proprietary LMs constrained by API access and cost.",
            "counterpoint": false,
            "uuid": "e7898.3",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Instruction Induction",
            "name_full": "Instruction Induction: From few examples to natural language task descriptions",
            "brief_description": "A method demonstrating that LMs can induce natural-language task descriptions/instructions from few examples, effectively synthesizing task-level structured knowledge from example pairs.",
            "citation_title": "Instruction induction: From few examples to natural language task descriptions.",
            "mention_or_use": "mention",
            "paper_title": "Instruction induction: From few examples to natural language task descriptions.",
            "authors": "Or Honovich et al.",
            "year": 2022,
            "method_name": "Instruction Induction",
            "method_description": "Uses LMs to infer concise natural-language instructions or task descriptions given input–output exemplars, effectively distilling the implicit rule underlying examples into text.",
            "input_type": "Input–output exemplars (example pairs) usually from datasets/tasks",
            "output_type": "Natural-language task descriptions/instructions or rules",
            "prompting_technique": "Few-shot prompting and induction via LM generation",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7898.4",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "LMs-as-Inductive-Reasoners",
            "name_full": "Language models as inductive reasoners",
            "brief_description": "Work showing LLMs can perform inductive reasoning by inferring rules or generalizations from sets of textual examples, thereby enabling rule/hypothesis generation in natural language.",
            "citation_title": "Language models as inductive reasoners.",
            "mention_or_use": "mention",
            "paper_title": "Language models as inductive reasoners.",
            "authors": "Yongchao Yang et al.",
            "year": 2022,
            "method_name": "LM Inductive Reasoning",
            "method_description": "Demonstrates that LMs can induce natural-language rules (if...then...) and generalizations from training examples, useful for generating hypotheses and explaining patterns in data.",
            "input_type": "Sets of textual examples/data points",
            "output_type": "Natural-language rules/hypotheses (induced generalizations)",
            "prompting_technique": "Prompting with examples; extraction of inferred rules",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7898.5",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Interpretable Autoprompting",
            "name_full": "Explaining patterns in data with language models via interpretable autoprompting",
            "brief_description": "An approach that uses LMs to generate interpretable prompts or rules (autoprompts) that explain patterns in datasets, thereby producing human-readable explanations of discriminative features.",
            "citation_title": "Explaining patterns in data with language models via interpretable autoprompting.",
            "mention_or_use": "mention",
            "paper_title": "Explaining patterns in data with language models via interpretable autoprompting.",
            "authors": "Chandan Singh et al.",
            "year": 2022,
            "method_name": "Interpretable Autoprompting",
            "method_description": "Employs LMs to search or synthesize short natural-language prompts (autoprompts) that capture discriminative patterns between classes or datasets, producing interpretable explanations.",
            "input_type": "Labeled datasets or class-separated examples",
            "output_type": "Interpretable natural-language prompts/rules that explain dataset patterns",
            "prompting_technique": "Autoprompt search and LM-generated explanations",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7898.6",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Zhong2022",
            "name_full": "Describing differences between text distributions with natural language",
            "brief_description": "Prior work that uses LMs to describe corpus-level differences without conditioning on an explicit user goal; used as a baseline for the present D5 system.",
            "citation_title": "Describing differences between text distributions with natural language.",
            "mention_or_use": "mention",
            "paper_title": "Describing differences between text distributions with natural language.",
            "authors": "Ruiqi Zhong, Charlie Snell, Dan Klein, Jacob Steinhardt",
            "year": 2022,
            "method_name": "Distributional Difference Description (baseline)",
            "method_description": "Generates natural-language predicates describing differences between corpora using LMs, but without conditioning on explicit exploration goals; acts as a comparison point for goal-conditioned D5.",
            "input_type": "Paired text corpora",
            "output_type": "Natural-language predicates describing corpus differences",
            "prompting_technique": "LM prompting over samples from corpora (no goal-conditioning)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7898.7",
            "source_info": {
                "paper_title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Instruction induction: From few examples to natural language task descriptions.",
            "rating": 2
        },
        {
            "paper_title": "Language models as inductive reasoners.",
            "rating": 2
        },
        {
            "paper_title": "Explaining patterns in data with language models via interpretable autoprompting.",
            "rating": 2
        },
        {
            "paper_title": "Describing differences between text distributions with natural language.",
            "rating": 2
        },
        {
            "paper_title": "Guess the instruction! making language models stronger zero-shot learners.",
            "rating": 1
        },
        {
            "paper_title": "Large language models are human-level prompt engineers.",
            "rating": 1
        }
    ],
    "cost": 0.01747925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Goal Driven Discovery of Distributional Differences via Language Descriptions</h1>
<p>Ruiqi Zhong, ${ }^{*}$ Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</p>
<h4>Abstract</h4>
<p>Exploring large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a user-specified exploration goal ("comparing the side effects of drug $A$ and drug $B$ ") and a corpus pair (collections of patients' self-reported reactions after taking each drug). The output is a goal-relevant description (discovery) of how these corpora differ (patients taking drug A "mention feelings of paranoia" more often). We build a D5 system, and to quantitatively evaluate its performance, we 1) build a diagnostic benchmark, SYND5, to test whether it can recover known differences between two synthetic corpora, and 2) contribute a meta-dataset, OPEND5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health. With both synthetic and real datasets, we confirm that language models can leverage user-specified goals to propose more relevant candidate discoveries, and they sometimes produce discoveries previously unknown to the authors, including demographic differences in discussion topics, political stances in speech, insights in commercial reviews, and error patterns in NLP models. Finally, we discuss the limitations of our D5 system, which discovers correlation rather than causation and potentially reinforces biases when misused; therefore, practitioners should treat the outputs of our system with caution.</p>
<h2>1 Introduction</h2>
<p>Exploring large corpora and generating discoveries from them can be ad hoc and laborious. For example, to compare the side effects of drug A and drug B , doctors might inspect two large corpora of patients' self-reported reactions after taking each drug; based on ad hoc insights, they hypothesize that patients taking drug A more often "mentions feelings of paranoia", and then validate this hypothesis by laboriously inspecting the two corpora. Since machines can automatically process a large amount of texts, we might hope for ML systems to facilitate exploratory analyses like the one above.</p>
<p>However, an ML task requires a unified input-output space and evaluation metric so that it can be automated, benchmarked, learned, and analyzed. To this end, we formalize one type of exploratory analysis problem as a natural language generation task: goal driven discovery of differences between text distributions via language descriptions (D5). As shown in Figure 1, the input to the D5 task is a "problem" comprising a description of a user-specified exploration goal (understanding side effects) and a corpus pair (text samples from the distributions of self-reported reactions after taking each drug). The output is a "discovery" represented as a natural language predicate ("mentions feelings of paranoia"). We evaluate a discovery with two criteria (Section 3): (1) validity: it should describe a true difference (Zhong et al., 2022); and (2) relevance to the goal (McGarry, 2005).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Each problem in OPENd5 contains 1) a corpus pair, which has $\sim 17 \mathrm{~K}$ samples on average and is partitioned into two halves called "exploration split" and "validation split", and 2) a natural language description of the exploration goal, which also contains information about how the corpus pair was collected. A D5 system takes the goal and the exploration split as inputs and generates valid and relevant discoveries in natural language as outputs. The underlined texts in the exploration goal vary across problems, while the rest are templates.</p>
<p>Since D5 is open-ended and aims at discovering unknowns, the most popular benchmark practicecomparing system-generated outputs with human-written references on a test set-is infeasible. We therefore design two evaluation strategies.</p>
<ul>
<li>Diagnostic: we synthesized a dataset of D5 problems with known solutions, SYND5, to diagnose whether a D5 system can recover known differences between two synthetic corpora. This strategy is cheap and automated but might not reflect user utility in real applications.</li>
<li>Open-ended: we collected a dataset, OPENd5, by aggregating 675 open-ended D5 problems ranging across business, social sciences, humanities, health, and machine learning (Figure 2), comprising 4.4 million text samples in total across problem corpora. We then manually evaluated a subset of the output discoveries. This strategy is subjective and expensive, but useful for obtaining qualitative insights on more realistic applications.
These two strategies allow us to quantitatively evaluate and compare D5 systems. For example, we compared 1) the system from Zhong et al. (2022) designed to describe corpus-level differences without goals, and 2) a goal-conditioned variant that we develop in Section 4. We found language models successfully use the specified goal: the goal-conditioned variant is correct $12 \%$ more often on SYND5, and it produces relevant candidate discoveries $31 \%$ more often on OPENd5.
We envision OPENd5 to be a growing, diverse repository of open-ended D5 problems. They will not only help us evaluate D5 systems more reliably, but also allow the following operations:
Facilitate exploratory analysis. Every time we build a better D5 system, we can apply it to a repository of open problems and send the discoveries to researchers who posed them. We show this paradigm is plausible by using our system to automatically produce useful discoveries on OPENd5 (Section 6.1), including insights from commercial reviews, temporal and demographic differences in discussion topics, political stances and stereotypes in speeches, differences in lyric styles, and error patterns in NLP systems. We anticipate future systems to produce more discoveries.
Analyze the limitations of our evaluation. Using concrete examples from OPENd5, we show that our current evaluation metrics do not encourage diverse findings, do not always produce causal conclusions, and cannot evaluate discoveries involving heavy expert knowledge (Section 6.2). More D5 problems can help us identify more limitations, which inform areas for future improvement.
Train better D5 systems. Like other ML tasks, we can train a system once we have a dataset. We describe a self-supervised learning algorithm that uses a repository of problems (without reference solutions) to train LMs to propose more valid hypotheses (Section 4.3). As a proof-of-concept, we show that it can make LMs better describe the differences between small groups of text samples.</li>
</ul>
<p>To conclude, we show that D5 can be quantitatively evaluated, automated, analyzed, and learned. Like other ML tasks, it would benefit from a more diverse, authentic, and larger dataset. We hope future works can gather feedback from domain experts and curate an ever-larger dataset of D5 problems, thus accelerating exploratory analyses and facilitating scientific discoveries. ${ }^{2}$</p>
<h1>2 Datasets: SYND5 and OPENd5</h1>
<p>We first introduce how each input problem is formatted. Then we discuss 1) how we synthesized SyND5, which is used for automatic diagnostic evaluation, and 2) how we collected OPENd5, which is used to investigate the practical value of D5 systems in open-ended applications.</p>
<h3>2.1 Task Format</h3>
<p>Each D5 problem is represented by a corpus pair (Corpus A/B) and a description of the exploration goal. For example, Corpus A/B might be self-reported reactions after taking drug A/B, and the goal description would be "comparing the side effects of drug $A$ and drug $B$ ". The desired output is valid and relevant discoveries in the form of natural language predicates (Figure 1), e.g. Corpus A has more samples that "mentions feelings of paranoia".</p>
<h3>2.2 SyND5, a Diagnostic Benchmark with Reference Solutions and an Automatic Metric</h3>
<p>To automatically diagnose a D5 system, we synthesized SyND5, a dataset of D5 problem with reference solutions. To synthesize Corpus A and Corpus B for each input problem, we used a language model (LM) to generate two corpora that simultaneously differ on two dimensions, one of which is goal-relevant and one of which is a distractor. For instance, suppose the goal is to "understand how Corpus A differs from Corpus B in terms of topic". Then we would synthesize an example where Corpus A is more sports-related while B is more art-related (goal-relevant: varying topic), while additionally Corpus A is in English while B is in French (distractor: varying language). The reference solution is the difference on the goal-relevant dimension, e.g. "is sports-related".
In more detail, to synthesize an example in SYND5, we first picked one goal-relevant and one distractor dimension from the set {topic, genre, language}, and sampled a value for each corpus and dimension (e.g. Corpus A: {sports, English}; Corpus B: {art, French}). We then synthesized Corpus A/B such that all its samples are in English/French (i.e. completely different on the distractor dimension) while $V$ percent of them are sports-related/art-related, where we varied $V$ from 0.6 to 1 . Since the distractor difference is more salient, SYND5 penalizes D5 systems that ignore the goal and output the incorrect distractor difference "is in English". We synthesized 300 problems in total to create SYND5; see Appendix 9 for a detailed description of the pipeline.
To compute a D5 system's accuracy, we prompted Claude-v1.3 (Bai et al., 2022b) to judge how often the output discovery is semantically equivalent to the reference. We construct the prompt by using 6 pairs of predicates with the labels of "equivalent", "similar", or "irrelevant" as fewshot examples, and ask Claude-v1.3 to judge whether the output discovery and the reference are "equivalent". As a result, we can automatically diagnose a D5 system. See Appendix 12 for the prompt for equivalence judgement and Appendix 10 for two robustness checks, which (a) consider "similar" discoveries to be correct as well, and (b) use other LMs for equivalence judgement.</p>
<h3>2.3 OPENd5, a Realistic Open-Ended Dataset without Reference Solutions</h3>
<p>To evaluate a D5 system's utility under realistic applications, we also gathered OPEND5, a realistic dataset of 675 open-ended D5 problems. These problems range across business, social sciences, humanities, health, and machine learning; see Figure 2 for a few examples. To build OPENd5, two of the authors performed an extensive literature review on problems that could potentially benefit from our system, e.g., reading survey papers (Nguyen et al., 2020) and courses on computational social sciences, and skimming through the ACL proceedings from the past decade and datasets from Kaggle that have an NLP tag; we then annotated the exploration goals, scraped/generated the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Example Datasets</th>
<th style="text-align: center;">How the Corpus Pairs are Generated</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Corpus A</td>
<td style="text-align: center;">Corpus B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">87 Business problems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Commercial Reviews</td>
<td style="text-align: center;">Airline reviews</td>
<td style="text-align: center;">1 -class passenger reviews</td>
<td style="text-align: center;">Economy passenger reviews</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Product Reviews</td>
<td style="text-align: center;">Reviews that give 10 stars</td>
<td style="text-align: center;">Reviews that give 0 star</td>
</tr>
<tr>
<td style="text-align: center;">Finance</td>
<td style="text-align: center;">TC startups</td>
<td style="text-align: center;">Successful startup descriptions</td>
<td style="text-align: center;">Failed startup descriptions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">News Headlines</td>
<td style="text-align: center;">Top headlines when S\&amp;P rises</td>
<td style="text-align: center;">Top headlines when S\&amp;P falls</td>
</tr>
<tr>
<td style="text-align: center;">278 Social Sciences problems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Politics</td>
<td style="text-align: center;">Administration policy</td>
<td style="text-align: center;">Admin policy from Trump</td>
<td style="text-align: center;">Admin policy from Obama</td>
</tr>
<tr>
<td style="text-align: center;">News</td>
<td style="text-align: center;">Reuters headlines</td>
<td style="text-align: center;">Headlines from 2014</td>
<td style="text-align: center;">Headlines from 2015</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Craiglist Negotiations</td>
<td style="text-align: center;">Dialogue from successes</td>
<td style="text-align: center;">Dialogue from failures</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Diplomacy Dialogues</td>
<td style="text-align: center;">Lies</td>
<td style="text-align: center;">Honest statements</td>
</tr>
<tr>
<td style="text-align: center;">Sociology</td>
<td style="text-align: center;">Happy moments</td>
<td style="text-align: center;">Self-reported happy moments from females</td>
<td style="text-align: center;">Self-reported happy moments from males</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rate My Professor</td>
<td style="text-align: center;">Reviews of female lecturers</td>
<td style="text-align: center;">Reviews of male lecturers</td>
</tr>
<tr>
<td style="text-align: center;">169 Humanities problems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Arts</td>
<td style="text-align: center;">Music lyrics</td>
<td style="text-align: center;">Drake rap lyrics</td>
<td style="text-align: center;">Kanye rap lyrics</td>
</tr>
<tr>
<td style="text-align: center;">Education</td>
<td style="text-align: center;">Student essays</td>
<td style="text-align: center;">Essays that received full score</td>
<td style="text-align: center;">Essays with only partial credit</td>
</tr>
<tr>
<td style="text-align: center;">10 Health problems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Health</td>
<td style="text-align: center;">Doctor's note</td>
<td style="text-align: center;">Patients diagnosed with pneumonia</td>
<td style="text-align: center;">Patients not diagnosed with pneumonia</td>
</tr>
<tr>
<td style="text-align: center;">131 Machine Learning problems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Machine Learning</td>
<td style="text-align: center;">NLI - distribution shift</td>
<td style="text-align: center;">Samples from SNLI</td>
<td style="text-align: center;">Samples from MNLI</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QQP - spurious correlation</td>
<td style="text-align: center;">Individual questions with label "paraphrase"</td>
<td style="text-align: center;">Individual questions with label "non-paraphrase"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM's output</td>
<td style="text-align: center;">Generations from one LM</td>
<td style="text-align: center;">Generations from another LM</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Inputs - error analysis</td>
<td style="text-align: center;">Inputs where one model is correct</td>
<td style="text-align: center;">Inputs where one model is wrong</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiText - clustering</td>
<td style="text-align: center;">Samples from one cluster</td>
<td style="text-align: center;">Samples not from a cluster</td>
</tr>
</tbody>
</table>
<p>Figure 2: OpenD5 contains 675 problems. See citations in Appendix 23.
corresponding corpora, and post-processed them over nine months (see complete list of citations in Appendix 23). As shown in Figure 1, each goal describes the original dataset, how the two given corpora are generated, who is using the system, and what property of the two corpora does the user want to understand. Each OpenD5 problem is reviewed by at least two of the authors to reduce grammatical mistakes and ambiguous interpretations of the goal.
Each corpus contains around 17 K text samples on average, and OpenD5 in total comprises 4.4 million distinct text samples. We use $50 \%$ of each corpus as the "exploration" split and $50 \%$ as the "validation" split. The system can only access the exploration split, while the validation split is reserved for the evaluators to validate the discovery. A validation split prevents overfitting the discoveries to the given samples and is analogous to the train-test split in machine learning.
Since we hope to build systems that can tackle challenging open-ended problems, we did not avoid cases where we do not know the ground truth answer. This is different from standard benchmarking practices, where humans can provide a reference solution to evaluate an AI system. However, even though we do not know the ground truth, once a system produces a discovery, we can still evaluate it. We present our evaluation metrics in the next section.</p>
<h1>3 Evaluation Metrics for Open-Ended D5 problems</h1>
<p>For the goal of comparing the side effects of drug A and drug B, how do we evaluate a systemgenerated discovery that Corpus A "mention feelings of paranoia" more often? First, it needs to be valid, such that indeed more samples from Corpus A satisfy this predicate, which can be evaluated (approximately) objectively. Second, it needs to be relevant to the goal of understanding side effects, which depends on the user's subjective judgement. We define validity and relevance below.
Validity. Similar to Zhong et al. (2022), we require an output discovery $h$ to be a truth predicate on a text sample. For example, if $h=$ "mentions about family", then $h$ is true on the string $x_{1}=$ "My daughter loves me" and false on the string $x_{2}=$ "I'm going to school". Define $T(h, x) \in[0,1]$ as "the certainty that $h$ is true on $x$ ", e.g., $T\left(h, x_{1}\right) \approx 1$ and $T\left(h, x_{2}\right) \approx 0$. We approximate $T(h, x)$ by asking three Turkers how certain they are and averaging their responses (see Appendix 11 for details).
Let $\mathcal{D}<em B="B">{A}^{\text {val }}$ and $\mathcal{D}</em>$ denote the validation sets for Corpus A and B . We define the "validity" $V$ as}^{\text {val }</p>
<p>$$
V(h):=\mathbb{E}<em A="A">{x \sim \mathcal{D}</em>}^{\text {val }}}[T(h, x)]-\mathbb{E<em B="B">{x \sim \mathcal{D}</em>[T(h, x)]
$$}^{\text {val }}</p>
<p>Computing $V(h)$ is expensive since it requires human annotations $T(h, x)$ on a set of text samples even to evaluate a single discovery $h$. In practice, we do not have the budget to compute $V(h)$ on the entire validation split; therefore, we approximate this quantity by randomly sampling from Corpus $A$ and Corpus $B$. We use these samples to compute an empirical estimate of $V$, as well as a $p$-value for the null hypothesis that $V \leq 0$ using a one-sided t-test.</p>
<p>Relevance. A discovery may be irrelevant even if $V=1$. For example, if the goal is to understand the writing style differences between higher-scoring essays (Corpus A) and lower-scoring ones (Corpus B), the discovery that Corpus A "achieves higher scores" has high validity score by definition but irrelevant to the goal of understanding stylistic differences.</p>
<p>Therefore, we designed a procedure to evaluate relevance, where human or language model evaluators would score each discovery with (2)/(1)/(0). The evaluators used the rubric below, which illustrates the meaning of each score with the essay example above:</p>
<ul>
<li>(2), relevant; e.g. the discovery "write in first person" is directly related to the writing style.</li>
<li>(1), indirectly relevant; e.g. the discovery "use the word "I"", is not exactly a writing style, but can still inform the relevant underlying principle of "write in first person".</li>
<li>(0), irrelevant; e.g. the discovery "argue for abortion" is unrelated to the writing style.</li>
</ul>
<p>To minimize biases while comparing two systems, the evaluators are blind to which system generates which discoveries.</p>
<p>To conclude, an ideal discovery would have a high $V$ value with a small $p$-value and achieve ratings of (2) in relevance. In the next section, we will build a D5 system that addresses these criteria by first proposing goal-relevant candidate discoveries (hypotheses) and then automatically validate them.
Other metrics. We also explored two other subjective metrics, novelty (how difficult it is to generate the discovery) and significance (how beneficial it is to learn about the discovery). Due to space limit, we present their rubrics and related results in Appendix 14.</p>
<h1>4 Methods: Building a D5 System</h1>
<p>We describe our D5 system, which maps from a corpus pair and an exploration goal to a set of natural language predicates. Our system is inspired by a two-stage model of how humans discover patterns in data: creatively brainstorming hypotheses and then rigorously validating them on the data (Ludwig \&amp; Mullainathan, 2022). Analogously, we first propose hypotheses conditioned on the exploration goal and a subset of samples from the corpus pair (Section 4.1). We then use a language model to approximately compute the validity of each hypothesis, and output the valid ones as the final discoveries (Section 4.2). Our system closely mirrors that of Zhong et al. (2022), except that we leverage the goal to propose more relevant hypotheses. Finally, we present a self-supervised learning algorithm to improve an LM's ability to propose more valid hypotheses (Section 4.3); however, due to API access constraint, we cannot apply it to fine-tune gpt-3, so we provide a proof of concept experiment on Flan-T5 (Chung et al., 2022).</p>
<h3>4.1 Hypothesis Proposer</h3>
<p>We prompt gpt-3 (Ouyang et al., 2022) to propose hypotheses. Denoting the exploration split of Corpus A/B as $\mathcal{D}<em B="B">{A}^{\text {exp }} / \mathcal{D}</em>}^{\text {exp }}$, we construct the prompt by concatenating a few random samples from $\mathcal{D<em B="B">{A}^{\text {exp }}$ and $\mathcal{D}</em>$, the exploration goal, and an instruction to output a list of hypotheses. Figure 3 (left) depicts an example of the resulting prompt, together with a typical language model output.}^{\text {exp }</p>
<p>Since the entire corpus pair might not fit into one prompt, we construct multiple prompts with different sets of samples so that gpt-3 can "see" as many different samples as possible in our pipeline. We continue sampling hypotheses with different prompts until obtaining a set of 60 hypotheses, which we call $H_{\text {init }}$. Appendix 15 includes more details on selecting the sets of samples for different prompts.</p>
<h3>4.2 Hypothesis Validator</h3>
<p>Many hypotheses in $H_{\text {init }}$ have low validity: they are not more often true on $\mathcal{D}<em B="B">{A}$ than on $\mathcal{D}</em>$ to simulate the Turkers'}$ (i.e. $V(h) \leq 0)$. To automatically filter them out, we use a language model $T^{\prime</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: All underlined content in the prompt differs across problems, while the other content in the prompt is templated. Left: proposer prompt. The generated hypotheses are in blue. All content with colored background is excluded for brevity. For the baseline of not using the exploration goal, we removed the "exploration goal" block from the prompt. Right: the validator prompt.</p>
<p>judgement $T$ and hence approximate the validity score $V(h)$ with the function $V^{\prime}(h)$, defined as</p>
<p>$$
V^{\prime}(h):=\mathbb{E}<em A="A">{x \sim \mathcal{D}</em>}^{\text {top }}}\left[T^{\prime}(h, x)\right]-\mathbb{E<em B="B">{x \sim \mathcal{D}</em>(h, x)\right]
$$}^{\text {top }}}\left[T^{\prime</p>
<p>To compute $T^{\prime}$, we ask <strong>Flan-T5</strong> whether $x$ satisfies $h$ with the prompt shown in Figure 3 (right). To better simulate Turker's judgment, we collected additional Turker annotations to fine-tune <strong>FLAN-T5</strong> (see Appendix 16 for details about the data collection process). We then obtain a significance value $p^{\prime}$ by performing a t-test to compare the mean value of $V^{\prime}(h, x)$ on the exploration split of Corpus $A$ to that of Corpus $B$, rule out the hypotheses with $p^{\prime}$ greater than 0.001, and output the remainder as discoveries. Finally, we obtain additional discoveries by repeating the same process but asking our system to propose and validate hypotheses about Corpus $B$ rather than Corpus $A$. Appendix Figure 5 visualizes our entire pipeline and Appendix 8 discusses the computational resources we used.</p>
<h3>4.3 Self-Supervised Learning with Open-Ended Problems: A Proof of Concept</h3>
<p>Since D5 problems are open-ended, future systems could potentially produce discoveries with higher validity scores than any known discovery. Therefore, we design a self-supervised learning algorithm to improve an LM's ability to propose more valid hypotheses, using the principle that it is easier to validate a discovery than to generate one.</p>
<p><strong>Algorithm.</strong> Suppose we are given a set of problems for training and an initial language model $m_0$. Our goal is to automatically generate a set of <em>prompt-completion</em> pairs to fine-tune $m_0$ so that it can propose hypotheses that are more valid. To generate a <em>prompt</em>, we randomly sample a problem and create a proposer prompt following the procedure in Section 4.1. To generate the desired <em>completion</em> given a prompt, we sample multiple hypotheses from $m_{\text{init}}$, approximate their $V^{\prime}$ score on the samples in the proposer prompt with the same language model $m_{\text{init}}$ (Section 4.2), and select the highest scoring hypothesis. Finally, we use the prompt-completion pairs to fine-tune $m_0$.</p>
<p><strong>A Proof of Concept Experiment.</strong> Since we cannot fine-tune <strong>text-davinci-003</strong>, we can only experiment with <strong>Flan-T5-xxl</strong> (Chung et al., 2022), an open-sourced instruction-tuned model that might only work well for easier "mini-problems". As a proof of concept, we tested the above self-supervised learning algorithm on the task of describing groups of four samples, where each group comes from a text cluster.</p>
<p>We computed both the automated "self-evaluation" validity score $V^{\prime}$ and the "true" validity score $V$ according to Turker evaluation for evaluation. After self-training, $V^{\prime}$ improves substantially from 0.22 to 0.37, and the $V$ improves from 0.07 to 0.10, with a $p$-value of 0.02. This result provides preliminary evidence that self-training could be applied to a large set of problems to improve the</p>
<table>
<thead>
<tr>
<th>text-davinci-003</th>
<th>w/ goal</th>
<th>wo/ goal</th>
<th>gpt-4</th>
<th>w/ goal</th>
<th>wo/ goal</th>
</tr>
</thead>
<tbody>
<tr>
<td>w/ validator</td>
<td>12%</td>
<td>2%</td>
<td>w/ validator</td>
<td>27%</td>
<td>15%</td>
</tr>
<tr>
<td>wo/ validator</td>
<td>4%</td>
<td>1%</td>
<td>wo/ validator</td>
<td>8%</td>
<td>5%</td>
</tr>
</tbody>
</table>
<p>Table 1: The accuracy on SYND5 using different proposers, with/without incorporating goals, and with/without using validators. Using the validator, the goals, and gpt-4 leads to better results.</p>
<table>
<thead>
<tr>
<th>Hypothesis Relevance</th>
<th>⑦</th>
<th>⑪</th>
<th>⑪</th>
<th>average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Using the goal</td>
<td>79%</td>
<td>9%</td>
<td>12%</td>
<td>1.68</td>
</tr>
<tr>
<td>Not using the goal</td>
<td>52%</td>
<td>16%</td>
<td>32%</td>
<td>1.20</td>
</tr>
</tbody>
</table>
<p>Table 2: How often the hypotheses proposed by text-davinci-003 are rated by the authors as ⑦/⑪/⑪ in terms of relevance (Section 3). Overall, using the goal significantly increases relevance.</p>
<p>validity of the hypotheses; we expect future validators to simulate human judgments better, hence decreasing the approximated gap of improvement between $V$ and $V^{\prime}$. We discuss more training and evaluation detail in Appendix 20.</p>
<h2>5 Quantitative Evaluation on SYND5 and OPENd5</h2>
<p>We show that both SYND5 and OPENd5 can be used to quantitatively evaluate D5 systems. Since SYND5 is automatic, we used it to compare a broad range of D5 systems and studied the contributions of three different factors: the quality of the proposer model (gpt-4 vs. text-davinci-003), the use of a validator, and the use of a goal. We then further investigated the effect of using goals under realistic applications through human evaluation on OPENd5.</p>
<p>Automatically comparing different variants with SYND5. As mentioned above, we ablated 3 factors, resulting in $2^{3}=8$ variants. We compared 1) using text-davinci-003 vs. gpt-4 as the hypothesis proposer; 2) using the validator to compute $V^{\prime}$ for each hypothesis and outputting the highest-scoring hypothesis, vs. not using the validator and outputting a random hypothesis; and 3) using the goal vs. replacing it with “I want to understand how Corpus A is different from Corpus B.”. We then automatically calculated the accuracy for each variant as described in Section 2.2.</p>
<p>We report the results in Table 1. We find that using the validator and the goals significantly improve the performance, and gpt-4 outperforms text-davinci-003 with goals and the validator ($p&lt;$ 1% under a t-test). We conducted two additional robustness checks in the Appendix 10: (a) using text-davinci-003 instead of Claude-v1.3 to judge predicate equivalence, and (b) considering discoveries semantically similar to the references also to be correct; our conclusions do not change.</p>
<p>Finally, to improve the accessibility of our research, we ran the same experiments using gpt-3.5-turbo and flan-t5-xxl as our proposer, and report the results in Appendix Table 6. To show that our conclusions are general and not only apply to synthetically generated texts, we additionally constructed an extension of SYND5 with human-written texts by adapting the NYT dataset from Wang et al. (2023), where each text sample is a New York Times article with a topic and a location label: the topic dimension has 9 different values (e.g., politics, arts) and the location dimension has 10 different values (French, Italy); we then followed the same procedure described in Section 2.2 to create this extension of SYND5, and report our systems’ performance in Appendix Table 7. In all experiments, using the validator and the goal improves the performance.</p>
<p>Investigating whether using goals improves relevance on OPENd5. We then investigated whether text-davinci-003 can leverage the goals to propose more relevant hypotheses on more realistic applications in OPENd5. We sampled 100 problems from OPENd5 with distinct goals and randomly sampled 2 hypotheses from text-davinci-003 with/without using goals (see Figure 3), resulting in 400 hypotheses to evaluate. Three authors then rated their relevance based on the rubric in Section 3, while being blinded about which hypotheses were generated with the goal. Our main paper focuses on presenting the evaluations performed by ourselves, since crowdworkers might be noisy and untrustworthy (Veselovsky et al., 2023; Suhr et al., 2021).</p>
<p>We report the results in Table 5. Since this evaluation is subjective, the inter-annotator agreement is only moderate (Kappa=0.56); however, we can still robustly conclude that text-davinci-003 can leverage goals to propose hypotheses with higher average relevance rating, since this conclusion can</p>
<p>be independently reproduced by every individual evaluator with $p&lt;10^{-8}$. To make sure that the same conclusion can be robustly reproduced by external non-authors, we also evaluated the relevance of the hypotheses with Amazon Mechanical Turks, gpt-3.5-turbo, Claude-v1.3, and gpt-4. We report the results in Appendix Table 8 and found that our conclusion robustly holds under five different types of evaluators, including expert authors, external crowdworkers, and language models from different companies with different levels of capabilities.</p>
<p>Finally, we conducted similar experiments for the novelty and significance metrics in Appendix 14 and found that they both benefit from using goals as well. In the next section, we present example discoveries on OPEN D5 to qualitatively understand what a D5 system can achieve.</p>
<h1>6 Qualitatively Analyzing Discoveries and Limitations with OPEN D5</h1>
<p>To understand the utility and the limitation of a D5 system, we ran it on OPEN D5, a set of realistic D5 problems, and analyze the output discoveries qualitatively.</p>
<h3>6.1 Producing Discoveries on OPEN D5 and Analyzing Them</h3>
<p>We ran our D5 system on OPEN D5, producing 3296 discoveries in total. However, we do not have enough budget to validate every finding, since estimating $V$ is expensive (Section 3). Therefore, from the remaining 3296 discoveries, we manually selected 21 discoveries that 1) achieve a relevance score of $\bigcirc$, 2) are representative of potential use cases, 3) do not require expert knowledge for Turkers to judge, and 4) are likely to achieve a small $p$-value with fewer than 200 samples from $\mathcal{D}^{\text {val }}$.
We then estimated their validity based on the procedure described in Section 3 by using fewer than 200 samples from the validation split and calculated the $p$-values, which cost us $\sim \$ 1500$ in total on MTurk. Since we are testing multiple discoveries and each of them can be statistically significant merely due to chance, we keep 13 discoveries with $V$ that are significantly non-zero with $p$-value below $7 \%$, a threshold determined by the Benjamini Hochberg's procedure with a false discovery rate of $10 \%$. In other words, $&lt;10 \%$ of the discoveries presented are false discoveries in expectation.
We detail 5 of the 13 discoveries in this section, with the remainder in Appendix 18. For each discovery, we report its automated validity score $V^{\prime}$, the estimated true validity score $V$, and their respective $p$ values in Table 3.
Understanding political stances and stereotypes in speeches. When comparing presidential speeches on immigrants from Obama to those from Trump, the former "argues for a path forward to promote the fair and just treatment of immigrants", while the latter more frequently "refers to illegal immigrants as criminals".
Analyzing errors in NLP systems. We fine-tuned a pair of models on two different natural language inference datasets, (a) MNLI and (b) SNLI. To understand their patterns of errors, we defined Corpus A to be the subset of MNLI where a is right and b is wrong, and Corpus B to be where b is right and a is wrong. We found that the latter more often "has an informal tone, such as slang or colloquial speech". One possible explanation is that MNLI contains more different genres and hence more informal speeches, causing the former model to perform better on these examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Output discovery</th>
<th style="text-align: center;">$V$</th>
<th style="text-align: center;">$p$</th>
<th style="text-align: center;">$V^{\prime}$</th>
<th style="text-align: center;">$p^{\prime}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"argues for a path forward to promote the fair ..."</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">$1.26 \mathrm{e}-04$</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$2.01 \mathrm{e}-73$</td>
</tr>
<tr>
<td style="text-align: left;">"refers to illegal immigrants as criminals"</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">$6.17 \mathrm{e}-03$</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$3.17 \mathrm{e}-38$</td>
</tr>
<tr>
<td style="text-align: left;">"has an informal tone, such as slang or colloqu..."</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">$2.35 \mathrm{e}-03$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">$1.46 \mathrm{e}-35$</td>
</tr>
<tr>
<td style="text-align: left;">"mentions lack of legroom "</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">$1.15 \mathrm{e}-03$</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$1.34 \mathrm{e}-45$</td>
</tr>
<tr>
<td style="text-align: left;">"mentions children or family"</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">$1.00 \mathrm{e}-05$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$8.05 \mathrm{e}-09$</td>
</tr>
</tbody>
</table>
<p>Table 3: A subset of discoveries presented in Section 6.1 and their associated estimated validity score $V$, validity score approximated by a model $V^{\prime}$, and their respective $p$-values $p\left(p^{\prime}\right)$ for the null hypothesis that $V\left(V^{\prime}\right)&lt;0$ under a t-test. We present the full set of 13 discoveries in Table 10.</p>
<p>Analyzing airline customer reviews. We compared the concerns in reviews of the airline Air Canada v.s. its subsidiary, Air Canada Rogue, which is considered a low-price wing of Air Canada. The latter more often "mentions lack of legroom".</p>
<p>Analyzing gender differences in self-reported happy moments. Compared to self-reported happy moments written by males, those by females "mentions children or family" more often. Caution: misinterpreting this correlation as causation could reinforce societal biases (Section 6.2).</p>
<p>Due to space constraints, we list more examples on analyzing distribution shifts, text clusters, lyric styles, and news headlines in Appendix 18 and their associated $V$ and $V^{\prime}$ values in Appendix Table 10. Across these discoveries, the approximated validity score $V^{\prime}$ has a $71 \%$ spearman rank correlation with human rating $V$ ( $66 \%$ for Pearson correlation), thus providing informative yet unreliable signals to practitioners about their validity. We hope that $V^{\prime}$ can better approximate $V$ values in the future as the quality of the validators improve. Finally, future works can collect more open problems, allowing D5 systems to produce more impactful discoveries.</p>
<h1>6.2 Concrete Examples in OPENd5 Inform Limitations of D5 Evaluation</h1>
<p>We discuss limitations of D5 evaluation in this section using concrete examples from OPENd5.
Our metrics do not evaluate diversity. There are often multiple valid and relevant discoveries, and our system ideally should generate all of them. For example, when comparing low-rating and high-rating reviews to understand what stands out to customers, both "mentions the hidden fees and poor customer service at the airport" and "mentions the airline charging extra for carry-on items" could be valid discoveries. Our current evaluation does not reward diverse discoveries, and the current system sometimes repeats a discovery using similar paraphrases, e.g., "mentions the rude and unprofessional attitude of the staff" and "mentions the staff being rude and unhelpful". Future evaluation metrics can take diversity into account.</p>
<p>Interpreting discoveries requires domain experts. We used Turkers' judgment when computing $T(h, x)$ to judge the validity of a discovery. However, many discoveries require expert knowledge to interpret properly. For example, it requires medical training to reliably judge whether a self-reported drug-use experience satisfies "mentions psychedelics, such as LSD and shrooms."</p>
<p>Correlation $\neq$ causation. Our metrics currently do not evaluate whether the discovery is causally related to how the corpus pair was generated. For example, when comparing self-reported happy moments from females and males, even if the former corpus has more samples that "mention children and family", it does not necessarily imply family plays a more important role in inter-personal relations for females; an alternative hypothesis is that females might mention people in general more often than males do, hence leading to the observation that they mention family more often. Spurious correlations could also sneak into our validity evaluation: for example, if the Turkers implicitly associate female activities as family-related Greenwald \&amp; Banaji (1995), then we might falsely make this discovery due to evaluator biases. Future metrics should also consider plausible alternative hypotheses to evaluate causality and control the potential biases from the human evaluators. Additionally, we should treat the discovery from D5 with caution to prevent automating and amplifying societal biases.</p>
<p>We discuss other limitations, such as restricting the discovery to be a single predicate, the biases in authors' qualitative evaluation, and the incomprehensiveness of OPENd5 in Appendix 19.</p>
<h2>7 Related Work and Discussion</h2>
<p>Inductive Reasoning with NLP Models. Recent works show that language models are capable of inductive reasoning under restricted settings, discovering patterns from a set of text data points and describing them with language (Honovich et al., 2022). Yang et al. (2022) use this capability to induce natural language rules with the format of "if ...then ...". Zhou et al. (2022) and Ye et al. (2022) use this capability to improve zero/few-shot accuracy by inferring the most likely instruction using input-output example(s) of the target task. Zhong et al. (2022) and Singh et al. (2022) use this capability to discover patterns in datasets, and we improve by building an automatic benchmark and a dataset of open-ended problems and require the discovery to be relevant.</p>
<p>ML models can also perform inductive reasoning in other modalities, such as vision. Hernandez et al. (2021) describes visual features that activate a neuron; Zhu et al. (2022) describes distribution</p>
<p>shifts between the training distribution and the test distribution for images; and Eyuboglu et al. (2022) describes errors made by vision models. We hope future models can perform inductive reasoning in other modalities, such as sound (Aghajanyan et al., 2023) or physical senses (Thomason et al., 2016).</p>
<p>Exploratory Analysis and Automated Discovery. It is not new to automatically discover patterns by learning from empirical data. To list a few classical methods, linear regression analyzes the effect of each real-valued feature by interpreting the learned weights (Draper \&amp; Smith, 1998); n-gram models can extract discriminative phrases, thus yielding insights about corpus-level differences (Manning \&amp; Schutze, 1999); topic models (Blei et al., 2003) can extract major topical variations across documents, where each topic is represented as a distribution over words; small decision trees can extract interpretable if-then statements (Letham et al., 2015); and an entity embedding model learned on existing relations between entities can predict unseen relations (Socher et al., 2013). In comparison, D5 produces discoveries in the form of natural language predicates, which are interpretable and can express abstract concepts; additionally, it is more directed at the goal, while machine learning classifiers like naïve bayes or linear regression will pick up any discriminative features: Appendix 21 offers a more comprehensive discussion using examples from SYND5. Given the respective strength of D5 and traditional exploratory methods, we envision D5 to serve as a complementary method to traditional methods.</p>
<p>Epistemology. While the process of validating a hypothesis is well-formulated, it is much less wellunderstood how to automatically generate hypotheses and decide what discoveries are meaningful (Shapere, 1964; Heckman \&amp; Singer, 2017). Related works in this area have been sparse, among which McGarry (2005) sketches high-level principles for evaluating knowledge discoveries and Ludwig \&amp; Mullainathan (2022) proposes to crowd-source hypotheses from MTurk workers. We concur with the perspective of Polanyi et al. (2000) that meaningfulness of a hypothesis cannot be explicitly verbalized with simple logic but is dependent on implicit community norms; therefore, the process of proposing hypotheses should be learned from empirical data (e.g. pre-training, self-training, or human feedback) rather than deduced from a priori analysis of concepts (Quine, 1969). We hope contributions from other domains can provide more empirical data on what discoveries are meaningful, hence guiding our system to produce more important discoveries.</p>
<h1>Acknowledgement</h1>
<p>We thank Xiaochuang Han and Sam Bowman for their early discussions on this project. We thank Cathy Chen, Erik Jones, Jessy Lin, Alex Pan, Chenglei Si, Xi Ye, and Tianyi Zhang for their helpful feedback on the paper draft. We thank OpenAI and Anthropic for providing model access.</p>
<h2>References</h2>
<p>The hewlett foundation: Automated essay scoring, 2012. URL https://kaggle.com/ competitions/asap-aes.</p>
<p>The hewlett foundation: Short answer scoring, 2013. URL https://kaggle.com/competitions/ asap-sas.</p>
<p>Ad observer. https://adobserver.org/, 2021. Accessed: 2022-12-30.
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.</p>
<p>Roee Aharoni and Yoav Goldberg. Unsupervised domain clusters in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7747-7763, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.692. URL https://aclanthology.org/2020.acl-main.692.</p>
<p>Mohammad Alali, Shaayan Syed, Mohammed Alsayed, Smit Patel, and Hemanth Bodala. Justice: A benchmark dataset for supreme court's judgment prediction. arXiv preprint arXiv:2112.03414, 2021.</p>
<p>Akari Asai, Sara Evensen, Behzad Golshan, Alon Halevy, Vivian Li, Andrei Lopatenko, Daniela Stepanov, Yoshihiko Suhara, Wang-Chiew Tan, and Yinzhan Xu. Happydb: A corpus of 100,000 crowdsourced happy moments. arXiv preprint arXiv:1801.07746, 2018.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Miriam Barnum and James Lo. Is the npt unraveling? evidence from text analysis of review conference statements. Journal of Peace Research, 57(6):740-751, 2020.</p>
<p>Alexander Baturo, Niheer Dasandi, and Slava J Mikhaylov. Understanding state preferences with text as data: Introducing the un general debate corpus. Research \&amp; Politics, 4(2):2053168017712821, 2017.</p>
<p>Akshay Bhalotia. Yc company scraper. https://github.com/akshaybhalotia/yc_company_ scraper, 2022.</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O’Reilly Media, Inc.", 2009.</p>
<p>David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993-1022, 2003.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. The snli corpus. 2015.</p>
<p>Divy Bramhecha. Poetry Foundation Poems, 2019. URL https://www.kaggle.com/datasets/ tgdivy/poetry-foundation-poems.</p>
<p>Dallas Card, Serina Chang, Chris Becker, Julia Mendelsohn, Rob Voigt, Leah Boustan, Ran Abramitzky, and Dan Jurafsky. Replication code and data for "Computational analysis of 140 years of US political speeches reveals more positive but increasingly polarized framing of immigration" [dataset]. https://github.com/dallascard/us-immigration-speeches/, 2022.</p>
<p>Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in english. arXiv preprint arXiv:1906.02059, 2019.</p>
<p>Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims. In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. URL http://cogcomp.org/papers/CXYCR19.pdf.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.</p>
<p>Norman R Draper and Harry Smith. Applied regression analysis, volume 326. John Wiley \&amp; Sons, 1998.</p>
<p>Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré. Domino: Discovering systematic errors with crossmodal embeddings. arXiv preprint arXiv:2203.14960, 2022.</p>
<p>Yujia Gao, Jinu Jang, and Diyi Yang. Understanding the usage of online media for parenting from infancy to preschool at scale. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-12, 2021.</p>
<p>Anthony G Greenwald and Mahzarin R Banaji. Implicit social cognition: attitudes, self-esteem, and stereotypes. Psychological review, 102(1):4, 1995.</p>
<p>Ivan Habernal and Iryna Gurevych. Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1589-1599, Berlin, Germany, 2016. Association for Computational Linguistics. URL http://www.aclweb. org/anthology/P16-1150.</p>
<p>Kevin Hartman. Advertisement Transcripts from Various Industries, 2019. URL https://tinyurl. com/5w36dwdx.</p>
<p>He He, Derek Chen, Anusha Balakrishnan, and Percy Liang. Decoupling strategy and generation in negotiation dialogues, 2018.</p>
<p>Jibo He. Big Data Set from RateMyProfessor.com for Professors' Teaching Evaluation, 2020. URL https://data.mendeley.com/datasets/fvtfjyvw7d/2.</p>
<p>Samuel He. Goodbye world: using natural language processing to identify suicidal posts, 2021. URL https://github.com/hesamuel/goodbye_world.</p>
<p>James J Heckman and Burton Singer. Abducting economics. American Economic Review, 107(5): 298-302, 2017.</p>
<p>Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2021.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.</p>
<p>Nabil Hossain, John Krumm, and Michael Gamon. " president vows to cut&lt; taxes&gt; hair": Dataset and analysis of creative text editing for humorous headlines. arXiv preprint arXiv:1906.00274, 2019.</p>
<p>Kaggle. TMDB 5000 Movie Dataset, 2018. URL https://www.kaggle.com/datasets/tmdb/ tmdb-movie-metadata.</p>
<p>Rohit Kulkarni. A Million News Headlines, 2018. URL https://doi.org/10.7910/DVN/ SYBGZL.</p>
<p>Rohit Kulkarni. The Examiner - Spam Clickbait Catalog, 2020a. URL https://www.kaggle.com/ datasets/therohk/examine-the-examiner.</p>
<p>Rohit Kulkarni. Urban Dictionary Words And Definitions, 2020b. URL https://www.kaggle. com/datasets/therohk/urban-dictionary-words-dataset.</p>
<p>Rohit Kulkarni. India News Headlines Dataset, 2022. URL https://www.kaggle.com/datasets/ therohk/india-headlines-news-dataset.</p>
<p>Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3), Sep 2015. ISSN 1932-6157. doi: 10.1214/15-aoas848. URL http: //dx.doi.org/10.1214/15-AOAS848.</p>
<p>Derek Lim and Austin R Benson. Expertise and dynamics within crowdsourced musical knowledge curation: A case study of the genius platform. In ICWSM, pp. 373-384, 2021.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. Wanli: Worker and ai collaboration for natural language inference dataset creation, January 2022. URL https://arxiv.org/pdf/ 2201.05955.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Zhi Liu. Reuter_50_50 Data Set, 2011. URL https://archive.ics.uci.edu/ml/datasets/ Reuter_50_50.</p>
<p>Jens Ludwig and Sendhil Mullainathan. Algorithmic behavioral science: Machine learning as a tool for scientific discovery. Chicago Booth Research Paper, (22-15), 2022.</p>
<p>Christopher Manning and Hinrich Schutze. Foundations of statistical natural language processing. MIT press, 1999.</p>
<p>Ken McGarry. A survey of interestingness measures for knowledge discovery. The knowledge engineering review, 20(1):39-61, 2005.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.</p>
<p>Natan Mish. Federal Reserve Governors Speeches 1996 - 2020, 2020. URL https://tinyurl. com/3j2e79a6.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022.</p>
<p>Rishabh Misra and Prahal Arora. Sarcasm detection using hybrid neural network. arXiv preprint arXiv:1908.07414, 2019.</p>
<p>Rishabh Misra and Jigyasa Grover. Sculpting Data for ML: The first act of Machine Learning. 01 2021. ISBN 9798585463570.</p>
<p>Nuno Moniz and Luã€™is Torgo. Multi-source social feedback of online news feeds. CoRR, [Web Link], 2018.</p>
<p>Mickaël Mouillé. Kickstarter Projects, 2017. URL https://www.kaggle.com/datasets/ kemical/kickstarter-projects?select=ks-projects-201612.csv.</p>
<p>Dong Nguyen, Maria Liakata, Simon DeDeo, Jacob Eisenstein, David Mimno, Rebekah Tromble, and Jane Winters. How we do things with words: Analyzing text as social and cultural data. Frontiers in Artificial Intelligence, 3:62, 2020.</p>
<p>Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp. 188-197, 2019.</p>
<p>Elle O’Brien. iterative/aita_dataset: Praw rescrape of entire dataset, February 2020. URL https : //doi.org/10.5281/zenodo.3677563.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Verónica Pérez-Rosas and Rada Mihalcea. Experiments in open domain deception detection. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. $1120-1125,2015$.</p>
<p>Verónica Pérez-Rosas, Mohamed Abouelenien, Rada Mihalcea, and Mihai Burzo. Deception detection using real-life trial data. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, pp. 59-66, 2015.</p>
<p>Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. Automatic detection of fake news. arXiv preprint arXiv:1708.07104, 2017.</p>
<p>Denis Peskov, Benny Cheng, Ahmed Elgohary, Joe Barrow, Cristian Danescu-Niculescu-Mizil, and Jordan Boyd-Graber. It takes two to lie: One to lie and one to listen. In Association for Computational Linguistics, 2020.</p>
<p>John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, and Wlodzislaw Duch. A shared task involving multi-label classification of clinical free text. In Biological, translational, and clinical language processing, pp. 97-104, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/ WO7-1013.</p>
<p>Michael Polanyi, John Ziman, and Steve Fuller. The republic of science: its political and economic theory minerva, i (1)(1962), 54-73. Minerva, 38(1):1-32, 2000.</p>
<p>Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, and Jeffrey Sorensen. Six attributes of unhealthy conversation. arXiv preprint arXiv:2010.07410, 2020.</p>
<p>Demand Progress. Statements of Administration Policy, 2022. URL https: //github.com/unitedstates/statements-of-administration-policy# statements-of-administration-policy.</p>
<p>PromptCloud. U.S. Technology Jobs on Dice.com, 2017. URL https://www.kaggle.com/ datasets/PromptCloudHQ/us-technology-jobs-on-dicecom.</p>
<p>WVO Quine. Naturalistic epistemology. Ontological relativity and other essays, pp. 69-90, 1969.
Quora. Quora Question Pairs, 2017. URL https://www.kaggle.com/c/ quora-question-pairs.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.</p>
<p>Justin Robischon. Wikipedia Movie Plots, 2019. URL https://www.kaggle.com/datasets/ jrobischon/wikipedia-movie-plots.</p>
<p>Allen Roush and Arvind Balaji. DebateSum: A large-scale argument mining and summarization dataset. In Proceedings of the 7th Workshop on Argument Mining, pp. 1-7, Online, December 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020. argmining-1.1.</p>
<p>Dudley Shapere. The structure of scientific revolutions. The Philosophical Review, 73(3):383-394, 1964.</p>
<p>Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv:2210.01848, 2022.</p>
<p>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. Advances in neural information processing systems, 26, 2013.</p>
<p>Alane Suhr, Clara Vania, Nikita Nangia, Maarten Sap, Mark Yatskar, Samuel Bowman, and Yoav Artzi. Crowdsourcing beyond annotation: Case studies in benchmark data collection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pp. 1-6, 2021.</p>
<p>J Sun. Daily News for Stock Market Prediction, 2017. URL https://www.kaggle.com/ datasets/aaron7sun/stocknews.</p>
<p>Jesse Thomason, Jivko Sinapov, Maxwell Svetlik, Peter Stone, and Raymond J Mooney. Learning multi-modal grounded linguistic semantics by playing" i spy". In IJCAI, pp. 3477-3483, 2016.</p>
<p>Andrew Thompson. All the News 1.0, 2019. URL https://components.one/datasets/ all-the-news-articles-dataset.</p>
<p>Elsbeth Turcan and Kathleen McKeown. Dreaddit: A reddit dataset for stress analysis in social media. arXiv preprint arXiv:1911.00133, 2019.</p>
<p>Udacity. Armenian Online Job Postings, 2017. URL https://www.kaggle.com/datasets/ udacity/armenian-online-job-postings.</p>
<p>Veniamin Veselovsky, Manoel Horta Ribeiro, and Robert West. Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks. arXiv preprint arXiv:2306.07899, 2023.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705, 2022.</p>
<p>Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clustering via language descriptions. arXiv preprint arXiv:2305.13749, 2023.</p>
<p>Orion Weller and Kevin Seppi. The rJokes dataset: a large scale humor collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 6136-6141, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https : //aclanthology.org/2020.lrec-1.753.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.</p>
<p>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. arXiv preprint arXiv:2212.10923, 2022.</p>
<p>Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. Guess the instruction! making language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022.</p>
<p>Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In International Conference on Machine Learning, pp. 27099-27116. PMLR, 2022.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.</p>
<p>Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: A framework for explaining distribution shifts in natural language. arXiv preprint arXiv:2206.15007, 2022.</p>
<h1>8 Cost for Running the Experiments</h1>
<p>For each problem, we ran the proposer for 10 times on average; assuming each prompt to be at most 4000 tokens, we spent around $\$ 2.4$ for each problem on OpenAI APIs if we use gpt-4 and text-davinci-003, and the cost would decrease to $\$ 0.8$ if we use gpt-3.5-turbo. Notice that these estimates are computed based on the prices as of 05/14/2023, and we expect the price to further decrease in the future. We ran the Flan-T5 based validator for 2 hours on 180 G A100 GPUs.
The total amount of computational resources spent in this research paper is around $\$ 2,500$ in terms of OpenAI API and 3,000 hours of compute on A100 GPU with 80G memory.</p>
<h2>9 Generation Process of SyND5</h2>
<p>The high-level description is in Section 2.2. Here we discuss the procedure that generated SYND5.
We consider three dimensions of differences: topic, genre, and style. For each, we generated 14/9/7 values, e.g., "celebrity love stories" and "sports team recruiting athletes for the topic attribute, "rap lyrics" and "screen play" for the style attribute, and "French" and "Spanish" for the language attribute. We then used GPT-4 and the Claude API to synthesize 54K text samples, where for each text sample we sampled a topic, genre, and style randomly, e.g. "Write a rap about a sports team recruiting athletes in French". To synthesize a random SYND5 problem, we randomly sampled a distractor dimension (e.g. language) and a target dimension (e.g. topic), and for each dimension we sampled two random values (e.g. English and French for language, sports and art for topic).
For each problem, we sampled 10 texts for corpus $A$ such that all of them satisfy one sampled value for the distractor dimension (e.g. corpus $A$ is entirely in English), and 10 texts for corpus $B$ for to satsify the other distractor dimension (e.g. corpus $B$ is entirely in French). Then we set $V$ fraction of corpus $A$ to satisfy the reference target attribute, e.g. "is sports-related", and $f$ fraction of corpus $B$ to satisfy the other value for the target dimension (e.g. "is art-related"). We chose $V$ uniformly at random from $[0.6,0.8,1]$. Finally, we provide $k$ example hypotheses from the target dimension other than the target dimension values for Corpus A and Corpus B, and we chose $k$ from [0, 2] uniformly at random. We then sampled 300 D5 problems in total from this distribution.</p>
<h2>10 Robustness Checks for Results on SYND5</h2>
<p>Table 4 shows the accuracy of different systems using text-davinci-003 as the judge for semantic equivalence. Table 5 shows the accuracy of different systems if we consider outputs semantically similar to the reference to be correct. Across all setups, we found that the conclusion reached in Section 5 still holds under these robustness checks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">text-davinci-003</th>
<th style="text-align: center;">w/ goal</th>
<th style="text-align: center;">wo/ goal</th>
<th style="text-align: center;">gpt-4</th>
<th style="text-align: center;">w/ goal</th>
<th style="text-align: center;">wo/ goal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">w/ validator</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">w/ validator</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">wo/ validator</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">wo/ validator</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Same Table as 1, except that we use text-davinc-003 instead Claude-v1.3 to judge similarity. Using the validator, the goals, and gpt-4 leads to better results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">text-davinci-003</th>
<th style="text-align: center;">w/ goal</th>
<th style="text-align: center;">wo/ goal</th>
<th style="text-align: center;">gpt-4</th>
<th style="text-align: center;">w/ goal</th>
<th style="text-align: center;">wo/ goal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">w/ validator</td>
<td style="text-align: center;">$46 \%$</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">w/ validator</td>
<td style="text-align: center;">$53 \%$</td>
<td style="text-align: center;">$43 \%$</td>
</tr>
<tr>
<td style="text-align: center;">wo/ validator</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">wo/ validator</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$24 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Same Table as 1, except that we calculate how often the output is similar, rather than equivalent, to the reference. Using the validator, the goals, and gpt-4 leads to better results.</p>
<p>To improve the accessibility of our research, we ran the same experiments with gpt-3.5-turbo and flan-t5-xxl, and report the results in Appendix Table 6. To show that our conclusions are general and not only apply to synthetically generated texts, we additionally constructed an extension of SYND5 with human-written texts by adapting the NYT dataset from Wang et al. (2023), where each text sample is a New York Times article with a topic and a location label: the topic dimension</p>
<p>has 9 different values (e.g., politics, arts) and the location dimension has 10 different values (French, Italy); we then followed the same procedure described in Section 2.2 to create this extension of SYND5, and report our systems' performance in Appendix Table 7. Under all experimental setups, using the validator and the goal improves the performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$\mathrm{w} / \mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{wo} / \mathrm{v}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">flan-t5-xxl</td>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">0.27</td>
<td style="text-align: left;">0.10</td>
<td style="text-align: left;">0.08</td>
<td style="text-align: left;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">0.27</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">0.08</td>
<td style="text-align: left;">0.05</td>
</tr>
</tbody>
</table>
<p>Table 6: Similar to Table 9, we used gpt-3.5-turbo and flan-t5-xxl as the proposer to tackle the SynD5 dataset, and report the performance with/without using the goal (g), and with/without using the vadliator (v). We find that using the goal and the validator significantly improves the performance, and open-sourced models lag significantly behind. Additionally, gpt-4 does not significantly outperform gpt-3.5-turbo.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$\mathrm{w} / \mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{w} / \mathrm{v}$</th>
<th style="text-align: left;">wo/ $\mathrm{g}, \mathrm{wo} / \mathrm{v}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">0.24</td>
<td style="text-align: left;">0.22</td>
<td style="text-align: left;">0.10</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">0.55</td>
<td style="text-align: left;">0.28</td>
<td style="text-align: left;">0.22</td>
<td style="text-align: left;">0.16</td>
</tr>
</tbody>
</table>
<p>Table 7: We created an extension of SynD5 by adapting a dataset of New York Times articles with two dimensions: topic and locations, each with 9 and 10 values. We then used gpt-3.5-turbo and gpt-4 as the proposer, and found the same conclusion: using the goal and a validator improves the performance. Additionally, gpt-4 does not significantly outperform gpt-3.5-turbo.</p>
<h1>11 Computing Turker Judgement</h1>
<p>Scoring. To estimate $T(h, x)$ with Turker's rating, where $h$ is a truth predicate of a text sample $x$, the Turker needs to read $h$ and $x$ and then choose among six options: "Certainly Yes", "Likely Yes", "Neutral", "Likely No", "Certainly No", and "Confusing/Cannot be decided." For each $(h, x)$ pair, we collect responses from three Turkers. To compute the average across them, we collect a list of scores using the following rule: each "Certainly Yes" would receive a score of 1.00, "Likely Yes" 0.75 , "Neutral" 0.50 , "Likely No" 0.25 , "Certainly No" 0.00 , and "Confusing/Cannot be decided." receive two scores of 0.50 . We then take the average over all the scores we collected from the Turkers for one $h$ and $x$. "Confusing/Cannot be decided." receives two scores of 0.50 because we want such a response to drag the average rating towards neutral and it has a larger effect than choosing "Neutral".
Payment. We adjust the payment for each HIT task based on the number of words they need to read. We pay them approximately 0.001 cent per word, and using the conservative estimate that adults read about 200 words per minute, we pay them around $\$ 12$ per hour. We spent in total around $\$ 5 \mathrm{~K}$ on this HIT task.
Qualification. We only recruited Turkers who are located in the U.S. Additionally, we designed qualification test with 8 questions; the questions are designed to be easy to answer as long as they have read our instructions below, and we only accepted turkers who made mistakes on at most one questions.
Annotation Instruction. We show our annotation instruction below. We only show examples of choosing "Certainly Yes", "Certainly No", and "Confusing" to encourage the Turkers not to choose neutral ratings. Additionally, we explicitly tried to address Halo effect - where the text does not satisfy a predicate $h$ but satisfies a predicate $h^{\prime}$ that is highly correlated with $h$. For example, for the text sample $x=$ "Really love the flight!!" does not satisfy the predicate $h=$ "mentions that the breakfast is good on the plane", even though it satisfies a highly correlated predicate $h^{\prime}=$ "likes the flight."</p>
<h3>11.1 Instructions</h3>
<p>Below are the same instructions we have shown you during the qualification. Thanks for visiting this page and refresh your memory about the instruction!</p>
<p>Instruction: In this task, you will check whether a TEXT satisfies a PROPERTY</p>
<h1>Example 1</h1>
<p>Property: mentions a natural scene.
Text: I love the way the sun sets in the evening.</p>
<ul>
<li>A) Certainly Yes.</li>
<li>B) Likely Yes.</li>
<li>C) Neutral.</li>
<li>D) Likely No.</li>
<li>E) Certainly No.</li>
<li>F) Confusing/Cannot be decided.</li>
</ul>
<p>Answer. A. sun set is nature-related; if you feel a bit ambivalent, B is also acceptable.</p>
<h2>Example 2</h2>
<p>Property: writes in a 1st person perspective.
Text: Makima is cute.</p>
<ul>
<li>A) Certainly Yes.</li>
<li>B) Likely Yes.</li>
<li>C) Neutral.</li>
<li>D) Likely No.</li>
<li>E) Certainly No.</li>
<li>F) Confusing/Cannot be decided.</li>
</ul>
<p>Answer. E. This text is undoubtedly written in the 3rd person perspetive, so E.</p>
<h2>Example 3</h2>
<p>Property: is better than group B.
Text: I also need to buy a chair.</p>
<ul>
<li>A) Certainly Yes.</li>
<li>B) Likely Yes.</li>
<li>C) Neutral.</li>
<li>D) Likely No.</li>
<li>E) Certainly No.</li>
<li>F) Confusing/Cannot be decided.</li>
</ul>
<p>Answer. F. It is unclear what the hypothesis mean (e.g., what does group B mean?) and doesn't seem related to the text. So F.</p>
<h2>Example 4</h2>
<p>Property: mentions that the breakfast is good on the airline.
Text: The airline staff was really nice! Enjoyable flight.</p>
<ul>
<li>A) Certainly Yes.</li>
<li>B) Likely Yes.</li>
<li>C) Neutral.</li>
<li>D) Likely No.</li>
<li>E) Certainly No.</li>
<li>F) Confusing/Cannot be decided.</li>
</ul>
<p>Answer. E. Although the text appreciates the flight experience, it DOES NOT mention about the breakfast. So the answer is E.</p>
<h1>Example 5</h1>
<p>Property: appreciates the writing style of the author.
Text: The paper absolutely sucks because its underlying logic is wrong. However, the presentation of the paper is clear and the use of language is really impressive.</p>
<ul>
<li>A) Certainly Yes.</li>
<li>B) Likely Yes.</li>
<li>C) Neutral.</li>
<li>D) Likely No.</li>
<li>E) Certainly No.</li>
<li>F) Confusing/Cannot be decided.</li>
</ul>
<p>Answer. A. Although the text dislikes the paper, it DOES like the writing style. So the answer is A.</p>
<h2>12 Prompt to Judge Predicate Similarity</h2>
<p>We prompt Claude v1.3 (Bai et al., 2022b) to judge whether the predicated predicate is similar to the reference. We consider a response that leads to a"yes" to be correct when we require the discovery to be semantically equivalent to the reference, and consider a response that leads to a "yes" or "related" to be correct when we require the discovery to be semantically similar to the reference.
" Is text_a and text_b similar in meaning? respond with yes, related, or no.
Here are a few examples.
Example 1:
text_a: has a topic of protecting the environment
text_b: has a topic of environmental protection
and sustainability
output: yes
Example 2:
text_a: has a language of German
text_b: has a language of Deutsch
output: yes
Example 3:
text_a: has a topic of the relation between political figures
text_b: has a topic of international diplomacy
output: related
Example 4:
text_a: has a topic of the sports
text_b: has a topic of sports team recruiting new members
output: related
Example 5:
text_a: has a named language of Korean
text_b: uses archaic and poetic diction
output: no
Example 6:
text_a: has a named language of Korean
text_b: has a named language of Japanese
output: no</p>
<p>Target:
text_a: {predicate}
text_b: ${$ reference $}$
output:"</p>
<h1>13 Relevance Rating with External Non-Authors</h1>
<p>To make sure that the conclusion that "using goal in the context can improve hypotheses relevance" can be robustly reproduced by external non-authors, we also evaluated the relevance of the hypotheses with Amazon Mechanical Turks, GPT-3.5-turbo, Claude-v1.3, and GPT-4. We report the results in Table 8 and found that the conclusion still robustly holds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relevance Rater</th>
<th style="text-align: left;">w goal</th>
<th style="text-align: left;">w /o goal</th>
<th style="text-align: left;">$p$-value</th>
<th style="text-align: left;">spearmanr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Authors</td>
<td style="text-align: left;">1.68</td>
<td style="text-align: left;">1.20</td>
<td style="text-align: left;">$1 \times 10^{-10}$</td>
<td style="text-align: left;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Turkers</td>
<td style="text-align: left;">1.56</td>
<td style="text-align: left;">1.44</td>
<td style="text-align: left;">$4 \times 10^{-2}$</td>
<td style="text-align: left;">0.10</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">1.05</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">$5 \times 10^{-2}$</td>
<td style="text-align: left;">0.19</td>
</tr>
<tr>
<td style="text-align: left;">claude-v1.3</td>
<td style="text-align: left;">1.18</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">$2 \times 10^{-3}$</td>
<td style="text-align: left;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">1.49</td>
<td style="text-align: left;">1.12</td>
<td style="text-align: left;">$1 \times 10^{-6}$</td>
<td style="text-align: left;">0.45</td>
</tr>
</tbody>
</table>
<p>Table 8: We rated the relevance in the same way as Table 5. However, in this table we obtained the ratings not from the authors, but from four different evaluator types: Turkers, gpt-3.5-turbo, claude-v1.3 and gpt-4. For each evaluator type, we calculate (1) the average rating of the candidate discovery when goal is (not) present in the proposers' prompt, (2) the $p$-value that the average rating when goal is present is higher under a t-test, and (3) the spearman rank correlation between its rating and the authors' rating. We find that the $p$-value is smaller than 0.05 in all cases, indicating that our conclusion is robust; additionally, more capable models has a higher correlation with the authors.</p>
<h2>14 Meaningfulness: Relevance, Novelty, and Significance</h2>
<p>Not every valid discovery is meaningful. For example, if the goal is to understand the topical differences between news from 2008 (Corpus A) and news from 2007 (Corpus B), the discovery that Corpus A "contains news from 2008" is completely valid by definition but meaningless, since it provides only trivial information and is irrelevant to the goal of understanding topical differences.
McGarry (2005) surveyed a list of desirable properties for discovery, and we condensed them into three submetrics to rate how meaningful a discovery is based on the exploration goal: 1) relevance, 2) novelty, and 3) significance. We evaluate these independently of validity and assume that the discovery is already valid. For example, the discovery that "something can travel faster than light" is meaningful if true, even though it is highly implausible.
We rate each submetric with (1), (1), or (2), where higher is better. We show the evaluation instructions below and present our rating on text-davinci-003 proposed hypotheses.</p>
<h3>14.1 Evaluation Instructions</h3>
<p>Relevance. How relevant the discovery is to the goal. For example, suppose we were a student comparing essays rated as convincing vs. not convincing to figure out what writing style is convincing. Then:</p>
<ul>
<li>The discovery "write in first person" is directly related to the writing style, so we rate it (2).</li>
<li>The discovery "use the word "I"", is not exactly a writing style, but can still inform the relevant underlying principle of "write in first person", so we rate it (1).</li>
<li>The discovery "argue for abortion" does not tell us about the underlying writing style, so we rate it (1).</li>
</ul>
<p>Novelty. The difficulty of generating the discovery, e.g. can we think of the discovery in 5 minutes with the goal but without looking at the corpora? For example, suppose we were an airline manager</p>
<p>trying to find improvements to the flight experience, and we were comparing negative reviews vs. positive reviews. Then:</p>
<ul>
<li>The discovery "contain more negative language" is almost certain for negative reviews, so we rate it ( $)$. .</li>
<li>The discovery "complain about the crew members" is not entirely novel, but is not tautologically true and hence requires confirmation, so we rate it (1).</li>
<li>The discovery "mention a language barrier with the crew members" is specific and hard to think of without looking at the data, so we rate it (2).</li>
</ul>
<p>Note that our evaluation is "blinded to the samples": we still consider a discovery novel as long as it is hard to think of before looking at the corpora, even if it might be easy to think of after looking at the corpora. For example, the physical law that $F=m a$ is easy to observe if we have collected and plotted the data on acceleration, mass, and force; however, it might be difficult to think of before we see any such data, so we consider it novel.</p>
<p>Significance. Given the exploration goal, how beneficial is it to learn the discovery for the first time? For example, suppose we were an Amazon retailer trying to figure out what customers like and dislike about my product based on negative reviews and positive reviews. Then:</p>
<ul>
<li>The discovery "accuses the team pushing out a bad product" is not significant since it cannot direct the retailer to improve the product, so we rate it (1).</li>
<li>The discovery "asks for a more durable product" gives some hints about how to improve the product, but isn't sufficiently helpful on its own, so we rate it (1).</li>
<li>The discovery "says the wrench is missing" can lead to concrete actions for improvement, so we rate it (2).</li>
</ul>
<h1>14.2 Goal Leads to More Meaningful Hypotheses</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">with-goal</th>
<th style="text-align: center;">no-goal</th>
<th style="text-align: center;">kappa</th>
<th style="text-align: center;">spearmanr</th>
<th style="text-align: center;">$p$ of avg</th>
<th style="text-align: center;">worst $p$ of ind</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Relevance</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">$1 \times 10^{-10}$</td>
<td style="text-align: center;">$1 \times 10^{-8}$</td>
</tr>
<tr>
<td style="text-align: left;">Novelty</td>
<td style="text-align: center;">1.24</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$5 \times 10^{-6}$</td>
<td style="text-align: center;">$4 \times 10^{-2}$</td>
</tr>
<tr>
<td style="text-align: left;">Significance</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$2 \times 10^{-10}$</td>
<td style="text-align: center;">$2 \times 10^{-7}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Left. For each metric, we report the average rating on hypotheses generated with or without using the exploration goal, and find that the former performs better. Middle. The inter-annotator agreement rate averaged across pairs of author evaluators, measured by Kappa and Spearman rank coefficient; we find substantial correlations between evaluators across all these subjective metrics, with relevance $&gt;$ significance $&gt;$ novelty. Right. We compute the $p$-values for the null hypothesis that "with-goal and no-goal result in the same performance". The $p$ of avg column reports the $p$-values after we average the ratings from all evaluators, while the "worst $p$ of ind" column takes the max of all $p$-values based on ratings of individual evaluators. Overall, the conclusions are statistically significant and they can be robustly reproduced across individual evaluators.</p>
<p>Compared to Zhong et al. (2022), we added the exploration goal to our prompt when generating hypotheses. Does this improve the quality of the proposed hypotheses? To investigate this, we sampled 100 problems from OPEND5 with distinct exploration goals and randomly sampled 2 hypotheses from GPT-3 with and without using exploration goal (see Figure 3), resulting in 400 hypotheses to evaluate. Three authors then rated their meaningfulness based on the three metrics defined in Section 3, while being blinded about which hypotheses were generated with the exploration goal.</p>
<p>The results are shown in Table 9. We found that, when prompted with the exploration goal, GPT-3 on average proposes more relevant, novel, and significant hypotheses; additionally, it proposes hypotheses with ratings higher than (1) $31 \% / 21 \% / 28 \%$ more often in terms of relevance/novelty/significance. Since this is a subjective evaluation, the Kappa inter-annotator agreement is only moderate, ranging from 0.37 to 0.56 . However, we can still robustly conclude that the model can propose more meaningful hypotheses when conditioned on the goal: we calculate the $p$-values for the null hypothesis that with-goal and no-goal have equal performance, and we find $p$-values to be highly significant and robust across evaluators, for all three submetrics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Our code is released at https://github.com/ruiqi-zhong/D5 and our code to download OPEND5 is released at https://github.com/petezh/OpenD5. Given the limitations of our system, practitioners should interpret its outputs with caution and not use it to fully automate scientific discoveries.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4905 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4905</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4905</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3" target="_blank">Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper considers an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals.</p>
                <p><strong>Paper Abstract:</strong> Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4905.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4905.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GFlan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Flan-T5 (GFlan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent using FLAN-T5 (encoder-decoder) as a policy that is online-finetuned with PPO in a text-only BabyAI-Text environment; the model conditions on a prompt containing the goal, the current observation, and a short history of previous observations and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GFlan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>FLAN-T5 (780M) used as an action policy: for each candidate textual action the model computes the conditional token likelihood under the prompt, these log-likelihoods are softmax-normalized to form a policy; a value head (MLP) is attached to the decoder representation and PPO is used to finetune the whole model online in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Flan-T5 (780M)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>BabyAI-Text (textual adaptation of BabyAI / MiniGrid)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Language-conditioned navigation and manipulation tasks (Go to <object>, Pick up <object>, Put <A> next to <B>, temporal sequence tasks like Pick up A then go to B, Unlock door etc.) in a procedurally generated 1-room grid with distractors; tasks are described in natural language and observations are textual descriptions with distances.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Short-term history encoded in the LLM prompt / context window (sliding-window prompt memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text: the prompt explicitly contains the 3 previous observations and the last 2 actions (textual sentences describing distances, objects, and past actions).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updated every step by constructing the prompt with the latest observation and including the previous 3 observations / 2 actions (i.e., a sliding-window update at each timestep).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit retrieval via the LLM's attention over the full prompt / context (the past observations/actions are concatenated into the prompt so the model attends over them when computing token probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>High sample efficiency and final performance when used with the short-term prompt history: reaches ~0.8 mean success rate after ~250k environment steps and ~0.9 after ~600k in multi-task training (Figure 2); in a Go To task trained for 400k steps GFlan-T5 achieved 0.82 ± 0.02 success (Table 1); outperforms baselines (DRRN, NPAE-Flan-T5, Symbolic-PPO) which remain <0.2 after 1.5M steps or ~0.4 for Symbolic-PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No explicit ablation removing the short-term prompt history is reported in the main text; the authors compare GFlan-T5 to multiple baselines (zero-shot Flan-T5, DRRN, non-pretrained Flan-T5 variants) and perform ablations on LLM size, action-space size, and number of distractors. They report GFlan-T5 is robust to increased action-space size (3→9) and to more distractors (4→16) while still using the same short-term history prompt. Appendix contains additional ablations (LLM size, action-space, distractors) but no direct memory-on vs memory-off experiment is described in the main paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Memory is implemented as prompt-context (sliding short history), which uses the LLM context window and increases prompt length each step; computing per-action likelihoods (a forward pass per action sequence) is computationally expensive and scales poorly with action-space size and model size, prompting a distributed scoring solution (Lamorel). The paper notes limitations: experiments limited to textual environment, computational inefficiency when scaling action space and LLM size, and failure modes such as vocabulary overfitting (synonym substitution causes large drops) and poor generalization when many grounded symbols change at once (e.g., new language).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Include a short, task-relevant short-term history in the prompt (they used last 3 observations + last 2 actions) to satisfy short-term memory requirements of tasks; prefer online RL finetuning (PPO) to achieve functional grounding rather than only Behavioral Cloning; use distributed scoring/training (Lamorel) to mitigate computational cost of per-action likelihood scoring; be cautious about scaling (action-space and model size) due to inference/training cost and possible overfitting to vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4905.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4905.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (zero-shot, unfine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pre-trained FLAN-T5 model (before online PPO finetuning) used as a zero-shot policy baseline by prompting; it receives the same prompt (including short-term history) but is not grounded via environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Flan-T5 (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained FLAN-T5 used without PPO finetuning as a policy: probability of candidate actions computed from language-model conditional likelihoods under the prompt; otherwise identical inference pipeline but no RL updates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>BabyAI-Text</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same language-conditioned navigation and manipulation tasks (multi-task mix) as used for GFlan-T5 evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Short-term history in prompt / context window (same prompt format: 3 previous observations + 2 previous actions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text (previous observations and actions) inside the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Prompt updated each step (sliding window), same as GFlan-T5, but model parameters are not updated online.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit via model attention over the prompt; no external retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Low zero-shot performance: much worse than GFlan-T5; examples include very low success on composed/new tasks (e.g., on a new composition task Flan-T5 scored ~0.07 success vs GFlan-T5 0.12) and generally poor sample-efficiency compared with the PPO-finetuned GFlan-T5 (Flan-T5 performs near-random or low success in many tests). Exact per-task numbers for zero-shot are reported in Figure 4 and Figure 6 (Flan-T5 per-task performance much lower than GFlan-T5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper uses Flan-T5 as a zero-shot baseline to show the effect of online RL grounding: comparisons highlight that finetuning via PPO (GFlan-T5) drastically improves performance. No explicit ablation isolating the prompt-history memory effect in the zero-shot model is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Zero-shot FLAN-T5 lacks functional grounding and thus fails at environment-specific dynamics despite having linguistic priors; reliance on prompt history alone without grounding yields poor performance on the interactive tasks and weak generalization to new tasks or language changes.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use online RL (PPO) to ground pretrained LLMs in interactive environments rather than relying on zero-shot prompting alone; include short-term history in prompts to provide necessary immediate context, but expect zero-shot behavior to be insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4905.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4905.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network (DRRN) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ~1M-parameter RL agent architecture commonly used for text-based games that uses recurrent layers to handle partial observability and encodes observations and actions for value/policy estimation; used here as a non-LLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A text-based RL agent (implementation from Wang et al. 2022) with recurrent layers to process sequences of observations; acts on textual observations and candidate textual actions by encoding and scoring them.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>BabyAI-Text</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-task language-conditioned navigation/manipulation tasks as other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent hidden-state memory (RNN/LSTM-style) used to capture short-term temporal context across timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Distributed hidden-state vectors (learned recurrent state) summarizing past observations/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Hidden state updated at every timestep by recurrent cell(s) processing new observation/action representations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>The recurrent hidden state is used directly by downstream policy/value networks (no explicit retrieval; state is implicitly carried forward and consumed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Low performance in these experiments: DRRN remains under ~0.2 mean success after 1.5M environment steps on the multi-task setup, substantially below GFlan-T5 which reaches ~0.8+ much earlier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper contrasts DRRN (recurrent memory) with GFlan-T5 (prompt-based short-term memory within an LLM) to highlight the strong benefit of LLM priors plus online RL; authors note that although DRRN uses recurrent layers for short-term memory, it performs much worse than grounded LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Despite having explicit recurrent short-term memory, DRRN is not able to leverage prior language/world knowledge and is much less sample efficient in these tasks; symbolic observations (as used by Symbolic-PPO) can make learning easier than textual descriptions, complicating direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>When comparing memory mechanisms, LLMs with prompt-based history and pretraining can outperform smaller recurrent architectures; combining pretrained language priors with online grounding appears beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Scienceworld: Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decision-making <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4905",
    "paper_id": "paper-0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "GFlan-T5",
            "name_full": "Grounded Flan-T5 (GFlan-T5)",
            "brief_description": "An LLM-based agent using FLAN-T5 (encoder-decoder) as a policy that is online-finetuned with PPO in a text-only BabyAI-Text environment; the model conditions on a prompt containing the goal, the current observation, and a short history of previous observations and actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GFlan-T5",
            "agent_description": "FLAN-T5 (780M) used as an action policy: for each candidate textual action the model computes the conditional token likelihood under the prompt, these log-likelihoods are softmax-normalized to form a policy; a value head (MLP) is attached to the decoder representation and PPO is used to finetune the whole model online in the environment.",
            "llm_model_name": "Flan-T5 (780M)",
            "game_or_benchmark_name": "BabyAI-Text (textual adaptation of BabyAI / MiniGrid)",
            "task_description": "Language-conditioned navigation and manipulation tasks (Go to &lt;object&gt;, Pick up &lt;object&gt;, Put &lt;A&gt; next to &lt;B&gt;, temporal sequence tasks like Pick up A then go to B, Unlock door etc.) in a procedurally generated 1-room grid with distractors; tasks are described in natural language and observations are textual descriptions with distances.",
            "memory_used": true,
            "memory_type": "Short-term history encoded in the LLM prompt / context window (sliding-window prompt memory)",
            "memory_representation": "Raw text: the prompt explicitly contains the 3 previous observations and the last 2 actions (textual sentences describing distances, objects, and past actions).",
            "memory_update_mechanism": "Updated every step by constructing the prompt with the latest observation and including the previous 3 observations / 2 actions (i.e., a sliding-window update at each timestep).",
            "memory_retrieval_mechanism": "Implicit retrieval via the LLM's attention over the full prompt / context (the past observations/actions are concatenated into the prompt so the model attends over them when computing token probabilities).",
            "performance_with_memory": "High sample efficiency and final performance when used with the short-term prompt history: reaches ~0.8 mean success rate after ~250k environment steps and ~0.9 after ~600k in multi-task training (Figure 2); in a Go To task trained for 400k steps GFlan-T5 achieved 0.82 ± 0.02 success (Table 1); outperforms baselines (DRRN, NPAE-Flan-T5, Symbolic-PPO) which remain &lt;0.2 after 1.5M steps or ~0.4 for Symbolic-PPO.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No explicit ablation removing the short-term prompt history is reported in the main text; the authors compare GFlan-T5 to multiple baselines (zero-shot Flan-T5, DRRN, non-pretrained Flan-T5 variants) and perform ablations on LLM size, action-space size, and number of distractors. They report GFlan-T5 is robust to increased action-space size (3→9) and to more distractors (4→16) while still using the same short-term history prompt. Appendix contains additional ablations (LLM size, action-space, distractors) but no direct memory-on vs memory-off experiment is described in the main paper.",
            "challenges_or_limitations": "Memory is implemented as prompt-context (sliding short history), which uses the LLM context window and increases prompt length each step; computing per-action likelihoods (a forward pass per action sequence) is computationally expensive and scales poorly with action-space size and model size, prompting a distributed scoring solution (Lamorel). The paper notes limitations: experiments limited to textual environment, computational inefficiency when scaling action space and LLM size, and failure modes such as vocabulary overfitting (synonym substitution causes large drops) and poor generalization when many grounded symbols change at once (e.g., new language).",
            "best_practices_or_recommendations": "Include a short, task-relevant short-term history in the prompt (they used last 3 observations + last 2 actions) to satisfy short-term memory requirements of tasks; prefer online RL finetuning (PPO) to achieve functional grounding rather than only Behavioral Cloning; use distributed scoring/training (Lamorel) to mitigate computational cost of per-action likelihood scoring; be cautious about scaling (action-space and model size) due to inference/training cost and possible overfitting to vocabulary.",
            "uuid": "e4905.0",
            "source_info": {
                "paper_title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Flan-T5 (zero-shot)",
            "name_full": "FLAN-T5 (zero-shot, unfine-tuned)",
            "brief_description": "The pre-trained FLAN-T5 model (before online PPO finetuning) used as a zero-shot policy baseline by prompting; it receives the same prompt (including short-term history) but is not grounded via environment interactions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Flan-T5 (zero-shot)",
            "agent_description": "Pretrained FLAN-T5 used without PPO finetuning as a policy: probability of candidate actions computed from language-model conditional likelihoods under the prompt; otherwise identical inference pipeline but no RL updates.",
            "llm_model_name": "Flan-T5",
            "game_or_benchmark_name": "BabyAI-Text",
            "task_description": "Same language-conditioned navigation and manipulation tasks (multi-task mix) as used for GFlan-T5 evaluation.",
            "memory_used": true,
            "memory_type": "Short-term history in prompt / context window (same prompt format: 3 previous observations + 2 previous actions)",
            "memory_representation": "Raw text (previous observations and actions) inside the prompt.",
            "memory_update_mechanism": "Prompt updated each step (sliding window), same as GFlan-T5, but model parameters are not updated online.",
            "memory_retrieval_mechanism": "Implicit via model attention over the prompt; no external retrieval mechanism.",
            "performance_with_memory": "Low zero-shot performance: much worse than GFlan-T5; examples include very low success on composed/new tasks (e.g., on a new composition task Flan-T5 scored ~0.07 success vs GFlan-T5 0.12) and generally poor sample-efficiency compared with the PPO-finetuned GFlan-T5 (Flan-T5 performs near-random or low success in many tests). Exact per-task numbers for zero-shot are reported in Figure 4 and Figure 6 (Flan-T5 per-task performance much lower than GFlan-T5).",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "The paper uses Flan-T5 as a zero-shot baseline to show the effect of online RL grounding: comparisons highlight that finetuning via PPO (GFlan-T5) drastically improves performance. No explicit ablation isolating the prompt-history memory effect in the zero-shot model is reported.",
            "challenges_or_limitations": "Zero-shot FLAN-T5 lacks functional grounding and thus fails at environment-specific dynamics despite having linguistic priors; reliance on prompt history alone without grounding yields poor performance on the interactive tasks and weak generalization to new tasks or language changes.",
            "best_practices_or_recommendations": "Use online RL (PPO) to ground pretrained LLMs in interactive environments rather than relying on zero-shot prompting alone; include short-term history in prompts to provide necessary immediate context, but expect zero-shot behavior to be insufficient.",
            "uuid": "e4905.1",
            "source_info": {
                "paper_title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "DRRN baseline",
            "name_full": "Deep Reinforcement Relevance Network (DRRN) baseline",
            "brief_description": "A ~1M-parameter RL agent architecture commonly used for text-based games that uses recurrent layers to handle partial observability and encodes observations and actions for value/policy estimation; used here as a non-LLM baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "DRRN",
            "agent_description": "A text-based RL agent (implementation from Wang et al. 2022) with recurrent layers to process sequences of observations; acts on textual observations and candidate textual actions by encoding and scoring them.",
            "llm_model_name": null,
            "game_or_benchmark_name": "BabyAI-Text",
            "task_description": "Same multi-task language-conditioned navigation/manipulation tasks as other agents.",
            "memory_used": true,
            "memory_type": "Recurrent hidden-state memory (RNN/LSTM-style) used to capture short-term temporal context across timesteps.",
            "memory_representation": "Distributed hidden-state vectors (learned recurrent state) summarizing past observations/actions.",
            "memory_update_mechanism": "Hidden state updated at every timestep by recurrent cell(s) processing new observation/action representations.",
            "memory_retrieval_mechanism": "The recurrent hidden state is used directly by downstream policy/value networks (no explicit retrieval; state is implicitly carried forward and consumed).",
            "performance_with_memory": "Low performance in these experiments: DRRN remains under ~0.2 mean success after 1.5M environment steps on the multi-task setup, substantially below GFlan-T5 which reaches ~0.8+ much earlier.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Paper contrasts DRRN (recurrent memory) with GFlan-T5 (prompt-based short-term memory within an LLM) to highlight the strong benefit of LLM priors plus online RL; authors note that although DRRN uses recurrent layers for short-term memory, it performs much worse than grounded LLMs.",
            "challenges_or_limitations": "Despite having explicit recurrent short-term memory, DRRN is not able to leverage prior language/world knowledge and is much less sample efficient in these tasks; symbolic observations (as used by Symbolic-PPO) can make learning easier than textual descriptions, complicating direct comparisons.",
            "best_practices_or_recommendations": "When comparing memory mechanisms, LLMs with prompt-based history and pretraining can outperform smaller recurrent architectures; combining pretrained language priors with online grounding appears beneficial.",
            "uuid": "e4905.2",
            "source_info": {
                "paper_title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Scienceworld: Is your agent smarter than a 5th grader?",
            "rating": 2
        },
        {
            "paper_title": "Pre-trained language models for interactive decision-making",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        }
    ],
    "cost": 0.015281999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</h1>
<p>Thomas Carta*<br>Inria (Flowers)<br>University of Bordeaux, France<br>thomas.carta@inria.fr</p>
<p>Clément Romac*<br>Inria (Flowers)<br>University of Bordeaux, France<br>Hugging Face<br>clement.romac@inria.fr<br>Thomas Wolf<br>Hugging Face<br>Sylvain Lamprier<br>Univ Angers, LERIA,<br>SFR MATHSTIC, F-49000 Angers, France<br>Olivier Sigaud<br>Sorbonne Université, ISIR, Paris, France</p>
<p>Pierre-Yves Oudeyer<br>Inria (Flowers)<br>University of Bordeaux, France</p>
<h4>Abstract</h4>
<p>Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.</p>
<h2>1 Introduction</h2>
<p>The recent rise of Transformer-based Large Language Models (LLMs) trained on massive text datasets in Natural Language Processing has led to models exhibiting impressive capabilities (e.g. natural language generation, question answering, reasoning, translation...) [Devlin et al., 2019, Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022, Scao et al., 2022]. Recently, LLMs were shown to capture aspects of the physical rules in our world, e.g. about space Patel and Pavlick [2022], colors Abdou et al. [2021] or even affordances between bodies and objects Ahn et al. [2022]. This form of prior knowledge was exploited to suggest plans of action to solve goals in robotics Huang et al. [2022b], Ahn et al. [2022], Liang et al. [2022]. However, LLMs are known to suffer from a lack of grounding which prevents them from properly dealing with the meaning of inter-related concepts</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and their use for functional competence in interactive environments <em>Mahowald et al. (2023)</em>. Indeed, alignment between statistical structures in such LLMs and environments can be very limited, or even sometimes entirely wrong. This is partly due to 1) a training process (predicting next words) that is not directly incentivized to solve problems in an environment, 2) lack of abilities to intervene in the environment to identify causal structures; 3) lack in abilities to learn based on data collected as a result of interacting with the environment <em>(Bender and Koller, 2020; Bisk et al., 2020)</em>.</p>
<p>In the literature, language grounding has referred to various related objectives <em>Thill et al. (2014)</em>. First, symbol grounding can be formulated as the general problem of connecting a symbol system <em>(Harnad, 1990)</em>, internal to an agent, to the environment, in such a way that internal processing of these symbols can be used to act appropriately in this environment. One dimension of this problem is associating "elementary" symbols, such as the names of objects, with invariant structures in high-dimensional perceptual modalities such as vision <em>Cangelosi et al. (2010); Wiriyathammabhum et al. (2016)</em>. Such a grounding, called "direct grounding", has been extensively studied in the past leading to various efficient methods <em>(Alayrac et al., 2022; Radford et al., 2021; Lu et al., 2023)</em>, included in the context of robotic bodies <em>Cangelosi and Stramandinoli (2018)</em>. Another dimension is how to ground higher-order symbolic tokens, or abstract concepts, into elementary symbols, often through approaches such as distributional semantics <em>Harris (1981); Boleda (2019)</em>. This has been called "grounding transfer" <em>Cangelosi and Stramandinoli (2018)</em>. Beyond such mere associations, a key question about grounding is how internal processes that manipulate symbols can model, predict and control external physical and social processes: they need to be aligned on and constrained by these external dynamics and relational structures (at various levels of abstraction). This last notion of grounding, which we refer here as "functional grounding", is relative to a particular environment which may be the human physical environment but also more abstract interactive environments simulated in computers (where abstract physics can differ from human environments).</p>
<p>In this paper, we consider interactive textual worlds <em>Côté et al. (2018); Jansen (2021)</em>, which are precisely designed to focus on these higher-level forms of functional grounding. In textual worlds, environments can encode rich forms of physical structures inspired by the ones in the human world, e.g. <em>Wang et al. (2022)</em>, yet agents act and perceive in these environments only through the textual modality. In this context, this paper aims to make progress towards the following largely open question: how could LLMs be used as agent policies producing actions towards goals in interactive environments, perceiving the outcome of these actions, and incrementally grounding and updating their knowledge with the new observations they collect?</p>
<p>Building on recent works successfully using Reinforcement Learning (RL) to finetune LLMs for natural language generation tasks <em>(Stiennon et al., 2020; Ouyang et al., 2022; Ramamurthy et al., 2022)</em>, we propose the first study about functional grounding of LLMs through incremental online RL. In particular, we aim at empirically answering the following open scientific questions:</p>
<ul>
<li>Q1. Sample efficiency How fast can an LLM adapt and learn to solve various spatial and navigation problems specified in natural language? How does the use of pre-trained knowledge from LLM boosts sample efficiency?</li>
<li>Q2. Generalization to new objects: Once functionally grounded, how can an LLM generalize to various kinds of changes about objects, yet staying in trained tasks?</li>
<li>Q3. Generalization to new tasks: How can such an interactively trained LLM perform zero-shot generalization to new tasks? How does generalization depend on the kind of new tasks?</li>
<li>Q4. Impact of online interventions: What is the empirical impact of grounding using online RL with incremental interactions in comparison with offline Behavioral Cloning from a dataset of expert trajectories?</li>
</ul>
<p>To answer these scientific questions in Section 4, we present a functional grounding method for LLMs (see Figure 1 and Section 3), and transpose the BabyAI environment <em>(Chevalier-Boisvert et al., 2019)</em> into a textual version. Additionally, we aim to help the RL community further develop grounding techniques for LLMs in interactive environments by releasing, in addition of the code of this paper, a Python library named Lamorel facilitated by the use of LLMs at scale for RL practitioners. While many</p>
<p>https://github.com/flowersteam/Grounding_LLMs_with_online_RL</p>
<p>tools already exist for LLMs and NLP tasks, moving to an RL setting with interactive environments requires adaptations (e.g. very frequent need of fast inference to compute action probabilities) making previous tools not well suited for RL practitioners (see Section 3.4).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The GLAM method: we use an LLM as agent policy in an interactive textual RL environment (BabyAI-Text) where the LLM is trained to achieve language goals using online RL (PPO), enabling functional grounding. (a) BabyAI-Text provides a goal description for the current episode as well as a description of the agent observation and a scalar reward for the current step. (b) At each step, we gather the goal description and the observation in a prompt sent to our LLM. (c) For each possible action, we use the encoder to generate a representation of the prompt and compute the conditional probability of tokens composing the action given the prompt. Once the probability of each action is estimated, we compute a softmax function over these probabilities and sample an action according to this distribution. That is, the LLM is our agent policy. (d) We use the reward returned by the environment to finetune the LLM using PPO. For this, we estimate the value of the current observation by adding a value head on top of our LLM. Finally, we backpropagate the gradient through the LLM (and its value head).</p>
<h1>2 Related work</h1>
<p>Language-conditioned RL We position our work in the Language-conditioned RL setting, where an instruction-following agent learns a policy that executes actions in an interactive environment in order to fulfill a language instruction [Luketina et al., 2019]. While several works studied this setting for various tasks in 2D or 3D environments [Hermann et al., 2017, Misra et al., 2017, Bahdanau et al., 2018, Colas et al., 2020, Chevalier-Boisvert et al., 2019], we here focus on text-only interactions (i.e. performing textual commands given textual observations) as in Shridhar et al. [2020]. However, our work studies how LLMs can not only encode this instruction [Hill et al., 2020] but also be directly used as agent policies choosing actions given the observation.</p>
<p>Textual environments for RL Many text-only environments have been used and developed [Jansen, 2021, Wang et al., 2022]. They usually implement high-level text commands along with very large action spaces and complex dynamics between entities, often aiming to study functional grounding of abstract policies. While these environments offer interesting properties, we had to introduce a new one given the purpose and constraints of our study. Dealing here with computationally expensive LLMs, we chose to trade complex action spaces for systematic experiments studying the questions of the introduction. Second, to perform an in-depth analysis of our functional grounding method, we focused on lower-level navigation skills in spatial environments (which lacks in most textual environments as the agent can usually just change room and has direct access to objects in a room). Moreover, several ablation studies shown in Appendix B. 6 required precise control over the procedural generation (usually not offered by textual environments). For these reasons, we adapted the BabyAI platform [Chevalier-Boisvert et al., 2019] into a procedural text-only version that enables decoupling exploration challenges from perception challenges. Additionally, we are still able to use BabyAI's visualization tools to analyze trajectories (see Figure 1).</p>
<p>Foundation Models for decision making Foundation models trained on massive datasets were shown to exhibit impressive abilities along with fast adaptation to a wide range of downstream tasks in vision [Yuan et al., 2021], language [Devlin et al., 2019, Brown et al., 2020] and cross-modalities [Ramesh et al., 2021, Jiang et al., 2022, Alayrac et al., 2022]. While such abilities have been leveraged to provide reward to RL agents Gupta et al. [2022], Fan et al. [2022], a recent line of work started focusing on using Foundation Models (and in particular LLMs) to guide agents policy.</p>
<p>First, SayCan [Ahn et al., 2022], Code as Policies [Liang et al., 2022] and Inner Monologue [Huang et al., 2022b] used LLMs as high-level planners in robotics setups. Because their LLM is not directly used as agent policy for low-level actions and is not grounded using its interactions with the environment, Ahn et al. [2022] had to use an external affordance function to re-rank the actions proposed by the LLM. Similarly, Yao et al. [2022] also featured a closed-loop feedback between an LLM that is the planner and an agent that is the actor but this time in a textual environment. Expanding on this, Dasgupta et al. [2022] added a reporter observing the environment and reporting useful information to the planner. While hinting at the usefulness of prior knowledge contained in LLMs for embodied tasks, these works are limited by the absence of grounding.</p>
<p>Second, several works proposed to first finetune LLMs on expert trajectories before using them in the environment. Using their ScienceWorld benchmark, Wang et al. [2022] showed that LLMs finetuned using Behavioral Cloning performed worse than a much smaller and randomly initialized Deep Q-Network trained using RL supporting the hypothesis that grounding in the environment through direct interactions is crucial. Finally, Reid et al. [2022] reused LLMs to perform offline RL in non-linguistic environments leveraging the internal structures learned by LLMs but no longer using words or symbols they were trained to manipulate (Takagi [2022] investigated how these internal structures can be relevant for unrelated tasks).</p>
<p>Finally, one may also pretrain a policy using Behavioral Cloning or offline RL from expert trajectories before finetuning it with interactions with an environment. Related to our work, the Online Decision Transformer [Zheng et al., 2022] first uses offline RL to pretrain a transformer model and eventually finetunes it with online RL. But compared to our study, it did not use a general Language Modeling pretraining objective and therefore did not study functional grounding of language symbols.</p>
<p>Finetuning LLMs with RL Recent works successfully leveraged RL to finetune LLMs. RL was used in particular to improve alignment between generated text and human preferences [Stiennon et al., 2020, Ouyang et al., 2022, Ramamurthy et al., 2022]. In this Reinforcement Learning from Human Feedback (RLHF) framework, text generation is viewed as a sequential decision-making problem where each "action" of the LLM is a new token and the "state" corresponds to the prompt. Most of these methods used PPO [Schulman et al., 2017] to finetune their LLMs using a reward function learned on a dataset of collected human interactions. With this technique, Ouyang et al. [2022] managed to generate more human-aligned outputs despite having a model (InstructGPT) with 100 times fewer parameters than GPT-3 [Brown et al., 2020]. While our work shares the PPO-based finetuning with RLHF, our setup diverges from it in multiple aspects. First, our LLM is functionally grounded using an external task-conditioned reward from the environment (which happens to be sparse in our BabyAI-Text environment) and not a learned reward model. Second, the RLHF setup has no external environment dynamics controlling the next state given a previous state and an action (the next state in RLHF is just the previous state with the last generated token appended). In comparison, our work exposes an outer loop controlled by the environment whose dynamics, providing the next state and reward, are unknown to the LLM (in comparison to RLHF where the RL loop is an inner loop in the token generation process). We believe using RL to finetune LLMs can be taken from a broader perspective in which both our framework and RLHF are particular applications.</p>
<h1>3 GLAM: Grounding LLMs with online RL</h1>
<p>We introduce the GLAM method (for Grounded LAnguage Models) where an LLM is used as agent policy and is functionally grounded in an interactive environment using online RL, leveraging collected observations and rewards to improve itself towards achieving goals formulated in language. We detail this method in the following paragraphs and redirect the reader to Figure 1 for a schematic view. We first formalize the textual RL problem we tackle (a). Then, we detail how we use an LLM as agent policy to interact with BabyAI-Text (b, c). Finally, we explain how online RL finetuning is used to ground the LLM in BabyAI-Text (d).</p>
<h1>3.1 Problem statement</h1>
<p>We assume a textual RL setting where, given a language vocabulary $\mathcal{V}$, our environment returns an observation $o \in \mathcal{V}^{N}$ and a reward $r \in \mathbb{R}$ following an action $a \in \mathcal{A} \subset \mathcal{V}^{N}$ (i.e. actions are sequences of tokens). We also assume a task or goal description $g \in \mathcal{G} \subset \mathcal{V}^{N}$ which conditions the reward. Such an environment can be framed as a goal-augmented Partially Observable Markov Decision Process $\mathcal{M}=(\mathcal{S}, \mathcal{V}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \mathcal{G}, \mathcal{O}, \gamma)$ with $\mathcal{S}$ the state space, $\mathcal{A} \subset \mathcal{V}^{N}$ the action space, $\mathcal{G} \subset \mathcal{V}^{N}$ the goal space, $\mathcal{T}: \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}$ the transition function, $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{G} \mapsto \mathbb{R}$ the goal-conditioned reward function, $\mathcal{O}: \mathcal{S} \mapsto \mathcal{V}^{N}$ the observation function mapping a state to a textual description and finally $\gamma$ the discount factor.
In this work, we extend the BabyAI platform [Chevalier-Boisvert et al., 2019] initially designed for grounded language learning and propose a text-only extension named BabyAI-Text. We leverage BabyAI's inner procedurally generated minigrid environment where an agent navigates and interacts with objects through 6 text commands: turn left, turn right, go forward, pick up, drop and toggle. We also reuse the set of tasks introduced in BabyAI as well as their associated description along with the sparse scalar reward. Our key difference is the textual description $o \in \mathcal{V}^{N}$ of the agent's partial observation returned by BabyAI-Text instead of the symbolic representation initially returned by BabyAI (see Appendix A.2). We leverage BabyAI-Text in Section 4 to assess our grounding method.</p>
<h3>3.2 LLMs as policies in interactive environments</h3>
<p>In order to use the LLM as the policy in such a textual interactive environment, we gather the task description, the textual description of the current observation and the set of possible actions in a prompt used to feed the LLM. We chose a single arbitrary and simple prompt template (see Appendix C for examples) and did not perform any intensive prompt engineering. Indeed, as we finetune the LLM, we expect it to adapt to the chosen prompt template. Nonetheless, a more careful design of prompts could improve the results shown in Section 4.
Given this prompt, we now need the LLM to output a probability distribution over the possible actions $\mathbb{P}(\mathcal{A})$. For this, Huang et al. [2022a], Li et al. [2022], Wang et al. [2022] used the LLM to generate text. If the generated sequence of characters corresponds to one of the possible actions (i.e. $s \in \mathcal{A}$ ), this action is chosen by the agent. Otherwise, an ad-hoc mapping must be performed to select an action $a_{i} \in \mathcal{A}$ given $s$. As an alternative method, one could also use more standard RL practices by adding action heads - a Multi-Layer Perceptron (MLP) with $|\mathcal{A}|$ outputs - on top of the LLM. Finally, Ahn et al. [2022] proposed to directly use the LLM to compute the (log) probability of each action $a_{i} \in \mathcal{A}$ by computing the conditional probability of each token in action $a_{i}=\left{w_{0}, \ldots, w_{\left|a_{i}\right|}\right}$ given the prompt $p$ :</p>
<p>$$
\mathbb{P}<em i="i">{L L M}\left(a</em>} \mid p\right)=\prod_{j=0}^{\left|a_{i}\right|} \mathbb{P<em j="j">{L L M}\left(w</em>\right)
$$} \mid p, w_{&lt;j</p>
<p>with $\mathbb{P}<em j="j">{L L M}\left(w</em>$ (see (c) from Figure 1). This method suffers from requiring a forward pass on the LLM for each action to compute the probability of its sequence of tokens (especially in comparison to new action heads that require a single forward pass on the prompt to compute all actions' probability). However, it also has several advantages, in particular, 1) there is no need of potential ad-hoc mapping as when text is generated, 2) we use only pretrained operations from the LLM and leverage language modeling heads' prior and 3) this method is robust to any action space and can thus be used on any textual environment with no change.
For these reasons, we chose the latter method. We first use log probabilities instead of normalized probabilities using $\mathbb{L} \mathbb{P}} \mid p, w_{&lt;j}\right)$ the probability computed by the LLM of token $w_{j}$ given prompt $p$ and previous tokens $w_{&lt;j<em i="i">{L L M}\left(a</em>} \mid p\right)=\sum_{j=0}^{\left|a_{i}\right|} \log \mathbb{P<em j="j">{L L M}\left(w</em>} \mid p, w_{&lt;j}\right)$ in replacement of $\mathbb{P<em i="i">{L L M}\left(a</em>$ using a softmax function:} \mid p\right)$ to avoid multiple normalization operations. We eventually normalize all the log probabilities to obtain a distribution over $\mathcal{A</p>
<p>$$
\mathbb{P}\left(a_{i} \mid p\right)=\frac{e^{\mathbb{L} \mathbb{P}<em i="i">{L L M}\left(a</em>} \mid p\right)}}{\sum_{a_{j} \in \mathcal{A}} e^{\mathbb{P<em j="j">{L L M}\left(a</em>
$$} \mid p\right)}</p>
<h1>3.3 PPO finetuning</h1>
<p>We now propose to leverage experiences gathered by the LLM to perform functional grounding. More formally, we aim to learn a policy $\pi: O \times \mathcal{G} \mapsto \mathbb{P}(\mathcal{A})$ that maximizes the expected discounted sum of rewards for any given goal $g \in \mathcal{G}$. We use for this the PPO algorithm [Schulman et al., 2017] that both learns a policy $\hat{\pi}: O \times G \mapsto \mathbb{P}(A)$ and a value function $\hat{V}: \mathcal{O} \times \mathcal{G} \mapsto \mathbb{R}$ approximating the true value $V(s, g)=\mathbb{E}<em i="i">{a \sim \hat{\pi}(\mathcal{O}(s), g)}[R(s, g, a)+\gamma V(\mathcal{T}(s, a), g)]$.
As mentioned in Section 3.2, we compute the probability of each action $a</em> \mid p\right)$.
For value approximation, we we add an MLP with a single output for the value on top of the last layer of the first Decoder block (i.e. in place of the language modeling heads) in order to compute $\hat{V}(o \mid g)=\hat{V}(p)$ (see (d) from Figure 1). This position is explained by the fact that we use EncoderDecoder LLMs in our experiments but our method could easily be used with Decoder-only models by attaching the value head to the Decoder block encoding the last token of the prompt.} \in \mathcal{A}$ using the likelihood computed by the LLM as $\hat{\pi}\left(a_{i} \mid o, g\right)=\mathbb{P}\left(a_{i</p>
<h3>3.4 Distributed LLM policies using Lamorel</h3>
<p>Using LLMs to compute probabilities over action space is computationally expensive as it requires computing $\prod_{j=0}^{\left|a_{i}\right|} \mathbb{P}<em j="j">{L L M}\left(w</em>\right}$. When one uses very large LLMs (i.e. more than hundreds of million parameters), computing the probability of a single action already means performing a long forward pass over the whole network. As a result, computing the probability of each possible action at every step becomes very slow. Considering the number of interactions usually required to solve tasks in BabyAI (and by extension BabyAI-Text), performing online RL finetuning of LLMs easily became intractable with a single LLM distributed over multiple GPUs. To overcome this, we deployed $N$ LLM workers each handling a subset of actions to score in parallel (allowing a quasi-linear time decrease with $N$ ). We add to this distributed inference the possibility to also perform distributed training (i.e. compute the gradient of minibatches in parallel and gather gradients before updating models). We wrap all this in a Python library named Lamorel designed for RL practitioners eager to use LLMs. It allows one to use LLMs as black-box but also to perform more advanced methods such as adding new heads on top of them. See Appendix E for more details.} \mid p, w_{&lt;j}\right)$ for each action $a_{i}=\left{w_{0}, \ldots, w_{\left|a_{i}\right|</p>
<h2>4 Experiments</h2>
<p>We design a set of experiments in BabyAI-Text aiming to provide answers for the scientific questions introduced in Section 1. In these experiments, we use Flan-T5 780M [Rae et al., 2021] for 1) the close link between its training corpus (containing instruction-following documents) and our languageconditioned interactive environment, and 2) its simple open-source access through the Hugging Face tools ${ }^{3}$. We apply our GLAM method to Flan-T5 (which we name GFlan-T5 in experiments below for Grounded Flan-T5) and compare it with three baselines. First, we also train a non-pretrained Flan-T5 where we only reuse the pretrained embedding layer and add action heads on top of it (see Figure 10 in appendices). As for GFlan-T5, we propagate the gradient through the entire graph (included the action heads here). We call this baseline NPAE-Flan-T5 (Non-Pretrained with Action heads and Embedding Flan-T5). We show in Appendix B. 4 that using a non-pretrained Flan-T5 while keeping the scoring method fails. We also provide as a more classic RL baseline a DRRN [He et al., 2016] agent of approximately 1 M parameters which is often used for TextWorlds. We especially reuse the implementation from Wang et al. [2022] which gave SOTA results and outperformed LLMs. At each step, we feed our 3 agents above with the following prompt template filled using the information returned by BabyAI-Text (see Appendix C for examples):</p>
<ul>
<li>A header listing what actions are accessible (but not necessarily useful) in the environment in the form of:
Possible action of the agent: $&lt;$ list of actions $&gt;$</li>
<li>The goal of the agent: Goal of the agent: $&lt;$ goal $&gt;$</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>The 3 previous observations and last 2 actions, used as a short-term memory required to complete BabyAI-Text tasks (in comparison, the DRRN uses recurrent layers to deal with short-term memory requirements):
Obs. 0: <description from BabyAI-Text at step $t-2>$
Action 0:<action chosen by the agent at step $t-2>$
Obs. 1: <description from BabyAI-Text at step $t-1>$
Action 1: <action chosen by the agent at step $t-1>$
Obs. 2: <description from BabyAI-Text at step $t>$
Action 2: <the next action to be chosen by the agent>
Finally, as BabyAI-Text simply provides an alternative mapping of observations, we add as an indication the performance of the PPO agent used in [Chevalier-Boisvert et al., 2019] that runs on BabyAI rather than BabyAI-Text (i.e. using symbolic observations instead of textual descriptions) and name this agent Symbolic-PPO in results below. In Appendix B.3, we show that symbolic observations provided by BabyAI encode biases that ease learning compared to text descriptions. However, even with this advantage, GFlan-T5 outperforms Symbolic-PPO in all our setups.
We first study Q1 by training the different agents in a multi-task setting assessing their efficiency at learning the different tasks. We then address questions Q2, Q3 and Q4 using a set of generalization experiments (Figure 4) on the zero-shot abilities of the resulted trained agents mostly inspired from [Colas et al., 2020] and [Valmeekam et al., 2022]. We report their average success rate as well as standard deviation. We compare the results of GFlan-T5, DRRN as well as Flan-T5 (i.e. the LLM used in GFlan-T5 but before our finetuning) to show how our grounding method impacted it. All results below are given with their $99 \%$ confidence interval (mathematical details are given in Appendix G).</li>
</ul>
<h1>4.1 How fast can an LLM adapt and learn to solve tasks? (Q1)</h1>
<p>In order to study question Q1, we train our agents for 1.5 million steps in BabyAI-Text where each episode is a task randomly sampled from the following:</p>
<ul>
<li>Go to $&lt;$ object $&gt;$, a simple navigation task that requires reasoning abilities to choose the right plan given objects' position;</li>
<li>Pick up $&lt;$ object $&gt;$, a reasoning task that combines navigation tasks;</li>
<li>Put $&lt;$ object $\mathbf{A}&gt;$ next to $&lt;$ object $\mathbf{B}&gt;$, which requires first reaching $&lt;$ object $\mathrm{A}&gt;$, picking it up, reaching $&lt;$ object $\mathrm{B}&gt;$ and finally dropping $&lt;$ object $\mathrm{A}&gt;$ next to $&lt;$ object $\mathrm{B}&gt;$;</li>
<li>Pick up $&lt;$ object $\mathbf{A}&gt;$ then go to $&lt;$ object $\mathbf{B}&gt;$ and Go to $&lt;$ object $\mathbf{B}&gt;$ after pick up $&lt;$ object $\mathbf{A}&gt;$, both serving to test reasoning abilities on temporal sequences;</li>
<li>Unlock <door $>$, a task that includes inferring that a key is needed to unlock the door, finding the right key (i.e. the one colored as the door) and eventually using the toggle action with the key on the door.</li>
</ul>
<p>In each task, the agent must navigate in one procedurally generated room with 8 distractors (i.e. useless objects for the completion of the task).
We plot the mean and standard deviation of the success rate (i.e. 1 if the goal has been reached, 0 otherwise) over 4 seeds of GFlan-T5, NPAE-Flan-T5, DRRN and Symbolic-PPO in Figure 2. In addition, we also monitor the evolution of probability of each possible action on a set of 11 evaluation prompts to assess agents' abilities to solve each task in Appendix C. By plotting the evolution of the distribution over possible actions in Figure 18, we better grasp how and when the agents learn skills (e.g. navigation skills).</p>
<p>Looking at the evolution of the average success rate, GFlan-T5 quickly reaches 0.8 after only 250.000 steps (and 0.9 after approximately 600.000 steps). In comparison, both DRRN and NPAE-Flan-T5 are still under 0.2 after 1.5 million steps. Even when compared to Symbolic-PPO, which uses symbolic observations (easier to process than language as shown in Appendix 7), GFlan-T5 exhibits a drastically better sample efficiency with Symbolic-PPO almost reaching 0.4 after 1.5 million steps. Figure 18 and Table 2 highlight how GFlan-T5 leverages its knowledge about the relationships between entities to learn navigation tasks in less than a hundred updates.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Q1. Sample efficiency: Evolution over 4 seeds of the average success rate and standard deviation on all Q1 tasks. The details of average success rate calculation over the goals is given in Appendix B. 2
<img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Impact of the action space size ( 3,6 or 9 actions with always only 3 useful actions).
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Impact of the number of distractors.</p>
<p>Figure 3: Impact of the aciton space size and number of distractors on the sample efficiency measure (Equation (3)). We report results averaged over 2 seeds for training on the Go To task.</p>
<p>The failure of NPAE-Flan-T5 both highlights how GFlan-T5 leverages the LLM's pretrained knowledge to deal with the proposed tasks and how the finetuning method helps achieve the grounding objective. Furthermore, the fact that GFlan-T5 strongly outperforms Symbolic-PPO and the latter is better than NPAE demonstrates how language can be used as a tool to scaffold learning if already acquired. It also exaplins how counterproductive it can be if one asks an agent to both learn a task and language at the same time (see Appendix B. 3 for further results).
We now perform a deeper analysis of this sample efficiency by studying the impact of varying the action space and the number of distractors. We provide both the evolution of success rate and a sample efficiency measure $S E$ :</p>
<p>$$
S E=\frac{1}{T} \sum_{t=0}^{T} S R_{t}
$$</p>
<p>where $T$ is the number of steps or frames seen and $S R$ the success rate at frame $t$.</p>
<h1>4.1.1 Impact of the dimension of the action space</h1>
<p>In this experiment, we test the sensitivity of LLMs to the size of the action space by using 3 different action spaces when trained on the Go to $&lt;$ object $&gt;$ task:</p>
<ul>
<li>The restricted action space composed of the only 3 useful actions: turn left, turn right, go forward.</li>
<li>The canonical action space composed of the 6 actions that can be performed in the environment with 3 useful and 3 useless actions that are pick up, drop and, toggle (they are useless here as the agent is only navigating).</li>
<li>The augmented action space composed of 9 actions ( 3 useful and 6 useless with pick up, drop, toggle, sleep, do nothing and think). The last three actions have been chosen such that they clearly have no use for the Go To <object> task and consequently should not impact an agent that has knowledge about the world.</li>
</ul>
<p>We conduct our tests in an environment with 1 room, 8 distractors and report results in Figure 3a. Results show no impact on GFlan-T5, while the performance of other agents decreases with larger action spaces. We hypothesize that this is due to the LLM's ability to discard useless actions quickly at the beginning of finetuning.</p>
<h3>4.1.2 Impact of the number of distractors</h3>
<p>Similarly, we expect LLMs to be less sensitive to variations in task complexity. We assess this by plotting the evolution of sample efficiency (Equation (3)) for 4,8 and 16 distractors. We conduct these tests in an environment with 1 room and observe a slight performance loss from GFlan-T5 when the number of distractors increases (Figure 3b). In comparison, Symbolic-PPO degrades as the number of distractors increases with a success rate decreasing by $38 \%$ from 4 to 16 distractors whereas the GFlan-T5 success rate only decreases by $14 \%$. We hypothesize that the LLM manages to focus on the relevant aspect of the environment quickly.
Thus, GFlan-T5 seems robust with similar learning curves when one increases the action space size (from 3 to 9 actions with only 3 useful ones) or the number of distractors (from 4 to 16) We also provide in Appendix B an additional ablation analyzing the impact of the LLM's size B.5. Results highlight that the number of parameters has a high impact on the learning process. Indeed, we observe a strong difference on sample efficiency and asymptotic performance between a small LLM ( 80 million parameters) and the 780 million parameters we used here. We also plot the full learning curves for the ablation on the action space size and the number of distractors in appendices B.6.1 and B.6.2 respectively.</p>
<h3>4.2 Q2. Generalization to new objects</h3>
<p>In this section, we analyze how a functionally grounded agent can generalize its skills to new objects. Indeed, we expect our agents to focus on the the geometry of the environment (how objects are positioned and how their positioning is described), but not on the identity of the objects themselves (e.g. Go to $&lt;$ object $&gt;$ should be achieved even if the object has not been seen during training). We test if this property is present in our trained agents by measuring their zero-shot performance in two environments. First, an environment with nouns not in the training vocabulary (e.g. "tree") ${ }^{4}$ and second, an environment with invented objects (made of an invented adjectives and an invented nouns such as faze dax). ${ }^{5}$ We use the environment the agent has been finetuned on (i.e. without any word substitutions) as a control environment. Results in Figure 4 (Q2 part) indicate that GFlan-T5 is not affected when tasks contain out-of-vocabulary nouns. Moreover, even if the GFlan-T5's success rate decreases by $13 \%$ when it is in an environment with invented objects, it still retains strong performances compared to baselines. These results support the hypothesis that GFlan-T5 has functionally grounded the symbols that describe the geometry of the environment and the instructions (e.g. words such as "in front", or the meaning of "steps" as a distance measure) ${ }^{6}$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Generalization tests: We train all agents on a mix of 5 different tasks and evaluate their generalization abilities on 1000 test episodes (also containing a mix of these 5 tasks) (a). We compare them to two baselines: an agent choosing actions randomly (Random) and the zero-shot Flan-T5 (without any finetuning). We then perform several generalization studies to answer Q2 and Q3 by (b) substituting object names out-of-vocabulary names, (c) substituting objects and colors by invented words, (d) testing a new composition of tasks, (e) substituting actions by synonyms and (f) translating the whole environment to French for the Go To task. For each agent, we plot its mean success rate over 2 seeds along with the confidence interval and the delta with performance on the same task without any change (except for (d), on which no baseline result can be provided as this task is completely new). The details of average success rate calculation over the goals is given in Appendix B.2.</p>
<h1>4.3 Q3. Generalization to new tasks</h1>
<p>In this Section, we perform generalization tests as in Section 4.2, but with new unseen tasks. Using these, we verify to what extent an agent is able to compose and generalize over the symbols it has grounded during finetuning.</p>
<p>Table 1: Generalization tests for Behavioral Cloning</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: left;">Environments</th>
<th style="text-align: left;">GFlan-T5</th>
<th style="text-align: left;">BC-GFlan-T5</th>
<th style="text-align: left;">BC-Bot</th>
<th style="text-align: left;">Random</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q4</td>
<td style="text-align: left;">Go To task no change</td>
<td style="text-align: left;">$0.82 \pm 0.02$</td>
<td style="text-align: left;">$0.69 \pm 0.08$</td>
<td style="text-align: left;">$0.73 \pm 0.07$</td>
<td style="text-align: left;">$0.30 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Go To task with invented words</td>
<td style="text-align: left;">$0.74 \pm 0.004$</td>
<td style="text-align: left;">$0.7 \pm 0.07$</td>
<td style="text-align: left;">$0.63 \pm 0.08$</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>New composition of learned tasks: Pick up $&lt;$ object $\mathbf{A}&gt;$ then/after pick up $&lt;$ object $\mathbf{B}&gt;$ During finetuning, agents learn to do both 1) Pick up $&lt;$ object $A&gt;$ and 2) Pick up $&lt;$ object $A&gt;$ then go to $&lt;$ object $B&gt;$ or Go to $&lt;$ object $B&gt;$ after pick up $&lt;$ object $A&gt;$ tasks. We test in this experiment if an agent can compose grounded symbols to solve the new tasks Pick up $&lt;$ object $A&gt;&lt;$ then/after $&gt;$ pick up $&lt;$ object $B&gt;$. Results in Figure 4 (Q3 part) hint that, while all agents fail to solve these new tasks, GFlan-T5 outperforms other baselines by reaching an 0.12 success rate compared to Flan-T5 (0.07) or Random ( 0.05 ). These low results can be explained by the fact that none of the agents managed to master the Pick up $&lt;$ object $A&gt;$ then go to $&lt;$ object $B&gt;$ or Go to $&lt;$ object $B&gt;$ after pick up $&lt;$ object $A&gt;$ tasks during training (see Appendix C). More details about the grounding of "then" and "after" are given in the Appendix D.4.</p>
<p>Seen tasks with synonym actions In this task, we test the robustness of our agents to actions by replacing the actions used during training by synonyms. For instance, "go forward" is replaced with</p>
<p>"move ahead"7. We expect LLMs, which already learned to map words to an embedding space, to also ground synonyms as they ground words of the environment. In this environment (see Figure 4 Q3 part), the success rate of GFlan-T5 is 0.12 vs 0.01 for Flan-T5. Thus the grounding of some words (here the actions) also improves the grounding of their synonyms. However, we observe an $87 \%$ drop in performance compared to the original settings, which we assume is due to an over-fitting of the actions' vocabulary.</p>
<p>New language In order to understand how far agents can generalize, we test them with a language not seen during training (French). Knowing that Flan-T5 has been pretrained with a multilingual corpus and is able to translate simple sentences, we test whether grounding in GFlan-T5 has also impacted its manipulation of other languages. However, we observe that even only for a simple navigation task (i.e. Go To), the model fails to generalize to a new language with a success rate ( 0.02 ) worse than random ( 0.30 ). We hypothesize that when too many grounded symbols are modified at once, functional grounding fails to be transferred to this new subsystem of symbols. Complementary experiments that confirm and reinforce this result are presented in appendices D. 2 and D.3.</p>
<h1>4.4 What is the impact of using RL vs Behavioral Cloning for grounding? (Q4)</h1>
<p>Finally, we study how online interactions with an environment, enabling learning through interventions and trial-and-error, improves grounding in comparison to pure Behavioral Cloning (BC). We compare a GFlan-T5 trained on the Go To task over 400000 steps with two baselines trained with Behavioral Cloning using 400000 transitions (see Appendix F.2). For the baseline called BC-GFlan-T5, transitions are collected from GFlan-T5 finetuned on the Go To task. For BC-Bot, transitions are collected using the BabyAI procedural bot achieving a success rate of 1.</p>
<p>In Table 1, we measure the success rate of GFlan-T5 and the baselines on two tasks: Go To and Go To with invented nouns and adjectives. First, once can see that GFlan-T5 outperforms all baselines in both tasks. Second, as GFlan-T5 does not achieve a success rate of 1 on the Go To task, its collected trajectories for BC can contain deceptive transitions in comparison to the ones collected by the bot. Hence, we obtain the expected result that BC-Bot outperforms BC-GFlan-T5. Finally, we expect our agents not to be affected by an environment where nouns and adjectives are replaced by invented ones in such navigation tasks. Experiments show that GFlan-T5 is less affected ( $0.82 \rightarrow 0.74$ ) than the BC-Bot ( $0.73 \rightarrow 0.63$ ). GFlan-T5 also performs better in the invented words task than the BC-GFlan-T5 (success rate of 0.7 ).</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed the GLAM method for functional grounding (i.e. aligning internal symbols to external dynamics so that the agent can use them to solve tasks in the environment) of LLMs in interactive textual environments based on online RL. Using our new BabyAI-Text environment, we performed several experiments studying 4 scientific questions. We showed how GLAM, which requires almost no environment-specific modifications on the LLM, enables to drastically improve performances to solve RL tasks in this environment as compared to zero-shot use the LLM, to supervised finetuning and to RL finetuning of non-pretrained LLMs. We showed how it boosts both sample efficiency and generalization abilities in zero-shot tests (both to new objects and several new tasks). In addition to these key results, we provided in-depth ablations showing the effect of several parameters (e.g. size) on grounding. We believe this method can act as a milestone towards grounding and using LLMs in interaction with our world. However, this study still suffers several limitations, in particular the fact that current experiments are limited to a textual environment, and the computational inefficiency when scaling up the action space and the size of the LLM. This computational inefficiency constrained this paper to using a single environment and rather small LLMs. Yet, improving computational efficiency (or access to more computational resources) could enable to leverage recent multi-modal Foundation models [Alayrac et al., 2022]) for grounding LLMs in broader environments (e.g. to robotics setups [Lu et al., 2021, Ahn et al., 2022]). Parallel to this, a future direction would be to study how functionally grounding an LLM on a specific environment affects its zero-shot abilities but also its plasticity and ability to acquire new skills in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>other environments. Moreover, these results hint that using LLMs as agent policies opens an avenue for escaping the Tabula-Rasa RL setting and creating much more sample efficient RL agents.</p>
<p>Finally, the recent rise of real-world deployed applications using LLMs highlighted the various societal and ethical challenges of using such models in real-world scenarios. Similar to RLHF, our work studies how to better align LLMs (but this time to environments in which tasks must be solved). While our approach stands as a first important building block for future works making LLMs more in line with their environment, it is not designed to be ready for real-world deployment and thus we do not recommend to use it in such an applicative context.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>Experiments presented in this paper were carried out using the HPC resources of IDRIS under the allocation 2022-[A0131011996] made by GENCI. This work also has received funding from the European Commission's Horizon Europe Framework Programme under grant agreement No 101070381 (PILLAR-robots project). We would also like to thank Victor Gondat for his kind help on schemas.</p>
<h2>References</h2>
<p>Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders Søgaard. Can language models encode perceptual structure without grounding? a case study in color. In Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL),, 2021.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, KuangHuei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. ArXiv, abs/2204.01691, 2022.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022.</p>
<p>Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Seyedarian Hosseini, Pushmeet Kohli, and Edward Grefenstette. Learning to understand goal specifications by modelling reward. In International Conference on Learning Representations, 2018.</p>
<p>Emily M. Bender and Alexander Koller. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 463.</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. Experience Grounds Language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718-8735, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 703.</p>
<p>Gemma Boleda. Distributional semantics and linguistic theory. ArXiv, abs/1905.01896, 2019.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>Angelo Cangelosi and Francesca Stramandinoli. A review of abstract concept learning in embodied agents and robots. Philosophical Transactions of the Royal Society B: Biological Sciences, 373 (1752):20170131, June 2018. doi: 10.1098/rstb.2017.0131. Publisher: Royal Society.</p>
<p>Angelo Cangelosi, Giorgio Metta, Gerhard Sagerer, Stefano Nolfi, Chrystopher Nehaniv, Kerstin Fischer, Jun Tani, Tony Belpaeme, Giulio Sandini, Francesco Nori, et al. Integration of action and language knowledge: A roadmap for developmental robotics. IEEE Transactions on Autonomous Mental Development, 2(3):167-195, 2010.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In International Conference on Learning Representations (ICLR), 2019 .</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.</p>
<p>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, and Pierre-Yves Oudeyer. Language as a cognitive tool to imagine goals in curiositydriven exploration. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. In CGW@IJCAI, 2018.</p>
<p>Ishita Dasgupta, Christine Kaeser Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. In Advances in Neural Information Processing Systems (NeurIPS) LaReL workshop, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.</p>
<p>Linxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. ArXiv, abs/2206.08853, 2022.</p>
<p>Tarun Gupta, Peter Karkus, Tong Che, Danfei Xu, and Marco Pavone. Foundation models for semantic novelty in reinforcement learning. ArXiv, abs/2211.04878, 2022.</p>
<p>Steve Harnad. The symbol grounding problem. Physica D, 42:335-346, 1990.
Zellig S. Harris. Distributional Structure. In Zellig S. Harris and Henry Hiż, editors, Papers on Syntax, Synthese Language Library, pages 3-22. Springer Netherlands, Dordrecht, 1981. ISBN 978-94-009-8467-7. doi: 10.1007/978-94-009-8467-7_1.</p>
<p>Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong Li, and Li Deng. Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads. arXiv:1606.03667 [cs], September 2016. URL http://arxiv.org/abs/1606.03667. arXiv: 1606.03667 .</p>
<p>Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech M. Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, and Phil Blunsom. Grounded language learning in a simulated 3d world. ArXiv, abs/1706.06551, 2017.</p>
<p>Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text. arXiv:2005.09382 [cs], May 2020. arXiv: 2005.09382.</p>
<p>Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022a.</p>
<p>Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. ArXiv, abs/2207.05608, 2022b.</p>
<p>Peter A. Jansen. A Systematic Survey of Text Worlds as Embodied Natural Language Environments. arXiv:2107.04132 [cs], July 2021. arXiv: 2107.04132.</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi (Jim) Fan. Vima: General robot manipulation with multimodal prompts. ArXiv, abs/2210.03094, 2022.</p>
<p>Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.</p>
<p>Shuang Li, Xavier Puig, Yilun Du, Clinton Jia Wang, Ekin Akyürek, Antonio Torralba, Jacob Andreas, and Igor Mordatch. Pre-trained language models for interactive decision-making. ArXiv, abs/2202.01771, 2022.
J. Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. ArXiv, abs/2209.07753, 2022.</p>
<p>Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UNIFIED-IO: A Unified Model for Vision, Language, and Multi-modal Tasks. February 2023. URL https://openreview.net/forum?id=E01k9048soZ.</p>
<p>Yao Lu, Karol Hausman, Yevgen Chebotar, Mengyuan Yan, Eric Jang, Alexander Herzog, Ted Xiao, Alex Irpan, Mohi Khansari, Dmitry Kalashnikov, and Sergey Levine. Aw-opt: Learning robotic skills with imitation and reinforcement at scale. In Conference on Robot Learning, 2021.</p>
<p>Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob N. Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning informed by natural language. In International Joint Conference on Artificial Intelligence, 2019.</p>
<p>Kyle Mahowald, Anna A. Ivanova, Idan Asher Blank, Nancy G. Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. Dissociating language and thought in large language models: a cognitive perspective. ArXiv, abs/2301.06627, 2023.</p>
<p>Suvir Mirchandani, Siddharth Karamcheti, and Dorsa Sadigh. Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</p>
<p>Dipendra Kumar Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In Conference on Empirical Methods in Natural Language Processing, 2017.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.</p>
<p>Roma Patel and Elizabeth-Jane Pavlick. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations, 2022.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In arXiv preprint arXiv:2103.00020v1, 2021.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. ArXiv, abs/2112.11446, 2021.</p>
<p>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. ArXiv, abs/2210.01241, 2022.</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021.</p>
<p>Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement learning? ArXiv, abs/2201.12122, 2022.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo G. Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, Mar'ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux,</p>
<p>Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L’opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Févry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall'ee, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie N'ev'eol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, Lívia Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Modupe Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully A. Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, María Andrea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, R. Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th'eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100, 2022.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. ArXiv, abs/2010.03768, 2020.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. ArXiv, abs/2009.01325, 2020.</p>
<p>Shiro Takagi. On the effect of pre-training for transformer in different modality on offline reinforcement learning. ArXiv, abs/2211.09817, 2022.</p>
<p>Serge Thill, Sebastia Padó n, and Tom Ziemke. On the importance of a rich embodiment in the grounding of concepts: Perspectives from embodied cognitive science and computational linguistics. Topics in Cognitive Science, 6(3):545-558, 2014. doi: https://doi.org/10.1111/tops. 12093 .</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). ArXiv, abs/2206.10498, 2022.</p>
<p>Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? ArXiv, abs/2203.07540, 2022.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022.</p>
<p>Peratham Wiriyathammabhum, Douglas Summers-Stay, Cornelia Fermüller, and Yiannis Aloimonos. Computer vision and natural language processing: Recent approaches in multimedia and robotics. ACM Comput. Surv., 49(4), dec 2016. ISSN 0360-0300. doi: 10.1145/3009906.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.</p>
<p>Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel C. F. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A new foundation model for computer vision. ArXiv, abs/2111.11432, 2021.</p>
<p>Qinqing Zheng, Amy Zhang, and Aditya Grover. Online Decision Transformer. In Proceedings of the 39th International Conference on Machine Learning, pages 27042-27059. PMLR, June 2022. URL https://proceedings.mlr.press/v162/zheng22c.html. ISSN: 2640-3498.</p>
<h1>Appendices</h1>
<p>This supplementary material provides additional results and discussion, as well as implementation details.</p>
<ul>
<li>Section A presents the BabyAI and BabyAI-Text environments.</li>
<li>Section B, contains several additional results. We report the per-task success rate at the end of the training (B.1) and how we have averaged the results over the tasks in B.2. We also analyze the influence of the observation's structure (i.e. either a symbolic image for the Symbolic-PPO agent or text for LLM based agents) in B.3. We then study the influence of pretraining in B. 4 and conduct several ablation tests to understand the influence of the size of the LLM (B.5), the impact of the size of the action space (B.6.1), and the effect of the number of distractors (B.6.2). Eventually, we verify in B. 7 the robustness of our method to domain-specific vocabulary.</li>
<li>Section C is a qualitative analysis of GFlan-T5 during its training on the environment with a mix of tasks. We plot the evolution of the distribution of actions during training for 11 prompts.</li>
<li>In Section D, we detail complementary tests for questions Q2 (D.2) and Q3 (D.3). We also analyze the functional grounding of temporal symbols "then" and "after" (D.4).</li>
<li>Section E gives details related to the distributed experimental setup.</li>
<li>Section F reports hyperparameters and implementation details used to finetune the models using PPO or Behavioral Cloning.</li>
<li>In Section G, we detail how the confidence intervals given in Figure 4 and Appendix D are obtained.</li>
<li>In Section H, we give the word substitutions used in our generalization experiments from sections 4.2 and 4.3.</li>
</ul>
<h2>A Environments</h2>
<p>We extend the BabyAI platform [Chevalier-Boisvert et al., 2019] and create a text-only version named BabyAI-Text that encapsulates BabyAI and returns linguistic observations. Figure 5 explains our environment.</p>
<h2>A. 1 BabyAI</h2>
<p>BabyAI Chevalier-Boisvert et al. [2019] is a language-conditioned environment where the agent has a limited number of steps to complete a language goal. This platform relies on a gridworld environment (MiniGrid) to generate a set of complex instructions-following environments. It has been specifically designed for research on grounded language learning and related sample efficiency problems. The gridworld environment is populated with the agent and objects (of 6 possible colors): boxes, balls, doors, and keys. These entities are placed in rooms of $8 \times 8$ tiles that are connected by doors that can be locked or closed. The grid is procedurally generated (i.e. objects populating an episode are randomly chosen and their position, as well as the agent's position, are also random). Some of the objects are useful for the task to achieve, while others are considered as distractors (objects can't be crossed, the agent has to either bypass them or move them). The agent can do 6 primitive actions: turn left, turn right, go forward, toggle, pick up to solve the language instruction (for instance Pick up the red box). To observe its environment, the agent has access to a partial view (i.e. it only sees the objects that belong to the $6 \times 6$ grid in front of it). BabyAI proposed to access this partial view through a symbolic mapping that returns 3 matrices of size $6 \times 6$. The first matrix contains which object is in the observed cells, the second gives the color of these objects, and the last one their state (e.g. locked, open). When the agent completes the task after $N$ steps, it receives the reward $r_{N}=1-0.9 \frac{N}{H}$, where $H$ is the maximum number of steps. During training, we multiply all rewards by 20 to ensure a good propagation of the rewards as per [Mirchandani et al., 2021]. If the agent has not completed the task in the current step, the reward is 0 . Additionally, BabyAI also provides visualization tools for experimenters to observe the grid and better grasp agents' behaviors.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: An illustration of how our BabyAI-Text environment encapsulates BabyAI. We keep the inner minigrid environment as well as task descriptions and reward but map the partial view of the agent to a text description.</p>
<h1>A. 2 BabyAI-Text</h1>
<p>BabyAI-Text is a textual environment that encapsulates BabyAI and provides a description of each observation instead of a symbolic representation. A textual description consists of a list of template descriptions with the following structure:</p>
<ul>
<li>"You see a $&lt;$ object $&gt;&lt;$ location $&gt;$ " if the object is a key, a ball, a box or a wall.</li>
<li>"You see a(n) open/closed door $&lt;$ location $&gt;$ " , if the agent sees a door.</li>
<li>"You carry a $&lt;$ object $&gt;$ ", if the agent carries an object.</li>
</ul>
<p>The $&lt;$ object $&gt;$, is composed of an adjective (among 6 possible colours: red, blue, green, yellow, grey, purple) and a noun (among 4 possible: key, door, box, ball). The $&lt;$ location $&gt;$ is given as the number of steps right, left, and or forward from the agent to the object. We illustrate this in the leftmost observation of Figure 5 where the "yellow box" is " 2 steps left and 1 step forward" from the agent (the red triangle). Thus an object described as " 1 step forward" is right in front of the agent that does not need to go forward if it wants to pick that object. Walls of the room are the only spatially extended objects in BabyAI-Text. We give their location at the closest distance to the agent. See the leftmost image of Figure 5 for an example where the agent sees a wall " 2 steps forward" and another wall " 2 steps left". All of the choices for describing the environment constitute what we call the geometry of the environment, that the agent has to ground in order to succeed in the task. The presence of a fine grained geometry (with distances in steps to the different object in the room) is one of the main differences from other textual games such as TextWorld or ScienceWorld where all objects in a room are not spatially described.
Thanks to this extension, BabyAI-Text resembles a TextWorld (i.e. provides text descriptions of the observation and executes text commands) while keeping the inner minigrid environment along with BabyAI's tasks and visualization tools. Moreover, as our extension simply provides an alternative mapping of observations, one can both use and compare agents that either expect text-only observations (with BabyAI-Text) or symbolic observations (with BabyAI).</p>
<h1>B Additional results</h1>
<h2>B. 1 Per-task success rate</h2>
<p>In order to get a better understanding of our agent's capabilities, we report in Figure 6 the success rate on each task from our "no-change" evaluation in Figure 4 (assessing the post-training performance of agents on 1000 test episodes of our mixed setup) of Flan-T5 and GFlan-T5. These results (with 4 seeds after 1.5 million training steps) show that our functional grounding leads GFlan-T5 to master the GoTo and PickUp tasks while improving results on PutNextTo and PickupThen/AfterPickup. However, GFlan-T5 has not found yet any robust strategy for the OpenDoor task (being the hardest as the agent must find the right key and discover that the action "toggle" opens the door) in the relatively short allocated time.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Per-task success rate for the 1000 evaluation trajectories performed in Figure 4.</p>
<h2>B. 2 Averaging success rate over the tasks</h2>
<p>In our environments, goals are divided into five types of tasks of varying difficulty. Each goal is sampled randomly from the tasks when the environment is reset. We obtain the success rate for the goals at update $u$ by averaging over the completed trajectories during the collection phase. As several environments run in parallel, goals from easier tasks that are completed more quickly, such as GoTo and PickUp, tend to be more represented in the buffer.</p>
<h2>B. 3 Textual vs symbolic representation</h2>
<p>In order to understand how the structure of the observation (i.e. either symbolic image using 3 matrices containing integers defining respectively the object seen, its color and property if any or text) influences the success rate of an RL agent, we compare the DRRN and Symbolic-PPO respectively trained on BabyAI-Text and BabyAI on the Go To Red Ball task. In this task, the agent has to go in front of a red ball in 1 room without any distractor (i.e. the task never changes, only the position of agent and red ball do). The task has been voluntarily chosen as trivial so that the main difference only comes from the way the information is given to the agent. Both the DRRN and Symbolic-PPO agents have a similar number of parameters ( 1 M ), they both use recurrent layers to deal with partial observability and use the canonical action space.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Average success rate for DRRN and Symbolic-PPO on the Go To Red Ball task with standard deviation over two random seeds. The PPO receives symbolic information and the DRRN gets textual observations.</p>
<p>Contrary to what one might assume in Figure 7 the PPO agent converges faster than the DRRN agent on this trivial task. Thus, symbolic observations make the learning easier for the agent. We conclude that even if language contains high-level information, understanding the link between spatial information and language is far more difficult than using symbolic information given in a matrix. Indeed, the matrix already contains a geometric bias favorable to the agent. We also want to point out that the DRRN is an off-policy RL method compared to PPO (which is on-policy) and that consequently, the DRRN was expected to be, by-design, more sample efficient.</p>
<h1>B. 4 Impact of pretraining</h1>
<p>We test how pretraining structured our LLM allowing for efficient finetuning. We vary which weights of Flan-T5 are kept pretrained as well as how we compute actions' probability (i.e. either using our method reusing language modeling heads or using new action heads with an MLP). We evaluate the performance of 5 models:</p>
<ul>
<li>The full LLM is pretrained and language modeling heads are used for actions probability: GFlan-T5 (Figure 8)</li>
<li>The full LLM is pretrained and new action heads are used: AFlan-T5 (Figure 9)</li>
<li>Only the embedding layer's weights are kept pretrained (the rest of the LLM is randomly initialized) and new action heads are used: NPAE-Flan-T5 (Figure 10)</li>
<li>Only the embedding layer's weights are kept pretrained (the rest of the LLM is randomly initialized) and the (randomly initialized) language modeling heads are used for actions' probability: NPEFlanT5 (Figure 11)</li>
<li>All LLM's weights are randomly initialized and action heads are used: NPA-Flan-T5 (Figure 12)</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ A table giving all the used synonyms is given in Appendix H. 3&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6599 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6599</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6599</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-4faf26ae9b3ec62c3bf49864cdd1c71037c5a60d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4faf26ae9b3ec62c3bf49864cdd1c71037c5a60d" target="_blank">Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> GandR is a retrieval procedure that retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction and sets the state of the art on multiple low-resource semantic parsing tasks.</p>
                <p><strong>Paper Abstract:</strong> A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandR first generates a preliminary prediction with input-based retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6599.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6599.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GandR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate-and-Retrieve</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step retrieval-augmented structured prediction method that first generates a preliminary prediction using input-similarity exemplars, then retrieves exemplars whose outputs are similar to that preliminary prediction (and whose inputs are similar) to produce a final parse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GandR (Generate-and-Retrieve)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A T5-based sequence-to-sequence agent augmented with an external exemplar memory: (1) generate a preliminary prediction using exemplars retrieved by input TF-IDF similarity; (2) compute a hybrid relevance score combining TF-IDF on inputs and TF-IDF between the preliminary predicted parse and candidate exemplar outputs; (3) retrieve top-K exemplars (K=4) according to this relevance score, append retrieved input-output pairs to the input, and generate the final parse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (sparse TF-IDF over training exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text training exemplars: input utterances and their serialized parses (intent/slot sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Sparse TF-IDF similarity search: relevance R_ij = (1-α) TF-IDF(x_i,x_j) + α TF-IDF(ĥy_i, y_j) where ĥy_i is the preliminary prediction; top-K retrieval (K=4) at inference; during training sampling from ranked list with geometric distribution p(1-p)^r; two-stage training uses M1 to produce preliminary predictions for M2.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task-oriented semantic parsing: MTOP (domain-bootstrapping, MTOP_1k, MTOP_25%), TOPv2 (Weather and Reminder low-resource domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>task-oriented semantic parsing / structured prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MTOP_boot (dev): 76.4% EM / 84.6% (dual number reported in table); MTOP_1k (test): 67.8% EM; MTOP_25%: 80.1% EM; TOPv2_Weather: 80.5% EM; TOPv2_Reminder: 71.7% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>T5 baseline (no retrieval): MTOP_boot: 72.9% / 83.3% ; MTOP_1k: 62.8% EM; MTOP_25%: 78.5% EM; TOPv2_Weather: 79.2% EM; TOPv2_Reminder: 68.8% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (%) (sequence exact-match between predicted and gold parse)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Using output similarity (hybrid retrieval) increases template recall and low-resource performance but requires tuning of the output weight α and number of exemplars K; output-similarity-only retrieval (output TF-IDF) can have higher template recall yet lower end-task performance, indicating a trade-off between retrieving matching templates and retrieving exemplars that are useful for the model. The paper notes no measured latency or memory-footprint numbers; two-stage training was used for convenience but a single-model approach is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Output similarity can sometimes distract the model away from lexically similar and informative exemplars, causing incorrect predictions (reported failure case). Retrieval augmentation yields little benefit in high-resource settings. Sensitivity to α (weighting of output similarity) and K (number of exemplars) is reported; output TF-IDF alone can increase template-recall but reduce final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, Fei Sha. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6599.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6599.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 + input TF-IDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 with input-based TF-IDF exemplar retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline retrieval-augmented agent that retrieves and appends training exemplars selected by TF-IDF similarity on the input utterance, then feeds the augmented input to a T5 seq2seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5 + input TF-IDF retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5 sequence-to-sequence model augmented at inference by concatenating K retrieved input-output exemplars chosen by TF-IDF similarity between the test input and training inputs; exemplars are appended to the model input and the model decodes the parse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (sparse TF-IDF over training exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text training exemplars (input utterances and their parses)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>TF-IDF similarity on inputs; top-K retrieval (K=4); during training sampling of exemplars uses geometric rank-based sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MTOP (domain-bootstrapping, MTOP_1k, MTOP_25%), TOPv2 (Weather and Reminder low-resource domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>task-oriented semantic parsing / structured prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MTOP_boot (dev): 74.9% / 84.5%; MTOP_1k: 67.2% EM; MTOP_25%: 79.4% EM; TOPv2_Weather: 79.9% EM; TOPv2_Reminder: 71.0% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>T5 baseline (no exemplars): MTOP_boot: 72.9% / 83.3%; MTOP_1k: 62.8% EM; MTOP_25%: 78.5% EM; TOPv2_Weather: 79.2% EM; TOPv2_Reminder: 68.8% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Input TF-IDF is a strong and simple baseline; less sensitive to retrieval method in high-resource settings. No explicit latency/memory numbers reported. Requires lexical overlap between input and exemplars to be effective; may be outperformed by hybrid retrieval in low-resource or label-sparse scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails when lexical overlap retrieves exemplars that are not label-relevant (example shown where lexical match 'musicals' retrieved a non-relevant exemplar), and is less effective than GandR where output-structure similarity is more informative.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6599.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6599.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>output TF-IDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output-space TF-IDF retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval method that uses TF-IDF similarity between the model's preliminary predicted parse (structure) and candidate exemplar outputs to select exemplars, ignoring input lexical similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Output TF-IDF retriever (used to augment T5)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieves exemplars by computing TF-IDF similarity between the predicted structure (preliminary prediction) and exemplar outputs (true parses), then appending those exemplars to the input for final decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (sparse TF-IDF over exemplar outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>serialized parse structures (intent/slot sequences) from the training set</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>TF-IDF similarity computed between preliminary predicted parse ĥy and candidate exemplar parse y_j; top-K retrieval (K=4).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MTOP (domain-bootstrapping) and TOPv2 low-resource domains (used for retrieval-quality analysis and template-recall measurement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>task-oriented semantic parsing / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Task-level exact-match performance numbers for output-only TF-IDF are not reported; retrieval-quality (template recall@K=4) reported: MTOP_boot recall@4 = 70.3%; TOPv2_Weather recall@4 = 74.8%; TOPv2_Reminder recall@4 = 53.7%. The paper notes output TF-IDF often has higher template-recall but lower final accuracy than GandR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Input TF-IDF template recall@4 for comparison: MTOP_boot = 35.9%; TOPv2_Weather = 55.1%; TOPv2_Reminder = 20.1%. Exact-match task performance without retrieval is the T5 baseline (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Template recall@K (proxy) and Exact Match (%) discussed qualitatively; template recall@4 reported as above.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Output-only retrieval increases template recall (i.e., finds exemplar with same template) but can yield lower end-task performance than hybrid retrieval, indicating a trade-off between matching template and exemplar informativeness. Requires a reasonable preliminary prediction to compute useful output similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Output-only retrieval can retrieve examples that match template but are not sufficiently informative for the model, and overall can underperform hybrid methods; depends on quality of preliminary prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6599.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6599.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CASPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controllable semantic parsing via retrieval augmentation (CASPER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior retrieval-augmented semantic parsing method that retrieves exemplars (reported in Pasupat et al., 2021) to improve parsing; cited and compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Controllable semantic parsing via retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CASPER (Pasupat et al., 2021) - retrieval-augmented semantic parser</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned prior work that augments semantic parsing by retrieving exemplars; in related work the retrieval used pretrained neural encoders to find input-similar exemplars and append them to the input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (neural-encoder based exemplar retrieval as described in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>input-output exemplar pairs (training samples), retrieved by encoded similarity</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>In the cited work retrieval was based on encodings from a pretrained sentence encoder (as reported in this paper's related work); exact retrieval mechanics not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MTOP (domain-bootstrapping) - reported for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>task-oriented semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in this paper (from Pasupat et al., 2021) for MTOP_boot: 73.3% / 83.9% (as presented in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (%) (as reported in this paper's comparison table)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not discussed in detail in this paper (only cited and compared).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here; see the original Pasupat et al. (2021) paper for details.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Controllable semantic parsing via retrieval augmentation <em>(Rating: 2)</em></li>
                <li>RETRONLU: retrieval augmented task-oriented semantic parsing <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Few-shot learning with retrieval augmented language models <em>(Rating: 2)</em></li>
                <li>Training data is more valuable than you think: A simple and effective method by retrieving from training data <em>(Rating: 2)</em></li>
                <li>Case-based reasoning for natural language queries over knowledge bases <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6599",
    "paper_id": "paper-4faf26ae9b3ec62c3bf49864cdd1c71037c5a60d",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "GandR",
            "name_full": "Generate-and-Retrieve",
            "brief_description": "A two-step retrieval-augmented structured prediction method that first generates a preliminary prediction using input-similarity exemplars, then retrieves exemplars whose outputs are similar to that preliminary prediction (and whose inputs are similar) to produce a final parse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GandR (Generate-and-Retrieve)",
            "agent_description": "A T5-based sequence-to-sequence agent augmented with an external exemplar memory: (1) generate a preliminary prediction using exemplars retrieved by input TF-IDF similarity; (2) compute a hybrid relevance score combining TF-IDF on inputs and TF-IDF between the preliminary predicted parse and candidate exemplar outputs; (3) retrieve top-K exemplars (K=4) according to this relevance score, append retrieved input-output pairs to the input, and generate the final parse.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented (sparse TF-IDF over training exemplars)",
            "memory_representation": "raw text training exemplars: input utterances and their serialized parses (intent/slot sequences)",
            "memory_access_mechanism": "Sparse TF-IDF similarity search: relevance R_ij = (1-α) TF-IDF(x_i,x_j) + α TF-IDF(ĥy_i, y_j) where ĥy_i is the preliminary prediction; top-K retrieval (K=4) at inference; during training sampling from ranked list with geometric distribution p(1-p)^r; two-stage training uses M1 to produce preliminary predictions for M2.",
            "task_name": "Task-oriented semantic parsing: MTOP (domain-bootstrapping, MTOP_1k, MTOP_25%), TOPv2 (Weather and Reminder low-resource domains)",
            "task_category": "task-oriented semantic parsing / structured prediction",
            "performance_with_memory": "MTOP_boot (dev): 76.4% EM / 84.6% (dual number reported in table); MTOP_1k (test): 67.8% EM; MTOP_25%: 80.1% EM; TOPv2_Weather: 80.5% EM; TOPv2_Reminder: 71.7% EM.",
            "performance_without_memory": "T5 baseline (no retrieval): MTOP_boot: 72.9% / 83.3% ; MTOP_1k: 62.8% EM; MTOP_25%: 78.5% EM; TOPv2_Weather: 79.2% EM; TOPv2_Reminder: 68.8% EM.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (%) (sequence exact-match between predicted and gold parse)",
            "tradeoffs_reported": "Using output similarity (hybrid retrieval) increases template recall and low-resource performance but requires tuning of the output weight α and number of exemplars K; output-similarity-only retrieval (output TF-IDF) can have higher template recall yet lower end-task performance, indicating a trade-off between retrieving matching templates and retrieving exemplars that are useful for the model. The paper notes no measured latency or memory-footprint numbers; two-stage training was used for convenience but a single-model approach is possible.",
            "limitations_or_failure_cases": "Output similarity can sometimes distract the model away from lexically similar and informative exemplars, causing incorrect predictions (reported failure case). Retrieval augmentation yields little benefit in high-resource settings. Sensitivity to α (weighting of output similarity) and K (number of exemplars) is reported; output TF-IDF alone can increase template-recall but reduce final accuracy.",
            "citation": "Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, Fei Sha. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing.",
            "uuid": "e6599.0",
            "source_info": {
                "paper_title": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "T5 + input TF-IDF",
            "name_full": "T5 with input-based TF-IDF exemplar retrieval",
            "brief_description": "Baseline retrieval-augmented agent that retrieves and appends training exemplars selected by TF-IDF similarity on the input utterance, then feeds the augmented input to a T5 seq2seq model.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "T5 + input TF-IDF retrieval",
            "agent_description": "T5 sequence-to-sequence model augmented at inference by concatenating K retrieved input-output exemplars chosen by TF-IDF similarity between the test input and training inputs; exemplars are appended to the model input and the model decodes the parse.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented (sparse TF-IDF over training exemplars)",
            "memory_representation": "raw text training exemplars (input utterances and their parses)",
            "memory_access_mechanism": "TF-IDF similarity on inputs; top-K retrieval (K=4); during training sampling of exemplars uses geometric rank-based sampling.",
            "task_name": "MTOP (domain-bootstrapping, MTOP_1k, MTOP_25%), TOPv2 (Weather and Reminder low-resource domains)",
            "task_category": "task-oriented semantic parsing / structured prediction",
            "performance_with_memory": "MTOP_boot (dev): 74.9% / 84.5%; MTOP_1k: 67.2% EM; MTOP_25%: 79.4% EM; TOPv2_Weather: 79.9% EM; TOPv2_Reminder: 71.0% EM.",
            "performance_without_memory": "T5 baseline (no exemplars): MTOP_boot: 72.9% / 83.3%; MTOP_1k: 62.8% EM; MTOP_25%: 78.5% EM; TOPv2_Weather: 79.2% EM; TOPv2_Reminder: 68.8% EM.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (%)",
            "tradeoffs_reported": "Input TF-IDF is a strong and simple baseline; less sensitive to retrieval method in high-resource settings. No explicit latency/memory numbers reported. Requires lexical overlap between input and exemplars to be effective; may be outperformed by hybrid retrieval in low-resource or label-sparse scenarios.",
            "limitations_or_failure_cases": "Fails when lexical overlap retrieves exemplars that are not label-relevant (example shown where lexical match 'musicals' retrieved a non-relevant exemplar), and is less effective than GandR where output-structure similarity is more informative.",
            "citation": "here",
            "uuid": "e6599.1",
            "source_info": {
                "paper_title": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "output TF-IDF",
            "name_full": "Output-space TF-IDF retrieval",
            "brief_description": "A retrieval method that uses TF-IDF similarity between the model's preliminary predicted parse (structure) and candidate exemplar outputs to select exemplars, ignoring input lexical similarity.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Output TF-IDF retriever (used to augment T5)",
            "agent_description": "Retrieves exemplars by computing TF-IDF similarity between the predicted structure (preliminary prediction) and exemplar outputs (true parses), then appending those exemplars to the input for final decoding.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented (sparse TF-IDF over exemplar outputs)",
            "memory_representation": "serialized parse structures (intent/slot sequences) from the training set",
            "memory_access_mechanism": "TF-IDF similarity computed between preliminary predicted parse ĥy and candidate exemplar parse y_j; top-K retrieval (K=4).",
            "task_name": "MTOP (domain-bootstrapping) and TOPv2 low-resource domains (used for retrieval-quality analysis and template-recall measurement)",
            "task_category": "task-oriented semantic parsing / retrieval",
            "performance_with_memory": "Task-level exact-match performance numbers for output-only TF-IDF are not reported; retrieval-quality (template recall@K=4) reported: MTOP_boot recall@4 = 70.3%; TOPv2_Weather recall@4 = 74.8%; TOPv2_Reminder recall@4 = 53.7%. The paper notes output TF-IDF often has higher template-recall but lower final accuracy than GandR.",
            "performance_without_memory": "Input TF-IDF template recall@4 for comparison: MTOP_boot = 35.9%; TOPv2_Weather = 55.1%; TOPv2_Reminder = 20.1%. Exact-match task performance without retrieval is the T5 baseline (see other entries).",
            "has_comparative_results": true,
            "performance_metric": "Template recall@K (proxy) and Exact Match (%) discussed qualitatively; template recall@4 reported as above.",
            "tradeoffs_reported": "Output-only retrieval increases template recall (i.e., finds exemplar with same template) but can yield lower end-task performance than hybrid retrieval, indicating a trade-off between matching template and exemplar informativeness. Requires a reasonable preliminary prediction to compute useful output similarity.",
            "limitations_or_failure_cases": "Output-only retrieval can retrieve examples that match template but are not sufficiently informative for the model, and overall can underperform hybrid methods; depends on quality of preliminary prediction.",
            "uuid": "e6599.2",
            "source_info": {
                "paper_title": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "CASPER",
            "name_full": "Controllable semantic parsing via retrieval augmentation (CASPER)",
            "brief_description": "A prior retrieval-augmented semantic parsing method that retrieves exemplars (reported in Pasupat et al., 2021) to improve parsing; cited and compared in this paper.",
            "citation_title": "Controllable semantic parsing via retrieval augmentation",
            "mention_or_use": "mention",
            "agent_name": "CASPER (Pasupat et al., 2021) - retrieval-augmented semantic parser",
            "agent_description": "Mentioned prior work that augments semantic parsing by retrieving exemplars; in related work the retrieval used pretrained neural encoders to find input-similar exemplars and append them to the input.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented (neural-encoder based exemplar retrieval as described in the cited work)",
            "memory_representation": "input-output exemplar pairs (training samples), retrieved by encoded similarity",
            "memory_access_mechanism": "In the cited work retrieval was based on encodings from a pretrained sentence encoder (as reported in this paper's related work); exact retrieval mechanics not detailed here.",
            "task_name": "MTOP (domain-bootstrapping) - reported for comparison",
            "task_category": "task-oriented semantic parsing",
            "performance_with_memory": "Reported in this paper (from Pasupat et al., 2021) for MTOP_boot: 73.3% / 83.9% (as presented in Table 1).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Exact Match (%) (as reported in this paper's comparison table)",
            "tradeoffs_reported": "Not discussed in detail in this paper (only cited and compared).",
            "limitations_or_failure_cases": "Not discussed here; see the original Pasupat et al. (2021) paper for details.",
            "citation": "Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021).",
            "uuid": "e6599.3",
            "source_info": {
                "paper_title": "Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Controllable semantic parsing via retrieval augmentation",
            "rating": 2
        },
        {
            "paper_title": "RETRONLU: retrieval augmented task-oriented semantic parsing",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Few-shot learning with retrieval augmented language models",
            "rating": 2
        },
        {
            "paper_title": "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
            "rating": 2
        },
        {
            "paper_title": "Case-based reasoning for natural language queries over knowledge bases",
            "rating": 1
        }
    ],
    "cost": 0.014492999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing</h1>
<p>Yury Zemlyanskiy ${ }^{<em> \dagger}$ Michiel de Jong ${ }^{</em> \dagger}$ Joshua Ainslie ${ }^{\ddagger}$ Panupong Pasupat ${ }^{\ddagger}$ Peter Shaw ${ }^{\ddagger}$ Linlu Qiu ${ }^{\ddagger}$ Sumit Sanghai ${ }^{\ddagger}$ Fei Sha ${ }^{\ddagger}$<br>${ }^{\dagger}$ University of Southern California ${ }^{\ddagger}$ Google Research<br>{yury.zemlyanskiy,msdejong}@usc.edu<br>{jainslie,ppasupat, petershaw,linluqiu, sumitsanghai,fsha}@google.com</p>
<h4>Abstract</h4>
<p>A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandR first generates a preliminary prediction with inputbased retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.</p>
<h2>1 Introduction</h2>
<p>A common and successful approach to structured prediction problems (Li et al., 2021; Chen et al., 2020) is to treat the gold structure as a sequence and fine-tune a sequence-to-sequence model such as T5 (Raffel et al., 2020) or BART (Lewis et al., 2020). However, the performance of fine-tuned models suffers in low resource scenarios where available training data is limited relative to the complexity of the task (Chen et al., 2020).</p>
<p>Existing work (Pasupat et al., 2021; Gupta et al., 2021; Wang et al., 2022) has found that retrieving related training samples, denoted exemplars, and appending the retrieved input-output pairs to the sample input before processing the sample can improve performance in low resource settings. In principle, all information from exemplars is available to the model during training and could be stored in model parameters. However, in practice the model</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>may not successfully retain all information, and reminding the model of salient input-output patterns at test time appears to help.</p>
<p>That raises the question: what exemplars are most informative for the model? Existing work focuses on retrieving exemplars for which the input is partially similar to the test input, effectively answering "What is the output for similar inputs?". In this work we explore whether there is complementary information in exemplars that answer the inverse question, "What is the input for similar outputs?".</p>
<p>We propose Generate-and-Retrieve (GandR), a method to retrieve exemplars with similar output as well as input. As the true output of a sample is in general unknown, GandR proceeds in two steps. First, a preliminary prediction is generated using retrievals with similar input only. Then, a new set of exemplars is retrieved based on a relevance measure that balances the similarity of the inputs and the similarity of the preliminary prediction and the exemplar output. Figure 1 provides an overview of the method.</p>
<p>We evaluate GandR in the setting of taskoriented semantic parsing, a core component of widely used virtual assistants. We show that similarity in output space provides a complementary signal to input similarity, yielding retrievals that prove more informative for the model. Moreover, for many structured prediction tasks the output space is more structured than the free-form input text, so that simple, non-learned distance measures work well for outputs even when inputs are lexically dissimilar. Table 5 demonstrates an example where our proposed similarity function retrieves an example that is somewhat less similarly phrased but with more similar output, and the model produces a better prediction as a result. Finally, the model has the opportunity to verify that its preliminary predictions are valid outputs in the target language.</p>
<p>The proposed method strongly improves performance in low-resource settings for semantic</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of GandR. First, GandR generates a preliminary prediction using an input augmented with exemplars with similar inputs. Then, GandR retrieves exemplars based on a relevance measure balancing input similarity and similarity between the preliminary prediction and exemplar outputs, and generates a final prediction based on these exemplars.
parsing, achieving state of the art results for lowresource and transfer benchmarks in MTOP (Li et al., 2021) and TopV2 (Chen et al., 2020).</p>
<h2>2 Method</h2>
<p>We approach semantic parsing as a conditional language generation task and apply a T5 sequence-tosequence model (Raffel et al., 2020) to predict a parse $y$ given a query $x$. For each sample, we retrieve $K=4$ relevant training exemplars sampled according to a relevance scoring function. We append the retrieved input-output pairs to the sample input and apply the T5 model to the augmented input to predict a parse output. In particular, let $\left(x_{1}^{\prime}, y_{1}^{\prime}\right), \ldots,\left(x_{K}^{\prime}, y_{K}^{\prime}\right)$ denote the retrieved inputoutput pairs, then the augmented input is</p>
<p>$$
x^{\prime}=x \quad\left|\begin{array}{llllll}
x_{1}^{\prime} &amp; \&amp; y_{1}^{\prime} &amp; \left|\mid x_{2}^{\prime} \&amp; y_{2}^{\prime}\right| &amp; \ldots
\end{array}\right.
$$</p>
<p>Our approach closely follows that of Pasupat et al. (2021), differing primarily in the choice of relevance function. During evaluation we retrieve the top $K$ most relevant exemplars. During training, we sample retrievals according to a geometric distribution over the relevance score rank. In particular, the probability that we retrieve an exemplar is given by $p(1-p)^{r}$ where $r$ is the rank of the exemplar according to relevance score and $p$ is a temperature hyperparameter.</p>
<p>In Pasupat et al. (2021), the relevance score is given by the inner product of Universal Sentence Encoder (Cer et al., 2018) encodings of the candidate input and the sample input. We found that a simple TF-IDF (Ramos et al., 2003) similarity baseline achieves comparable or better results.</p>
<p>Our proposed approach, GandR, builds on the input-similarity baseline by constructing a hybrid
similarity measure that takes into account not only the similarity between sample and candidate inputs, but also the similarity between the sequence predicted by the model and the candidate output. See Figure 1 for an overview. First, GandR generates a preliminary prediction using an input augmented with exemplars with similar inputs. Then, GandR retrieves exemplars based on a hybrid similarity measure over inputs and outputs, and generates a final prediction based on these exemplars.</p>
<p>Specifically, let $\hat{y}<em j="j">{i}$ be preliminary prediction, then the proposed output similarity between samples $i$ and $j$ is given by the TF-IDF similarity between the predicted structure (in our case, the set of intents and slots) and the structure of the true parse $y</em>$. Our proposed relevance score is a weighted sum of input and output similarity (see Appendix A for detailed description on how output similarity is computed):
$R_{i j}=(1-\alpha) \operatorname{TF}-\operatorname{IDF}\left(x_{i}, x_{j}\right)+\alpha \operatorname{TF}-\operatorname{IDF}\left(\hat{y}<em j="j">{i}, y</em>\right)$</p>
<h3>2.1 Training</h3>
<p>For simplicity, we train GandR in two stages. We start training with TF-IDF input relevance scoring, yielding model $M_{1}$. Model $M_{1}$ is used to generate GandR preliminary predictions during training and evaluation. We continue training $M_{1}$ for the remaining training steps, yielding $M_{2}$, which is used to generate final predictions augmented with retrievals from $M_{1}$. Note that this two-stage training is for convenience only, and it is possible to use a single set of weights $M_{\text {single }}$ to generate preliminary and final GandR predictions. In that case, $M_{\text {single }}$ needs to be trained with a mix of inputonly and GandR retrieval augmentations to ensure it is able to use either effectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">MTOP $_{\text {boot }}$</th>
<th style="text-align: left;">MTOP $_{1 \mathrm{k}}$</th>
<th style="text-align: left;">MTOP $_{25 \%}$</th>
<th style="text-align: center;">TOPv2 $_{W}$</th>
<th style="text-align: center;">TOPv2 $_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reptile (Chen et al., 2020)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">70.5</td>
</tr>
<tr>
<td style="text-align: left;">RAF (Shrivastava et al., 2022)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">CASPER (Pasupat et al., 2021)</td>
<td style="text-align: left;">$73.3 / 83.9$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">$72.9 / 83.3$</td>
<td style="text-align: left;">62.8</td>
<td style="text-align: left;">78.5</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">T5 with input TF-IDF</td>
<td style="text-align: left;">$74.9 / 84.5$</td>
<td style="text-align: left;">67.2</td>
<td style="text-align: left;">79.4</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: left;">GandR</td>
<td style="text-align: left;">$\mathbf{7 6 . 4 / 8 4 . 6}$</td>
<td style="text-align: left;">$\mathbf{6 7 . 8}$</td>
<td style="text-align: left;">$\mathbf{8 0 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on semantic parsing benchmarks. We report the percentage exact match between true and predicted labels as sequences. Results are on test set for all benchmarks except MTOP ${ }_{\text {boot }}$, where we report on dev to remain comparable with CASPER.</p>
<h2>3 Related Work</h2>
<p>Sequence-to-sequence models (Raffel et al., 2020; Lewis et al., 2020) have achieved state-of-the-art performance on task-oriented semantic parsing (Li et al., 2021; Chen et al., 2020; Aghajanyan et al., 2020) as well as other structured prediction tasks (Raffel et al., 2020). The general approach is to pre-train on language modeling and perform fine-tuning on the specific domain of interest.</p>
<p>Several works augment the input with retrieved exemplars from the training data, with differing methods for selecting informative examples. Pasupat et al. (2021) and Gupta et al. (2021) retrieve exemplars with similar input encodings from a pretrained neural encoder, evaluating on semantic parsing. Wang et al. (2022) retrieves examplars for which the input has high BM25 similarity with the sample input, with good performance on language generation. We adopt a similar approach with TFIDF similarity as a baseline for semantic parsing.</p>
<p>Black et al. (2021) and Das et al. (2021) learn dense retrievers in the spirit of Karpukhin et al. (2020), providing another path to incorporate label information for retrieval. Izacard et al. (2022) proposes other methods to fine-tune a dense retriever. These approaches require training a separate model specifically for retrieval, possibly with additional learning signal. In contrast, we employ a sparse similarity measure over model predictions that are produced incidentally in the course of fine-tuning the main model.</p>
<p>Selecting relevant training exemplars is also important for in-context prompting (Liu et al., 2021b). Similar to related fine-tuning literature, work in this direction uses either a pre-trained (Gao et al., 2020) or fine-tuned (Liu et al., 2021a) sentence encoder to retrieve exemplars.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">#Train</th>
<th style="text-align: right;">#Dev</th>
<th style="text-align: right;">#Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MTOP</td>
<td style="text-align: right;">15667</td>
<td style="text-align: right;">2234</td>
<td style="text-align: right;">4385</td>
</tr>
<tr>
<td style="text-align: left;">MTOP $_{1 \mathrm{k}}$</td>
<td style="text-align: right;">1096</td>
<td style="text-align: right;">2234</td>
<td style="text-align: right;">4385</td>
</tr>
<tr>
<td style="text-align: left;">MTOP $_{25 \%}$</td>
<td style="text-align: right;">3916</td>
<td style="text-align: right;">2234</td>
<td style="text-align: right;">4385</td>
</tr>
<tr>
<td style="text-align: left;">TOPv2 $_{S}$</td>
<td style="text-align: right;">83703</td>
<td style="text-align: right;">11967</td>
<td style="text-align: right;">27336</td>
</tr>
<tr>
<td style="text-align: left;">TOPv2 $_{W}$</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">147</td>
<td style="text-align: right;">5682</td>
</tr>
<tr>
<td style="text-align: left;">TOPv2 $_{R}$</td>
<td style="text-align: right;">493</td>
<td style="text-align: right;">337</td>
<td style="text-align: right;">5767</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset statistics: the number of examples per dataset and split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MTOP</th>
<th style="text-align: center;">TOPv2 $_{S}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: left;">CASPER</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: left;">T5 input TF-IDF</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: left;">GandR</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on high-resource settings.</p>
<h2>4 Experiments</h2>
<h3>4.1 Setup</h3>
<p>We evaluate GandR and baselines on semantic parsing benchmarks MTOP (Li et al., 2021) and TOPv2 (Chen et al., 2020), focusing on low-resource and transfer settings. MTOP is a medium-sized semantic parsing dataset used in Pasupat et al. (2021), for which we evaluate on the domain bootstrapping setting in which one of the domains is limited to a very small amount of training data. We also evaluate on low-resource settings MTOP $<em 25="25" _="\%">{1 \mathrm{k}}$ and MTOP $</em>$, denoted}$ in which we randomly sample 1 k and $25 \%$ of training samples, respectively. TOPv2 is centered on transfer to low-resource domains: models are trained on a set of high resource-domains denoted as TOPv2 $_{S}$ and then fine-tuned on lowresource Weather and Reminder domains ${ }^{1</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on MTOP ${ }_{\text {boot }}$ development set as a function of output similarity weight $\alpha$.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance on the development set of MTOP $<em _mathrm_W="\mathrm{W">{1 \mathrm{k}}$ as a function of the number of retrieved exemplars $K$.
as TOPV2 ${ }</em>$. We show the sizes of datasets and splits in Table 2. See Appendix B for details on the training setup.}}$ and TOPV2 ${ }_{\mathrm{R}</p>
<h3>4.2 Main results</h3>
<p>The results of our primary experiments are shown in Table 1. We find that input TF-IDF is a strong baseline, rivaling or improving over prior work. Further, GandR retrieval outperforms all baselines, setting the state of the art on evaluated settings.</p>
<h3>4.3 Ablations and discussion</h3>
<p>Retrieval is less important for high-resource settings Table 3 shows results on the high-resource full MTOP and TOPv2 datasets. In higher-resource settings, augmenting the input with exemplars appears to be both less effective and less sensitive to retrieval method, with almost identical results among methods with and without retrieval for the highest resource TOPv2 dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Retriever</th>
<th style="text-align: center;">MTOP $_{\text {boot }}$</th>
<th style="text-align: center;">TOPv2 $_{W}$</th>
<th style="text-align: center;">TOPv2 $_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">input TF-IDF</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: left;">output TF-IDF</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">53.7</td>
</tr>
<tr>
<td style="text-align: left;">GandR</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">52.5</td>
</tr>
</tbody>
</table>
<p>Table 4: Template recall@K=4 on the development sets for MTOP $<em W="W">{\text {boot }}$, TOPv2 $</em>$.}$ and TOPv2 $_{R</p>
<p>Using hybrid similarity leads to better retrieval quality Figure 2 displays MTOP ${ }<em 1="1" _mathrm_k="\mathrm{k">{\text {boot }}$ performance as a function of TF-IDF output weight $\alpha$. The results demonstrate that input and output similarity signals are strongly complementary. See Figure 4 and Figure 5 in the Appendix for similar experiments on the MTOP ${ }</em>$ benchmarks.}}$ and MTOP $_{25 \%</p>
<p>Considering output similarity leads to higher template recall Following Pasupat et al. (2021), we compute template recall@ $K$ as a proxy metric for retrieval. This measure corresponds to the proportion of evaluation samples for which at least one of the top K retrievals has the same template (identical intents and slots) as the gold parse. Results show (Table 4) that considering output as well as input similarity increases template recall. We note that output TF-IDF has similar or higher template recall than GandR even though it has lower performance. Ultimately, template recall is only a proxy, and we are really interested in retrieval informativeness; GandR's performance shows that balancing input similarity and template recall leads to exemplars that are most helpful for the model.</p>
<p>Hybrid similarity helps across different numbers of retrieved exemplars Figure 3 shows that GandR outperforms input TF-IDF similarity on MTOP $<em 25="25" _="\%">{1 \mathrm{k}}$ when we retrieve different number of exemplars: 1, 2 or 4 . See Figure 6 in the Appendix for a similar experiment on the MTOP $</em>$ benchmark.</p>
<h3>4.4 Error analysis</h3>
<p>The primary motivation for GandR is that hybrid similarity leads to more informative exemplars. Informativeness can only be objectively measured through model performance, but our motivating intuition appears to be borne out by samples in the data. We observe a number of different cases for which output or hybrid-similarity retrieval can help. Table 5 shows an example of a case for which input TF-IDF retrieves an irrelevant example with lexical overlap, while GandR retrieves an example with both lexical and parse overlap, leading to a</p>
<p>Input sample
$x$ : Could you connect me to the Musicals group
y: [IN:CREATE_CALL [SL:GROUP Musicals] ]
Training sample with similar input
$x_{1}$ : musicals in windham this weekend
$y_{1}$ : [IN:GET_EVENT [SL:CATEGORY_EVENT musicals ] [SL:LOCATION windham ]
[SL:DATE_TIME this weekend ] ]
$\dot{y}$ : [IN:CREATE_CALL [SL:CONTACT me]
[SL:GROUP Musicals] ]
Training sample with similar input and label
$x_{1}$ : can you please send text to the
development group
$y_{1}$ : [IN:SEND_MESSAGE [SL:GROUP
development]]
$\dot{y}$ : [IN:CREATE_CALL [SL:GROUP Musicals] ]
Table 5: Input TF-IDF retrieves an exemplar with lexical overlap ('musicals') that is not relevant to the sample. The GandR retrieval balances lexical and label similarity and leads to a correct prediction. Single representative exemplar out of 4 displayed for each method. See Table 9 in the Appendix for all retrieved exemplars.
correct prediction. Using preliminary predictions for retrieval can also allow the model to verify whether its predictions are correct. A common simple case when this can help is if the model generates a prediction that is dissimilar to any samples in the training set in which case the model may reconsider whether that prediction is correct (Table 7). Considering output similarity does come with tradeoffs. Table 8 demonstrates a situation where output similarity distracts the model away from a lexically similar and informative exemplar and the model is wrong as a result.</p>
<h2>5 Conclusion</h2>
<p>We propose GandR, a new method for structured prediction that generates a preliminary prediction, retrieves training exemplars with similar outputs (and similar inputs), and augments the input with the retrieved exemplars to generate a final prediction. We demonstrate that using output similarity yields improvements for semantic parsing in lowresource settings, achieving state of the art results on several semantic parsing benchmarks.</p>
<h2>Acknowledgments</h2>
<p>We thank William Cohen, Nicholas Fitzgerald and Luke Vilnis for insightful discussions and reviewers for their feedback. This work is partially supported by NSF Awards IIS-1513966/ 1632803/1833137, CCF-1139148, DARPA Awards#: FA8750-18-2-0117, FA8750-19-1-0504, DARPA-D3M - Award UCB-00009528, Google Research Awards, gifts from Facebook and Netflix, and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Michael Haeger, Haoran Li, Yashar Mehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis, and Sonal Gupta. 2020. Conversational semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5026-5035. Association for Computational Linguistics.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. 58 .</p>
<p>Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder. CoRR, abs/1803.11175.</p>
<p>Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, and Sonal Gupta. 2020. Low-resource domain adaptation for compositional task-oriented semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5090-5100. Association for Computational Linguistics.</p>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Case-based reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 9594-9611. Association for Computational Linguistics.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Vivek Gupta, Akshat Shrivastava, Adithya Sagar, Armen Aghajanyan, and Denis Savenkov. 2021. RETRONLU: retrieval augmented task-oriented semantic parsing. CoRR, abs/2109.10410.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6769-6781. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.</p>
<p>Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 2950-2962. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 76837698. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, volume 242, pages 29-48. Citeseer.</p>
<p>Akshat Shrivastava, Shrey Desai, Anchit Gupta, Ali Elkahky, Aleksandr Livshits, Alexander Zotov, and Ahmed Aly. 2022. Retrieve-and-fill for scenariobased task-oriented semantic parsing. CoRR, abs/2202.00901.</p>
<p>Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. 2022. Training data is more valuable than you think: A simple and effective method by retrieving from training data. CoRR, abs/2203.08773.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We are using 25 SPIS low resource split from Chen et al. (2020).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-786 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-786</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-786</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-68070067</p>
                <p><strong>Paper Title:</strong> Causal Discovery with Attention-Based Convolutional Neural Networks</p>
                <p><strong>Paper Abstract:</strong> : Having insight into the causal associations in a complex system facilitates decision making, e.g., for medical treatments, urban infrastructure improvements or ﬁnancial investments. The amount of observational data grows, which enables the discovery of causal relationships between variables from observation of their behaviour in time. Existing methods for causal discovery from time series data do not yet exploit the representational power of deep learning. We therefore present the Temporal Causal Discovery Framework (TCDF), a deep learning framework that learns a causal graph structure by discovering causal relationships in observational time series data. TCDF uses attention-based convolutional neural networks combined with a causal validation step. By interpreting the internal parameters of the convolutional networks, TCDF can also discover the time delay between a cause and the occurrence of its effect. Our framework learns temporal causal graphs, which can include confounders and instantaneous effects. Experiments on ﬁnancial and neuroscientiﬁc benchmarks show state-of-the-art performance of TCDF on discovering causal relationships in continuous time series data. Furthermore, we show that TCDF can circumstantially discover the presence of hidden confounders. Our broadly applicable framework can be used to gain novel insights into the causal dependencies in a complex system, which is important for reliable</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e786.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e786.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TCDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Causal Discovery Framework (TCDF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep‑learning framework that discovers temporal causal graphs from multivariate observational time series by training one attention‑based convolutional network per target series, interpreting attention and kernel weights, and validating candidate causes with an intervention‑like permutation test.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Temporal Causal Discovery Framework (TCDF)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TCDF trains N independent Attention‑based Dilated Depthwise Separable Temporal Convolutional Networks (AD‑DSTCNs), one per target time series, to predict each series from the past (and current) values of all series. Causal discovery proceeds in three algorithmic stages: (1) attention interpretation — each network learns a trainable attention vector over input channels and high attention scores identify potential causes; (2) causal validation — the Permutation Importance Validation Method (PIVM) permutes candidate cause time series (no retraining) and measures change in prediction loss to refute spurious associations; (3) delay discovery — the depthwise kernel weights per input channel are interpreted to estimate the time delay between cause and effect. AD‑DSTCNs use dilated causal convolutions (no future leakage), depthwise separable convolutions to keep channels separate (allowing per‑input kernel interpretation), residual connections, and a HardSoftmax semi‑binarization of attention scores to select potential causes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated multivariate temporal observational datasets (FINANCE and FMRI benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to simulated financial market time series (FINANCE benchmark: 25 time series, 4000 timesteps; datasets include many confounders) and simulated BOLD fMRI datasets (FMRI benchmark, various sizes and lengths). These are passive observational simulation environments (not interactive/active experimental platforms); TCDF uses data from these virtual labs to infer causal graphs but does not perform physical interventions in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Two‑stage approach: attention‑based variable selection to downweight irrelevant channels (soft attention followed by HardSoftmax thresholding) combined with Permutation Importance Validation (PIVM) to refute distractors/spurious correlates by breaking chronological structure and testing predictive impact.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measured confounders (shared causes), irrelevant variables/distractors, coincidental (spurious) correlations; partial handling of hidden (unmeasured) confounders (can detect some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Candidate distractors/spurious associations are detected first by attention scores (channels with high learned attention are considered potential causes). Hidden‑confounder indicators are inferred later if mutual instantaneous effects (zero delays) are detected between two variables. The PIVM detects spurious associations by measuring whether permuting a candidate cause's time series significantly degrades prediction loss.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Soft attention (Softmax) during training scales inputs; after training HardSoftmax thresholding zeros attention scores below a data-driven threshold (largest gap heuristic) to effectively downweight/ignore many irrelevant inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Permutation Importance Validation Method (PIVM): for each potential cause, randomly permute its time order in the input dataset, run the trained network (no retraining) and compare intervention loss L_I to ground loss L_G; a candidate is accepted as a true cause only if L_I increases sufficiently relative to the training improvement (threshold controlled by parameter s).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On the FINANCE benchmark (9 datasets) TCDF with attention+PIVM achieved macro‑averaged F1 = 0.64 ± 0.06 and F1' = 0.77 ± 0.08. PIVM substantially improved precision (reduced false positives) in FINANCE (see below). Delay discovery accuracy ~95–97% (FINANCE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>TCDF without the causal validation step (no PIVM; attention only) on FINANCE produced much worse performance: macro F1 = 0.22 ± 0.09 and F1' = 0.30 ± 0.13, i.e., a large drop in causal discovery accuracy and substantial increase in false positives. For FMRI benchmarks, PIVM had little effect (FINANCE contains many confounders; FMRI fewer).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Varied per dataset; FINANCE contains many confounders and correlated effects (special FINANCE_HIDDEN experiments hide exactly one series per dataset to test hidden confounder handling).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining attention‑based selection with a permutation‑based validation test is effective at distinguishing predictive correlations from putative causal influences in multivariate time series; PIVM greatly reduces false positives in datasets with many confounders (FINANCE). Attention alone can identify candidate causes but will select spurious correlates when shared causes exist; permutation validation is necessary to refute these. TCDF can sometimes detect the presence of an unmeasured confounder when it induces instantaneous mutual dependencies (both inferred delays = 0), but TCDF may incorrectly infer directed causation when a hidden confounder has unequal delays to its effects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e786.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIVM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Permutation Importance Validation Method (PIVM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal‑validation procedure that treats random permutation of a candidate cause's time ordering as a proxy intervention: if permuting the channel does not significantly degrade predictive loss, the candidate is rejected as a true cause.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Permutation Importance Validation Method (PIVM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a trained predictive model (AD‑DSTCN) and a set of potential causes (from attention), PIVM constructs for each candidate an intervened dataset by randomly permuting that candidate's time series (preserving marginal distribution but destroying chronology). The trained model is run on the permuted data (no retraining) to obtain intervention loss L_I. Compare L_I to ground loss L_G and to the initial (epoch 1) loss L_1 using the improvement ∆L_G = L_1G − L_EG and ∆L_I = L_1G − L_I; a candidate is accepted as causal only if ∆L_I ≤ s·∆L_G (user parameter s ∈ [0,1]), i.e., permuting substantially undid the model's learned improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used within TCDF on the FINANCE and FMRI simulated benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on pre‑collected multivariate time series datasets (passive observational simulation environments); functions as a post‑training validation step (not an environment that supports interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Refutation via permutation (intervention proxy) — removes chronological signal of a variable to test causal influence on target predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measured confounders producing spurious predictive correlations; irrelevant variables; coincidental temporal alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious associations by measuring the model's sensitivity to destruction of temporal order in the candidate cause: small change in prediction loss implies non‑causal/predictive redundancy; large change implies causal influence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Random permutation of candidate time series values (chronology destroyed) and statistical comparison of resulting prediction loss to baseline using a relative threshold based on training loss improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When applied in TCDF, PIVM raised FINANCE F1 from 0.22 to 0.64 (macro) — a large improvement in precision/overall F1 by removing false positives due to confounders. On FMRI datasets (fewer confounders), PIVM produced little change.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>See TCDF entry: without PIVM, attention alone led to many false positives (FINANCE F1 fell to 0.22).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Permutation of a candidate cause's chronology is an effective, model‑agnostic way to refute spurious predictive associations when the model relies on temporal structure; combining permutation validation with attention variable selection yields much higher causal precision on datasets with many confounders.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e786.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention+HardSoftmax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention mechanism with HardSoftmax semi‑binarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trainable soft attention vector over input channels used during CNN training to highlight informative time series, followed by a semi‑binarization (HardSoftmax) that zeros attention scores below a data‑driven threshold to select potential causes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Attention-based channel scoring with HardSoftmax thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Each AD‑DSTCN has a trainable attention vector a_j with one scalar per input channel; these scalars are Softmaxed during training (soft attention), yielding continuous weights that scale each input channel. After training, attention scores are semi‑binarized via a HardSoftmax rule: rank attention scores, find the largest gap in the sorted list (with additional heuristics: threshold ≥1, gap in first half, gap not first position), and set all scores below the threshold τ_j to zero — producing a sparse set of potential causes P_j.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used within TCDF on multivariate time series (FINANCE and FMRI benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to static observational datasets (simulated financial/fMRI time series). The attention operates as an internal model weighting mechanism rather than an active experimental selector.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Soft attention reduces influence of irrelevant inputs during training; HardSoftmax zeros low‑scoring channels to remove candidate distractors from further consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables/distractors and weak spurious correlations that do not produce strong learned attention.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Channels with low learned attention are detected and excluded by thresholding; potential causes are those with attention > τ_j.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Softmax scaling during training; hard zeroing below τ_j after training.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Used in combination with PIVM for refutation; attention alone does not refute spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When attention + HardSoftmax is combined with PIVM in TCDF, it yields high precision and F1 on FINANCE (see TCDF). Attention alone (without PIVM) selects potential causes but leaves many false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Attention only (no PIVM) yields low precision/high false positive rate (FINANCE F1 = 0.22) because spurious correlates can receive high attention.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Soft attention effectively identifies candidate predictive inputs; the HardSoftmax gap heuristic provides a principled way to sparsify attention outputs. However attention by itself cannot reliably distinguish causal from spurious predictive signals — it must be paired with a validation/refutation step like PIVM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e786.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCMCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PCMCI (time‑series PC‑based causal discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint‑based temporal causal discovery algorithm (time‑series adaptation of PC) that uses conditional independence tests (e.g., partial correlation) to infer directed dependencies and can estimate delays/causal strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PCMCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A temporal adaptation of PC (constraint‑based) that performs conditional independence tests (e.g., ParCorr for linear relations) to identify parents and uses moment‑based measures to estimate causal strength and lags; in the paper PCMCI was run with ParCorr tests, max lag set to 3, and significance optimized by AIC.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied to FINANCE and FMRI benchmarks (used as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for multivariate stationary time series; works on observational datasets (not an interactive lab).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>partially</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Conditioning on other variables via conditional independence tests reduces the influence of measured confounders (i.e., distinguishes direct from indirect effects) but PCMCI (as a PC variant) does not necessarily handle hidden/unmeasured confounders robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measured confounders/indirect effects, linear dependencies (with ParCorr); less capable for hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conditional (partial correlation) independence testing across lags to determine presence/absence of direct links and lags.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Testing and conditioning remove edges that become independent when conditioning on other lags/variables.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical independence tests determine whether putative edges remain after conditioning (no explicit permutation‑refutation used in this method).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On FINANCE PCMCI achieved macro F1 = 0.55 ± 0.22 and F1' = 0.56 ± 0.22; it recovered delays with high accuracy (~100% in FINANCE delay experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PCMCI (with ParCorr) is competitive on the tested benchmarks and accurately recovers delays, but it is sensitive to assumptions (stationarity, correct conditioning set) and may not detect hidden/unmeasured confounders; in the paper PCMCI produced fewer incorrect causal relationships in hidden‑confounder experiments than TCDF but it does not explicitly indicate the presence of hidden confounders.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e786.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>tsFCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>tsFCI (time‑series FCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal variant of the FCI algorithm that uses conditional independence tests to discover causal structure while allowing for latent (hidden) confounders by outputting partial ancestral graphs and special bidirected edges to indicate possible unmeasured confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>tsFCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Time‑series adaptation of FCI that runs a sequence of conditional independence tests across time lags and outputs edges (including special marks like X_i ↔ X_j) that indicate possible hidden confounding. In experiments tsFCI was run with max lag = 3 and p‑value cutoff = 0.01; majority rule was used to reduce conservatism.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Evaluated on FINANCE and FMRI benchmarks as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on multivariate stationary time series observational datasets; explicitly designed to report structures compatible with latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Constraint‑based independence testing with special graph marks to indicate unmeasured confounding (bidirected edges) — outputs conservative representations (PAG) that reflect uncertainty due to latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden/unmeasured confounders, measured confounders (via conditioning), indirect effects.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Independence tests and orientation rules that yield edge types indicating potential latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Conservative inference (PAG) rather than explicit downweighting; edges remaining after testing are kept but ambiguous/confounded relations are marked.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No permutation‑style refutation; relies on conditional independence testing logic to avoid asserting incorrect directed edges where confounding is suspected.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On FINANCE tsFCI achieved macro F1 = 0.37 ± 0.11 (F1' = 0.37 ± 0.12); in the paper tsFCI did not reliably detect hidden confounders in the FINANCE_HIDDEN case study and in some cases produced incorrect causal edges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>tsFCI is designed to acknowledge latent confounding via special edge annotations, but on the evaluated simulated benchmarks it was conservative and in practice did not consistently identify the hidden confounders tested in the case study; performance depended on choice of independence test and parameters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e786.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse+LowRank (S+L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse plus low‑rank network identification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system identification approach that models observed multivariate time series as generated by a sparse network of direct interactions plus a low‑rank latent factor component that explicitly models unobserved common causes (hidden confounders).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Sparse + Low‑Rank (S+L) network identification</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Identifies a decomposition of the system dynamics into a sparse component (direct interactions among observed variables) and a low‑rank component (effects of latent factors), thereby allowing identification of networks in the presence of hidden/unmeasured variables. Typically formulated within nonparametric or autoregressive identification frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned as a system‑identification solution for time series with hidden factors; not applied in paper experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for multivariate dynamical systems / time series where latent factors can be represented as a low‑rank influence; not an interactive/active experimental environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit latent factor modeling via a low‑rank component that captures shared variation due to unobserved confounders; sparse component isolates direct edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden/unmeasured confounders (latent factors) and indirect coupling mediated by those factors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Model decomposition: infer a low‑rank representation whose effects explain residual correlations not attributable to sparse direct links.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Separation of variance into low‑rank (latent) vs. sparse (direct) components reduces influence of confounding on inferred sparse edges.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>S+L approaches are a principled way to model and remove effects of unobserved common causes by splitting observed dynamics into sparse direct interactions and low‑rank latent effects; cited in the paper as an alternative to methods that cannot accommodate hidden confounders.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e786.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANLTSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Additive Nonlinear Time Series Model (ANLTSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint‑based approach that performs causal discovery in linear and nonlinear time series using additive model regression and statistical independence tests; can handle some hidden confounders under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ANLTSM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs causal discovery by fitting additive models and using tests based on regression residual independence to infer directed relations; can detect nonlinear relationships and, under certain assumptions, handle hidden confounders and instantaneous effects.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned in related work for temporal causal discovery (not used experimentally in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works on multivariate time series observational datasets (stationary assumption typical); not an active/interactive environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>partially</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses additive modeling and residual independence tests that can, under model assumptions, separate direct influences from spurious correlations due to confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measured confounders, some hidden confounders (under limited assumptions), nonlinear dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Independence testing of residuals from additive regression fits across variables/time lags.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ANLTSM is able to recover nonlinear causal relations and claims some robustness to hidden confounding in specific settings, but requires model assumptions and stationarity and may not scale well to high dimensions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e786.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalSignificance α(c,e)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Significance measure α(c,e)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that isolates and quantifies the effect of a specific candidate cause c on an effect e, estimating delay and impact but assuming linear additive relationships and that all causes are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal Significance α(c,e)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Computes a causal significance metric for a candidate cause/effect pair by isolating the statistical impact of c on e (including delay estimation) using linearity/additive assumptions; designed to quantify causal impact and estimate time delay.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned in related work for temporal causal discovery (not used experimentally in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Intended for multivariate time series with observed causes; requires linear/additive structure and full observation of causes.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measured confounders (method assumes all causes are observed — thus limited handling of spurious correlations due to unobserved confounders).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Isolation of candidate cause's contribution under linear/additive model assumptions and hypothesis testing.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides delay and impact estimates under linear/additive assumptions and can be robust when assumptions hold; experimentally sensitive when assumptions are violated.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e786.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e786.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Agnostic Model (SAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non‑temporal deep generative approach for causal graph reconstruction that uses learnable input scaling (attention‑like multiplicative scores) and adversarial/penalized losses to discover structure from iid data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Structural Agnostic Model (SAM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SAM uses neural nets and adversarial/penalized learning to reconstruct causal graphs from iid data; it uses multiplicative trainable scores on inputs (similar to attention) to weigh variables, but SAM does not perform a temporal causal validation step and is not designed for time series with temporal precedence constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned as a related deep‑learning based causal discovery approach (non‑temporal)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for i.i.d. data (non‑temporal) and thus not suitable for temporal precedence constrained datasets without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>partially</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses input scoring / sparsity penalties to suppress irrelevant inputs; no permutation‑style temporal refutation for time series.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables/distractors in i.i.d. settings; not designed for temporally induced spurious correlations/confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Learnable multiplicative input scores and adversarial/penalty objectives to encourage sparse causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Trainable input scores multiplied with inputs and sparsity regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SAM demonstrates that input‑wise multiplicative scores (attention‑like mechanisms) can serve as variable selection in deep causal discovery for i.i.d. data; the paper notes SAM's similarity to TCDF's attention but highlights that SAM lacks temporal validation for time series.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Detecting causal associations in large nonlinear time series datasets <em>(Rating: 2)</em></li>
                <li>On causal discovery from time series using FCI <em>(Rating: 2)</em></li>
                <li>Sparse plus low rank network identification: A nonparametric approach <em>(Rating: 1)</em></li>
                <li>Statistical inference for variable importance <em>(Rating: 2)</em></li>
                <li>Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-786",
    "paper_id": "paper-68070067",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "TCDF",
            "name_full": "Temporal Causal Discovery Framework (TCDF)",
            "brief_description": "A deep‑learning framework that discovers temporal causal graphs from multivariate observational time series by training one attention‑based convolutional network per target series, interpreting attention and kernel weights, and validating candidate causes with an intervention‑like permutation test.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Temporal Causal Discovery Framework (TCDF)",
            "method_description": "TCDF trains N independent Attention‑based Dilated Depthwise Separable Temporal Convolutional Networks (AD‑DSTCNs), one per target time series, to predict each series from the past (and current) values of all series. Causal discovery proceeds in three algorithmic stages: (1) attention interpretation — each network learns a trainable attention vector over input channels and high attention scores identify potential causes; (2) causal validation — the Permutation Importance Validation Method (PIVM) permutes candidate cause time series (no retraining) and measures change in prediction loss to refute spurious associations; (3) delay discovery — the depthwise kernel weights per input channel are interpreted to estimate the time delay between cause and effect. AD‑DSTCNs use dilated causal convolutions (no future leakage), depthwise separable convolutions to keep channels separate (allowing per‑input kernel interpretation), residual connections, and a HardSoftmax semi‑binarization of attention scores to select potential causes.",
            "environment_name": "Simulated multivariate temporal observational datasets (FINANCE and FMRI benchmarks)",
            "environment_description": "Applied to simulated financial market time series (FINANCE benchmark: 25 time series, 4000 timesteps; datasets include many confounders) and simulated BOLD fMRI datasets (FMRI benchmark, various sizes and lengths). These are passive observational simulation environments (not interactive/active experimental platforms); TCDF uses data from these virtual labs to infer causal graphs but does not perform physical interventions in the environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Two‑stage approach: attention‑based variable selection to downweight irrelevant channels (soft attention followed by HardSoftmax thresholding) combined with Permutation Importance Validation (PIVM) to refute distractors/spurious correlates by breaking chronological structure and testing predictive impact.",
            "spurious_signal_types": "Measured confounders (shared causes), irrelevant variables/distractors, coincidental (spurious) correlations; partial handling of hidden (unmeasured) confounders (can detect some cases).",
            "detection_method": "Candidate distractors/spurious associations are detected first by attention scores (channels with high learned attention are considered potential causes). Hidden‑confounder indicators are inferred later if mutual instantaneous effects (zero delays) are detected between two variables. The PIVM detects spurious associations by measuring whether permuting a candidate cause's time series significantly degrades prediction loss.",
            "downweighting_method": "Soft attention (Softmax) during training scales inputs; after training HardSoftmax thresholding zeros attention scores below a data-driven threshold (largest gap heuristic) to effectively downweight/ignore many irrelevant inputs.",
            "refutation_method": "Permutation Importance Validation Method (PIVM): for each potential cause, randomly permute its time order in the input dataset, run the trained network (no retraining) and compare intervention loss L_I to ground loss L_G; a candidate is accepted as a true cause only if L_I increases sufficiently relative to the training improvement (threshold controlled by parameter s).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On the FINANCE benchmark (9 datasets) TCDF with attention+PIVM achieved macro‑averaged F1 = 0.64 ± 0.06 and F1' = 0.77 ± 0.08. PIVM substantially improved precision (reduced false positives) in FINANCE (see below). Delay discovery accuracy ~95–97% (FINANCE).",
            "performance_without_robustness": "TCDF without the causal validation step (no PIVM; attention only) on FINANCE produced much worse performance: macro F1 = 0.22 ± 0.09 and F1' = 0.30 ± 0.13, i.e., a large drop in causal discovery accuracy and substantial increase in false positives. For FMRI benchmarks, PIVM had little effect (FINANCE contains many confounders; FMRI fewer).",
            "has_ablation_study": true,
            "number_of_distractors": "Varied per dataset; FINANCE contains many confounders and correlated effects (special FINANCE_HIDDEN experiments hide exactly one series per dataset to test hidden confounder handling).",
            "key_findings": "Combining attention‑based selection with a permutation‑based validation test is effective at distinguishing predictive correlations from putative causal influences in multivariate time series; PIVM greatly reduces false positives in datasets with many confounders (FINANCE). Attention alone can identify candidate causes but will select spurious correlates when shared causes exist; permutation validation is necessary to refute these. TCDF can sometimes detect the presence of an unmeasured confounder when it induces instantaneous mutual dependencies (both inferred delays = 0), but TCDF may incorrectly infer directed causation when a hidden confounder has unequal delays to its effects.",
            "uuid": "e786.0"
        },
        {
            "name_short": "PIVM",
            "name_full": "Permutation Importance Validation Method (PIVM)",
            "brief_description": "A causal‑validation procedure that treats random permutation of a candidate cause's time ordering as a proxy intervention: if permuting the channel does not significantly degrade predictive loss, the candidate is rejected as a true cause.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Permutation Importance Validation Method (PIVM)",
            "method_description": "Given a trained predictive model (AD‑DSTCN) and a set of potential causes (from attention), PIVM constructs for each candidate an intervened dataset by randomly permuting that candidate's time series (preserving marginal distribution but destroying chronology). The trained model is run on the permuted data (no retraining) to obtain intervention loss L_I. Compare L_I to ground loss L_G and to the initial (epoch 1) loss L_1 using the improvement ∆L_G = L_1G − L_EG and ∆L_I = L_1G − L_I; a candidate is accepted as causal only if ∆L_I ≤ s·∆L_G (user parameter s ∈ [0,1]), i.e., permuting substantially undid the model's learned improvement.",
            "environment_name": "Used within TCDF on the FINANCE and FMRI simulated benchmarks",
            "environment_description": "Operates on pre‑collected multivariate time series datasets (passive observational simulation environments); functions as a post‑training validation step (not an environment that supports interventions).",
            "handles_distractors": true,
            "distractor_handling_technique": "Refutation via permutation (intervention proxy) — removes chronological signal of a variable to test causal influence on target predictions.",
            "spurious_signal_types": "Measured confounders producing spurious predictive correlations; irrelevant variables; coincidental temporal alignments.",
            "detection_method": "Detects spurious associations by measuring the model's sensitivity to destruction of temporal order in the candidate cause: small change in prediction loss implies non‑causal/predictive redundancy; large change implies causal influence.",
            "downweighting_method": null,
            "refutation_method": "Random permutation of candidate time series values (chronology destroyed) and statistical comparison of resulting prediction loss to baseline using a relative threshold based on training loss improvement.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When applied in TCDF, PIVM raised FINANCE F1 from 0.22 to 0.64 (macro) — a large improvement in precision/overall F1 by removing false positives due to confounders. On FMRI datasets (fewer confounders), PIVM produced little change.",
            "performance_without_robustness": "See TCDF entry: without PIVM, attention alone led to many false positives (FINANCE F1 fell to 0.22).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Permutation of a candidate cause's chronology is an effective, model‑agnostic way to refute spurious predictive associations when the model relies on temporal structure; combining permutation validation with attention variable selection yields much higher causal precision on datasets with many confounders.",
            "uuid": "e786.1"
        },
        {
            "name_short": "Attention+HardSoftmax",
            "name_full": "Attention mechanism with HardSoftmax semi‑binarization",
            "brief_description": "A trainable soft attention vector over input channels used during CNN training to highlight informative time series, followed by a semi‑binarization (HardSoftmax) that zeros attention scores below a data‑driven threshold to select potential causes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Attention-based channel scoring with HardSoftmax thresholding",
            "method_description": "Each AD‑DSTCN has a trainable attention vector a_j with one scalar per input channel; these scalars are Softmaxed during training (soft attention), yielding continuous weights that scale each input channel. After training, attention scores are semi‑binarized via a HardSoftmax rule: rank attention scores, find the largest gap in the sorted list (with additional heuristics: threshold ≥1, gap in first half, gap not first position), and set all scores below the threshold τ_j to zero — producing a sparse set of potential causes P_j.",
            "environment_name": "Used within TCDF on multivariate time series (FINANCE and FMRI benchmarks)",
            "environment_description": "Applied to static observational datasets (simulated financial/fMRI time series). The attention operates as an internal model weighting mechanism rather than an active experimental selector.",
            "handles_distractors": true,
            "distractor_handling_technique": "Soft attention reduces influence of irrelevant inputs during training; HardSoftmax zeros low‑scoring channels to remove candidate distractors from further consideration.",
            "spurious_signal_types": "Irrelevant variables/distractors and weak spurious correlations that do not produce strong learned attention.",
            "detection_method": "Channels with low learned attention are detected and excluded by thresholding; potential causes are those with attention &gt; τ_j.",
            "downweighting_method": "Softmax scaling during training; hard zeroing below τ_j after training.",
            "refutation_method": "Used in combination with PIVM for refutation; attention alone does not refute spurious correlations.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When attention + HardSoftmax is combined with PIVM in TCDF, it yields high precision and F1 on FINANCE (see TCDF). Attention alone (without PIVM) selects potential causes but leaves many false positives.",
            "performance_without_robustness": "Attention only (no PIVM) yields low precision/high false positive rate (FINANCE F1 = 0.22) because spurious correlates can receive high attention.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Soft attention effectively identifies candidate predictive inputs; the HardSoftmax gap heuristic provides a principled way to sparsify attention outputs. However attention by itself cannot reliably distinguish causal from spurious predictive signals — it must be paired with a validation/refutation step like PIVM.",
            "uuid": "e786.2"
        },
        {
            "name_short": "PCMCI",
            "name_full": "PCMCI (time‑series PC‑based causal discovery)",
            "brief_description": "A constraint‑based temporal causal discovery algorithm (time‑series adaptation of PC) that uses conditional independence tests (e.g., partial correlation) to infer directed dependencies and can estimate delays/causal strengths.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "PCMCI",
            "method_description": "A temporal adaptation of PC (constraint‑based) that performs conditional independence tests (e.g., ParCorr for linear relations) to identify parents and uses moment‑based measures to estimate causal strength and lags; in the paper PCMCI was run with ParCorr tests, max lag set to 3, and significance optimized by AIC.",
            "environment_name": "Applied to FINANCE and FMRI benchmarks (used as a baseline comparison)",
            "environment_description": "Designed for multivariate stationary time series; works on observational datasets (not an interactive lab).",
            "handles_distractors": "partially",
            "distractor_handling_technique": "Conditioning on other variables via conditional independence tests reduces the influence of measured confounders (i.e., distinguishes direct from indirect effects) but PCMCI (as a PC variant) does not necessarily handle hidden/unmeasured confounders robustly.",
            "spurious_signal_types": "Measured confounders/indirect effects, linear dependencies (with ParCorr); less capable for hidden confounders.",
            "detection_method": "Conditional (partial correlation) independence testing across lags to determine presence/absence of direct links and lags.",
            "downweighting_method": "Testing and conditioning remove edges that become independent when conditioning on other lags/variables.",
            "refutation_method": "Statistical independence tests determine whether putative edges remain after conditioning (no explicit permutation‑refutation used in this method).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On FINANCE PCMCI achieved macro F1 = 0.55 ± 0.22 and F1' = 0.56 ± 0.22; it recovered delays with high accuracy (~100% in FINANCE delay experiment).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "PCMCI (with ParCorr) is competitive on the tested benchmarks and accurately recovers delays, but it is sensitive to assumptions (stationarity, correct conditioning set) and may not detect hidden/unmeasured confounders; in the paper PCMCI produced fewer incorrect causal relationships in hidden‑confounder experiments than TCDF but it does not explicitly indicate the presence of hidden confounders.",
            "uuid": "e786.3"
        },
        {
            "name_short": "tsFCI",
            "name_full": "tsFCI (time‑series FCI)",
            "brief_description": "A temporal variant of the FCI algorithm that uses conditional independence tests to discover causal structure while allowing for latent (hidden) confounders by outputting partial ancestral graphs and special bidirected edges to indicate possible unmeasured confounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "tsFCI",
            "method_description": "Time‑series adaptation of FCI that runs a sequence of conditional independence tests across time lags and outputs edges (including special marks like X_i ↔ X_j) that indicate possible hidden confounding. In experiments tsFCI was run with max lag = 3 and p‑value cutoff = 0.01; majority rule was used to reduce conservatism.",
            "environment_name": "Evaluated on FINANCE and FMRI benchmarks as a baseline",
            "environment_description": "Operates on multivariate stationary time series observational datasets; explicitly designed to report structures compatible with latent confounding.",
            "handles_distractors": true,
            "distractor_handling_technique": "Constraint‑based independence testing with special graph marks to indicate unmeasured confounding (bidirected edges) — outputs conservative representations (PAG) that reflect uncertainty due to latent variables.",
            "spurious_signal_types": "Hidden/unmeasured confounders, measured confounders (via conditioning), indirect effects.",
            "detection_method": "Independence tests and orientation rules that yield edge types indicating potential latent confounding.",
            "downweighting_method": "Conservative inference (PAG) rather than explicit downweighting; edges remaining after testing are kept but ambiguous/confounded relations are marked.",
            "refutation_method": "No permutation‑style refutation; relies on conditional independence testing logic to avoid asserting incorrect directed edges where confounding is suspected.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On FINANCE tsFCI achieved macro F1 = 0.37 ± 0.11 (F1' = 0.37 ± 0.12); in the paper tsFCI did not reliably detect hidden confounders in the FINANCE_HIDDEN case study and in some cases produced incorrect causal edges.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "tsFCI is designed to acknowledge latent confounding via special edge annotations, but on the evaluated simulated benchmarks it was conservative and in practice did not consistently identify the hidden confounders tested in the case study; performance depended on choice of independence test and parameters.",
            "uuid": "e786.4"
        },
        {
            "name_short": "Sparse+LowRank (S+L)",
            "name_full": "Sparse plus low‑rank network identification",
            "brief_description": "A system identification approach that models observed multivariate time series as generated by a sparse network of direct interactions plus a low‑rank latent factor component that explicitly models unobserved common causes (hidden confounders).",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Sparse + Low‑Rank (S+L) network identification",
            "method_description": "Identifies a decomposition of the system dynamics into a sparse component (direct interactions among observed variables) and a low‑rank component (effects of latent factors), thereby allowing identification of networks in the presence of hidden/unmeasured variables. Typically formulated within nonparametric or autoregressive identification frameworks.",
            "environment_name": "Mentioned as a system‑identification solution for time series with hidden factors; not applied in paper experiments",
            "environment_description": "Designed for multivariate dynamical systems / time series where latent factors can be represented as a low‑rank influence; not an interactive/active experimental environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit latent factor modeling via a low‑rank component that captures shared variation due to unobserved confounders; sparse component isolates direct edges.",
            "spurious_signal_types": "Hidden/unmeasured confounders (latent factors) and indirect coupling mediated by those factors.",
            "detection_method": "Model decomposition: infer a low‑rank representation whose effects explain residual correlations not attributable to sparse direct links.",
            "downweighting_method": "Separation of variance into low‑rank (latent) vs. sparse (direct) components reduces influence of confounding on inferred sparse edges.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "S+L approaches are a principled way to model and remove effects of unobserved common causes by splitting observed dynamics into sparse direct interactions and low‑rank latent effects; cited in the paper as an alternative to methods that cannot accommodate hidden confounders.",
            "uuid": "e786.5"
        },
        {
            "name_short": "ANLTSM",
            "name_full": "Additive Nonlinear Time Series Model (ANLTSM)",
            "brief_description": "A constraint‑based approach that performs causal discovery in linear and nonlinear time series using additive model regression and statistical independence tests; can handle some hidden confounders under assumptions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "ANLTSM",
            "method_description": "Performs causal discovery by fitting additive models and using tests based on regression residual independence to infer directed relations; can detect nonlinear relationships and, under certain assumptions, handle hidden confounders and instantaneous effects.",
            "environment_name": "Mentioned in related work for temporal causal discovery (not used experimentally in the paper)",
            "environment_description": "Works on multivariate time series observational datasets (stationary assumption typical); not an active/interactive environment.",
            "handles_distractors": "partially",
            "distractor_handling_technique": "Uses additive modeling and residual independence tests that can, under model assumptions, separate direct influences from spurious correlations due to confounding.",
            "spurious_signal_types": "Measured confounders, some hidden confounders (under limited assumptions), nonlinear dependencies.",
            "detection_method": "Independence testing of residuals from additive regression fits across variables/time lags.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "ANLTSM is able to recover nonlinear causal relations and claims some robustness to hidden confounding in specific settings, but requires model assumptions and stationarity and may not scale well to high dimensions.",
            "uuid": "e786.6"
        },
        {
            "name_short": "CausalSignificance α(c,e)",
            "name_full": "Causal Significance measure α(c,e)",
            "brief_description": "A framework that isolates and quantifies the effect of a specific candidate cause c on an effect e, estimating delay and impact but assuming linear additive relationships and that all causes are observed.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Causal Significance α(c,e)",
            "method_description": "Computes a causal significance metric for a candidate cause/effect pair by isolating the statistical impact of c on e (including delay estimation) using linearity/additive assumptions; designed to quantify causal impact and estimate time delay.",
            "environment_name": "Mentioned in related work for temporal causal discovery (not used experimentally in this paper)",
            "environment_description": "Intended for multivariate time series with observed causes; requires linear/additive structure and full observation of causes.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Measured confounders (method assumes all causes are observed — thus limited handling of spurious correlations due to unobserved confounders).",
            "detection_method": "Isolation of candidate cause's contribution under linear/additive model assumptions and hypothesis testing.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Provides delay and impact estimates under linear/additive assumptions and can be robust when assumptions hold; experimentally sensitive when assumptions are violated.",
            "uuid": "e786.7"
        },
        {
            "name_short": "SAM",
            "name_full": "Structural Agnostic Model (SAM)",
            "brief_description": "A non‑temporal deep generative approach for causal graph reconstruction that uses learnable input scaling (attention‑like multiplicative scores) and adversarial/penalized losses to discover structure from iid data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Structural Agnostic Model (SAM)",
            "method_description": "SAM uses neural nets and adversarial/penalized learning to reconstruct causal graphs from iid data; it uses multiplicative trainable scores on inputs (similar to attention) to weigh variables, but SAM does not perform a temporal causal validation step and is not designed for time series with temporal precedence constraints.",
            "environment_name": "Mentioned as a related deep‑learning based causal discovery approach (non‑temporal)",
            "environment_description": "Designed for i.i.d. data (non‑temporal) and thus not suitable for temporal precedence constrained datasets without adaptation.",
            "handles_distractors": "partially",
            "distractor_handling_technique": "Uses input scoring / sparsity penalties to suppress irrelevant inputs; no permutation‑style temporal refutation for time series.",
            "spurious_signal_types": "Irrelevant variables/distractors in i.i.d. settings; not designed for temporally induced spurious correlations/confounding.",
            "detection_method": "Learnable multiplicative input scores and adversarial/penalty objectives to encourage sparse causal graphs.",
            "downweighting_method": "Trainable input scores multiplied with inputs and sparsity regularization.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "SAM demonstrates that input‑wise multiplicative scores (attention‑like mechanisms) can serve as variable selection in deep causal discovery for i.i.d. data; the paper notes SAM's similarity to TCDF's attention but highlights that SAM lacks temporal validation for time series.",
            "uuid": "e786.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Detecting causal associations in large nonlinear time series datasets",
            "rating": 2,
            "sanitized_title": "detecting_causal_associations_in_large_nonlinear_time_series_datasets"
        },
        {
            "paper_title": "On causal discovery from time series using FCI",
            "rating": 2,
            "sanitized_title": "on_causal_discovery_from_time_series_using_fci"
        },
        {
            "paper_title": "Sparse plus low rank network identification: A nonparametric approach",
            "rating": 1,
            "sanitized_title": "sparse_plus_low_rank_network_identification_a_nonparametric_approach"
        },
        {
            "paper_title": "Statistical inference for variable importance",
            "rating": 2,
            "sanitized_title": "statistical_inference_for_variable_importance"
        },
        {
            "paper_title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "rating": 1,
            "sanitized_title": "algorithmic_transparency_via_quantitative_input_influence_theory_and_experiments_with_learning_systems"
        }
    ],
    "cost": 0.025107249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Causal Discovery with Attention-Based Convolutional Neural Networks
7 January 2019</p>
<p>Meike Nauta m.nauta@utwente.nl 0000-0002-0558-3810
Faculty of EEMCS
University of Twente
PO Box 2177500 AEEnschedeThe Netherlands</p>
<p>Doina Bucur d.bucur@utwente.nl 0000-0002-0558-3810
Faculty of EEMCS
University of Twente
PO Box 2177500 AEEnschedeThe Netherlands</p>
<p>Christin Seifert c.seifert@utwente.nl 
Faculty of EEMCS
University of Twente
PO Box 2177500 AEEnschedeThe Netherlands</p>
<p>Causal Discovery with Attention-Based Convolutional Neural Networks
7 January 2019AACDD896266D5D2928B7F6C628715E7010.3390/make1010019Received: 5 November 2018; Accepted: 27 December 2018;convolutional neural networktime seriescausal discoveryattentionmachine learning
Having insight into the causal associations in a complex system facilitates decision making, e.g., for medical treatments, urban infrastructure improvements or financial investments.The amount of observational data grows, which enables the discovery of causal relationships between variables from observation of their behaviour in time.Existing methods for causal discovery from time series data do not yet exploit the representational power of deep learning.We therefore present the Temporal Causal Discovery Framework (TCDF), a deep learning framework that learns a causal graph structure by discovering causal relationships in observational time series data.TCDF uses attention-based convolutional neural networks combined with a causal validation step.By interpreting the internal parameters of the convolutional networks, TCDF can also discover the time delay between a cause and the occurrence of its effect.Our framework learns temporal causal graphs, which can include confounders and instantaneous effects.Experiments on financial and neuroscientific benchmarks show state-of-the-art performance of TCDF on discovering causal relationships in continuous time series data.Furthermore, we show that TCDF can circumstantially discover the presence of hidden confounders.Our broadly applicable framework can be used to gain novel insights into the causal dependencies in a complex system, which is important for reliable predictions, knowledge discovery and data-driven decision making.</p>
<p>Introduction</p>
<p>What makes a stock's price increase?What influences the water level of a river?Although machine learning has been successfully applied to predict these variables, most predictive models (such as decision trees and neural networks) cannot answer those causal questions: they make predictions on the basis of correlations alone, but correlation does not imply causation [1].Measures of correlation are symmetrical, since correlation only tells us that there exists a relation between variables.In contrast, causation is usually asymmetrical and therefore gives the directionality of a relation.Correlation which is not causation often arises if two variables have a common cause, or if there is a spurious correlation such that the values of two unrelated variables are coincidentally statistically correlated.</p>
<p>Most machine learning methods, including Neural Networks, aim for a high prediction accuracy encoding only correlations.A predictive model based on correlations alone cannot guarantee robust relationships, making it impossible to foresee when a predictive model will stop working [2], unless the correlation function is carefully modelled to ensure stability (e.g., [3]).If a model would learn causal relationships, we can make more robust predictions.In addition to making forecasts, the goal in many sciences is often to understand the mechanisms by which variables come to take on their values, and to predict what the values would be if the naturally occurring mechanisms were subject to outside manipulations [4].Those mechanisms can be understood by discovering causal associations between events.Knowledge of the underlying causes allows us to develop effective policies to prevent or produce a particular outcome [2].</p>
<p>The traditional way to discover causal relations is to manipulate the value of a variable by using interventions or real-life experiments.All other influencing factors of the target variable can be held fixed, to test whether a manipulation of a potential cause changes the target variable.However, such experiments and interventions are often costly, time-consuming, unethical or even impossible to carry out.With the current advances in digital sensing, the amount of observational data grows, allowing us to do causal discovery [5], i.e., reveal (hypothetical) causal information by analysing this data.Causal discovery helps to interpret data, formulate and test hypotheses, prioritize experiments, and build or improve theories or models.Since humans use causal beliefs and reasoning to generate explanations [6], causal discovery is also an important topic in the rapidly evolving field of Explainable Artificial Intelligence (XAI) that aims to construct interpretable and transparent algorithms that can explain how they arrive at their decisions [7].</p>
<p>The notion of time aids the discovery of the directionality of a causal relationship, since a cause generally happens before the effect.Most algorithms that have been developed to discover causal relationships from multivariate temporal observational data are statistical measures, which rely on idealized assumptions that rarely hold in practice, e.g., assumptions that the time series data is linear, stationary or without noise [8,9], that the underlying causal structure has no (hidden) common causes nor instantaneous effects [10,11].Furthermore, existing methods are usually only designed to discover causal associations, and they cannot be used for prediction.</p>
<p>We exploit the representational power of deep learning by using Attention-based Deep Neural Networks (DNNs) for both time series prediction and temporal causal discovery.DNNs are able to discover complex underlying phenomena by learning and generalizing from examples without knowledge of generalization rules, and have a high degree of error resistivity which makes them less sensitive to noise in the data [12].</p>
<p>Our framework, called Temporal Causal Discovery Framework (TCDF), consists of multiple convolutional neural networks (CNNs), where each network receives all observed time series as input.One network is trained to predict one time series, based on the past values of all time series in a dataset.While a CNN performs supervised prediction, it trains its internal parameters using backpropagation.We suggest using these internal parameters for unsupervised causal discovery and delay discovery.More specifically, TCDF applies attention mechanisms that allow us to learn to which time series a CNN attends to when predicting a time series.After training the attention-based CNNs, TCDF validates whether a potential cause (found by the attention mechanism) is an actual cause of the predicted time series by applying a causal validation step.In this validation step, we intervene on a time series to test if it is causally related with a predicted time series.All validated causal relationships are included in a temporal causal graph.TCDF also includes a novel method to learn the time delay between cause and effect from a CNN, by interpreting the network's internal parameters.In summary:</p>
<p>•</p>
<p>We present a new temporal causal discovery method (TCDF) that uses attention-based CNNs to discover causal relationships in time series data, to discover the time delay between each cause and effect, and to construct a temporal causal graph of causal relationships with delays.</p>
<p>•</p>
<p>We evaluate TCDF and several other temporal causal discovery methods on two benchmarks: financial data describing stock returns, and FMRI data measuring brain blood flow.</p>
<p>The remainder of the paper is organized as follows.Section 2 presents a formal problem statement.Section 3 surveys the existing temporal causal discovery methods, the recent advances in non-temporal causal discovery with deep learning, time series prediction methods based on CNNs, and describes various causal validation methods.Section 4 presents our Temporal Causal Discovery Framework.The evaluation is detailed in Section 5. Section 6 discusses hyperparameter tuning and experiment limitations.The conclusions, including future work, are in Section 7.</p>
<p>Problem Statement</p>
<p>Temporal causal discovery from observational data can be defined as follows.Given a dataset X containing N observed continuous time series of the same length T (i.e., X = {X 1 , X 2 , ..., X N } ∈ R N×T ), the goal is to discover the causal relationships between all N time series in X and the time delay between cause and effect, and to model both in a temporal causal graph.In the directed causal graph G = (V, E), vertex v i ∈ V represents an observed time series X i and each directed edge e i,j ∈ E from vertex v i to v j denotes a causal relationship where time series X i causes an effect in X j .Furthermore, we denote by p = v i , ..., v j a path in G from v i to v j .In a temporal causal graph, every edge e i,j is annotated with a weight d(e i,j ), that denotes the time delay between the occurrence of cause X i and the occurrence of effect X j .An example is shown in Figure 1.Causal discovery methods have major challenges if the underlying causal model is complex: 2a).Pairwise methods, i.e., methods that only find causal relationships between two variables, are often unable to make this distinction [10].In contrast, multivariate methods take all variables into account to distinguish between direct and indirect causality [11].
• The method should distinguish direct from indirect causes. Vertex v i is seen as an indirect cause of v j if e i,j ∈ G and if there is a two-edge path p = v i , v k , v j ∈ G (Figure</p>
<p>•</p>
<p>The method should learn instantaneous causal effects, where the delay between cause and effect is 0 time steps.Neglecting instantaneous influences can lead to misleading interpretations [13].</p>
<p>In practice, instantaneous effects mostly occur when cause and effect refer to the same time step that cannot be causally ordered a priori, because of a too coarse time scale.</p>
<p>•</p>
<p>The presence of a confounder, a common cause of at least two variables, is a well-known challenge for causal discovery methods (Figure 2b).Although confounders are quite common in real-world situations, they complicate causal discovery since the confounder's effects (X 2 and X 3 in Figure 2b) are correlated, but are not causally related.Especially when the delays between the confounder and its effects are not equal, one should be careful to not incorrectly include a causal relationship between the confounder's effects (the grey edge in Figure 2b).</p>
<p>•</p>
<p>A particular challenge occurs when a confounder is not observed (a hidden (or latent) confounder).Although it might not even be known how many hidden confounders exist, it is important that a causal discovery method can hypothesise the existence of a hidden confounder to prevent learning an incorrect causal relation between its effects.
1 3 X 1 X 2 X 3 4
(a) X 1 directly causes X 2 with a delay of 1 time step, and indirectly causes X 3 with a total delay of 1 + 3 = 4 time steps.</p>
<p>Related Work</p>
<p>Section 3.1 discusses existing approaches for temporal causal discovery and classifies a selection of recent temporal causal discovery algorithms along various dimensions.From this overview, we can conclude that there are no other temporal causal discovery methods based on deep learning.Therefore, Section 3.2 describes deep learning approaches for non-temporal causal discovery.Since TCDF discovers causal relationships by predicting time series using CNNs, Section 3.3 discusses related network architectures for time series prediction.Section 3.4 shortly discusses the attention mechanism.</p>
<p>Temporal Causal Discovery</p>
<p>Causal discovery algorithms are used to discover hypothetical causal relations between variables.Whereas most causal discovery methods are designed for independent and identically distributed (i.i.d.) data, temporal data present a number of distinctive challenges and can require different causal discovery algorithms [14].Since there is no sense of time in the usual i.i.d.setting, causality as defined by the i.i.d.approaches is not philosophically consistent with causality for time series, as temporal data should also comply with the 'temporal precedence' assumption [15].The problem of discovering causal relationships from temporal observational data is not only studied in computer science and statistics, but also in the systems and control domain, where networks of dynamical systems, connected by causal transfer functions, are identified from observational data [16].In addition, application areas such as neurobiology use dynamic causal modeling to estimate the connectivity of neuronal networks [17].</p>
<p>Table 1 shows recent temporal causal discovery models, categorized by approach and assessed along various dimensions.The table only reflects some of the most recent approaches for each type of model, since the amount of literature is very large (surveyed for instance in [18]).The 'Features' columns in Table 1 show whether the algorithm can deal with (hidden) confounders, and if it can discover instantaneous effects and the time delay between cause and effect.The 'Data' columns in Table 1 show whether the algorithm can deal with specific types of data, namely multivariate (more than two time series), continuous, non-stationary, non-linear and noisy data.Stationarity means that the joint probability distribution of the stochastic process does not change when shifted in time [19].Furthermore, some methods require discrete data and cannot handle continuous values.Continuous variables can be discretized, but different discretizations can yield different causal structures and discretization can make non-linear causal dependencies difficult to detect [14].[22] Structural Equation Model 2 3  Causal graph (or remains undecided) VAR-LiNGAM [13] Structural Equation Model DAG with causal strengths SDI [23] Information-theoretic Causal relationships with a 'degree of causation' PSTE [11] Information-theoretic Causal Relationships</p>
<p>Granger Causality (GC) [24] is one of the earliest methods developed to quantify the causal effects between two time series.Time series X i Granger causes time series X j if the future value of X j (at time t + 1) can be better predicted by using both the values of X i and X j up to time t than by using only the past values of X j itself.Since pairwise methods cannot correctly handle indirect causal relationships, conditional Granger causality takes a third time series into account [25].However, in practice not all relevant variables may be observed and GC cannot correctly deal with unmeasured time series, including hidden confounders [4].In the system identification domain, this limitation is overcome with sparse plus low-rank (S + L) networks that include an extra layer in a causal graph to explicitly model hidden variables (called factors) [26].Furthermore, GC only captures the linear interdependencies between time series.Various extensions have been made to nonlinear and higher-order causality, e.g., [27,28].A recent extension that outperforms other GC methods is based on conditional copula, that allows to dissociate the marginal distributions from their joint density distribution to focus only on statistical dependence between variables [10].</p>
<p>Constraint-based Time Series approaches are often adapted versions of non-temporal causal graph discovery algorithms.The temporal precedence constraint reduces the search space of the causal structure [29].The well-known algorithms PC and FCI both have a time series version: PCMCI [8] and tsFCI [21].PC [30] makes use of a series of tests to efficiently explore the whole space of Directed Acyclic Graphs (DAGs).FCI [30] can, contrary to PC, deal with hidden confounders by using independence tests.Both temporal algorithms require stationary data.Additive Non-linear Time Series Model (ANLTSM) [20] does causal discovery in both linear and non-linear time series data, and can also deal with hidden confounders.It uses statistical tests based on additive model regression.</p>
<p>Structural Equation Model approaches assume that a causal system can be represented by a Structural Equation Model (SEM) that describes a variable X j as a function of other variables X −j , and an error term X to account for additive noise such that X := f (X −j , X ) [29].It assumes that the set X −j is jointly independent.TiMINo [22] discovers a causal relationship if the coefficient of X t i for any t is nonzero for X t j =i .Self-causation is not discovered.TiMINo remains undecided if the direct causes of X i are not independent, instead of drawing possibly wrong conclusions.TiMINo is not suitable for large datasets, since small differences between the data and the fitted model may lead to failed independence tests.VAR-LiNGAM [13] is a restricted SEM.It makes additional assumptions on the data distribution and combines a non-Gaussian instantaneous model with autoregressive models.</p>
<p>Information-theoretic approaches for temporal causal discovery exist, such as (mutual) shifted directed information [23] and transfer entropy [11].Their main advantage is that they are model free and are able to detect both linear and non-linear dependencies [19].The universal idea is that X i is likely a cause of X j , i = j, if X j can be better sequentially compressed given the past of both X i and X j than given the past of X j alone.Transfer entropy cannot, contrary to directed information [31], deal with non-stationary time series.Partial Symbolic Transfer Entropy (PSTE) [11] overcomes this limitation, but is not effective when only linear causal relationships are present.</p>
<p>Causal Significance is a causal discovery framework that calculates a causal significance measure α(c, e) for a specific cause-effect pair by isolating the impact of cause c on effect e. [9].It also discovers time delay and impact of a causal relationship.The method assumes that causal relationships are linear and additive, and that all causes are observed.However, the authors experimentally demonstrate that low false discovery and negative rates are achieved if some assumptions are violated.</p>
<p>Our Deep Learning approach uses neural networks to learn a function for time series prediction.Although learning such a function is comparable to SEM, the interpretation of coefficients is different (Section 4.2).Furthermore, we apply a validation step that is to some extent comparable to conditional Granger causality.Instead of removing a variable, we randomly permute its values (Section 4.3).</p>
<p>Deep Learning for Non-Temporal Causal Discovery</p>
<p>Deep Neural Networks (DNNs) are usually complex, black-box models.DNNs are therefore not yet applied for the purpose of causal discovery from time series, since only recently the rapidly emerging field of explainable machine learning enables DNN interpretation [7].Feature importance proposed by an interpretable LSTM already showed to be highly in line with results from the Granger causality test [32].Multiple deep learning models exist for non-temporal causal discovery: Variational Autoencoders [33] to estimate causal effects, Causal Generative Neural Networks to learn functional causal models [34] and the Structural Agnostic Model (SAM) [35] for causal graph reconstruction.Although called 'causal filters' by the authors, SAM uses an attention mechanism by multiplying each observed input variable by a trainable score, comparable to the TCDF approach.Contrary to TCDF, SAM does not perform a causal validation step.Non-temporal methods however cannot be applied to time series data, since they do not check the temporal precedence assumption (cause precedes effect).</p>
<p>Time Series Prediction</p>
<p>TCDF uses Convolutional Neural Networks (CNNs) for time series prediction.A CNN is a type of feed-forward neural network, consisting of a sequence of convolutional layers, which makes them rather easy to interpret.A convolutional layer of a CNN limits the number of connections to only some of the input neurons by sliding a kernel (a weight matrix) over the input and at each time step it computes the dot product between the input and the kernel.The kernel will then learn specific repeating patterns in the input series to forecast future values of the target time series.</p>
<p>Usually, Recurrent Neural Networks (RNNs) are regarded as the default starting point to solve sequence learning, since RNNs are theoretically capable of having infinite memory [36].However, long-term information has to sequentially travel through all cells before getting to the present processing cell, causing the well-known vanishing gradients problem [37].Other issues with RNNs are the high memory usage to store partial results, their complex architecture making them hard to interpret and the impossibility of parallelism which hinders scaling [36].RNNs are therefore slowly falling out of favor for modern convolutional architectures for sequence data.CNNs are already successfully applied for sequence to sequence problems, including machine translation [38] and image generation from text [39].However, although sequence to sequence modeling is related to our time series problem, such methods use the entire input sequence (including "future" states) to predict each output which does not satisfy the causal constraint that there can be no information 'leakage' from future to past.Convolutional architectures for time series are still scarce, but deep convolutional architectures were recently used for noisy financial time series forecasting [40] and for multivariate asynchronous time series prediction [41].</p>
<p>Attention Mechanism in Neural Networks</p>
<p>An attention mechanism ('attention' in short) equips a neural network with the ability to focus on a subset of its inputs.The concept of 'attention' has a long history in classical computer vision, where an attention mechanism selects relevant parts of the image for object recognition in cluttered scenes [42].</p>
<p>Only recently attention has made its way into deep learning.The idea of today's attention mechanism is to let the model learn what to attend to based on the input data and what it has learnt so far.Prior work on attention in deep learning mostly addresses recurrent networks, but Facebook's FairSeq [38] for neural machine translation and the Attention Based Convolutional Neural Network [43] for modeling sentence pairs have shown that attention is very effective in CNNs.Besides the increased accuracy, attention allows us to interpret where the network attends to, which allows TCDF to identify which input variables are possibly causally associated with the predicted variable.</p>
<p>TCDF-Temporal Causal Discovery Framework</p>
<p>This section details our Temporal Causal Discovery Framework (TCDF).TCDF is implemented in Python and PyTorch and available at https://github.com/M-Nauta/TCDF.Figure 3 gives a global overview of TCDF, showing the four steps to learn a Temporal Causal Graph from data: Time Series Prediction, Attention Interpretation, Causal Validation and Delay Discovery.More specifically, TCDF consists of N independent attention-based CNNs, all with the same architecture but a different target time series.An overview of TCDF containing multiple networks is shown in Figure 4.This shows that the goal of the jth network N j is to predict its target time series X j by minimizing the loss L between the actual values of X j and the predicted Xj .The input to network N j consists of a N × T dataset X consisting of N equal-sized time series of length T. Row X j from the dataset corresponds to the target time series, while all other rows in the dataset, X −j , are the so-called exogenous time series.
Input Output T . . . X 1 X2 Xn • • • 3 1 4 6 1 X2 1 1 6 3 4 X 1 X 1 X2 Xn X2 + W2 + a2 X 1 X2 Xn X1 + W 1 + a 1 N2 N 1 X 1 X2 Xn Xn + Wn + an 1 1 6 3 4 1 1 6 3 4 Nn Xn . . . X 1 X i X2 Xn X j</p>
<p>Attention Interpretation Causal Validation Delay Discovery</p>
<p>Figure 4. TCDF with N independent CNNs N 1 ...N n , all having time series X 1 ...X n of length T as input (N is equal to the number of time series in the input data set).N j predicts X j and also outputs, besides Xj , the kernel weights W j and attention scores a j .After attention interpretation, causal validation and delay discovery, TCDF constructs a temporal causal graph.</p>
<p>When network N j is trained to predict X j , the attention scores a j of the attention mechanism explain where network N j attends to when predicting X j .Since the network uses the attended time series for prediction, this time series must contain information that is useful for prediction, implying that this time series is potentially causally associated with the target time series X j .By including the target time series in the input as well, the attention mechanism can also learn self-causation.We designed a specific architecture for these attention-based CNNs that allows TCDF to discover these potential causes.We call our networks Attention-based Dilated Depthwise Separable Temporal Convolutional Networks (AD-DSTCNs).</p>
<p>The rest of this section is structured as follows: Section 4.1 describes the architecture of AD-DSTCNs.Section 4.2 presents our algorithm to detect potential causes of a predicted time series.Section 4.3 describes our Permutation Importance Validation Method (PIVM) to validate potential causes.For delay discovery, TCDF uses the kernel weights W j of each AD-DSTCN N j , which will be discussed in more detail in Section 4.4.TCDF merges the results of all networks to construct a Temporal Causal Graph that shows the discovered causal relationships and their delays.</p>
<p>The Architecture for Time Series Prediction</p>
<p>We base our work on the generic Temporal Convolutional Network (TCN) architecture of [36], a model for univariate time-series modelling.A TCN consists of a CNN architecture with a 1D kernel in which each layer has length T, where T is the number of time steps in both the input and the target time series.It does supervised learning by minimizing the loss L between the actual values of target X 2 and the predicted X2 .A TCN predicts time step t of the target time series based on the past and current values of the input time series, i.e., from time step 1 up to and including time step t.Including the current value of the input time series enables the detection of instantaneous effects.No future values are used for this prediction: a TCN does a so-called causal convolution in which there is no information 'leakage' from the future to the past.</p>
<p>A TCN predicts each time step of the target time series X 2 by sliding a kernel over input
X 1 of which the input values are [X 1 1 , X 2 1 , ..., X t 1 , ..., X T 1 ]
. When predicting the value of X 2 at time step t, denoted X t 2 , the 1D kernel with a user-specified size K calculates the dot product between the learnt kernel weights W, and the current input value plus its
K − 1 previous values, i.e., W [X t−K+1 1 , X t−K+2 1 ..., X t−1 1 , X t 1 ]
. However, when the first value of X 2 , X 1 2 , has to be predicted, the input data only consists of X 1  1 and past values are not available.This means that the kernel cannot fill its kernel size if K &gt; 1.Therefore, TCN applies left zero padding such that the kernel can access K − 1 values of zero.For example, if K = 4, the sliding kernel first sees [0, 0, 0, X 1  1 ], followed by [0, 0,
X 1 1 , X 2 1 ], [0, X 1 1 , X 2 1 , X 3 1 ], etc., until [X T−3 1 , X T−2 1 , X T−1 1 , X T 1 ]
. While a TCN uses ReLU, we use PReLU as a non-linear activation function, since PReLu has shown to improve model fitting with nearly zero extra computational cost and little overfitting risk compared to the traditional ReLU [44].</p>
<p>Dilations</p>
<p>In a TCN with only one layer (i.e., no hidden layers), the receptive field (the number of time steps seen by the sliding kernel) is equal to the user-specified kernel size K.To successfully discover a causal relationship, the receptive field should be as least as large as the delay between cause and effect.To increase the receptive field, one can increase the kernel size or add hidden layers to the network.A convolutional network with a 1D kernel has a receptive field that grows linearly in the number of layers, which is computationally expensive when a large receptive field is needed.More formally, the receptive field R of a CNN is
R CNN = 1 + (L + 1)(K − 1) = 1 + L ∑ l=0 (K − 1) ,(1)
with K the user-specified kernel size and L the number of hidden layers.L = 0 gives a network without hidden layers, where one convolution in a channel maps an input time series to the output.TCN, inspired by the well-known WaveNet architecture [45], employs dilated convolutions instead.A dilated convolution applies a kernel over an area larger than its size by skipping input values with a certain step size f .This step size f , called dilation factor, increases exponentially depending on the chosen dilation coefficient c, such that f = c l for layer l.An example of dilated convolutions is shown in Figure 5.With an exponentially increasing dilation factor f , a network with stacked dilated convolutions can operate on a coarser scale without loss of resolution or coverage.The receptive field R of a kernel in a 1D Dilated TCN (D-TCN) is
X1 2 X2 2 X3 2 X4 2 ... X16 2 ... XT 2 Output Hidden Hidden Hidden Input X 1 1 X 2 1 X 3 1 X 4 1 ... X16R D-TCN = 1 + L ∑ l=0 (K − 1) • c l . (2)
This shows that dilated convolutions support an exponential increase of the receptive field while the number of parameters grows only linearly, which is especially useful when there is a large delay between cause and effect.</p>
<p>Adaption for Discovering Self-Causation</p>
<p>We allow the input and predicted time series to be the same in order to discover self-causation, which can model the concept of repeated behavior.For this purpose, we adapt the TCN architecture of [36] slightly, since we should not include the current value of the target time series in the input.With an exogenous time series as input, the sliding kernel with size
K can access [X t−K+1 i , X t−K+2 i ..., X t−1 i , X t
i ] with i = j to predict X t j for time step t.However, the kernel should only access the past values of the target time series X j , thus excluding the current value X t j , since that is the value to be predicted.TCDF solves this by shifting the target input data one time step forward with left zero padding, such that the input target time series in the dataset equals [0, X 1 j , X 2 j , ..., X T−1 j ] and the kernel therefore can access
[X t−K j , X t−K+1 j ..., X t−2 j , X t−1 j ] to predict X t j .</p>
<p>Adaption for Multivariate Causal Discovery</p>
<p>A restriction of the TCN architecture is that it is designed for univariate time series modeling, meaning that there is only one input time series.Multivariate time series modeling in CNNs is usually achieved by merging multiple time series into a 2D-input.A 2D-kernel slides over the 2D-input such that the kernel weights are element-wise multiplied with the input.This creates a 1D-output in the first hidden layer.For a deep TCN, 1D-convolutional layers can be added to the architecture.However, the disadvantage of this approach is that the output from each convolutional layer is always one-dimensional, meaning that the input time series are mixed.This mixing of inputs hinders causal discovery when a deep network architecture is desired.</p>
<p>To allow for multivariate causal discovery, we extend the univariate TCN architecture to a one-dimensional depthwise separable architecture in which the input time series stay separated.The depthwise separable convolution is introduced in [46] and became popular with Google's Xception architecture for image classification [47].It consists of depthwise convolutions, where channels are kept separate by applying a different kernel to each input channel, followed by a 1 × 1 pointwise convolution that merges together the resulting output channels [47].This is different from normal convolutional architectures that have only one kernel per layer.A depthwise separable architecture improves accuracy and convergence speed [47], and the separate channels allow us to correctly interpret the relation between an input time series and the target time series, without mixing the inputs.</p>
<p>Our TCDF architecture consists of N channels, one for each input time series.In network N j , channel j corresponds to the target time series
X j = [0, X 1 j , X 2 j , ..., X T−1 j
] and all other channels correspond to the exogenous time series
X i =j = [X 1 i , X 2 i , ..., X T−1 i , X T i
].An overview of this architecture is shown in Figure 6, including the attention mechanism that is discussed next.The attention scores a are multiplied element-wise with the input time series, followed by an element-wise multiplication with the kernel.In the pointwise convolution, all channel outputs are combined to construct the prediction X2 .
X1 2 X2 2 X3 2 X4 2 X5 2 X6 2 X7 2 X8 2 X9 2 X10 2 X11 2 X12 2 X13 2 a2,1 X 1 2 X 2 2 X 3 2 X 4 2 X 5 2 X 6 2 X 7 2 X 8 2 X 9 2 X 10 2 X 11 2 X 12 2 X 13 2 a2,2 X 1 n X 2 n X 3 n X 4 n X 5 n X 6 n X 7 n X 8 n X 9 n X 10 n X 11 n X 12 n X 13 n a2,n Depthwise Pointwise X 1 1 X 2 1 X 3 1 X 4 1 X 5 1 X 6 1 X 7 1 X 8 1 X 9 1 X 10 1 X 11 1 X 12 1 X13</p>
<p>The Attention Mechanism</p>
<p>To find out where a network focuses on when predicting a time series, we extend the network architecture with an attention mechanism.We call these attention-based networks 'Attention-based Dilated Depthwise Separable Temporal Convolutional Networks' (AD-DSTCNs).</p>
<p>We implement attention as a trainable 1 × N-dimensional vector a that is element-wise multiplied with the N input time series.Each value a ∈ a is called an attention score.In our framework, each network N j has its own attention vector a j = [a 1,j , a 2,j , ..., a j,j , ..., a N,j ].Attention score a i,j is multiplied with input time series X i in network N j .This is indicated with at the top of Figure 6.Thus, attention score a i,j ∈ a j shows how much N j attends to input time series X i for predicting target X j .A high value for a i,j ∈ a j means that X i might cause X j .A low value for a i,j means that X i is probably not a cause of X j .Note that i = j is possible since we allow self-causation.The attention scores will be used after training of the networks to determine which time series are potential causes of a target time series.</p>
<p>Residual Connections</p>
<p>An increasing number of hidden layers in a network usually results in a higher training error.This accuracy degradation problem is not caused by overfitting, but by the standard backpropagation being unable to find optimal weights in a deep network [48].The proven solution is to use residual connections.A convolution layer transforms its input x to F (x), after which an activation function is applied.With a residual connection, the input x of the convolutional layer is added to
F (x) such that the output o is o = PReLU(x + F (x)) .(3)
We add a residual connection in each channel after each convolution from the input of the convolution to the output (first layer excluded), as shown in Figure 6.</p>
<p>Attention Interpretation</p>
<p>When the training of the network starts, all attention scores are initialized as 1,
a j = [1, 1, ..., 1].
While the networks use backpropagation to predict their target time series, the network also changes its attention scores: each score is either increased or decreased in every training epoch.After some training epochs, a j ∈ [−∞, ∞] N .The bounds depend on the number of training epochs and the specified learning rate.</p>
<p>The literature distinguishes between soft attention, where a j ∈ [0, 1] N , and hard attention, where a j ∈ {0, 1} N .Soft attention is usually realized by applying the Softmax function σ to the attention scores such that ∑ N i=1 a i,j = 1.A limitation of the Softmax transformation is that the resulting probability distribution always has full support, σ(a i,j ) = 0 [49].Intuitively, one would prefer hard attention for causal discovery, since the network should make a binary decision: a time series is either causal or non-causal.However, hard attention is non-differentiable due to its discrete nature, and therefore cannot be optimized through backpropagation [50].We therefore first use the soft attention approach by applying the Softmax function σ to each a ∈ a j in each training epoch.After training network N j , we apply our straightforward semi-binarization function HardSoftmax that truncates all attention scores that fall below a threshold τ j to zero:
h = HardSoftmax(a) = σ(a) if a ≥ τ j 0 if a &lt; τ j . (4)
We denote by h j the set of attention scores in a j to which the HardSoftmax function is applied.TCDF creates a set of potential causes P j for each time series X j ∈ X.Time series X i is considered a potential cause of the target time series X j if h i,j ∈ h j &gt; 0.</p>
<p>We created an algorithm that determines τ j by finding the largest gap between the attention scores in a j .The algorithm ranks the attention scores from high to low and searches for the largest gap g between two adjacent attention scores a i,j and a k =i,j .The threshold τ j is then equal to the attention score on the left side of the gap.This approach is graphically shown in Figure 7.We denote by G the list of gaps [g 0 , ..., g N−1 ].We have set three requirements for determining τ j (in priority order):</p>
<p>• We require that τ j ≥ 1, since all scores are initialized at 1 and a score will only be increased through backpropagation if the network attends to that time series.</p>
<p>• Since a temporal causal graph is usually sparse, we require that the gap selected for τ j lies in the first half of G (if N &gt; 5) to ensure that the algorithm does not include low attention scores in the selection.At most 50% of the input time series can be a potential cause of target X j .By this requirement, we limit the number of time series labeled as potential causes.Although this number can be configured, we experimentally estimated that 50% gives good results.</p>
<p>•</p>
<p>We require that the gap for τ j cannot be in first position (i.e., between the highest and second-highest attention score).This ensures that the algorithm does not truncate to zero the scores for time series which were actually a cause of the target time series, but were weaker than the top scorer.Thus, the potential causes P j for target X j will include at least two time series.</p>
<p>With τ j determined, the HardSoftmax function is applied.Time series X i is added to P j if a i,j ∈ a j &gt; τ j , so if h i,j ∈ h j &gt; 0. We have the following cases between HardSoftmax scores h i,j and h j,i :</p>
<p>1.</p>
<p>h i,j = 0 and h j,i = 0: X i is not correlated with X j and vice versa.2.</p>
<p>h i,j = 0 and h j,i &gt; 0: X j is added to P i since X j is a potential cause of X i because of:</p>
<p>(a) (In)direct causal relation from X j to X i , or (b) Presence of a (hidden) confounder between X j and X i where the delay from the confounder to X j is smaller than the delay to X i .</p>
<p>3.</p>
<p>h i,j &gt; 0 and h j,i = 0: X i is added to P j since X i is a potential cause of X j because of:</p>
<p>(a) (In)direct causal relation from X i to X j , or (b) Presence of a (hidden) confounder between X i and X j where the delay from the confounder to X i is smaller than the delay to X j .</p>
<p>4.</p>
<p>h i,j &gt; 0 and h j,i &gt; 0: X i is added to P j and X j is added to P i because of:</p>
<p>(a) Presence of a 2-cycle where X i causes X j and X j causes X i , or (b) Presence of a (hidden) confounder with equal delays to its effects X i and X j .</p>
<p>Note that a HardSoftmax score &gt; 0 could also be the result of a spurious correlation.However, since it is impossible to judge whether a correlation is spurious purely on the analysis of observational data, TCDF does not take the possibility of a spurious correlation into account.After causal discovery from observational data, it is up to a domain expert to judge or test whether a discovered causal relationship is correct.Section 6 presents a more extensive discussion on this topic.</p>
<p>By comparing all attention scores, we create a set of potential causes for each time series.Then, we will use our Permutation Importance Validation Method (PIVM) to validate if a potential cause is a true cause.More specifically, TCDF will apply PIVM to distinguish between case 2a and 2b, between case 3a and 3b and between case 4a and 4b.</p>
<p>Causal Validation</p>
<p>After interpreting the HardSoftmax scores h j to find potential causes, TCDF validates if a potential cause in P j is an actual cause of time series X j .Potential causes that are validated will be called true causes, as described in Section 4.3.1.The existence of hidden confounders can complicate the correct discovery of true causes.Section 4.3.2describes how TCDF handles a dataset in which not all confounders are measured.</p>
<p>A causal relationship is generally said to comply with two aspects [51]:</p>
<ol>
<li>Temporal precedence: the cause precedes its effect, 2. Physical influence: manipulation of the cause changes its effect.</li>
</ol>
<p>Since we use a temporal convolutional network architecture, there is no information leakage from future to past.Therefore, we comply with the temporal precedence assumption.The second aspect is usually defined in terms of interventions.More specifically, an observed time series X i is a cause of another observed time series X j if there exists an intervention on X i such that if all other time series X −i ∈ X are held fixed, X i and X j are associated [52].However, such controlled experiments in which other time series are held fixed may not be feasible in many time series applications (e.g., stock markets).In those cases, a data-driven causal validation measure can act as intervention method.A causal validation measure models the difference in evaluation score between the real input data and an intervened dataset in which a potential cause is manipulated to evaluate whether this changes the effect.</p>
<p>TCDF uses Permutation Importance (PI) [53] as causal validation method.This feature importance method measures how much an error score increases when the values of a variable are randomly permuted [53].According to van der Laan [54], the importance of a variable can be interpreted as causal effect if the observed data structure is chronologically ordered, consistent and contains no hidden confounding or randomization.(If the last assumption is violated, the variable importance measures can still be applied, and subsequent experiments can determine until what degree the variable importance is causal [54].)Permuting a time series' values removes chronologicity and therefore breaks a potential causal relationship between cause and effect.Only if the loss of a network increases significantly when a variable is permuted, the variable is a cause of the predicted variable.</p>
<p>A closely related measure is the Causal Quantitative Input Influence measure of [55].They construct an intervened distribution by retaining the marginal distribution over all other inputs from the dataset and randomly sampling the input of interest from its prior distribution.Instead of intervening on variables, the "destruction of edges" [56] (intervening on the edges) in a Bayesian network can be used to validate and quantify causal strength by calculating the relative entropy between the old and intervened distribution.The method excludes instantaneous effects.</p>
<p>Note that the Permutation Importance method is a more adequate causal validation method than simply removing a potential cause from the dataset.Removing a correlated variable may lead to worse predictions, but this does not necessarily mean that the correlated variable is a cause of the predicted variable.For example, suppose that a dataset contains one variable with values in [0, 1], and all other variables in the dataset have values in [5000, 15, 000].If the predicted variable lies within [0, 1], a neural network might base its prediction on the variable having the same range of values.Removing it from the dataset then leads to a higher loss, even if the variable was not a cause of the predicted variable.</p>
<p>Permutation Importance Validation Method</p>
<p>To find potential causes, TCDF trains a network N j based on the original input dataset and measures its ground loss L G .To validate a potential cause, TCDF creates an intervened dataset for each potential cause X i ∈ P j .This equals the original input dataset, except that the values of a potential cause X i ∈ P j are randomly permuted.Since random permutations does not change the distribution of the dataset, network N j needs no retraining.TCDF simply runs the trained network N j on the intervened dataset to predict X j and measures the intervention loss L I .</p>
<p>If potential cause X i would be a real cause of X j , predictions based on the intervened dataset should be worse, since the chronology of X i was removed.Therefore, the intervention loss L I of the network should be significantly higher than the ground loss L G where the original dataset is used.If L I is not significantly higher than L G , then X i is not a cause of X j , since X j can be predicted without the chronological order of X i .Only the time series in P j that are validated are considered true causes of the target time series X j .We denote by C j the set of all true causes of X j .</p>
<p>As an example, we consider the case depicted in Figure 2b.Suppose that both X 1 and X 2 are potential causes for X 3 based on the attention score interpretation.The validation checks if these causes are true causes of X 3 .When the values of X 1 are randomly permuted to predict X 3 , the intervention loss L I will probably be higher than L G , since the network has no access to the chronological order of the values of confounder X 1 .On the other hand, if the validation is applied to X 2 , the loss will probably not change significantly, since the network still has access to the chronological order of the values of confounder X 1 to predict X 3 .TCDF will then conclude that only X 1 is a true cause of X 3 .</p>
<p>To determine whether an increase in loss between the original dataset and the intervened dataset is 'significant', one could require a certain percentage of increase.However, the required increase in loss is dependent on the dataset.A network applied to a dataset with clear patterns will, during training, decrease its loss more compared to one trained on a dataset without clear patterns.TCDF includes a small algorithm, called the Permutation Importance Validation Method (PIVM), to determine when an increase in loss between the original dataset and the intervened dataset is relatively significant.This is based on the initial loss at the first epoch, and uses a user-specified parameter s ∈ [0, 1] denoting a significance measure.We experimentally found that a significance of s = 0.8 gives good results, but the user can specify any other value in [0, 1].</p>
<p>TCDF trains a network N j for E epochs on the original dataset and measures the decrease in ground loss between epoch 1 and epoch E :
∆ L G = L 1 G − L E G .
This denotes the improvement in loss that N j can achieve by training on the input data.Subsequently, TCDF applies the trained network N j to an intervened dataset where the values of X i ∈ P j are randomly permuted, and measures the loss L I .It then calculates
∆ L I = L 1 G − L I .
This denotes the difference between the initial loss at the first epoch and the loss when the trained network is applied to the permuted dataset.</p>
<p>If this difference ∆ L I is greater than ∆ L G • s, then ∆ L I is significantly large, so the loss L I has not increased significantly compared to L G .TCDF then concludes that the permuted variable X i ∈ P j is not a true cause of X j .On the other hand, if ∆ L I is small (≤ ∆ L G • s), then the permuted dataset leads to loss L I that is larger than L G and relatively close to (or greater than) the initial loss at the first epoch.TCDF can therefore conclude that X i ∈ P j is a true cause of X j .</p>
<p>Dealing with Hidden Confounders</p>
<p>If we assume that all genuine causes are measured, the causal validation step of TCDF consisting of attention interpretation and PIVM should in theory only discover correct causal relationships (according to the data).Cases 2b, 3b and 4b from Section 4.2 then all arise because of a measured confounder.A time series X i that was correlated with time series X j because of a confounder would not be labeled as true cause by PIVM, since only the presence of the confounder would be needed to predict X j .</p>
<p>However, our PIVM approach might discover incorrect causal relationships if there exist hidden confounders, i.e., confounders that are not included in the dataset.This section describes how TCDF can successfully discover the presence of a hidden confounder with equal delays to its effects X i and X j (case 4b from Section 4.2).We also state that TCDF will probably not detect the presence of a hidden confounder when this has unequal delays to its effects (case 2b and 3b).</p>
<p>As shown in Table 1, not all temporal causal discovery methods can deal with unmeasured confounders.ANLTSM can only deal with hidden confounders that are linear and instantaneous.The authors of TiMINo claim to handle hidden confounders by staying undecided instead of inferring any (possibly incorrect) causal relationship.Lastly, tsFCI handles hidden confounders by including a special edge type (X i ↔ X j ) that shows that X i is not a cause of X j and that X j is not a cause of X i , from which one can conclude that there should be a hidden confounder that causes both X i and X j .TCDF can discover this X i ↔ X j relation in specific cases by applying PIVM.Based on cases 2-4, we distinguish three reasons why two time series are correlated: a causal relationship, a measured confounder, or a hidden confounder.(We exclude the possibility of a spurious correlation).If there is a measured confounder, PIVM should discover that the confounder's effects X i and X j are just correlated and not causally related.If there is a 2-cycle, PIVM should discover that X i causes X j with a certain delay and that X j causes X i with a certain delay.If there is a hidden confounder of X i and X j , PIVM will find that X i is a true cause of X j and vice versa.</p>
<p>When the delay from the confounder to X i is smaller than the delay to X j (case 3b), TCDF will, based on the temporal precedence assumption, discover an incorrect causal relationship from X i to X j .More specifically, TCDF will discover that the delay of this causal relationship will be equal to the delay from the confounder to X i minus the delay from the confounder to X j .Figure 8a shows an example of this situation.The same reasoning applies when the delay from the confounder to X i is greater than the delay to X j (case 2b).(a) TCDF will incorrectly discover a causal relationship from X 2 to X 3 when the delay from X 1 to X 2 is smaller than the delay from X 1 to X 3 .
0 0 4 4 X 2 X 3 X 1
(b) TCDF will discover a 2-cycle between X 2 and X 3 where both delays equal 0, such that there should exist a hidden confounder between X 2 and X 3 .</p>
<p>Figure 8.How TCDF deals, in theory, with hidden confounders (denoted by squares).A black square indicates that the hidden confounder is discovered by TCDF; a grey square indicates that it is not discovered.Black edges indicate causal relationships that will be included in the learnt temporal causal graph G L ; grey edges will not be included in G L .</p>
<p>However, TCDF will not discover a causal relationship when the hidden confounder has equal delays to its effects X i and X j (case 4b), and can even conclude that there should be a hidden confounder that causes both X i and X j .Because the confounder has equal delays to X i and X j , the delays from X i to X j and from X j to X i will both be 0. The zero delays give away the presence of a hidden confounder, since there cannot exist a 2-cycle where both time series have an instantaneous effect on each other.Recall that an instantaneous effect means that there is an effect within 1 measured time step.If both time series cause each other instantaneously, there will be an infinite causal influence between the time series within 1 time step, which is impossible.Therefore, TCDF will conclude that X i and X j are not causally related, and that there exists a hidden confounder between X i and X j .Figure 8b shows an example of this situation.</p>
<p>The advantage of our approach is that TCDF not only concludes that two variables are not causally related, but can also detect the presence of a hidden confounder.</p>
<p>Delay Discovery</p>
<p>Besides discovering the existence of a causal relationship, TCDF discovers the number of time steps between a true cause and its effect.This is done by interpreting the kernel weights W i for a causal input time series X i from a network N j predicting target time series X j .Since we have a depthwise separable architecture where input time series are not mixed, the relation between the kernel weights of one input time series and the target time series can be correctly interpreted.</p>
<p>The kernel that slides over the N input channels is a weight matrix with N rows and K columns (where K is the kernel size), and outputs the dot product between the input channel and the weight matrix.Contrary to regular neural networks, all output values of a channel share the same weights and therefore detect exactly the same pattern, as indicated by the identical colors in Figure 5.These shared weights not only reduce the total number of learnable parameters, but also allow delay interpretation.</p>
<p>Since a convolution is a linear operation, we can measure the influence of a specific delay between cause X i and target X j , by analyzing the weights of X i in the kernel.The K weights of each channel output show the 'importance' of each time delay.</p>
<p>An example is shown in Figure 9.The position of the highest kernel weight equals the discovered delay d(e i,j ).Since we also use the current values in the input data, the smallest delay can be 0 time steps, which indicates an instantaneous effect.The maximum delay that can be found equals the receptive field.To successfully discover a causal relationship, the receptive field should therefore be at least as large as the (estimated) delay between cause and effect.
X 1 1 X 2 1 X 3 1 X 4 1 X10f = 2 3 f = 2 2 f = 2 1 f = 2 0 Figure 9.
Discovering the delay between cause X 1 and target X 2 , both having T = 16.Starting from the top convolutional layer, the algorithm traverses through the path with the highest kernel weights.Eventually, the algorithm ends in input value X 10  1 , indicating a delay of 16 − 10 = 6 time steps.</p>
<p>Experiments</p>
<p>To evaluate our framework, we apply TCDF to two benchmarks, each consisting of multiple simulated datasets for which the true underlying causal structures are known.The benchmarks are discussed in Section 5.1.The ground truth allows us to evaluate the accuracy of TCDF.We compare the performance of TCDF with that of three existing temporal causal discovery methods described in Section 5.2.Besides causal discovery accuracy, we evaluate prediction performance, delay discovery accuracy and effectiveness of the causal validation step PIVM.We also evaluate how the architecture of AD-DSTCNs influences the discovery of correct causal relationships.However, since it would be impractical to test all parameter settings, we only vary the number of hidden layers L. As a side experiment, we evaluate how TCDF handles hidden confounders.The evaluation measures for these evaluations are described in Section 5.3.Results of all experiments are presented in Section 5.4.</p>
<p>Data Sets</p>
<p>We apply our framework to two benchmarks consisting of multiple data sets: simulated financial market data and simulated functional magnetic resonance imaging (FMRI) data.Figure 10 shows a plot of a dataset from each benchmark and a graph of the corresponding ground truth causal structure.Benchmark statistics are provided in Table 2.The first benchmark, called FINANCE, contains datasets for 10 different causal structures of financial markets [2].For our experiments, we exclude the dataset without any causal relationships (since this would result in an F1-score of 0).The datasets are created using the Fama-French Three-Factor Model [57] that can be used to describe stock returns based on the three factors 'volatility', 'size' and 'value'.A portfolio's return X t i depends on these three factors at time t plus a portfolio-specific error term [2].We use one of the two 4000-day observation periods for each financial portfolio.</p>
<p>To evaluate the ability to detect hidden confounders, we created the benchmark FINANCE HIDDEN containing four datasets.Each dataset corresponds to either dataset '20-1A' or '40-1-3' from FINANCE except that one time series is hidden by replacing all its values by 0. Figure 11 shows the underlying causal structures, in which a grey node denotes a hidden confounder.As can be seen, we test TCDF on hidden confounders with both equal delays and unequal delays to its effects.To evaluate the predictive ability of TCDF, we created training data sets corresponding to the first 80% of the data sets and utilized the remaining 20% for testing.These data sets are referred to as FINANCE TRAIN/TEST.The second benchmark, called FMRI, contains realistic, simulated BOLD (Blood-oxygen-level dependent) datasets for 28 different underlying brain networks [58].BOLD FMRI measures the neural activity of different regions of interest in the brain based on the change of blood flow.Each region (i.e., node in the brain network) has its own associated time series.Since not all existing methods can handle 50 time series, we excluded one dataset with 50 nodes.For each of the remaining 27 brain networks, we selected one dataset (scanning session) out of multiple available.All time series have a hidden external input, white noise, and are fed through a non-linear balloon model [59].</p>
<p>Since FMRI contains only six (out of 27) datasets with 'long' time series, we create an extra benchmark that is a subset of FMRI.This subset contains only datasets in which the time series have at least 1000 time steps, therefore denoted as FMRI T &gt; 1000, and coincidentally are all stationary.To evaluate the predictive ability of TCDF, we created a training and test set corresponding to the resp.first 80% and last 20% of the datasets, referred to as FMRI TRAIN/TEST and FMRI T &gt; 1000 TRAIN/TEST.</p>
<p>Experimental Setup</p>
<p>In the experiments, we compared four methods: the proposed framework TCDF, the constraintbased methods PCMCI [8] and tsFCI [21], and Structural Equation Model TiMINo [22].</p>
<p>TCDF: All AD-DSTCNs use the Mean Squared Error as loss function and the Adam optimization algorithm which is an extension to stochastic gradient descent [60].This optimizer computes individual adaptive learning rates for each parameter which allows the gradient descent to find the minimum more accurately.Furthermore, in all experiments, we train our AD-DSTCNs for 5000 training epochs, with learning rate λ = 0.01, dilation coefficient c = 4 and kernel size K = 4.We chose K such that the delays in the ground truth fall within the receptive field R. We vary the number of hidden layers in the depthwise convolution between L = 0, L = 1 and L = 2 to evaluate how the number of hidden layers influences to framework's accuracy.Note that increasing the number of hidden layers leads to an increased receptive field (according to Equation (2)), and therefore an increasing maximum delay.</p>
<p>PCMCI: We used the authors' implementation from the Python Tigramite module [8].We set the maximum delay to three time steps and the minimum delay to 0, equivalent to the minimum and maximum delay that can be found by TCDF in our AD-DSTCNs with K = 4 and L = 0. We use the ParCorr independence test for linear partial correlation.(Besides the linear ParCorr independence test, the authors present the non-linear GPACE test to discover non-linear causal relationships [8].However, since GPACE scales ∼ T 3 , we apply for computational reasons the linear ParCorr test.)We let PCMCI optimize the significance level by the Akaike Information criterion.tsFCI: We set the maximum delay to three time steps, equivalent to the maximum delay that can be found by TCDF in our AD-DSTCNs with K = 4 and L = 0. We experimented with cutoff value for p-values ∈ {0.001, 0.01, 0.1} and chose 0.01 because it gave the best results (and is also the default setting).Since tsFCI is in theory conservative [21], we applied the majority rule to make tsFCI slightly less conservative.We only take the discovered direct causes into account and disregard other edge types which denote uncertainty or the presence of a hidden confounder.Only in the experiment to discover hidden confounders, we look at all edge types.TiMINo: We set the maximum delay to 3, equivalent to the maximum delay that can be found by TCDF in our AD-DSTCNs with K = 4 and L = 0. We assumed a linear time series model, including instantaneous effects and shifted time series.(The authors present two other variants besides the linear model, of which 'TiMINo-GP' was shown to be more suitable for time series with more than 300 time steps [22], but only the linear model was fully implemented by the authors.)We experimented with significance level ∈ {0.05, 0.01, 0.001}.However, TiMINo did not give any result for all of the significance levels.Therefore, we set it to 0 such that TiMINo always obtains a DAG.</p>
<p>Evaluation Measures</p>
<p>In this section we describe how we evaluated the prediction performance of the time series, the discovered causal relationships, the discovered delays, the influence of the causal validation step with PIVM and the ability to detect hidden confounders.</p>
<p>For measuring the prediction performance for times series, we report the mean absolute scaled error (MASE), since it is invariant to the scale of the time series values and is stable for values close to zero (as opposed to the mean percentage error) [61].</p>
<p>We evaluate the discovered causal relationships in the learnt graph G L by looking at the presence and absence of directed edges compared to the ground truth graph G G .Since causality is asymmetric, all edges are directed.We used the standard evaluation measures precision and recall defined in terms of True Positives (TP), False Positives (FP) and False Negatives (FN).We apply the usual definitions from graph comparison, such that:
TP = |E(G G ) ∩ E(G L )|, FP = |E(G L ) \ E(G G )|, FN = |E(G G ) \ E(G L )|
where E(G) is the set of all edges in graph G.These TP and FP measures evaluate G L only based on the direct causes in G G .However, also an indirect cause has, although indirectly, a causal influence on the effect.Counting an indirect cause as a False Positive would not be objective (see Figure 12a,c for an example).We therefore construct the full ground-truth graph G F from the ground truth graph G G by adding edges that correspond to indirect causal relationships.This means that the full ground truth graph G F contains a directed edge e i,j for each directed path v i , v k , v j in ground truth graph G G .An example is given in Figure 12.Note that we do not adapt the False Negatives calculation, since methods should not be punished for excluding indirect causal relationships in their graph.Comparing the full ground-truth graph with the learnt graph we obtain the following measures:
TP' = |E(G F ) ∩ E(G L )|, FP' = |E(G L ) \ E(G F )|, F1 = 2TP 2TP + 2FN + FP , F1' = 2TP 2TP + 2FN + FP . 1 3 X 1 X 2 X 3 (a) Ground truth G G 1 3 X 1 X 2 X 3 4 (b) Full ground truth G F 4 X 1 X 3 (c) Learnt G L Figure 12.
Example with three variables showing that G L has TP = 0, FP = 1 (e 1,3 ), TP' = 1 (e 1,3 ), FP' = 0 and FN = 2 (e 1,2 and e 2,3 ).Therefore, F1 = 0 and F1'= 0.5.</p>
<p>We evaluate the discovered delay d(e i,j ∈ G L ) between cause X i and effect X j by comparing it to the full ground truth delay d(e i,j ∈ G F ).By comparing it to the full ground truth, we not only evaluate the delay of direct causal relationships, but can also evaluate if the discovered delay of indirect causal relationships is correct.The ground truth delay of an indirect causal relationship is the sum of the delays of its direct relationships.We only evaluate the delay of True Positive edges since the other edges do not exist in both the full ground truth graph G F and the learnt graph G L .We measure the percentage of delays on correctly discovered edges w.r.t. the full ground-truth graph.</p>
<p>We summarize the PIVM effectiveness by calculating the relative increase (or decrease) of the F1-score and F1'-score when PIVM is applied compared to when it is not.The goal of the Permutation Importance Validation Method (PIVM) is to label a subset of the potential causes as true causes.</p>
<p>We evaluate whether a causal discovery method discovers the existence of a hidden confounder between two time series by applying it to the FINANCE HIDDEN benchmark and counting how many hidden confounders were discovered.As discussed in Section 4.3.2,TCDF should be able to discover the existence of a hidden confounder between two time series X i and X j when the confounder has equal delays to its effects X i and X j .If the confounder has unequal delays to its effects, we expect that TCDF will discover an incorrect causal relationship between X i and X j .We therefore not only evaluate how many hidden confounders were discovered, but also how many incorrect causal relationships were learnt between the confounder and its effects.</p>
<p>Results</p>
<p>In this section we present the results obtained by the four compared methods for causal discovery and delay discovery.Additionally we show the impact of applying PIVM.The section ends with a small case study showing that TCDF can circumstantially detect hidden confounders.</p>
<p>Overall Performance</p>
<p>We first assessed the general performance of TCDF for predicting time series.The prediction results of TCDF when trained on FINANCE TRAIN, FMRI TRAIN and FMRI T &gt; 1000 TRAIN are shown in Table 3. TCDF (L = 0) predicts time series well, since MASE &lt; 1 so TCDF gives, on average, smaller errors than a naïve method.The good results from FMRI T &gt; 1000 show that (too) short time series in FMRI combined with a complex architecture in TCDF (L = 1, L = 2) are probably the reason that prediction accuracy of TCDF decreases.Table 4 shows F1 and F1'-scores obtained by the four compared methods for causal discovery.Recall that the F1-score evaluates only direct causal relationships (i.e., an edge from vertex v i to vertex v j in the ground truth graph).The F1'-score also evaluates the indirect causal relationships, such that a learnt edge from v i to v j can correspond with a path p = v i , v k , v j in the ground truth graph.When comparing the overall performance on the FINANCE benchmark, TCDF outperforms the other methods.Especially the F1'-score of TCDF is much higher, indicating that a substantial part of the False Positives of TCDF are correct indirect causes.Since Deep Learning models have many parameters that need to be fit during training and therefore usually need more data than models with a less complex hypothesis space [62], TCDF performs slightly worse on the FMRI benchmark compared to FINANCE because of some short time series in FMRI.Whereas all datasets in FINANCE contain 4000 time steps, FMRI contains only six (out of 27) datasets with more than 1000 time steps.The results for TCDF when applied only to datasets with T &gt; 1000 are therefore better than the overall average from all datasets.For FMRI T &gt; 1000, our results are slightly better than the performance of PCMCI, and TCDF clearly outperforms tsFCI and TiMINo.PCMCI is not affected by time series length and performs comparably for both FMRI benchmarks.TiMINo performs very poorly when applied to FINANCE and only slightly better on FMRI, which is mainly due to a large number of False Positives.TiMINo's poor results are in line with results from the authors, who already stated that TiMINo is not suitable for high-dimensional data [22].In contrast, where TiMINo discovers many incorrect causal relationships, tsFCI seems to be too conservative, missing many causal relationships in all benchmarks.Our poor results of tsFCI correspond with poor results of tsFCI in experiments done by the authors on continuous data [21].In terms of computation time, PCMCI and tsFCI are faster than TCDF for both benchmarks, as shown in Table 5.Table 6 shows the evaluation results for discovering the time delay between cause and effect.Since FMRI does not explicitly include delays and therefore does not have a delay ground truth, we only evaluate FINANCE.PCMCI discovered all delays correctly, closely followed by tsFCI and TCDF.Note that TiMINo only outputs causal relationships without delays.This experiment suggests that our delay discovery algorithm performs well not only without hidden layers (which makes the delay discovery relatively easy), but still keeps the percentage of correctly discovered delays relatively high when the number of hidden layers L (and therefore the number of kernels, the receptive field and maximum delay) is increased.Thus, the number of hidden layers seems of almost no influence for the accuracy of the delay discovery.</p>
<p>Impact of the Causal Validation</p>
<p>Table 7 shows the impact of the causal validation method PIVM by comparing the F1-scores and F1'-scores of TCDF with and without PIVM.The results for FINANCE show that performance decreases drastically when PIVM is removed.For FMRI and FMRI T &gt; 1000, the F1-scores are exactly the same when TCDF is applied with or without PIVM.</p>
<p>PIVM is probably very effective when applied to the FINANCE benchmark because of the many confounders in FINANCE.The attention mechanism can select one of the effects of a confounder as potential cause of another confounder's effect, but the potential cause will not be labeled as true cause by PIVM.In contrast, there are very few confounders in the datasets of FMRI, which might explain the same scores of TCDF with and without PIVM.This experiment therefore suggests that the impact of causal validation depends on the number of confounders (shared causes) in the data, but will usually not have a negative impact on the causal discovery accuracy.The results of TCDF applied to FINANCE HIDDEN are shown in Table 8.We apply TCDF with L = 1 since this architecture was most accurate for FINANCE.We denote by → a causal relationship that is discovered by TCDF using the method for hidden confounders described in Section 4.3.2.Table 9 shows a comparison between TCDF, PCMCI, tsFCI and TiMINo.
-1A X 16 X 8 , X 5 X 16 → X 8 , X 16 → X 5 40-1-3 X 7 X 8 , X 3 X 7 → X 8 , X 7 → X 3 40-1-3 X 0 X 5 , X 6 X 5 → X 6 40-1-3 X 8 X 23 , X 4 X 8 → X 23 , X 8 → X 4 40-1-3 X 8 X 15 , X 4 - 40-1-3 X 8 X 24 , X 4 - 40-1-3 X 8 X 24 , X 15 - 40-1-3 X 8 X 24 , X 23 X 24 → X 23 40-1-3 X 8 X 15 , X 23 -
It can be seen in Table 8 that TCDF discovered all hidden confounders with equal delays to the confounder's effects, which corresponds with our expectations.In two out of six cases, TCDF incorrectly learnt a causal relationship between the effects of a hidden confounder with unequal delays.TCDF (correctly) did not detect a causal relationship between some effects of the hidden confounder X 8 , because the attention mechanism did not discover the potential causal relationships.We think that a non-causal correlation that arises because of the hidden confounder with unequal delays was too weak to be selected as potential cause by the attention mechanism, which indicates that our attention interpretation method to select potential causes is effective and strict enough.</p>
<p>Table 9. Results of TCDF compared with PCMCI, tsFCI and TiMINo when applied to datasets with hidden confounders.The first row denotes the number of incorrect causal relationships that were discovered between the effects of the hidden confounders.The second row denotes the number of hidden confounders that were located.Whereas TCDF discovered two incorrect causal relationships because of a hidden confounder, PCMCI did not discover any incorrect causal relationship.However, in contrast to TCDF, PCMCI does not give any indication that two particular time series are correlated, or that there might be a hidden confounder between these time series.tsFCI should handle hidden confounders by including a special edge type (X i ↔ X j ) that shows that X i is not a cause of X j and that X j is not a cause of X i .However, the results of tsFCI in our experiment are not in accordance with the theoretical claims, since tsFCI did not discover any hidden confounder.In three cases, it even discovered incorrect causal relationships.TiMINo discovered in all cases except one an indirect causal relationship.</p>
<p>This case study suggests that TCDF performs as expected by successfully discovering the presence of a hidden confounder when the delays to the confounder's effects are equal and, in some cases, incorrectly discovering a causal relationship between the confounder's effects when the delays to the effects are unequal.Compared to other approaches, PCMCI performs better in terms of not discovering any incorrect causal relationships between the confounder's effects, but TCDF is the only method capable of locating the presence of a hidden confounder.</p>
<p>Summary</p>
<p>Besides being accurate in predicting time series, TCDF correctly discovers most causal relationships.TCDF outperforms the compared methods (PCMCI, tsFCI and TiMINo) in terms of causal discovery accuracy when applied to FINANCE and FMRI T &gt; 1000.Since a Deep Learning method has many parameters to fit, TCDF performs slightly worse on short time series in FMRI.In contrast, the accuracy of PCMCI is not affected by time series length.Although computation time is not so relevant in the domain of knowledge extraction, PCMCI is faster than TCDF.TCDF discovers roughly 95%-97% of delays correctly, which is only slightly worse than PCMCI and tsFCI.TCDF is the only method to locate the presence of a hidden confounder but, contrary to PCMCI, discovers in some cases an incorrect causal relationship between a confounder's effects.</p>
<p>Discussion</p>
<p>Since a causal discovery method based on observational data cannot physically intervene in a system to check if manipulating the cause changes the effect, causal discovery methods are principally used to discover and investigate hypotheses.Therefore, a constructed temporal causal graph by TCDF (and any other causal discovery method) should be interpreted as a hypothetical graph, learnt from observational time series data, which can subsequently be confirmed by a domain expert or experimentation.This is especially relevant in the case of spurious correlations, where the values of two unrelated variables are coincidentally statistically correlated.A causal discovery method will probably label a spurious correlation as a causal relationship if there are no counterexamples available.Only based on domain knowledge or experiments, one can conclude that the discovered causal relationship is incorrect.However, whereas most researchers are aware that real-life experiments are considered the "gold standard" for causal inference, manipulation of the independent variable of interest will often be unfeasible, unethical, or simply impossible [63].Thus, causal discovery from observational data is often the better (or only) option.</p>
<p>As shown in the previous section, our Temporal Causal Discovery Framework can discover causal relationships from time series data, including a correctly discovered time delay.In the following sections, we will discuss the limitations of our approach as well as the sensitivity of TCDF to hyperparameters.</p>
<p>Hyperparameters</p>
<p>In the experiments, we applied TCDF with different values for L, the number of hidden layers in the depthwise convolutions.From Table 4, we can conclude that the F1-scores of the FINANCE benchmark barely differ across different values for L. TCDF L = 2 performs worst on FMRI because the architecture is probably too complex for the dataset (there are too many parameters to fit) and the receptive field (and therefore the maximum delay) is unnecessary large.The results for TCDF with L = 2 improve substantially when applied to time series having more than 1000 time steps.Thus, the best number of hidden layers depends on the dataset and mainly on the length of the time series.</p>
<p>The number of hidden layers also influences the receptive field: TCDF with L = 2 and kernel size K = 4 has a receptive field of 64 time steps.Since the maximum delay in the FINANCE benchmark is three time steps, it might be more challenging for TCDF to discover the correct patterns.Interestingly, increasing the number of hidden layers barely influences the number of correctly discovered delays.The experiments show that despite the more complex delay discovery and the increased receptive field, our delay discovery algorithm correctly discovers almost all delays.</p>
<p>The underlying causal structure is not known when TCDF is applied to actual data, so the number of hidden layers is a hyperparameter that is difficult to choose.Since the receptive field should be as least as large as the expected maximum delay in the dataset, our first rule of thumb would be that when a large time delay is expected, more dilated hidden layers can be included, such that the receptive field increases exponentially.Secondly, the length of the time series can give an indication of the number of hidden layers.Short time series seem to require fewer hidden layers.</p>
<p>In our experiments, TCDF performs reasonably well on all benchmarks with L = 0.For future work, it is interesting to study whether this also holds for datasets with a much larger time delay between cause and effect, since a larger receptive field is required there.We also note that TCDF with L = 0 (i.e., no hidden layers in the depthwise convolution) is conceptually equivalent to a 2D convolution with one channel and a 2D kernel with a height equal to the number of time series.It is interesting to study whether such a simple 2D convolutional architecture with an attention mechanism would give significantly different results than our AD-DSTCN architecture with L = 0.</p>
<p>Besides the varying number of hidden layers, there are a few other hyperparameters than can be optimized: number of epochs, learning rate, loss function and learning rate.We leave this for future work.</p>
<p>Limitations of Experiments</p>
<p>A limitation of our experiments is that TCDF is only applied to two benchmarks (although containing multiple datasets).Especially our conclusions with respect to the detection of hidden confounders should be carefully considered, since the case study is only based on two datasets of one benchmark.The datasets in the benchmarks have some specific properties: the time series are stationary and there is only a small time delay between cause and effect.Furthermore, both benchmarks do not contain feedback loops (except self-causation).TCDF should be evaluated to more datasets with varying properties to evaluate the performance TCDF in more detail.Generating datasets will allow us to specifically control desired properties, such as noise level, (non-)linearity, (non-)stationarity, feedback loops, length of time series and time delays.</p>
<p>Summary and Future Work</p>
<p>In this paper, we introduced the Temporal Causal Discovery Framework (TCDF), a deep learning approach for causal discovery from time series data.TCDF consists of multiple attention-based convolutional neural networks which we call Attention-based Dilated Depthwise Separable Temporal Convolutional Networks (AD-DSTCNs).These networks have an architecture that is tailored towards predicting a time series based on a multivariate temporal dataset.Our experiments indicate that the implemented attention mechanism is accurate in discovering time series that are a potential cause of the predicted time series.TCDF interprets the attention scores and subsequently applies a causal validation step that randomly permutes the values of a potential cause to effectively distinguish causality from correlation.Our framework also interprets the internal parameters of each AD-DSTCN to discover the time delay between cause and effect.TCDF summarizes its findings by constructing a temporal causal graph that shows the discovered causal relationships between time series and their corresponding time delays.This temporal causal graph can be used for data interpretation and knowledge discovery, and might serve as a useful graphical explanation in the field of Explainable Artificial Intelligence.</p>
<p>We evaluated TCDF on two benchmarks with multiple datasets, both having a ground truth containing the underlying causal graph.TCDF discovered most of the causal relationships in the benchmark containing simulated financial data, and outperformed the existing methods we compared with.TCDF performed slightly worse on the benchmark containing neuroscientific FMRI data, which was mainly due to some short time series in the benchmark.When evaluating TCDF only on time series with at least 1000 time steps, TCDF outperforms the other compared methods.By interpreting the networks' internal parameters, TCDF discovered roughly 95-97% of the time delays correctly, which is only slightly worse than the delay discovery accuracy of other methods.In a small case study, we showed that TCDF can circumstantially locate the existence of hidden confounders.</p>
<p>Future work includes hyperparameter optimization, and applying TCDF to more datasets with different noise-levels, (non-)stationarity and various time delays.We might be able to increase performance by improving the attention interpretation or studying other causal validation methods.</p>
<p>Figure 1 .
1
Figure 1.A temporal causal graph learnt from multivariate observational time series data.A graph node models one time series.A directed edge denotes a causal relationship and is annotated with the time delay between cause and effect.</p>
<p>X 1 is a confounder of X 2 and X 3 with a delay of 1 resp.4 time steps.</p>
<p>Figure 2 .
2
Figure 2. Temporal causal graphs showing causal relationships and delays between cause and effect.</p>
<p>Figure 3 .
3
Figure 3. Overview of Temporal Causal Discovery Framework (TCDF).With time series data as input, TCDF performs four steps (gray boxes) using the technique described in the white box and outputs a temporal causal graph.</p>
<p>f = 2 3 f = 2 2 f = 2 1 fFigure 5 .
3215
Figure 5. Dilated TCN to predict X 2 , with L = 3 hidden layers, kernel size K = 2 (shown as arrows) and dilation coefficient c = 2, leading to a receptive field R = 16.A PReLU activation function is applied after each convolution.To predict the first values (shown as dashed arrows), zero padding is added to the left of the sequence.Weights are shared across layers, indicated by the identical colors.</p>
<p>Figure 6 .
6
Figure 6.Attention-based Dilated Depthwise Separable Temporal Convolutional Network N 2 to predict target time series X 2 .The N channels have T = 13 time steps, L = 1 hidden layer in the depthwise convolution and N × 2 kernels with kernel size K = 2 (denoted by colored blocks).The attention scores a are multiplied element-wise with the input time series, followed by an element-wise multiplication with the kernel.In the pointwise convolution, all channel outputs are combined to construct the prediction X2 .</p>
<p>2 Figure 7 .
27
Figure 7. Threshold τ j is set equal to the attention score at the left side of the largest gap g k where k = 0 and k &lt; |G|/2.In this example, τ j is set equal to the third largest attention score.</p>
<p>Figure 10 .
10
Figure 10.Example datasets and causal graphs: simulation 17 from FMRI (top), graph 20-1A from FINANCE (bottom).A colored line corresponds to one time series (node) in the causal graph.</p>
<p>Figure 11 .
11
Figure 11.Adapted ground truth for the hidden confounder experiment, showing graphs 20-1A (left) and 40-1-3 (right) from FINANCE.Only one grey node was removed per experiment.</p>
<p>Table 1 .
1
Causal discovery methods for time series data, classified among various dimensions.
ConfoundersHidden Conf.InstantaneousDelayMultivariateContinuousNon-StationaryNon-LinearNoiseAlgorithmMethodFeaturesDataOutputα(c, e) [9]Causal SignificanceCausal relationships, delay and impactCGC [10]GrangerCausal relationships with causal influencePCMCI [8]Constraint-basedCausal time series graph, delay and causal strengthANLTSM [20]Constraint-based1Partial Ancestral Graph with node for each time steptsFCI [21]Constraint-basedPartial Ancestral Graph with node for each time stepTiMINo</p>
<p>Table 2 .
2
Summary of evaluation benchmarks.Delays between cause and effect not available in FMRI.
FINANCEFMRIFMRI T &gt; 1000#datasets9276#non-stationary datasets010#variables (time series)25∈ {5, 10, 15}{5, 10}#causal relationships∈ {6, 20, 40} ∈ {10, 12, 13, 21, 33}∈ {10, 21}time series length400050-5000 (mean: 774) 1000-5000 (mean: 2867)delays [timesteps]1-3n.a.n.a.self-causationconfounderstype of relationshiplinearnon-linearnon-linear</p>
<p>Table 3 .
3
Time series prediction performance of TCDF, in terms of the mean absolute scaled error (MASE) averaged across all datasets, plus its standard deviation.Best results are highlighted in bold.
FINANCE TESTFMRI TEST FMRI T &gt; 1000 TESTTCDF (L = 0)0.38 ± 0.090.84 ± 0.380.71 ± 0.05TCDF (L = 1)0.38 ± 0.101.06 ± 0.490.72 ± 0.07TCDF (L = 2)0.40 ± 0.101.13 ± 0.450.74 ± 0.08</p>
<p>Table 4 .
4
Causal discovery overview for all data sets and all methods.Showing macro-averaged F1 and F1' scores and standard deviations.The highest score per benchmark is highlighted in bold.</p>
<p>FINANCE (9 Data Sets) FMRI (27 Data Sets) FMRI T &gt; 1000 (6 Data Sets)
F1F1'F1F1'F1F1'TCDF (L = 0) 0.64 ± 0.06 0.77 ± 0.08 0.60 ± 0.09 0.63 ± 0.09 0.68 ± 0.050.68 ± 0.05TCDF (L = 1) 0.65 ± 0.09 0.78 ± 0.10 0.58 ± 0.15 0.62 ± 0.14 0.65 ± 0.130.68 ± 0.11TCDF (L = 2) 0.64 ± 0.09 0.77 ± 0.09 0.55 ± 0.13 0.63 ± 0.11 0.70 ± 0.090.73 ± 0.08PCMCI0.55 ± 0.22 0.56 ± 0.22 0.63 ± 0.10 0.67 ± 0.11 0.67 ± 0.040.67 ± 0.04tsFCI0.37 ± 0.11 0.37 ± 0.12 0.49 ± 0.22 0.49 ± 0.22 0.48 ± 0.280.48 ± 0.28TiMINo0.13 ± 0.05 0.21 ± 0.10 0.23 ± 0.12 0.37 ± 0.14 0.23 ± 0.110.37 ± 0.15</p>
<p>Table 5 .
5
Run time in seconds, averaged over all datasets in the benchmark.TCDF (without parallelism) and TiMINo are run on a Ubuntu 16.04.4LTS computer with an Intel R Xeon R E5-2683-v4 CPU and NVIDIA TitanX 12GB GPU.PCMCI and tsFCI are run on a Windows 10 1803 computer with an Intel R Core TM i7-5500U CPU.
TCDF (L = 0) PCMCI tsFCI TiMINoFINANCE318 s10 s93 s499 sFMRI74 s1 s1 s14 s</p>
<p>Table 6 .
6
Delay discovery overview for all data sets of the FINANCE benchmark (nine datasets).Showing macro-averaged percentage of delays that are correctly discovered w.r.t. the full ground truth, and standard deviation.TiMINo does not discover delays.
TCDF (L = 0) TCDF (L = 1) TCDF (L = 2)PCMCItsFCITiMINoFINANCE 97.79% ± 2.56 96.42% ± 3.68 95.49% ± 4.15 100.00% ± 0.00 98.77% ± 3.49n.a.</p>
<p>Table 7 .
7
Impact of causal validation step.Showing macro-averaged F1 scores and standard deviation for TCDF with PIVM and TCDF without PIVM.∆ shows the change in F1-score or F1'-score in percent.
FINANCE (9 Data Sets)FMRI (27 Data Sets)FMRI T &gt; 1000 (6 Data Sets)F1F1'F1F1'F1F1'TCDF (L = 0)0.64 ± 0.06 0.77 ± 0.08 0.60 ± 0.09 0.63 ± 0.09 0.68 ± 0.050.68 ± 0.05TCDF (L = 0) w/o PIVM 0.22 ± 0.09 0.30 ± 0.13 0.60 ± 0.09 0.63 ± 0.09 0.68 ± 0.050.68 ± 0.05∆ (PIVM)−66%−61%0%0%0%0%5.4.3. Case Study: Detection of Hidden Confounders</p>
<p>Table 8 .
8
Results of our TCDF (L = 1) applied to FINANCE HIDDEN.'Equal Delays' denotes whether the delays from the confounder (conf.) to the confounder's effects are equal.Grey causal relationships denote that the discovered relationship was not causal according to the ground truth.
Dataset Hidden Conf.EffectsEqual Delays Conf</p>
<p>. Discovered Learnt Causal Relationships 20</p>
<p>Mach. Learn. Knowl. Extr. 2019, 1, 19; doi:10.3390/make1010019 www.mdpi.com/journal/make
Except hidden confounders that are instantaneous and linear[22].
TiMINo stays undecided by not inferring a causal relationship in case of a hidden confounder.
Although theoretically shown, the implemented algorithm does not explicitly output the discovered time delays.
Funding: This research received no external funding.Acknowledgments:The authors would like to thank Maurice van Keulen for the valuable feedback.Conflicts of Interest:The authors declare no conflict of interest.
Why: A Guide to Finding and Using Causes. S Kleinberg, 2015O'Reilly: Springfield, MA, USA</p>
<p>. S Kleinberg, Causality, Time Probability, 2013Cambridge University PressCambridge, UK</p>
<p>AR Identification of Latent-Variable Graphical Models. M Zorzi, R Sepulchre, 10.1109/TAC.2015.2491678IEEE Trans. 612016</p>
<p>Introduction to causal inference. P Spirtes, J. Mach. Learn. Res. 112010</p>
<p>Learning causality and causality-related learning: Some recent progress. K Zhang, B Schölkopf, P Spirtes, C Glymour, 10.1093/nsr/nwx137Natl. Sci. Rev. 52017</p>
<p>The Psychology of Causal Perception and Reasoning. D Danks, The Oxford Handbook of Causation. </p>
<p>. Helen Beebee, C H Menzies, P , 2009Oxford University PressOxford, UKChapter 21</p>
<p>Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. A Abdul, J Vermeulen, D Wang, B Y Lim, M Kankanhalli, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. the 2018 CHI Conference on Human Factors in Computing SystemsMontreal, QC, Canada; New York, NY, USAACMApril 2018. 2018582</p>
<p>Detecting causal associations in large nonlinear time series datasets. J Runge, D Sejdinovic, S Flaxman, arXiv:1702.070072017</p>
<p>Fast and Accurate Causal Inference from Time Series Data. Y Huang, S Kleinberg, Proceedings of the FLAIRS Conference. the FLAIRS ConferenceHollywood, FL, USA18-20 May 2015</p>
<p>A copula approach to assessing Granger causality. M Hu, H Liang, NeuroImage. 1002014</p>
<p>Detecting causality in non-stationary time series using partial symbolic transfer entropy: Evidence in financial data. A Papana, C Kyrtsou, D Kugiumtzis, C Diks, 10.1007/s10614-015-9491-xComput. Econ. 472016</p>
<p>Neural Networks: An Introduction. B Müller, J Reinhardt, M T Strickland, 2012SpringerBerlin/Heidelberg, Germany</p>
<p>Causal modelling combining instantaneous and lagged effects: An identifiable model based on non-Gaussianity. A Hyvärinen, S Shimizu, P O Hoyer, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine LearningHelsinki, FinlandJuly 2008</p>
<p>Causal discovery algorithms: A practical guide. D Malinsky, D Danks, Philos. 13e124702018</p>
<p>Estimating the directed information to infer causal relationships in ensemble neural spike train recordings. C J Quinn, T P Coleman, N Kiyavash, N G Hatsopoulos, 10.1007/s10827-010-0247-2J. Comput. Neurosci. 302011</p>
<p>On the identifiability of dynamical networks. M Gevers, A S Bazanella, A Parraga, 10.1016/j.ifacol.2017.08.1310201750</p>
<p>Analysing connectivity with Granger causality and dynamic causal modelling. K Friston, R Moran, A K Seth, 10.1016/j.conb.2012.11.010Curr. Opin. Neurobiol. 232013</p>
<p>Elements of Causal Inference: Foundations and Learning Algorithms. J Peters, D Janzing, B Schölkopf, 2017MIT PressCambridge, MA, USA</p>
<p>Identifying Causal Relationships in Case of Non-Stationary Time Series. A Papana, K Kyrtsou, D Kugiumtzis, C Diks, 2014AmsterdamThe NetherlandsUniversiteit van AmsterdamTechnical Report</p>
<p>Search for additive nonlinear time series causal models. T Chu, C Glymour, J. Mach. Learn. Res. 92008</p>
<p>On causal discovery from time series data using FCI. D Entner, P O Hoyer, Proceedings of the Fifth European Workshop on Probabilistic Graphical Models. the Fifth European Workshop on Probabilistic Graphical ModelsHelsinki, FinlandSeptember 2010</p>
<p>Causal inference on time series using restricted structural equation models. J Peters, D Janzing, B Schölkopf, Advances in Neural Information Processing Systems. Cambridge, MA, USAThe MIT Press2013</p>
<p>Universal estimation of directed information. J Jiao, H H Permuter, L Zhao, Y H Kim, T Weissman, 10.1109/TIT.2013.2267934IEEE Trans. Inf. Theory. 592013</p>
<p>Investigating causal relations by econometric models and cross-spectral methods. C W Granger, 10.2307/1912791Econom. J. Econom. Soc. 371969</p>
<p>Frequency decomposition of conditional Granger causality and application to multivariate neural field potential data. Y Chen, S L Bressler, M Ding, 10.1016/j.jneumeth.2005.06.011J. Neurosci. Methods. 1502006</p>
<p>Sparse plus low rank network identification: A nonparametric approach. M Zorzi, A Chiuso, 10.1016/j.automatica.2016.08.014Automatica. 762017</p>
<p>Kernel method for nonlinear Granger causality. D Marinazzo, M Pellicoro, S Stramaglia, 10.1103/PhysRevLett.100.144103Phys. Rev. Lett. 1002008. 144103</p>
<p>Attention-dependent modulation of cortical taste circuits revealed by Granger causality with signal-dependent noise. Q Luo, T Ge, F Grabenhorst, J Feng, E T Rolls, 10.1371/journal.pcbi.1003265PLoS Comput. Biol. 92013. e1003265</p>
<p>Causal discovery and inference: Concepts and recent methodological advances. P Spirtes, K Zhang, Applied Informatics. Berlin, GermanySpringer201633</p>
<p>Scheines, R. Causation, Prediction, and Search. P Spirtes, C N Glymour, 2000MIT PressCambridge, MA, USA</p>
<p>The relationship between transfer entropy and directed information. Y Liu, S Aviyente, Proceedings of the Statistical Signal Processing Workshop (SSP). the Statistical Signal Processing Workshop (SSP)Ann Arbor, MI, USAAugust 2012</p>
<p>An Interpretable LSTM Neural Network for Autoregressive Exogenous Model. T Guo, T Lin, Y Lu, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsVancouver, BC, Canada30 April-3 May 2018</p>
<p>Causal effect inference with deep latent-variable models. C Louizos, U Shalit, J M Mooij, D Sontag, R Zemel, M Welling, Advances in Neural Information Processing Systems. Cambridge, MA, USAThe MIT Press2017</p>
<p>. O Goudet, D Kalainathan, P Caillou, I Guyon, D Lopez-Paz, M Sebag, arXiv:1711.08936v2Causal Generative Neural Networks. arXiv. 2018</p>
<p>D Kalainathan, O Goudet, I Guyon, D Lopez-Paz, M Sebag, Sam, arXiv:1803.04929Structural Agnostic Model, Causal Discovery and Penalized Adversarial Learning. 2018</p>
<p>Convolutional Sequence Modeling Revisited. S Bai, J Z Kolter, V Koltun, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsVancouver, BC, Canada30 April-3 May 2018</p>
<p>Learning long-term dependencies with gradient descent is difficult. Y Bengio, P Simard, P Frasconi, 10.1109/72.279181IEEE Trans. Neural Netw. 51994</p>
<p>Convolutional Sequence to Sequence Learning. J Gehring, M Auli, D Grangier, D Yarats, Y N Dauphin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, AustraliaAugust 201770</p>
<p>Conditional image generation with pixelCNN decoders. A Van Den Oord, N Kalchbrenner, L Espeholt, O Vinyals, A Graves, K Kavukcuoglu, Advances in Neural Information Processing Systems. Cambridge, MA, USAThe MIT Press2016</p>
<p>Conditional time series forecasting with convolutional neural networks. A Borovykh, S Bohte, C W Oosterlee, Lecture Notes in Computer Science/Lecture Notes in Artificial Intelligence. 2017Springer</p>
<p>M Binkowski, G Marti, P Donnat, Autoregressive, arXiv:1703.04122Convolutional Neural Networks for Asynchronous Time Series. 2017</p>
<p>On the usefulness of attention for object recognition. D Walther, U Rutishauser, C Koch, P Perona, Proceedings of the Workshop on Attention and Performance in Computational Vision at ECCV. the Workshop on Attention and Performance in Computational Vision at ECCVPrague, Czech Republic15 May 2004</p>
<p>ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs. W Yin, H Schütze, B Xiang, B Zhou, 10.1162/tacl_a_00097Trans. Assoc. Comput. Linguist. 42016</p>
<p>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). the 2015 IEEE International Conference on Computer Vision (ICCV)Santiago, ChileDecember 2015</p>
<p>A Van Den Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, A Senior, K Kavukcuoglu, Wavenet, arXiv:1609.03499A generative model for raw audio. 2016</p>
<p>Rigid-Motion Scattering for Image Classification. L Sifre, S Mallat, 2014. 15 October 2018</p>
<p>Xception: Deep Learning with Depthwise Separable Convolutions. F Chollet, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Honolulu, HI, USAJuly 2017</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Las Vegas, NV, USAJune 2016</p>
<p>From softmax to sparsemax: A sparse model of attention and multi-label classification. A Martins, R Astudillo, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningNew York, NY, USAJune 2016</p>
<p>Reinforced Self-Attention Network: A Hybrid of Hard and Soft Attention for Sequence Modeling. T Shen, T Zhou, G Long, J Jiang, S Wang, C Zhang, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. the Twenty-Seventh International Joint Conference on Artificial IntelligenceStockholm, Sweden2018. July 2018</p>
<p>Causal inference in time series analysis. M Eichler, Causality: Statistical Perspectives and Applications. Hoboken, NJ, USAWiley2012</p>
<p>Making Things Happen: A Theory of Causal Explanation. J Woodward, 2005Oxford University PressOxford, UK</p>
<p>Random forests. L Breiman, 10.1023/A:1010933404324Mach. Learn. 452001</p>
<p>Statistical inference for variable importance. M J Van Der Laan, 10.2202/1557-4679.1008Int. J. Biostat. 22006</p>
<p>Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. A Datta, S Sen, Y Zick, Proceedings of the IEEE Symposium on Security and Privacy (SP). the IEEE Symposium on Security and Privacy (SP)San Jose, CA, USAMay 2016</p>
<p>Quantifying causal influences. D Janzing, D Balduzzi, M Grosse-Wentrup, B Schölkopf, 10.1214/13-AOS1145Ann. Stat. 412013</p>
<p>The cross-section of expected stock returns. E F Fama, K R French, 10.1111/j.1540-6261.1992.tb04398.xJ. Financ. 471992</p>
<p>. S M Smith, K L Miller, G Salimi-Khorshidi, M Webster, C F Beckmann, T E Nichols, J D Ramsey, M W Woolrich, 10.1016/j.neuroimage.2010.08.063Network modelling methods for FMRI. Neuroimage. 542011</p>
<p>Dynamics of blood flow and oxygenation changes during brain activation: The balloon model. R B Buxton, E C Wong, L R Frank, 10.1002/mrm.1910390602Magn. Reson. Med. 391998</p>
<p>A method for stochastic optimization. D P Kingma, J Ba, Adam, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsSan Diego, CA, USA2014. May 2015</p>
<p>Automatic Time Series Forecasting: The forecast Package for R. R Hyndman, Y Khandakar, 10.18637/jss.v027.i03J. Stat. Softw. 27954052008</p>
<p>Deep Learning. I Goodfellow, Y Bengio, A Courville, 2016. 3 December 2018MIT PressCambridge, MA, USA</p>
<p>Thinking clearly about correlations and causation: Graphical causal models for observational data. J M Rohrer, 10.1177/2515245917745629Adv. Methods Pract. Psychol. Sci. 12018Licensee MDPI. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license</p>            </div>
        </div>

    </div>
</body>
</html>
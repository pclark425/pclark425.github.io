<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7851 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7851</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7851</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-271891943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.08781v1.pdf" target="_blank">Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</a></p>
                <p><strong>Paper Abstract:</strong> LLMs-as-a-judge is a recently popularized method which replaces human judgements in task evaluation (Zheng et al. 2024) with automatic evaluation using LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement, such as the coherence of a text. While this seems beneficial, it is not clear whether the assessments by an LLM-as-a-judge constitute only an evaluation based on the instructions in the prompts, or reflect its preference for high-quality data similar to its fine-tune data. To investigate how much influence prompting the LLMs-as-a-judge has on the alignment of AI judgements to human judgements, we analyze prompts with increasing levels of instructions about the target quality of an evaluation, for several LLMs-as-a-judge. Further, we compare to a prompt-free method using model perplexity as a quality measure instead. We aggregate a taxonomy of quality criteria commonly used across state-of-the-art evaluations with LLMs and provide this as a rigorous benchmark of models as judges. Overall, we show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7851.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7851.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregated rubric effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate effect of adding full rubric information to LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across 8 benchmark datasets and 34 criteria, providing more detailed rubric instructions to LLMs-as-a-judge increases Pearson agreement with human judgements only marginally (~4% overall), and can sometimes reduce performance for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Aggregate across NLG and reasoning evaluation tasks (summarization, story generation, reasoning, dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Aggregate (8 benchmark datasets: SummEval, TopicalChat, OpinSummEval, InstruSumm, Hanna, TheNextChapter, Roscoe, Flask)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Models evaluated include GPT4-Turbo-0125 (closed, large), Llama3 70b and 8b (open), Mistral-v0.3 (small), Phi3 Medium-128k (fine-tuned for reasoning), Prometheus-2 (fine-tuned/eval-specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators as provided by each benchmark dataset (dataset-specific human annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>only marginal overall improvement from detailed rubrics; possible performance decrease for some models when given full rubrics (e.g., Phi3); limited benefit except for some individual metrics</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Providing full rubric information yields only a small aggregate gain (~4% increase in Pearson correlation) and can sometimes harm agreement for certain models; strongest models (GPT-4) show high agreement even with minimal prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Indicates that high-performing LLMs can align with humans without expensive rubric engineering, reducing annotation protocol complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Four prompting settings (1 Perplexity/no prompt, 2 Generic quality prompt, 3 Criteria-specific prompt (name only), 4 Full rubric prompt with definitions and score guidance). Pearson correlation between model scores (or perplexity) and human annotations averaged across criteria; temperature=0.3; average of generated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7851.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity vs Prompting (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model perplexity compared to prompted LLM-as-a-judge for textual content metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For text/content-related quality criteria, model perplexity (prompt-free) can correlate with human judgements as well as or better than prompted LLM judgements; reported Pearson correlations 0.51 for perplexity vs 0.44 for prompted LLM scoring on textual content metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Textual quality evaluation (content-based metrics; e.g., fluency, structural text quality) across summarization and story generation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Aggregate textual-content related metrics (e.g., SummEval, Hanna and other content-focused subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Perplexity-based scoring (model-specific perplexity) vs prompted LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Perplexity computed under the corresponding LLM (same family used to compute perplexity as judge comparisons); prompting experiments used same LLMs under generic/specific/full-rubric prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.51</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Prompted LLMs (simple prompts) can underperform perplexity for structural/textual criteria; prompting sometimes reduces alignment compared to prompt-free perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Perplexity (no prompt) achieved a Pearson correlation of 0.51 on textual content-related metrics vs 0.44 for prompted LLM judgements, suggesting perplexity may better reflect human judgements for many text-quality aspects without prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Perplexity requires no prompt engineering and transparently measures alignment with model training data; it's simple and can outperform prompt-based judgements on pure text-quality criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Perplexity scoring (no prompt) contrasted with three prompting levels (generic, criterion name only, full rubric); correlations computed per criterion against human annotations; average across content-related criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7851.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Content-based perplexity lift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity yields >20% higher agreement for content-based criteria vs prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On content-based (textual) quality criteria, the paper reports that agreement with human annotations is on average more than 20% higher when using model perplexity compared to prompting the model for scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Content/textual quality evaluation (fluency, grammar, structural quality)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Content-focused subsets across datasets (e.g., SummEval fluency annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Perplexity (prompt-free) vs prompted LLM scoring</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Perplexity computed per evaluated LLM; prompting used LLM-as-a-judge under generic and rubric prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from benchmark datasets (e.g., SummEval)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Prompt-based LLM judgements can be substantially less aligned with humans on structural/textual criteria than perplexity; prompts add little benefit for content measures</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>For structural textual quality (e.g., fluency), perplexity often outperforms prompt-based scoring, with average agreement >20% higher; simple generic prompting produces similar results to more detailed prompts for many content criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Perplexity is simpler, prompt-free, and often more aligned with human judgments for text-structure related metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparison of Pearson correlations between perplexity and human annotations vs prompted LLM scores; models evaluated under four settings (perplexity, generic, criteria name, full rubric).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7851.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on logical correctness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (as judge) performance on integrity/logical correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4-as-a-judge attains substantially higher agreement with human annotations for logical correctness (integrity) on the Flask reasoning dataset, with Pearson correlation reported at 0.68, outperforming other evaluated models by a wide margin.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Task-specific reasoning evaluation (logical correctness of step-by-step solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Flask (integrity/logical correctness criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT4-Turbo-0125 (closed, large model used as judge in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from the Flask benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.68</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Smaller or less capable models (Llama3 70b, Phi3, etc.) show much lower agreement (examples: Llama3 70b 0.34, Phi3 0.33) and tend to over-score incorrect solutions as high</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 is able to assign low scores to logically incorrect solutions (matching human judgments), while many other models predominantly give high scores to bad logical responses; suggests judge capability correlates with ability to solve/understand the task.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High-capacity models like GPT-4 can reliably evaluate task-specific integrity criteria close to human performance without extensive rubric engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Integrity evaluations on Flask using all four prompting settings; Pearson correlation computed between model-generated scores (or perplexity) and human annotations; plotting of score distributions showed GPT-4 produced lower scores for poor responses unlike other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7851.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Engagement criteria low agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge performance on engagement-based criteria (empathy example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Engagement-based criteria (subjective measures like empathy) show lower agreement with human annotations and higher variance across model scores; reported Pearson values around 0.30â€“0.32 for top models (GPT-4 0.32, Phi3 0.31, Llama3-8b 0.30 for empathy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Engagement/subjective evaluation (e.g., empathy in story/dialogue generation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Hanna (empathy criterion) and other engagement-focused criteria</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4; Phi3; Llama3-8b (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT4-Turbo-0125; Phi3 Medium-128k; Llama3 8b</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from the Hanna dataset (subjective ratings with high variance)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.32</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Low overall agreement with human annotators; high variance in both human labels and model outputs; models often cluster around single scores (low diversity) except some (Phi3) with higher variability</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Engagement metrics are more subjective and harder for models to match; inter-model differences are smaller and model outputs exhibit greater variance; Phi3 produced more variable scores while many models cluster.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>None strongly reported for engagement criteria; full rubric information can help but gains are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluation under four prompt settings; violin plots of generated scores vs human annotations; Pearson correlations reported per model for empathy example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7851.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi3 rubric sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi3 performance sensitivity to full rubric prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phi3's agreement with human judgements decreased when full rubric details were provided compared to simpler prompts that only named the criterion, indicating a sensitivity to prompt instruction detail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Aggregate across multiple criteria where Phi3 was evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Aggregate (subset of evaluated datasets where Phi3 was used)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Phi3</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Phi3 Medium-128k (reasoning-finetuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Performance can degrade when detailed rubric information is provided (Phi3 had higher agreement with humans with simpler prompts); indicates mismatch between model priors and rubric guidance</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Phi3's prior knowledge about evaluation sometimes aligns better with humans than when given explicit full rubric instructions, suggesting instruction overrides can hurt some fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Phi3's prior evaluation behavior may be useful without heavy prompt engineering in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparison of four prompt settings; observation that Phi3's Pearson correlation decreased for full rubric prompt relative to simpler criteria-name prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7851.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7851.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prometheus-2 limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prometheus-2 poor prompting performance and reliance on perplexity/full-rubric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prometheus-2 did not perform well under simple or criterion-specific prompts (settings 2 and 3) and the authors report only results for perplexity and full-rubric prompts for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluation tasks across datasets where Prometheus-2 was tested</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Subset of datasets used for Prometheus-2 experiments (not explicitly enumerated for this model)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Prometheus-2</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Prometheus-2 (fine-tuned/evaluation-specialized open model)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotations from benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Very poor results in simple and criterion-specific prompt settings (settings 2 and 3), so only perplexity and full-rubric settings reported for Prometheus-2</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Prometheus-2's fine-tuning may bias it in ways that make it fail under typical simple prompting; perplexity and carefully matched rubric prompts were the only settings that produced usable alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted limitation rather than advantage; suggests model-specific fine-tuning can change sensitivity to prompt types.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prometheus-2 evaluated but settings 2 and 3 returned very poor results; only perplexity and full rubric prompting reported for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-ajudge with MT-bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>NLG Evaluation using GPT-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets <em>(Rating: 2)</em></li>
                <li>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores <em>(Rating: 2)</em></li>
                <li>Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models <em>(Rating: 1)</em></li>
                <li>LLM-based NLG Evaluation: Current Status and Challenges <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7851",
    "paper_id": "paper-271891943",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Aggregated rubric effect",
            "name_full": "Aggregate effect of adding full rubric information to LLM-as-a-judge",
            "brief_description": "Across 8 benchmark datasets and 34 criteria, providing more detailed rubric instructions to LLMs-as-a-judge increases Pearson agreement with human judgements only marginally (~4% overall), and can sometimes reduce performance for some models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Aggregate across NLG and reasoning evaluation tasks (summarization, story generation, reasoning, dialogue)",
            "dataset_name": "Aggregate (8 benchmark datasets: SummEval, TopicalChat, OpinSummEval, InstruSumm, Hanna, TheNextChapter, Roscoe, Flask)",
            "judge_model_name": "Multiple (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2)",
            "judge_model_details": "Models evaluated include GPT4-Turbo-0125 (closed, large), Llama3 70b and 8b (open), Mistral-v0.3 (small), Phi3 Medium-128k (fine-tuned for reasoning), Prometheus-2 (fine-tuned/eval-specialized)",
            "human_evaluator_type": "Human annotators as provided by each benchmark dataset (dataset-specific human annotations)",
            "agreement_metric": "Pearson correlation",
            "agreement_score": null,
            "reported_loss_aspects": "only marginal overall improvement from detailed rubrics; possible performance decrease for some models when given full rubrics (e.g., Phi3); limited benefit except for some individual metrics",
            "qualitative_findings": "Providing full rubric information yields only a small aggregate gain (~4% increase in Pearson correlation) and can sometimes harm agreement for certain models; strongest models (GPT-4) show high agreement even with minimal prompting.",
            "advantages_of_llm_judge": "Indicates that high-performing LLMs can align with humans without expensive rubric engineering, reducing annotation protocol complexity.",
            "experimental_setting": "Four prompting settings (1 Perplexity/no prompt, 2 Generic quality prompt, 3 Criteria-specific prompt (name only), 4 Full rubric prompt with definitions and score guidance). Pearson correlation between model scores (or perplexity) and human annotations averaged across criteria; temperature=0.3; average of generated scores.",
            "uuid": "e7851.0",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Perplexity vs Prompting (textual)",
            "name_full": "Model perplexity compared to prompted LLM-as-a-judge for textual content metrics",
            "brief_description": "For text/content-related quality criteria, model perplexity (prompt-free) can correlate with human judgements as well as or better than prompted LLM judgements; reported Pearson correlations 0.51 for perplexity vs 0.44 for prompted LLM scoring on textual content metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Textual quality evaluation (content-based metrics; e.g., fluency, structural text quality) across summarization and story generation",
            "dataset_name": "Aggregate textual-content related metrics (e.g., SummEval, Hanna and other content-focused subsets)",
            "judge_model_name": "Perplexity-based scoring (model-specific perplexity) vs prompted LLM-as-a-judge",
            "judge_model_details": "Perplexity computed under the corresponding LLM (same family used to compute perplexity as judge comparisons); prompting experiments used same LLMs under generic/specific/full-rubric prompts",
            "human_evaluator_type": "Human annotations from benchmark datasets",
            "agreement_metric": "Pearson correlation",
            "agreement_score": 0.51,
            "reported_loss_aspects": "Prompted LLMs (simple prompts) can underperform perplexity for structural/textual criteria; prompting sometimes reduces alignment compared to prompt-free perplexity",
            "qualitative_findings": "Perplexity (no prompt) achieved a Pearson correlation of 0.51 on textual content-related metrics vs 0.44 for prompted LLM judgements, suggesting perplexity may better reflect human judgements for many text-quality aspects without prompt engineering.",
            "advantages_of_llm_judge": "Perplexity requires no prompt engineering and transparently measures alignment with model training data; it's simple and can outperform prompt-based judgements on pure text-quality criteria.",
            "experimental_setting": "Perplexity scoring (no prompt) contrasted with three prompting levels (generic, criterion name only, full rubric); correlations computed per criterion against human annotations; average across content-related criteria.",
            "uuid": "e7851.1",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Content-based perplexity lift",
            "name_full": "Perplexity yields &gt;20% higher agreement for content-based criteria vs prompting",
            "brief_description": "On content-based (textual) quality criteria, the paper reports that agreement with human annotations is on average more than 20% higher when using model perplexity compared to prompting the model for scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Content/textual quality evaluation (fluency, grammar, structural quality)",
            "dataset_name": "Content-focused subsets across datasets (e.g., SummEval fluency annotations)",
            "judge_model_name": "Perplexity (prompt-free) vs prompted LLM scoring",
            "judge_model_details": "Perplexity computed per evaluated LLM; prompting used LLM-as-a-judge under generic and rubric prompts",
            "human_evaluator_type": "Human annotations from benchmark datasets (e.g., SummEval)",
            "agreement_metric": "Pearson correlation",
            "agreement_score": null,
            "reported_loss_aspects": "Prompt-based LLM judgements can be substantially less aligned with humans on structural/textual criteria than perplexity; prompts add little benefit for content measures",
            "qualitative_findings": "For structural textual quality (e.g., fluency), perplexity often outperforms prompt-based scoring, with average agreement &gt;20% higher; simple generic prompting produces similar results to more detailed prompts for many content criteria.",
            "advantages_of_llm_judge": "Perplexity is simpler, prompt-free, and often more aligned with human judgments for text-structure related metrics.",
            "experimental_setting": "Comparison of Pearson correlations between perplexity and human annotations vs prompted LLM scores; models evaluated under four settings (perplexity, generic, criteria name, full rubric).",
            "uuid": "e7851.2",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-4 on logical correctness",
            "name_full": "GPT-4 (as judge) performance on integrity/logical correctness",
            "brief_description": "GPT-4-as-a-judge attains substantially higher agreement with human annotations for logical correctness (integrity) on the Flask reasoning dataset, with Pearson correlation reported at 0.68, outperforming other evaluated models by a wide margin.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Task-specific reasoning evaluation (logical correctness of step-by-step solutions)",
            "dataset_name": "Flask (integrity/logical correctness criterion)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT4-Turbo-0125 (closed, large model used as judge in experiments)",
            "human_evaluator_type": "Human annotations from the Flask benchmark",
            "agreement_metric": "Pearson correlation",
            "agreement_score": 0.68,
            "reported_loss_aspects": "Smaller or less capable models (Llama3 70b, Phi3, etc.) show much lower agreement (examples: Llama3 70b 0.34, Phi3 0.33) and tend to over-score incorrect solutions as high",
            "qualitative_findings": "GPT-4 is able to assign low scores to logically incorrect solutions (matching human judgments), while many other models predominantly give high scores to bad logical responses; suggests judge capability correlates with ability to solve/understand the task.",
            "advantages_of_llm_judge": "High-capacity models like GPT-4 can reliably evaluate task-specific integrity criteria close to human performance without extensive rubric engineering.",
            "experimental_setting": "Integrity evaluations on Flask using all four prompting settings; Pearson correlation computed between model-generated scores (or perplexity) and human annotations; plotting of score distributions showed GPT-4 produced lower scores for poor responses unlike other models.",
            "uuid": "e7851.3",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Engagement criteria low agreement",
            "name_full": "LLM-as-a-judge performance on engagement-based criteria (empathy example)",
            "brief_description": "Engagement-based criteria (subjective measures like empathy) show lower agreement with human annotations and higher variance across model scores; reported Pearson values around 0.30â€“0.32 for top models (GPT-4 0.32, Phi3 0.31, Llama3-8b 0.30 for empathy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Engagement/subjective evaluation (e.g., empathy in story/dialogue generation)",
            "dataset_name": "Hanna (empathy criterion) and other engagement-focused criteria",
            "judge_model_name": "GPT-4; Phi3; Llama3-8b (examples reported)",
            "judge_model_details": "GPT4-Turbo-0125; Phi3 Medium-128k; Llama3 8b",
            "human_evaluator_type": "Human annotations from the Hanna dataset (subjective ratings with high variance)",
            "agreement_metric": "Pearson correlation",
            "agreement_score": 0.32,
            "reported_loss_aspects": "Low overall agreement with human annotators; high variance in both human labels and model outputs; models often cluster around single scores (low diversity) except some (Phi3) with higher variability",
            "qualitative_findings": "Engagement metrics are more subjective and harder for models to match; inter-model differences are smaller and model outputs exhibit greater variance; Phi3 produced more variable scores while many models cluster.",
            "advantages_of_llm_judge": "None strongly reported for engagement criteria; full rubric information can help but gains are limited.",
            "experimental_setting": "Evaluation under four prompt settings; violin plots of generated scores vs human annotations; Pearson correlations reported per model for empathy example.",
            "uuid": "e7851.4",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Phi3 rubric sensitivity",
            "name_full": "Phi3 performance sensitivity to full rubric prompts",
            "brief_description": "Phi3's agreement with human judgements decreased when full rubric details were provided compared to simpler prompts that only named the criterion, indicating a sensitivity to prompt instruction detail.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Aggregate across multiple criteria where Phi3 was evaluated",
            "dataset_name": "Aggregate (subset of evaluated datasets where Phi3 was used)",
            "judge_model_name": "Phi3",
            "judge_model_details": "Phi3 Medium-128k (reasoning-finetuned model)",
            "human_evaluator_type": "Human annotations from benchmark datasets",
            "agreement_metric": "Pearson correlation",
            "agreement_score": null,
            "reported_loss_aspects": "Performance can degrade when detailed rubric information is provided (Phi3 had higher agreement with humans with simpler prompts); indicates mismatch between model priors and rubric guidance",
            "qualitative_findings": "Phi3's prior knowledge about evaluation sometimes aligns better with humans than when given explicit full rubric instructions, suggesting instruction overrides can hurt some fine-tuned models.",
            "advantages_of_llm_judge": "Phi3's prior evaluation behavior may be useful without heavy prompt engineering in some contexts.",
            "experimental_setting": "Comparison of four prompt settings; observation that Phi3's Pearson correlation decreased for full rubric prompt relative to simpler criteria-name prompts.",
            "uuid": "e7851.5",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Prometheus-2 limitations",
            "name_full": "Prometheus-2 poor prompting performance and reliance on perplexity/full-rubric",
            "brief_description": "Prometheus-2 did not perform well under simple or criterion-specific prompts (settings 2 and 3) and the authors report only results for perplexity and full-rubric prompts for this model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "evaluation_task": "Evaluation tasks across datasets where Prometheus-2 was tested",
            "dataset_name": "Subset of datasets used for Prometheus-2 experiments (not explicitly enumerated for this model)",
            "judge_model_name": "Prometheus-2",
            "judge_model_details": "Prometheus-2 (fine-tuned/evaluation-specialized open model)",
            "human_evaluator_type": "Human annotations from benchmark datasets",
            "agreement_metric": "Pearson correlation",
            "agreement_score": null,
            "reported_loss_aspects": "Very poor results in simple and criterion-specific prompt settings (settings 2 and 3), so only perplexity and full-rubric settings reported for Prometheus-2",
            "qualitative_findings": "Prometheus-2's fine-tuning may bias it in ways that make it fail under typical simple prompting; perplexity and carefully matched rubric prompts were the only settings that produced usable alignment.",
            "advantages_of_llm_judge": "Noted limitation rather than advantage; suggests model-specific fine-tuning can change sensitivity to prompt types.",
            "experimental_setting": "Prometheus-2 evaluated but settings 2 and 3 returned very poor results; only perplexity and full rubric prompting reported for this model.",
            "uuid": "e7851.6",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-ajudge with MT-bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "NLG Evaluation using GPT-4 with Better Human Alignment",
            "rating": 2,
            "sanitized_title": "nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
            "rating": 2,
            "sanitized_title": "flask_finegrained_language_model_evaluation_based_on_alignment_skill_sets"
        },
        {
            "paper_title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "rating": 1,
            "sanitized_title": "perplexed_by_perplexity_perplexitybased_data_pruning_with_small_reference_models"
        },
        {
            "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
            "rating": 1,
            "sanitized_title": "llmbased_nlg_evaluation_current_status_and_challenges"
        }
    ],
    "cost": 0.014151,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions
16 Aug 2024</p>
<p>Bhuvanashree Murugadoss 
Christian Poelitz cpoelitz@microsoft.com 
Ian Drosos 
Vu Le 
Microsoft Redmond</p>
<p>Nick Mckenna 
Carina Suzana Negreanu 
Chris Parnin 
Microsoft Redmond</p>
<p>Advait Sarkar 
Microsoft Research </p>
<p>Microsoft Research Cambridge
UK</p>
<p>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions
16 Aug 2024A7F8E37D6ECFF9380C47F93F61914ED2arXiv:2408.08781v1[cs.AI]
LLMs-as-a-judge is a recently popularized method which replaces human judgements in task evaluation(Zheng et al. 2024) with automatic evaluation using LLMs.Due to widespread use of RLHF (Reinforcement Learning from Human Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement, such as the coherence of a text.While this seems beneficial, it is not clear whether the assessments by an LLM-as-a-judge constitute only an evaluation based on the instructions in the prompts, or reflect its preference for high-quality data similar to its finetune data.To investigate how much influence prompting the LLMs-as-a-judge has on the alignment of AI judgements to human judgements, we analyze prompts with increasing levels of instructions about the target quality of an evaluation, for several LLMs-as-a-judge.Further, we compare to a promptfree method using model perplexity as a quality measure instead.We aggregate a taxonomy of quality criteria commonly used across state-of-the-art evaluations with LLMs and provide this as a rigorous benchmark of models as judges.Overall, we show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.</p>
<p>Introduction</p>
<p>Recently, new automatic evaluation approaches that rely on LLMs have been proposed on several NLG tasks, such as summarization (Liu et al. 2023b) and machine translation (Kocmi and Federmann 2023).Previous approaches (Siledar et al. 2024) show that for certain situations, such as when assessing textual consistency or fluency, there is high agreement between human judgements and LLM assessments, even without detailed instructions like for example how to assign specific scores.Most of these approaches prompt an LLM to give a judgement as Likert score (Likert 1932) with only simple information about the scale, e.g."give a judgement between 1 (bad) and 5 (good)."More recently, LLM-based evaluations on more fine-grained task-specific criteria (Ye et al. 2024) have also reported high agreement with human judgement, such as assessing the completeness of a solution for a question answering task.</p>
<p>For these evaluations, often more detailed instructions are given about when to assign a specific score, similarly to rubric scoring (Andrade 2005).</p>
<p>While these results are promising for the future of automatic evaluation, it is less clear how the models achieve this agreement, and in general, it is a challenge to identify for any given task, which LLM is most appropriate to evaluate it, and with how much information, respectively how much instructions about the evaluation.Alarmingly, recent results show a clear bias in LLMs preferring their own output over others (Panickssery, Bowman, and Feng 2024), and LLMs' perplexity has emerged as a possible quality criteria for filtering (Ankner et al. 2024) on textual quality.This raises the question of whether some of the results on automatic evaluations with LMs reflect a model's preference for data similar to its own (high quality) fine-tuning data instead of following the provided instructions on how to measure the quality of an answer.Especially for fine-grained evaluations with detailed rubric information, we expect the instructions about when to assign a score to be adhered closely.</p>
<p>In this paper we report our findings when using LLMs-asa-judge (Zheng et al. 2024), where LLMs are used as surrogates for humans judgements to evaluate several NLG and LLM-based tasks, in skill-specific settings (e.g.completeness) and skill-unspecific (e.g.textual coherence).We show the annotations for many of these quality criteria in state-ofthe-art benchmarks have a high correlation with the perplexity of the LLMs, often higher than prompting the LLM for a score.We identify which evaluation settings can benefit the most from more detailed prompting and for which settings simple generic prompts, or just using models' perplexity as quality score, suffice.</p>
<p>In detail, we make the following three main contributions: 1.We propose a novel taxonomy of qualitative evaluation criteria useful for assessing the competence of automatic evaluation methods by LLMs-as-a-judge.Our taxonomy consists of 4 evaluation categories (Content, Relevance, Integrity, and Engagement) which encapsulate 34 metrics as tested by 8 distinct state-of-the-art benchmark datasets.2. We systematically evaluate the effectiveness of LLMsas-a-judge using the taxonomy with several major LLM families including GPT4, Llama3, Mistral, and Phi3 across 4 levels of increasing prompt instruction.We find that, aggregated across the taxonomy, increasing instruction by including more granular evaluation rubrics only somewhat improves the Pearson correlation of models with human judgements, by only as much as 4%.However, some individual metrics may benefit.3. We evaluate the potential of simple model perplexity as an alternative automatic evaluation to LLMs-as-a-judge.While perplexity often outperforms minimal prompting in terms of correlation with human judgements when detailed rubrics are not available, textual content-related metrics are the closest aligned category.For these metrics perplexity achieves a Pearson correlation of 0.51 in contrast to 0.44 when prompting the LLM-as-a-judge, suggesting it is the better choice for simple scenarios.</p>
<p>Related Work</p>
<p>LLMs as evaluators for general NLG (Liu et al. 2023b), as well as for knowledge and problem-solving tasks (Ye et al. 2024) have been widely studied recently (Chiang and Lee 2023;Li et al. 2024;Gao et al. 2024).Most previous approaches, either perform pair-wise evaluations (Ji et al. 2023;Chen et al. 2023), measuring the preference of one of two examples for a given criterion, or perform direct assessments for a single given example and a evaluation criterion (Liu et al. 2023b;Ye et al. 2024).Additionally, they distinguish between reference-free evaluations, where the LLM is presented only an example and the criterion for evaluation, and reference-based evaluations with given annotated examples of different qualities or ground truth for each example.Generally, previous works use LLMs as evaluators by using simple prompting strategies (Siledar et al. 2024), only few fine-tuned models are available (Kim et al. 2023(Kim et al. , 2024) ) for measuring for specific quality criteria.Most other finetuning approaches concentrate on scenario-specific quality feedbacks (Li et al. 2023;Wang et al. 2023) or on specific use-cases (McAleese et al. 2024).</p>
<p>Recently, there are several approaches (Liu, Moosavi, and Lin 2024;Liu et al. 2024Liu et al. , 2023c;;Stureborg, Alikaniotis, and Suhara 2024) reporting biases and mismatches with human annotations, but our work is the first to study whether the models' perplexity can be a better surrogate for quality then prompting the corresponding model and whether instructions in the prompts are impacting the results across a number of different LLMs-as-a-judge.</p>
<p>Evaluating LLMs-as-a-judge</p>
<p>In this section we give a short definition of LLMs-as-a-judge and automatic evaluation using AI.We define different settings of prompting the LLMs-as-a-judge to measure the impact on the alignment of LLM judgments with human judgements.We also evaluate a prompt-free metric using simple model perplexity.This alternative approach requires no prompt engineering and transparently measures alignment with training data without bias from a prompt, so it is a compelling alternative for evaluation.Finally, we introduce a new taxonomy, aggregating the quality criteria most frequently used in state-of-the-art benchmarks for automatic evaluation with LLMs.We categorize these into 4 groups representing the major aspects of evaluating AI generated responses.</p>
<p>LLMs-as-a-judge</p>
<p>As LLM-as-a-judge we refer to the definition introduced by (Zheng et al. 2024) as potential replacement for human annotations by prompting an LLM for a judgement of an AI assistant response.We concentrate on judging textual examples only e.g., AI generated summaries for news articles (Fabbri et al. 2021) or step-by-step solution to mathematical reasoning questions (Golovneva et al. 2023).We phrase the task to judge an AI generated response as the following: Given a task A and an AI generated solution B, judge the quality of the solution B considering only the task A. In contrast to other previous approaches, we perform a reference-free evaluation where we do not provide a possible correct reference solution.We solely rely on the models' ability to judge the solution given only the task.</p>
<p>To measure the impact of prompting the LLM-as-a-judge, we study the LLMs' performance in 4 different settings:</p>
<ol>
<li>
<p>Perplexity: We score each task solution by its perplexity under the corresponding LLM, given only the task description.This approach is unbiased by prompts, so it transparently measures alignment with model training data, providing a good comparison and alternative to prompt-based approaches.</p>
</li>
<li>
<p>Generic quality prompt: We prompt each LLM-as-ajudge with a basic instruction to measure the quality of the task solution, but give no specific criteria or instructions.In this case, we rely solely on the models' prior knowledge about the quality for the task solution from the examples generated.</p>
</li>
<li>
<p>Criteria specific prompt: We prompt each LLM-as-ajudge with an instruction to measure the quality for a specific criteria e.g., coherence.We only provide the name of the criteria, not a definition.We rely on the models' prior knowledge of the specific quality criteria only.</p>
</li>
</ol>
<p>Full rubric prompt:</p>
<p>We prompt each LLM-as-a-judge with an instruction to measure the quality for a specific quality criterion, together with a definition of the criterion and instructions when to assign each rubric score e.g., "Score 1: Incoherent text with many logical flaws."</p>
<p>We evaluate different LLMs-as-a-judge under the above settings on several different benchmarks (as described in the next subsection).We use the criteria as specified in the corresponding annotation guidelines from the benchmark datasets.For setting 4, we use all available annotation guidelines with information about the criteria and when to assign each score.We extract this information directly from benchmarks into a full rubric containing information about the criteria and the scores.For our experiments, we structure the settings from least instructive (Perplexity / no prompting) to most instructive (Full rubric information with instructions when to assign a score).In Fig. 1 we show the different setting of prompting for an example quality criterion.</p>
<p>Generic prompt # Task to evaluate</p>
<p>Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the quality of the response.The AI responses are for: {task description}.</p>
<h1 example="example">Sample to evaluate</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 (bad) and 5 (very good) as: ## Score: [Number] 3. Criteria specific prompt # Task to evaluate Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the response for the given rubric below.Use the rubric to guide you evaluating and base all your evaluation decision on the rubric.The AI responses are for: {task description}.</p>
<h1 example="example">Sample to evaluate</h1>
<h1>Rubrics Logicality</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 (bad) and 5 (very good) as: ## Score: [Number] 4. Full rubric prompt # Task to evaluate Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the response for the given rubric below.Use the rubric to guide you evaluating and base all your evaluation decision on the rubric.The AI responses are for: {task description}.</p>
<h1>Sample to evaluate {example} # Rubrics Logicality: Measure how much the story obeys your commonsense.Score 1: The story is full of absurd things.Score 2: The story has one or two things make sense, but generally very absurd.Score 3: The story roughly makes sense.Score 4: The story largely makes sense, except one or two things.Score 5: The story totally complies with commonsense.</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 and 5 as: ## Score: [Number] Figure 1: Our prompting settings.We measure how much influence the information about the actual evaluation has for model performance as LLM-as-a-judge.For setting 1, perplexity, we don't prompt the models but calculate the models' perplexity for the task solution in the example instead.The example prompts shown above are used for the LLMs-as-a-judge to measure the quality for the criterion logicality as defined in for the benchmark dataset TheNextChapter.</p>
<p>Datasets</p>
<p>We use 8 different open-source benchmark datasets commonly used for LLM-based evaluations with human annotations for several evaluation criteria per task.The datasets contain task which span several aspects from coarse-grained NLG-quality evaluations, to fine-grained very task specific evaluations with detailed information about how to score examples.</p>
<p>Firstly, we leverage two of the most prominently used datasets for coarse-grained NLG-quality evaluations: The SummEval (Fabbri et al. 2021) dataset contains news article summaries generated by different models together with human annotations for 4 different quality criteria e.g., fluency; and the TopicalChat (Gopalakrishnan et al. 2019) dataset contains human conversations over 8 different topics annotated by humans for 5 different quality criteria e.g., engagement.Further, we use two more challenging benchmark datasets for coarse-grained NLG-evaluations: the OpinSummEval (Shen and Wan 2023) dataset is a opinion summarization dataset, which consists of review summaries annotated for aspects, opinions and sentiments; the InstruSumm (Liu et al. 2023a) dataset, consists of news article summaries following specific instructions with human annotations for content specific quality-criteria e.g., amount of missing information.</p>
<p>Second, we use two benchmark datasets for more finegrained NLG evaluations: the Hanna (Chhun et al. 2022) dataset and the TheNextChapter (Xie, Cohn, and Lau 2023) dataset contain creative stories generated for a given initial user prompt.Each story is annotated by humans for NLG and style based criteria e.g., coherence, but also for more unconventional criteria like surprise.Finally, we use two task-specific evaluation benchmark datasets with quality-criteria depending in task solution quality: Roscoe (Golovneva et al. 2023) is a collection of datasets of reasoning tasks, together with GPT3 generated with stepby-step solutions.The human annotations cover coarsegrained task specific evaluation criteria like "missing step"; the Flask (Ye et al. 2024) dataset contains several knowledge and problem solving tasks with LLM generated solutions.The human annotations cover more fine-grained taskspecific criteria like completeness and factuality.Most criteria need an understanding of the solution e.g., completeness.</p>
<p>Criteria taxonomy</p>
<p>We introduce a simple taxonomy of quality evaluation criteria based on current state-of-the-art benchmark datasets and quality criteria commonly used for automatic evaluations by LLMs.We define 4 groups of quality criteria, relevant for automatic evaluation:</p>
<ol>
<li>Content-based criteria: Measure how well the solution is presented to the user, for example, whether a news article summary is fluent.We assign each of the criteria used in the benchmark datasets above to these 4 groups (Fig. 2).For content-based criteria, we are interested in how to measure the quality of the content as it is presented to the user.This includes mainly criteria of the textual quality of the solution.For example, the criterion fluency is used for measuring the quality of the summaries in the SummEval dataset and hence is a content-based criteria.The engagement-based criteria, combine criteria of how the AI generated solution engages with the user.This includes for example the empathy criterion used to measure the quality of the generated stories in the Hanna dataset.The remaining two groups concentrate on more task specific evaluation criteria.Integrity-based criteria measure the coherence of task solution and whether it makes sense logically.For example the criterion logical correctness, used for measuring the quality of (mathematical) resonsning or coding task solutions in the Flask dataset, is a integrity-based criterion.Finally, relevance-based criteria measure the direct relevance of a task solution to the actual task.This includes for example the criterion relevance, used for measuring the connections of task solutions and initial task in several of the benchmark datasets e.g., TheNextChapter dataset.The separation of the quality criteria classes are not a 100% perfect and there are overlaps, for example content-based criteria like readability, can also be seen as engagement-based, since less legible solution are also less engaging.</li>
</ol>
<p>Model Selection for LLM-as-a-judge</p>
<p>To</p>
<p>Results</p>
<p>In this section, we present the main results of the evaluations using the different LLMs-as-a-judge under the different settings of prompting.Analogous to previous work (Liu et al. 2023b), to measure the quality of the evaluations we calculate the Pearson correlation of the generated scores by the LLMs-as-a-judge, respectively the perplexity values, and the human annotations given for each quality criteria from the benchmark dataset.We split the results section into model level, dataset level and criteria level results.In the model level results subsection, we present the results comparing the different LLMs under the setting of prompting, averaging over all criteria; the dataset level results subsection presents the results when we compare the different datasets under the setting of prompting average over all criteria; the criteria level results subsection presents the results when we compare models and setting of prompting under the different groups of criteria.Finally, we present the results of a detailed analysis for each group of criteria from the introduced taxonomy.</p>
<p>Model level results</p>
<p>There is only small effect adding full rubric information.</p>
<p>Providing the LLMs-as-a-judge with more detailed rubric information of the quality criteria, generally has only small influence on evaluation performance for the large and midsize models, and might even be disadvantageous in certain situations (see Tab.1).For instance, Phi3's performance decreases when complete rubric details are provided compared to simple prompts which only mention the criterion name in the prompt.Here, Phi3's prior knowledge about evaluating the criteria has higher agreement with human annotators compared to when using full rubric information.Only the smaller Llama3 8b and Mistral models see improvements when given comprehensive rubric information for assessment.Among the open models, Llama3, both the 70b and 8b versions, perform best.Meanwhile, Mistral and Prometheus-2 do not show improvements when the LLM is prompted, with models' perplexity having higher correlation then the generated scores.For Prometheus-2, we only report perplexity and full rubric information in the prompts since this aligns with the fine-tuning data for this model and both setting 2 and 3 did return very poor results.GPT4 performs best among all models.As may be expected, prompting GPT4-as-a-judge, even for a generic quality judgement, results in the highest performance in terms of agreement with human annotations compared to all other models tested.Further, GPT4's judgements do only improve marginally from prompting setting 3 to 4, indicating that GPT4's prior knowledge about evaluating does already agree with the human judgements to a high degree without the need to add more detailed rubric information about the evaluation.</p>
<p>Dataset level results</p>
<p>Perplexity correlates with text quality criteria.We observe (see Tab. 2) that the quality criteria from datasets with simple textual content creation tasks e.g., summarization in the SummEval dataset or story generation in the Hanna dataset, show high agreement with models' perplexity compared to simple prompting (setting 2 and 3).For more complex NLG tasks, which depend on several aspects and multiple possible steps, the human annotation correlate less strongly with perplexity compared prompting the LLMsas-a-judge with more information.For example the opinion summary evaluations from the OpinSummEval dataset uses criteria which depend on sentiment identification and extractions of the key aspects, in these cases prompting the LLM seems necessary.</p>
<p>Full rubric information helps for non-default textual quality evaluations.Unusual textual quality evaluation tasks which measure the quality beyond simple textual criteria like fluency, can benefit from more full rubric information about the the evaluation task.For example, we observe that for the TheNextChapter dataset, full rubric information in the prompts to the LLMs-as-a-judge leads to judgements with the highest correlations with human annotations.Compared to other datasets for textual quality evaluation, these two datasets contain much more complex texts e.g., creative stories with non-default quality criteria like relatedness which is difficult to estimate without additional information by an LLM.Furthermore, evaluating more complex tasks which include more than text quality, like the logical reasoning tasks, benefit also from more detailed rubric information in the prompts.The logical and mathematical reasoning tasks in the Roscoe datasets for example do benefit from more information to effectively judge as shown by the higher correlations with the human judgements compared to prompting with less information or using perplexity.</p>
<p>Dataset level analysis can be misleading.Models' perplexity on both the Flask dataset and the SummEval dataset, outperforms simple prompting in aligning to human judgements.While the quality criteria in the SummEval dataset primarily focuses on textual quality where we expect perplexity to perform well for example, the Flask dataset consist a variety of different quality criteria which make it difficult to generalise and the average correlations values might be biased the high values on the text related criteria.In the next subsection, we investigate this issue by using the previously introduced taxonomy to analyze results on a percriteria class basis rather than average results per dataset.</p>
<p>Criteria level analysis</p>
<p>Content-based quality criteria correlate the most with perplexity.When evaluating quality with a focus on textual context, perplexity seems a viable alternative to prompting LLMs-as-a-judge.We observe that on average the agreement with human annotations is more then 20% higher when using models' perplexity to judge the quality compared to prompting (Fig. 3).Further, there are only small differences between using a simple generic quality prompt for evaluation compared to all other settings of prompting, showing that models' prior knowledge generates judgements with high agreement with human judgements on textual quality.</p>
<p>Engagement-based quality criteria benefit the most from full rubric information.These criteria are unconventional as they assess the likelihood of a user feeling personally engaged, as opposed to merely evaluating straightforward text quality.Access to full rubric information can help judging with directives, particularly when the evaluation is more unusual and different from the text quality alone.</p>
<p>Often, there is only little improvement of adding full rubric information for most criteria groups.Except for the engagement-based criteria, there is only limited effect on adding full rubrics information to the prompts.Further, simple prompts for generic quality judgements results in similar correlation values with the human annotations than detailed information for content and relevance based evaluation crite-  ria.This confirms again that advanced models' prior knowledge of text quality or relevance already has a high agreement with human judgements.GPT4 clearly outperforms all other models.GPT-4 particularly outperforms in relevance and integrity quality criteria, surpassing other models on these criteria (Fig. 4).Although Phi3 matches GPT-4's performance in engagement based criteria, it generally performs less consistent with human annotations across other evaluative measures; Mitral's performance falls short in criteria associated with relevance and integrity; LLamA3-70b exhibits a marginally improved performance over Phi3 when it comes to content and relevance-based criteria.</p>
<p>Details on content-based criteria results</p>
<p>Drilling down the content-based evaluations criteria, we observe that perplexity outperforms prompting mainly on structural text quality criteria.For example, human annotations for fluency from the SummEval dataset have much higher agreement with perplexity compared to all prompting methods.This quality criterion judges grammar, spelling, and sentence structure for example.On the other hand, evaluating for more complex, specific and subjective contentbased criteria like naturalness, which measures how natural the task response sounds, benefits from more instructions in the prompts for the LLMs-as-a-judge.Here, instructions to judge how much the task solution resembles a human answer improves agreement with human judgements.</p>
<p>Notably, we observe that model perplexity has 100% agreement with the human annotations for harmlessness on Flask datasets.This might reflect the strong influence fine-tuning has in inhibiting the generation of harmful content (Dubey et al. 2024).Conversely, prompting for measuring harmfulness performs much lower until we provide full rubric information in the prompts.</p>
<p>Details on engagement-based criteria results</p>
<p>Engagement-based criteria are more challenging to judge since they are often subjective.We observe that there are fewer performance differences between the models compared to the results on the other criteria e.g., GPT4 judgements have an agreement (by Pearson correlation) of 0.32, Phi3 of 0.31 and Llama3 8b of 0.3.The overall performance of all models on these criteria is lower and the scores generated by the models have higher variance compared to other criteria.Further, human annotations for these criteria have on average lower scores with higher variance.For example, for the criterion empathy from the Hanna dataset (Fig. 5), human annotations show a significant degree of variation, and Phi3 notably generates scores with higher variability,   unlike most other models that tend to cluster around a singular score.Hence, the distribution in engagement-based criteria may explain why Phi3 outperforms other models.</p>
<p>Details on relevance-based criteria results</p>
<p>For relevance-based quality criteria, we assume that the models need robust information about the problem to measure whether the information in the task solution is relevant, and estimate to what degree.Models' perplexity, but also generic prompts seem not sufficient for evaluation since relevance is more specific to certain aspects of the task solution.</p>
<p>Still, we also observe that including a full rubric doesn't always appear necessary; instead the size of the models seem more important.For the criterion groundedness from the TopicalChat dataset for example (Tab.4), we identify a clear trend of increasing agreement with the human judgements with larger models as LLM-as-a-judge.</p>
<p>Details on integrity-based criteria results</p>
<p>Similar to the relevance-based quality criteria, we assume that to measure the quality for integrity-based criteria the LLMs-as-a-judge need to have an understanding of the task, but also the ability to solve the task itself.We hypothesize that, for task-specific evaluations, the underlying LLM-asa-judge actually needs to be able to solve the task itself to apply the correct score.As reported in (Lin et al. 2024), LLMs' ability to critic a task solution correlates with its ability to solve the task.We exemplify this by the evaluations for the integritybased evaluation criterion on the criterion logical correctness from the Flask dataset.This criterion reflects the correctness elements which are reflected in human annotations, which are more clearly scored as high (for logically correct) or low (for logically incorrect) scores.To select the appropriate scores, the LLM-as-a-judges need to know what is correct and what is wrong.Here, GPT-4 significantly outperforms all other models by a wide margin with a Pearson correlation of 0.68 with the human judgements in contrast to 0.34 for Llama3 70b and 0.33 for Phi3, for example.</p>
<p>To illustrate this, we plot the generated scores of the LLMs-as-a-judge (Fig. 6) and the human annotations.We observe that only GPT4 is able to generate lower scores to judge a task solution as "bad."All other models predominantly give high scores, consistently grading bad logically incorrect responses as "very good."</p>
<p>Conclusion</p>
<p>In this paper, we investigate how increasing levels of prompting impact the automatic evaluations made by LLMsas-a-judge in measuring the quality of AI-generated text.We introduce a new taxonomy of quality criteria, summarizing commonly used criteria in automatic evaluations with LLMs into four broad categories: Content, Relevance, Integrity, and Engagement.We systematically evaluated several LLMs, including GPT-4, Llama-3, and others, across all settings of prompting to determine if more detailed instructions enhance the LLMs' alignment with human judgements.Key findings include:</p>
<p>â€¢ Detailed quality criteria information might not be necessary in the most powerful models; for instance, GPT-4 shows a high level of agreement with human judgements even without detailed instruction.â€¢ Simple perplexity values are very effective at estimating textual quality, often outperforming the results of prompting the LLMs-as-a-judge with basic instructions.</p>
<p>â€¢ Judging task-specific quality criteria like relevance or logical correctness requires more capable, larger models, aligning with previous research on the necessary model capabilities for critiquing (Lin et al. 2024).</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of quality criteria summarizing current state-of-the-art benchmark datasets and criteria used for automatic evaluations with LLMs.We group all 34 quality criteria as defined in the 8 different benchmark datasets into 4 groups: Content-based, Engagement-based, Integritybased, Relevance-based criteria.</p>
<p>understand how model size and finetuning affect performance across the different quality criteria and settings of prompting, we test several current LLMs: GPT4-Turbo (OpenAI et al. 2024)-0125 as large closed-model baseline; Llama3 70b (Touvron et al. 2023; Dubey et al. 2024) as a medium size open-model; Llama3 8b, Mistral-v0.3 (Jiang et al. 2023) as small open-models, Phi3 (Abdin et al. 2024)-Medium-128k as fine-tuned model for reasoning, and Prometheus-2 (Kim et al. 2024) as fine-tuned models for evaluation tasks 1 .</p>
<p>Figure 3 :
3
Figure 3: Radar chart of average Pearson correlations for the quality criteria groups for each of different settings of prompting (1 -Perplexity / No Prompting, 2 -Generic prompt, 3 -Specific prompt, 4 -Full rubric) over all models.</p>
<p>Figure 4 :
4
Figure 4: Radar chart of average Pearson correlations for the quality criteria groups for each of the different LLMs-as-ajudge over all setting of prompting.</p>
<p>Figure 5 :
5
Figure 5: Violin plot of the generated scores by the LLMsas-a-judge for the engagement-based criterion empathy, together with the corresponding human annotations.</p>
<p>Figure 6 :
6
Figure 6: Violin plot of the generated scores by the LLMsas-a-judge for integrity-based criterion logical correctness from the Flask dataset, together with human annotations.</p>
<p>Table 3
3: Pearson correlations of scores generated by differ-ent LLMs-as-a-judge with human annotations for content-based evaluation criteria, from the benchmark dataset for re-spective settings (1 -Perplexity (No Prompt), 2 -Genericprompt, 3 -Specific prompt, 4 -Full rubric)</p>
<p>Table 4 :
4
Pearson correlations of the scores generated by different LLMs-as-a-judge with the human annotations for
groundedness for the different setting (1 -Perplexity / NoPrompting, 2 -Generic prompt, 3 -Specific prompt, 4 -Full rubric)
. Engagement-based criteria: Measure how engaging the solution is, for example, whether a generated story contains an element of surprise.
3. Integrity-based criteria: Measure how consistent and logical coherent the solution is, for example, whether a math solution is correct.
4.Relevance-based criteria: Measure how relevant the solution is for the given task, for example, whether a legal advice answer contains irrelevant information.
In all experiments, we generate
responses with a temperature of 0.3 and use the average of all generated scores, similar to(Liu et al. 2023b).</p>
<p>. M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, J Bao, H Behl, A Benhaim, M Bilenko, J Bjorck, S Bubeck, Q Cai, M Cai, C C T Mendes, W Chen, V Chaudhary, D Chen, D Chen, Y.-C Chen, Y.-L Chen, P Chopra, X Dai, A D Giorno, G De Rosa, M Dixon, R Eldan, V Fragoso, D Iter, M Gao, M Gao, J Gao, A Garg, A Goswami, S Gunasekar, E Haider, J Hao, R J Hewett, J Huynh, M Javaheripi, X Jin, P Kauffmann, N Karampatziakis, D Kim, M Khademi, L Kurilenko, J R Lee, Y T Lee, Y Li, Y Li, C Liang, L Liden, C Liu, M Liu, W Liu, E Lin, Z Lin, C Luo, P Madan, M Mazzola, A Mitra, H Modi, A Nguyen, B Norick, B Patra, D Perez-Becker, T Portet, R Pryzant, H Qin, M Radmilac, C Rosset, S Roy, O Ruwase, O Saarikivi, A Saied, A Salim, M Santacroce, S Shah, N Shang, H Sharma, S Shukla, X Song, M Tanaka, A Tupini, X Wang, L Wang, C Wang, Y Wang, R Ward, G Wang, P Witte, H Wu, M Wyatt, B Xiao, C Xu, J Xu, W Xu, S Yadav, F Yang, J Yang, Z Yang, Y Yang, D Yu, L Yuan, C Zhang, C Zhang, J Zhang, L L Zhang, Y Zhang, Y Zhang, Y Zhang, arXiv:2404.14219and Zhou, X. 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</p>
<p>Teaching With Rubrics: The Good, the Bad, and the Ugly. H G Andrade, College Teaching. 5312005</p>
<p>Z Ankner, C Blakeney, K Sreenivasan, M Marion, M L Leavitt, M Paul, arXiv:2405.20541Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models. 2024</p>
<p>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study. Y Chen, R Wang, H Jiang, S Shi, R Xu, arXiv:2304.007232023</p>
<p>Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. C Chhun, P Colombo, F M Suchanek, C Clavel, N Calzolari, C.-R Huang, H Kim, J Pustejovsky, L Wanner, K.-S Choi, P.-M Ryu, H.-H Chen, L Donatelli, H Ji, S Kurohashi, P Paggio, N Xue, S Kim, Y Hahm, He, Proceedings of the 29th International Conference on Computational Linguistics. T K Lee, E Santus, F Bond, S.-H Na, the 29th International Conference on Computational LinguisticsGyeongju2022Republic of Korea: International Committee on Computational Linguistics</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations. C.-H Chiang, H.-Y Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>. Canada ; Toronto, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Roziere, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Zhang, G Synnaeve, G Lee, G L Anderson, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I Kloumann, I Misra, I Evtimov, J Copet, J Lee, J Geffert, J Vranes, J Park, J Mahadeokar, J Shah, J Van Der Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J Jia, K V Alwala, K Upasani, K Plawiak, K Li, K Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, L Rantala-Yeary, L Van Der Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L De Oliveira, M Muzzi, M Pasupuleti, M Singh, M Paluri, M Kardas, M Oldham, M Rita, M Pavlova, M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N Chatterji, O Duchenne, O Â¸elebi, P Alrassy, P Zhang, P Li, P Vasic, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Petrovic, W Chu, W Xiong, W Fu, W Meers, X Martinet, X Wang, X E Tan, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, ; A Chen, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Franco, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, C Mejia, C Wang, C Kim, C Zhou, C Hu, C Chu, C.-H Cai, C Tindal, C Feichtenhofer, C Civin, D Beaty, D Kreymer, D Li, D Wyatt, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, D , Papakipos, Z.Singh, A.Grattafiori, A.Jain, A.Kelsey, A.Shajnfeld, A.Gangidi, A.Victoria, A.Goldstand, A.Menon, A.Sharma, A.Boesenberg, A.Vaughan, A.Baevski, A.Feinstein, A.Kallet, A.Sangani, A.Yunus,Association for Computational LinguisticsStojkovic, B; Montalvo, B; Parker, CDowling, E.; Jamil, E.; Montgomery, E.; Presani, E.; Hahn, E.; Wood, E.; Brinkman, E.; Arcaute, E.; Dunbar, E.; Smothers, E.; Sun, F.; Kreuk, F.; Tian, F.; Ozgenel, F.; Caggioni, F.; GuzmÃ¡n, F.; Kanayet, F.; Seide, F.; Florez, G. M.; Schwarz, G.; Badeer, G.; Swee, G.; Halpern, G.; Thattai, G.; Herman, G.; Sizov, G.; Guangyi; Zhang</p>
<p>. G Lakshminarayanan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, I Molybog, I Tufanov, I.-E Veliche, I Gat, J Weissman, J Geboski, J Kohli, J Asher, J.-B Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, J Cummings, J Carvill, J Shepard, J Mcphie, J Torres, J Ginsburg, J Wang, K Wu, U , K H Saxena, K Prasad, K Khandelwal, K Zand, K Matosich, K Veeraraghavan, K Michelena, K Li, K Huang, K Chawla, K Lakhotia, K Huang, K Chen, L Garg, L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, L Khabsa, M Avalani, M Bhatt, M Tsimpoukelli, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M Seltzer, M L Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M Hermoso, M J Metanat, M Rastegari, M Bansal, M Santhanam, N Parks, N White, N Bawa, N Singhal, N Egebo, N Usunier, N Laptev, N P Dong, N Zhang, N Cheng, N Chernoguz, O Hart, O Salpekar, O Kalinli, O Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollar, P Zvyagina, P Ratanchandani, P Yuvraj, P Liang, Q Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Li, R Hogan, R Battey, R Wang, R Maheswari, R Howes, R Rinott, R Bondu, S J Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Lindsay, S Feng, S Lin, S Zha, S C Shankar, S Zhang, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S Cho, S Virk, S Subramanian, S Choudhury, S Goldman, S Remez, T Glaser, T Best, T Kohler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, T Vontimitta, V Ajayi, V Montanez, V Mohan, V Kumar, V S Mangla, V Ionescu, V Poenaru, V Mihailescu, V T Ivanov, V Li, W Wang, W Jiang, W Bouaziz, W Constable, W Tang, X Wang, X Wu, X Wang, X Xia, X Wu, X Gao, X Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Y Yu, Wang , </p>
<p>Y Hao, Y Qian, Y He, Z Rait, Z Devito, Z Rosnbrick, Z Wen, Z Yang, Z Zhao, arXiv:2407.21783The Llama 3 Herd of Models. 2024</p>
<p>A R Fabbri, W KryÅ›ciÅ„ski, B Mccann, C Xiong, R Socher, D Radev, arXiv:2007.12626SummEval: Re-evaluating Summarization Evaluation. 2021</p>
<p>M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.01383LLMbased NLG Evaluation: Current Status and Challenges. 2024</p>
<p>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. O Golovneva, M Chen, S Poff, M Corredor, L Zettlemoyer, M Fazel-Zarandi, A Celikyilmaz, K Gopalakrishnan, B Hedayatnia, Q Chen, A Gottardi, S Kwatra, A Venkatesh, R Gabriel, D Hakkani-TÃ¼r, arXiv:2212.07919ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. 2023. 2019. 2019Proc. Interspeech</p>
<p>Exploring ChatGPT's Ability to Rank Content: A. Y Ji, Y Gong, Y Peng, C Ni, P Sun, D Pan, B Ma, X Li, arXiv:2303.07610Preliminary Study on Consistency with Human Preferences. 2023</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, S Kim, J Shin, Y Cho, J Jang, S Longpre, H Lee, S Yun, S Shin, S Kim, J Thorne, arXiv:2310.06825arXiv:2302.14520Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. 2023. 2023. 2024arXiv preprintPrometheus: Inducing Fine-grained Evaluation Capability in Language Models. and Federmann, C. 2023. Large Language Models Are State-of-the-Art Evaluators of Translation Quality</p>
<p>J Li, S Sun, W Yuan, R.-Z Fan, H Zhao, P Liu, arXiv:2310.05470Generative Judge for Evaluating Alignment. 2023</p>
<p>Z Li, X Xu, T Shen, C Xu, J.-C Gu, C Tao, arXiv:2401.07103Leveraging Large Language Models for NLG Evaluation: A Survey. 2024</p>
<p>A technique for the measurement of attitudes. Archives of psychology. R Likert, Z Lin, Z Gou, T Liang, R Luo, H Liu, Y Yang, arXiv:2402.14809CriticBench: Benchmarking LLMs for Critique-Correct Reasoning. 1932. 2024</p>
<p>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization. Y Liu, A R Fabbri, J Chen, Y Zhao, S Han, S Joty, P Liu, D Radev, C.-S Wu, A Cohan, arXiv:2311.091842023a</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C 2023b Zhu, G-Eval, arXiv:2303.16634NLG Evaluation using GPT-4 with Better Human Alignment. </p>
<p>Y Liu, N S Moosavi, C Lin, arXiv:2311.09766LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. 2024</p>
<p>Y Liu, T Yang, S Huang, Z Zhang, H Huang, F Wei, W Deng, F Sun, Q Zhang, arXiv:2309.13308Calibrating LLM-Based Evaluator. 2023c</p>
<p>Y Liu, H Zhou, Z Guo, E Shareghi, I VuliÄ‡, A Korhonen, N Collier, arXiv:2403.16950Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators. 2024</p>
<p>N Mcaleese, R M Pokorny, J F C Uribe, E Nitishinskaya, M Trebacz, J Leike, arXiv:2407.00215LLM Critics Help Catch LLM Bugs. OpenAI2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, R Avila, I Babuschkin, S Balaji, V Balcom, P Baltescu, H Bao, M Bavarian, J Belgum, I Bello, J Berdine, G Bernadett-Shapiro, C Berner, L Bogdonoff, O Boiko, M Boyd, A.-L Brakman, G Brockman, T Brooks, M Brundage, K Button, T Cai, R Campbell, A Cann, B Carey, C Carlson, R Carmichael, B Chan, C Chang, F Chantzis, D Chen, S Chen, R Chen, J Chen, M Chen, B Chess, C Cho, C Chu, H W Chung, D Cummings, J Currier, Y Dai, C Decareaux, T Degry, N Deutsch, D Deville, A Dhar, D Dohan, S Dowling, S Dunning, A Ecoffet, A Eleti, T Eloundou, D Farhi, L Fedus, N Felix, S P Fishman, J Forte, I Fulford, L Gao, E Georges, C Gibson, V Goel, T Gogineni, G Goh, R Gontijo-Lopes, J Gordon, M Grafstein, S Gray, R Greene, J Gross, S S Gu, Y Guo, C Hallacy, J Han, J Harris, Y He, M Heaton, J Heidecke, C Hesse, A Hickey, W Hickey, P Hoeschele, B Houghton, K Hsu, S Hu, X Hu, J Huizinga, S Jain, S Jain, J Jang, Jiang, A.; Jiang, R.; Jin, H.; Jin, D.; Jomoto, S.; Jonn, B.; Jun, H.; Kaftan, T.; Åukasz Kaiser</p>
<p>A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, J H Kirchner, J Kiros, M Knight, D Kokotajlo, A Åukasz Kondraciuk; Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mcgrew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D Mossing, T Mu, M Murati, O Murk, D MÃ©ly, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, L Ouyang, C O'keefe, J Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, H P De Oliveira Pinto, M Michael; Pokorny; Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, Y Shen, X Wan, arXiv:2303.08774arXiv:2402.11683OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. Bowman, S. R.; and Feng2024. 2023Technical ReportLLM Evaluators Recognize and Favor Their Own Generations. Nath, S.; Muddu, S. S. R. R.; Rangaraju, R.; Nath, S.; Bhattacharyya, P.; Banerjee, S.; Patil, A.; Singh, S. S.; Chelliah, M.; and Garera, N. 2024. One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation</p>
<p>Large Language Models are Inconsistent and Biased Evaluators. R Stureborg, D Alikaniotis, Y Suhara, arXiv:2405.017242024</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B RoziÃ¨re, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, T Wang, P Yu, X E Tan, S O'brien, R Pasunuru, J Dwivedi-Yu, O Golovneva, L Zettlemoyer, M Fazel-Zarandi, A Celikyilmaz, Z Xie, T Cohn, J H Lau, S Ye, D Kim, S Kim, H Hwang, S Kim, Y Jo, J Thorne, J Kim, arXiv:2302.13971arXiv:2307.10928The Next Chapter: A Study of Large Language Models in Storytelling. 2023. 2023. 2023Shepherd: A Critic for Language Model Generation. and Seo, M. 2024. FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</p>
<p>Judging LLM-as-ajudge with MT-bench and Chatbot Arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-270372057</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05872v1.pdf" target="_blank">STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STARLING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-supervised Text-bAsed RL learNING (STARLING)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based RL agent pre-trained on 75 GPT-3-generated interactive fiction games (75 train / 25 eval of 100 total) using a GRU-based encoder and A2C; pretraining with LLM-generated auxiliary games improves early learning, avoids failure states, and boosts performance on TWC, ScienceWorld and Zork1 relative to a vanilla TBRL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STARLING (GRU-based TBRL agent with LLM-generated pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-input GRU-based text agent: separate GRUs encode observed text, inventory, current room description, and valid-action list; individual representations are concatenated to form the state representation and passed to an A2C policy/value head. The agent is pre-trained on GPT-3-generated games and then evaluated on TextWorld Commonsense (TWC), ScienceWorld and Zork1.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td>200K (approx; model parameters for the GRU-based agent reported ≈200K)</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC), ScienceWorld, Zork1 (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>TWC: commonsense everyday-object tasks across easy/medium/hard; ScienceWorld: science reasoning tasks (change-of-state, classification, biology) with many variations; Zork1: classic large-scale interactive fiction with long trajectories, sparse rewards and large location/action space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit recurrent working memory (GRU hidden states)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Per-modality recurrent hidden states (separate GRUs for observation, inventory, room, valid actions) concatenated into a single state vector; no external explicit memory module reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Recent observations, inventory contents, current room description, and admissible action list as encoded in the GRU hidden states (i.e., latent state representation summarizing recent context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>No explicit retrieval mechanism; the recurrent hidden state is used directly as the agent's state (i.e., implicit, online access to encoded context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated at each time step by feeding new observations/inventory/room/action list through the corresponding GRUs (standard recurrent updates).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Forming the state representation for action selection and value estimation; tracking inventory and recent context to avoid failure states and select valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Pre-trained (STARLING) on generated pre-training games: mean normalized score 0.72 ± 0.063 and mean moves 28.105 ± 1.876 on 25 unseen pre-training games (Table 1). Across target benchmarks, STARLING outperforms the vanilla (non-pretrained) GRU-based TBRL baseline on TWC (all difficulty levels), ScienceWorld (4 tasks tested) and simplified Zork1 killing-troll goal, with notable gains in early episodes and better avoidance of failure states (no single consolidated numeric table in paper for all benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla TBRL (no LLM-based pretraining) is consistently worse than STARLING on the benchmark tasks; in the pre-training evaluation random agent scored 0.05 normalized while the pre-trained agent scored 0.72 (human = 1.0). Exact numeric baselines for every downstream benchmark are not fully enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The agent's implicit recurrent memory (via GRUs) combined with LLM-generated pretraining helps in early learning (faster reward acquisition in first episodes), choosing valid actions, and avoiding failure states; however, because the pretraining games lack navigational and long-horizon trajectories, the learned (implicit) memory/skills transfer less effectively to tasks requiring long-horizon planning and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit external memory; GRU-based implicit memory struggles when target tasks require long trajectories or navigation because pretraining games had short trajectories and limited spatial complexity — i.e., the learned recurrent state does not substitute for planning/memory needed for long-horizon navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT is discussed as an example of a conversational LLM applied to interactive fiction; the authors report empirical difficulties with ChatGPT's ability to track in-game object states and preconditions accurately when used to play Inform7-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (as used/interacted-with for playing text games in the authors' experiments/observations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A conversational large language model (not used as the trained RL policy in experiments), observed by the authors to produce actions and text responses but to struggle with consistent state-tracking and precondition enforcement in interactive fiction games.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Inform7-based games / STARLING generated games (qualitative observation)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Inform7 interactive fiction games produced by STARLING; authors interacted with ChatGPT on these games (qualitative) and observed failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Authors note ChatGPT "struggles to keep track of the states of all in-game objects and the pre-conditions necessary to use those actions" (e.g., failing to require turning on a stove before using it); it also suffers small factual errors and non-determinism, indicating limitations in its effective state memory when used in text-game interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Inconsistent state-tracking and precondition enforcement; small factual errors and non-deterministic outputs reduce reproducibility — these are reported as shortcomings when ChatGPT is used to play interactive fiction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based world-model / LLM action agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based world models and action-generation agents (references: Ammanabrolu et al., Wang et al., Yao et al., etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works are cited that use LLMs for action generation, building textual world models, or other roles in text-game play; the paper notes these lines of work and comments that using LLMs to learn underlying text representations does not necessarily improve downstream RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-based agents (Behavior cloning, Text Decision Transformers, CALM-GPT2, LLM world models and action-generation approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Various LLM-based approaches referenced in the paper include behavior-cloned transformers, decision transformers, GPT/GPT-2/GPT-style models used for action generation or for building textual world models; they are compared in Table 3 of the paper where STARLING (GRU-based) is reported to outperform many LLM-based baselines on ScienceWorld test variations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>ScienceWorld (test variations) among others</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>ScienceWorld science-domain tasks; referenced LLM-based models (e.g., BC, TDT, CALM) were used by other works to tackle these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports that STARLING (GRU-based, ≈200K params) outperforms both GRU-based and several LLM-based models (Behavior Cloning, Text Decision Transformers, CALM-GPT2) on the majority of tested ScienceWorld tasks (Table 3), but the paper does not include detailed per-model memory architectures or explicit memory-based performance metrics for these LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper notes prior work showing that using LLMs to learn underlying text representations does not necessarily yield better performance (citing Wang et al., 2022) — implying that model size or LLM usage alone is not a substitute for appropriate memory/representation or RL algorithm design.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No detailed memory architectures for these LLM-based approaches are provided in this paper; the authors explicitly state they outperform some LLM-based methods despite using a much smaller GRU-based model, indicating that large capacity LLMs do not automatically provide effective memory for these tasks without suitable integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models play text games well? current state-of-the-art and open questions <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Planning to explore via self-supervised world models <em>(Rating: 1)</em></li>
                <li>Behavior cloned transformers are neurosymbolic reasoners <em>(Rating: 2)</em></li>
                <li>Learning phrase representations using rnn encoder-decoder for statistical machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2725",
    "paper_id": "paper-270372057",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "STARLING",
            "name_full": "Self-supervised Text-bAsed RL learNING (STARLING)",
            "brief_description": "A text-based RL agent pre-trained on 75 GPT-3-generated interactive fiction games (75 train / 25 eval of 100 total) using a GRU-based encoder and A2C; pretraining with LLM-generated auxiliary games improves early learning, avoids failure states, and boosts performance on TWC, ScienceWorld and Zork1 relative to a vanilla TBRL baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "STARLING (GRU-based TBRL agent with LLM-generated pretraining)",
            "agent_description": "Multi-input GRU-based text agent: separate GRUs encode observed text, inventory, current room description, and valid-action list; individual representations are concatenated to form the state representation and passed to an A2C policy/value head. The agent is pre-trained on GPT-3-generated games and then evaluated on TextWorld Commonsense (TWC), ScienceWorld and Zork1.",
            "base_model_size": "200K (approx; model parameters for the GRU-based agent reported ≈200K)",
            "game_benchmark_name": "TextWorld Commonsense (TWC), ScienceWorld, Zork1 (Jericho)",
            "game_description": "TWC: commonsense everyday-object tasks across easy/medium/hard; ScienceWorld: science reasoning tasks (change-of-state, classification, biology) with many variations; Zork1: classic large-scale interactive fiction with long trajectories, sparse rewards and large location/action space.",
            "uses_memory": true,
            "memory_type": "implicit recurrent working memory (GRU hidden states)",
            "memory_structure": "Per-modality recurrent hidden states (separate GRUs for observation, inventory, room, valid actions) concatenated into a single state vector; no external explicit memory module reported.",
            "memory_content": "Recent observations, inventory contents, current room description, and admissible action list as encoded in the GRU hidden states (i.e., latent state representation summarizing recent context).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "No explicit retrieval mechanism; the recurrent hidden state is used directly as the agent's state (i.e., implicit, online access to encoded context).",
            "memory_update_strategy": "Updated at each time step by feeding new observations/inventory/room/action list through the corresponding GRUs (standard recurrent updates).",
            "memory_usage_purpose": "Forming the state representation for action selection and value estimation; tracking inventory and recent context to avoid failure states and select valid actions.",
            "performance_with_memory": "Pre-trained (STARLING) on generated pre-training games: mean normalized score 0.72 ± 0.063 and mean moves 28.105 ± 1.876 on 25 unseen pre-training games (Table 1). Across target benchmarks, STARLING outperforms the vanilla (non-pretrained) GRU-based TBRL baseline on TWC (all difficulty levels), ScienceWorld (4 tasks tested) and simplified Zork1 killing-troll goal, with notable gains in early episodes and better avoidance of failure states (no single consolidated numeric table in paper for all benchmarks).",
            "performance_without_memory": "Vanilla TBRL (no LLM-based pretraining) is consistently worse than STARLING on the benchmark tasks; in the pre-training evaluation random agent scored 0.05 normalized while the pre-trained agent scored 0.72 (human = 1.0). Exact numeric baselines for every downstream benchmark are not fully enumerated in the paper.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The agent's implicit recurrent memory (via GRUs) combined with LLM-generated pretraining helps in early learning (faster reward acquisition in first episodes), choosing valid actions, and avoiding failure states; however, because the pretraining games lack navigational and long-horizon trajectories, the learned (implicit) memory/skills transfer less effectively to tasks requiring long-horizon planning and navigation.",
            "memory_limitations": "No explicit external memory; GRU-based implicit memory struggles when target tasks require long trajectories or navigation because pretraining games had short trajectories and limited spatial complexity — i.e., the learned recurrent state does not substitute for planning/memory needed for long-horizon navigation.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2725.0",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT (mentioned)",
            "name_full": "ChatGPT (OpenAI conversational large language model)",
            "brief_description": "ChatGPT is discussed as an example of a conversational LLM applied to interactive fiction; the authors report empirical difficulties with ChatGPT's ability to track in-game object states and preconditions accurately when used to play Inform7-based games.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "ChatGPT (as used/interacted-with for playing text games in the authors' experiments/observations)",
            "agent_description": "A conversational large language model (not used as the trained RL policy in experiments), observed by the authors to produce actions and text responses but to struggle with consistent state-tracking and precondition enforcement in interactive fiction games.",
            "base_model_size": null,
            "game_benchmark_name": "Inform7-based games / STARLING generated games (qualitative observation)",
            "game_description": "Inform7 interactive fiction games produced by STARLING; authors interacted with ChatGPT on these games (qualitative) and observed failure modes.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Authors note ChatGPT \"struggles to keep track of the states of all in-game objects and the pre-conditions necessary to use those actions\" (e.g., failing to require turning on a stove before using it); it also suffers small factual errors and non-determinism, indicating limitations in its effective state memory when used in text-game interactions.",
            "memory_limitations": "Inconsistent state-tracking and precondition enforcement; small factual errors and non-deterministic outputs reduce reproducibility — these are reported as shortcomings when ChatGPT is used to play interactive fiction.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2725.1",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-based world-model / LLM action agents (mentioned)",
            "name_full": "LLM-based world models and action-generation agents (references: Ammanabrolu et al., Wang et al., Yao et al., etc.)",
            "brief_description": "Prior works are cited that use LLMs for action generation, building textual world models, or other roles in text-game play; the paper notes these lines of work and comments that using LLMs to learn underlying text representations does not necessarily improve downstream RL performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "LLM-based agents (Behavior cloning, Text Decision Transformers, CALM-GPT2, LLM world models and action-generation approaches)",
            "agent_description": "Various LLM-based approaches referenced in the paper include behavior-cloned transformers, decision transformers, GPT/GPT-2/GPT-style models used for action generation or for building textual world models; they are compared in Table 3 of the paper where STARLING (GRU-based) is reported to outperform many LLM-based baselines on ScienceWorld test variations.",
            "base_model_size": null,
            "game_benchmark_name": "ScienceWorld (test variations) among others",
            "game_description": "ScienceWorld science-domain tasks; referenced LLM-based models (e.g., BC, TDT, CALM) were used by other works to tackle these tasks.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": "Paper reports that STARLING (GRU-based, ≈200K params) outperforms both GRU-based and several LLM-based models (Behavior Cloning, Text Decision Transformers, CALM-GPT2) on the majority of tested ScienceWorld tasks (Table 3), but the paper does not include detailed per-model memory architectures or explicit memory-based performance metrics for these LLM baselines.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "The paper notes prior work showing that using LLMs to learn underlying text representations does not necessarily yield better performance (citing Wang et al., 2022) — implying that model size or LLM usage alone is not a substitute for appropriate memory/representation or RL algorithm design.",
            "memory_limitations": "No detailed memory architectures for these LLM-based approaches are provided in this paper; the authors explicitly state they outperform some LLM-based methods despite using a much smaller GRU-based model, indicating that large capacity LLMs do not automatically provide effective memory for these tasks without suitable integration.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2725.2",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Planning to explore via self-supervised world models",
            "rating": 1,
            "sanitized_title": "planning_to_explore_via_selfsupervised_world_models"
        },
        {
            "paper_title": "Behavior cloned transformers are neurosymbolic reasoners",
            "rating": 2,
            "sanitized_title": "behavior_cloned_transformers_are_neurosymbolic_reasoners"
        },
        {
            "paper_title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "rating": 1,
            "sanitized_title": "learning_phrase_representations_using_rnn_encoderdecoder_for_statistical_machine_translation"
        }
    ],
    "cost": 0.0119205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</p>
<p>Shreyas Basavatia sbasavatia3@gatech.edu 
Keerthiram Murugesan keerthiram.murugesan@ibm.com </p>
<p>Georgia Institute of Technology</p>
<p>IBM Research</p>
<p>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models
2D0B809C95D3B5B3CA011797160EAD1F
Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents.Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills.In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment.These games let the agent hone their skills on a predefined set of tasks.We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision.Experimental results based on both the human participants and baseline text-based RL agents reveal that current stateof-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can.These results enforce STAR-LING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.* Equal Contributions † Work done while SB was a high school student at Pelham Memorial High School.Code and data can be found at https: //github.com/IBM/starling-agent.</p>
<p>Introduction</p>
<p>Interactive fiction games such as Zork can be utilized as an important test-bed to improve the generalization capabilities of text-based reinforcement learning (TBRL) agents (Hausknecht et al., 2020;Jansen, 2022).In these games, both the observed state of the game and the actions taken are in natural language.To play these games, the agents (or human players) need to understand the observed text from the environment and take relevant action toward the goal.These games encourage agents to understand the underlying state of the game and take actions to interact with the environment.In order to be successful, agents must use previously learned skills in new situations to complete an overarching goal.Current environments of interactive fiction games suffer from two major problems.First, environments such as TextWorld Commonsense measure simple commonsense reasoning based on one-hop relationships between entities (e.g., apple → refrigerator) (Murugesan et al., 2021a) but lack game complexity (besides a fewer number of games) to learn skills and generalize to novel domains.Second, environments such as ScienceWorld (even though many variations of task-based games are available) and Jericho are arXiv:2406.05872v1[cs.LG] 9 Jun 2024 domain-specific so agents that play these environments may perform well while conducting specific tasks like completing science experiments but lack generalized skills to apply them to other situations (Wang et al., 2022;Hausknecht et al., 2020).Most importantly, in order to generate a large number of games to train the RL agents to master skills in these environments, we will have to employ human annotators to manually design, generate, and deploy the game.Therefore, the purpose of this work is to develop an efficient approach that generates a large amount of text-based games to train the RL agents to master the desired skills and excel at the target environments such as TWC, ScienceWorld, etc.</p>
<p>As developing a set of text-based games is a time-intensive manual process, we propose Self-supervised Text-bAsed RL learnING, "STAR-LING", an interactive environment that utilizes Large Language Models (LLM) and an integrated interactive fiction game engine (Inform7 (Nelson, 2006)) to easily produce games in any domain.We generate a set of 100 text-based games using GPT3 (Brown et al., 2020) based on the input game ideas (seed list) that emphasize the need for the everyday skills such as boiling water, cooking pasta, etc. in (pre-)training text-based RL agents.These games require agents to use a specific sequence of actions to achieve the goal and successfully complete the game.For example, while cooking pasta, an agent must first gather the ingredients, fill pot with water, boil the water, and put the pasta in the pot.We then deploy the pre-trained RL agent on the target environments.This novel game-generation method can easily be used by others to create their own games and be adapted for future applications to build challenging RL agents in various domains.Figure 1 shows the overview of the proposed approach for self-supervised text-based reinforcement learning using LLM.</p>
<p>2 Self-supervised Text-based RL Self-supervised RL involves bootstrapping RL agents with auxiliary tasks in an unsupervised or semi-supervised setting to accelerate learning and generalize in the target tasks.With the recent interest in LLMs, in this paper, we consider LLMs as an alternative option to pre-train an RL agent with minimal human supervision.Unlike in the other textbased environments such as TextWorld (Côté et al., 2019), TextWorld Commonsense (TWC) (Muruge- san et al., 2021a), Jericho (Hausknecht et al., 2020), ScienceWorld (Wang et al., 2022), we utilize the skill generation capability of the large language models (Huang et al., 2022) to automatically generate text-based games based on the input game ideas with minimal human supervision.Our proposed approach for self-supervised TBRL, STARLING is an interactive text-based environment with assistance from LLM and enables the text-based RL agent to hone their extra-curricular skills1 .In this paper, we assume a seed list of game ideas is already available as input to STARLING.These game ideas are chosen to exhibit specific skills either for creating a generalized agent or targeting domain-specific environments.Optionally, the RL agent can generate a new set of game ideas during training, specific to the target domain to improve its performance.</p>
<p>Constructing Pre-training Games</p>
<p>In this section, we briefly describe how we generate the pre-training games from the game ideas using LLM and Inform7.Given the set of game ideas (seed) to LLM (Brown et al., 2020), we design a method that procedurally generates text-based games based on the interactive fiction game engine.In this paper, we use GPT3 as our LLM.Inform7 is an interactive fiction programming language that allows users to create interactive fiction games using natural language instructions (Nelson, 2006).Previous text-based environments such as TextWorld, Jericho, ScienceWorld, etc. use Inform7 (in the backend) to generate a handful of text-based games manually that require agents to explore the environment and take a sequence of actions to complete a goal such as cooking a pasta.Based on our observation from these environments, we find that the game generation can be modularized into four parts: setup, object creation, custom action, and reward assignment:</p>
<ol>
<li>
<p>Setup -defines basic properties about the game such as the room, entity types, any external libraries (Inform7), etc.</p>
</li>
<li>
<p>Object Creation -creates in-game entities such as bread or jelly.Each entity is placed in its proper location like the refrigerator or cabinet and assigned properties such as portable, open, or closed.</p>
</li>
<li>
<p>Custom actions -defines actions not native to Inform7.Each custom action checks for the pre-conditions and then executes the action by initiating the relevant state changes, and returning the proper observations to the agent.We utilize predefined action templates to incorporate custom actions during the game generation.</p>
</li>
<li>
<p>Rewards -assigns reward value for gathering the necessary entities and completing custom actions to achieve the goal.Once all the rewards are collected for each game, the game ends.</p>
</li>
</ol>
<p>Figure 2 shows the overview of the game generation using STARLING.When we feed a game idea from the seed list, STARLING prepares a prompt using natural language instruction and example game metadata as shown in Figure 3(a), with information about the setup, objects, custom actions, and rewards required for the game idea.We input this prompt to an LLM which generates the requested information as shown in Figure 3(b).We initiate each prompt for a game idea with the necessary objects that the agent needs and agents must collect those objects and use them to cook, clean, build, or complete the high-level task.In order to be successful in these games, agents must understand the properties, location, and affordances of objects in addition to the specific sequence of actions needed to accomplish the task.</p>
<p>Next, we write the output from the GPT3 output into a JSON file as shown in Figure 13 (supplementary).The objects, actions, and tasks from the GPT3 output correspond to the entities, custom actions, and verbs sections of the JSON file respectively.At this stage, the user may update or change game information in the JSON file.If the user approves the game-related data in the JSON file, we write the Inform7 code based on the JSON file.We then convert this code into an Inform7 game for a given game idea using the Glulx2 interpreter for interactive fiction games.</p>
<p>Parsing LLM Response</p>
<p>Since the response generated by LLM may not strictly follow the desired format, we follow additional steps to mitigate the irrelevant content in the response from LLM.First, we request a specific set of game-related data from LLM in a slotfilling style text generation to reduce the amount of long unstructured text generation (Rakotonirina et al., 2022).Since LLM are good at instructionfollowing when few-shot examples (input-output pairs) of a similar problem are given as a part of the prompt input, we add k-shot examples (k=3) to guide the LLM to generate a response.Figure 3(a) shows one of the three examples given as a part of the input prompt.Finally, during game compilation, the Glulx interpreter verifies whether the information extracted from the response adheres to the Inform7 programming language syntax.In addition, the pipeline for game generation provides an option for users to review the generated JSON file.When the generated game files still contain irrelevant content, we repeat the text generation multiple times to get the desired response from LLM.</p>
<p>Game Insights</p>
<p>Using the above approach repeatedly, we built a set of 100 games with minimal human supervision for training and evaluating the text-based RL agents with skills.These games have on average 2 skills and 4 rewarded states per game.These games have multiple sub-tasks which indicate that agents must utilize at least 2 skills (on average) for each game in the correct order.</p>
<p>Agents must take approximately 7 actions on average to complete each game, though some of these actions do not necessarily have to be completed in order (e.g. the agent can "turn on the stove" before "fill the pot with water" and vice versa).Games in TWC only require agents to gather objects and take actions to place them in their commonsense locations.These actions can often be completed in any order, whereas generated games, such as cooking pasta, require agents to gather objects and use other related skills in a specific sequence to achieve the final goal.</p>
<p>Experiments</p>
<p>In this section, we report the experimental results of the proposed approach: STARLING.We pre-train the RL agent on the generated 100 games (train split).We evaluate the pre-trained agent (STAR-LING) on three benchmark environments for textbased games: TextWorld Commonsense (TWC) with easy, medium, and hard difficulty levels (Murugesan et al., 2021a) 2) ScienceWorld with 4 tasks and variations (Wang et al., 2022).3) Zork1 from Jericho.</p>
<p>Text-based RL Agent</p>
<p>In this section, we briefly describe the text-based agent used for all the experiments.Based on the recent observation that using LLM to learn the underlying representation of text in the environment does not necessarily improve the performance (Wang et al., 2022), we follow previous works (He et al.,  We use individual GRUs (Cho et al., 2014) for the information from the text-based environment such as observed text, the content of the inventory, the description of the room where the agent currently is located, and a valid list of actions.We learn the state representation by concatenating the individual representations from their GRUs (Cho et al., 2014).We compute the action probability from both the state and action representations.We use Advantage Action-critic (A2C) to train the network (Mnih et al., 2016).In order to limit the impact of architecture and text-based RL algorithms, we consider these standard architectures for representation learning and RL algorithms that have proven to work well in text-based environments.We plan to study the impacts of different RL algorithms and diverse sets of representation learning approaches in our future work.All the results reported in this paper are averaged over 3 runs.</p>
<p>Pre-training Text-based RL Agent</p>
<p>We generate 100 games that demonstrate basic skills in these environments such as cooking pasta, painting the living room, boiling water, lighting a candle, etc.We split these 100 games into 75 games for training and 25 for evaluation.We trained the vanilla TBRL agent on these 75 pretraining games over 100 episodes (50 max.steps per episode) and evaluated it on 25 held-out games.</p>
<p>We compare the performance of the vanilla TBRL agent (pre-training) against both the random agent (picks random action at each step) and the Human performance.We use the mean normalized score and mean moves/steps taken by the agent for comparison.We collect the Human performance results based on the 48 participants (Section B). Figure 4 shows the training performance and Table 1 shows the evaluation results.We can see that human participants (high-school students) solved these games with a perfect normalized score of 1.0 indicating that these games are easy to solve.In order to successfully finish a game, an agent needs to take certain actions in a particular order.The order of actions taken decides the future states of the entities involved in the game.</p>
<p>Self-supervised Training of Text-based RL Agent</p>
<p>Next, we deploy the TBRL agent pre-trained on ronments.We call the pre-trained TBRL agent STARLING.We expect that STARLING will outperform the vanilla TBRL agent by utilizing the skills learned using LLM and boosting the performance and generalization capabilities to reach the goal of the target environments: ScienceWorld, TWC, and Zork1.</p>
<p>TextWorld Commonsense Environment</p>
<p>TextWorld Commonsense environment evaluates the agent on commonsense reasoning about everyday objects such as toothbrush, dirty towel, etc.The environment, based on Textworld engine (Côté et al., 2019), includes three difficulty levels: easy, medium, and hard depending on the number of objects to find and the number of rooms to explore.Each difficulty level includes 5 training games and 5 evaluation games similar to the distribution of the training games3 for a total of 30 games with a batch size of 1 for this experiment.We train the STAR-LING agent on these 15 games for 100 episodes with a maximum of 50 steps.</p>
<p>Figure 5 shows the training curves of the three difficulty levels in the TWC environment.We can see that the STARLING agent gets a boost in performance both in the scores achieved and the moves taken compared to the vanilla TBRL.This shows that the pre-training step using LLM in STARLING leverages the basic skills mastered using the 75 generated games.Table 2 confirms our hypothesis that the pre-training step in STARLING improves the overall performance across different difficulty levels.</p>
<p>ZORK1 Environment</p>
<p>Zork1 is a human-made interactive fictional game environment and one of the earliest known textbased games created based on the underworld characters with dark themes and characters such as dungeon, grue, elvish sword, etc. Zork1 is one of the 33 interactive games released as a part of the Jericho game suite.Unlike TWC and ScienceWorld, Zork1 includes a diverse set of locations (over 200 locations), larger action space, sparser rewards, and longer trajectories, making it a challenging environment.</p>
<p>In order to evaluate the effect of pre-training in the Zork1 environment, we simplified the game with "killing troll" as a final goal (Zahavy et al., 2018).The agent needs to find the lantern and sword from the house, locate the hidden passageway to the underworld, light the lantern, and kill the troll.Without the lantern and sword, the agent entering the troll room reaches the failure state with negative rewards.In addition to these intermediate rewards, the game includes additional rewards when the agent collects a jewel-encrusted golden egg from the tall tree in the forest (+5 score) and a painting from the art gallery in the house (+4 score).We train the agents on 100 episodes with a maximum step of 100 steps per episode.</p>
<p>ScienceWorld Environment</p>
<p>ScienceWorld environment evaluates the science reasoning abilities of the TBRL agents.It consists of several tasks from topics such as change of state, biology, classification, etc.We choose all the 30 science-domain tasks from the themes.Each of these tasks contains 10 − 1400 variations of the game and are split into 50% training, 25% for evaluation set, and 25% for test set.We train the STARLING agent with 100k maximum steps on a single environment (with a maximum of 100 steps per game play) 4 .</p>
<p>Figure 7 shows the training curves for both the 4 Unlike in the "number of moves taken" metric in the pre-training results, the number of moves taken in the Science-World measures how long the agent survived without reaching the failure state.An agent may reach a failure state if it takes an action that results in abruptly ending the game such as pouring water on the floor for the boiling task, etc  scores received and moves taken on the 4 tasks.We can see that STARLING outperforms vanilla TBRL on all the tasks.We notice that the pretraining steps improved the performance of STAR-LING in the first few episodes of the classification task (find a living thing).On the other hand, pretraining games such as boiling water, cooking pasta, planting a tree, etc. may have influenced the performance of STARLING in the later episodes of tasks: change of state -boiling and grow a fruit, by adapting the learned skills during pre-training to the target environment.</p>
<p>Table 3 shows the performance comparison of GRU-based and LLM-based models against STAR-LING on the test variations of the ScienceWorld.We compare the STARLING against the stateof-the-art LLM-based models Behavior cloning (BC), Text Decision Transformers (TDT), CALM-GPT2, etc (Wang et al., 2023a;Ammanabrolu and Hausknecht;Yao et al., 2020).We notice that our pretraining strategy has generally improved the performance of STARLING in the majority of the tasks.We outperform both GRU-based and LLMbased models.It is worth noting that both BC and TDT use T5-base (11 billion parameters) initialized with Macaw, CALM uses GPT2 with 131 million parameters, whereas, GRU-based STARLING uses approx 200K parameters.</p>
<p>Discussion</p>
<p>Our experiments on three text-based game environments show that pre-training these agents using LLM-generated games as auxiliary tasks generally boosts the performance of the agent.We notice that most of the performance gain achieved in the first few episodes of the gameplay on some game instances (e.g., training curves in ScienceWorld tasks find a living thing, TWC Easy, and Hard, Zork1), whereas, STARLING adapts the basic skills learned during the pre-training to the target environment to improve the final scores (e.g., training curves in ScienceWorld tasks: change of state -boiling and grow a fruit).We notice that STARLING avoids the failure states better than vanilla TBRL as can be seen in Zork1 and ScienceWorld.Similarly, STARLING tends to choose valid actions from the action space more effectively than vanilla TBRL (for example, see Figure 12 supplementary for the sample trajectories from ScienceWorld taken by vanilla TBRL and STARLING within the first few episodes for the task: changes of state -boiling).</p>
<p>Since these pre-training games lack navigational complexity that elicits skills such as planning, we observe that STARLING tends to suffer in games that require navigational skills (e.g., example, in ScienceWorld task find a non-living thing, TWC hard difficulty and Zork1).Since the pre-training games involve fewer sequences of actions (short trajectories) to collect the reward and reach the final goal, STARLING struggles when the target environment has longer trajectories to reach the goal.</p>
<p>On the other hand, STARLING tends to collect bonus scores in Zork1 that are reachable within fewer steps instead of just chasing the larger rewarded states (e.g., see Figure 6 right).</p>
<p>4 Related Work</p>
<p>Text-based Games</p>
<p>Text-based games provide a challenging benchmark for RL agents interacting with the environment in natural language.The common challenges for text-based RL are partial observability, combinatorial action space, sparse rewards, longhorizon planning, etc.To circumvent some of these challenges, it is typical that the games are often manually curated to evaluate a specific set of skills such as commonsense reasoning (Murugesan et al., 2021a), knowledge graphs (Ammanabrolu and Hausknecht;Murugesan et al., 2021b), exploration strategies (Côté et al., 2019), etc. Environments such as TextWorld Commonsense (Murugesan et al., 2021a) measure simple commonsense reasoning based on the one-hop relationship between a pair of everyday objects such (apple and refrigerator, dirty towel and laundry basket), but lacks diversity and complexity to learn a general set of skills.Environments such as ScienceWorld (Wang et al., 2022) are often domain-specific environments that require domain knowledge to perform well in these environments.Jericho (Hausknecht et al., 2020), on the other hand, includes humangenerated games that require a complex set of skills to show any progress in the gameplay.These environments are manually created by humans with very limited automation in the variations of game generation by replacing similar or related objects, changing the layout/orientation of the environment, etc.Unlike these environments, STARLING provides an approach to leverage the skill generation capability of LLMs (Huang et al., 2022) to automatically generate text-based games based on the input game ideas with minimal human supervision.</p>
<p>These games are generated automatically by requesting a specific set of game-related facts from LLM in a slot-filling style text generation (Rakotonirina et al., 2022).</p>
<p>Self-supervised RL</p>
<p>Self-supervised RL has been a popular topic in vision-based RL and robotic environments (Sekar et al., 2020;Li et al., 2022).To the best of our knowledge, we are the first to utilize LLM to generate the games to train text-based RL agents5 .Previous works have utilized LLM for action generation (Yao et al., 2020), play interactive fictional game (Tsai et al., 2023), build a world model (Ammanabrolu et al., 2020;Wang et al., 2023b), etc.These works showed that using LLM to learn the underlying representation of text in the environment does not necessarily improve the performance (Wang et al., 2022).It is shown that novel exploration strategies and efficient RL algorithms along with the learning model similar to the one in Fig- ure 11 outperform all the other LLM-based agents (Tuyls et al., 2021).On the other hand, generalist agents have been recently explored to generalize across multiple environments (Reed et al., 2022), but the performance of these agents on a diverse set of environments is less convincing (Cobbe et al., 2019).</p>
<p>Conclusion</p>
<p>In this paper, we proposed a novel self-supervised training for a text-based reinforcement learning agent, STARLING, with the help of the generalized skill generation capability of large language models like GPT3.We generated a set of text-based games that require agents to learn basic skills such as cooking pasta, boiling water, etc., and utilize sequential decision-making over the modality of text.The proposed STARLING uses the GPT3 pretrained language model to automatically generate these games.This approach can be used to create additional games or adapted to build games for new domains with minimal human intervention.We showed that the STARLING agent pre-trained on the games generated by LLM outperforms vanilla TBRL.We evaluated STARLING on three environments: ScienceWorld, TWC, and Zork1.In all these environments, STARLING showed enhanced skills in the target environment.</p>
<p>Human participants were volunteers from a local High School that agreed to participate in this study.This may have introduced a bias into the human participant data since all participants were high school educated, from one geographic region, between the ages of 15 and 18, and volunteers.Many of these participants complete homework assignments and assessments often which may make their reasoning skills better than potential participants outside of school.In the future, testing human participants from various geographic locations, age groups, and levels of education may reduce bias.The STAR-LING currently requires human intervention and/or the Game Validator (from the glulx compiler) to build functioning games.We will continue to work on building an end-to-end version of the STAR-LING, that can take a game idea and turn it into an interactive fiction game without any human intervention.This would speed up development time so a larger set of games can be created.</p>
<p>Large language models such as ChatGPT have been developed recently with the ability to interact with users in a manner conversationally similar to the interactions found in interactive fiction games.From our experimentation, ChatGPT struggles to keep track of the states of all in-game objects and the pre-conditions necessary to use those actions (e.g.ChatGPT does not always require the player to turn on the stove before using it) as well as Inform7-based games.In addition, it suffers from small factual errors, and is hard to reproduce the same result, through this could also be seen as a benefit.Despite these challenges exploring the use of models such as ChatGPT to interact with agents shows promise in the future.</p>
<p>Ethical Impact</p>
<p>We asked the human participants to play the games generated by STARLING to evaluate the game's complexity and clarity.After receiving IRB approval from a local High School and informed consent from each of the 48 human participants, we asked the participants to play five randomly assigned games via iplayif.com,an online interactive fiction game player.Players received the goal of the game and the list of admissible actions.We collected the number of steps that each player took and the score received for each game via Google form.We did not collect any personal information or personally identifiable information as a part of this study.</p>
<p>Since we use large language models such as GPT3 to generate a set of text-based games, the bias and other fairness/ethical concerns that come with the LLM may unintentionally transfer to the pre-trained agent.Additional mitigation steps may be required to filter harmful contents from the generated response.</p>
<p>Reproducibility</p>
<p>As an effort to encourage further research in selfsupervised text-based RL, we plan to release the 100 games generated as a part of this paper, the source code for STARLING, script to generate game-related files based on a set of game ideas and LLM (including game templates and metadata) as an open-source project.between the game engine and TextWorld Gym environment.This wrapper ensures that the user to freely define any object or action type and the environment works with any Glulx compiled game file without dependence on the TextWorld-generated metadata to track the state of objects throughout the game.The wrapper does this by parsing the observation state returned by game engine after every step to generate certain data-points like admissible commands, current score, last action, number of steps taken and inventory required by the TextWorld Gym environment.</p>
<p>B Human Participants</p>
<p>Humans are considered to have exemplary compositional skill learning so comparing their performance to pre-trained agent's performance is valuable to validate generated games's difficulty and effectiveness as a pre-training task.After receiving IRB approval from a local High School and informed consent from each of the 48 human participants, we asked the participants to play five randomly assigned games via iplayif.com,an online interactive fiction game player.Players received the goal of the game and the list of admissible actions.We collected the number of steps that each player took and the score received for each game via Google form.6     {"libraries" : [ {"name": "measured liquid", "author": "Emily Short"}, { "name": "modern conveniences", "author": "Emily Short"}],</p>
<p>C Pretraining Game Statistics</p>
<p>"modules" : ["scoring"],</p>
<p>"room" : { "name": "home kitchen", "description": ""}, "custom entities" : [ "Food is a kind of thing.Food is usually edible.Food can be raw or cooked.Food is usually raw."],</p>
<p>"entities" : [{"name": "pot", "type": "container", "properties": ["portable", "open"],</p>
<p>"location": "in the cabinet"}, { "name": "pasta", "type": "food", "properties": "", "location": "in the refrigerator"}, {"name": "sauce", "type": "food", "properties": "", "location": "in the refrigerator"}],</p>
<p>"scoring" : [ {"condition": "taking the pot for the first time", "increment": "1"}, {"condition": "taking the pasta for the first time", "increment": "1"}, {"condition": "taking the sauce for the first time", "increment": "1"}],</p>
<p>"actions" : [{"name": "", "declaration": { "command": "cook [something] with [something]", "alias": "cooking it with", "applicable_to": "one carried thing and one thing"}, "prerequisites": [],</p>
<p>"constraints": [{"condition": "the noun is not a food", "prompt": "You can't cook that."},{ "condition": "the second noun is not a stove", "prompt": "You can't cook that."}],</p>
<p>"definition": { "tasks": [ "increase score by 1"],</p>
<p>"prompt": "You cooked the [noun] with the [second noun]."}, "postrequisties": [] } ],</p>
<p>"end_game" : { "condition": "4", "task": "end the story finally", "tasks": ["look", "inventory", "open cabinet", "take pot", "drop pot", "open fridge", "take pasta", "drop pasta", "turn on sink", "turn off sink", "fill pot with water", "turn on stove", "turn off stove", "boil water in pot", "cook pasta with stove"]}}</p>
<p>Figure 13: Example JSON file produced for cooking pasta game idea.The libraries, modules, and room sections were part of the setup, the custom entities and entities sections correspond to object creation, the actions correspond to the custom actions, and the scoring and end game correspond to the rewards sections of each game.The entities section describes names, types, and properties of entities present in the game.The actions section defines custom actions including their declaration, alias, and constraints not part of Inform7 by default.The end-game section defines the maximum score and the list of admissible actions that the user can take.</p>
<p>Figure 1 :
1
Figure 1: Architecture diagram for Self-supervised Text-based Reinforcement Learning using LLM (STARLING).</p>
<p>Figure 2 :
2
Figure 2: Workflow of the STARLING Game Generator using large language model (GPT3).</p>
<p>Figure 3 :
3
Figure 3: (A) GPT3 input prompt for cooking games with one action example.The actual prompt contains four action examples.(B) GPT3 output for cooking pasta game idea.GPT3 reliably outputs accurate and necessary game information very similar to the input.</p>
<p>Figure 4 :
4
Figure 4: Training curves for pre-training step of STARLING depicting the normalized scores (left) and number of moves taken (right) of text-based reinforcement learning agents.</p>
<p>Figure 5 :
5
Figure 5: Training curves for TWC easy (left), medium (middle), and hard (right) games depicting the normalized scores (top) and number of moves (bottom) of both vanilla TBRL and STARLING agents.</p>
<p>Figure 6 :
6
Figure 6: (left) Performance of TBRL agents on Zork1.(right) Sample trajectory from STARLING showing the bonus points of (+4 scores) for collecting the painting in the art gallery within the first few episodes.</p>
<p>Figure 6 (
6
Figure 6 (left) shows the performance of both the STARLING and vanilla TBRL on the Zork1 environment.As in TWC, we can see that the pretraining step boosts the performance of STARLING in the first few episodes compared to the vanilla TBRL agent.As in ScienceWorld, STARLING successfully avoids the failure state compared to the vanilla TBRL agent.</p>
<p>Figure 7 :
7
Figure 7: Training curves for ScienceWorld -Boil Substance (Matter), Find a living thing (Classification), Find a non-living thing (Classification), and Grow a fruit (Biology) games depicting the scores (left) and number of moves (right) of text-based reinforcement learning agents.</p>
<p>Figure 8 :
8
Figure 8: (Left) shows the overview of the proposed self-supervised text-based reinforcement learning with large language model.(Right) shows the sample text-only agent play through of cooking pasta game.Players must use the boil skill at the correct time to be successful.</p>
<p>Figure 9 :Figure 10 :
910
Figure 9: Architecture diagram for Self-supervised Text-based Reinforcement Learning using LLM (STARLING).</p>
<p>Figure 11 :
11
Figure 11: Vanilla Text-based RL agent used in this paper.</p>
<p>Figure 12 :
12
Figure 12: Sample Trajectories taken by STARLING and Vanilla TBRL for the task: change of state (boiling) task in ScienceWorld.</p>
<p>Table 1 :
1AgentsMean Normalized Score Mean Moves TakenRandom0.050 ± 0.0150 ± 0.0Pre-training0.72 ± 0.06328.105 ± 1.876Human1.000 ± 0.0009.640 ± 5.620
Performance of random and pre-trained agents on a set of 25 unseen pre-training games after training on 75 pre-training games over 100 episodes.Mean Norm.Score (higher is better, normalized with maximum score achievable per game) and Mean Moves Taken (to achieve the goal, lower is better).</p>
<p>Table 2 :
2
Murugesan et al., 2021a;f vanilla TBRL (without pre-tYao et al., 2020;LING on the test set with the three difficulty levels of Textworld Commonsense (TWC).All the scores and moves are averaged over 3 runs.2016;Murugesanetal., 2021a; Ammanabrolu and  Hausknecht;Yao et al., 2020; Atzeni et al., 2022)and use GRU-based Vanilla TBRL agent to evaluate our proposed approach (Figure11supplementary).</p>
<p>Table 3 :
3
Performance comparison of STARLING against other GRU-based and LLM-based agents on test variations from ScienceWorld.</p>
<p>*indicates the reliance on a valid action list during evaluation.</p>
<p>In this work, we define the skills based on the auxiliary task such as "boil <object>", "fill <container>", "cook <object>", etc. We assume that LLM such as GPT3 has the necessary knowledge to generate text-based games based on these basic skills.
https://en.wikipedia.org/wiki/Glulx
In addition to the 5 evaluation games, TWC includes 5 test games from out-of-distribution. Since these games require external knowledge such as ConceptNet(Speer et al., 2017), ATOMIC(Hwang et al., 2021) etc., we exclude them from our experiments.
Several LLM-based interactive fictional environments for entertainment purposes exist(AID, 2019).
No personal information was collected as a part of this study.
A Additional DetailsIn order to generate games that require composing previously learned skills, we take inspiration from household chores, cooking, and maintenance tasks.We generate 100 game ideas and use the STAR-LING to generate a set of 100 games.We choose the game ideas carefully for the learning agent to utilize similar skills (ex.baking, mixing, spreading, using a hammer, etc) in new situations therefore forcing the agent to generalize skills and compose them with other skills.For example, while cooking pasta, an agent must learn how to boil water which is a skill that can be applied for a related game idea "brewing tea".LLMs such as GPT-3 are prone to making factual and grammatical errors, in addition to violating the specified format.We check for any errors in the generated game(s) using a Game Validator as a part of glulx compiler which uses depth first search (DFS) to explore all the possible trajectories in the game.To correct for minor errors and inconsistencies in each game, information from GPT-3 can also be optionally verified by the human authors in the JSON file.We found that, in cases when the created game has errors, restarting the game generation a few times usually results in a playable game.A.1 From GPT3 output to game JSONWe extract the information from GPT-3 using Python simple regular expression rules by first splitting the output into three sections: task sequence (ex.Open cabinet, take pot), objects (ex.Pot), and actions (ex.Fill pot with water).We add the task sequence to the list of admissible actions the player could execute within the game.We store the objects internally with a type, a location, a name, and a set of properties.We further split the actions section into default actions and custom actions which are actions native and non-native to inform7 respectively.Similar to the objects, we store each custom action internally with a name, a declaration, a definition, a set of constraints, a set of prerequisites, and a set of post-requisites.A.2 Inform7 Code GeneratorWe wrote a simple script based on the JSON structure and the action templates to generate the In-form7 Code for a given game idea.This script along with the other code &amp; data will be shared to the public in the near future.A.3 Modifying TextWorld Gym for STARLINGOpenAI Gym is a general reinforcement learning framework that acts as an interface between RL agents and Inform7-based STARLING game engine(Brockman et al., 2016).Gym connects environments with agents by using a monitor to keep track of every step, state of the game, the final score of agents, and the sample complexity or the amount of time an agent takes to learn.Most default environments in Gym support a continuous or discrete action space although interactive fiction games require combinatorial action spaces in natural language(Hausknecht et al., 2020).The TextWorld Gym customized the OpenAI Gym for interactive fiction games.In this work, we repurpose the custom Gym environment created for TextWorld environment with Inform7 object and action types.TextWorld's Gym environment only supports TextWorld-generated games which includes a Glulx compiled game file and a TextWorld-generated JSON file with game metadata defined in proprietary TextWorld classes.This restricted our ability to create games with objects and actions previously undefined in TextWorld environment.These objects and actions must be defined according to TextWorld's grammar and logic rules.This is a time consuming process and is prone to many errors.The goal of the STARLING Game Generator is to allow users to automate the game creation using LLM, and most importantly, create games without learning a new programming language or familiarizing themselves with any grammar rules.To get rid of these restrictions, an entirely new wrapper was created which acted as an interface
Ai dungeon. </p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. </p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Keerthiram Murugesan, and Mrinmaya Sachan. 2022. Case-based reasoning for better generalization in text-adventure games. Mattia Atzeni, Shehzaad Dhuliawala, ICLR 2022</p>
<p>Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016</p>
<p>Language models are fewshot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, . . Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Quantifying generalization in reinforcement learning. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman, International Conference on Machine Learning. PMLR2019</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Stockholm, SwedenSpringer2019. 2018. July 13. 2018Revised Selected Papers 7</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics2016</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>(comet-) atomic 2020: on symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jeff Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>A systematic survey of text worlds as embodied natural language environments. Peter Jansen, 10.18653/v1/2022.wordplay-1.1Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022). the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022)Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Does self-supervised learning really improve reinforcement learning from pixels?. Xiang Li, Jinghuan Shang, Srijan Das, Michael Ryoo, Advances in Neural Information Processing Systems. 202235</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, AAAI. 2021a</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2021b2</p>
<p>. Graham Nelson, 2006. Inform7</p>
<p>Can discrete information extraction prompts generalize across language models?. Nathanaël Carraz Rakotonirina, Roberto Dessi, Fabio Petroni, Sebastian Riedel, Marco Baroni, The Eleventh International Conference on Learning Representations. 2022</p>
<p>. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, A generalist agent. Transactions on Machine Learning Research. 2022</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, International Conference on Machine Learning. PMLR2020</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Can large language models play text games well? current state-of-the-art and open questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, arXiv:2304.028682023arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, Sham M Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2021</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Behavior cloned transformers are neurosymbolic reasoners. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational Linguistics2023a</p>
<p>Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Côté, Peter Jansen, Bytesized32: A corpus and challenge task for generating task-specific world models expressed as text games. arXiv e-prints. 2023b2305</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in neural information processing systems. 201831</p>            </div>
        </div>

    </div>
</body>
</html>
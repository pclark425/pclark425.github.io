<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1943 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1943</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1943</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-278207593</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.21530v1.pdf" target="_blank">RoboGround: Robotic Manipulation with Grounded Vision-Language Priors</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in robotic manipulation have high- lighted the potential of intermediate representations for improving policy generalization. In this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by large-scale vision-language models pretrained on diverse grounding datasets. We introduce ROBOGROUND, a grounding-aware robotic manipulation policy that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks. To further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions. Extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies. Code and data will be available at robo-ground.github.io.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1943.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1943.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROBOGROUND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied manipulation system that uses pixel-level grounding masks from a grounded VLM as an intermediate representation to guide a language-conditioned policy; masks are integrated via channel concatenation and a mask-guided 'Grounded Perceiver' that biases patch-level attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ROBOGROUND</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipeline: (1) a grounded vision-language model (based on GLaMM) ingests an image and a language instruction and outputs binary pixel masks for the target object and placement area using an LLM with a <SEG> token and a SAM-like decoder; (2) a language-conditioned transformer policy (GR-1 style) that encodes historical images with ViTMAE, concatenates predicted masks as extra image channels, and passes local patch features through a Grounded Perceiver whose attention is explicitly biased to mask regions before action decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP vision encoder (for VLM input), ViTMAE encoder (for policy visual input), SAM encoder (initialized in grounding head decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP (used as image feature encoder in the VLM; pretrained on web image-text corpora as cited), ViTMAE (pretrained MAE-style vision encoder referenced), SAM (Segment Anything model encoder initialization). Exact dataset sizes for CLIP/ViTMAE are not specified in this paper beyond citations.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Pixel-level segmentation via an LLM conditioned on projected CLIP visual tokens and the text instruction: a special <SEG> token's last-layer embeddings are projected into a SAM-like decoder to produce binary masks. Masks then guide the policy by (a) channel concatenation to the image input and (b) mask-guided attention in a grounded perceiver where attention entries over masked patch regions are filled with the current maximum to force focus.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Pixel-level / region-level (binary segmentation masks for object and placement area)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel masks and patch-level attention over a 14x14 patch grid; no explicit 3D coordinates or depth used (spatial guidance is provided by masks and patch indices)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation (pick-and-place) and other manipulation primitives (open/close, press, turn/twist)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RoboCasa simulated manipulation tasks (pick-and-place variants and 22 atomic tasks from RoboCasa)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (RoboCasa rendering with multiple camera views: left, right, hand view)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contact rate (%) and success rate (%) for pick-and-place; success rate (%) for other atomic skills</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pick-and-place (Ours): Easy 89.0% contact / 43.3% success; Appearance 78.5% / 30.5%; Spatial 81.0% / 33.5%; Common-sense 76.3% / 30.0. Other skills: Open/Close 72.0% success, Press 69.3% success, Turn/Twist 54.5% success. (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation (no channel concat, no grounded perceiver) baseline: Easy 68% contact / 24% success; Appearance 42% / 12%; Spatial 52% / 16%; Common-sense 40% / 12% (Table 4, first row).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Using both channel concatenation and grounded perceiver improves Easy from 68/24 to 86/42 (absolute +18% contact, +18% success); Appearance from 42/12 to 72/32 (+30% contact, +20% success); Spatial from 52/16 to 78/38 (+26% contact, +22% success); Common-sense from 40/12 to 70/32 (+30% contact, +20% success) (Table 4, differences between first and last rows).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>The paper compares grounding representation modalities (pixel masks vs. bounding boxes vs. center points and low-dimensional encodings): pixel masks give the best performance; lower-dimensional representations (points, bbox) are harder to learn and perform worse. For the grounded VLM, zero-shot mIoU = 13.2; fine-tuning on simulated grounding data raises mIoU to 45.5; fine-tuning on simulated + original VLM data yields 48.2 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify two perception-related bottlenecks: (1) the standard perceiver token-resampling can lose critical localized information about target regions, harming manipulation; (2) gap between contact and success rates suggests perception/localization alone is insufficient for grasp precision—mask localization helps contact but not reliably correct grasp pose across thousands of diverse objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Key failure modes related to grounding: (a) segmentation/grounding errors harm downstream success (VLM zero-shot mIoU 13.2 -> after fine-tuning mIoU up to 48.2); (b) high contact but low success (e.g., Ours: Easy 89% contact vs 43.3% success) indicating grasping failures even when localization is correct; (c) limited placement-area diversity (noted as dataset limitation); (d) loss of information via token resampling in perceiver can cause missed local details — addressed by mask-biased attention. Quantitative frequencies are presented in metrics (contact vs success) but per-failure-type percentages beyond these are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>They fine-tune the grounded VLM on simulated instruction-following grounding data and combine simulated data with original VLM grounding data to preserve prior knowledge; this reduces domain mismatch and improves mIoU from 13.2 (zero-shot) to 48.2 (fine-tuned on sim + VLM data). Masks are computed once per episode (assumes single-turn static target positions) to reduce inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Zero-shot/unseen evaluation (Table 2): Unseen instance (no mask) 71.0% contact / 35.5% success; with predicted mask 85.8% / 42.5%. Unseen class (no mask) 53.0% / 17.5%; with predicted mask 79.5% / 27.3. This demonstrates substantial absolute improvements in both contact and success when masks are used for novel objects/classes.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>For the grounded VLM, zero-shot (effectively 'frozen' prompt-only behavior) mIoU = 13.2; fine-tuning on simulated data raises mIoU to 45.5, and adding original VLM grounding data yields 48.2 (Table 6). The paper does not present a frozen vs fine-tuned comparison for the policy visual encoder (ViTMAE) explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Two fusion paths: (1) early/low-level channel concatenation of binary masks with RGB image followed by a linear projection and ViTMAE encoding; (2) mask-conditioned attention inside the perceiver where attention cells corresponding to masked patches are set to the maximum attention value to force queries to focus on masked regions. The VLM fuses visual CLIP features and text via projection into the LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Paper provides dataset sizes (24K generated demonstrations with 112K instructions added to 66K RoboCasa demos), and ablations show mask guidance is most helpful when training includes the more complex generated data; no explicit sample-efficiency ratio (e.g., x vs y demos) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Pixel-level grounding masks from a grounded VLM (fine-tuned on simulated instruction-following data) are an effective intermediate representation for robotic manipulation: they substantially improve generalization to unseen instances and classes, outperform simple low-dimensional grounding (points/bboxes), and mitigate information loss from token resampling when integrated via a mask-biased perceiver; however, masks mainly improve localization/contact and do not fully solve grasping precision, indicating perception grounding is necessary but not sufficient for high success rates across highly diverse object sets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1943.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1943.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLaMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLaMM (Pixel grounding large multimodal model as used in ROBOGROUND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pixel-level grounded vision-language model used to generate binary segmentation masks for instruction-specified target objects and placement areas; in this paper GLaMM is fine-tuned on simulated instruction-following segmentation data to improve mask accuracy for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Glamm: Pixel grounding large multimodal model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLaMM (fine-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based grounded VLM that encodes an image with CLIP vision features projected into the LLM token space (256 token prompt representation), integrates the text instruction, and exposes a special <SEG> token whose embeddings are projected into a SAM-like pixel decoder (encoder initialized from SAM) to produce binary segmentation masks per <SEG> token for object and placement region.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP vision encoder (used to produce visual tokens for the LLM prompt), SAM encoder used in pixel grounding head, and a SAM-like decoder for mask generation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>GLaMM is initialized from a checkpoint pre-trained on the 'Grounding-anything' dataset (paper states this dataset contains 7.5M unique concepts spanning 810M regions); CLIP provides the image features used as LLM input (CLIP pretraining on web image-text is cited).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>LLM integrates projected CLIP visual tokens and instruction text; <SEG> token embeddings are projected into a SAM-like decoder to produce per-token binary segmentation masks (pixel-level grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Pixel-level segmentation masks</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel masks; patch-aware decoding via SAM-like decoder; no explicit 3D geometry reported</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Provides grounding masks for object manipulation tasks (pick-and-place) used by the policy</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Mask generation for RoboCasa simulated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Image observations from photorealistic RoboCasa simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mIoU (mean Intersection over Union) for predicted masks against ground-truth simulated masks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VLM mIoU: Zero-shot 13.2; Fine-tuned with simulated grounding data 45.5; Fine-tuned with simulated data + original VLM data 48.2 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Zero-shot mIoU 13.2 (i.e., without fine-tuning on simulation instruction-following data).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Fine-tuning on simulated data increases mIoU from 13.2 to 45.5 (+32.3 mIoU absolute); adding original VLM data yields 48.2 (+34.9 absolute from zero-shot) (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper evaluates the effect of fine-tuning GLaMM with different grounding data sources (zero-shot prompting vs. sim-only vs. sim + original VLM data), showing large improvements in mIoU from fine-tuning; no multiple CLIP variants are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Zero-shot grounding is inadequate (low mIoU) for precise manipulation; fine-tuning on simulated instruction-following masks is required to reach practical segmentation quality for downstream policy use.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Zero-shot prompting via dependency-parsed nouns produced ambiguous mask outputs and poor mIoU (13.2). Ambiguity in instruction parsing and lack of instruction-following fine-tuning are primary causes of low mask quality before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mitigated by fine-tuning GLaMM on a simulation-based instruction-following dataset and by combining simulated grounding examples with original VLM grounding data to preserve prior knowledge (improves mIoU).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>After fine-tuning, masks enable better zero-shot policy generalization to unseen instances and classes (policy-level numbers reported in Table 2); VLM mask quality improvements correlate with downstream gains but exact per-object mIoU breakdown for novel classes is not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Fine-tuning yields large gains in mIoU relative to zero-shot prompting (13.2 -> 45.5 -> 48.2), indicating substantial benefit from instruction-following fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>GLaMM base checkpoint was trained on a very large grounding dataset (Grounding-anything: 7.5M unique concepts / 810M regions as cited); paper leverages that pretraining but does not perform explicit scaling studies.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Visual CLIP features are projected into the LLM embedding space and combined with text tokens; a <SEG> token in LLM output is then decoded to a pixel decoder for segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuning on simulated instruction-following data (size not individually enumerated beyond overall dataset) greatly improves mIoU; exact sample-efficiency ratios are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A pretrained pixel-grounding VLM (GLaMM) requires instruction-following fine-tuning on simulation-ground-truth masks to produce masks of sufficient quality for downstream manipulation; fine-tuning using both sim data and original grounding data best preserves prior knowledge and maximizes mIoU.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1943.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1943.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounded Perceiver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Perceiver (mask-guided perceiver for policy token reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified perceiver module that augments global learnable queries with object and placement-area query groups whose attention is explicitly biased towards mask-covered image patches, reducing token loss of task-relevant local information during token resampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grounded Perceiver</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Perceiver-style token resampler taking 14x14 patch features and three sets of k learnable query tokens: global queries Qg, object queries Qo, placement queries Qp. During attention computation the attention weights for entries corresponding to mask-covered patches (Mo for object queries, Mp for placement queries) are replaced with the current maximum attention value to enforce focus on masked regions; the final perceived visual features are concatenation of outputs from all three query groups.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Operates on ViTMAE patch features (14x14 grid) produced earlier in the policy pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>ViTMAE (pretrained MAE-style encoder as cited), used to produce patch features; Grounded Perceiver itself is a learned module trained from scratch with the policy</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Mask-guided attention: during cross-attention between query tokens and patch features, attention entries corresponding to mask regions are set to the maximum attention value so object and placement queries attend strongly to masked patches.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Patch-level (operates over 14x14 patch features derived from ViTMAE), guided by pixel-level masks</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses 2D patch grid indices aligned with masks; no explicit 3D geometry</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Component in policies for object manipulation (pick-and-place) and other manipulation primitives</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used within the ROBOGROUND policy evaluated on RoboCasa tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation images from RoboCasa (multi-view inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream contact/success rates for pick-and-place tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Inclusion of Grounded Perceiver (with channel concat) yields e.g., Easy 86%/42% compared to 78%/36% (channel concat only) and 68%/24% (no mask integration) — see Table 4. Combined with mask channel concat it achieves the best reported ablation results.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without mask-guided perceiver (but possibly with channel concatenation) performance is lower (e.g., channel concat only: Easy 78%/36 vs channel concat + Grounded Perceiver 86%/42). Without any mask integration performance is 68%/24 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Adding Grounded Perceiver on top of channel concatenation yields modest further gains: channel concat only 78/36 -> +8/6 to 86/42 when Grounded Perceiver is added (Easy case). For harder reasoning types improvements are larger (e.g., Appearance 60/26 -> 72/32 with both).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Perceiver token resampling can drop critical localized information; grounded perceiver mitigates this by explicitly forcing attention to mask regions so object- and placement-specific tokens retain important local features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Without grounded perceiver, policies may miss fine-grained local details needed for manipulation; grounded perceiver reduces but does not eliminate downstream grasping failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>When used with predicted masks, grounded perceiver contributes to the improved zero-shot/unseen-instance performance shown in Table 2 (e.g., unseen instance with mask 85.8/42.5).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Patch-level cross-attention between query groups and patch features with attention values for mask-covered patches replaced by the max attention value to bias focus.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Biasing perceiver attention using segmentation masks preserves task-relevant local patch information that standard token-resampling perceivers may discard, improving downstream manipulation success especially on harder reasoning tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1943.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1943.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mask Representation (vs bbox/point)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixel-level Mask Grounding Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of binary pixel masks for object and placement area grounding, compared to lower-dimensional representations (center point or 2D bounding box) and low-dimensional encodings; masks capture object shape/size and provide richer spatial guidance for policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mask-based grounding (representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel-level binary masks produced by the grounded VLM (GLaMM) are used as (a) an extra image channel concatenated to RGB inputs and (b) inputs to mask-guided perceiver attention. Alternative representations (center point, 2D bbox, or low-dim encodings) were evaluated in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (representation level)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Pixel mask outputs from VLM; masks used for early-fusion channel concatenation and for biasing attention in perceiver.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Pixel-level (binary segmentation mask) vs low-dimensional (center point, bbox)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel masks encoding spatial extent and shape</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation (pick-and-place) and manipulation primitives</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RoboCasa pick-and-place and atomic tasks used in paper</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (RoboCasa)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contact / success rates; mIoU for mask quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablations (Table 5 summary): masks give best downstream performance; bounding boxes and points as low-dimensional inputs result in lower policy performance. Specific numbers are reported qualitatively: masks provide modest improvements over bbox; low-dim encodings are more challenging to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Using points or bbox as low-dimensional inputs yields lower performance than masks (exact per-method numbers are in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Mask-based grounding outperforms bbox/points; masks better capture object shape/size and yield improved success/contact rates (absolute differences are task-dependent as shown in the ablation table).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Low-dimensional grounding loses shape/size cues and spatial extent, making learning downstream policies more difficult; masks alleviate this by preserving richer spatial information.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Policies relying on low-dimensional grounding (point/bbox) struggle more in cluttered scenes and discriminative appearance/spatial reasoning tasks compared to mask-based policies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Mask guidance substantially improves generalization to unseen instances and classes compared to non-mask baselines (Table 2 shows large relative improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Channel concatenation (masks added as image channels) and mask-driven attention in perceiver.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Pixel-level masks are the most effective grounding representation evaluated: they give richer spatial cues (shape and size) that materially improve policy generalization in diverse, cluttered simulated scenes compared to bounding boxes or center points.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Glamm: Pixel grounding large multimodal model <em>(Rating: 2)</em></li>
                <li>Segment anything <em>(Rating: 2)</em></li>
                <li>GR-1 <em>(Rating: 1)</em></li>
                <li>RoboCasa: Large-scale simulation of everyday tasks for generalist robots <em>(Rating: 2)</em></li>
                <li>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1943",
    "paper_id": "paper-278207593",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "ROBOGROUND",
            "name_full": "ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors",
            "brief_description": "An embodied manipulation system that uses pixel-level grounding masks from a grounded VLM as an intermediate representation to guide a language-conditioned policy; masks are integrated via channel concatenation and a mask-guided 'Grounded Perceiver' that biases patch-level attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ROBOGROUND",
            "model_description": "Two-stage pipeline: (1) a grounded vision-language model (based on GLaMM) ingests an image and a language instruction and outputs binary pixel masks for the target object and placement area using an LLM with a &lt;SEG&gt; token and a SAM-like decoder; (2) a language-conditioned transformer policy (GR-1 style) that encodes historical images with ViTMAE, concatenates predicted masks as extra image channels, and passes local patch features through a Grounded Perceiver whose attention is explicitly biased to mask regions before action decoding.",
            "visual_encoder_type": "CLIP vision encoder (for VLM input), ViTMAE encoder (for policy visual input), SAM encoder (initialized in grounding head decoder)",
            "visual_encoder_pretraining": "CLIP (used as image feature encoder in the VLM; pretrained on web image-text corpora as cited), ViTMAE (pretrained MAE-style vision encoder referenced), SAM (Segment Anything model encoder initialization). Exact dataset sizes for CLIP/ViTMAE are not specified in this paper beyond citations.",
            "grounding_mechanism": "Pixel-level segmentation via an LLM conditioned on projected CLIP visual tokens and the text instruction: a special &lt;SEG&gt; token's last-layer embeddings are projected into a SAM-like decoder to produce binary masks. Masks then guide the policy by (a) channel concatenation to the image input and (b) mask-guided attention in a grounded perceiver where attention entries over masked patch regions are filled with the current maximum to force focus.",
            "representation_level": "Pixel-level / region-level (binary segmentation masks for object and placement area)",
            "spatial_representation": "2D pixel masks and patch-level attention over a 14x14 patch grid; no explicit 3D coordinates or depth used (spatial guidance is provided by masks and patch indices)",
            "embodied_task_type": "Object manipulation (pick-and-place) and other manipulation primitives (open/close, press, turn/twist)",
            "embodied_task_name": "RoboCasa simulated manipulation tasks (pick-and-place variants and 22 atomic tasks from RoboCasa)",
            "visual_domain": "Photorealistic simulation (RoboCasa rendering with multiple camera views: left, right, hand view)",
            "performance_metric": "Contact rate (%) and success rate (%) for pick-and-place; success rate (%) for other atomic skills",
            "performance_value": "Pick-and-place (Ours): Easy 89.0% contact / 43.3% success; Appearance 78.5% / 30.5%; Spatial 81.0% / 33.5%; Common-sense 76.3% / 30.0. Other skills: Open/Close 72.0% success, Press 69.3% success, Turn/Twist 54.5% success. (Table 1)",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation (no channel concat, no grounded perceiver) baseline: Easy 68% contact / 24% success; Appearance 42% / 12%; Spatial 52% / 16%; Common-sense 40% / 12% (Table 4, first row).",
            "grounding_improvement": "Using both channel concatenation and grounded perceiver improves Easy from 68/24 to 86/42 (absolute +18% contact, +18% success); Appearance from 42/12 to 72/32 (+30% contact, +20% success); Spatial from 52/16 to 78/38 (+26% contact, +22% success); Common-sense from 40/12 to 70/32 (+30% contact, +20% success) (Table 4, differences between first and last rows).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "The paper compares grounding representation modalities (pixel masks vs. bounding boxes vs. center points and low-dimensional encodings): pixel masks give the best performance; lower-dimensional representations (points, bbox) are harder to learn and perform worse. For the grounded VLM, zero-shot mIoU = 13.2; fine-tuning on simulated grounding data raises mIoU to 45.5; fine-tuning on simulated + original VLM data yields 48.2 (Table 6).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify two perception-related bottlenecks: (1) the standard perceiver token-resampling can lose critical localized information about target regions, harming manipulation; (2) gap between contact and success rates suggests perception/localization alone is insufficient for grasp precision—mask localization helps contact but not reliably correct grasp pose across thousands of diverse objects.",
            "failure_mode_analysis": "Key failure modes related to grounding: (a) segmentation/grounding errors harm downstream success (VLM zero-shot mIoU 13.2 -&gt; after fine-tuning mIoU up to 48.2); (b) high contact but low success (e.g., Ours: Easy 89% contact vs 43.3% success) indicating grasping failures even when localization is correct; (c) limited placement-area diversity (noted as dataset limitation); (d) loss of information via token resampling in perceiver can cause missed local details — addressed by mask-biased attention. Quantitative frequencies are presented in metrics (contact vs success) but per-failure-type percentages beyond these are not enumerated.",
            "domain_shift_handling": "They fine-tune the grounded VLM on simulated instruction-following grounding data and combine simulated data with original VLM grounding data to preserve prior knowledge; this reduces domain mismatch and improves mIoU from 13.2 (zero-shot) to 48.2 (fine-tuned on sim + VLM data). Masks are computed once per episode (assumes single-turn static target positions) to reduce inference cost.",
            "novel_object_performance": "Zero-shot/unseen evaluation (Table 2): Unseen instance (no mask) 71.0% contact / 35.5% success; with predicted mask 85.8% / 42.5%. Unseen class (no mask) 53.0% / 17.5%; with predicted mask 79.5% / 27.3. This demonstrates substantial absolute improvements in both contact and success when masks are used for novel objects/classes.",
            "frozen_vs_finetuned": "For the grounded VLM, zero-shot (effectively 'frozen' prompt-only behavior) mIoU = 13.2; fine-tuning on simulated data raises mIoU to 45.5, and adding original VLM grounding data yields 48.2 (Table 6). The paper does not present a frozen vs fine-tuned comparison for the policy visual encoder (ViTMAE) explicitly.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Two fusion paths: (1) early/low-level channel concatenation of binary masks with RGB image followed by a linear projection and ViTMAE encoding; (2) mask-conditioned attention inside the perceiver where attention cells corresponding to masked patches are set to the maximum attention value to force queries to focus on masked regions. The VLM fuses visual CLIP features and text via projection into the LLM token space.",
            "sample_efficiency": "Paper provides dataset sizes (24K generated demonstrations with 112K instructions added to 66K RoboCasa demos), and ablations show mask guidance is most helpful when training includes the more complex generated data; no explicit sample-efficiency ratio (e.g., x vs y demos) is reported.",
            "key_findings_grounding": "Pixel-level grounding masks from a grounded VLM (fine-tuned on simulated instruction-following data) are an effective intermediate representation for robotic manipulation: they substantially improve generalization to unseen instances and classes, outperform simple low-dimensional grounding (points/bboxes), and mitigate information loss from token resampling when integrated via a mask-biased perceiver; however, masks mainly improve localization/contact and do not fully solve grasping precision, indicating perception grounding is necessary but not sufficient for high success rates across highly diverse object sets.",
            "uuid": "e1943.0"
        },
        {
            "name_short": "GLaMM",
            "name_full": "GLaMM (Pixel grounding large multimodal model as used in ROBOGROUND)",
            "brief_description": "A pixel-level grounded vision-language model used to generate binary segmentation masks for instruction-specified target objects and placement areas; in this paper GLaMM is fine-tuned on simulated instruction-following segmentation data to improve mask accuracy for manipulation tasks.",
            "citation_title": "Glamm: Pixel grounding large multimodal model",
            "mention_or_use": "use",
            "model_name": "GLaMM (fine-tuned variant)",
            "model_description": "LLM-based grounded VLM that encodes an image with CLIP vision features projected into the LLM token space (256 token prompt representation), integrates the text instruction, and exposes a special &lt;SEG&gt; token whose embeddings are projected into a SAM-like pixel decoder (encoder initialized from SAM) to produce binary segmentation masks per &lt;SEG&gt; token for object and placement region.",
            "visual_encoder_type": "CLIP vision encoder (used to produce visual tokens for the LLM prompt), SAM encoder used in pixel grounding head, and a SAM-like decoder for mask generation",
            "visual_encoder_pretraining": "GLaMM is initialized from a checkpoint pre-trained on the 'Grounding-anything' dataset (paper states this dataset contains 7.5M unique concepts spanning 810M regions); CLIP provides the image features used as LLM input (CLIP pretraining on web image-text is cited).",
            "grounding_mechanism": "LLM integrates projected CLIP visual tokens and instruction text; &lt;SEG&gt; token embeddings are projected into a SAM-like decoder to produce per-token binary segmentation masks (pixel-level grounding).",
            "representation_level": "Pixel-level segmentation masks",
            "spatial_representation": "2D pixel masks; patch-aware decoding via SAM-like decoder; no explicit 3D geometry reported",
            "embodied_task_type": "Provides grounding masks for object manipulation tasks (pick-and-place) used by the policy",
            "embodied_task_name": "Mask generation for RoboCasa simulated tasks",
            "visual_domain": "Image observations from photorealistic RoboCasa simulation",
            "performance_metric": "mIoU (mean Intersection over Union) for predicted masks against ground-truth simulated masks",
            "performance_value": "VLM mIoU: Zero-shot 13.2; Fine-tuned with simulated grounding data 45.5; Fine-tuned with simulated data + original VLM data 48.2 (Table 6).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Zero-shot mIoU 13.2 (i.e., without fine-tuning on simulation instruction-following data).",
            "grounding_improvement": "Fine-tuning on simulated data increases mIoU from 13.2 to 45.5 (+32.3 mIoU absolute); adding original VLM data yields 48.2 (+34.9 absolute from zero-shot) (Table 6).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper evaluates the effect of fine-tuning GLaMM with different grounding data sources (zero-shot prompting vs. sim-only vs. sim + original VLM data), showing large improvements in mIoU from fine-tuning; no multiple CLIP variants are compared.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Zero-shot grounding is inadequate (low mIoU) for precise manipulation; fine-tuning on simulated instruction-following masks is required to reach practical segmentation quality for downstream policy use.",
            "failure_mode_analysis": "Zero-shot prompting via dependency-parsed nouns produced ambiguous mask outputs and poor mIoU (13.2). Ambiguity in instruction parsing and lack of instruction-following fine-tuning are primary causes of low mask quality before fine-tuning.",
            "domain_shift_handling": "Mitigated by fine-tuning GLaMM on a simulation-based instruction-following dataset and by combining simulated grounding examples with original VLM grounding data to preserve prior knowledge (improves mIoU).",
            "novel_object_performance": "After fine-tuning, masks enable better zero-shot policy generalization to unseen instances and classes (policy-level numbers reported in Table 2); VLM mask quality improvements correlate with downstream gains but exact per-object mIoU breakdown for novel classes is not tabulated.",
            "frozen_vs_finetuned": "Fine-tuning yields large gains in mIoU relative to zero-shot prompting (13.2 -&gt; 45.5 -&gt; 48.2), indicating substantial benefit from instruction-following fine-tuning.",
            "pretraining_scale_effect": "GLaMM base checkpoint was trained on a very large grounding dataset (Grounding-anything: 7.5M unique concepts / 810M regions as cited); paper leverages that pretraining but does not perform explicit scaling studies.",
            "fusion_mechanism": "Visual CLIP features are projected into the LLM embedding space and combined with text tokens; a &lt;SEG&gt; token in LLM output is then decoded to a pixel decoder for segmentation.",
            "sample_efficiency": "Fine-tuning on simulated instruction-following data (size not individually enumerated beyond overall dataset) greatly improves mIoU; exact sample-efficiency ratios are not reported.",
            "key_findings_grounding": "A pretrained pixel-grounding VLM (GLaMM) requires instruction-following fine-tuning on simulation-ground-truth masks to produce masks of sufficient quality for downstream manipulation; fine-tuning using both sim data and original grounding data best preserves prior knowledge and maximizes mIoU.",
            "uuid": "e1943.1"
        },
        {
            "name_short": "Grounded Perceiver",
            "name_full": "Grounded Perceiver (mask-guided perceiver for policy token reduction)",
            "brief_description": "A modified perceiver module that augments global learnable queries with object and placement-area query groups whose attention is explicitly biased towards mask-covered image patches, reducing token loss of task-relevant local information during token resampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Grounded Perceiver",
            "model_description": "Perceiver-style token resampler taking 14x14 patch features and three sets of k learnable query tokens: global queries Qg, object queries Qo, placement queries Qp. During attention computation the attention weights for entries corresponding to mask-covered patches (Mo for object queries, Mp for placement queries) are replaced with the current maximum attention value to enforce focus on masked regions; the final perceived visual features are concatenation of outputs from all three query groups.",
            "visual_encoder_type": "Operates on ViTMAE patch features (14x14 grid) produced earlier in the policy pipeline",
            "visual_encoder_pretraining": "ViTMAE (pretrained MAE-style encoder as cited), used to produce patch features; Grounded Perceiver itself is a learned module trained from scratch with the policy",
            "grounding_mechanism": "Mask-guided attention: during cross-attention between query tokens and patch features, attention entries corresponding to mask regions are set to the maximum attention value so object and placement queries attend strongly to masked patches.",
            "representation_level": "Patch-level (operates over 14x14 patch features derived from ViTMAE), guided by pixel-level masks",
            "spatial_representation": "Uses 2D patch grid indices aligned with masks; no explicit 3D geometry",
            "embodied_task_type": "Component in policies for object manipulation (pick-and-place) and other manipulation primitives",
            "embodied_task_name": "Used within the ROBOGROUND policy evaluated on RoboCasa tasks",
            "visual_domain": "Simulation images from RoboCasa (multi-view inputs)",
            "performance_metric": "Downstream contact/success rates for pick-and-place tasks",
            "performance_value": "Inclusion of Grounded Perceiver (with channel concat) yields e.g., Easy 86%/42% compared to 78%/36% (channel concat only) and 68%/24% (no mask integration) — see Table 4. Combined with mask channel concat it achieves the best reported ablation results.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without mask-guided perceiver (but possibly with channel concatenation) performance is lower (e.g., channel concat only: Easy 78%/36 vs channel concat + Grounded Perceiver 86%/42). Without any mask integration performance is 68%/24 (Table 4).",
            "grounding_improvement": "Adding Grounded Perceiver on top of channel concatenation yields modest further gains: channel concat only 78/36 -&gt; +8/6 to 86/42 when Grounded Perceiver is added (Easy case). For harder reasoning types improvements are larger (e.g., Appearance 60/26 -&gt; 72/32 with both).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Perceiver token resampling can drop critical localized information; grounded perceiver mitigates this by explicitly forcing attention to mask regions so object- and placement-specific tokens retain important local features.",
            "failure_mode_analysis": "Without grounded perceiver, policies may miss fine-grained local details needed for manipulation; grounded perceiver reduces but does not eliminate downstream grasping failures.",
            "domain_shift_handling": null,
            "novel_object_performance": "When used with predicted masks, grounded perceiver contributes to the improved zero-shot/unseen-instance performance shown in Table 2 (e.g., unseen instance with mask 85.8/42.5).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Patch-level cross-attention between query groups and patch features with attention values for mask-covered patches replaced by the max attention value to bias focus.",
            "sample_efficiency": null,
            "key_findings_grounding": "Biasing perceiver attention using segmentation masks preserves task-relevant local patch information that standard token-resampling perceivers may discard, improving downstream manipulation success especially on harder reasoning tasks.",
            "uuid": "e1943.2"
        },
        {
            "name_short": "Mask Representation (vs bbox/point)",
            "name_full": "Pixel-level Mask Grounding Representation",
            "brief_description": "Use of binary pixel masks for object and placement area grounding, compared to lower-dimensional representations (center point or 2D bounding box) and low-dimensional encodings; masks capture object shape/size and provide richer spatial guidance for policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mask-based grounding (representation)",
            "model_description": "Pixel-level binary masks produced by the grounded VLM (GLaMM) are used as (a) an extra image channel concatenated to RGB inputs and (b) inputs to mask-guided perceiver attention. Alternative representations (center point, 2D bbox, or low-dim encodings) were evaluated in ablations.",
            "visual_encoder_type": "N/A (representation level)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Pixel mask outputs from VLM; masks used for early-fusion channel concatenation and for biasing attention in perceiver.",
            "representation_level": "Pixel-level (binary segmentation mask) vs low-dimensional (center point, bbox)",
            "spatial_representation": "2D pixel masks encoding spatial extent and shape",
            "embodied_task_type": "Object manipulation (pick-and-place) and manipulation primitives",
            "embodied_task_name": "RoboCasa pick-and-place and atomic tasks used in paper",
            "visual_domain": "Simulation (RoboCasa)",
            "performance_metric": "Contact / success rates; mIoU for mask quality",
            "performance_value": "Ablations (Table 5 summary): masks give best downstream performance; bounding boxes and points as low-dimensional inputs result in lower policy performance. Specific numbers are reported qualitatively: masks provide modest improvements over bbox; low-dim encodings are more challenging to learn.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Using points or bbox as low-dimensional inputs yields lower performance than masks (exact per-method numbers are in Table 5).",
            "grounding_improvement": "Mask-based grounding outperforms bbox/points; masks better capture object shape/size and yield improved success/contact rates (absolute differences are task-dependent as shown in the ablation table).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Low-dimensional grounding loses shape/size cues and spatial extent, making learning downstream policies more difficult; masks alleviate this by preserving richer spatial information.",
            "failure_mode_analysis": "Policies relying on low-dimensional grounding (point/bbox) struggle more in cluttered scenes and discriminative appearance/spatial reasoning tasks compared to mask-based policies.",
            "domain_shift_handling": null,
            "novel_object_performance": "Mask guidance substantially improves generalization to unseen instances and classes compared to non-mask baselines (Table 2 shows large relative improvements).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Channel concatenation (masks added as image channels) and mask-driven attention in perceiver.",
            "sample_efficiency": null,
            "key_findings_grounding": "Pixel-level masks are the most effective grounding representation evaluated: they give richer spatial cues (shape and size) that materially improve policy generalization in diverse, cluttered simulated scenes compared to bounding boxes or center points.",
            "uuid": "e1943.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Glamm: Pixel grounding large multimodal model",
            "rating": 2
        },
        {
            "paper_title": "Segment anything",
            "rating": 2
        },
        {
            "paper_title": "GR-1",
            "rating": 1
        },
        {
            "paper_title": "RoboCasa: Large-scale simulation of everyday tasks for generalist robots",
            "rating": 2
        },
        {
            "paper_title": "Anygrasp: Robust and efficient grasp perception in spatial and temporal domains",
            "rating": 1
        }
    ],
    "cost": 0.01920225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors
30 Apr 2025</p>
<p>Haifeng Huang huanghaifeng@zju.edu.cn 
Zhejiang University</p>
<p>Shanghai AI Laboratory</p>
<p>Xinyi Chen 
Shanghai AI Laboratory</p>
<p>Yilun Chen 
‡ Hao Li 
Shanghai AI Laboratory</p>
<p>Xiaoshen Han 
Shanghai AI Laboratory</p>
<p>Zehan Wang 
Zhejiang University</p>
<p>Shanghai AI Laboratory</p>
<p>Tai Wang 
Shanghai AI Laboratory</p>
<p>Jiangmiao Pang 
Shanghai AI Laboratory</p>
<p>Zhou Zhao 
Zhejiang University</p>
<p>Shanghai AI Laboratory</p>
<p>Shanghai AI Laboratory</p>
<p>ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors
30 Apr 202568C2AAFAAAE2E01DB735BDEAA9E91D80arXiv:2504.21530v1[cs.RO]
Recent advancements in robotic manipulation have highlighted the potential of intermediate representations for improving policy generalization.In this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by largescale vision-language models pretrained on diverse grounding datasets.We introduce ROBOGROUND, a groundingaware robotic manipulation system that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks.To further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions.Extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies.Code and data are available at robo-ground.github.io.</p>
<p>Introduction</p>
<p>The early low-level policies for manipulation [8,25,38,50] primarily relied on imitation learning from collected demonstrations, focusing on acquiring specific skills within predefined scenes.Consequently, these policies exhibited limited generalization capabilities.More recently, Vision-Language-Action (VLA) models [5,19] have emerged as a promising approach for learning generalizable robotic policies.These models leverage large-scale robot training data alongside pretrained Vision-Language Models (VLMs) [23,32,51] to develop broad manipulation capabilities.However, it is still challenging for these methods to generalize Examples of generated data and mask guidance for manipulation policy.The generated data includes more object distractors in the scene, leading to higher scene complexity.The robot policy is guided by masks to localize the target object and placement area.</p>
<p>to novel settings without access to extensive datasets and additional fine-tuning, both of which are costly.</p>
<p>Intermediate representations [2,3,15,37] offer an alternative approach to generalized policy learning by providing structured guidance to policy networks across diverse scenarios.Research in this area typically falls into two categories: accessible yet coarse-grained representations, such as language instructions [2,49], which are easy to generate but often lack the spatial precision required for finegrained object manipulation; and fine-grained but resource-intensive representations, such as goal images [4] or point flows [3,12,33,45,47], which provide detailed spatial guidance but demand extensive training data and computational resources, limiting their scalability.</p>
<p>In this work, we introduce grounding masks as a promising intermediate representation that balances two key aspects: (1) Effective spatial guidance, which not only specifies target objects and placement areas but also conveys information about object shape and size, allowing low-level policies to accurately interpret spatial information; and (2) Broad generalization potential, facilitated by large-scale vision-language models [21,32,44,48] pretrained on diverse grounding datasets, enhancing adaptability to novel objects and environments.To fully explore grounding mask representations for low-level policy networks, we propose ROBOGROUND, a grounding-aware system guided by object masks for robotic manipulation.This system employs a grounded vision-language model to generate masks for both the target object and placement area, which are then seamlessly integrated into the policy network through channellevel and patch-level designs.Specifically, we enhance image inputs via channel concatenation with the masks and introduce a grounded perceiver that attends to mask-guided regions at the patch level, preserving essential spatial information for precise manipulation.</p>
<p>As shown in Figure 1, existing datasets [10,[26][27][28]40] often suffer from limited instruction diversity and scene complexity, leading policy networks to overfit to specific conditions and straightforward instruction-scene mappings.In this work, we aim to investigate the effectiveness of grounding mask representations in maintaining consistent performance across diverse scenarios and instructions.To address dataset limitations, we propose an automated pipeline for generating simulated manipulation data with a diverse set of objects and instructions.By systematically increasing scene complexity through the inclusion of distractor objects, we create a challenging dataset that promotes generalization across varied instructions and object configurations.Specifically, we generate 24K demonstrations with 112K diverse instructions, covering object appearance, spatial relationships, and commonsense knowledge.Scene complexity is further enhanced by sampling distractor objects from a large pool of 3,526 objects spanning 176 categories.This high-complexity dataset encourages models to leverage intermediate object mask guidance for interpreting instruction-scene relationships, providing a rigorous evaluation of our mask-guided model's generalization potential.</p>
<p>We conduct extensive experiments to evaluate the model's generalization across diverse instructions, unseen objects and categories, and core robotic skills.Our results highlight the effectiveness of grounding masks as intermediate guidance, showing that they substantially enhance the generalization capabilities of robot policies while demonstrating the value of our dataset.</p>
<p>Related Work</p>
<p>Intermediate Representations for Robot Policy.Various intermediate guidance strategies have been explored to enhance policy learning, including language instructions [2,49], 2D trajectories [15,39,42], point flows [3,12,33,45,47], goal images [4,6], goal sketches [37], and objectcentric representations [14,35,36].For instance, RT-Trajectory [15] utilizes rough trajectory sketches to guide policies in performing new tasks, while Im2Flow2Act [45] predicts object flows and maps them to actions.However, acquiring such intermediate guidance during evaluation is often impractical due to the lack of largescale, diverse datasets and generalizable foundation models.Some approaches leverage object-centric representations to enhance policy robustness.KITE [36] decomposes tasks into keypoints using an additional grounding module, while MOKA [14] employs prompt engineering to derive keypoints that effectively capture affordances in manipulation tasks.Closely related to our mask-based approach, MOO [35] uses pretrained Vision-Language Models (VLMs) to generate coarse bounding boxes, which are then overlaid onto the original image.In contrast, our method focuses on obtaining fine-grained object masks and introduces an efficient Grounded Perceiver to better utilize mask-based intermediate guidance for improved manipulation performance.Large Vision-Language Models.Large Vision-Language Models (LVLMs) have recently demonstrated remarkable capabilities in image understanding and image-to-text generation, as seen in BLIP-2 [22], LLaVA [23], Instruct-BLIP [9], and .While these models excel at text generation and holistic image comprehension, they often struggle with fine-grained understanding of local regions within images.To address this, several methods [7,29,41,46] enhance region-level perception by incorporating bounding boxes and spatial location bins, allowing LMMs to focus on specific interactive areas.Furthermore, recent approaches [21,24,32,44,48] have advanced grounding capabilities, enabling the identification of target objects or regions to support downstream tasks.Our method builds upon this progress by leveraging GLaMM [32] to generate grounding masks for both target objects and placement areas, providing structured guidance for the low-level policy network in robotic manipulation.</p>
<p>Data Generation</p>
<p>Existing language-conditioned simulation datasets often suffer from limited object and environment diversity [18,27,34] or lack a broad range of instructions and com- Pick the object between the carrot and the canned food.</p>
<p>GPT 4o</p>
<p>Object Features GPT 4o plex scenes [26,28], making them less effective for training robust robotic policies and evaluating model generalization.To address these limitations, we introduce an automated data generation pipeline built upon RoboCasa [28], designed to enhance object variety, instruction diversity, and scene complexity for more challenging manipulation tasks.Our approach produces 24K new demonstrations paired with 112K diverse instructions, covering 176 distinct object categories and a total of 3,526 unique objects.</p>
<p>Object Set</p>
<p>A diverse object set is essential for increasing scene complexity and enabling varied instruction generation.Expanding on the original RoboCasa dataset, we curated additional 1,017 high-quality tabletop manipulation objects from Objaverse [11], selected from a pool of over 760K items.The initial filtering process, supported by GPT-4 [1], leveraged object tags and descriptions to identify (1) tabletop-suitable items, (2) kitchen-related objects, and (3) exclude multiitem collections.GPT-4 further assisted in categorizing objects based on their characteristics.The final selection underwent manual review to ensure quality.By incorporating distractor objects from this extensive set, we create complex scenes that challenge models to accurately interpret instructions in diverse and dynamic environments.</p>
<p>Diverse Instructions</p>
<p>Existing language-conditioned datasets often rely on fixedformat instructions that require minimal reasoning, leading models to depend on simple mappings between instructions and tasks rather than developing a deeper understanding of the language's semantics.This can hinder the model's ability to generalize to novel objects or tasks.To address this limitation, we propose designing instructions that focus on discriminative understanding of object appearance, spatial relationships, and commonsense knowledge, as illustrated in Figure 2. We present a target object alongside up to 10 contextually relevant objects (distractors) in the scene, encouraging the model to make precise distinctions between the target and surrounding objects.</p>
<p>Appearance-Based Instructions.To enable detailed visual understanding, we render each object from four perspectives (front, back, left, and right) and combine these views into a composite image that offers a comprehensive visual overview.This image is then processed by GPT-4, which extracts key attributes for each object-such as class, color, shape, and material-both as keywords and full descriptive phrases.When selecting a target object, we randomly assign one attribute as its unique characteristic within the scene.Objects with overlapping keywords for this attribute are filtered out, resulting in a refined set</p>
<p>Pick the gray object from the counter and place it in the cabinet.</p>
<p>Large Language Model</p>
<p>Transformer Decoder</p>
<p>The target object is the rectangular object <seg>.</p>
<p>The target placement area is the cabinet <seg>. of valid objects.From this set, distractors are sampled by ranking them based on the similarity of their textual and visual embeddings, derived from CLIP [31] text encoder and image encoder, encoded using the full attribute descriptions and multi-view images, respectively.</p>
<p>Image Encoder</p>
<p>Grounded Perceiver</p>
<p>Appearance-Based Instructions.To enable detailed visual understanding, we render each object from four perspectives (front, back, left, and right) and combine these views into a composite image that provides a comprehensive visual overview.This image is then processed by GPT-4, which extracts key attributes for each object-such as class, color, shape, and material-both as keywords and full descriptive phrases.When selecting a target object, we randomly assign one attribute as its unique characteristic within the scene.Objects with overlapping keywords for this attribute are filtered out, resulting in a refined set of valid objects.From this set, distractors are sampled by ranking them based on the similarity of their textual and visual embeddings, derived from the CLIP [31] text and image encoders, using the full attribute descriptions and multi-view images, respectively.Spatial Instructions.For randomly generated scenarios specifying a target object and its desired location, we employ a rule-based approach to create instructions that require spatial understanding.First, we extract the coordinates of all objects in the robot's frame.We then generate spatial instructions by identifying the object(s) closest to the target object in a specific direction, ensuring that no other objects of the same type are present in the scene and that the angular deviation in other directions does not exceed 30 degrees.This method produces clear and unambiguous spatial instructions.Commonsense Instructions.We leverage GPT-4 to generate diverse instructions grounded in commonsense reasoning.For each task, we provide images from three perspectives (robot's left view, right view, and hand view), along with details about the target object and its intended position.Instead of explicitly mentioning the target object, GPT-4 generates pick-and-place task instructions framed within a familiar daily scenario.We prompt GPT-4 to ensure that the scenario is uniquely tailored to the target object, reinforcing the need for commonsense understanding.</p>
<p>Method</p>
<p>Overview</p>
<p>Our motivation is to leverage grounding masks as an intermediate representation to enhance the generalization ability of robotic manipulation policies.This requires a largescale, pre-trained vision-language model capable of generating grounded information about target objects and placement areas, referred to as the Grounded Vision-Language Model, discussed in Section 4.2.We then incorporate this grounding knowledge into the low-level policy network, where the grounded masks function as both an attention mechanism within the Grounded Perceiver and an additional input channel for vision data, as detailed in Section 4.3.Finally, we outline the training and evaluation procedures for the complete framework in Section 4.4.</p>
<p>Grounded Vision-Language Model</p>
<p>Recent advances in large vision-language models [21,32,44,48] have enabled generalizable, object-centric ground-ing.Our approach builds on GLaMM [32], a state-of-theart model that generates pixel-level segmentation masks, to guide the low-level policy network in accurately localizing target objects and placement areas.Base Model.The grounded vision-language model takes an image observation and a language instruction as input and outputs binary masks for target objects and/or target placement areas specified by the instruction, as shown in Figure 3(a).Given an image x v and a text instruction x t , the image is encoded by the CLIP vision encoder [31] to obtain the visual feature F v = CLIP(x v ).This feature is then projected into the LLM's embedding space via an MLP projector
f v (•), yielding f v (F v ) ∈ R 256×Dt .
The LLM L(•) integrates both the projected visual features and the text instruction to generate the output y t :
y t = L(f v (CLIP(x v )), x t ).
(
)1
The model perceives the image through a prompt formatted as: "The <IMAGE> provides an overview of the picture," where the <IMAGE> token is replaced by the projected visual feature, represented as a sequence of 256 tokens.</p>
<p>Pixel-level Grounding.To achieve pixel-level grounding, we employ a fine-grained image encoder E(•) and a pixel decoder D(•) within the grounding head.The encoder is initialized with a pre-trained SAM [20] encoder, while the decoder follows a SAM decoder-like architecture.A special token <SEG> is introduced into the LLM's vocabulary to extract grounding-related features.The last-layer embeddings F seg corresponding to the <SEG> token are projected into the decoder's feature space using a projector f s .The binary segmentation mask M is then generated as follows:
M = D (f s (F seg ), E(x v )) .(2)
For robotic manipulation tasks, the model is prompted to segment both the target object and the placement area (if applicable).As illustrated in Figure 3(a), the model's output specifies these regions using separate <SEG> tokens.Each <SEG> token is decoded into an individual mask, yielding the target object mask M o and the placement area mask M p , which are subsequently provided to the Grounded Policy Network.</p>
<p>Grounded Policy Network</p>
<p>After the grounded VLM translates the language instruction into masks for target objects and placement areas, these masks provide useful spatial guidance for the robot's policy.Rather than requiring explicit grounding of semantic descriptions, the policy can focus on leveraging this structured information to improve object localization and action execution.Given the strong generalization ability of the grounded VLM, the masks serve as a bridge between high-level language instructions and low-level manipulation strategies, helping the policy adapt to novel objects and environments more effectively.Base Model.For the policy network, we employ a language-conditioned transformer architecture, following the GR-1 model [43].As shown in Figure 3(b), this model processes a sequence of historical image observations, robot states and a language instruction as input to predict future robot actions.Specifically, for each input image x v , along with corresponding masks M o and M p , we apply channel concatenation and project the combined channels back to a size of 3 using a linear layer.The result is then fed into a pre-trained ViTMAE encoder [16].The encoded visual feature Z v ∈ R 197×Dv is computed as follows:
Z v = ViTMAE(Linear(Concat(x v , M o , M p ))),(3)
where D v denotes the hidden dimension of the vision encoder.The encoded feature Z v consists of a global representation Z CLS v ∈ R 1×Dv , obtained from the CLS token, and a set of local patch representations Z P v ∈ R 196×Dv , corresponding to a 14×14 spatial grid.These patch features are then processed by the grounded perceiver, P(•), which interacts with the grounding masks to reduce the number of tokens, ultimately producing the refined visual token features P(Z P v ).Additionally, the input language instruction x t is encoded using the CLIP text encoder, yielding the text token feature Z t .The input robot state x s is projected through an MLP to obtain the state token feature Z s .To enable action prediction, a learnable ACT token with feature Z a is also included.Consequently, the token sequence for a single timestep input is structured as follows:
Z CLS v , P(Z P v ), Z t , Z s , Z a .(4)
For a history length of N , the complete token sequence for one forward pass is constructed by aggregating token features from the past N timesteps.This sequence is then processed by a transformer decoder, which predicts the nextstep action tokens through the output <ACT> tokens.</p>
<p>Grounded Perceiver A standard perceiver model [17], used in frameworks like GR-1, functions as a token resampler, reducing the number of tokens derived from the initial visual features.This is achieved through iterative attention layers between a small set of learnable query tokens and the original visual features.However, this token resampling process may lead to information loss, potentially limiting policy learning by failing to capture critical details about the target objects and placement areas.To address this, we propose guiding attention toward regions defined by grounded masks, ensuring that essential information is preserved for effective manipulation.</p>
<p>The perceiver takes as input the 14×14 patch features, denoted as Z P v , extracted from the vision encoder, along with a set of learnable query tokens, Q g ∈ R k×Dp , where k is the number of query tokens and D p is the token dimension.These initial queries, referred to as global queries, capture information from the entire image.To integrate grounded masks, we introduce two additional sets of query tokens: Q o ∈ R k×Dp for the target object and Q p ∈ R k×Dp for the target placement area.During each attention layer, these additional query tokens interact with the patch features, with attention guided by the respective masks, M o and M p , applied via a mask fill operation (see supplementary material for details).This process produces the final set of perceived visual features, P(Z P v ), with a total token length of 3 × k.</p>
<p>Training and Inference</p>
<p>VLM Fine-tuning.Although GLaMM was pre-trained on a large-scale grounding dataset, it may not precisely follow the prompt to identify the target object and placement area from a given manipulation instruction in a zero-shot setting.To address this, we construct an instruction-following dataset based on the generated simulation data to fine-tune the VLM.The grounded VLM model is fine-tuned using two types of losses: auto-regressive cross-entropy loss for text generation, and a linear combination of per-pixel binary cross-entropy loss and DICE loss for segmentation.Policy Training.In each forward pass, the policy network receives image observations, robot states over N consecutive timesteps, and the corresponding language instruction.Based on this input, the network generates N action tokens, which are processed through linear layers to predict arm and gripper actions.Since arm actions are continuous, we use Smooth-L1 loss L arm for optimization.For binary gripper actions, we apply Binary Cross Entropy (BCE) loss L gripper .Thus, the total training loss for the policy network is: L total = L arm + L gripper .Inference.The inference process starts by prompting the grounded VLM to generate segmentation masks for the target objects and placement areas.These initial masks are provided to the grounded policy network to predict the next action tokens.To optimize inference time, segmentation masks are extracted from the grounded VLM only once, at the beginning of the episode.These masks are then used throughout the entire manipulation task as inputs to the policy network.This approach is well-suited for single-turn manipulation tasks, where the target object's position remains stable until the gripper interacts with it, and the placement area remains fixed.Thus, the initial masks provide consistent guidance for the policy network throughout the task.</p>
<p>For multi-view image observations, each view is processed independently by the grounded VLM to obtain the corresponding masks, allowing for parallel processing.In the grounded policy network, each image and its corre-sponding masks are separately passed through the vision encoder and grounded perceiver to generate visual token features.During each forward pass of the policy network, only the action for the next timestep is predicted.</p>
<p>Experiments</p>
<p>Simulation Setting</p>
<p>Our simulation environment is built upon RoboCasa [28], a large-scale framework offering an automated scene generation pipeline.We leverage this pipeline to create diverse scene layouts and textures for data collection.Our main experiments are conducted within the RoboCasa environment, which includes a range of manipulation configurations.We classify the original RoboCasa data as "Easy," due to its relatively simple scene complexity and straightforward instructions.To introduce more complexity, we generate challenging pick-and-place data that requires advanced semantic understanding and reasoning.The instructions are divided into three categories: "Appearance," "Spatial," and "Common-sense," each representing different types of taskspecific knowledge.Additionally, we include fundamental manipulation skills, such as opening and closing doors or drawers, pressing buttons, turning levers, and twisting knobs, to further challenge the model.In these tasks, target masks (e.g., for a drawer handle) are also generated to guide the robot's policy in precise manipulation.</p>
<p>Main Results</p>
<p>To evaluate our approach, we compare it with three wellestablished, easy-to-implement methods as baselines.Each method is trained on the same dataset, which includes RoboCasa's 66K demonstrations for foundational skills, alongside our proposed dataset containing 24K demonstrations and 112K instructions with increased diversity and complexity.For evaluation, we generate 400 test samples for each instruction type within the pick-and-place task and for each fundamental skill.The results are presented in Table 1.Metrics for pick-and-place tasks are reported as "a / b", where a is the contact rate (%) and b is the success rate (%).The contact rate refers to the percentage of attempts where the gripper makes contact with the target object, while the success rate indicates the percentage of tasks completed successfully.To ensure a fair comparison, we standardize key settings across methods, including history length, input image views, and image sizes.RoboMimic, which integrates language input by encoding it with CLIP and combining it with vision representations through FiLM [30] layers.• GR-1 [43]: A GPT-style model designed for languageconditioned visual robotic manipulation.The original GR-1 model was pre-trained on a large video dataset and predicts both robot actions and future images.Since the pre-trained model is unavailable, we reproduce it here without large-scale pre-training or image prediction.Analysis.Compared to baseline models, our method consistently outperforms across all tasks.ACT generally performs poorly due to the absence of language input, which is crucial for task-specific language comprehension and reasoning.While BC-Transformer and GR-1 benefit from language input, their performance on more challenging tasks remains relatively low.This limitation likely arises from design shortcomings, as these models encode language input as a single, global text feature, which is inadequate for the nuanced understanding and reasoning required by semantically rich instructions.</p>
<p>Our approach, which leverages grounding masks to guide the policy, shows substantial improvements in challenging tasks, highlighting the effectiveness of incorporating such masks.Interestingly, we observe a consistent gap between the success rate and the contact rate, with the latter being significantly higher.This discrepancy suggests that the model's grasping capability could be further refined.We attribute this to the diverse range of objects in the dataset, which makes learning accurate grasping poses more diffi-cult.Integrating pose-prediction networks, such as Any-Grasp [13], could potentially help address this challenge.Nonetheless, the high contact rate demonstrates the effectiveness of our method.</p>
<p>Zero-shot Evaluation.</p>
<p>For zero-shot evaluation, we categorize unseen settings into two levels: unseen instance and unseen class.Unseen instances refer to evaluation on new objects within classes present in the training data, while unseen classes involve evaluation on objects from entirely new classes not included in the training data.We exclude 30 classes, comprising 597 objects, as unseen and filter out 1/4 of the objects from each of the remaining classes, resulting in a total of 1,335 unseen instances.For each unseen setting, we generate 400 test samples for each reasoning type within the pick-and-place task.The evaluation results are presented in Table 2.</p>
<p>Analysis.The experimental results on both unseen instances and unseen classes show that incorporating mask information significantly enhances the model's generalization to novel settings, while maintaining strong performance on previously seen tasks.This improved generalization is reflected in better performance across new data distributions.Notably, in more challenging scenarios, mask guidance achieves approximately 100% relative improvement over non-mask baselines, highlighting its crucial role in handling complex, unseen situations.</p>
<p>Ablation Study.</p>
<p>To accelerate our ablation study, we use a subset of the dataset, as full training and evaluation would require several days on 8 NVIDIA 4090 GPUs.This subset includes 3K original samples and 3K generated reasoning samples for a single pick-and-place scenario in RoboCasa.For evaluation, we generate 50 test samples per instruction type while maintaining the original diversity in scenes and objects to ensure representative and meaningful results.Training Data and Mask.We perform ablation studies by training models with different datasets and mask configurations.The results, shown in Table 3, indicate that when trained solely on the original simple data, the model eas-  ily fits the "Easy" set-even without a mask-but struggles with more challenging sets.This supports our hypothesis that the limited complexity of the original data restricts the model's generalization ability.In contrast, training on our new dataset highlights the critical role of mask guidance in improving generalization.Furthermore, combining both original and new data enhances performance, suggesting that scaling the dataset could further strengthen groundingaware policies.</p>
<p>Modules for Incorporating Masks.We evaluate two methods for integrating grounding information: concatenating masks as an additional channel and using the grounded perceiver.As shown in Table 4, both approaches effectively incorporate grounding knowledge.While simple mask concatenation allows the model to utilize mask information, the grounded perceiver enables a more comprehensive exploitation of mask features, resulting in improved performance.Grounding Representations.</p>
<p>We evaluate different grounding representations and their impact on model performance.As shown in Table 5, models perform best when using masks as grounding representations.In contrast, lower-dimensional vector representations, such as points and bounding boxes, pose greater challenges for effective learning.Masks provide richer information by accurately capturing the shape and size of the target object, leading to modest improvements over bounding box representations.Grounded VLM We fine-tune the VLM to improve its Table 5. Ablation Study on Grounding Representations."Point" and "Bbox" denote the center pixel point and the 2D bounding box extracted from predicted masks, respectively."Lowdim" refers to using the point or bbox as a low-dimensional input, while "Image" indicates incorporating them as an additional channel in the image input.</p>
<p>Representation</p>
<p>Method mIoU</p>
<p>Zero-shot 13.2 Fine-tuned w/ Sim.data 45.5 Fine-tuned w/ Sim.data &amp; VLM data 48.2 instruction-following capability by training it on both simulated and original VLM data.Specifically, we create an instruction-following dataset based on simulated data using the following prompt format: "Given a robotic manipulation instruction: <Instruction>, identify the target object for manipulation and, if applicable, the target placement area."The response is formulated as "The target object is xxx <SEG>" to generate the target object mask, and "The target placement area is xxx <SEG>" if a placement area is specified.</p>
<p>For evaluation, we compute the mean Intersection over Union (mIoU) of the predicted simulation results, with a performance comparison shown in Table 6.The initial zeroshot evaluation uses a dependency parsing tool to identify target nouns in the instructions, which are then used to prompt the VLM to generate a mask for each noun.However, this approach leads to suboptimal results due to ambiguities introduced by the dependency parsing.Fine-tuning on simulation data alone significantly improves results but risks losing the knowledge embedded in the original VLM dataset.To mitigate this, we fine-tune the model using both simulated data and the VLM's original grounding data, thereby enhancing simulation performance while preserving the model's prior knowledge.For the grounded VLM, we fine-tune GLaMM [32] starting from its publicly available checkpoint, pre-trained on the Grounding-anything Dataset, which contains 7.5M unique concepts spanning 810M regions.The fine-tuning process adopts an instruction-following approach to enable grounded conversational capabilities.For the grounded policy network, we re-implement the model architecture of GR-1 [43], omitting the image prediction head due to the unavailability of their video dataset for pre-training.Instead, we train the model from scratch using our simulation data.The training employs a base learning rate of 5e-4 with a cosine annealing schedule, a batch size of 32, and spans 5 epochs.Full training takes approximately 70 hours on 8 NVIDIA RTX 4090 GPUs.For faster experimentation during ablation studies, we use a subset of the data, reducing the training time to 5 hours and evaluation time to 1 hour.</p>
<p>Mean Variance</p>
<p>Original Data 0.961 0.00043 Generated Data 0.888 0.00387</p>
<p>B. Data Details</p>
<p>Instruction Diversity.To assess the instruction diversity of our generated data, we first visualize word clouds for pick-and-place tasks in Figure 4, which clearly demonstrate the higher diversity of our generated data.Additionally, we quantitatively evaluate the diversity by using BERT to obtain CLS embeddings for each instruction.We then compute the cosine similarities for all instruction pairs and calculate the mean and variance of the similarity matrix as measures of diversity.The results, presented in Table 7, show that the generated data exhibit lower mean similarity and higher variance, indicating greater diversity.</p>
<p>Prompts for Data Generation.We provide the prompts utilized for GPT-4 in our work.The prompt for filtering kitchen-related objects, shown in Figure 5, includes a list of valid kitchen-related object types and object attributes.GPT-4 is tasked with determining whether a given object is related to the kitchen based on this information.It is worth noting that the type list and object attributes were initially generated by GPT-4 in earlier stages of our process.</p>
<p>The prompt for generating key attributes of objects is illustrated in Figure 6.In this case, GPT-4 is instructed to describe the object's attributes using distinct descriptive words, referred to as key attributes.Similarly, the prompt for generating descriptive phrases is depicted in Figure 7, where GPT-4 is tasked with describing object attributes using detailed descriptive phrases.</p>
<p>The prompt for generating common-sense instructions is presented in Figure 8.In this scenario, GPT-4 is provided with multiple views of the scene, including the target object, the placement area, and other surrounding objects.It is then instructed to generate instructions that require common-sense reasoning.</p>
<p>Simulation Tasks.We adopt the setup of 22 atomic tasks defined in RoboCasa [28], categorizing them into four task types:Pick and Place, Open/Close, Press, and Turn/Twist, as summarized in Table 8.Beyond the original dataset of 3,000 generated samples for each task (referred to as "Easy" data), we introduce more complex scenes and instructions specifically for pick-and-place tasks.These enhancements aim to increase diversity by incorporating variations in appearance, spatial relationships, and common-sense reasoning.Based on the given information: 1. List of kitchen-related object types: [alcohol, apple, avocado, bagel, bagged_food, baguette, banana, bar, bar_soap, beer, bell_pepper, bottled_drink, bottled_water, bowl, boxed_drink, boxed_food, bread, broccoli, cake, can, candle, canned_food, carrot, cereal, cheese, chips, chocolate, coffee_cup, condiment, corn, croissant, cucumber, cup, cupcake, cutting_board, donut, egg, eggplant, fish, fork, garlic, hot_dog, jam, jug, ketchup, kettle, kiwi, knife, ladle, lemon, lime, mango, milk, mug, mushroom, onion, orange, pan, peach, pear, plate, potato, rolling_pin, scissors, shaker, soap_dispenser, spatula, sponge, spoon, spray, squash, steak, sweet_potato, tangerine, teapot, tomato, tray, waffle, water_bottle, wine, yogurt] 2. Attributes of the object: -Name: Fantasy Windmill Tower -Description: A whimsical, hand-painted wooden windmill tower designed for low-poly environments, featuring vibrant colors and a fantasy aesthetic.</p>
<p>-Material: Wood -Shape: Tower -Primary color: Multi-colored -Size: 30 -Other tags: tower, wooden, windmill, handpainted, low-poly, fantasy Task: Determine whether the object should be placed in the kitchen.If it belongs to one of the specified kitchen-related object types, indicate the type.Otherwise, state it does not belong to any of the listed categories.</p>
<p>Answer format:</p>
<p>Yes, it should be placed in the kitchen.It belongs to the type [kitchen-related type].No, it shouldn't be placed in the kitchen.Yes, it should be placed in the kitchen.However, the object [object name] does not belong to any of the kitchen-related object types.</p>
<p>C. Details of Grounded Perceiver</p>
<p>The grounded perceiver is composed of multiple attention layers.To illustrate its mechanism, consider a single attention layer where the input queries consist of 9 global query tokens Q g ∈ R 9×Dp , 9 target object query tokens Q o ∈ R 9×Dp , and 9 target placement query tokens Q p ∈ R 9×Dp .These queries are concatenated for parallel computation and projected to the hidden dimension of the attention layer, forming Q ∈ R 27×d , where d represents the hidden dimension.The input 14×14 patch features Z P v ∈ R 196×Dv are concatenated with the query features to construct the Key K ∈ R 223×d and Value V ∈ R 223×d .
3K - - - CloseSingleDoor 3K - - - OpenDoubleDoor 3K - - - CloseDoubleDoor 3K - - - OpenDrawer 3K - - - CloseDrawer 3K - - - Press CoffeePressButton 3K - - - TurnOnMicrowave 3K - - - TurnOffMicrowave 3K - - - Turn / Twist TurnOnSinkFaucet 3K - - - TurnOffSinkFaucet 3K - - - TurnSinkSpout 3K - - - TurnOnStove 3K - - - TurnOffStove 3K - - -
The attention matrix is computed as A = QK ⊤ √ k ∈ R 27×223 .To incorporate the masks for target objects and placement areas, the attention values corresponding to the masked regions are replaced with the highest attention value in the current matrix.Specifically, the target object masks M o are applied to A [9:18,:196] , and the placement masks M p are applied to A [18:,:196] .This ensures that the target object and placement query tokens focus more effectively on the relevant masked areas.</p>
<p>D. Limitation and Future Work</p>
<p>Although extensive experiments have demonstrated the effectiveness of our proposed method using grounding masks as a guide-particularly its strong generalization ability to unseen domains-there remain some limitations that warrant further exploration in future work.</p>
<p>For object picking, we observe a significant gap between the contact rate and the success rate, suggesting that the model struggles with reliably grasping target objects.This limitation stems primarily from the high diversity of the thousands of objects in our dataset, making it more challenging for the model to overfit compared to previous, less varied datasets.Furthermore, while grounding masks excel at providing localization guidance, they offer limited support for enhancing grasping precision.To address this, a This image shows four different views of a single banana.Please describe its attributes following the format: <COLOR> its color </COLOR> <SHAPE> its shape </SHAPE> <MATERIAL> its material </MATERIAL>.</p>
<p>Here are four example outputs for your reference: 1. <COLOR> green on top and yellow on the bottom </COLOR> <SHAPE> rectangular prism </SHAPE> <MATERIAL> sponge </MATERIAL>.2. <COLOR> white </COLOR> <SHAPE> cylindrical with a flip-top lid </SHAPE> <MATERIAL> plastic </MATERIAL>.3. <COLOR> blue </COLOR> <SHAPE> flat with an elongated handle and slotted flipper </SHAPE> <MATERIAL> plastic or metal </MATERIAL>.4. <COLOR> brown cap and white stem </COLOR> <SHAPE> typical mushroom shape with a rounded cap and cylindrical stem </SHAPE> <MATERIAL> organic fungus </MATERIAL>.Now please describe the object shown in the image.promising approach could involve integrating a pre-trained grasp pose prediction network, such as AnyGrasp [13], which boasts strong generalization capabilities for novel objects.Incorporating such a network could enable the development of a more robust and generalizable policy network.</p>
<p>For data generation, our focus has primarily been on increasing the diversity of the target object, while largely overlooking the diversity of target placement areas.Enhancing this aspect can be achieved by generating a broader range of target placement options.To accomplish this, we need to collect additional human demonstrations for these newly generated scenes (trajectories to new placement areas) and leverage automated methods, such as Mimic-Gen [26], to further augment the dataset.</p>
<p>For model architecture, we currently treat it as two distinct components: a grounded VLM for mask prediction and a policy network for action prediction.Future exploration of end-to-end architectures or slow-fast systems could be both promising and challenging, potentially enabling more robust policies and harder tasks such as longhorizon tasks.</p>
<p>We hope our findings inspire further research into intermediate representations that can guide low-level policies, and provide valuable insights for generating more diverse scenes and instructions in robot manipulation.</p>
<p>These images depict a kitchen scene where a Franka robotic arm interacts with various items.The robot is instructed to perform a pick-and-place manipulation task.Three viewpoints are provided: left view, right view, and robot hand view.Each viewpoint includes two types of images: the raw image and one with masked targets.The target object, a water bottle, is highlighted in red, while the target placement area, the cabinet, is marked in green.Other objects present in the scene include a water bottle, tomato, kettle, coffee cup, spoon, and chips.</p>
<p>Please generate five new instructions that require common-sense reasoning about the target object for me.Remember, you cannot change the target placement location.Do not mention the masked aspect.ATTENTION: Only refer to objects that exist in the scene.</p>
<p>For example, the target object is kettle, the expected answer format is: <ANSWER> &lt;1&gt; I want to drink, please pick the related object to the cabinet.&lt;/1&gt; &lt;2&gt; I'm thirsty, please pick the related object to the cabinet.&lt;/2&gt; &lt;3&gt; I'm going for a hike and need to stay hydrated, please pick up the appropriate item and place it in the cabinet.&lt;/3&gt; &lt;4&gt; I need to refill something for gardening, could you put the refillable item into the cabinet?&lt;/4&gt; &lt;5&gt; I'm heading to the gym and need to fill up my container, place it in the cabinet.&lt;/5&gt;</ANSWER></p>
<p>Figure 1 .
1
Figure1.Examples of generated data and mask guidance for manipulation policy.The generated data includes more object distractors in the scene, leading to higher scene complexity.The robot policy is guided by masks to localize the target object and placement area.</p>
<p>Pick the purple object from the counter.The images show different views of a single coffee cup.Please describe its attributes from color, shape and material.Commonsense Instructions:We're making jam, can you grab the appropriate item?I'm making a fruit salad, could you pick the related item?The robot is given a pick-and-place instruction to perform a manipulation task.Please generate new instructions that requires common-sense reasoning of the target object.Key Attributes color: brown, white shape: cylindrical, tapered material: paper, plastic Descriptive Phrases color: brown with a white lid shape: tapered cylindrical with a domed lid material: likely paper for the cup and plastic for the lid Pick the object to the right of the carrot.Pick the object in front of the apple.</p>
<p>Figure 2 .
2
Figure 2. Data Generation Pipeline.The pipeline is composed of three key stages: (a) First, we extract informative object attributes in both keyword and descriptive phrase formats; (b) Next, appearance-based instructions are generated using these attributes, where keywords filter objects and descriptive phrases calculate appearance similarity; (c) Finally, spatial and commonsense instructions are generated through rule-based methods and GPT-generated techniques, respectively.</p>
<p>Figure 3 .
3
Figure 3. Overall Architecture of ROBOGROUND.To enhance policy generalization, we leverage grounding masks as intermediate representations for spatial guidance.Specifically, (a) The grounded vision-language model processes the instruction and image observation to generate target masks.(b) The grounded policy network integrates mask guidance by concatenating masks with the image input and directing attention within the grounded perceiver.</p>
<p>Baselines.• ACT [50]: A transformer-based policy network introduced by ALOHA [50].Trained as a conditional VAE, it performs action chunking when predicting future actions.• BC-Transformer [25]: A standard behavior cloning policy built on a transformer architecture.We use the publicly available BC-Transformer implementation from</p>
<p>(a) Word cloud of original data (b) Word cloud of generated data</p>
<p>Figure 4 .
4
Figure 4. Comparison of Word Clouds: Original Data (left) vs. Generated Data (right).</p>
<p>Figure 5 .
5
Figure 5. Prompt for Filtering Kitchen-related Objects.</p>
<p>Figure 6 .
6
Figure 6.Prompt for Generating Key Attributes.</p>
<p>Figure 7 .
7
Figure 7. Prompt for Generating Descriptive Phrases.</p>
<p>Figure 8 .
8
Figure 8. Prompt for Generating Common-sense Instructions.</p>
<p>Table 1 .
1
Performance Comparison on Simulated Tasks.Metrics for pick-and-place tasks are reported as "a / b", where a is the contact rate (%) and b is the success rate (%).We also report the success rates for other fundamental skills, including door opening/closing, button pressing, lever turning and knob twisting.
MethodEasyPick and Place Appearance SpatialCommon-senseOpen/Close Press Turn/TwistACT [50]47.3 / 18.318.5 / 3.817.5 / 3.515.3 / 2.832.030.826.5BC-Transformer [25] 79.8 / 34.831.3 / 6.838.3 / 7.826.0 / 5.854.850.845.3GR-1 [43]85.3 / 42.8 49.5 / 13.8 54.5 / 16.343.0 / 11.566.858.849.5Ours89.0 / 43.3 78.5 / 30.5 81.0 / 33.576.3 / 30.072.069.354.5</p>
<p>Table 2 .
2
Evaluation Results of Unseen Settings.Unseen instance denotes evaluation on new objects belonging to classes present in the training data.In contrast, unseen class refers to evaluation on objects from entirely new classes that were not included in the training data.
Unseen Level MaskEasyAppea.SpatialComm.Instance71.0 / 35.5 38.0 / 11.5 38.8 / 11.5 30.8 / 7.8Instance✓85.8 / 42.5 75.5 / 29.5 77.8 / 31.8 72.0 / 28.0Class53.0 / 17.5 27.5 / 5.330.3 / 6.328.0 / 5.8Class✓79.5 / 27.3 68.5 / 14.3 69.5 / 14.8 67.3 / 13.8</p>
<p>Table 3 .
3
Ablation Study on Training Data and Grounded Masks."Ori.Data" refers to original data in RoboCasa, while "New Data" denotes our proposed data with diverse instructions."Pred" indicates the use of predicted masks, whereas "GT" denotes ground-truth masks.
Ori. Data New Data MaskEasyAppea. Spatial Comm.✓80 / 38 26 / 620 / 426 / 4✓78 / 34 42 / 12 52 / 16 40 / 12✓✓90 / 60 46 / 16 54 / 20 38 / 12✓Pred 88 / 44 48 / 14 50 / 16 44 / 12✓Pred 86 / 42 72 / 32 78 / 38 70 / 32✓✓Pred 90 / 64 70 / 40 80 / 38 74 / 36✓✓GT96 / 68 80 / 48 82 / 44 80 / 42</p>
<p>Table 4 .
4
Ablation Study on Modules for Incorporating Grounding Masks."Channel Concat."denotes whether to do the channel concatenation for the image and mask."Grounded Perceiver" denotes whether to incorporate masks into the perceiver.
Channel Concat.Grounded PerceiverEasyAppea. Spatial Comm.68 / 24 42 / 12 52 / 16 40 / 12✓78 / 36 60 / 26 68 / 30 62 / 26✓80 / 36 56 / 22 64 / 28 64 / 30✓✓86 / 42 72 / 32 78 / 38 70 / 32</p>
<p>Table 6 .
6
Ablation Study on Grounded VLM."Zero-shot" refers to the zero-shot evaluation of the grounded VLM."Sim.data" and "VLM data" denotes the use of simulated grounding data and VLM data for fine-tuning.</p>
<p>tion relabeling.In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 9168-9175.IEEE, 2024. 1, 2 [50] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn.Learning fine-grained bimanual manipulation with low-cost hardware.arXiv preprint arXiv:2304.13705,2023.
1, 6, 7[51] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-hamed Elhoseiny. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models. arXivpreprint arXiv:2304.10592, 2023. 1, 2</p>
<p>Table 7 .
7
Embedding Similarity of Instructions: Original vs. Generated Data.A lower mean similarity and higher variance suggest greater diversity in the generated data compared to the original data.</p>
<p>Table 8 .
8
Task and Data Split.We adopt the setup of 22 atomic tasks defined in RoboCasa, categorizing them into four task types.For pick-and-place tasks, we create more complex scenes and instructions to increase the level of diversity.The quantity of training data for each task type is detailed in the table.
Task TypeTask NameTrain Data Easy Appea. Spatial Comm.PnPCounterToCab3K3K5K6KPnPCabToCounter3K3K5K6KPnPCounterToSink3K3K5K6KPick and PlacePnPSinkToCounter PnPCounterToMicrowave3K 3K3K 3K5K 5K6K 6KPnPMicrowaveToCounter3K3K5K6KPnPCounterToStove3K3K5K6KPnPStoveToCounter3K3K5K6KOpenSingleDoorOpen / Close
AcknowledgmentThis work was supported by National Key R&amp;D Program of China (2022ZD0162000).Supplementary MaterialThe supplementary materials are organized as follows:1. Implementation details are provided in Appendix A.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Debidatta Chebotar, Dorsa Dwibedi, Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 20241arXiv preprint</p>
<p>Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani, 20241</p>
<p>Zero-shot robotic manipulation with pretrained imageediting diffusion models. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2310.106392023arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Closed-loop visuomotor control with generative expectation for robotic manipulation. Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li, arXiv:2409.090162024arXiv preprint</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, 2023</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, The International Journal of Robotics Research. 1027836492412736682023</p>
<p>Instructblip: Towards generalpurpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, Dieter Fox, arXiv:2305.16309Imitating task and motion planning with visuomotor transformers. 2023arXiv preprint</p>
<p>Objaverse: A universe of annotated 3d objects. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Flowbot3d: Learning 3d articulation flow to manipulate articulated objects. Ben Eisner, Harry Zhang, David Held, arXiv:2205.043822022arXiv preprint</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. Chenxi Hao-Shu Fang, Hongjie Wang, Minghao Fang, Jirong Gou, Hengxu Liu, Wenhai Yan, Yichen Liu, Cewu Xie, Lu, IEEE Transactions on Robotics. 742023</p>
<p>Moka: Open-world robotic manipulation through markbased visual prompting. Kuan Fang, Fangchen Liu, Pieter Abbeel, Sergey Levine, Robotics: Science and Systems (RSS). 2024</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, arXiv:2311.0197720231arXiv preprint</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Perceiver: General perception with iterative attention. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, Joao Carreira, International conference on machine learning. PMLR2021</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.02643Piotr Dollár, and Ross Girshick. Segment anything. 2023</p>
<p>Lisa: Reasoning segmentation via large language model. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202424</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, 20231</p>
<p>Groma: Localized visual tokenization for grounding multimodal large language models. Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi, European Conference on Computer Vision. Springer2025</p>
<p>Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín, arXiv:2108.03298What matters in learning from offline human demonstrations for robot manipulation. 202167arXiv preprint</p>
<p>Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox, arXiv:2310.17596Mimicgen: A data generation system for scalable robot learning using human demonstrations. 202334arXiv preprint</p>
<p>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>Robocasa: Large-scale simulation of everyday tasks for generalist robots. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, Yuke Zhu, arXiv:2406.025232024. 2, 3, 6, 1arXiv preprint</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, ArXiv, abs/23062023</p>
<p>Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. Ethan Perez, Florian Strub, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2018</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR202145</p>
<p>Glamm: Pixel grounding large multimodal model. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Eric Rao M Anwer, Ming-Hsuan Xing, Fahad S Yang, Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024. 1, 2, 4, 5</p>
<p>Toolflownet: Robotic manipulation with tools via predicting tool flow from point clouds. Daniel Seita, Yufei Wang, J Sarthak, Edward Yao Shetty, Zackory Li, David Erickson, Held, Conference on Robot Learning. 2023</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on robot learning. 2022</p>
<p>Open-world object manipulation using pre-trained vision-language models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, arXiv:2303.009052023arXiv preprint</p>
<p>Kite: Keypoint-conditioned policies for semantic manipulation. Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg, arXiv:2306.166052023</p>
<p>Rt-sketch: Goal-conditioned imitation learning from hand-drawn sketches. Priya Sundaresan, Quan Vuong, Jiayuan Gu, Peng Xu, Ted Xiao, Sean Kirmani, Tianhe Yu, Michael Stark, Ajinkya Jain, Karol Hausman, 20241</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang, arXiv:2412.151092024arXiv preprint</p>
<p>Robotap: Tracking arbitrary points for few-shot visual imitation. Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, Jon Scholz, 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024</p>
<p>Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang, arXiv:2310.01361Generating robotic simulation tasks via large language models. Gensim2023arXiv preprint</p>
<p>The all-seeing project v2: Towards general relation comprehension of the open world. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, arXiv:2402.194742024arXiv preprint</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Unleashing large-scale video generative pretraining for visual robot manipulation. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong, arXiv:2312.13139202371arXiv preprint</p>
<p>Florence-2: Advancing a unified representation for a variety of vision tasks. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202424</p>
<p>Flow as the cross-domain manipulation interface. Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, Shuran Song, 8th Annual Conference on Robot Learning. 2024</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang, arXiv:2310.077042023arXiv preprint</p>
<p>Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, arXiv:2401.11439General flow as foundation affordance for scalable robot learning. 2024arXiv preprint</p>
<p>Ferret-v2: An improved baseline for referring and grounding with large language models. Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, arXiv:2404.07973202424arXiv preprint</p>
<p>Sprint: Scalable policy pre-training via language instruc-This image shows four different views of a single banana. Please describe its attributes following these rules: 1. The format should be: <COLOR> its color. Jesse Zhang, Karl Pertsch, Jiahui Zhang, Joseph J Lim, </p>
<p>For each attribute, only include the descriptive words. For example, if you describe the color as "yellow surface with brown spots. please simplify it to "yellow, brown</p>
<p>Do not describe minor parts of the object. For example, if the picture shows a red apple with a small green stem, the color description should be. red" rather than "red, green". Here are three example outputs for your reference</p>
<p><COLOR> silver. </p>            </div>
        </div>

    </div>
</body>
</html>
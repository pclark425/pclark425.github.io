<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-aec95e6330033e0ec39fb5a069d647288c03b945</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aec95e6330033e0ec39fb5a069d647288c03b945" target="_blank">Assessing Large Language Models on Climate Information</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive evaluation framework, grounded in science communication research, is presented to assess LLM responses to questions about climate change, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues.</p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Presentational Adequacy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Presentational Adequacy (style, clarity, correctness, tone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of evaluation criteria focusing on the surface/formal qualities of LLM outputs: style, clarity, linguistic correctness, and tone, drawn from science communication and MQM-inspired metrics and used to assess readability and appropriateness for public-facing climate information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Various contemporary instruction-tuned and chat LLMs evaluated in Sept-Oct 2023; GPT-4 also used to produce auxiliary data and assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Science communication / Climate science information</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human rater assessment (raters with relevant education), 3 raters per answer; presentational dimensions assessed without retrieved evidence/paragraphs (except assistance not used for these dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Style (register, length), Clarity (conciseness, jargon avoidance), (Linguistic) Correctness (spelling/grammar/punctuation), Tone (neutrality, persuasiveness, valence).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Answers to 300 climate-related questions (100 each from Google Trends, Skeptical Science, synthetic Wikipedia); answers were 3-4 sentence paragraphs generated using a simple instruction prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High performance across presentational dimensions for most LLMs (mean ratings ~4.3–4.6 out of 5 on style/clarity/correctness for top models); tone scored substantially lower (means around ~3.0). ChatGPT often best on presentational metrics. See Table 3 for bootstrapped means and CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity in ratings; medium inter-rater agreement; tone is subtle and may require finer-grained measures; dimension trade-offs exist (improvements in epistemological aspects sometimes degraded presentational quality).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM outputs are surface-fluent and comparable to human experts on fluency but differ qualitatively in pragmatic style (e.g., preachy or generic consensus appeals) compared to domain expert answers which cite direct specific evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate presentational and epistemological qualities separately; focus presentational evaluation on subtler aspects like tone; use raters with relevant education and admission testing to control quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Epistemological Adequacy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epistemological Adequacy (accuracy, specificity, completeness, uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of criteria measuring how well LLM outputs represent current scientific knowledge: factual accuracy, appropriate specificity (spatial/temporal), completeness (coverage of question aspects), and explicit communication of uncertainty/consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Contemporary instruction-following LLMs evaluated on climate Q&A; GPT-4 additionally used to generate auxiliary evidence and critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Science communication / Climate science information</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human rater assessment supported by grounded AI Assistance (evidence retrieval from Wikipedia); each epistemological dimension rated by raters, assisted by model-generated critiques and verbatim retrieved evidence when available.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (incorrect/wrong/self-contradictory/context misuse), Specificity (location/time/up-to-date), Completeness (addresses all question parts and depth), Uncertainty (agreement level and confidence stated).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same 300 climate questions and corresponding LLM-generated answers; evidence retrieval limited to Wikipedia paragraphs returned by GPT-4 pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Epistemological ratings substantially lower than presentational: accuracy means roughly 3.5–3.8 for top models, specificity/completeness/uncertainty often in the 2.0–3.1 range (see Table 4). Falcon-180B-Chat scored best on epistemological dimensions overall; GPT-4 better than ChatGPT but worse on some presentational metrics. Short 3–4 sentence answers exacerbate completeness and uncertainty deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Short answer length constraints; absence of gold truth makes direct factuality validation hard; subjectivity in rating; AI Assistance can bias raters beyond simply surfacing issues; attribution methods miss omissions (what's missing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human experts tend to cite direct, multi-source evidence and include images/data; AIS (attribution) is complementary but orthogonal — attributable quotes do not guarantee epistemological adequacy.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use grounded evidence retrieval to assist raters; separate out and report multiple epistemological sub-dimensions instead of a single score; consider longer/structured responses or follow-up prompts to allow completeness and uncertainty expression.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Assistance (Scalable Oversight Protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Assistance for Scalable Oversight (model-generated critiques with retrieved evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical, scalable oversight protocol where GPT-4 generates dimension-specific critiques and, for epistemological dimensions, provides verbatim Wikipedia evidence to help human raters detect issues, increasing recall of real problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (used to generate auxiliary critiques/evidence); evaluated across multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 used as a single LLM to extract keypoints, propose supporting Wikipedia URLs, rank paragraphs, and synthesize per-dimension critiques quoted with evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology for LLM outputs in climate science communication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human raters shown LLM answer plus AI Assistance (critiques + retrieved paragraphs) when available; study compared settings with and without assistance and with prior exposure to assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effectiveness measured by number of issues detected per dimension, detection recall on synthetic gold examples, and rater-reported helpfulness correlated with rating behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the 300-question dataset and to a validation set of 30 manually generated examples illustrating specific issues.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>AI Assistance increased issue detection: majority-of-three raters detected 77% of known issues with assistance vs 60% without. Raters with prior exposure performed intermediate. When assistance judged unhelpful, raters gave higher ratings, indicating critical appraisal rather than blind following.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Assistance can influence raters beyond surfacing issues (potential bias); model-generated critiques can themselves be incorrect and mislead; need to study and mitigate these effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Extends classical human review by amplifying rater recall and enabling non-experts to approach expert-level detection when guided; analogous to sandwiching/single-turn debate paradigms in scalable oversight literature.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Always provide grounded evidence with critiques for epistemological assessments; train raters with exposure to assistance and admission tests; validate assistance on gold examples; study mitigation for undue influence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIS (Attribution to Identified Source)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribution to Identified Source (AIS) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attribution-focused evaluation that checks whether an LLM's answer includes explicit quotes from existing documents that support its claims; used here to compare attribution signals against epistemological ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring Attribution in Natural Language Generation Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (used for AIS experiment and for producing candidate Wikipedia URLs/evidence); applied to answers from multiple evaluated LLMs but AIS experiment run only on GPT-4 answers here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 generated candidate Wikipedia article URLs and extracted evidence paragraphs; human annotators judged per-keypoint support (fully/partially/not supported).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP factuality and grounding evaluation in climate information</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human annotation of keypoint-to-evidence support; answer-level attribution defined as all keypoints supported (fully/partially), otherwise partial/none.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fraction of keypoints fully/partially supported; percent of answers fully/partially attributed; Spearman correlations with epistemological dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Keypoints and evidence produced for 300 answers (87.7% of examples produced a valid Wikipedia article); human judgments on support for each keypoint.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>66.79% of keypoints were fully or partially supported by retrieved evidence; 46.08% of answers were fully or partially supported at answer level. Correlation between AIS and epistemological ratings was near zero (Spearman coefficients: accuracy 0.03, specificity -0.06, uncertainty 0.002, completeness -0.02), indicating AIS and epistemological adequacy are mostly orthogonal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>AIS evaluates only what is present (not omissions), so it misses missing important information; depends on quality of retrieval and selection of supporting documents; limited to available documents (here Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Complementary to direct epistemological human assessment; attribution can indicate grounding but not guarantee completeness or nuanced uncertainty communication.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use AIS as a complement, not a substitute, to epistemological evaluation; ensure retrieval covers diverse, high-quality sources and evaluate omissions separately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9687.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>300-Q Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>300-Question Climate Q&A Benchmark (GTrends / SkepticalScience / Synthetic Wikipedia)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated evaluation set of 300 climate-related user information needs: 100 high-volume Google Trends questions, 100 rephrased Skeptical Science myth-questions, and 100 synthetic questions generated from Wikipedia paragraphs and filtered for context and specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Answers generated from multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat) using a uniform 3-4 sentence answer prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Uniform short-answer prompt: 'You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph.' Answers used for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science communication / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human rating of each answer across presentational and epistemological dimensions (3 raters per answer), with auxiliary AI Assistance for epistemological dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>The 8 dimensions (4 presentational + 4 epistemological) and 30 finer issues used to annotate failures (see Table 7 for issue taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>The 300-question dataset itself; Wikipedia was used for evidence retrieval for assistance/AIS.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Dataset supported detection of consistent patterns: high presentational vs low epistemological quality across models; no large differences by question source, though Wikipedia-sourced questions scored slightly lower.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No gold-truth answers exist; synthetic questions filtered but still may bias models; short answer length constrains certain epistemological criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No analogous standardized climate QA benchmark existed; the dataset is positioned similarly to domain-specific evaluation sets (e.g., medical benchmarks) but for public climate information.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a diverse set of question sources and pre-filter for context-independence; include topic and causal/prediction stratification when sampling; provide the exact evaluation protocols and prompts for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9687.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Evaluation Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Evaluation Protocol with Educated Raters and Admission Testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A carefully controlled human evaluation setup using raters with relevant educational background, a brief training tutorial, an admission test to qualify raters, and 3-rater redundancy per answer, allowing partial consultation of external sources but discouraging extensive research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applied to answers from multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM itself; protocol governs human ratings of LLM outputs produced with a short expert prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human evaluation methodology for NLP in climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Three human raters per answer; raters selected by relevant education; admission test and tutorial; ratings collected with and without AI Assistance to measure Assistant effects; inter-rater agreement computed (Krippendorff's alpha referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency (three raters), use of AI Assistance exposure conditions (no assistance, no assistance but prior exposure, with assistance) to measure influence, and validation on 30 hand-crafted examples.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the 300-question benchmark; validation set of 30 manually generated issue-exhibiting examples used to measure assistance recall.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Assistance improved detection recall (77% vs 60% on validation examples); medium inter-rater agreement at answer-level but robust at system-level; raters with prior exposure to assistance detected more issues even when assistance unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity in dimensions leads to noise; lack of gold ratings complicates calibration; potential influence of assistance beyond issue discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Improves classical human-only review by increasing issue recall and allowing scalable oversight, but introduces new biases and dependencies on model-generated assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use admission testing, domain-educated raters, and redundancy; include validation examples; monitor and study effects of assistance exposure and potential undue influence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9687.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Keypoints + Retrieval Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keypoint Extraction and Wikipedia Retrieval Pipeline (GPT-4 grounded evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reproducible auxiliary-data pipeline where GPT-4 extracts 1–3 keypoints per answer, proposes supporting Wikipedia URLs, ranks article paragraphs for relevance, and supplies the top-3 paragraphs per keypoint as verbatim evidence for AI Assistance and AIS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (used for keypoint extraction, URL proposal, paragraph ranking, and AI Assistance generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 used as a consistent single-model retrieval/evidence selection system (not a general web retriever); evidence limited to Wikipedia for uniform quality.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evidence grounding and retrieval for evaluation of LLM outputs in climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic keypoint extraction from answers, model-suggested Wikipedia article selection, paragraph-level relevance scoring and top-3 paragraph selection per keypoint; paragraphs included in AI Assistance for raters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coverage of keypoints by retrieved evidence, fraction of valid Wikipedia article suggestions (87.7% produced valid articles), and usefulness in improving issue detection.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to all 300 Q&A pairs; retrieval limited to Wikipedia paragraphs from selected articles.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pipeline effective: GPT-4 suggested valid Wikipedia articles for 87.7% of questions; using keypoints improved retrieval effectiveness over naive retrieval; retrieved evidence supported 66.79% of keypoints (fully/partially).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Constrained to Wikipedia (limits source diversity); retrieval quality depends on GPT-4 URL proposal; AIS and retrieval miss missing information and cannot penalize omitted relevant knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More scalable than manual evidence curation and often superior to off-the-shelf sparse/dense retrieval in this setup, but still relies on LLM-produced retrieval cues.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use keypoint-based retrieval rather than whole-answer retrieval; limit evidence sources to uniform high-quality repositories if consistency desired; validate retrieval choices and assess coverage of missing information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9687.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dimension-aware Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dimension-aware Prompting Experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment that augments the LLM prompt with explicit descriptions of the evaluation criteria (the dimensions) to test whether prompting the model with evaluation-aware instructions improves epistemological quality of answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (experiment performed with GPT-4, November 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 prompted with either 'basic' or 'dimension-aware' prompts; results compared on presentational and epistemological metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Prompt engineering for improving epistemological qualities of LLM outputs in climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare ratings for GPT-4 answers produced under basic prompt vs dimension-aware prompt across the eight evaluation dimensions, using same human rater protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Change in mean ratings per dimension (presentational and epistemological) and statistical significance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Subset of the same 300-question dataset; experiment executed in November 2023 (post a major GPT-4 release).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Including evaluation-criteria descriptions improved performance on difficult epistemological dimensions and tone, but sometimes degraded presentational quality (trade-off observed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Improvements may introduce trade-offs with surface form; experiment limited to GPT-4 and cannot generalize without further testing; timing differences (experiment after GPT-4 update) complicate direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Shows that explicitly instructing models about human evaluation criteria can partially substitute for human guidance in improving epistemological content, but does not replace human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use dimension-aware prompts to bias generation toward epistemological adequacy, but monitor for adverse effects on presentation; combine with post-generation grounding and human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9687.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9687.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical Analysis & Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical Evaluation Suite (bootstrapped means, pairwise t-tests, Krippendorff's alpha, issue taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's quantitative evaluation approach: bootstrapped mean ratings with 95% CIs per model/dimension, pairwise t-tests comparing models, calculation of inter-rater agreement (Krippendorff's alpha), and a fine-grained issue taxonomy (30 issues across 8 dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applied across evaluations of multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM but a statistical evaluation toolkit applied to human ratings of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Experimental evaluation and statistical analysis for NLP and science communication assessments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute bootstrapped means and 95% CIs for each dimension/model (Tables 3–4), run pairwise t-tests to indicate significant differences (Tables 5–6), report issue frequencies (Table 7), and compute inter-rater reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Statistical significance thresholds (p<0.01, p<0.05), bootstrapped confidence intervals, and issue detection recall in validation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the full 300-question evaluation and validation subsets; results reported per-system and per-dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Sufficient resolution to distinguish models on specific dimensions; significant differences found between models on many dimensions; issue taxonomy revealed common failure modes (e.g., vagueness, not enough detail, missing uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Inter-rater agreement moderate at answer-level; absence of gold standards complicates calibration and absolute performance claims; some statistical comparisons confounded by GPT-4 being used to generate auxiliary data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides a robust multi-faceted analysis compared to single-metric automated evaluations; aligns with expert-level evaluation needs in other domains (e.g., medicine) but without a gold ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report bootstrapped CIs, use multiple statistical comparisons, include fine-grained issue taxonomies, and validate evaluation resolution with pairwise tests and human validation examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring Attribution in Natural Language Generation Models <em>(Rating: 2)</em></li>
                <li>Concrete problems in AI safety <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Evaluating verifiability in generative search engines <em>(Rating: 2)</em></li>
                <li>AI safety via debate <em>(Rating: 1)</em></li>
                <li>Large language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Med-PaLM: Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
                <li>The Oxford Handbook of the Science of Science Communication <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9687",
    "paper_id": "paper-aec95e6330033e0ec39fb5a069d647288c03b945",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Presentational Adequacy",
            "name_full": "Presentational Adequacy (style, clarity, correctness, tone)",
            "brief_description": "A set of evaluation criteria focusing on the surface/formal qualities of LLM outputs: style, clarity, linguistic correctness, and tone, drawn from science communication and MQM-inspired metrics and used to assess readability and appropriateness for public-facing climate information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)",
            "llm_description": "Various contemporary instruction-tuned and chat LLMs evaluated in Sept-Oct 2023; GPT-4 also used to produce auxiliary data and assistance.",
            "scientific_domain": "Science communication / Climate science information",
            "evaluation_method": "Human rater assessment (raters with relevant education), 3 raters per answer; presentational dimensions assessed without retrieved evidence/paragraphs (except assistance not used for these dimensions).",
            "evaluation_criteria": "Style (register, length), Clarity (conciseness, jargon avoidance), (Linguistic) Correctness (spelling/grammar/punctuation), Tone (neutrality, persuasiveness, valence).",
            "benchmark_or_dataset": "Answers to 300 climate-related questions (100 each from Google Trends, Skeptical Science, synthetic Wikipedia); answers were 3-4 sentence paragraphs generated using a simple instruction prompt.",
            "results_summary": "High performance across presentational dimensions for most LLMs (mean ratings ~4.3–4.6 out of 5 on style/clarity/correctness for top models); tone scored substantially lower (means around ~3.0). ChatGPT often best on presentational metrics. See Table 3 for bootstrapped means and CIs.",
            "limitations_or_challenges": "Subjectivity in ratings; medium inter-rater agreement; tone is subtle and may require finer-grained measures; dimension trade-offs exist (improvements in epistemological aspects sometimes degraded presentational quality).",
            "comparison_to_human_or_traditional": "LLM outputs are surface-fluent and comparable to human experts on fluency but differ qualitatively in pragmatic style (e.g., preachy or generic consensus appeals) compared to domain expert answers which cite direct specific evidence.",
            "recommendations_or_best_practices": "Evaluate presentational and epistemological qualities separately; focus presentational evaluation on subtler aspects like tone; use raters with relevant education and admission testing to control quality.",
            "uuid": "e9687.0",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Epistemological Adequacy",
            "name_full": "Epistemological Adequacy (accuracy, specificity, completeness, uncertainty)",
            "brief_description": "A set of criteria measuring how well LLM outputs represent current scientific knowledge: factual accuracy, appropriate specificity (spatial/temporal), completeness (coverage of question aspects), and explicit communication of uncertainty/consensus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)",
            "llm_description": "Contemporary instruction-following LLMs evaluated on climate Q&A; GPT-4 additionally used to generate auxiliary evidence and critiques.",
            "scientific_domain": "Science communication / Climate science information",
            "evaluation_method": "Human rater assessment supported by grounded AI Assistance (evidence retrieval from Wikipedia); each epistemological dimension rated by raters, assisted by model-generated critiques and verbatim retrieved evidence when available.",
            "evaluation_criteria": "Accuracy (incorrect/wrong/self-contradictory/context misuse), Specificity (location/time/up-to-date), Completeness (addresses all question parts and depth), Uncertainty (agreement level and confidence stated).",
            "benchmark_or_dataset": "Same 300 climate questions and corresponding LLM-generated answers; evidence retrieval limited to Wikipedia paragraphs returned by GPT-4 pipeline.",
            "results_summary": "Epistemological ratings substantially lower than presentational: accuracy means roughly 3.5–3.8 for top models, specificity/completeness/uncertainty often in the 2.0–3.1 range (see Table 4). Falcon-180B-Chat scored best on epistemological dimensions overall; GPT-4 better than ChatGPT but worse on some presentational metrics. Short 3–4 sentence answers exacerbate completeness and uncertainty deficits.",
            "limitations_or_challenges": "Short answer length constraints; absence of gold truth makes direct factuality validation hard; subjectivity in rating; AI Assistance can bias raters beyond simply surfacing issues; attribution methods miss omissions (what's missing).",
            "comparison_to_human_or_traditional": "Human experts tend to cite direct, multi-source evidence and include images/data; AIS (attribution) is complementary but orthogonal — attributable quotes do not guarantee epistemological adequacy.",
            "recommendations_or_best_practices": "Use grounded evidence retrieval to assist raters; separate out and report multiple epistemological sub-dimensions instead of a single score; consider longer/structured responses or follow-up prompts to allow completeness and uncertainty expression.",
            "uuid": "e9687.1",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AI Assistance (Scalable Oversight Protocol)",
            "name_full": "AI Assistance for Scalable Oversight (model-generated critiques with retrieved evidence)",
            "brief_description": "A practical, scalable oversight protocol where GPT-4 generates dimension-specific critiques and, for epistemological dimensions, provides verbatim Wikipedia evidence to help human raters detect issues, increasing recall of real problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (used to generate auxiliary critiques/evidence); evaluated across multiple LLMs",
            "llm_description": "GPT-4 used as a single LLM to extract keypoints, propose supporting Wikipedia URLs, rank paragraphs, and synthesize per-dimension critiques quoted with evidence.",
            "scientific_domain": "Evaluation methodology for LLM outputs in climate science communication",
            "evaluation_method": "Human raters shown LLM answer plus AI Assistance (critiques + retrieved paragraphs) when available; study compared settings with and without assistance and with prior exposure to assistance.",
            "evaluation_criteria": "Effectiveness measured by number of issues detected per dimension, detection recall on synthetic gold examples, and rater-reported helpfulness correlated with rating behavior.",
            "benchmark_or_dataset": "Applied to the 300-question dataset and to a validation set of 30 manually generated examples illustrating specific issues.",
            "results_summary": "AI Assistance increased issue detection: majority-of-three raters detected 77% of known issues with assistance vs 60% without. Raters with prior exposure performed intermediate. When assistance judged unhelpful, raters gave higher ratings, indicating critical appraisal rather than blind following.",
            "limitations_or_challenges": "Assistance can influence raters beyond surfacing issues (potential bias); model-generated critiques can themselves be incorrect and mislead; need to study and mitigate these effects.",
            "comparison_to_human_or_traditional": "Extends classical human review by amplifying rater recall and enabling non-experts to approach expert-level detection when guided; analogous to sandwiching/single-turn debate paradigms in scalable oversight literature.",
            "recommendations_or_best_practices": "Always provide grounded evidence with critiques for epistemological assessments; train raters with exposure to assistance and admission tests; validate assistance on gold examples; study mitigation for undue influence.",
            "uuid": "e9687.2",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AIS (Attribution to Identified Source)",
            "name_full": "Attribution to Identified Source (AIS) evaluation",
            "brief_description": "An attribution-focused evaluation that checks whether an LLM's answer includes explicit quotes from existing documents that support its claims; used here to compare attribution signals against epistemological ratings.",
            "citation_title": "Measuring Attribution in Natural Language Generation Models",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (used for AIS experiment and for producing candidate Wikipedia URLs/evidence); applied to answers from multiple evaluated LLMs but AIS experiment run only on GPT-4 answers here.",
            "llm_description": "GPT-4 generated candidate Wikipedia article URLs and extracted evidence paragraphs; human annotators judged per-keypoint support (fully/partially/not supported).",
            "scientific_domain": "NLP factuality and grounding evaluation in climate information",
            "evaluation_method": "Human annotation of keypoint-to-evidence support; answer-level attribution defined as all keypoints supported (fully/partially), otherwise partial/none.",
            "evaluation_criteria": "Fraction of keypoints fully/partially supported; percent of answers fully/partially attributed; Spearman correlations with epistemological dimensions.",
            "benchmark_or_dataset": "Keypoints and evidence produced for 300 answers (87.7% of examples produced a valid Wikipedia article); human judgments on support for each keypoint.",
            "results_summary": "66.79% of keypoints were fully or partially supported by retrieved evidence; 46.08% of answers were fully or partially supported at answer level. Correlation between AIS and epistemological ratings was near zero (Spearman coefficients: accuracy 0.03, specificity -0.06, uncertainty 0.002, completeness -0.02), indicating AIS and epistemological adequacy are mostly orthogonal.",
            "limitations_or_challenges": "AIS evaluates only what is present (not omissions), so it misses missing important information; depends on quality of retrieval and selection of supporting documents; limited to available documents (here Wikipedia).",
            "comparison_to_human_or_traditional": "Complementary to direct epistemological human assessment; attribution can indicate grounding but not guarantee completeness or nuanced uncertainty communication.",
            "recommendations_or_best_practices": "Use AIS as a complement, not a substitute, to epistemological evaluation; ensure retrieval covers diverse, high-quality sources and evaluate omissions separately.",
            "uuid": "e9687.3",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "300-Q Benchmark",
            "name_full": "300-Question Climate Q&A Benchmark (GTrends / SkepticalScience / Synthetic Wikipedia)",
            "brief_description": "A curated evaluation set of 300 climate-related user information needs: 100 high-volume Google Trends questions, 100 rephrased Skeptical Science myth-questions, and 100 synthetic questions generated from Wikipedia paragraphs and filtered for context and specificity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Answers generated from multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat) using a uniform 3-4 sentence answer prompt.",
            "llm_description": "Uniform short-answer prompt: 'You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph.' Answers used for human evaluation.",
            "scientific_domain": "Climate science communication / NLP evaluation",
            "evaluation_method": "Human rating of each answer across presentational and epistemological dimensions (3 raters per answer), with auxiliary AI Assistance for epistemological dimensions.",
            "evaluation_criteria": "The 8 dimensions (4 presentational + 4 epistemological) and 30 finer issues used to annotate failures (see Table 7 for issue taxonomy).",
            "benchmark_or_dataset": "The 300-question dataset itself; Wikipedia was used for evidence retrieval for assistance/AIS.",
            "results_summary": "Dataset supported detection of consistent patterns: high presentational vs low epistemological quality across models; no large differences by question source, though Wikipedia-sourced questions scored slightly lower.",
            "limitations_or_challenges": "No gold-truth answers exist; synthetic questions filtered but still may bias models; short answer length constrains certain epistemological criteria.",
            "comparison_to_human_or_traditional": "No analogous standardized climate QA benchmark existed; the dataset is positioned similarly to domain-specific evaluation sets (e.g., medical benchmarks) but for public climate information.",
            "recommendations_or_best_practices": "Use a diverse set of question sources and pre-filter for context-independence; include topic and causal/prediction stratification when sampling; provide the exact evaluation protocols and prompts for reproducibility.",
            "uuid": "e9687.4",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Human Evaluation Protocol",
            "name_full": "Human Evaluation Protocol with Educated Raters and Admission Testing",
            "brief_description": "A carefully controlled human evaluation setup using raters with relevant educational background, a brief training tutorial, an admission test to qualify raters, and 3-rater redundancy per answer, allowing partial consultation of external sources but discouraging extensive research.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applied to answers from multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat).",
            "llm_description": "Not an LLM itself; protocol governs human ratings of LLM outputs produced with a short expert prompt.",
            "scientific_domain": "Human evaluation methodology for NLP in climate communication",
            "evaluation_method": "Three human raters per answer; raters selected by relevant education; admission test and tutorial; ratings collected with and without AI Assistance to measure Assistant effects; inter-rater agreement computed (Krippendorff's alpha referenced).",
            "evaluation_criteria": "Consistency (three raters), use of AI Assistance exposure conditions (no assistance, no assistance but prior exposure, with assistance) to measure influence, and validation on 30 hand-crafted examples.",
            "benchmark_or_dataset": "Applied to the 300-question benchmark; validation set of 30 manually generated issue-exhibiting examples used to measure assistance recall.",
            "results_summary": "Assistance improved detection recall (77% vs 60% on validation examples); medium inter-rater agreement at answer-level but robust at system-level; raters with prior exposure to assistance detected more issues even when assistance unavailable.",
            "limitations_or_challenges": "Subjectivity in dimensions leads to noise; lack of gold ratings complicates calibration; potential influence of assistance beyond issue discovery.",
            "comparison_to_human_or_traditional": "Improves classical human-only review by increasing issue recall and allowing scalable oversight, but introduces new biases and dependencies on model-generated assistance.",
            "recommendations_or_best_practices": "Use admission testing, domain-educated raters, and redundancy; include validation examples; monitor and study effects of assistance exposure and potential undue influence.",
            "uuid": "e9687.5",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Keypoints + Retrieval Pipeline",
            "name_full": "Keypoint Extraction and Wikipedia Retrieval Pipeline (GPT-4 grounded evidence)",
            "brief_description": "A reproducible auxiliary-data pipeline where GPT-4 extracts 1–3 keypoints per answer, proposes supporting Wikipedia URLs, ranks article paragraphs for relevance, and supplies the top-3 paragraphs per keypoint as verbatim evidence for AI Assistance and AIS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (used for keypoint extraction, URL proposal, paragraph ranking, and AI Assistance generation)",
            "llm_description": "GPT-4 used as a consistent single-model retrieval/evidence selection system (not a general web retriever); evidence limited to Wikipedia for uniform quality.",
            "scientific_domain": "Evidence grounding and retrieval for evaluation of LLM outputs in climate communication",
            "evaluation_method": "Automatic keypoint extraction from answers, model-suggested Wikipedia article selection, paragraph-level relevance scoring and top-3 paragraph selection per keypoint; paragraphs included in AI Assistance for raters.",
            "evaluation_criteria": "Coverage of keypoints by retrieved evidence, fraction of valid Wikipedia article suggestions (87.7% produced valid articles), and usefulness in improving issue detection.",
            "benchmark_or_dataset": "Applied to all 300 Q&A pairs; retrieval limited to Wikipedia paragraphs from selected articles.",
            "results_summary": "Pipeline effective: GPT-4 suggested valid Wikipedia articles for 87.7% of questions; using keypoints improved retrieval effectiveness over naive retrieval; retrieved evidence supported 66.79% of keypoints (fully/partially).",
            "limitations_or_challenges": "Constrained to Wikipedia (limits source diversity); retrieval quality depends on GPT-4 URL proposal; AIS and retrieval miss missing information and cannot penalize omitted relevant knowledge.",
            "comparison_to_human_or_traditional": "More scalable than manual evidence curation and often superior to off-the-shelf sparse/dense retrieval in this setup, but still relies on LLM-produced retrieval cues.",
            "recommendations_or_best_practices": "Use keypoint-based retrieval rather than whole-answer retrieval; limit evidence sources to uniform high-quality repositories if consistency desired; validate retrieval choices and assess coverage of missing information.",
            "uuid": "e9687.6",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Dimension-aware Prompting",
            "name_full": "Dimension-aware Prompting Experiment",
            "brief_description": "An experiment that augments the LLM prompt with explicit descriptions of the evaluation criteria (the dimensions) to test whether prompting the model with evaluation-aware instructions improves epistemological quality of answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (experiment performed with GPT-4, November 2023)",
            "llm_description": "GPT-4 prompted with either 'basic' or 'dimension-aware' prompts; results compared on presentational and epistemological metrics.",
            "scientific_domain": "Prompt engineering for improving epistemological qualities of LLM outputs in climate communication",
            "evaluation_method": "Compare ratings for GPT-4 answers produced under basic prompt vs dimension-aware prompt across the eight evaluation dimensions, using same human rater protocol.",
            "evaluation_criteria": "Change in mean ratings per dimension (presentational and epistemological) and statistical significance.",
            "benchmark_or_dataset": "Subset of the same 300-question dataset; experiment executed in November 2023 (post a major GPT-4 release).",
            "results_summary": "Including evaluation-criteria descriptions improved performance on difficult epistemological dimensions and tone, but sometimes degraded presentational quality (trade-off observed).",
            "limitations_or_challenges": "Improvements may introduce trade-offs with surface form; experiment limited to GPT-4 and cannot generalize without further testing; timing differences (experiment after GPT-4 update) complicate direct comparisons.",
            "comparison_to_human_or_traditional": "Shows that explicitly instructing models about human evaluation criteria can partially substitute for human guidance in improving epistemological content, but does not replace human expert review.",
            "recommendations_or_best_practices": "Use dimension-aware prompts to bias generation toward epistemological adequacy, but monitor for adverse effects on presentation; combine with post-generation grounding and human verification.",
            "uuid": "e9687.7",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Statistical Analysis & Metrics",
            "name_full": "Statistical Evaluation Suite (bootstrapped means, pairwise t-tests, Krippendorff's alpha, issue taxonomy)",
            "brief_description": "The paper's quantitative evaluation approach: bootstrapped mean ratings with 95% CIs per model/dimension, pairwise t-tests comparing models, calculation of inter-rater agreement (Krippendorff's alpha), and a fine-grained issue taxonomy (30 issues across 8 dimensions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applied across evaluations of multiple LLMs (GPT-4, ChatGPT-3.5, InstructGPT variants, PaLM-2, Falcon-180B-Chat)",
            "llm_description": "Not an LLM but a statistical evaluation toolkit applied to human ratings of LLM outputs.",
            "scientific_domain": "Experimental evaluation and statistical analysis for NLP and science communication assessments",
            "evaluation_method": "Compute bootstrapped means and 95% CIs for each dimension/model (Tables 3–4), run pairwise t-tests to indicate significant differences (Tables 5–6), report issue frequencies (Table 7), and compute inter-rater reliability.",
            "evaluation_criteria": "Statistical significance thresholds (p&lt;0.01, p&lt;0.05), bootstrapped confidence intervals, and issue detection recall in validation tasks.",
            "benchmark_or_dataset": "Applied to the full 300-question evaluation and validation subsets; results reported per-system and per-dimension.",
            "results_summary": "Sufficient resolution to distinguish models on specific dimensions; significant differences found between models on many dimensions; issue taxonomy revealed common failure modes (e.g., vagueness, not enough detail, missing uncertainty).",
            "limitations_or_challenges": "Inter-rater agreement moderate at answer-level; absence of gold standards complicates calibration and absolute performance claims; some statistical comparisons confounded by GPT-4 being used to generate auxiliary data.",
            "comparison_to_human_or_traditional": "Provides a robust multi-faceted analysis compared to single-metric automated evaluations; aligns with expert-level evaluation needs in other domains (e.g., medicine) but without a gold ground truth.",
            "recommendations_or_best_practices": "Report bootstrapped CIs, use multiple statistical comparisons, include fine-grained issue taxonomies, and validate evaluation resolution with pairwise tests and human validation examples.",
            "uuid": "e9687.8",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring Attribution in Natural Language Generation Models",
            "rating": 2,
            "sanitized_title": "measuring_attribution_in_natural_language_generation_models"
        },
        {
            "paper_title": "Concrete problems in AI safety",
            "rating": 2,
            "sanitized_title": "concrete_problems_in_ai_safety"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Evaluating verifiability in generative search engines",
            "rating": 2,
            "sanitized_title": "evaluating_verifiability_in_generative_search_engines"
        },
        {
            "paper_title": "AI safety via debate",
            "rating": 1,
            "sanitized_title": "ai_safety_via_debate"
        },
        {
            "paper_title": "Large language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Med-PaLM: Large language models encode clinical knowledge",
            "rating": 1,
            "sanitized_title": "medpalm_large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "The Oxford Handbook of the Science of Science Communication",
            "rating": 1,
            "sanitized_title": "the_oxford_handbook_of_the_science_of_science_communication"
        }
    ],
    "cost": 0.019209499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing Large Language Models on Climate Information</h1>
<p>Jannis Bulian<em> ${ }^{</em> 1}$ Mike S. Schäfer<em>2 Afra Amini ${ }^{</em> 13}$ Heidi Lam ${ }^{<em> 4}$ Massimiliano Ciaramita ${ }^{</em> 1}$ Ben Gaiarin ${ }^{<em> 1}$ Michelle Chen Hübscher</em> ${ }^{<em> 1}$ Christian Buck ${ }^{</em> 1}$ Niels G. Mede ${ }^{<em> 2}$ Markus Leippold ${ }^{</em> 15}$ Nadine Strauß ${ }^{* 2}$</p>
<h4>Abstract</h4>
<p>As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.</p>
<h2>1. Introduction</h2>
<p>As concerns around climate change intensify (Poushter et al., 2022; WHO, 2021), more and more people turn to digital media as their primary source of information (Newman et al., 2021). However, in spite of ubiquitous access to information, there remains a considerable gap in climate literacy, exacerbated by the spread of mis- and disinformation (Leiserowitz et al., 2022). The challenge of conveying climate data arises also from the nature of scientific communication: science, as an evolving domain, is laden with specialized knowledge,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>complexity, and inherent uncertainties (Moser, 2016). The digital media landscape, characterized by soaring amounts of AI-generated content (Thompson et al., 2024), limited attention spans and adversarial dynamics, further compounds these challenges (Pearce et al., 2019).</p>
<p>While AI's promise in addressing global climate challenges is evident through its applications in climate modeling, energy optimization, and disaster management (Rolnick et al., 2022), its intersection with Natural Language Processing (NLP) is still under-explored. Given recent advancements in LLMs (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Gemini Team, 2023) there is hope that generative AI will also help addressing climate information challenges. However, using LLMs to address science-related information raises factuality concerns (Weidinger et al., 2021). Eloquence and advanced dialogue behaviors are trusted by users, even in the absence of trustworthy information (Chiesurin et al., 2023). This makes evaluating LLMs difficult. Research on evaluating systems that may achieve or exceed human abilities, or scalable oversight (Amodei et al., 2016) is so far mostly theoretical (Irving et al., 2018; Leike et al., 2018; Christiano et al., 2018), with some recent more practical advances (Michael et al., 2023).</p>
<p>We introduce a framework based on Science Communication research (Jamieson et al., 2017), to begin evaluating LLMs' responses within the climate change context in a principled way. ${ }^{1}$ The evaluation relies on raters with relevant educational background. We assess presentational properties such as style, clarity, linguistic correctness, and tone. More importantly, we also assess epistemological issues: accuracy, specificity, completeness, and uncertainty. To test the relevance of the evaluation, we run an empirical study on a diverse set of 300 climate change-related questions involving some of the most recent and prominent LLMs.</p>
<p>Our main findings are as follows:</p>
<ul>
<li>To increase the recall of detected issues and improve rating quality, it is crucial to introduce scalable oversight protocols that use grounded AI Assistance (cf. Figure 1). However, while AI assistance demonstrably</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of the evaluation pipeline as described in Section 3. Starting with a question-answer pair, we use an LLM to extract key points from the answer. We also use the LLM to find a relevant Wikipedia page from which we extract paragraphs. For each key point we rank the paragraphs and keep the top ones. We combine all this information to generate AI assistance for each of our evaluation dimensions. Presentational dimensions are evaluated without the additional paragraphs. This assistance, if available, is presented to our human raters along with the answer. Note that the raters for presentational and epistemological dimension are not shown the key points or retrieved paragraphs. We however use the key points and paragraphs evaluate attribution, cf. Appendix A.12.
improves rating quality, its influence on raters extends beyond this enhancement. Understanding and mitigating these broader effects remains an open question for future research.</p>
<ul>
<li>Results suggest that the epistemological quality of responses on climate information of current LLMs is substantially lower than the presentational quality.</li>
<li>We find preliminary evidence that summarizing the evaluation dimensions in the prompt can improve performance on the epistemological dimensions.</li>
<li>We notice trade-offs between dimensions. Most notably, there seems to be a trade-off between epistemological and presentational quality.</li>
<li>We analyze the relation of our evaluation and attribution-based evaluations of LLMs (Rashkin et al., 2022) and find that they emerge as mostly orthogonal and complementary.</li>
</ul>
<h2>2. Evaluative Dimensions</h2>
<p>Scholarship on science communication - originating from disciplines such as communication science, sociology, psychology, human geography, and education, among others (Trench \&amp; Bucchi, 2021; Nisbet et al., 2018; Jamieson et al., 2017) - offers conceptual arguments and empirical evidence for appropriately disseminating scientific information, e.g., on climate change, to the general public (König et al., 2023;</p>
<p>Lewis Jr. \&amp; Wai, 2021). Building on this knowledge, we distinguish between two basic dimensions. (1) Presentational features of the message that address its comprehensibility (Lang, 2000). (2) Epistemological features aiming to capture the degree to which the conveyed information represents current scientific knowledge adequately and comprehensively, while being specific and appropriately communicating associated uncertainties (Fähnrich et al., 2023).</p>
<h3>2.1. Presentational Adequacy</h3>
<p>An adequate presentation should comply with three criteria (Jamieson et al., 2017): (1) be comprehensible, (2) aid understanding through layout and visualizations, and (3) use appropriate sources and references. Here we focus primarily on comprehensibility, evaluated along four criteria: style, clarity, linguistic correctness, and tone.</p>
<p>Style. The language should not be too informal or colloquial (Mazer \&amp; Hunt, 2008), as this can undermine the credibility of information (Scharrer et al., 2012). Answers should not be too short or too long: brief snippets of information can lead to a "feeling of knowing" (Leonhard et al., 2020), while long texts require motivation and cognitive resources that readers may not want to invest (Lang, 2000). In addition, we borrow some stylistic dimensions from the Multidimensional Quality Metrics (MQM) framework for the evaluation of translations (Lommel et al., 2013).</p>
<p>Clarity. Responses should be concise and clearly formulated (Maibach et al., 2023). The use of jargon and diffi-</p>
<p>cult technical content should be avoided (Baram-Tsabari \&amp; Lewenstein, 2013; Baram-Tsabari et al., 2020).
(Linguistic) Correctness. As in MQM, messages should adhere to linguistic conventions, i.e., the correct use of punctuation, spelling, and grammar. ${ }^{2}$ Violations can damage perceived credibility (Berger, 2020; Mollick, 2014)</p>
<p>Tone. The tone of a message concerns its perceived neutrality, its persuasiveness and its positivity or negativity. Science communication, especially climate-related, can be more effective if it doesn't lean towards a certain valence, worldview, or ideological conviction (Blanton \&amp; Ikizer, 2019; Yuan \&amp; Lu, 2020; Kerr et al., 2022; Munoz-Carrier et al., 2020). Likewise, messages should not use too positively or negatively valenced language, particularly if the goal is to convey factual information (Palm et al., 2020).</p>
<h3>2.2. Epistemological Adequacy</h3>
<p>The epistemological adequacy of climate-related messages is of greatest importance. This entails several aspects: (1) accuracy, (2) specificity, (3) completeness, (4) the degree of (un)certainty, and (5) the presentation of methods and methodology. Here we focus on the first four.</p>
<p>Accuracy. Scientific information should be accurate (Kelesidou \&amp; Chabrol, 2021). This is crucial, considering known issues of LLMs such as hallucination (Schäfer, 2023; Ji et al., 2023). We identify issues that deal with incorrect, wrong, or self-contradictory information, as well messages that take scientific findings, or anecdotal evidence, out of context (Hinnant et al., 2016).</p>
<p>Specificity. Information that is relevant to the audience should not be missed, while ignoring irrelevant information. Responses should address the spatial and temporal context; as specific, local information leads to higher perceived relevance (Lee et al., 2015; Leiserowitz \&amp; Smith, 2017; Holmes et al., 2020). In the absence of a specific time frame, the answer should generally be based on up-to-date knowledge.</p>
<p>Completeness. Rather than only referring to a part of the question posed, answers should be formulated in a way that addresses all aspects of the question in full (Leiserowitz \&amp; Smith, 2017; Bergquist et al., 2022). At the same time, the information given should reflect the depth and breadth of relevant scientific knowledge available regarding the topic(s) addressed (Kelesidou \&amp; Chabrol, 2021).</p>
<p>Uncertainty. Communicating the level of agreement and confidence regarding scientific findings, and supporting evidence, can be crucial to adequately informing the audience (Howe et al., 2019; Budescu et al., 2012; Keohane et al., 2014). This is particularly important in climate communication (Maertens et al., 2020; Chinn \&amp; Hart, 2021; Goldberg</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2022), scientific consensus on climate change has been found to function as a "gateway belief" and motivate public action (van der Linden et al., 2015).</p>
<h3>2.3. Aggregation of scores across dimensions</h3>
<p>In this paper we don't address the important question of how individual dimensions should be combined in a single metric, e.g., for model selection and benchmarking. This is a complex topic which requires assigning a value to each individual dimension. We also believe that the combination of these scores will vary by application.</p>
<h2>3. Human Evaluation Study</h2>
<p>We test our evaluative dimensions in a human rating study. The rating task involves evaluating an answer based on the presentational (Section 2.1) and epistemological dimensions (Section 2.2). Screenshots of the template can be found in Appendix A.10. We select candidate raters with relevant educational background (see Appendix A.7). To be admitted, after finishing a brief tutorial, the raters need to pass an admission test (see Appendix A.9). A summary of the broad demographics of participants can be found in Appendix A.7. Each answer is assessed by three human raters. We don't discourage brief consultations of external sources to clarify specific points but advise against extensive research.</p>
<h3>3.1. Question and Answer Data</h3>
<h3>3.1.1. QUESTIONS</h3>
<p>A comprehensive evaluation would ideally cover a broad spectrum of information needs, including the basics of climate science, mitigation and adaptation, as well as contextspecific issues; e.g., to address the concerns of vulnerable or under-resourced communities (Amini et al., 2023). However, no standardized tests exist to assess climate-related knowledge; in contrast to e.g. the medical domain (Singhal et al., 2023). Hence, we begin by creating a diverse set of 300 questions about topics that are either popular among search users, controversial or context-specific.</p>
<p>We collect questions from three different sources. For the first set, we use Google Trends, which provides data on public interest in specific search topics. ${ }^{3}$ We collect the most popular questions, by search volume, from the U.S., for the topics 'Climate Change' and 'Global Warming' for 2020-2022. For the second set, we turn to Skeptical Science, a website that publishes authoritative information about climate science. We take the list of debated myths ${ }^{4}$ and manually rephrase them as questions. Lastly, we use GPT-4 to</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>generate synthetic questions from the English Wikipedia. We manually select a list of articles related to climate change (e.g., "Global Dimming", "Polar Amplification"), or discuss the topic in specific locations (e.g., "Climate Change in [COUNTRY]"), for a total of 139 articles. Then we split the documents in paragraphs and ask GPT-4 to generate questions that can be answered by the paragraph. We apply several filters to assure that the Wikipedia questions are not overly dependent on the context and are therefore answerable only from the given paragraph. See Appendix A.3.1 for more details and a discussion of filtering choices.</p>
<p>We post-process all questions to remove duplicates, questions that are not related to climate change, or taken out of context. Finally, we sample 100 questions from each set.</p>
<h3>3.1.2. ANSWERS</h3>
<p>Generated answers can display a great deal of variation depending on prompt engineering, reasoning schemes, incontext learning, etc. However, a direct question is the most common way for users to get answers from LLMs. As such, a plain question provides a valuable baseline, reducing variance due to individual LLM's skills and optimization effort, and limiting confounding factors. To obtain answers we use a simple prompt consisting of the instruction: You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph. We include the answer length information to anchor the expected response to an objective value.</p>
<h3>3.2. Auxiliary Data</h3>
<p>We support raters with AI Assistance, consisting of a modelgenerated critique for each evaluated dimension. For epistemological dimensions the assistance is grounded in verbatim evidence from relevant passages extracted from Wikipedia articles. To produce all necessary auxiliary data we carefully design a simple, robust baseline system (Figure 1), which relies on a single LLM. For consistency, we always use GPT-4 for this purpose. Besides testing our evaluation we also run a comparison with an attribution-based evaluation (AIS) (Rashkin et al., 2022), on the same data.</p>
<p>Keypoints. To find supporting evidence for an answer, for AI Assistance and AIS evaluation (Section 4.6), we extract keypoints from each answer. To do so, we instruct GPT-4 to examine all the statements in the answer, and identify one to three key statements that are made in answering the question. We find this to provide better signal to retrieve evidence (see the next paragraph) than either using the whole answer or all sentences individually (Liu et al., 2023).</p>
<p>Evidence. For each keypoint we fetch evidence from Wikipedia. Given the question and the answer, we first ask GPT-4 to provide the URL of a Wikipedia article that
supports the answer. See Table 8 for the exact prompt. We limit evidence to Wikipedia because GPT-4 is fairly consistent in generating relevant, valid Wikipedia URLs, while the quality is lower for the unrestricted web. Furthermore, Wikipedia is uniform in style and quality as it adheres to established guidelines. ${ }^{5}$ While random web pages can vary significantly in content and presentation quality.</p>
<p>We break down the relevant article into its paragraphs. For each keypoint, we ask the model to score the paragraphs based on their relevance to the keypoint and the question. We pick the 3 highest scoring ones as evidence (cf. Table 18 for an example). We find that using keypoints, in combination with URL generation and evidence selection, is a simple and effective solution. In particular, we find this to work better than off-the-shelf sparse or dense retrieval (e.g., using BM25/GTR (Ni et al., 2022)) over Wikipedia passages.</p>
<p>AI Assistance. To assist human raters, we use GPT-4 to critique the answer along the dimensions introduced in Section 2. For each dimension, we ask the model to express its agreement or disagreement that the information is presented well according to that dimension. For epistemological dimensions, we also provide the retrieved evidence and instruct the model to quote the evidence verbatim to support its disagreement (if any).</p>
<p>Please refer to Table 8 for a complete list of prompts used to generate the data, and to Appendix A. 5 for some statistics of the generated answers.</p>
<h2>4. Experimental Results</h2>
<p>Here we present the findings from the experiments using Figure 2 as a summary. Full results tables, including confidence intervals, are reported in Table 3 and Table 4. We also report pairwise LLM t-tests in Tables 5 and 6. We compute pairwise distance and Krippendorff's alpha agreement metrics for all experiments in Appendix A.11, including an analysis of rating timing Appendix A.16. Accurate rating of climate information is challenging, but we find the main conclusion proposed below to be adequately supported.</p>
<p>LLMs. We evaluate the following models: GPT-4 (OpenAI, 2023), ChatGPT-3.5, InstructGPT (turbo), InstructGPT (text-davinci-003), InstructGPT (text-davinci-002) ${ }^{6}$, as well as PaLM2 (text-bison) (Anil et al., 2023) and Falcon-180B-Chat ${ }^{7}$. This data was collected in the months of September and October 2023.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Bootstrapped mean rating, and 95\% confidence intervals, for all presentational (left) and epistemological (right) dimensions.</p>
<h3>4.1. Performance Results</h3>
<p>Presentational Results. Overall, except for InstructGPT (text-davinci-002), LLMs produce clear, fluent, linguistically correct text. This confirms how far LLMs have come in terms of surface form quality, seemingly thanks to RLHF (Ouyang et al., 2022). We note, however, a marked performance drop for tone. This suggests that the evaluation of LLM's presentation should probably shift its focus on subtler aspects of language use (cf. also Section 4.4).</p>
<p>Epistemological Results. Compared to presentation, the epistemological evaluation reveals lower performance across all models and dimensions. Results are consistently low for the last three: specificity, completeness and uncertainty. We note that these dimensions may be difficult to satisfy in short 3-4 sentence answers. Being comprehensive in such a short space may be harder than being accurate.</p>
<p>On the other hand, LLMs don't seem to make a good use of space (see Section 4.4). Thus, space constraints alone do not seem sufficient to explain the result. Overall, on climate information, current top-of-the-line LLMs have significant headroom for improvement. For examples, please see Tables 27 to 30.</p>
<p>Dimension-Aware Prompts. In a follow-up experiment, using only GPT-4 ${ }^{8}$, we found that including a description of the evaluation criteria in the prompt can improve performance on the difficult dimensions: epistemological and tone. Table 1 compares GPT-4 with either the 'basic' or 'dimension-aware' prompts (see Table 8 for the actual</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Table 1. GPT-4 with basic vs. evaluation-aware prompts.
prompt's text). Interestingly, better performance on "knowledge" comes at the cost of worse quality in the presentational dimensions (except for tone), providing additional evidence for the existence of intrinsic tradeoffs between surface and epistemological quality.</p>
<h3>4.2. Resolution and Range.</h3>
<p>The results suggest that the evaluation possesses sufficient resolution to distinguish models on specific dimensions, indicate where they differ and suggest trends. For instance, ChatGPT is the best on all presentation dimensions, except tone, but ranks between position 4 and last on the epistemological scores. This indicates the possibility of complex dependencies between presentational and epistemological properties. As a matter of fact, GPT-4 is always better than ChatGPT across the epistemological dimensions but worse on most presentational ones.</p>
<p>The difference between the best LLM and the worst in specific dimensions - e.g., Falcon-180B-Chat and InstructGPT (text-davinci-002) on the epistemologi-</p>
<p>Question: Are we heading towards an ice age?
Answer: No, we are not heading towards an ice age. The current scientific consensus is that human activities, particularly the emission of greenhouse gases, are causing the planet to warm at an unprecedented rate. This rapid warming is leading to a variety of climate changes, including more frequent and severe heatwaves, rising sea levels, and changes in precipitation patterns. While natural processes have triggered ice ages in the past, these are very slow processes that take thousands of years, and the current rapid warming trend is not consistent with the onset of a new ice age.</p>
<p>Table 2. Example of common LLMs’ argumentative style.
cal ones, and, respectively, ChatGPT and InstructGPT (text-davinci-002) on the first three presentational ones-is large compared to the estimated standard deviation, providing evidence that the evaluation has sufficient dynamic range. Noticeably, the most competitive model-at the time of writing-on the epistemological dimensions is an open model, Falcon-180B-Chat. This LLM’s performance may be related to its large size, but we can only speculate as this information is not generally available for all models.</p>
<h3>4.3. Influence of Question Source.</h3>
<p>Comparing the rating outcome by source of the question Skeptical Science, GTrends, and synthetic questions from Wikipedia-we find no major differences, except for slightly lower scores for Wikipedia questions. In particular, we see no evidence that GPT-4 performs better on questions that were generated with GPT-4 compared to the other sources. Similarly, the topic of the question does not show a strong correlation with answer quality. See Appendix A. 13 for additional discussion and figures.</p>
<h3>4.4. Pragmatic Style</h3>
<p>LLMs often fill parts of the answer with somewhat relevant but unspecific, cautionary and sometimes preachy reminders. The example in Table 2 illustrates the point. Due to the highlighted part, the response may result in sub-optimal cooperative conversation, in a Gricean’s sense (Levinson, 1983). For instance, one could argue that the maxim of quantity is being violated ('do not provide more information than required') as most of what follows the first sentence is strictly speaking unnecessary. The maxim of manner ('be relevant') may also be violated: comments on extreme weather and rising sea levels are only loosely related to the question. That space could be used to provide more specific information.</p>
<p>Furthermore, the answer relies generically on the notion of scientific consensus, which happens relatively frequently in our data. Besides the possibility of being superficially</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3. Number of issues detected depending on AI Assistance exposure.
interpreted as an 'argument from authority', research suggests (Orchinik et al., 2023) that the 'consensus' argument can be surprisingly ineffective due to complex belief system underlying how such arguments are processed. Orchinik et al. (2023) argue that perceived scientists credibility, which in turn may depend on general worldview, affects how consensus-based messages are received and receptiveness to future messaging. This presentation style may not appeal to the different audiences of science communication and possibly lead to diminished interest and fatigue (Schäfer et al., 2018). To further complicate the picture, recent work points out that after conversing with AI on climate change, people with a skeptical stance end up dissatisfied but also more supportive of scientific consensus (Chen et al., 2024)</p>
<p>In these respects, LLMs answers differ from some human experts' answers to similar questions. ${ }^{9}$ The latter tend to rely on direct and specific scientific evidence; e.g., in the case of the question above, an expert may cite land, atmospheric and ocean data for temperature trends, from multiple scientific sources. ${ }^{10}$ Our framework captures some of these aspects in dimensions like tone and specificity, but the pragmatics aspects of Generative AI should probably be investigated more directly in the future.</p>
<h3>4.5. Role of AI Assistance.</h3>
<p>We expect human raters to identify more (real) issues with assistance, because it makes them aware of them. We find supporting evidence in two separate experiments.</p>
<p>Figure 3 reports the number of issues detected for each di-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>mension on GPT-4 answers in three different settings, each with a different degree of the raters’ exposure to assistance. ’Without AI Assistance’ refers to a setting where a specific pool of raters is never exposed to rating with AI Assistance. ’Without AI Assistance, but previous exposure’ refers to a setting where no assistance was shown, but the raters have worked on previous studies that included assistance. Lastly, ’With AI Assistance’ denotes the standard setting where assistance is shown anytime is available.</p>
<p>Results suggest that the presence of assistance is key for detecting more issues. This is consistent with the results from <em>Saunders et al. (2022)</em>. Raters with previous exposure to assistance are in a “middle” position: They detect more issues than the assistance-unaware group, but less than the group provided with assistance for the experiment. This suggests that raters learn from repeated exposure to assistance, and show improved performance even when no assistance is present.</p>
<p>Further evidence of the usefulness of AI Assistance comes from our validation experiments (cf. Appendix A.14 for more details). Similar to <em>Saunders et al. (2022)</em>, we want to determine if assistance helps surface real issues, without general access to gold truth in our data. To do this, the authors manually generated 30 different examples, each exhibiting a particular issue. We found that the majority of three raters detected 77% of issues when shown assistance, while the majority of three raters only detected 60% of the issues when not shown assistance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4. The relationship between rating and reported helpfulness of the AI assistance (on the same scale).</p>
<p>In our experiments we collected feedback from raters on the helpfulness of assistance. The data suggests that when raters do not find assistance helpful, they give higher ratings (see Figure 4). This indicates that the raters can think critically about the assistance and do not follow it blindly. These experiments provide evidence that the AI Assistance helps the raters find real issues that they would not have otherwise been reported.</p>
<h3>4.6. Epistemological Adequacy and Attribution</h3>
<p>Grounding LLMs responses in retrieved documents, or Retrieval Augmented Language Models (RALM) <em>(Guu et al., 2020; Lewis et al., 2020)</em>, has been proposed to improve LLMs’ response quality and alleviate factuality limitations <em>(Menick et al., 2022)</em>. Analogously, on the evaluation side, frameworks such as Attribution to Identified Source (AIS) argue in favour of dedicated evaluations that bypass difficult direct factuality assessments <em>(Rashkin et al., 2023; Dziri et al., 2022)</em>: an attributable answer must include an explicit quote, from an existing document. AIS signals can be also modeled automatically <em>(Bohnet et al., 2023)</em> enabling training via reinforcement learning <em>(Roit et al., 2023)</em>.</p>
<p>While evaluating the ability of LLMs to properly ground their statements goes beyond the scope of this paper, we begin examining the relationship between attribution and the epistemological dimensions with an AIS experiment. We run this experiment only on GPT-4.</p>
<p>In our data, each answer is associated with a set of keypoints which, in turn, are used to identify Wikipedia articles that are likely to contain supporting evidence. For 87.7% of the questions, GPT-4 produces a valid Wikipedia article from which evidence passages can be extracted. We evaluate the attribution of each keypoint individually by asking the human annotators whether a keypoint is fully, partially or not supported by the evidence. 66.79% of keypoints are either fully or partially supported. We consider an answer to be fully attributed if all its keypoints are supported. An answer is not supported if all its keypoints are not supported. At the answer level, 46.08% of the answers are fully or partially supported by the evidence. While providing only preliminary evidence, the data suffices for a first analysis.</p>
<p>Figure 5 compares the distribution of average epistemological ratings, with respect to the attribution of answers, revealing interesting trends. In both the <em>accuracy</em> and <em>specificity</em> dimensions, we observe that answers that are fully attributed have higher minimum ratings compared to answers that are only partially attributed, or not attributed at all. Interestingly, we see an opposite pattern in the <em>completeness</em> dimension: Answers that are fully attributed have lower minimum ratings on <em>completeness</em>. This result highlights a blind spot for attribution methods; AIS can only consider</p>
<p>For instance, as proposed by <em>Liu et al. (2023)</em>, this may involve evaluating generative search engines.</p>
<p><sup>11</sup>We do make sure that the raters have not worked on the same examples before and have never seen assistance for the specific examples they are working on.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5. Comparing AIS ratings with average ratings of the 4 epistemological dimensions.
what is included in the answers, and not what important information is missing. In the uncertainty dimension, we observe that there are more answers with low uncertainty ratings among the answers that are not attributed, compared to answers that are either partially or fully attributed.</p>
<p>More generally, there does not seem to be any correlation between AIS and epistemological results. The Spearman's coefficient between AIS and the 3 -raters mean rating value for accuracy, specificity, uncertainty and completeness are, respectively: $0.03,-0.06,0.002,-0.02$, with corresponding p -values: $0.65,0.31,0.97,0.78$. We interpret this as evidence that AIS and epistemological assessments are mostly orthogonal. We provide a few examples in Table 19, in particular, of answers that are fully attributable but score low on epistemological dimensions. This suggests that, while practical and complementary, attribution, either human or model-based, is not a substitute for direct epistemological assessment.</p>
<p>Science and climate communication is more likely to be trusted if the source is perceived as credible, engaged and concerned about the audience's interests (Brown \&amp; Bruhn, 2011; Maibach et al., 2023; Hayhoe, 2018). An adequate presentation of climate information should include curated references. In future work we plan to extend our framework to evaluate references in a principled, systematic ways.</p>
<h2>5. Limitations and Future Work</h2>
<p>While our agreement analysis (Appendix A.11) suggests that the evaluation is robust at the system level, the rating dimensions inherently have a subjective component, introducing noise when evaluating at the answer-level. As we do not have access to gold ratings, calibration of raters re-
main an open issue, as reflected by the medium inter-rater agreement discussed in Appendix A.11. Future work should consider explicitly addressing this subjectivity in the data collection process (cf. Rottger et al. (2022)).</p>
<p>AI Assistance is an essential part of our evaluation, because it helps raters identify issues in the answers, particularly for the epistemological dimensions. As Tables 3 and 4 shows, raters would fail to recognize many issues without the AI Assistance ('GPT-4 no assistance'). However, the assistance may also influence the raters beyond enhancing discovery. It may only help in the discovery of some issues but not others. There may also be errors caused by models falsely pointing out issues and wrongly convincing the raters. The issues identified will likely vary by model. There is definitely a need to better understand these issues and to identify mitigation strategies. This links this research to the broader AI alignment field and will be one of the main focuses of our future work. A related topic is the role of LLMs as raters. Preliminary experiments are promising (Appendix A.15). We found that, as with humans, LLMs benefit from AI Assistance and that humans and LLM raters tend to agree on major points.</p>
<p>Ideally, an answer would be tailored towards the audience, and take into account their specific attributes (Hendriks et al., 2016; Klinger \&amp; Metag, 2021). Unless specifically prompted, LLMs do not do this and the evaluation of such setting would introduce additional challenges. Another important area for future work concerns multimodal responses. Research provides abundant evidence on the importance of supplementing textual information with visual aids. (Flemming et al., 2018; Brown \&amp; Bruhn, 2011). Visual complements can be especially useful for understanding quantitative data (Fagerlin \&amp; Peters, 2011) and in the case of limited literacy (Wolf et al., 2010). The abstract nature of climate change, and its distant implications, makes visualization particularly challenging (Schäfer, 2020).</p>
<h2>6. Related Work</h2>
<p>Evaluating LLMs. While LLMs can generate fluent text, responses are not always adequately grounded, attributable to reliable sources, and complete. For instance, Liu et al. (2023) assess four generative search engines and report that, although responses are perceived as high quality, only half are fully supported. Their findings reveal an inverse correlation between fluency/utility and evidential support. Xu et al. (2023) advocate for expert-level human evaluations in question answering, cautioning against over-reliance on single metrics instead of comprehensive assessments.</p>
<p>Another domain that needs expert-level evaluation is the medical domain. Singhal et al. (2023) propose Med-PaLM, an LLM for medical information, and introduces a clini-</p>
<p>cal evaluation framework which covers criteria like alignment with scientific consensus, potential harm, and comprehension. Evaluating LLMs on climate information is another domain that can benefit from expert-level evaluation. However, prior work mainly focused on text classification tasks (Diggelmann et al., 2020; Varini et al., 2020; Coan et al., 2021; Paschoal et al., 2021; Webersinke et al., 2022; Bingler et al., 2022; Spokoyny et al., 2023; Lacombe et al., 2023). This study aims to fill this gap by providing a comprehensive evaluation framework for generative climate information.</p>
<p>Scalable Oversight. This area, introduced by Amodei et al. (2016), studies the question of how to scale human oversight, especially in the setting where evaluating (or supervising) models becomes increasingly difficult. Contributions have initially focused on theoretical proposals for how AI can help humans supervise models that exceed their abilities (Irving et al., 2018; Leike et al., 2018; Christiano et al., 2018). Following Irving et al. (2018), one can see our AI Assistance as a single-turn debate, where the human annotator is shown the answer proposed by the model and a single response to that answer. ${ }^{13}$</p>
<p>Two recent studies provide interesting proofs of concepts for AI Assistance: Bowman et al. (2022) study sandwiching, an approach where non-experts align a model with the help of a model while experts provide validation. They show that non-expert raters perform better on an (artificially) difficult multiple-choice task when interacting with a dialogue agent. Several studies also evaluated short debates in this setting with mixed results (Parrish et al., 2022b;a; Michael et al., 2023). Saunders et al. (2022) report that human raters of summarization tasks produce more critiques when given the opportunity to accept or edit critiques written by a model. Our work contributes a study of a scalable oversight protocol to improve rating quality in a realistic setting.</p>
<p>AI Ratings. Recent studies explore the feasibility of evaluations performed by AI. Kocmi \&amp; Federmann (2023) indicate that LLMs can perform state-of-the-art quality assessment of translations, even without references. Their work has been extended to automatic MQM annotation by Fernandes et al. (2023). Gilardi et al. (2023) reports that ChatGPT has a higher agreement with expert-level raters than with less qualified ones. Chiang \&amp; Lee (2023) argue that humans and LLMs ratings are correlated but point out LLM's factuality and bias limitations. Instead of replacing human raters entirely, in our work we demonstrate the effectiveness of using AI Assistance to aid educated raters.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>7. Conclusion</h2>
<p>We introduce an evaluation framework informed by science communication research and assess LLMs on a first set of climate information needs. The task is difficult for human raters. To support them, an important part of our framework relies on a novel and practical protocol for scalable oversight that leverages AI Assistance. It is important to realize that these are the first results of this kind and more research is needed. In particular, while there is evidence that AI Assistance is valuable, we need to develop a framework to understand and mitigate undesired influence on the raters. Overall, our results suggest that, while presentationally adequate, current LLMs have much room for improvement regarding the epistemological qualities of their outputs. More research is needed to understand and improve these aspects of LLMs.</p>
<h2>Impact Statement</h2>
<p>In this work we present an evaluation framework to assess the quality of answers to climate-related questions. Our evaluation is based on science-communication principles and aims to evaluate responses to genuine information needs of the public. Progress in correctly answering such questions can have a large impact for the dissemination of scientific results and can lead to positive effects on climate literacy, also reducing the public's susceptibility to misinformation.</p>
<p>As with any evaluation there are however limits to its validity. Specifically, the evaluation of systems to be deployed in critical contexts requires additional grounding and expert verification. This is especially the case when system responses inform actions. Moreover, the evaluation is limited to the evaluated context, and we make no claims that models can be trusted and deployed outside of that context.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank the following people for their feedback and support: Leslie Leuenberger, Claire FoulquierGazagnes, Yana Koroleva, Srini Narayanan, Andrew Novikov, Annalisa Pawlosky, Fernando Pereira, Rachana Jayaram, Maria Ryabtseva. We also extend our appreciation to the anonymous reviewers for their valuable suggestions and feedback.</p>
<h2>References</h2>
<p>Amini, A. and Ciaramita, M. Probing in context: Toward building robust classifiers via probing large language models, 2023. URL https://arxiv.org/ abs/2305.14171.</p>
<p>Amini, A., Buck, C., Brown, H., Bulian, J., Huebscher, M. C., Ciaramita, M., Das, S., Gaiarin, B., Gordon, C., Gupta, R., Kutu, K., Lartey, D. L., Leippold, M., Leuenberger, L., and Mensah, M. A. AI and climate information needs in africa. In Deep Learning Indaba 2023, 2023. URL https://openreview.net/forum? $\mathrm{id}=\mathrm{MpUHGbNHbS}$.</p>
<p>Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Mané, D. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http: //arxiv.org/abs/1606.06565.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., MeierHellstern, K., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H., Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and Wu, Y. Palm 2 technical report, 2023.</p>
<p>Baram-Tsabari, A. and Lewenstein, B. V. An instrument for assessing scientists' written skills in public communication of science. Science Communication, 35(1):56-85, 2013. ISSN 1075-5470. doi: 10.1177/ 1075547012440634.</p>
<p>Baram-Tsabari, A., Wolfson, O., Yosef, R., Chapnik, N., Brill, A., and Segev, E. Jargon use in public understand-
ing of science papers over three decades. Public Understanding of Science, 29(6):644-654, 2020. ISSN 09636625. doi: 10.1177/0963662520940501.</p>
<p>Berger, C. R. Planning strategic interaction: Attaining goals through communicative action. Routledge, 2020. ISBN 9781003064190. doi: 10.4324/9781003064190.</p>
<p>Bergquist, P., Marlon, J. R., Goldberg, M. H., Gustafson, A., Rosenthal, S. A., and Leiserowitz, A. Information about the human causes of global warming influences causal attribution, concern, and policy support related to global warming. Thinking \&amp; Reasoning, 28(3):465-486, 2022.</p>
<p>Bingler, J. A., Kraus, M., Leippold, M., and Webersinke, N. Cheap talk and cherry-picking: What climatebert has to say on corporate climate risk disclosures. Finance Research Letters, 47, 2022. URL https://www.sciencedirect.com/ science/article/pii/S1544612322000897.</p>
<p>Blanton, H. and Ikizer, E. G. Elegant science narratives and unintended influences: An agenda for the science of science communication. Social Issues and Policy Review, 13(1):154-181, 2019. ISSN 17512395. doi: 10.1111/sipr. 12055 .</p>
<p>Bohnet, B., Tran, V. Q., Verga, P., Aharoni, R., Andor, D., Soares, L. B., Ciaramita, M., Eisenstein, J., Ganchev, K., Herzig, J., Hui, K., Kwiatkowski, T., Ma, J., Ni, J., Saralegui, L. S., Schuster, T., Cohen, W. W., Collins, M., Das, D., Metzler, D., Petrov, S., and Webster, K. Attributed question answering: Evaluation and modeling for attributed large language models, 2023.</p>
<p>Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiūtė, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Kernion, J., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lovitt, L., Elhage, N., Schiefer, N., Joseph, N., Mercado, N., DasSarma, N., Larson, R., McCandlish, S., Kundu, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z., Mann, B., and Kaplan, J. Measuring progress on scalable oversight for large language models, 2022.</p>
<p>Brown, M. and Bruhn, C. Chapter 11: Information and persuasion. In Fischhoff, B., Brewer, N. T., and Downs, J. S. (eds.), Communicating risks and benefits: An evidencebased user's guide, pp. 101-109. US Department of Health and Human Services, Washington, D.C., 2011.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,</p>
<p>G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf.</p>
<p>Budescu, D. V., Por, H.-H., and Broomell, S. B. Effective communication of uncertainty in the ipcc reports. Climatic Change, 113:181-200, 2012.</p>
<p>Chen, K., Shao, A., Burapacheep, J., and Li, Y. Conversational ai and equity through assessing gpt-3's communication with diverse social groups on contentious topics. Scientific Reports, 14, 2024. URL https: //doi.org/10.1038/s41598-024-51969-w.</p>
<p>Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Chiesurin, S., Dimakopoulos, D., Sobrevilla Cabezudo, M. A., Eshghi, A., Papaioannou, I., Rieser, V., and Konstas, I. The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 947-959, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 60. URL https://aclanthology.org/2023. findings-acl. 60.</p>
<p>Chinn, S. and Hart, P. S. Effects of consensus messages and political ideology on climate change attitudes: inconsistent findings and the effect of a pretest. Climatic Change, 167(3-4):47, 2021.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,
M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Christiano, P. F., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts. CoRR, abs/1810.08575, 2018. URL http://arxiv.org/ abs/1810.08575.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.</p>
<p>Coan, T. G., Boussalis, C., Cook, J., and Nanko, M. O. Computer-assisted classification of contrarian claims about climate change. Scientific Reports, 2021. URL https://doi.org/10.1038/ s41598-021-01714-4.</p>
<p>Diggelmann, T., Boyd-Graber, J., Bulian, J., Ciaramita, M., and Leippold, M. Climate-fever: A dataset for verification of real-world climate claims. In NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning, 2020.</p>
<p>Dziri, N., Rashkin, H., Linzen, T., and Reitter, D. Evaluating attribution in dialogue systems: The begin benchmark. Transactions of the Association for Computational Linguistics, 10:1066-1083, 2022.</p>
<p>Fagerlin, A. and Peters, E. Chapter 7: Quantitative information. In Fischhoff, B., Brewer, N. T., and Downs, J. S. (eds.), Communicating risks and benefits: An evidencebased user's guide, pp. 53-64. US Department of Health and Human Services, Washington, D.C., 2011.</p>
<p>Fähnrich, B., Weitkamp, E., and Kupper, J. F. Exploring 'quality' in science communication online: Expert thoughts on how to assess and promote science communication quality in digital media contexts. Public Understanding of Science, 32(5):605-621, 2023. ISSN 09636625. doi: 10.1177/09636625221148054.</p>
<p>Fernandes, P., Deutsch, D., Finkelstein, M., Riley, P., Martins, A. F. T., Neubig, G., Garg, A., Clark, J. H., Freitag, M., and Firat, O. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation, 2023.</p>
<p>Flemming, D., Cress, U., Kimmig, S., Brandt, M., and Kimmerle, J. Emotionalization in science communication: The impact of narratives and visual representations on knowledge gain and risk perception. Frontiers in Communication, 3:3, 2018.</p>
<p>Gemini Team. Gemini: A family of highly capable multimodal models, 2023.</p>
<p>Gilardi, F., Alizadeh, M., and Kubli, M. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023. doi: 10.1073/pnas. 2305016120. URL https://www.pnas.org/doi/ abs/10.1073/pnas. 2305016120.</p>
<p>Goldberg, M. H., Gustafson, A., van der Linden, S., Rosenthal, S. A., and Leiserowitz, A. Communicating the scientific consensus on climate change: diverse audiences and effects over time. Environment and Behavior, 54 (7-8):1133-1165, 2022.</p>
<p>Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pretraining. In Proceedings of the 37th International Conference on Machine Learning, 2020. URL https://proceedings.mlr.press/v119/ guu20a.html.</p>
<p>Hayhoe, K. When facts are not enough. Science, 360(6392):943-943, 2018. doi: 10.1126/science. aau2565. URL https://www.science.org/ doi/abs/10.1126/science.aau2565.</p>
<p>Hendriks, F., Kienhues, D., and Bromme, R. Trust in science and the science of trust. In Blöbaum, B. (ed.), Trust and communication in a digitized world, pp. 143-159. Springer, Cham, 2016. ISBN 978-3-319-28059-2.</p>
<p>Hinnant, A., Subramanian, R., and Young, R. User comments on climate stories: impacts of anecdotal vs. scientific evidence. Climatic Change, 138(3-4):411-424, 2016. ISSN 0165-0009. doi: 10.1007/s10584-016-1759-1.</p>
<p>Holmes, K. J., Wender, B. A., Weisenmiller, R., Doughman, P., and Kerxhalli-Kleinfield, M. Climate assessment moves local. Earth's Future, 8(2), 2020. ISSN 2328-4277. doi: 10.1029/2019EF001402.</p>
<p>Howe, L. C., MacInnis, B., Krosnick, J. A., Markowitz, E. M., and Socolow, R. Acknowledging uncertainty impacts public acceptance of climate scientists' predictions. Nature Climate Change, 9(11):863-867, 2019.</p>
<p>Irving, G., Christiano, P. F., and Amodei, D. AI safety via debate. CoRR, abs/1805.00899, 2018. URL http: //arxiv.org/abs/1805.00899.</p>
<p>Jamieson, K. H., Kahan, D. M., and Scheufele, D. A. The Oxford Handbook of the Science of Science Communication. Oxford University Press, 2017. URL https://doi.org/10.1093/oxfordhb/ 9780190497620.001 .0001 .</p>
<p>Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023. ISSN 0360-0300. doi: $10.1145 / 3571730$.</p>
<p>Kelesidou, F. and Chabrol, E. (eds.). A comprehensive guide to Science Communication. Hindawi, 2021.</p>
<p>Keohane, R. O., Lane, M., and Oppenheimer, M. The ethics of scientific communication under uncertainty. Politics, Philosophy \&amp; Economics, 13(4):343-368, 2014. ISSN 1470-594X. doi: 10.1177/1470594X14538570.</p>
<p>Kerr, J. R., Schneider, C. R., Freeman, A. L. J., Marteau, T., and van der Linden, S. Transparent communication of evidence does not undermine public trust in evidence. PNAS nexus, 1(5):pgac280, 2022. doi: 10.1093/pnasnexus/ pgac280.</p>
<p>Klinger, K. and Metag, J. Media effects in the context of environmental issues. In Takahashi, B., Metag, J., Thaker, J., and Comfort, S. E. (eds.), The Handbook of International Trends in Environmental Communication, pp. 3149. Routledge, New York, 2021. ISBN 9780367275204.</p>
<p>Kocmi, T. and Federmann, C. Large language models are state-of-the-art evaluators of translation quality, 2023.</p>
<p>König, L. M., Altenmüller, M. S., Fick, J., Crusius, J., Genschow, O., and Sauerland, M. How to communicate science to the public? recommendations for effective written communication derived from a systematic review, Aug 2023. URL psyarxiv.com/cwbrs.</p>
<p>Lacombe, R., Wu, K., and Dilworth, E. Climatex: Do llms accurately assess human expert confidence in climate statements?, 2023.</p>
<p>Lang, A. The limited capacity model of mediated message processing. Journal of Communication, 50(1):46-70, 2000. ISSN 0021-9916. doi: 10.1111/j.1460-2466.2000. tb02833.x.</p>
<p>Lee, T. M., Markowitz, E. M., Howe, P. D., Ko, C.-Y., and Leiserowitz, A. A. Predictors of public climate change awareness and risk perception around the world. Nature Climate Change, 5(11):1014-1020, 2015.</p>
<p>Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: a research direction. CoRR, abs/1811.07871, 2018. URL http://arxiv.org/abs/1811.07871.</p>
<p>Leiserowitz, A. and Smith, N. Affective imagery, risk perceptions, and climate change communication. In Leiserowitz, A. and Smith, N. (eds.), Oxford research encyclopedia of climate science. Oxford University Press, Oxford, 2017. ISBN 9780190228620. doi: 10.1093/ acrefore/9780190228620.013.307.</p>
<p>Leiserowitz, A., Maibach, E., Rosenthal, S., Kotcher, J., Neyens, L., Marlon, J., Carman, J., Lacroix, K., and Goldberg, M. Global warming's six Americas, 2022.</p>
<p>Leonhard, L., Karnowski, V., and Kümpel, A. S. Online and (the feeling of being) informed: Online news usage patterns and their relation to subjective and objective political knowledge. Computers in Human Behavior, 103:181-189, 2020. ISSN 07475632. doi: 10.1016/j.chb. 2019.08.008.</p>
<p>Levinson, S. C. Pragmatics. Cambridge Textbooks in Linguistics. Cambridge University Press, 1983. doi: 10. 1017/CBO9780511813313.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrievalaugmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper. pdf.</p>
<p>Lewis Jr., N. A. and Wai, J. Communicating what we know and what isn't so: Science communication in psychology. Perspectives on Psychological Science, 16(6):1242-1254, 2021. doi: 10.1177/1745691620964062.</p>
<p>Liu, N. F., Zhang, T., and Liang, P. Evaluating verifiability in generative search engines, 2023.</p>
<p>Lommel, A., Burchardt, A., and Uszkoreit, H. Multidimensional quality metrics: a flexible system for assessing translation quality. In Proceedings of Translating and the Computer 35, London, UK, November 28-29 2013. Aslib. URL https://aclanthology.org/2013. tc-1.6.</p>
<p>Maertens, R., Anseel, F., and van der Linden, S. Combatting climate change misinformation: Evidence for longevity of inoculation and consensus messaging effects. Journal of Environmental Psychology, 70, 2020. ISSN 02724944. doi: 10.1016/j.jenvp.2020.101455.</p>
<p>Maibach, E. W., Uppalapati, S. S., Orr, M., and Thaker, J. Harnessing the power of communication and behavior science to enhance society's response to climate
change. Annual Review of Earth and Planetary Sciences, 51(1):53-77, 2023. ISSN 0084-6597. doi: 10.1146/annurev-earth-031621-114417.</p>
<p>Mazer, J. P. and Hunt, S. K. "cool" communication in the classroom: A preliminary examination of student perceptions of instructor use of positive slang. Qualitative Research Reports in Communication, 9(1):20-28, 2008. ISSN 1745-9435. doi: 10.1080/17459430802400316.</p>
<p>Menick, J., Trebacz, M., Mikulik, V., Aslanides, J., Song, F., Chadwick, M., Glaese, M., Young, S., CampbellGillingham, L., Irving, G., and McAleese, N. Teaching language models to support answers with verified quotes, 2022. URL https://arxiv.org/abs/ 2203.11147.</p>
<p>Michael, J., Mahdi, S., Rein, D., Petty, J., Dirani, J., Padmakumar, V., and Bowman, S. R. Debate helps supervise unreliable experts. CoRR, abs/2311.08702, 2023. doi: 10.48550/ARXIV.2311.08702. URL https: //doi.org/10.48550/arXiv.2311.08702.</p>
<p>Mollick, E. The dynamics of crowdfunding: An exploratory study. Journal of Business Venturing, 29(1):1-16, 2014. ISSN 08839026. doi: 10.1016/j.jbusvent.2013.06.005.</p>
<p>Moser, S. Reflections on climate change communication research and practice in the second decade of the 21st century: what more is there to say? Wiley Interdisciplinary Reviews: Climate Change 7(3), 345-369, 2016.</p>
<p>Munoz-Carrier, G., Thomsen, D., and Pickering, G. J. Psychological and experiential factors affecting climate change perception: learnings from a transnational empirical study and implications for framing climate-related flood events. Environmental Research Communications, 2(4), 2020. doi: 10.1088/2515-7620/ab89f9.</p>
<p>Newman, N., Fletcher, R., Schulz, A., Andi, S., Robertson, C. T., and Nielsen, R. K. Reuters institute digital news report 2021, 2021.</p>
<p>Ni, J., Qu, C., Lu, J., Dai, Z., Hernandez Abrego, G., Ma, J., Zhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y. Large dual encoders are generalizable retrievers. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022. URL https://aclanthology. org/2022.emnlp-main.669.</p>
<p>Nisbet, M. C., Ho, S. S., Markowitz, E., O’Neill, S., Schäfer, M. S., and Thaker, J. (eds.). The Oxford encyclopedia of climate change communication. Oxford University Press, New York, 2018. ISBN 9780190498986. doi: 10.1093/acref/9780190498986.001.0001.</p>
<p>OpenAI. GPT-4 technical report, 2023.
Orchinik, R., Dubey, R., Gershman, S. J., Powell, D., and Bhui, R. Learning from and about climate scientists, 2023. URL https://doi.org/10.31234/osf. io/ezua5.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.</p>
<p>Palm, R., Bolsen, T., and Kingsland, J. T. "don't tell me what to do": Resistance to climate change messages suggesting behavior changes. Weather, Climate, and Society, 12(4):827-835, 2020.</p>
<p>Parrish, A., Trivedi, H., Nangia, N., Padmakumar, V., Phang, J., Saimbhi, A. S., and Bowman, S. R. Two-turn debate doesn't help humans answer hard reading comprehension questions. CoRR, abs/2210.10860, 2022a. doi: 10.48550/ ARXIV.2210.10860. URL https://doi.org/10. 48550/arXiv.2210.10860.</p>
<p>Parrish, A., Trivedi, H., Perez, E., Chen, A., Nangia, N., Phang, J., and Bowman, S. R. Single-turn debate does not help humans answer hard reading-comprehension questions. CoRR, abs/2204.05212, 2022b. doi: 10.48550/ ARXIV.2204.05212. URL https://doi.org/10. 48550/arXiv.2204.05212.</p>
<p>Paschoal, A. F. A., Pirozelli, P., Freire, V., Delgado, K. V., Peres, S. M., José, M. M., Nakasato, F., Oliveira, A. S., Brandão, A. A. F., Costa, A. H. R., and Cozman, F. G. Pirá: A bilingual portuguese-english dataset for question-answering about the ocean. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, CIKM '21. Association for Computing Machinery, 2021. doi: 10. 1145/3459637.3482012. URL https://doi.org/ 10.1145/3459637.3482012.</p>
<p>Pearce, W., Niederer, S., Özkula, S. M., and Sánchez Querubín, N. The social media life of climate change: Platforms, publics and future imaginaries. Wiley interdisciplinary reviews: Climate change, 10(2), e569., 2019.</p>
<p>Poushter, J., Fagan, M., and Gubbala, S. Climate change remains top global threat across 19-country survey, 2022.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring attribution in natural language generation models, 2022.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring Attribution in Natural Language Generation Models. Computational Linguistics, pp. 1-64, 2023. URL https://doi.org/10.1162/coli_ a_00486.</p>
<p>Roit, P., Ferret, J., Shani, L., Aharoni, R., Cideron, G., Dadashi, R., Geist, M., Girgin, S., Hussenot, L., Keller, O., Momchev, N., Ramos Garea, S., Stanczyk, P., Vieillard, N., Bachem, O., Elidan, G., Hassidim, A., Pietquin, O., and Szpektor, I. Factually consistent summarization via reinforcement learning with textual entailment feedback. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. URL https://aclanthology. org/2023.acl-long. 344.</p>
<p>Rolnick, D., Donti, P. L., Kaack, L. H., Kochanski, K., Lacoste, A., Sankaran, K., Ross, A. S., Milojevic-Dupont, N., Jaques, N., Waldman-Brown, A., Luccioni, A. S., Maharaj, T., Sherwin, E. D., Mukkavilli, S. K., Kording, K. P., Gomes, C. P., Ng, A. Y., Hassabis, D., Platt, J. C., Creutzig, F., Chayes, J., and Bengio, Y. Tackling climate change with machine learning. ACM Comput. Surv., 55(2), 2022. URL https://doi.org/10.1145/ 3485128.</p>
<p>Rottger, P., Vidgen, B., Hovy, D., and Pierrehumbert, J. Two contrasting data annotation paradigms for subjective NLP tasks. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 175-190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL https: //aclanthology.org/2022.naacl-main.13.</p>
<p>Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators, 2022.</p>
<p>Schäfer, M. S. Introduction to visualizing climate change. In Holmes, D. C. and Richardson, L. M. (eds.), Research handbook on communicating climate change, Elgar handbooks in energy, the environment and climate change, pp. 127-130. Edward Elgar Publishing, Cheltenham, UK, 2020. ISBN 9781789900392.</p>
<p>Schäfer, M. S. The notorious GPT: Science communication in the age of artificial intelligence. Journal of Science Communication, 22(2), 2023. ISSN 1824-2049. doi: 10.22323/2.22020402.</p>
<p>Schäfer, M. S., Füchslin, T., Metag, J., Kristiansen, S., and Rauchfleisch, A. The different audiences of science communication: A segmentation analysis of the swiss population's perceptions of science and their information and media use patterns. Public Understanding of Science, 27(7):836-856, 2018. doi: 10.1177/ 0963662517752886. URL https://doi.org/10. 1177/0963662517752886.</p>
<p>Scharrer, L., Bromme, R., Britt, M. A., and Stadtler, M. The seduction of easiness: How science depictions influence laypeople's reliance on their own evaluation of scientific information. Learning and Instruction, 22(3):231-243, 2012. ISSN 09594752. doi: 10.1016/j.learninstruc. 2011. 11.004.</p>
<p>Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. Large language models encode clinical knowledge. Nature, pp. 1-9, 2023.</p>
<p>Spokoyny, D., Laud, T., Corringham, T., and BergKirkpatrick, T. Towards answering climate questionnaires from unstructured climate reports, 2023.</p>
<p>Thompson, B., Dhaliwal, M. P., Frisch, P., Domhan, T., and Federico, M. A shocking amount of the web is machine translated: Insights from multi-way parallelism, 2024.</p>
<p>Trench, B. and Bucchi, M. (eds.). Routledge handbook of public communication of science and technology. Routledge, Abingdon and New York, 2021. ISBN 9781003039242. doi: 10.4324/9781003039242.
van der Linden, S. L., Leiserowitz, A. A., Feinberg, G. D., and Maibach, E. W. The scientific consensus on climate change as a gateway belief: experimental evidence. PloS One, 10(2):e0118489, 2015. doi: 10.1371/journal.pone. 0118489.</p>
<p>Varini, F. S., Boyd-Graber, J., Ciaramita, M., and Leippold, M. ClimaText: A dataset for climate change topic detection. In NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning, 2020.</p>
<p>Webersinke, N., Kraus, M., Bingler, J., and Leippold, M. ClimateBERT: a pretrained language model for climaterelated text. In Proceedings of AAAI 2022 Fall Symposium: The Role of AI in Responding to Climate Challenges, 2022.</p>
<p>Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., Isaac, W. S., Legassick, S., Irving, G., and Gabriel, I. Ethical and social risks of harm from language models.</p>
<p>CoRR, abs/2112.04359, 2021. URL https://arxiv. org/abs/2112.04359.</p>
<p>WHO. 2021 World Health Organization: health and climate change global survey report, 2021.</p>
<p>Wolf, M. S., Davis, T. C., Bass, P. F., Curtis, L. M., Lindquist, L. A., Webb, J. A., Bocchini, M. V., Bailey, S. C., and Parker, R. M. Improving prescription drug warnings to promote patient comprehension. Archives of internal medicine, 170(1):50-56, 2010. doi: 10.1001/archinternmed.2009.454.</p>
<p>Xu, F., Song, Y., Iyyer, M., and Choi, E. A critical evaluation of evaluations for long-form question answering. arXiv preprint arXiv:2305.18201, 2023.</p>
<p>Yuan, S. and Lu, H. "It's global warming, stupid": Aggressive communication styles and political ideology in science blog debates about climate change. Journalism \&amp; Mass Communication Quarterly, 97(4):1003-1025, 2020.</p>
<h1>A. Appendix</h1>
<h2>A.1. Main Results</h2>
<p>Tables 3 and 4 report the full results for all evaluated models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">style</th>
<th style="text-align: center;">clarity</th>
<th style="text-align: center;">correctness</th>
<th style="text-align: center;">tone</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$4.54[4.50,4.58]$</td>
<td style="text-align: center;">$4.56[4.52,4.60]$</td>
<td style="text-align: center;">$4.58[4.54,4.61]$</td>
<td style="text-align: center;">$3.06[2.99,3.13]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$4.15[4.08,4.22]$</td>
<td style="text-align: center;">$4.43[4.38,4.47]$</td>
<td style="text-align: center;">$4.47[4.42,4.52]$</td>
<td style="text-align: center;">$3.20[3.12,3.28]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-002)</td>
<td style="text-align: center;">$3.22[3.13,3.31]$</td>
<td style="text-align: center;">$3.63[3.55,3.70]$</td>
<td style="text-align: center;">$3.82[3.74,3.90]$</td>
<td style="text-align: center;">$3.17[3.09,3.24]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (turbo)</td>
<td style="text-align: center;">$4.37[4.32,4.42]$</td>
<td style="text-align: center;">$4.40[4.36,4.45]$</td>
<td style="text-align: center;">$4.46[4.42,4.51]$</td>
<td style="text-align: center;">$3.41[3.33,3.48]$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$4.34[4.28,4.40]$</td>
<td style="text-align: center;">$4.48[4.43,4.53]$</td>
<td style="text-align: center;">$4.57[4.53,4.61]$</td>
<td style="text-align: center;">$3.19[3.11,3.27]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: center;">$4.35[4.30,4.40]$</td>
<td style="text-align: center;">$4.34[4.28,4.39]$</td>
<td style="text-align: center;">$4.38[4.34,4.41]$</td>
<td style="text-align: center;">$3.26[3.19,3.34]$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$4.36[4.31,4.41]$</td>
<td style="text-align: center;">$4.39[4.35,4.44]$</td>
<td style="text-align: center;">$4.41[4.36,4.45]$</td>
<td style="text-align: center;">$3.37[3.30,3.45]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance, prev. exposure</td>
<td style="text-align: center;">$4.59[4.54,4.63]$</td>
<td style="text-align: center;">$4.63[4.59,4.68]$</td>
<td style="text-align: center;">$4.66[4.63,4.70]$</td>
<td style="text-align: center;">$3.24[3.16,3.32]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance</td>
<td style="text-align: center;">$4.45[4.41,4.50]$</td>
<td style="text-align: center;">$4.57[4.53,4.61]$</td>
<td style="text-align: center;">$4.74[4.70,4.77]$</td>
<td style="text-align: center;">$4.35[4.29,4.42]$</td>
</tr>
</tbody>
</table>
<p>Table 3. Results along the presentational dimensions, with $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">accuracy</th>
<th style="text-align: center;">specificity</th>
<th style="text-align: center;">completeness</th>
<th style="text-align: center;">uncertainty</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$3.48[3.41,3.55]$</td>
<td style="text-align: center;">$2.71[2.63,2.78]$</td>
<td style="text-align: center;">$2.26[2.20,2.31]$</td>
<td style="text-align: center;">$2.05[2.00,2.09]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$3.52[3.44,3.60]$</td>
<td style="text-align: center;">$2.89[2.81,2.97]$</td>
<td style="text-align: center;">$2.43[2.36,2.50]$</td>
<td style="text-align: center;">$2.18[2.11,2.25]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-002)</td>
<td style="text-align: center;">$2.81[2.73,2.88]$</td>
<td style="text-align: center;">$2.49[2.42,2.56]$</td>
<td style="text-align: center;">$2.32[2.26,2.39]$</td>
<td style="text-align: center;">$2.35[2.29,2.41]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (turbo)</td>
<td style="text-align: center;">$3.65[3.58,3.73]$</td>
<td style="text-align: center;">$2.79[2.71,2.86]$</td>
<td style="text-align: center;">$2.43[2.37,2.50]$</td>
<td style="text-align: center;">$2.24[2.19,2.30]$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$3.47[3.39,3.55]$</td>
<td style="text-align: center;">$2.81[2.73,2.89]$</td>
<td style="text-align: center;">$2.57[2.50,2.65]$</td>
<td style="text-align: center;">$2.25[2.18,2.32]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: center;">$3.67[3.61,3.73]$</td>
<td style="text-align: center;">$3.13[3.05,3.21]$</td>
<td style="text-align: center;">$2.61[2.53,2.68]$</td>
<td style="text-align: center;">$2.21[2.15,2.27]$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$3.81[3.74,3.87]$</td>
<td style="text-align: center;">$3.15[3.07,3.23]$</td>
<td style="text-align: center;">$2.73[2.65,2.80]$</td>
<td style="text-align: center;">$2.55[2.47,2.62]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance, prev. exposure</td>
<td style="text-align: center;">$3.86[3.79,3.93]$</td>
<td style="text-align: center;">$3.43[3.35,3.52]$</td>
<td style="text-align: center;">$3.30[3.21,3.39]$</td>
<td style="text-align: center;">$2.78[2.69,2.87]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance</td>
<td style="text-align: center;">$4.49[4.44,4.55]$</td>
<td style="text-align: center;">$4.41[4.35,4.48]$</td>
<td style="text-align: center;">$4.32[4.25,4.39]$</td>
<td style="text-align: center;">$3.38[3.29,3.46]$</td>
</tr>
</tbody>
</table>
<p>Table 4. Results along the epistemological dimensions, with $95 \%$ confidence intervals.</p>
<h2>A.2. Pairwise t-tests</h2>
<p>Tables 5 and 6 report the results of LLM pairwise t-tests.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">davinci-002</th>
<th style="text-align: center;">InstructGPT <br> davinci-003</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT <br> text-bison</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4 <br> 180B-Chat</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">style</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">clarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">correctness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">tone</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5. Presentational dimensions - Pairwise t-test results. -- and ++ indicate high significance ( $p&lt;0.01$ ) that the model in the row scores lower/higher than the model in the colum. Single $-/+$ indicate $p&lt;0.05$ and $\sim$ indicates no significant difference between the models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">davinci-002</th>
<th style="text-align: center;">InstructGPT <br> davinci-003</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT <br> text-bison</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4 <br> 180B-Chat</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">specificity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">completeness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">uncertainty</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6. Epistemological dimensions. Pairwise t-test results. -- and ++ indicate high significance $(p&lt;0.01)$ that the model in the row scores lower/higher than the model in the colum. Single $-/+$ indicate $p&lt;0.05$ and $\sim$ indicates no significant difference between the models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Issue</th>
<th style="text-align: center;">InstructGPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">davinci-002</td>
<td style="text-align: center;">davinci-003</td>
<td style="text-align: center;">turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">180B-Chat</td>
</tr>
<tr>
<td style="text-align: center;">style</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">inconsistent</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">repetitive</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">too informal</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;">too long</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;">too short</td>
<td style="text-align: center;">10.14</td>
<td style="text-align: center;">8.56</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">clarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">hard math</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">sentences too long</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: center;">too technical</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">8.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">correctness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">incomplete sentence</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">incorrect grammar</td>
<td style="text-align: center;">6.29</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">incorrect punctuation</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;">incorrect spelling</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">tone</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">biased</td>
<td style="text-align: center;">28.50</td>
<td style="text-align: center;">34.44</td>
<td style="text-align: center;">24.78</td>
<td style="text-align: center;">42.38</td>
<td style="text-align: center;">33.85</td>
<td style="text-align: center;">30.33</td>
<td style="text-align: center;">23.72</td>
</tr>
<tr>
<td style="text-align: center;">negative</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1.89</td>
</tr>
<tr>
<td style="text-align: center;">persuasive</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">7.68</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">4.45</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">anecdotal</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">19.24</td>
<td style="text-align: center;">5.90</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">incorrect</td>
<td style="text-align: center;">20.92</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">11.69</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">4.57</td>
</tr>
<tr>
<td style="text-align: center;">science out of context</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">5.69</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">5.35</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">2.56</td>
</tr>
<tr>
<td style="text-align: center;">self contradictory</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">wrong use of terms</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">specificity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">irrelevant info</td>
<td style="text-align: center;">15.15</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">8.69</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">4.01</td>
</tr>
<tr>
<td style="text-align: center;">vague</td>
<td style="text-align: center;">49.42</td>
<td style="text-align: center;">44.78</td>
<td style="text-align: center;">48.88</td>
<td style="text-align: center;">58.40</td>
<td style="text-align: center;">51.67</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">35.86</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">1.34</td>
</tr>
<tr>
<td style="text-align: center;">completeness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">does not address main parts</td>
<td style="text-align: center;">29.91</td>
<td style="text-align: center;">22.56</td>
<td style="text-align: center;">11.16</td>
<td style="text-align: center;">9.79</td>
<td style="text-align: center;">15.92</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">9.47</td>
</tr>
<tr>
<td style="text-align: center;">does not address region</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: center;">does not address time</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">ignores science</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">14.11</td>
<td style="text-align: center;">6.92</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">5.44</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">not enough detail</td>
<td style="text-align: center;">52.89</td>
<td style="text-align: center;">60.22</td>
<td style="text-align: center;">64.06</td>
<td style="text-align: center;">79.53</td>
<td style="text-align: center;">58.13</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">51.89</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: center;">uncertainty</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">consensus missing</td>
<td style="text-align: center;">19.77</td>
<td style="text-align: center;">14.89</td>
<td style="text-align: center;">21.99</td>
<td style="text-align: center;">9.34</td>
<td style="text-align: center;">12.14</td>
<td style="text-align: center;">9.89</td>
<td style="text-align: center;">9.80</td>
</tr>
<tr>
<td style="text-align: center;">contradicting evidence missing</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">6.33</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">2.90</td>
</tr>
<tr>
<td style="text-align: center;">uncertainty missing</td>
<td style="text-align: center;">57.25</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">72.88</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">76.78</td>
<td style="text-align: center;">58.02</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>Table 7. Percentage of specific issues identified by raters.</p>
<h1>A.3. Questions</h1>
<p>In this section we explain the pipeline used for selection, generation, post-processing and sampling climate change related questions. The question set consists of 300 questions, with 100 questions gathered from 3 sources each: i) Synthetic questions generated based on Wikipedia articles, ii) Manually rephrased questions based on Skeptical Science website, and iii) questions taken from Google Trends.</p>
<h2>A.3.1. SYnthetic Questions from Wikipedia</h2>
<p>We started by gathering a set of Wikipedia articles related to climate change. We followed 3 strategies to select climate related articles from Wikipedia. Following the first strategy (REF.), we gather all the Wikipedia articles that are referenced in the main "Climate Change" article. ${ }^{14}$ In the second strategy (CAT.), we select all the articles that are directly listed in the climate change category. Finally, to cover regional articles (REG.), we manually curate a list of articles with titles "Climate Change in [country/region]". From a pool of articles gathered following these 3 strategies, we selected paragraphs within an article if the paragraph consists of more than 500 characters. In total, we obtained 1969 paragraphs from Wikipedia. The following table reports a break-down of number of paragraphs based on the selection strategy:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Strategy</th>
<th style="text-align: center;"># Articles</th>
<th style="text-align: center;"># Paragraphs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REF.</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">858</td>
</tr>
<tr>
<td style="text-align: left;">CAT.</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">434</td>
</tr>
<tr>
<td style="text-align: left;">REG.</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">677</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">1969</td>
</tr>
</tbody>
</table>
<p>We then input each selected paragraph in GPT-4. We ask the model to generate as many questions as possible that can be answered using the paragraph. The model is instructed to only generate questions that are salient and related to climate change. This process resulted in 15265 questions. We post process the questions and remove undesirable ones with 4 filters that we explain next.</p>
<p>Climate Change Filter. We remove all questions that are not climate change related. We use the climate-bert (Webersinke et al., 2022) classifier and label each question with two labels: climate related and not climate related. We remove 2647 questions that are not classified as climate-related questions.</p>
<p>Duplicate Filter. We remove questions that are a duplicate of another question. To this end, we embed all questions using a universal sentence encoder. ${ }^{15}$ We consider two questions as duplicates if the cosine similarity between their embeddings is greater than 0.85 . Therefore, we remove 1188 questions that are duplicates of other questions.</p>
<p>Context Dependent Filter. We filter out questions that are taken out of context. The reason that this filter is necessary is that we generate questions from paragraphs, therefore, some questions are nonsensical when they are not accompanied by the corresponding Wikipedia paragraph. An example of such a question is: "What are the two classes of climate engineering discussed in the study?"; without knowing which study is referred to, this question cannot be answered. To develop this filter, we build a dedicated classifier using in-context probing (Amini \&amp; Ciaramita, 2023). Specifically, we manually annotate 100 questions with two labels: context dependent, and not context dependent. Next, we contextualize the question with the instruction "Write Yes if the query is taken out of context, write No otherwise." and extract the last layer's representations of a flan-xxl encoder (Chung et al., 2022). Finally, we train a logistic regression probing classifier on the representations to detect context dependent questions. We find the context dependency filter to be $97 \%$ accurate on 100 manually annotated validation questions. Using this classifier, we detect 552 context dependent questions.</p>
<p>Specificity Filter. We remove questions that are asking about a very specific and narrow topic. In our study, we aim to evaluate large language models on a set of challenging and multifaceted questions that target information needs of users related to climate change. Therefore, questions that ask for a specific detail are not the target of this study and are typically easy to answer. An example of such question is: "What was the reason for shutting down reactor number one of</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6. Left: Distribution of the topic of the synthetic questions. The x axis shows the percentage of questions in each topic. The most frequent topic is "Weather-Temperature" topic. Right: Distribution of the causal feature of synthetic questions. There are more questions that are asking causal questions, or questions about predicting the future compare to the rest of the questions.
the Fessenheim Nuclear Power Plant on 4 August 2018?" To remove such specific questions, we again build in-context probing classifier on top of flan-xxl representations. We contextualize each question with the instruction: "Write Yes if the following query is asking about a specific subject, write No otherwise". and train the probe on top of extracted contextualized representations from the last layer of flan-xxl. We find the specificity filter to be $84 \%$ accurate on a sample of 100 annotated validation questions. We detect and remove 5472 specific questions.</p>
<p>After applying all 4 filters, the final post-processed question set consists of 5404 questions. The question set that is rated in our evaluation framework consists of 100 questions from each source. This means that we need to sample 100 diverse questions from this pool of $\approx 5 \mathrm{k}$ questions. To make sure that we cover different topics and type of questions, we first label each question with the topic and properties of the question, and then sample a 100 validation questions, where different topics and properties are equally presented. Next, we explain the classifiers that are developed for labeling the questions.</p>
<p>Topic Classifier. We use the same in-context probing approach as above and train a logistic regression classifier on top of flan-xxl encoder to classify questions based on the topics. Inspired by IPCC chapters, we consider the following 9 topics: "Energy", "Emissions-Pollutants", "Policies-Mitigation-Adaptation", "Weather-Temperature", "Land-Ocean-Food-Water", "Society-Livelihoods-Economy", "Health-Nutrition", "Biodiversity", "Cities-Settlements-Infra". We find this classifier to be $75 \%$ accurate on a sample of 100 annotated validation questions. The distribution of predicted questions' topics is depicted in Figure 6.</p>
<p>Causal Prediction Classifier. An important and challenging type of questions that one can ask about climate change is about causes or effects of climate change, or predictions about the future. To detect this type of questions, we classify questions into two classes: causal-prediction class and others. The instruction that is used for contextualizing the questions is: "Write Yes if the following query is asking about causes or effects of something, or is asking about predictions about the future. write No otherwise". We find this classifier to be $93 \%$ accurate on a sample of 100 annotated validation questions. The distribution of predictions is shown in Figure 6</p>
<p>For synthetic Wikipedia questions, we sample 108 questions, 6 from each of the 18 possible combinations of topic and causal-prediction features. We sample an equal number of questions from the other two sources, Skeptical Science and GTrends. We manually inspect all questions from all the 3 different sources. The final question set used in the evaluations consists of 300 questions, i.e., 100 questions from each source.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ https://en.wikipedia.org/wiki/Climate_change
${ }^{15}$ We use universal-sentence-encoder-qa/3 model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://trends.google.com/trends/
${ }^{4}$ https://skepticalscience.com/argument. php&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
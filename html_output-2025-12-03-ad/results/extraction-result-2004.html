<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2004 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2004</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2004</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-279402902</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.13253v1.pdf" target="_blank">Distinct Computations Emerge From Compositional Curricula in In-Context Learning</a></p>
                <p><strong>Paper Abstract:</strong> In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2004.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2004.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer trained with in-context subtask curriculum (curriculum-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-layer transformer trained from scratch with blocked in-context curricula (single-exponential subtasks followed by compositional double-exponential examples); exhibits improved zero-shot compositional generalization, greater robustness (fewer errors) and linearly-decodable intermediate subtask representations compared to vanilla training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (curriculum-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer trained from scratch with next-token prediction on sequences of (x,y) exemplar pairs presented in blocked curriculum order (single-exponential blocks for a and b, then compositional double-exponential block). Sinusoidal positional embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer with multi-head attention (8 heads), sinusoidal positional embeddings; no explicit modular/symbolic components</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning / modular arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular double-exponential task (compositional modular exponentiation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given integers x and task parameters a and b (primitive roots mod P), predict y = b^{(a^x)} mod P (double-exponential modular computation). Single-exponential subtasks are y = a^{x} mod P and y = b^{x} mod P. Compositionality arises because the compositional task requires inferring and combining intermediate subtask outputs (e.g., a^x mod (P-1) used as exponent reduction) to compute the final result.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>two-level function composition (single-exponential subtasks composed to form a double-exponential), i.e., depth = 2</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition (nested modular exponentiation)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>novel combinations of seen primitives: all individual a and b are seen during training but only an 80/20 train/test split over pairs (a,b) is used so test uses unseen combinations of seen primitives</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised next-token prediction of sequences presented in blocked in-context curricula; curriculum learning (in-context curriculum) vs. matched vanilla condition</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td>Each training sequence contains m exemplars of y=a^{x} mod P, then m exemplars of y=b^{x} mod P, followed by n exemplars of y=b^{(a^x)} mod P (notation in paper: m-m-n curricula). Total context length fixed to 48 tokens (24 (x,y) pairs). Multiple curricula tested, e.g. (10-10-4), (8-8-8), (6-6-12), (4-4-16), (11-11-2). Loss weighting controlled so compositional-task loss contributes 1/3 of total loss; vanilla condition sees same exemplar-level data but not the blocked in-sequence subtask→compositional correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>In-context exposure: curriculum sequences contain explicit subtask exemplars immediately prior to compositional exemplars (m examples per subtask). Number of in-context subtask exemplars varied across curricula (m values as above); tests use zero-shot on compositional block (i.e., first compositional exemplar following subtask blocks) among unseen (a,b) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Qualitatively high asymptotic accuracy on training distributions reported (both curriculum and vanilla achieve high final accuracy); final-layer decoding of y (task outputs) approaches near-perfect decoding according to linear probes (exact numeric accuracies not reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Curriculum-trained models show robust zero-shot inference on unseen compositional (a,b) combinations and systematically fewer errors across the compositional block compared to vanilla; improved robustness quantified by lower error counts in compositional block (figures show clear reduction, numeric magnitudes not provided in text). Linear probes reveal high decodability of intermediate subtask values (a^x mod (P-1) and b) in the compositional block, especially at zero-shot, indicating use of compositional strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Described qualitatively: curriculum training reduces errors at zero-shot compositional inference relative to vanilla; exact numerical gap not provided in the paper text (reported as fewer errors/error-count plots).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared directly to a 'vanilla' transformer trained with sequences containing only compositional examples in-sequence (but matched exemplar-level data and loss weighting). Curriculum model: fewer errors in compositional block, zero-shot compositional generalization and linear-decodable intermediate computations. Vanilla model: can still learn compositional task via few-shot in-context learning but shows higher error counts, poorer zero-shot performance and weaker linear-decodability of intermediate subtask values in compositional block.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>No alternative architectures compared; all experiments use the same transformer architecture (8 layers, hidden dim 128, 8 heads). The comparison is across training curricula rather than architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Not reported (single model scale used; multiple sizes not tested).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Not reported (model trained from scratch; no pretraining/transfer experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) In-context subtask curricula enable zero-shot inference on unseen compositions of seen primitives and increase robustness (fewer errors) on compositional tasks compared to matched vanilla training. 2) Curriculum-trained models represent intermediate subtask computations (linearly decodable), with highest decodability appearing in middle layers (layers ~5–6), suggesting stepwise compositional processing. 3) Curriculum design (lengths m and n) modulates whether the model predominantly uses a compositional strategy, a vanilla few-shot strategy, or a mixture; longer compositional blocks induce mixed strategies while shorter compositional blocks encourage persistent compositional decoding. 4) Curriculum changes training dynamics (order in which subtasks and compositional task are learned).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Curriculum-trained models fail when presented with mismatch sequences where subtask blocks (a,b) do not match the subsequent compositional block (a',b'); short-compositional-block curricula (e.g., 10-10-4, 8-8-8) produce near-complete failure on mismatched parameters, indicating reliance on inferred subtask info. Vanilla models are less reliant on explicit subtask representations and instead improve gradually with compositional exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Successful compositional generalization occurs when (a) in-context curricula include clear subtask exemplars preceding compositional exemplars, (b) subtask blocks are long enough for the model to learn decodable subtask representations (requirements vary with modulus P and curriculum parameters), and (c) training includes all individual primitive parameters a and b though not necessarily all pairs. The amount of compositional block examples (n) modulates whether the model uses compositional vs. vanilla few-shot strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2004.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2004.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer trained without in-context subtask curriculum (vanilla-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same architecture as the curriculum model but trained with sequences containing only compositional double-exponential exemplars in-sequence (though exemplar-level single-task pairs are present across training, they are not presented in the same sequence); learns the compositional mapping but with weaker zero-shot generalization and less representation of intermediate subtask values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (vanilla-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer trained from scratch using next-token prediction on sequences composed only of double-exponential exemplars (y = b^{(a^x)} mod P) aligned to match exemplar-level distribution and loss weighting of curriculum condition but lacking blocked subtask→compositional in-context correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer with multi-head attention (8 heads), sinusoidal positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning / modular arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular double-exponential task (compositional modular exponentiation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict y = b^{(a^x)} mod P given x and task parameters a,b; training sequences contain only compositional exemplars (no in-sequence subtask blocks), though single-exponential exemplar pairs exist elsewhere in training to match loss weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>two-level composition (depth = 2)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition (nested modular exponentiation)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>novel combinations of seen primitives (80/20 split over (a,b) pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised next-token prediction on compositional sequences (vanilla few-shot in-context learning); exemplar-level data matched to curriculum condition but lacking in-sequence subtask correlations</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>While training dataset contains the same exemplar-level single-exponential pairs as curriculum condition (to match loss weighting), these examples never appear in the same context sequence as the compositional examples; thus no in-context subtask inoculation for a given compositional query.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High asymptotic accuracy on training distribution / compositional training sequences (both vanilla and curriculum reach high final accuracy), final-layer decoding of y nearly perfect per linear probes (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Can learn compositional mapping via few-shot in-context learning (errors decrease with more compositional exemplars), but shows worse zero-shot performance on unseen (a,b) combinations and higher error counts in compositional block compared to curriculum-trained models; linear probes show weaker decodability of intermediate subtask values in compositional block.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Qualitatively larger errors at compositional zero-shot compared to curriculum-trained model; exact numeric gap not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to curriculum-trained transformer: vanilla model has higher error counts and lacks strong linear-decodability of intermediate subtask computations in compositional block; vanilla model can still saturate performance with more compositional exemplars but is less robust at zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>No architectural variants compared; differences are in training sequence structure only.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla training without blocked in-context subtasks yields competent few-shot in-context learning for compositional tasks but weaker zero-shot generalization and less explicit representation of intermediate subtask values; curriculum (in-context subtask) is the primary factor improving zero-shot compositional generalization and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Vanilla models do not encode intermediate subtask computations in a linearly-decodable manner in the compositional block and show continued errors even after many compositional exemplars (right-skewed error distribution indicated in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Vanilla few-shot strategy succeeds when sufficient compositional exemplars are provided in-context (long compositional blocks), but does not generalize as robustly zero-shot to unseen pairings of primitives without explicit subtask curricula.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Data distributional properties drive emergent in-context learning in transformers <em>(Rating: 2)</em></li>
                <li>When can transformers compositionally generalize in-context? ICML Next Generation of Sequence Modeling Architectures Workshop <em>(Rating: 1)</em></li>
                <li>Human curriculum effects emerge with in-context learning in neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2004",
    "paper_id": "paper-279402902",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Curriculum Transformer",
            "name_full": "Transformer trained with in-context subtask curriculum (curriculum-trained model)",
            "brief_description": "An 8-layer transformer trained from scratch with blocked in-context curricula (single-exponential subtasks followed by compositional double-exponential examples); exhibits improved zero-shot compositional generalization, greater robustness (fewer errors) and linearly-decodable intermediate subtask representations compared to vanilla training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (curriculum-trained)",
            "model_description": "Decoder-only Transformer trained from scratch with next-token prediction on sequences of (x,y) exemplar pairs presented in blocked curriculum order (single-exponential blocks for a and b, then compositional double-exponential block). Sinusoidal positional embeddings.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard Transformer with multi-head attention (8 heads), sinusoidal positional embeddings; no explicit modular/symbolic components",
            "task_domain": "mathematical reasoning / modular arithmetic",
            "task_name": "Modular double-exponential task (compositional modular exponentiation)",
            "task_description": "Given integers x and task parameters a and b (primitive roots mod P), predict y = b^{(a^x)} mod P (double-exponential modular computation). Single-exponential subtasks are y = a^{x} mod P and y = b^{x} mod P. Compositionality arises because the compositional task requires inferring and combining intermediate subtask outputs (e.g., a^x mod (P-1) used as exponent reduction) to compute the final result.",
            "compositional_depth": "two-level function composition (single-exponential subtasks composed to form a double-exponential), i.e., depth = 2",
            "composition_type": "function composition (nested modular exponentiation)",
            "split_type": "novel combinations of seen primitives: all individual a and b are seen during training but only an 80/20 train/test split over pairs (a,b) is used so test uses unseen combinations of seen primitives",
            "training_strategy": "supervised next-token prediction of sequences presented in blocked in-context curricula; curriculum learning (in-context curriculum) vs. matched vanilla condition",
            "curriculum_details": "Each training sequence contains m exemplars of y=a^{x} mod P, then m exemplars of y=b^{x} mod P, followed by n exemplars of y=b^{(a^x)} mod P (notation in paper: m-m-n curricula). Total context length fixed to 48 tokens (24 (x,y) pairs). Multiple curricula tested, e.g. (10-10-4), (8-8-8), (6-6-12), (4-4-16), (11-11-2). Loss weighting controlled so compositional-task loss contributes 1/3 of total loss; vanilla condition sees same exemplar-level data but not the blocked in-sequence subtask→compositional correlations.",
            "inoculation_details": "In-context exposure: curriculum sequences contain explicit subtask exemplars immediately prior to compositional exemplars (m examples per subtask). Number of in-context subtask exemplars varied across curricula (m values as above); tests use zero-shot on compositional block (i.e., first compositional exemplar following subtask blocks) among unseen (a,b) pairs.",
            "iid_performance": "Qualitatively high asymptotic accuracy on training distributions reported (both curriculum and vanilla achieve high final accuracy); final-layer decoding of y (task outputs) approaches near-perfect decoding according to linear probes (exact numeric accuracies not reported in text).",
            "compositional_performance": "Curriculum-trained models show robust zero-shot inference on unseen compositional (a,b) combinations and systematically fewer errors across the compositional block compared to vanilla; improved robustness quantified by lower error counts in compositional block (figures show clear reduction, numeric magnitudes not provided in text). Linear probes reveal high decodability of intermediate subtask values (a^x mod (P-1) and b) in the compositional block, especially at zero-shot, indicating use of compositional strategy.",
            "generalization_gap": "Described qualitatively: curriculum training reduces errors at zero-shot compositional inference relative to vanilla; exact numerical gap not provided in the paper text (reported as fewer errors/error-count plots).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared directly to a 'vanilla' transformer trained with sequences containing only compositional examples in-sequence (but matched exemplar-level data and loss weighting). Curriculum model: fewer errors in compositional block, zero-shot compositional generalization and linear-decodable intermediate computations. Vanilla model: can still learn compositional task via few-shot in-context learning but shows higher error counts, poorer zero-shot performance and weaker linear-decodability of intermediate subtask values in compositional block.",
            "architectural_comparison": "No alternative architectures compared; all experiments use the same transformer architecture (8 layers, hidden dim 128, 8 heads). The comparison is across training curricula rather than architecture.",
            "scale_effects": "Not reported (single model scale used; multiple sizes not tested).",
            "transfer_results": "Not reported (model trained from scratch; no pretraining/transfer experiments).",
            "key_findings": "1) In-context subtask curricula enable zero-shot inference on unseen compositions of seen primitives and increase robustness (fewer errors) on compositional tasks compared to matched vanilla training. 2) Curriculum-trained models represent intermediate subtask computations (linearly decodable), with highest decodability appearing in middle layers (layers ~5–6), suggesting stepwise compositional processing. 3) Curriculum design (lengths m and n) modulates whether the model predominantly uses a compositional strategy, a vanilla few-shot strategy, or a mixture; longer compositional blocks induce mixed strategies while shorter compositional blocks encourage persistent compositional decoding. 4) Curriculum changes training dynamics (order in which subtasks and compositional task are learned).",
            "failure_analysis": "Curriculum-trained models fail when presented with mismatch sequences where subtask blocks (a,b) do not match the subsequent compositional block (a',b'); short-compositional-block curricula (e.g., 10-10-4, 8-8-8) produce near-complete failure on mismatched parameters, indicating reliance on inferred subtask info. Vanilla models are less reliant on explicit subtask representations and instead improve gradually with compositional exemplars.",
            "success_conditions": "Successful compositional generalization occurs when (a) in-context curricula include clear subtask exemplars preceding compositional exemplars, (b) subtask blocks are long enough for the model to learn decodable subtask representations (requirements vary with modulus P and curriculum parameters), and (c) training includes all individual primitive parameters a and b though not necessarily all pairs. The amount of compositional block examples (n) modulates whether the model uses compositional vs. vanilla few-shot strategies.",
            "uuid": "e2004.0"
        },
        {
            "name_short": "Vanilla Transformer",
            "name_full": "Transformer trained without in-context subtask curriculum (vanilla-trained model)",
            "brief_description": "Same architecture as the curriculum model but trained with sequences containing only compositional double-exponential exemplars in-sequence (though exemplar-level single-task pairs are present across training, they are not presented in the same sequence); learns the compositional mapping but with weaker zero-shot generalization and less representation of intermediate subtask values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (vanilla-trained)",
            "model_description": "Decoder-only Transformer trained from scratch using next-token prediction on sequences composed only of double-exponential exemplars (y = b^{(a^x)} mod P) aligned to match exemplar-level distribution and loss weighting of curriculum condition but lacking blocked subtask→compositional in-context correlations.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard Transformer with multi-head attention (8 heads), sinusoidal positional embeddings",
            "task_domain": "mathematical reasoning / modular arithmetic",
            "task_name": "Modular double-exponential task (compositional modular exponentiation)",
            "task_description": "Predict y = b^{(a^x)} mod P given x and task parameters a,b; training sequences contain only compositional exemplars (no in-sequence subtask blocks), though single-exponential exemplar pairs exist elsewhere in training to match loss weighting.",
            "compositional_depth": "two-level composition (depth = 2)",
            "composition_type": "function composition (nested modular exponentiation)",
            "split_type": "novel combinations of seen primitives (80/20 split over (a,b) pairs)",
            "training_strategy": "standard supervised next-token prediction on compositional sequences (vanilla few-shot in-context learning); exemplar-level data matched to curriculum condition but lacking in-sequence subtask correlations",
            "curriculum_details": null,
            "inoculation_details": "While training dataset contains the same exemplar-level single-exponential pairs as curriculum condition (to match loss weighting), these examples never appear in the same context sequence as the compositional examples; thus no in-context subtask inoculation for a given compositional query.",
            "iid_performance": "High asymptotic accuracy on training distribution / compositional training sequences (both vanilla and curriculum reach high final accuracy), final-layer decoding of y nearly perfect per linear probes (exact numbers not provided).",
            "compositional_performance": "Can learn compositional mapping via few-shot in-context learning (errors decrease with more compositional exemplars), but shows worse zero-shot performance on unseen (a,b) combinations and higher error counts in compositional block compared to curriculum-trained models; linear probes show weaker decodability of intermediate subtask values in compositional block.",
            "generalization_gap": "Qualitatively larger errors at compositional zero-shot compared to curriculum-trained model; exact numeric gap not provided.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to curriculum-trained transformer: vanilla model has higher error counts and lacks strong linear-decodability of intermediate subtask computations in compositional block; vanilla model can still saturate performance with more compositional exemplars but is less robust at zero-shot.",
            "architectural_comparison": "No architectural variants compared; differences are in training sequence structure only.",
            "scale_effects": "Not reported.",
            "transfer_results": "Not reported.",
            "key_findings": "Vanilla training without blocked in-context subtasks yields competent few-shot in-context learning for compositional tasks but weaker zero-shot generalization and less explicit representation of intermediate subtask values; curriculum (in-context subtask) is the primary factor improving zero-shot compositional generalization and robustness.",
            "failure_analysis": "Vanilla models do not encode intermediate subtask computations in a linearly-decodable manner in the compositional block and show continued errors even after many compositional exemplars (right-skewed error distribution indicated in figures).",
            "success_conditions": "Vanilla few-shot strategy succeeds when sufficient compositional exemplars are provided in-context (long compositional blocks), but does not generalize as robustly zero-shot to unseen pairings of primitives without explicit subtask curricula.",
            "uuid": "e2004.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Data distributional properties drive emergent in-context learning in transformers",
            "rating": 2
        },
        {
            "paper_title": "When can transformers compositionally generalize in-context? ICML Next Generation of Sequence Modeling Architectures Workshop",
            "rating": 1
        },
        {
            "paper_title": "Human curriculum effects emerge with in-context learning in neural networks",
            "rating": 1
        }
    ],
    "cost": 0.0122775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Distinct Computations Emerge From Compositional Curricula in In-Context Learning
16 Jun 2025</p>
<p>Jin Hwa Lee jin.lee.22@ucl.ac.uk 
University College London
LondonUK</p>
<p>Andrew K Lampinen 
Google Deepmind
Mountain ViewCAUSA</p>
<p>Aaditya K Singh 
University College London
LondonUK</p>
<p>Andrew M Saxe 
University College London
LondonUK</p>
<p>CIFAR Azrieli Global Scholar
CIFAR</p>
<p>Distinct Computations Emerge From Compositional Curricula in In-Context Learning
16 Jun 202520E391BB3A7C5FCA357B7004BCE54447arXiv:2506.13253v1[cs.LG]
In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs.Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns.We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks-and train transformer models to learn the task in-context.We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum.We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length.We study how the task and subtasks are represented across the two training regimes.We find that the models employ diverse strategies modulated by the specific curriculum design.Preprint.Under review.</p>
<p>Introduction</p>
<p>Many complex real-world tasks require composing multiple constituent functions or subtasks.This notion of systematic compositionality has been extensively studied [4,10,43] and is argued to be a key feature of flexible intelligence, enabling "infinite use from finite means."However, it has been a central controversy whether neural networks can exhibit human-like compositionality [9,41,17,18].The recent success of Large Language Models (LLMs) has only brought this controversy to a head, given their often inscrutable nature yet remarkable generalization capabilities [2,51].</p>
<p>A body of literature has studied the capabilities and limitations of compositional generalization in LLMs, including multi-hop reasoning [32,55], chain-of-thought reasoning [52,48], and the scratch-pad [28].For example, in a multi-hop reasoning query such as "the mother of the singer of 'Superstition"', LLMs seem to latently encode intermediate information "Stevie Wonder is the singer of 'Superstition"' and use it to produce the final answer [55].Furthermore, LLMs can improve their performance on complex hierarchical tasks by decomposing the tasks into component subtasks [57,59].On the other hand, such compositional generalization in LLMs is not always guaranteed.For tasks defined by composition of several functions or subtasks, models often find a surrogate strategy rather than learning the underlying true compositional structure [7,16].</p>
<p>To this end, it is important to understand the circumstances under which compositional generalization abilities may emerge, especially when the subtask or component knowledge is available.We approach this question from a data distributional perspective.While previous work showed that language-like data distributional properties such as skewedness and burstiness trigger in-context learning ability [3], another distinct property of natural language corpus is that it encompasses a variety of compositional Concretely, we design an algorithmic task utilizing the composition of two modular exponential tasks defined by two exponent bases, (a, b).To emulate in-context compositional curriculum structure, we design subtask curriculum with examples of single exponentials from each base followed by compositional double exponentials (see Figure 1 a).We train the model with example sequences sampled from sets of (a, b) and evaluate its generalization ability on unseen combinations of (a, b).</p>
<p>We first demonstrate that a model trained with subtask curriculum is capable of zero-shot inference on unseen compositional task queries and shows higher robustness compared to a model trained without a curriculum (Section 3.1).We show evidence that the curriculum enables the model to represent and combine the task parameters to be composed (Section 3.2).Finally, we study how the length of the curriculum affects the model's learning and potentially which strategy, or even mixture of strategies, is used to solve the compositional task (Section 3.3-3.4).</p>
<p>Our results highlight that "in-context curricula" can provide higher-level correlational structure between subtasks and compositional task in-context that lead to models employing a more compositional form of in-context learning at test time.Furthermore, the degree to which these correlations are present in an input context modulates the trade-off between compositional and standard few-shot in-context learning strategies.</p>
<p>2 Experimental Setup</p>
<p>Task</p>
<p>We use a modular arithmetic task of composition of an exponential function, namely b a x mod P , referred to as the modular double exponential task.Inspired by well-studied linear modular arithmetic tasks such as summation, multiplication or both [13,25], we chose the modular double exponential function for its greater complexity while still offering a deterministic functional mapping and effectively constraining the vocabulary size.As shown in Figure 1 a, we design a curriculum of in-context exemplars, which provides blocked examples for y = a x mod P and y = b x mod P , followed by y = b a x mod P .In contrast, vanilla in-context exemplars consist only of y = b a x mod P .We train transformer architecture on these sequences using a next token prediction task for every (x, y) pair in the sequence, rather than only on the final query.</p>
<p>To make a fair comparison, we provide single exponential task exemplars in vanilla training as well, but the main difference is that they never appear in the same sequence as exemplars for the compositional task.Curriculum-and vanilla-training thus see the same set of task inputs and outputs at an exemplar level, with the key difference being in-context correlations at the sequence level: curriculum-training see both subtask and compositional task in-context, while in vanilla training each sequence is just from one task.</p>
<p>In every example sequence, we randomly sample task parameters (a, b), and the model needs to learn to adapt its answer in-context according to (a, b).The task combinations (a, b) seen during the training include all possible individual a, and b, but not all pairs.We evaluate the trained model on unseen combinations of (a, b).We used 80/20 split of all possible pairs for train and test.We permit integers x ∈ [0, P ), and a and b are sampled from the primitive roots of P .Throughout the main experiments, we focus on P = 59, and we extend our findings to other values of P in Appendix B.2.</p>
<p>In the curriculum setting, we use an equal curriculum length m for each single exponential task and n for the compositional task, leading to total length 2m + n.We use the same 2m + n exemplars of compositional task for the vanilla setting.While varying the compositional task length in the curriculum, we maintain the importance of the compositional task equal to each single exponential task in the curriculum by controlling the weighting factor for the loss contribution from the compositional task (namely, making it such that the loss from the compositional task is 1/3 of the total loss).</p>
<p>Similarly, for a fair comparison, in the vanilla setting the network sees sequences for a single exponential task as well, with a ratio of 2 to 1 (to match the overall weight of the compositional task to the curriculum setting).By doing this, we effectively make it so that the same (exemplar, label) pairs are seen in both curriculum and vanilla settings, with the same loss weight for single vs. double exponential tasks.The key difference between the two settings is the in-context correlations: in the curriculum setting, these correlations are more complex/hierarchical (possibly leading to in-context compositional learning), while in the vanilla setting, they focus on single function learning (the standard few-shot ICL setting).</p>
<p>We fix the total context length to 48, corresponding to 24 pairs of (x, y), and ensure that all x for each task sequence in context are unique.We train 8-layer transformers with sinusoidal positional embeddings with time constant of 120, a hidden dimension size of 128, and 8 heads, using the Adam optimizer, a learning rate of 7.5 × 10 −4 , and a batch size of 512.All results we report are based on 2 data seeds and trained with 2 × 10 8 sequences unless otherwise mentioned.Specifically, for the vanilla model, we further trained it up to 3 × 10 8 sequences to ensure that the model's performance is saturated.See Appendix A for example loss curve and performance evolution.</p>
<p>Model and Training</p>
<p>Results</p>
<p>3.1 In-context curricula increase the robustness of in-context learning of a complex compositional task First, we demonstrate that the subtask curricula can make in-context learning of unseen compositional task more robust.As we see in Figure 1 b, both the vanilla and curricula settings tend to achieve high accuracy.</p>
<p>Here, we closely examine the performance difference through analyzing the error counts after each In Figure 3, we focus on the generalization ability on the compositional task by comparing the errors on n compositional task exemplars in each curriculum setting and the corresponding n last exemplars in the vanilla setting.We observe that the curriculum models result in higher robustness (fewer errors) compared to the vanilla model.Note that most of the curriculum settings exhibit rather consistent errors throughout the compositional task, but the rightmost curriculum (4-4-16) shows further decreasing of errors with more compositional exemplars without getting more subtask information.This suggests the model utilizes in-context compositional examples rather than solely relying on subtask curriculum.We dive deeper into this aspect in Section 3.3-3.4.</p>
<p>Based on the above behavioral evidence that in-context curriculum can make the compositional generalization more robust, we ask what is the mechanism behind this.</p>
<p>In-context curricula promote representation of compositional subtasks</p>
<p>In the previous section, we observed that the models can benefit from having subtask curricula for solving compositional tasks in-context, particularly its zero-shot inference ability on the compositional task and higher robustness compared to the vanilla model.In this section, we ask how in-context curricula may enable this compositional solution.We hypothesize that the in-context curriculum of the single exponential tasks provides unambiguous information about the subtasks that constitute the compositional task, thereby facilitating the model's learning of a compositional solution to the double exponential task.</p>
<p>To test this hypothesis, we investigated how the model represents intermediate values from constituent subtasks required for compositional solution.We trained a linear classifier1 with the hidden representation of each layer from evaluation sequences to decode the intermediate values required for compositional computation-a x mod (P − 1) and task parameter b 2 -in compositioanl task block.We additionally probe for corresponding task output y at each position and the subtask information in the subtask blocks (task a computation and task parameter b).We used 80/20 split of the 1K unseen test sequences for the linear probe training and testing of the decoding accuracy 3 .See Appendix C.3 for more results in other curriculum designs and experimental details.</p>
<p>In Figure 4, we first observe that the decoding of y values of corresponding tasks becomes near perfect at the final layer, as expected from the overall high accuracy in both the vanilla and curriculum models.However, we find a noticeable difference in decoding of intermediate computation values.Next, we take a closer look at the layer-wise decoding accuracy.In the curriculum model, we observe that the highest decoding of the intermediate computation values in the compositional task are not achieved in the last layer but in the earlier layers (layer 5-6).This suggests the layer-wise processing of subtask information in compositional block.That is, the representation from the subtask curriculum blocks in the context is transferred and processed in the earlier layers and go through further computation to combine those to perform compositional task.Additionally, we visualize the attention pattern of the heads in the earlier layers where we can find heads that attend to the earlier curriculum block from the compositional task block in Appendix D. Collectively, these results indicate that the curriculum-trained model encodes and utilizes the intermediate values required for the compositional task inferred from the curriculum.</p>
<p>To provide further evidence for our hypothesis, we test the curriculum model with mismatch sequences -single exponential tasks with task parameter (a, b) followed by double exponential task with (a ′ , b ′ ), where the compositional task block uses mismatching task parameters than the subtasks shown in context.If the model is using compositional strategy, we expect the model to fail on this task, as the subtask representations are not informative.In Figure 5, we show that this prediction is consistent in the curriculum model trained with short compositional task block (10-10-4, 8-8-8, etc, see Appendix E.1 for more results).</p>
<p>In brief, we show evidence that the model trained with in-context curriculum encodes the subtask information in its internal representation and is capable of using them for the compositional task using linear probes and controlled experiment.We show the decoding of intermediate values, which suggests step-by-step compositional computation in the curriculum model.In contrast, the vanilla model does not necessarily represent such intermediate computation from the subtasks.The mismatch experiment further confirms that having incorrect subtask information causes the curriculum-trained model to fail on the compositional task.We demonstrated that the in-context curriculum can enhance the model's robustness on compositional generalization possibly through promoting the encoding of the relevant subtasks representation.However, since the compositional task can be learned fairly well without curriculum but with standard few-shot in-context learning (as seen in the vanilla model), it is unclear why the model learns to use the subtask information inferred from the single exponential task examples.In other words, the model can learn the compositional task independently, even when the curriculum sequence is provided, without relying on the subtask information.Our earlier comment on the trend of decreasing error within the compositional task block in the curriculum (4-4-16) reflects this possibility (Figure 3).</p>
<p>In-context curriculum designs change the model's strategy on a compositional task</p>
<p>In the mismatch experiment, we can find a hint of this behavior in the model trained with longer compositional task block.In Figure 6, we see slight performance improvement despite mismatch of task parameters when the compositional task length is longer (4-4-16).This is not only due to the more examples given since the (10-10-4) or (8-8-8) models completely fail given 4-8 examples (Figure 5), but the (4-4-16) model shows improvement already from one-shot example.This implies that the model can still make some inference without compositional strategy, suggesting the model employs possibly both strategies -compositional and vanilla strategy-.This sign of mixed strategies becomes more clear in the linear probes of subtasks intermediate computation.In Figure 7, we show linear probing of compositional task block from different curriculum designs.In the curriculum design with long compositional task block (6-6-12, 4-4-16), we observe the intermediate values show high decodability at the zero-shot (at the dotted lines) of the compositional task, but kept low value similar to the vanilla setting in the rest.This can be interpreted that the model employs a compositional strategy exploiting subtask information on the zero-shot of compositional task since the vanilla strategy cannot provide any meaningful inference on the zero-shot example, while the model still utilizes vanilla few-shot learning strategy as well.On the other hand, the curriculum setting with a shorter compositional task shows consistently high decodability of intermediate computations across the entire block, indicating that the model is predominanlty relies on a compositional strategy.</p>
<p>It is notable that the intermediate value decodability is not all-or-none for (8-8-8) or (10-10-4).Rather, it increases continuously as the compositional task length decreases (see green and yellow lines in Figure 7), reflecting that the compositional strategy is not binary but graded.That is, the possible choice of strategy that a model employs is not mere binary choice of either compositional or non-compositional, but rather lies on a continuous spectrum.</p>
<p>In-context curricula designs modulate order in which the tasks are learned</p>
<p>Next, we look into the loss evolution and linear probe across the training phase to understand how do the different strategy choices develop.In Figure 8 a, we observe a clear difference in the order in which each task is learned when the curriculum length is varied.With long compositional task  ), and it decreases afterwards, indicating existence of both vanilla and compositional strategy.For the shorter compositional task design (10-10-4, 8-8-8), we observe high decodability consistently in entire block but the level of decodability is relatively higher with shorter compositional task length (yellow vs. green).These results suggest complex interplay between the different curricula design and a strategy the model employs.</p>
<p>sequence (4-4-16), the model is capable of vanilla few-shot learning on compositional task before the subtasks being learned (pink before gray).Nevertheless, the zero-shot loss of the compositional task decreases substantially shortly after the subtask learning (blue after gray).On the other hand, with shorter compositional task length (10-10-4), the rapid learning of compositional task happens only after the subtask learning (pink and blue after gray).Furthermore, both the zero-shot and the last-shot loss decrease almost simultaneously, suggesting the subtask information is utilized in the entire compositional task block.These observations show that different curricula can change the training dynamics, that is, in which the order of the tasks are learned.</p>
<p>We find that development of the subtask representation in different curriculum settings is also aligned with the above observation.In Figure 8 b, we show linear probe of the model checkpoints before and after the subtask learning (marked with the colored bars in Figure 8 a).In curriculum (4-4-16), the model can readily solve compositional task before learning the subtasks with sufficient examples and its linear probe shows low decodability of intermediate subtask computation (suggesting vanilla few-shot learning strategy).The model becomes capable of zero-shot inference shortly after the subtask learning, facilitated by the subtask representation (increased decoding accuracy of intermediate subtask values at the zero-shot).On the other hand, we see that in the curriculum (10-10-4) the compositional task performance rapidly increases only after the subtask learning and entire compositional task block encodes the intermediate values from the constituent subtasks.</p>
<p>Collectively, these suggest varying the correlational structure between subtask and compositional task given in-context by controlling curriculum design influences which task to be learned first and thus influencing the strategy that the model employs, in this case, compositional computation.</p>
<p>Related Works</p>
<p>In-context learning has brought significant interest recently, particularly due to the emergent capabilities of LLMs [51].The in-context few-shot learning ability of language models [2] 16), the model can solve compositional task after many examples before subtask learning (pink before gray).The zero-shot loss decreases sharply only after the subtask lerning (blue after gray).Bottom: In the curriculm (10-10-4), the subtask learning is followed by the compositional task learning.The learning of both zero-shot and the last shot happen almost spontaneously (pink and blue after gray).b) Linear probe at before and after the subtask learning (blue and orange mark in panel a).Top: In the curriculum (4-4-16), we see that the model readily make good prediction on y in compositional task block after few in-context examples without learning subtasks (blue, near-1.0accuracy after 12 shots).After the subtasks are learned (orange), the model is capable of zero-shot inference where we can find representation of the subtasks (note the increased decodability of y at shots 8-10, and corresponding peaks in decodability of intermediate values from a and b).Bottom: In the curriculum (10-10-4), the model can generalize to the compositional task only after the subtasks are learned, as evidenced by low decodability of y before subtask learning (blue) and the high decodability of y and intermediate values after subtask learning (orange).We also see here that subtasks representations are more decodable in earlier layers.See Appendix E.4 for results on other layers.</p>
<p>to earlier works on meta-learned few-shot learning [36,47,49].Many studies [3,54,34] have highlighted the importance of data properties for in-context learning.A few studies [14,46] have explored how different in-context tasks can be represented in LLMs in the form of task vectors.Russin et al. [35] show that ICL can match human patterns of compositional and non-compositional behavior in multi-output categorization tasks.</p>
<p>Compositionality in neural networks has been a central controversy.However, the recent success of LLMs, and their language comprehension and generation-which are thought to be a hallmark of compositional ability of humans-may have altered the question into how and when compositionality can emerge, rather than its existence.Indeed, various works have studied how meta-learning can enable systematic compositional generalization in neural networks [23,18].Correspondingly, evidence of compositional representation and computation in language models has been studied extensively in scopes ranging from representational structure [44,42] to geometric manifolds [20] to circuit level mechanistic interpretability [11,55,24,46].</p>
<p>Curriculum learning is critical in learning of humans and animals, well-attested in a body of literature [40,8,5,6].While its potential importance has long been acknowledged in machine learning community [1,50], the benefit from curriculum has been shown marginal in standard supervised learning benchmarks [53].However, the right curricula show greater significance in the context of reinforcement learning [15,45,27].In particular, [21] shows theoretical evidence of importance of subtask curricula in learning compositional task.However, there has been little exploration of curricula within in-context learning.</p>
<p>Modular arithmetic tasks have been used in a rich body of literature to understand how sequence models, such as transformers, can implement the internal mechanisms required to solve these tasks.For example, [31,58] used simple modular addition tasks to investigate the grokking phenomenon and demonstrate that transformers can implement multiple solutions.He et al. [13] studied how transformers can learn skill composition in-context with out-of-distribution tasks.Our work builds on these findings by exploring how transformers can utilize a curriculum of subtasks provided in-context to achieve compositional generalization on more complex modular arithmetic tasks.</p>
<p>Discussion</p>
<p>We investigated how transformers can leverage inferred subtask information from an in-context curriculum to generalize to unseen compositional tasks, using a modular double exponential task as a case study.We demonstrated that an in-context curriculum enables zero-shot inference on compositional tasks and increases robustness.As an initial step to understand the model's internal workings, we used a linear probe to explore how the model processes the curriculum.We found that the internal representation encodes subtask information from the curriculum blocks and the intermediate values of the compositional computation are effectively decoded as the compositional task sequence is processed.Finally, we observed that the amount of compositional task information provided in-context (controlled by curriculum length) affects both the learning strategy and the evolution of task representations during training.Our observations suggest that data properties present in context, such as curricula, can enable compositional generalization in models' emergent in-context learning abilities.</p>
<p>Importance of structure in data The types of compositional context structures we have emphasized in this work occur frequently in natural language data; from textbooks to novels, many documents introduce simpler elements in the beginning that build to yield more complex interactions later.Thus, while many theoretical works on in-context learning focus on presenting IID examples of a single task in context, our work highlights that language models may yield qualitatively different types of in-context learning when the contexts have a curricular compositional structure.Furthermore, our observation of diverse and even mixed strategies emerging from different curricula suggests rich inner working of in-context learning modulated by different data structures.These findings therefore highlight the importance of considering the many types of context structures that may contribute to in-context learning [cf. 19].We hope that our results will encourage more exploration of curricula and compositional learning in-context, both in controlled settings and at scale.</p>
<p>Mixtures of strategies and spectrum-like property of compositional generalization In Section 3.3, we observe different curriculum designs lead to the models showing signs of both compositional and non-compositional strategies.Indeed, compositional generalization seems to be not a binary strategy choice but rather a spectrum-like behavior, echoing similar observations in natural language learning [e.g.33,22].This suggests a complex interplay between the data structure providing compositional information and the degree of resulting compositional generalization.For example, even when the underlying compositional task structure is the same, depending on precisely how the subtask and compositional task examples are given, different levels of compositional generalization ability can be induced.We also observe that the strategy of the model is linked to the order in which the tasks are learned, which highlights the importance of dynamical aspects of the emergence of in-context learning [38,37,29,39,56].</p>
<p>Limitations and future directions Our analysis of mechanisms in this paper is limited to correlational evidence via linear probes.Further analysis with causal manipulation (e.g., path patching) would be necessary to gain a more precise understanding of the mechanisms behind the observed model behavior.Furthermore, in naturalistic settings there may be many types of compositional tasks that share common components, as well as cases in which the components are not always presented together with the full task; examining how our findings change in such settings would be an exciting direction for future work.It would be interesting to explore the representations of large language models as they learn novel tasks from compositional in-context curricula.The present work aims to lay the foundations for such explorations, by considering a controlled setting where we could train models from scratch with controlled manipulations of data properties.</p>
<p>Societal Impact and Reproducibility While our work aims fundamental understanding of language models, we do not anticipate any immediate societal impact from this research.The codebase will be open-sourced upon acceptance.</p>
<p>A Loss and performance curve vanilla curriculum (8-8-8) Figure 9: Example loss and performance evolution on the curriculum (8-8-8) model and the vanilla model.In curriculum model, we observe the loss at all shots go down except for the shot 0 and shot 8, which correspond to zero-shot inference of two single tasks, since there is no information available of the task.Furthermore, we observe loss at the later shots in the same task sequence decreases earlier since more information is available in-context.We observe similar for the shot 0 in vanilla model.We also observe multiple plateaus followed by sudden drop of loss in both settings.</p>
<p>B Extended robustness results</p>
<p>C Extended results on linear probe</p>
<p>We train linear probe for 1) corresponding task y at each position, 2) task computation of a and 3) task parameter b, which are required for the compositional computation of b a x .Since b a x mod P = b a x mod (P −1) mod P , the intermediate values from task a, b that we try to decode from compositional task blocks are a x mod P and b.We train probes for the intermediate values in the subtask block as well.In subtask a block, we simply decode a x mod P (same as target value) and in subtask b, we decode b.We used unseen 1K test sequences and use 80/20 split for training and evaluating linear probe.We used scikit [30]</p>
<p>C.2 Linear probe -control baseline</p>
<p>We performed a control experiment with shuffled task parameters (a, b) to check the baseline performance and verify that our decoding accuracy is meaningful.The below figure shows that baseline decoding accuracy from shuffled task parameters is almost 0, confirming that our probe decoding accuracy is non-trivial.</p>
<p>D Attention pattern analysis</p>
<p>We visualize subset of the attention heads in layer 4-7 in the curriculum trained model and vanilla trained model, averaged on 2K evaluation sequences.In vanilla trained model, the attention pattern is continuous without outstanding block structure.On the other hand, curriculum trained model develops attention heads that show attention pattern from compositional task block to curriculum block which supports our hypothesis that the curriculum subtask representation is utilized in compositional task.x-axis shows token position attending from and y-axis shows token position attending to.We show all 48 token positions of 24 input-label pairs.For curriculum model, each task boundary is marked by ticks on x and y axis.For example, head 0 and head 3 at layer 6 from curriculum (8-8-8) shows a pattern of attending from each subtask to the last compositional task block.</p>
<p>E Extended result on mixed strategy</p>
<p>Figure 1 :
1
Figure 1: Overview of the setup and key results.a) Task schema.In curriculum training, each training sequence is composed of m exemplars for two single-exponential tasks defined by a and b, respectively, followed by n composite double-exponential task exemplars.In vanilla training, the model is trained with a sequence of 2m + n in-context exemplars for the double-exponential task defined by task parameters (a, b).b) Example asymptotic accuracy at the end of training on training and evaluation tasks for curriculum training (m = 10, n = 4) (top) and vanilla training (bottom).c) Comparison of the error counts for the last compositional block for curriculum training and vanilla training (left: m = 11, n = 2, right: m = 6, n = 12).The curriculum condition (blue) enhances robustness on unseen compositional task block compare to vanilla training (orange).d) In-context curricula (as shown in (a) top) promote representation of intermediate task parameters.We show linear probe decoding accuracy of task parameter b from unseen evaluation sequences.The curriculum-trained model represents the task parameter in compositional task block (last 4 shots) while the vanilla-trained model does not.structures.For example, many essays and informative writings contain multiple paragraphs, each of which introduces supporting arguments, and at the end follows a conclusion paragraph that composes all of the previous points.Texts of instructions or math problem solutions similarly contain compositional curriculum-like structure: first presents component knowledge, followed by the composition of these components.Inspired by this, we demonstrate that an in-context curriculum data structure -in-context examples of components and their composition-can give rise to robust compositional generalization ability.</p>
<p>Figure 2 :
2
Figure 2: Errors across entire context in vanilla vs. curriculum model.Gray dotted lines indicate the task boundaries in the curriculum.The curriculum model generalizes to each subtask after few-shots (first two single exponential tasks) and once the compositional task is present, fewer errors occur compare to vanilla setting from the zero-shot on (after the second gray dotted line).</p>
<p>Figure 3 :
3
Figure 3: In-context error counts of compositional task in vanilla model vs. curriculum models.We zoom into the compositional task block of Figure 2. The leftmost point is zero-shot of compositional task for curriculum model.We compare the error counts on compositional task of curriculum (m-m-n) models to that of corresponding n last exemplars from the vanilla model.The curriculum model shows higher robust (fewer error) than the vanilla model.See Appendix B.1 for more visualizations.numberof in-context exemplars for 2K evaluation sequences sampled with unseen task parameter combinations (a, b) averaged over 2 seeds.In Figure2, we show the trend of error counts across the entire context comparing the vanilla (orange) and curriculum (blue) settings.First, we see that the curriculum model first generalizes to the two subtasks (first two blocks) after few-shots.We notice that the curriculum enables zero-shot inference on the compositional task (after the second dotted task boundary).Compared to the non-informative zero-shot inference of the vanilla model, the zero-shot performance on the compositional task exhibits significantly fewer errors in the curriculum model.In contrast, the vanilla model results in a gradual decrease of errors as more examplars are shown, but the errors eventually saturate.</p>
<p>Figure 4 :
4
Figure 4: Linear probe decoding accuracy of target y for corresponding tasks and intermediate values from the subtasks needed for compositional task in the vanilla (top) and curriculum (10-10-4) setting (bottom).Both settings show high decoding of y for corresponding task blocks.In the curriculum setting, the intermediate values from the subtasks (a, b) required for compositional computation show high decodability in corresponding single task block and importantly, in compositional task block (shot 20-23), but not in the vanilla model.Noticeably, in the curriculum model, the highest decodability of intermediate values in compositional task block come from not the final layer but earlier layers (light green) indicating layer-wise processing of compositional computation.With in-context curricula, each subtask's information is well decoded in the corresponding subtask block.Most of all, the intermediate computation values involving the subtasks (a, b) are highly decodable in compositional task block (shot 20-23), suggesting that the subtask representation inferred from the curriculum is utilized in the compositional task.Especially, the high decoding accuracy at the zero-shot of compositional task (shot 20) indicates that the model readily utilizes the intermediate computation values inferred from the subtask curriculum to solve compositional task.In contrast, the vanilla-trained model shows lower decoding accuracy of intermediate values required for compositional computation, indicating that training with in-context curricula is what incentivizes the more compositional representations (at least, in terms of linear decodability).</p>
<p>Figure 5 :
5
Figure 5: The curriculum model performance on mismatch sequences:single exponential tasks of parameter (a, b) followed by double exponential task with mismatching paramters (a ′ , b ′ ).The models trained with curriculum (8-8-8) and (10-10-4) fail on the mismatching compositional task, showing no improvement with few-shot compositional examples.It indicates that the model relies on composition of subtask information given in the curriculum to solve the compositional task and thus cannot solve the problem without correct subtasks information.</p>
<p>Figure 6 :
6
Figure 6: In mismatch experiment, the model trained with longer compositional context shows slight decreasing of error suggesting the model does not only relies on compositional strategy but vanilla fewshot learning strategy as well.</p>
<p>Figure 7 :
7
Figure 7: Evidence of mixed strategy between vanilla and compositional solution modulated by curriculum design.We show decodability of intermediate computation involving each subtask (a, b) in the compositional task block across different curriculum designs in layer 4-6.At the zero-shot of the compositional task (at the dotted lines), both intermediate values are highly decodable in longer compositional task curriculum design (6-6-12, 4-4-16), and it decreases afterwards, indicating existence of both vanilla and compositional strategy.For the shorter compositional task design (10-10-4, 8-8-8), we observe high decodability consistently in entire block but the level of decodability is relatively higher with shorter compositional task length (yellow vs. green).These results suggest complex interplay between the different curricula design and a strategy the model employs.</p>
<p>Figure 8 :
8
Figure 8: a) Top: In the curriculum(4-4-16), the model can solve compositional task after many examples before subtask learning (pink before gray).The zero-shot loss decreases sharply only after the subtask lerning (blue after gray).Bottom: In the curriculm (10-10-4), the subtask learning is followed by the compositional task learning.The learning of both zero-shot and the last shot happen almost spontaneously (pink and blue after gray).b) Linear probe at before and after the subtask learning (blue and orange mark in panel a).Top: In the curriculum (4-4-16), we see that the model readily make good prediction on y in compositional task block after few in-context examples without learning subtasks (blue, near-1.0accuracy after 12 shots).After the subtasks are learned (orange), the model is capable of zero-shot inference where we can find representation of the subtasks (note the increased decodability of y at shots 8-10, and corresponding peaks in decodability of intermediate values from a and b).Bottom: In the curriculum (10-10-4), the model can generalize to the compositional task only after the subtasks are learned, as evidenced by low decodability of y before subtask learning (blue) and the high decodability of y and intermediate values after subtask learning (orange).We also see here that subtasks representations are more decodable in earlier layers.See Appendix E.4 for results on other layers.</p>
<p>B. 1
1
Additional visualization of robustness P = last occured index (compositional task)</p>
<p>Figure 10 :
10
Figure 10: visualization of error counts complementary to Figure 2-3.(Top) Error counts in entire context length across different curriculum designs.(Bottom)The counts of the last error occurred at each shot in the compositional task.We observe that the curriculum setting makes fewer errors in total compare to the vanilla models (total counts of each color) reflecting higher robustness of the curriculum models.Furthermore, strong right skewedness of the vanilla model indicates that the model tends to make errors even after many examples suggesting the model is uncertain about the task information.</p>
<p>B. 2
2
Robustness results on other values of PWe extend our results in robustness in other modulo values, P = 37 and P = 41.We observe that in-context curriculum can increase the robustness in compositional task generalization.The model architecture and the training setup was identical as given in Section 2. All curricula designs increase robustness for P = 37 similar to P = 59.For P = 41, only the curriculum (11-11-2) increases the robustness of the curriculum.We notice that when the single task block length is less than 11, the model does not learn to solve the single task (see high error bars in the first two blocks even after 10 examples).This indicates that the model needs more single task examples to identify the each subtask and this explains why the other curricula design with shorter subtask lengths are not helping compositional generalization for P = 41.</p>
<p>Figure 11 :
11
Figure 11: Error counts in vanilla vs. curriculum model for P = 37 and P = 41.(Top) Error counts across entire context length.(Bottom) Error counts in the compositional task block, left aligned to zero-shot at the compositional task.</p>
<p>package for the classifier training.Below diagram shows what is decoded in each block.</p>
<p>Figure 12 :
12
Figure 12: We show what variables we are decoding in each block in main Figure 4. Since b a x mod P = b (a x mod (P −1)) , the actual intermediate computation value from task a used for the compositional task is a x mod (P − 1), and we can decode this value in the compositional task block when curriculum is given while less in vanilla model.Same for b.</p>
<p>Figure 13 :
13
Figure 13: Control linear probe decoding.We used shuffled labels for linear probe training to validate the baseline performance.</p>
<p>Figure 15 :
15
Figure 15: Attention pattern from selected heads in layer 4-7.x-axis shows token position attending from and y-axis shows token position attending to.We show all 48 token positions of 24 input-label pairs.For curriculum model, each task boundary is marked by ticks on x and y axis.For example, head 0 and head 3 at layer 6 from curriculum(8-8-8)  shows a pattern of attending from each subtask to the last compositional task block.</p>
<p>E. 1 Figure 16 :
116
Figure 16: Mismatch experiment result on other curricula designs.(Top) In the curriculum with short compositional task block (11-11-2, 10-10-4, 8-8-8), we see that the model completely fails on the mismatch compositional task block.On the other hand, the curriculum with longer compositional task block (6-6-12, 4-4-16) show slight decrease as more examples are given, indicating standard few-shot learning in-context is occruing.</p>
<p>Figure 17 :
17
Figure 17: Extended result of main Figure 7.We show the decoding of the intermediate values in the compositional block in all layers.</p>
<p>Figure 18 :FigureFigure
18
Figure18: Extended result of main Figure7.We show loss curve of al shots.The loss curve above and the main Figure8are filtered using Savitsky-Golay filter (using scikit implementation[30]) with length 51 and polynomial order 3.</p>
<p>While simple, linear probing is the typical first pass at studying internal representations[12,<br />
].2 b a x mod P = b (a x ) mod (P −1) mod P . 3 See Appendix C.2 for control experiment with shuffled labels to confirm the baseline probe performance.
AcknowledgementsWe thank Basile Confavreaux, Sara Dragutinović, Yedi Zhang for helpful discussions and feedback.This work was supported by the following funding sources: Gatsby Charitable Foundation (GAT3850) and Sainsbury Wellcome Centre Core Grant from Wellocme Trust (219627/Z/19/Z) to JHL, AKS and AMS; Schmidt Science Polymath Award to AMS.AMS is a CIFAR Azrieli Global Scholar in the Learning in Machines &amp; Brains program.
Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Data distributional properties drive emergent in-context learning in transformers. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James Mcclelland, Felix Hill, Advances in Neural Information Processing Systems. 202235</p>
<p>Noam Chomsky, Derivation by phase. 1999118158028</p>
<p>Real-world visual statistics and infants' first-learned object names. Elizabeth Elizabeth M Clerkin, James M Hart, Chen Rehg, Linda B Yu, Smith, Philosophical Transactions of the Royal Society B: Biological Sciences. 372201600551711. 2017</p>
<p>Curriculum learning for human compositional generalization. Fabian Ronald B Dekker, Christopher Otto, Summerfield, Proceedings of the National Academy of Sciences. 11941e22055821192022</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Ronan Bhagavatula, Le Bras, Advances in Neural Information Processing Systems. 202436</p>
<p>The effects of information order and learning mode on schema abstraction. Renee Elio, John R Anderson, Memory &amp; cognition. 1211984</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281-21988</p>
<p>Ueber sinn und bedeutung. Gottlob Frege, Philosophical Review. 572091948</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, arXiv:2304.147672023arXiv preprint</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, arXiv:2305.016102023arXiv preprint</p>
<p>Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks. Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov, arXiv:2406.025502024arXiv preprint</p>
<p>. Roee Hendel, Mor Geva, Amir Globerson, arXiv:2310.159162023arXiv preprintIn-context learning creates task vectors</p>
<p>Curriculum learning for motor skills. Andrej Karpathy, Michiel Van De Panne, Advances in Artificial Intelligence: 25th Canadian Conference on Artificial Intelligence. Toronto, ON, CanadaSpringerMay 28-30, 2012. 20122012Canadian</p>
<p>When can transformers compositionally generalize in-context? ICML Next Generation of Sequence Modeling Architectures Workshop. Seijin Kobayashi, Simon Schug, Yassir Akram, Florian Redhardt, Razvan Johannes Von Oswald, Guillaume Pascanu, João Lajoie, Sacramento, 2024</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, International conference on machine learning. PMLR2018</p>
<p>Human-like systematic generalization through a metalearning neural network. M Brenden, Marco Lake, Baroni, Nature. 62379852023</p>
<p>The broader spectrum of in-context learning. Andrew Kyle Lampinen, Stephanie C Y Chan, Aaditya K Singh, Murray Shanahan, 2024</p>
<p>Jin Hwa, Lee , Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng, arXiv:2410.01444Geometric signatures of compositionality across a language model's lifetime. 2024arXiv preprint</p>
<p>Why do animals need shaping? a theory of task composition and curriculum learning. Jin Hwa Lee, Stefano Sarao Mannelli, Andrew Saxe, arXiv:2402.183612024arXiv preprint</p>
<p>Capturing gradience, continuous change, and quasi-regularity in sound, word, phrase, and meaning. The handbook of language emergence. L James, Mcclelland, 2015</p>
<p>Universal linguistic inductive biases via meta-learning. Thomas Mccoy, Erin Grant, Paul Smolensky, Thomas L Griffiths, Tal Linzen, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202042</p>
<p>Circuit component reuse across tasks in transformer language models. Jack Merullo, Carsten Eickhoff, Ellie Pavlick, arXiv:2310.087442023arXiv preprint</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, 2023</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.009412023arXiv preprint</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 211812020</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Competition dynamics shape algorithmic phases of in-context learning. Core Francisco Park, Ekdeep Singh Lubana, Itamar Pres, Hidenori Tanaka, arXiv:2412.010032024arXiv preprint</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>Grokking: Generalization beyond overfitting on small algorithmic datasets. Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, Vedant Misra, CoRR, abs/2201.021772022</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.033502022arXiv preprint</p>
<p>Quasi-compositional mapping from form to meaning: A neural network-based approach to capturing neural responses during human language comprehension. Milena Rabovsky, James L Mcclelland, Philosophical Transactions of the Royal Society B. 375201903131791. 2020</p>
<p>Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Allan Raventós, Mansheej Paul, Feng Chen, Surya Ganguli, Advances in Neural Information Processing Systems. 202436</p>
<p>Human curriculum effects emerge with in-context learning in neural networks. Jacob Russin, Ellie Pavlick, Michael J Frank, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202446</p>
<p>Meta-learning with memory-augmented neural networks. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap, International conference on machine learning. PMLR2016</p>
<p>The transient nature of emergent in-context learning in transformers. Aaditya Singh, Stephanie Chan, Ted Moskovitz, Erin Grant, Andrew Saxe, Felix Hill, Advances in Neural Information Processing Systems. 202436</p>
<p>What needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation. K Aaditya, Ted Singh, Felix Moskovitz, Stephanie Cy Hill, Andrew M Chan, Saxe, arXiv:2404.071292024arXiv preprint</p>
<p>Strategy coopetition explains the emergence and transience of in-context learning. K Aaditya, Ted Singh, Sara Moskovitz, Felix Dragutinovic, Stephanie Cy Hill, Andrew M Chan, Saxe, arXiv:2503.056312025arXiv preprint</p>
<p>The behavior of organisms: An experimental analysis. Frederic Burrhus, Skinner, 2019BF Skinner Foundation</p>
<p>On the proper treatment of connectionism. Paul Smolensky, Behavioral and brain sciences. 1111988</p>
<p>Discovering the compositional structure of vector representations with role learning networks. Paul Soulos, Thomas Mccoy, Tal Linzen, Paul Smolensky, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2020</p>
<p>. Zoltán Gendler Szabó. Compositionality. Metaphysics Research Lab. 2024Stanford Universityfall 2024 edition</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Dipanjan Samuel R Bowman, Das, International Conference on Learning Representations. 2019</p>
<p>A deep hierarchical approach to lifelong learning in minecraft. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, Shie Mannor, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Eric Todd, Millicent L Li, Arnab , Sen Sharma, Aaron Mueller, Byron C Wallace, David Bau, arXiv:2310.15213Function vectors in large language models. 2023arXiv preprint</p>
<p>Matching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, Advances in neural information processing systems. 292016</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, Will Wu, Luke Zettlemoyer, Huan Sun, Proc. of The 61st Annual Meeting of the Association for Computational Linguistics. of The 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick, arXiv:1611.05763Learning to reinforcement learn. 2016arXiv preprint</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4492021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>When do curricula work. Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur, International Conference on Learning Representations. 2021</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2022</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 2024</p>
<p>Which attention heads matter for in-context learning?. Kayo Yin, Jacob Steinhardt, arXiv:2502.140102025arXiv preprint</p>
<p>Parsel: Algorithmic reasoning with language models by composing decompositions. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, Nick Haber, Advances in Neural Information Processing Systems. 202336</p>
<p>The clock and the pizza: Two stories in mechanistic explanation of neural networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-278033540</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.17087v1.pdf" target="_blank">Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\% improvement compared to raw judgments and about 8.37\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zheng et al. (2023) — LLMs-as-judges evaluation study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported work exploring strong LLMs used as judges for open-ended questions and examined alignment with human preferences, identifying multiple judge failure modes and proposing mitigation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended question evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench; Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human preference judgments (human annotators used as ground truth in alignment analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>positional bias; verbosity; self-enhancement bias; limited reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>While LLM judges can align with human preferences, they exhibit systematic failure modes (positional bias, verbosity, self-enhancement) and have limited reasoning in some cases; alignment with humans does not imply absence of these biases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>scalability and efficiency compared to human evaluation (noted generally in related literature)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thakur2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thakur et al. (2024) — evaluation of alignment and vulnerabilities in LLMs-as-judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analyzed judge alignment using the TriviaQA benchmark and emphasized choosing appropriate agreement statistics (Cohen's kappa over percent agreement), noting that models aligned with humans on some metrics may still perform poorly on ranking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Question answering (TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TriviaQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human judgments used as reference for alignment</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (recommended over percent agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>models aligned with human judgments may not excel at ranking tasks; potential misinterpretation if using simple percent agreement</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Raw percent agreement can be misleading; more robust inter-rater metrics (Cohen's kappa) reveal different alignment behavior and vulnerabilities when comparing LLM judges to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>none reported specifically beyond discussion of metrics; emphasis on need for robust evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used TriviaQA; recommended Cohen's kappa for evaluating alignment</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7839.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7839.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgeBench/Tan2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tan et al. (2024) — JudgeBench benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>JudgeBench argues human preference labels are unreliable indicators of factual accuracy and logic on complex tasks and provides an algorithmic ground-truth pipeline to evaluate LLM judges without relying on human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A benchmark for evaluating llm-based judges.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A benchmark for evaluating llm-based judges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Objective correctness/factuality and logical evaluation across challenging tasks (knowledge, reasoning, math, coding)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>JudgeBench (combines MMLU-Pro, LiveBench, LiveCodeBench)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human preferences (argued to be unreliable for certain complex tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>human preferences can be unreliable indicators of factual accuracy and logical correctness on complex/superhuman tasks</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>For tasks that exceed human capabilities, human preference alignment is a poor proxy for objective correctness; algorithmic/objective ground truth is preferable for benchmarking LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Enables evaluation against objective ground truth without human annotation; reduces reliance on biased or fallible human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Provides 350 GPT-4o-generated response pairs and 270 Claude-generated pairs with algorithmic ground-truth labels across knowledge, reasoning, math, coding</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7839.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7839.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Li2025_MetaJudge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Li et al. (2025) — Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper proposes a three-stage meta-judge pipeline (rubric design with GPT-4 + multi-agent scoring + threshold selection) and argues for LLM-only meta-evaluation as an alternative to human-aligned evaluation, reporting substantial precision gains on JudgeBench-derived judgment data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluation of LLM judgments across knowledge, reasoning, math, and coding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>JudgeBench (raw judgments derived from JudgeBench response pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, LLaMa-3.1-405B-Instruct (used as meta-judge agents/benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Four advanced LLMs evaluated as meta-judges: GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, and LLaMa-3.1-405B-Instruct (specific model sizing/training details not provided beyond model names)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Precision (against JudgeBench algorithmic ground-truth labels); discussion of human-alignment metrics in related work</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Human evaluation: costly; time-consuming; biased; limited capability; unreliable for superhuman tasks; prior reliance on human alignment can be inappropriate for complex factual/logic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM meta-judges can reliably identify correct and high-quality judgments without human intervention; multi-agent aggregation reduces single-agent failure modes; different tasks favor different rubric/agent configurations; panel discussions can cause opinion convergence which may hurt extremely hard problems.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Efficiency, scalability, reduced need for human annotation, ability to evaluate superhuman or highly complex tasks, capacity to construct algorithmic/objective ground truth-based evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three-stage pipeline: (1) GPT-4-refined rubric (7 criteria, weights assigned); (2) multi-agent scoring with N agents (strategies: weighted averaging, majority voting, panel discussion); (3) threshold selection (threshold=4.5) to select high-quality judgments. Evaluated on 350 raw judgments (JudgeBench derived) primarily from GPT-4o-mini judge; primary metric = precision (TP / (TP+FP)). Reported improvements: ~15.55% improvement over raw judgments and ~8.37% over single-agent baseline (overall/aggregate improvements reported by task in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena. <em>(Rating: 2)</em></li>
                <li>Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges. <em>(Rating: 2)</em></li>
                <li>A benchmark for evaluating llm-based judges. <em>(Rating: 2)</em></li>
                <li>Meta-rewarding language models: Self-improving alignment with llm-as-a-metajudge. <em>(Rating: 1)</em></li>
                <li>Self-rationalization improves llm as a fine-grained judge. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7839",
    "paper_id": "paper-278033540",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Zheng2023",
            "name_full": "Zheng et al. (2023) — LLMs-as-judges evaluation study",
            "brief_description": "Reported work exploring strong LLMs used as judges for open-ended questions and examined alignment with human preferences, identifying multiple judge failure modes and proposing mitigation strategies.",
            "citation_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "mention_or_use": "mention",
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "evaluation_task": "Open-ended question evaluation",
            "dataset_name": "MT-Bench; Chatbot Arena",
            "judge_model_name": null,
            "judge_model_details": null,
            "human_evaluator_type": "human preference judgments (human annotators used as ground truth in alignment analyses)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "positional bias; verbosity; self-enhancement bias; limited reasoning",
            "qualitative_findings": "While LLM judges can align with human preferences, they exhibit systematic failure modes (positional bias, verbosity, self-enhancement) and have limited reasoning in some cases; alignment with humans does not imply absence of these biases.",
            "advantages_of_llm_judge": "scalability and efficiency compared to human evaluation (noted generally in related literature)",
            "experimental_setting": null,
            "uuid": "e7839.0",
            "source_info": {
                "paper_title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Thakur2024",
            "name_full": "Thakur et al. (2024) — evaluation of alignment and vulnerabilities in LLMs-as-judges",
            "brief_description": "Analyzed judge alignment using the TriviaQA benchmark and emphasized choosing appropriate agreement statistics (Cohen's kappa over percent agreement), noting that models aligned with humans on some metrics may still perform poorly on ranking tasks.",
            "citation_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges.",
            "mention_or_use": "mention",
            "paper_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges.",
            "evaluation_task": "Question answering (TriviaQA)",
            "dataset_name": "TriviaQA",
            "judge_model_name": null,
            "judge_model_details": null,
            "human_evaluator_type": "human judgments used as reference for alignment",
            "agreement_metric": "Cohen's kappa (recommended over percent agreement)",
            "agreement_score": null,
            "reported_loss_aspects": "models aligned with human judgments may not excel at ranking tasks; potential misinterpretation if using simple percent agreement",
            "qualitative_findings": "Raw percent agreement can be misleading; more robust inter-rater metrics (Cohen's kappa) reveal different alignment behavior and vulnerabilities when comparing LLM judges to humans.",
            "advantages_of_llm_judge": "none reported specifically beyond discussion of metrics; emphasis on need for robust evaluation",
            "experimental_setting": "Used TriviaQA; recommended Cohen's kappa for evaluating alignment",
            "uuid": "e7839.1",
            "source_info": {
                "paper_title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "JudgeBench/Tan2024",
            "name_full": "Tan et al. (2024) — JudgeBench benchmark",
            "brief_description": "JudgeBench argues human preference labels are unreliable indicators of factual accuracy and logic on complex tasks and provides an algorithmic ground-truth pipeline to evaluate LLM judges without relying on human annotation.",
            "citation_title": "A benchmark for evaluating llm-based judges.",
            "mention_or_use": "use",
            "paper_title": "A benchmark for evaluating llm-based judges.",
            "evaluation_task": "Objective correctness/factuality and logical evaluation across challenging tasks (knowledge, reasoning, math, coding)",
            "dataset_name": "JudgeBench (combines MMLU-Pro, LiveBench, LiveCodeBench)",
            "judge_model_name": null,
            "judge_model_details": null,
            "human_evaluator_type": "human preferences (argued to be unreliable for certain complex tasks)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "human preferences can be unreliable indicators of factual accuracy and logical correctness on complex/superhuman tasks",
            "qualitative_findings": "For tasks that exceed human capabilities, human preference alignment is a poor proxy for objective correctness; algorithmic/objective ground truth is preferable for benchmarking LLM judges.",
            "advantages_of_llm_judge": "Enables evaluation against objective ground truth without human annotation; reduces reliance on biased or fallible human judgments.",
            "experimental_setting": "Provides 350 GPT-4o-generated response pairs and 270 Claude-generated pairs with algorithmic ground-truth labels across knowledge, reasoning, math, coding",
            "uuid": "e7839.2",
            "source_info": {
                "paper_title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Li2025_MetaJudge",
            "name_full": "Li et al. (2025) — Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
            "brief_description": "This paper proposes a three-stage meta-judge pipeline (rubric design with GPT-4 + multi-agent scoring + threshold selection) and argues for LLM-only meta-evaluation as an alternative to human-aligned evaluation, reporting substantial precision gains on JudgeBench-derived judgment data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
            "evaluation_task": "Evaluation of LLM judgments across knowledge, reasoning, math, and coding tasks",
            "dataset_name": "JudgeBench (raw judgments derived from JudgeBench response pairs)",
            "judge_model_name": "GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, LLaMa-3.1-405B-Instruct (used as meta-judge agents/benchmarks)",
            "judge_model_details": "Four advanced LLMs evaluated as meta-judges: GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, and LLaMa-3.1-405B-Instruct (specific model sizing/training details not provided beyond model names)",
            "human_evaluator_type": null,
            "agreement_metric": "Precision (against JudgeBench algorithmic ground-truth labels); discussion of human-alignment metrics in related work",
            "agreement_score": null,
            "reported_loss_aspects": "Human evaluation: costly; time-consuming; biased; limited capability; unreliable for superhuman tasks; prior reliance on human alignment can be inappropriate for complex factual/logic tasks",
            "qualitative_findings": "LLM meta-judges can reliably identify correct and high-quality judgments without human intervention; multi-agent aggregation reduces single-agent failure modes; different tasks favor different rubric/agent configurations; panel discussions can cause opinion convergence which may hurt extremely hard problems.",
            "advantages_of_llm_judge": "Efficiency, scalability, reduced need for human annotation, ability to evaluate superhuman or highly complex tasks, capacity to construct algorithmic/objective ground truth-based evaluation pipelines.",
            "experimental_setting": "Three-stage pipeline: (1) GPT-4-refined rubric (7 criteria, weights assigned); (2) multi-agent scoring with N agents (strategies: weighted averaging, majority voting, panel discussion); (3) threshold selection (threshold=4.5) to select high-quality judgments. Evaluated on 350 raw judgments (JudgeBench derived) primarily from GPT-4o-mini judge; primary metric = precision (TP / (TP+FP)). Reported improvements: ~15.55% improvement over raw judgments and ~8.37% over single-agent baseline (overall/aggregate improvements reported by task in tables).",
            "uuid": "e7839.3",
            "source_info": {
                "paper_title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges.",
            "rating": 2,
            "sanitized_title": "judging_the_judges_evaluating_alignment_and_vulnerabilities_in_llmsasjudges"
        },
        {
            "paper_title": "A benchmark for evaluating llm-based judges.",
            "rating": 2,
            "sanitized_title": "a_benchmark_for_evaluating_llmbased_judges"
        },
        {
            "paper_title": "Meta-rewarding language models: Self-improving alignment with llm-as-a-metajudge.",
            "rating": 1,
            "sanitized_title": "metarewarding_language_models_selfimproving_alignment_with_llmasametajudge"
        },
        {
            "paper_title": "Self-rationalization improves llm as a fine-grained judge.",
            "rating": 1,
            "sanitized_title": "selfrationalization_improves_llm_as_a_finegrained_judge"
        }
    ],
    "cost": 0.0121725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments
23 Apr 2025</p>
<p>Yuran Li 
Jama Hussein Mohamud 
Chongren Sun 
Di Wu 
Benoit Boulet 
Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments
23 Apr 2025EADFAFD91027125D4E2EA0400B22A106arXiv:2504.17087v1[cs.AI]
Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging.Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative.However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment.Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored.To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments.Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric.Experimental results on the JudgeBench dataset show about 15.55% improvement compared to raw judgments and about 8.37% improvement over the single-agent baseline.Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.</p>
<p>Introduction</p>
<p>With the growing adoption of Large Language Models (LLMs) across various domains and applications, particularly in fields such as medicine, law, and education, ensuring the quality of their responses has become increasingly critical (Hadi et al., 2023).Evaluating the quality of LLMs' responses help us discard incorrect information and provide feedback to improve the quality of LLMs' responses (Chang et al., 2024).</p>
<p>The existing evaluation frameworks for the quality of LLMs' output include human evaluation, automated bench-marking and model-based evaluation.Human evaluation is costly, time-consuming, and limited by the bias and capability of human evaluators.Automated benchmarking is constrained by its fixed form and also requires human annotation.In contrast, model-based evaluation overcomes these limitations, accommodating diverse input formats while being cost-effective and time-efficient.Therefore, recently, numerous works have emerged exploring how LLMs act as evaluators of LLMs' output.These proposals vary in their approaches to configuring LLMs as judges, ranging from simple prompting techniques (zero-shot, few-shot, chainof-thought) (Stahl et al., 2024) to training LLMs on sample judgment data (Chan et al., 2023a).The approaches also differ in their use of LLMs (single judge versus multiple judges (Verga et al., 2024)) and the level of engagement of the judges (single-turn prompts versus debates (Khan et al., 2024)).</p>
<p>Meanwhile, just as we need to evaluate the quality of LLMs' responses, we also need to assess the quality of their judgments.Previous research has focused on evaluating LLMs' judgments based on alignment with human preference (Zheng et al., 2023).However, in factually and logically complex tasks that exceed human capabilities, LLMs have demonstrated significant advantages over human (Tan et al., 2024).In such cases, using human alignment as the primary evaluation metric becomes less applicable.An automated LLM-meta-judge pipeline for evaluating judgment data could enhance LLMs' ability to address superhuman tasks.As a result, recent research has been actively exploring advancements in LLM-as-a-judge through self-reflection, enabling models to evaluate their judgments (LLM-as-metajudge) (Wu et al., 2024) (Trivedi et al., 2024).These studies show significant potential for improving LLMs' judgment capabilities through meta-judging.However, current methods only adopt simple meta-judging methods, using the same model for both judging and meta-judging tasks.Fur-thermore, the performance of meta-judging is evaluated indirectly, based solely on its impact on LLM-as-a-judge results, without directly assessing the precision or accuracy of meta-judging itself.</p>
<p>A systematic approach to meta-judging remains largely unexplored.To address this significant research gap, we propose a meta-judging approach that incorporates comprehensive rubric design, a multi-agent module, and score-based threshold selection.Moreover, we benchmark the performance of various LLMs as meta-judges in different configurations.</p>
<p>Our main contributions are:</p>
<p>• We establish a complete meta-judging pipeline incorporating a multi-agent module to accurately identify and select correct judgments from a mixed set.</p>
<p>• We extensively evaluate diverse large language models with various prompt configurations and multi-agent strategies, demonstrating significant precision improvement and analyzing underlying factors.</p>
<p>Related Work</p>
<p>Evaluation of LLM Responses</p>
<p>As LLMs continue to advance, evaluating the quality of their responses has become a critical research focus.Guo (Guo et al., 2023) highlights key evaluation aspects, including knowledge and capability, alignment, and safety.Evaluation methods typically fall into three categories: automated benchmarking, human evaluation, and model-based evaluation, each with distinct advantages and limitations (Fourrier, 2023).Automated benchmarking uses datasets (e.g., TruthfulQA (Lin et al., 2021), BIG-bench (Srivastava et al., 2022), and SelfAware (Yin et al., 2023)) and metrics to evaluate LLMs on specific tasks (Zhang et al., 2022) (Zhang et al., 2019) but faces issues like dataset contamination, difficulty in task decomposition, and normalization challenges.</p>
<p>Human evaluation allows for open-ended assessments and avoids contamination but is costly and prone to biases, such as reliance on first impressions and domain knowledge gaps (Wang et al., 2023).Model-based evaluation provides a more efficient alternative, using high-capability generalist models or specialized models trained on preference data.Methods like NLI-based approaches (Maynez et al., 2020), QAQG (Manakul et al., 2023a), FActScore (Min et al., 2023), and self-evaluation (Manakul et al., 2023b) are prominent but suffer from biases, scoring inconsistencies, and occasional misalignment with human rankings.</p>
<p>Techniques encouraging reasoning before scoring can reduce these issues, though challenges in consistency and interpretability persist.</p>
<p>Evaluation of LLM-as-a-Judge</p>
<p>The LLM-as-a-judge paradigm offers a scalable alternative to the human evaluation of LLMs.Zheng (Zheng et al., 2023) explored strong LLMs as judges for open-ended questions, identifying limitations such as positional bias, verbosity, self-enhancement bias, and limited reasoning while proposing mitigation strategies and verifying alignment with human preferences.Thakur (Thakur et al., 2024), using the TriviaQA benchmark, emphasized Cohen's kappa over percent agreement for evaluating alignment, revealing that models aligned with human judgments may not excel at ranking tasks.Verga (Verga et al., 2024)  Currently, some works have been done to bypass human annotation, and evaluate LLM-as-a-judge in a more objective perspective.As in the paper, JudgeBench (Tan et al., 2024), Tan argues that human preferences are unreliable indicators of factual accuracy and logic in complex tasks.They propose a pipeline to generate challenging response pairs for LLMs to assess the precision of their judgments against objective ground truth labels, eliminating reliance on human annotation.Meanwhile, some experts have proposed that a promising way for improving LLM-as-judge is self-reflection on their own judgments (meta-judge), as discussed in (Wu et al., 2024) and (Trivedi et al., 2024).This approach leverages the meta-judge capabilities of LLMs to refine and enhance their own evaluation processes.These studies highlight a critical aspect of evaluating the judgment capabilities of LLMs through meta-judging and underscore the importance of accurately selecting correct judgments generated by them.The information flow for responses evaluation, judgments evaluation, and reinforcement learning from AI feedback is illustrated in Figure 1.</p>
<p>Actor Judge Meta-Judge</p>
<p>Response Judgment</p>
<p>LLMs</p>
<p>Approach</p>
<p>In this section, we introduce a three-stage approach for effectively selecting correct judgments from the raw judgment dataset generated by LLMs. the framework is indicated in Figure 2. Firstly, at the prompt design stage, GPT-4 improves the basic human-crafted rubric into detailed descriptions and assigns weight to each criterion.In meta-judge score calculation stage, N LLMs collaborate to score the judgements based on the preset rubric.In the selection stage, the meta-judge score shows the judgments' alignment with the ground truth label and the judgments with a meta-judge score higher than the threshold are selected as trustworthy judgments.</p>
<p>Prompt Design</p>
<p>As illustrated in Figure 2, the Meta-Judge framework begins with the prompt design stage, where human input is initially required to define a basic rubric.GPT-4 is then utilized to refine the rubric into a detailed description and explain the corresponding scoring system.The rubric consists of seven criteria on a scale of 1 to 5, which collectively evaluate the judgment from multiple perspectives, including accuracy, logical soundness, completeness of evaluation, fairness, relevance to context, clarity of explanation, and impactfulness.An example of a criterion is shown in Table 1.Furthermore, GPT-4 can assign weights to each criterion based on the specific meta-judge scenario.</p>
<p>The final input prompt includes instruction for the LLMas-a-judge, the judgment (comprising a decisive conclusion and explanation), and the rubric for the meta-judge.</p>
<p>Meta-Judge</p>
<p>In the meta-judge stage, a multi-agent module comprising N advanced LLMs evaluates the judgments of the LLMs based on the rubric established during the initial prompt design phase.Each agent can generate its meta-judge score for each criterion, either independently or collaboratively through discussion.The scores from each agent for each criterion are then aggregated to produce a comprehensive final score.</p>
<p>Three multi-agent collaboration strategies are put forward: weighted averaging, majority voting, and panel discussion.</p>
<p>WEIGHTED AVERAGING</p>
<p>When we adopt the weighted average strategy, the metajudge score is independently generated by each agent, then the final score is calculated as equation ( 1):
final score = M i=1 w agent i N j=1 w c j * S ij(1)
Criterion: Logical Soundness Description: Measures if the decision-making process follows a clear and logical reasoning path.</p>
<p>Score 1: Illogical, with no clear reasoning or consistency.</p>
<p>Score 2: Mostly illogical, with several gaps or flawed reasoning.</p>
<p>Score 3: Partially logical, with some inconsistencies in the reasoning process.</p>
<p>Score 4: Mostly logical, with minor flaws in reasoning.</p>
<p>Score 5: Highly logical, with a clear and consistent reasoning process throughout.</p>
<p>Table 1: Example of meta-judging rubric.This is a short version.In the experiment section, a longer version is also adopted, featuring a three-sentence description and a twosentence explanation of the scoring system.</p>
<p>where S ij represents the score generated by agent i for criterion j.The weight w c j reflects the varying importance of different criterion,</p>
<p>MAJORITY VOTING</p>
<p>When majority voting is adopted, each agent independently generates a score.The number of scores that exceed the selection threshold is counted.If more than half of the agents assign scores above the threshold, the final score is set to 5; otherwise, it is set to 1.It is calculated as equation ( 2):
final score = 5, if M i=1 I(S i &gt; T ) &gt; M 2 , 1, otherwise. (2)
where
S i = N j=1 w c j * S ij
, is the score generated by a single agent.I(S i &gt; T ) is an indicator function, it equals 1, if S i &gt; T , and 0 otherwise.</p>
<p>PANEL DISCUSSION</p>
<p>As shown in papers (Li et al., 2024) and (Chan et al., 2023b), having each agent play a distinct role and engage in collaborative discussions can lead to less biased and higher-quality outputs.Therefore, in the multi-agents module, collaborative reasoning is designed to derive the meta-judge score</p>
<p>Selection Based on Meta-Judge Score</p>
<p>According to the preset rubric, the final meta-judge score indicates the reliability of judgment.Therefore, the metajudge score can be used to filter out those unreliable judg-ments and only keep those high-quality ones that provide more reliable, decisive conclusions and more helpful explanations for their judgments.</p>
<p>To evaluate the capability of LLMs as meta-judges, it is necessary to verify whether judgments with high scores are truly correct.Specifically, we must determine a score threshold above which judgments can be considered reliable.Hence, a threshold T is set, then judgments with a metajudge score higher than the threshold are selected as correct judgments, while the judgment with a meta-judge score lower than the threshold will be discarded.This thresholdbased selection process enables the evaluation of precision, which serves as an indicator of the LLMs' effectiveness as meta-judges.On the basis of the JudgeBench dataset, multiple judgments for response pairs could be generated.The collection of these judgments form our meta-judge experimental dataset, referred to as the raw judgment dataset.An example of a raw judgment dataset is shown in Fig. 4.</p>
<p>Experiment</p>
<p>JudgeBench Dataset</p>
<p>Pair_id Evaluation Metric: JudgeBench provides the ground truth labels for the response pairs.The judgment GT label is determined based on whether it matches the ground truth label of the corresponding response pair.we compute precision between the GT labels and our generated labels as equation ( 4):
Precision = N correct N selected = T P T P + F P(4)
The reason we choose precision as our primary evaluation metric is that, for effective meta-judging to improve judging performance, providing high-quality and correct metajudgments is our top priority.Minimizing false positives, where incorrect meta-judgments are classified as correct, is our key objective.</p>
<p>Benchmarking Agents: We choose four advanced LLMs as our benchmarking agents: GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet,and LLaMa-3.1-405B-Instruct.</p>
<p>Baselines and Experimental Configurations:</p>
<p>To date, meta-judging has been explored only in single-agent settings.Therefore, for comparison, multiple meta-judge agents are first evaluated individually to identify the bestperforming single-agent strategy.Then, different collaboration approaches in multi-agent setups are compared.</p>
<p>For the single-agent meta-judge comparison, we consider the following configurations: (1) Baseline meta-judge method as in paper (Wu et al., 2024).( 2) Short Scoring Rubric: Meta-judging with a short rubric to generate a score (soft label).</p>
<p>(3) Long Scoring Rubric: Meta-judging with a long rubric to generate a score (soft label).( 4) Binary Rubric: Meta-judging to generate a True/False decision (hard label).</p>
<p>For the multi-agent meta-judge comparison, we consider the following approaches: (1) Weighted Averaging: calculates the meta-judge score by taking a weighted average of the scores independently derived by each agent.</p>
<p>(2) Majority voting: Determines the meta-judge score by counting how many agents' scores exceed a predefined threshold.(3) Panel discussion: derives the meta-judge score through a collaborative discussion among the agents.</p>
<p>Hyperparameters: The weight w c j of each criterion in the rubric is determined based on its importance and can be dynamically adjusted for different scenarios.GPT-4 is tasked with assigning these weights during the rubric refinement process.All the criterion weights are shown in Table 2.The weight w agent i is allocated based on the performance of agent i on task.By default, all agents are treated equally, with w agent i = 1/number(agents).In some cases, if the agent i demonstrates superior performance on a specific task, a higher weight could be assigned to w agent i .The selection threshold is set to a fixed value of 4.5 to ensure both high precision and robustness.</p>
<p>Criterion</p>
<p>Results</p>
<p>SINGLE AGENT PRECISION COMPARISON</p>
<p>The current meta-judging method, which uses a onesentence rubric to guide the judge model in evaluating its own judgments (Wu et al., 2024), serves as the baseline.In comparison, various rubrics and meta-judging agents are evaluated to identify the optimal configuration for a single meta-judging model.</p>
<p>From  3.</p>
<p>nearly half being incorrect.Meta-judge selection significantly improves precision in the selected judgments.For the Knowledge task, using the same meta-judge model as the judging model with a concise baseline rubric achieves the highest precision, likely due to internal knowledge overlap.For the Reasoning task, a longer, detailed rubric with a finer-grained scoring range improves precision, though overall performance remains lower due to task complexity.For the Math task, adopting a more detailed and comprehensive rubric improves performance over the baseline.For the Coding task, a simpler rubric is more effective, as criteria such as contextual relevance and fairness are less applicable to programming systems.Including irrelevant criteria can negatively impact meta-judging performance.</p>
<p>In conclusion, for knowledge tasks, using the same metajudge model as the judging model is beneficial due to internal knowledge overlap, which enhances self-reflection.</p>
<p>Complex reasoning tasks benefit from powerful large models and longer rubrics, while math tasks perform better with comprehensive rubrics.Although Claude, with a detailed rubric, achieves high meta-judge precision, its high variance suggests it is better suited for specific tasks.For diverse tasks, a simple baseline rubric shows more robust performance.</p>
<p>Table 4 examines whether using a more powerful model (GPT-4o) for meta-judging, instead of relying on selfreflection, is more effective when the judge model (LLaMA-3.1-8B)has limited capability.The results show that adopting a stronger LLM for meta-judging, paired with a rubric designed based on the above conclusions, improves precision by approximately 16%.</p>
<p>MULTI-AGENT PRECISION COMPARISON</p>
<p>Given the limitations of a single agent, where no single model can achieve optimal performance across all tasks, we propose utilizing a multi-agent module to address this challenge.Three multi-agent aggregation approaches, discussed in Section 3.2 are evaluated, and the results are presented in Table 5.</p>
<p>From   in knowledge and coding tasks, with gains of 6.61% and 19.04%, respectively.The weighted average strategy excels in reasoning tasks, achieving a 9.40% improvement.Meanwhile, the panel discussion strategy performs particularly well in math tasks, yielding a 9.00% improvement.</p>
<p>These three strategies aggregate agents' capabilities from different perspectives.Majority voting and weighted averaging are forms of late aggregation; agents do not influence each other's generation.Majority voting selects a result agreed upon by more than half of the agents, while weighted averaging computes the final score by combining and averaging the opinions of all agents.In contrast, panel discussion represents early aggregation, where agents' opinions influence one another during the early stages, ultimately producing a final result.In Q&amp;A and multi-turn dialogue tasks, panel discussions consistently outperform other methods.However, when applied to meta-judging tasks, their performance fell short of expectations.This discrepancy could be attributed to the handling of challenging tasks, where maintaining diverse opinions among different agents helps improve the correctness of the final decision.However, during panel discussions, the opinions of different agents tend to converge over time, which is unfavourable for addressing extremely difficult problems.</p>
<p>Computation Cost Analysis: When each agent generates its meta-judge score independently, we only need to run each agent once.In this case, the total computation cost is given by N i=1 cost i , where N is the number of agents.In contrast, if the agents engage in a discussion to exchange opinions, as illustrated in Figure 3</p>
<p>ABLATION STUDY</p>
<p>In this section, we evaluate the performance of various panel discussion structures to analyze the impact of the number of agents and distinct role assignments on meta-judging precision.</p>
<p>Number of Agents:</p>
<p>Setting aside the summarization agent, we first evaluate the impact of the number of agents in the discussion panel, as shown in Fig 5 .The results indicate that, across all tasks, a two-agent panel discussion structure performs best.However, when different roles such as expert, critic, and general public are assigned to each agent, the performance slightly decreases.Furthermore, increasing the number of agents to three does not lead to further improvement; instead, the performance shows a slight decline.Possible explanations for this results could be: First, in the meta-judging task, each agent is assigned a role as a metajudge, and introducing additional role descriptions could cause ambiguity in role definition.Second, the different roles in the meta-judging tasks do not result in significantly varied solutions to the problem.Third, as the number of panel agents increases, their cognitive errors may interact and amplify one another.</p>
<p>Implementation of the Summarization Agent: With a fixed number of agents, we evaluate the impact of the summarization agent, as shown in Table 6.The results indicate that incorporating a summarization agent does not improve the performance of meta-judging.Instead, it slightly weakens the meta-judge capability of the multi-agent module.The result may be due to information overload, where the summarization agent must manage information for the question, responses, judgments, and all the meta-judgments from different agents.</p>
<p>Summarization Overall</p>
<p>✗</p>
<p>72.58 ✓ 65.38</p>
<p>Table 6: Ablation study on the impact of the summarization agent.The multi-agent module consists of two agents.</p>
<p>When the summarization agent is not utilized, majority voting is used to aggregate the scores and derive the final result.</p>
<p>Discussion</p>
<p>In our work, we put forward a multi-agents meta-judge selection framework.Through this framework, good judgments with high-quality explanations and correct decisive conclusions could be selected, and the precision of the selected judgment set could be improved.</p>
<p>Current works primarily focus on evaluating LLM judgments based on their alignment with human judgment, treating human judgment as the ground truth to optimize LLM performance.However, these studies overlook biases and mistakes in human judgment.Additionally, they do not propose methods for selecting judgments with correct conclusions and high-quality explanations.In contrast, our work demonstrates that LLMs can evaluate judgments without human intervention, accurately identifying good and bad judgments through the meta-judge score.</p>
<p>Limitations: Our experiment evaluates the performance of the meta-judge selection framework using a limited judgment set consisting of 350 judgments for response pairs generated by GPT-4o-mini.The limited dataset may restrict the generalizability of some of our experimental conclusions.</p>
<p>Future Work: The remarkable performance of LLMs as meta-judges shows their potential for leveraging meta-judge scores to construct preference datasets, facilitating the training of LLMs as judges and enhancing their performance in an unsupervised manner.As demonstrated in prior work (Wu et al., 2024) and (Trivedi et al., 2024), this approach can be further enhanced through our proposed meta-judge framework, which incorporates more sophisticated rubrics and aggregated results from multiple agent evaluations, thereby improving the reliability and robustness of the meta-judge process.</p>
<p>Conclusion</p>
<p>In this paper, we propose a meta-judge selection framework aimed at minimizing human intervention and fully leveraging LLMs for meta-judgment.The framework requires only a human-defined basic rubric at the initial stage, after which all subsequent steps are carried out autonomously by LLMs.Additionally, the source experimental dataset we adopted, JudgBench, reduces reliance on human annotations by utilizing algorithmically generated objective labels.To further enhance evaluation accuracy, our framework incorporates a multi-agent module that effectively combines the strengths of multiple LLMs, overcoming the limitations of single-agent approaches.</p>
<p>Experimental results on raw judgment sets derived from JudgBench demonstrate that LLMs can perform highprecision evaluations of their own judgments.The metajudge scores produced by our framework reliably indicate judgment quality, enabling the precise identification of highquality judgments with an appropriately chosen threshold.This work lays a solid foundation for extending the capabilities of LLMs as judges to tackle superhuman-level tasks.By leveraging meta-judgment, we provide a promising pathway toward fully autonomous, scalable, and accurate evaluation systems driven by LLMs.</p>
<p>A. Detail of different rubrics</p>
<p>In the single-agent precision comparison section, we analyzed the impact of four different rubric configurations.Section 3.1 provides an example of the 'logical soundness' criterion in a short rubric.Here, we offer more details regarding the baseline, long, and binary rubrics.</p>
<p>Criterion: Logical Soundness Description:</p>
<p>Assesses whether the judgment or decision follows a coherent and logical progression from the evidence or reasoning process.A well-reasoned decision should clearly demonstrate how conclusions were drawn and avoid logical fallacies or contradictions.This ensures the reasoning process is transparent and defensible.</p>
<p>Score 1: Decision-making process is illogical, lacking clear reasoning or consistency.The conclusion appears arbitrary or disconnected from the supporting evidence.</p>
<p>Score 2: Decision-making process shows significant gaps or logical flaws, making it difficult to follow.Reasoning is inconsistent, and critical errors undermine the validity of the conclusion.</p>
<p>Score 3: Decision-making process is moderately logical, but some inconsistencies or gaps weaken its coherence.While the reasoning is partially sound, certain steps may appear unclear or unsupported.</p>
<p>Score 4: Decision-making process is mostly logical, with minor issues that do not undermine its overall integrity.The reasoning is generally clear and follows a structured progression with only slight missteps.</p>
<p>Score 5: Decision-making process is entirely logical, with clear and consistent reasoning throughout.Every step in the reasoning process is well-supported and leads naturally to the conclusion.</p>
<p>Table 7: Example of meta-judging long version rubric.This version offers a detailed three-sentence description for each criterion and a two-sentence explanation for each scoring range.</p>
<p>Criterion: Logical Soundness</p>
<p>Description: Measures if the decision-making process follows a clear and logical reasoning path.</p>
<p>Prompt: Finally, determine the correctness of the judgment and decision based on the above rubrics.Please respond strictly in the following format: result: [correct/wrong], Explanation:</p>
<p>Table 8: Example of meta-judging binary version rubric.This version provides a brief description of each criterion and requires meta-judges to make a correct or incorrect determination based on all criteria collectively rather than evaluating them separately.</p>
<p>Description: Explain which judgment is more accurate according to the original rubric and why.Combine judgment and decision to finally assign a score and consider factors such as adherence, accuracy, and consistency to the judgment instruction.</p>
<p>Prompt: Please respond strictly in the following format: Score: [1-5], Explanation:</p>
<p>Table 9: Example of meta-judging baseline rubric.This baseline rubric is referred from the current paper regarding meta-judging (Wu et al., 2024).This rubric is adapted from the current paper on meta-judging (Wu et al., 2024).The baseline rubric does not include descriptions for individual factors, such as adherence and accuracy, and assigns an overall score encompassing all factors rather than separate scores for each.</p>
<p>Figure 1 :
1
Figure 1: Method components and interactions.The actor (an LLM) generates responses, the judge evaluates their quality, and the meta-judge assesses the judge's evaluation.In the diagram, the backward RL (DPO) arrow represents Reinforcement Learning from AI Feedback (RLAIF), using DPO as the training method.</p>
<p>Both weights can be dynamically adjusted to suit specific downstream tasks.</p>
<p>Figure 2 :Figure 3 :
23
Figure2: LLM-as-meta-judge framework.The rubric is predefined in the prompt design stage.We benchmark the judgment using N agents, each providing a score based on a rubric.These N scores are then aggregated through metric calculations to yield a comprehensive score reflecting the LLM judge's performance.</p>
<p>We select JudgeBench(Tan et al., 2024) as source dataset to evaluate how well our method performs.JudgeBench combines challenging datasets such as MMLU-Pro(Wang et al., 2024), LiveBench(White et al., 2024), and LiveCodeBench(Jain et al., 2024), comprising 154 questions on knowledge, 98 on reasoning, 56 on mathematics, and 42 on coding, to generate difficult-to-distinguish response pairs for LLM evaluation.It includes 350 unique response pairs generated by GPT-4o and 270 unique response pairs generated by Claude-3.5-Sonnet.The datasets contain objective ground truth labels and algorithms to verify correctness.</p>
<p>Figure 5 :
5
Figure5: Ablation study on the impact of the number and roles of agents across different tasks.The vertical axis represents the precision of the selected judgments by multi-agent meta-judging.The horizontal axis represents different tasks and the overall summary of all tasks.</p>
<p>(a) and (b), the computation cost increases.Specifically: For the scenario depicted in Figure 3 (a), the cost doubles to 2 * N i=1 cost i .For the scenario depicted in Figure 3 (b), an additional aggregation cost is incurred, yielding 2 * N i=1 cost i + cost sum .</p>
<dl>
<dt>Figure 4: Generation of Raw Judgments from the JudgeBench Dataset.A judgment is considered true if it matches the label for the answer pairs; otherwise, it is false.</dt>
<dd>001Source: mmlu-pro-law Question: …………… ×350Answer 1: ………Answer 2: ………Label: Answer 1 &gt; Answer 2LLM-as-judge modelRaw Judgment SetPair_id: 001Label: Answer 1 &gt; Answer 2Judgment: Answer 1 = Answer2Explaination: ………</dd>
</dl>
<p>Table 2 :
2
Hyperparameter for the weight of each criterion in the rubric.A higher weight means the criterion score has a larger proportion in the final score, making it more important.
w c j</p>
<p>Table 3
3ConfigAgentKnowledge Reasoning Math Coding OverallRaw Judgments \62.3452.0478.5759.5261.71baselinesame as judge model73.6860.8771.4366.6768.89SR Selectiongpt-4o68.1357.9777.7858.0665.68gpt-4o-mini71.6262.5076.3260.0068.65claude65.7163.4979.1770.5067.47llama66.1455.5677.5561.1164.51LR Selectiongpt-4o67.9258.3377.5052.1765.24gpt-4o-mini67.1465.9175.6860.0067.84claude71.0566.6779.1758.6270.15llama65.6254.2277.5560.5363.76BR Selectiongpt-4o62.0754.7676.4757.5061.88gpt-4o-mini63.5754.7678.8560.5363.38claude62.9151.5877.7858.5461.58llama63.7055.2977.3659.5263.19
, we observe that raw judgments generated by the GPT-4o-mini judge model exhibit low precision, with</p>
<p>Table 3 :
3
Meta-Judge Precision of different agents across various configurations.Raw judgments are from Arena-Hard Judge on JudgeBench using GPT-4o-mini.Configuration LR means Long Rubric, SR means Short Rubric, and BR means Binary Rubric.
ConfigAgentKnowledge Reasoning Math Coding OverallRaw Judgments \50.0053.0655.3638.1050.29baselinesame as judge model50.9954.1759.6239.0251.76SelectionGPT-4o66.0773.3366.6762.5067.77</p>
<p>Table 4 :
4
Meta-Judge Precision of different agents across various configurations.Raw judgments are from Arena-Hard Judge on JudgeBench using Llama-3.1-8B-Instruct.Selection uses a stronger meta-judging model with a rubric designed on the basis of Table</p>
<p>Table 5 :
5
Table5, we can see that the majority voting strategy significantly improves meta-judge selection precision Precision on Different multi-agent configurations.Raw judgments are from Arena-Hard Judge on JudgeBench using GPT-4o-mini.The multi-agent module includes the two best-performing models based on meta-judging: GPT-4o-mini and Claude.
ConfigKnowledge Reasoning Math Coding Overallraw judgment collection62.3452.0478.5759.5261.71baseline73.6860.8771.4366.6768.89Majority Voting79.0769.2380.0085.7177.26Weighted Average75.0070.2778.7285.7175.56Panel Discussion71.6268.5780.4375.0072.58
Intelligent Automation Lab, McGill University
Mila, Quebec AI Institute. Correspondence to: Yuran Li <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#117;&#114;&#97;&#110;&#46;&#108;&#105;&#64;&#109;&#97;&#105;&#108;&#46;&#109;&#99;&#103;&#105;&#108;&#108;&#46;&#99;&#97;">&#121;&#117;&#114;&#97;&#110;&#46;&#108;&#105;&#64;&#109;&#97;&#105;&#108;&#46;&#109;&#99;&#103;&#105;&#108;&#108;&#46;&#99;&#97;</a>.
Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.B. Panel discussion detailThe panel discussion is conducted by recording the meta-judging history of each agent and providing it as a reference for other agents during meta-judging.The detailed prompt template and role description are shown in Table10.The role description is outlined in detail in Section B.1.[Role description]you are a ......[Question]Question from mmlu-pro.Both assistants provided incomplete solutions, but Assistant A's approach is closer to the correct logic.Assistant B's condition is fundamentally incorrect.[Decision][meta-judgment history] Criterion: Accuracy of Judgment Score: 5 Explanation: The judgment accurately interprets the problem and correctly identifies Assistant A has more correct logic.[System] While scoring, refer to the meta-judging results from other agents.Identify the agreedupon opinions and analyze any differing viewpoints.Table10: The prompt template for a panel discussion with different role descriptions.B.1. Role description for agentsGeneral PublicYou are a general public meta-judge assistant designed to ensure fairness in evaluating the quality of the judgment and decision made by a judge assistant.ExpertYou are an expert meta-judge assistant with advanced expertise in evaluating the quality of the judgment and decision made by a judge assistant.CriticYou are a critic meta-judge assistant tasked with providing critical analysis in evaluating the quality of the judgement and decision made by a judge assistant.SummarizationYou are a meta-judge coordinator assistant in aggregating the meta-judgments from other agents.
C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Towards better llm-based evaluators through multi-agent debate. 2023aarXiv preprint</p>
<p>C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Towards better llm-based evaluators through multi-agent debate. 2023barXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>C Fourrier, Evaluating llms: A comprehensive guide. 2023</p>
<p>Z Guo, R Jin, C Liu, Y Huang, D Shi, L Yu, Y Liu, J Li, B Xiong, D Xiong, arXiv:2310.19736Evaluating large language models: A comprehensive survey. 2023arXiv preprint</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M B Shaikh, N Akhtar, J Wu, S Mirjalili, 2023Authorea Preprints</p>
<p>Livecodebench: Holistic and contamination free evaluation of large language models for code. N Jain, K Han, A Gu, W.-D Li, F Yan, T Zhang, S Wang, A Solar-Lezama, K Sen, I Stoica, arXiv:2403.079742024arXiv preprint</p>
<p>A Khan, J Hughes, D Valentine, L Ruis, K Sachan, A Radhakrishnan, E Grefenstette, S R Bowman, T Rocktäschel, E Perez, arXiv:2402.06782Debating with more persuasive llms leads to more truthful answers. 2024arXiv preprint</p>
<p>Mateval: A multi-agent discussion framework for advancing open-ended text evaluation. Y Li, S Zhang, R Wu, X Huang, Y Chen, W Xu, G Qi, Min , D , 2024</p>
<p>S Lin, J Hilton, O Evans, Truthfulqa, arXiv:2109.07958Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Multiplechoice question answering and generation for assessing information consistency in summarization. P Manakul, A Liusie, M J Gales, Mqag, arXiv:2301.123072023aarXiv preprint</p>
<p>Zeroresource black-box hallucination detection for generative large language models. P Manakul, A Liusie, M J Gales, Selfcheckgpt, arXiv:2303.088962023barXiv preprint</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, arXiv:2005.006612020arXiv preprint</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. S Min, K Krishna, X Lyu, M Lewis, W.-T Yih, P W Koh, M Iyyer, L Zettlemoyer, H Hajishirzi, arXiv:2305.142512023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Exploring llm prompting strategies for joint essay scoring and feedback generation. M Stahl, L Biermann, A Nehring, H Wachsmuth, arXiv:2404.158452024arXiv preprint</p>
<p>S Tan, S Zhuang, K Montgomery, W Y Tang, A Cuadron, C Wang, R A Popa, I Stoica, Judgebench, arXiv:2410.12784A benchmark for evaluating llm-based judges. 2024arXiv preprint</p>
<p>A S Thakur, K Choudhary, V S Ramayapally, S Vaidyanathan, D Hupkes, arXiv:2406.12624Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges. 2024arXiv preprint</p>
<p>P Trivedi, A Gulati, O Molenschot, M A Rajeev, R Ramamurthy, K Stevens, T S Chaudhery, J Jambholkar, J Zou, N Rajani, arXiv:2410.05495Self-rationalization improves llm as a fine-grained judge. 2024arXiv preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. P Verga, S Hofstatter, S Althammer, Y Su, A Piktus, A Arkhangorodsky, M Xu, N White, P Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Aligning large language models with human: A survey. Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang, L Shang, X Jiang, Q Liu, arXiv:2307.129662023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, arXiv:2406.015742024arXiv preprint</p>
<p>C White, S Dooley, M Roberts, A Pal, B Feuer, S Jain, R Shwartz-Ziv, N Jain, K Saifullah, S Naidu, arXiv:2406.19314A challenging, contamination-free llm benchmark. 2024arXiv preprint</p>
<p>T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao, J Weston, S Sukhbaatar, arXiv:2407.19594Meta-rewarding language models: Self-improving alignment with llm-as-a-metajudge. 2024arXiv preprint</p>
<p>Z Yin, Q Sun, Q Guo, J Wu, X Qiu, X Huang, arXiv:2305.18153Do large language models know what they don't know?. 2023arXiv preprint</p>
<p>Mme-crs: multi-metric evaluation based on correlation re-scaling for evaluating open-domain dialogue. P Zhang, X Hu, K Yu, J Wang, S Han, C Liu, C Yuan, arXiv:2206.094032022arXiv preprint</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8206 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8206</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8206</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-272753096</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12294v1.pdf" target="_blank">RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties. Recent LLM-based decision-making methods (also referred to as LLM-based agents), when paired with appropriate critics, have demonstrated potential in solving complex, long-horizon tasks with relatively few interactions. However, most existing LLM-based agents lack the ability to retain and learn from past interactions - an essential trait of learning-based robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based agents with a memory of past interactions and incorporates critics to evaluate the agents' decisions. The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decision-making. Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld domains, we demonstrate significant improvements in task success rates and efficiency, showing that the proposed RAG-Modulo framework outperforms state-of-the-art baselines.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8206.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8206.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent framework that augments a language model with an interaction-level memory and a bank of critics; it retrieves past successful interactions as in‑context examples (RAG) to guide action selection and updates memory online to enable learning from experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-guided high-level planner augmented with (i) a bank of critics (syntax, semantics, low-level policy) that return feasibility feedback, and (ii) an interaction memory which stores tuples of (goal, previous action, feasibility feedback, observation) paired with the chosen action; retrieved in-context examples are concatenated to the LLM prompt to influence action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O (LLM) and TEXT-EMBEDDING-3-LARGE (embedder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4O used as the action-generating language model (greedy decoding, token limits specified); OpenAI TEXT-EMBEDDING-3-LARGE generates 3072-d embeddings for interactions used for semantic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI (Synth, BossLevel) and AlfWorld (Seen, Unseen)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Object-centric, goal-driven sequential embodied tasks under partial observability and stochasticity, requiring long-horizon planning and execution of high-level actions (options) to satisfy natural-language goals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon sequential decision-making / embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external episodic/interaction memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory stores interaction-level tuples in a database and encodes them into fixed-size vectors via TEXT-EMBEDDING-3-LARGE; at each decision step the current interaction is embedded and the top-K memory items by cosine similarity are retrieved and inserted as in-context examples into the LLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Interaction tuples I = (g, c_{t-1}, f, o) paired with chosen action c_t; stored only when the current action is feasible (SUCCESS), and includes rectifications after previous FAILURES.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search (cosine similarity) over fixed embeddings; top-K retrieved interactions concatenated into prompt as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BabyAI-Synth SR=0.48 ± 0.15; InExec=5.18 ± 1.18; Len=14.82 ± 2.14. BabyAI-BossLevel SR=0.57 ± 0.10; InExec=3.74 ± 0.78; Len=12.48 ± 1.49. AlfWorld-Seen SR=0.52 ± 0.08; InExec=5.36 ± 1.39; Len=20.54 ± 1.71. AlfWorld-Unseen SR=0.54 ± 0.09; InExec=7.17 ± 1.73; Len=19.64 ± 1.75.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations: varying K (number of retrieved examples) shows SR peaks at K=5 for BossLevel and K=10 for Synth, and declines for larger K due to distracting/irrelevant context; retrieval-level ablation (trajectory-level retrieval) yields slightly worse or similar SR and higher in-executability/episode length compared to interaction-level retrieval; initial memory seeding with expert demonstrations improves performance, but starting from empty memory still allows online learning; removing memory causes a significant drop in performance (see 'without memory' variant).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing and retrieving interaction-level experiences (including critics' feedback and rectifications) significantly improves success rates, reduces in-executability and episode length compared to baselines; best performance with modest K (≈5–10); interaction-level retrieval outperforms trajectory-level retrieval; prior expert seeding helps but is not strictly required for eventual improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance sensitive to number and quality of retrieved examples (too many examples or noisy examples degrade output); relies on quality of embedding/retrieval; memory stores only successful interactions by design (design choice may limit diversity); evaluated only in simulated benchmarks (BabyAI, AlfWorld); added token/compute cost due to retrieval and longer prompts; potential distraction effects of irrelevant context for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8206.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Modulo (trajectory-level retrieval ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG-Modulo variant: trajectory-level retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of RAG-Modulo that first finds the most similar task trajectory in memory (by comparing goals) and then extracts the top-K interactions from that single trajectory for in-context use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-Modulo (trajectory-level retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as RAG-Modulo except the retrieval step selects a most-similar task trajectory (goal-level similarity) and returns top-K interactions from that single trajectory rather than retrieving across the whole memory by interaction similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O and TEXT-EMBEDDING-3-LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM/embedding setup as RAG-Modulo experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI (Synth, BossLevel)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as main experiments (object-centric sequential tasks in gridworld).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (trajectory-constrained) episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Trajectory-level retrieval: compute cosine similarity between goals to pick most similar stored task, then select top-K interactions from that task's trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Same interaction tuples, but retrieval restricted to a single task trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search over goals to select trajectory, then within-trajectory top-K selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BabyAI-Synth SR=0.50 ± 0.10; InExec=5.30 ± 0.93; Len=15.42 ± 2.04. BabyAI-BossLevel SR=0.52 ± 0.10; InExec=4.22 ± 0.76; Len=13.24 ± 1.43.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to interaction-level retrieval, trajectory-level retrieval generally yields slightly higher in-executability and longer episode lengths while maintaining similar or slightly lower SR; suggests interaction-level retrieval gives richer diverse examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieving interactions from across memory (interaction-level) generally outperforms restricting retrieval to a single most-relevant trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less diverse context available; may miss useful corrections present across multiple tasks; slightly worse episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8206.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Modulo (no prior experience)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG-Modulo variant: empty initial memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAG-Modulo initialized without expert-provided prior interactions to study online accumulation of experience and its effect on performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-Modulo (no prior experience)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same as RAG-Modulo but memory starts empty and is filled online only from successful interactions during evaluation/training tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O and TEXT-EMBEDDING-3-LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI (Synth, BossLevel)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same BabyAI evaluation levels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic memory (empty initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory begins empty and is appended with successful interaction tuples during task execution; retrieval proceeds as in main method.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Interaction tuples accumulated online.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic top-K retrieval over accumulated interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BabyAI-Synth SR=0.44 ± 0.10; InExec=4.67 ± 0.88; Len=15.92 ± 2.05. BabyAI-BossLevel SR=0.54 ± 0.10; InExec=4.68 ± 0.88; Len=13.16 ± 1.48.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to RAG-Modulo seeded with expert demonstrations, performance is generally lower but still better than the variant without any memory component; demonstrates ability to bootstrap learning online.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior expert seeding helps, but an agent starting with empty memory can still improve by collecting experiences online and eventually outperform agents that have no memory mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Longer warm-up period; early performance is weaker without prior examples; depends on ability to generate some initial successful interactions to seed memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8206.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Modulo (without memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Modulo-like variant: critics but no interaction memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation representing the RAG-Modulo framework with the memory/storage/retrieval component removed (i.e., critics + LLM only) to test the utility of memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-Modulo (without memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM plus critics (syntax, semantics, low-level) but no mechanism to store or retrieve past interactions; essentially the LLM-Modulo baseline used for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM as used in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI (Synth, BossLevel)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks used for other variants.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>BabyAI-Synth SR=0.43 ± 0.10; InExec=6.72 ± 1.13; Len=16.23 ± 2.03. BabyAI-BossLevel SR=0.37 ± 0.09; InExec=6.07 ± 0.99; Len=14.48 ± 1.47.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Removing the memory component leads to a significant drop in performance: e.g., ~0.20 decrease in success rate on BabyAI-BossLevel (paper reports) and increases in average episode length by ~1.4–2.0 steps relative to RAG-Modulo with memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory is a major contributor to improved success rate and efficiency; critics alone (LLM-Modulo) are insufficient to realize the learning benefits shown by RAG-Modulo.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No capacity to learn from past successes/failures; worse sample efficiency and longer episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8206.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Modulo framework (critic-aided LLM planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior framework coupling language models with verifiers/critics to generate more executable plans; RAG-Modulo is explicitly an extension of this framework that adds memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms can't plan, but can help planning in llm-modulo frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework where an LLM proposes plans/actions and separate verifiers/critics check syntax, semantics, and executability to ground the LLM outputs; by design in prior work it does not incorporate an interaction memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning with verification</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>RAG-Modulo extends LLM-Modulo by adding interaction memory; the paper compares RAG-Modulo to a variant without memory (LLM-Modulo-like) and finds memory yields substantial improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-Modulo benefits from critics but lacks mechanisms for continual improvement (learning) from past interactions; adding memory produces measurable learning gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No built-in mechanism to remember and reuse past interaction-level experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8206.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Planner: Few-shot grounded planning for embodied agents with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline LLM-based online planner that uses grounded replanning and (in some forms) retrieval-augmented generation but does not store interaction-level successes/failures as RAG-Modulo does.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-Planner: Few-shot grounded planning for embodied agents with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based online grounded replanning method that updates plans dynamically in response to observation and action outcomes; uses retrieval-augmented generation in a different manner than RAG-Modulo and does not maintain an expanding database of interaction-level experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI and AlfWorld (as baseline comparisons in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as baseline for sequential embodied tasks in BabyAI and AlfWorld benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Uses retrieval in planning but not an interaction memory of past successes/failures (per paper's description).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported in Table I: BabyAI-Synth SR=0.48 ± 0.11; InExec=2.02 ± 2.17; Len=14.72 ± 2.13. BabyAI-BossLevel SR=0.24 ± 0.08; InExec=13.98 ± 1.48; Len=16.16 ± 1.35. AlfWorld-Seen SR=0.20 ± 0.07; InExec=21.24 ± 1.76; Len=25.65 ± 1.46. AlfWorld-Unseen SR=0.17 ± 0.06; InExec=21.73 ± 1.72; Len=26.36 ± 1.40.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>RAG-Modulo outperforms LLM-Planner on success rate, in-executability, and episode length in reported benchmarks, attributed to interaction memory and critics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamic replanning is beneficial, but explicit interaction-level memory with critic feedback (RAG-Modulo) yields better success rates and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not retain past interaction-level failures/successes for future tasks in the way RAG-Modulo does (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8206.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8206.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProgPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProgPrompt: Generating situated robot task plans using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static LLM-based planner that generates a complete plan offline at the start of a task and uses assertion checks to ground the plan; it does not use a memory of past interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progprompt: Generating situated robot task plans using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ProgPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An offline/static planner that prompts an LLM to produce a full plan before execution and uses assertions to verify grounding; representative of LLM agents that do not learn from experience.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI and AlfWorld (used as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark tasks for sequential decision-making; ProgPrompt generates full plans at start rather than online incremental planning with memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>offline planning / long-horizon planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported in Table I: BabyAI-Synth SR=0.24 ± 0.08; BabyAI-BossLevel SR=0.11 ± 0.06; AlfWorld results SR≈0.09 ± 0.05 (Seen) and SR≈0.08 ± 0.05 (Unseen) according to table entries (InExec/Len not applicable for offline ProgPrompt).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>RAG-Modulo achieves higher success rates than ProgPrompt across domains, attributed to RAG-Modulo's memory and critics and its interactive online planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Static planning without learning from prior task interactions performs worse in these partially observable, stochastic long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot adapt or improve from online experience; offline plans may fail when encountering unforeseen interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Bootstrap your own skills: Learning to solve new tasks with large language model guidance <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented web agent planning with llms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8206",
    "paper_id": "paper-272753096",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "RAG-Modulo",
            "name_full": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
            "brief_description": "An LLM-based agent framework that augments a language model with an interaction-level memory and a bank of critics; it retrieves past successful interactions as in‑context examples (RAG) to guide action selection and updates memory online to enable learning from experience.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG-Modulo",
            "agent_description": "LLM-guided high-level planner augmented with (i) a bank of critics (syntax, semantics, low-level policy) that return feasibility feedback, and (ii) an interaction memory which stores tuples of (goal, previous action, feasibility feedback, observation) paired with the chosen action; retrieved in-context examples are concatenated to the LLM prompt to influence action generation.",
            "model_name": "GPT-4O (LLM) and TEXT-EMBEDDING-3-LARGE (embedder)",
            "model_description": "GPT-4O used as the action-generating language model (greedy decoding, token limits specified); OpenAI TEXT-EMBEDDING-3-LARGE generates 3072-d embeddings for interactions used for semantic retrieval.",
            "task_name": "BabyAI (Synth, BossLevel) and AlfWorld (Seen, Unseen)",
            "task_description": "Object-centric, goal-driven sequential embodied tasks under partial observability and stochasticity, requiring long-horizon planning and execution of high-level actions (options) to satisfy natural-language goals.",
            "task_type": "long-horizon sequential decision-making / embodied planning",
            "memory_used": true,
            "memory_type": "retrieval-augmented external episodic/interaction memory",
            "memory_mechanism": "Memory stores interaction-level tuples in a database and encodes them into fixed-size vectors via TEXT-EMBEDDING-3-LARGE; at each decision step the current interaction is embedded and the top-K memory items by cosine similarity are retrieved and inserted as in-context examples into the LLM prompt.",
            "memory_representation": "Interaction tuples I = (g, c_{t-1}, f, o) paired with chosen action c_t; stored only when the current action is feasible (SUCCESS), and includes rectifications after previous FAILURES.",
            "memory_retrieval_method": "Semantic search (cosine similarity) over fixed embeddings; top-K retrieved interactions concatenated into prompt as in-context examples.",
            "performance_with_memory": "BabyAI-Synth SR=0.48 ± 0.15; InExec=5.18 ± 1.18; Len=14.82 ± 2.14. BabyAI-BossLevel SR=0.57 ± 0.10; InExec=3.74 ± 0.78; Len=12.48 ± 1.49. AlfWorld-Seen SR=0.52 ± 0.08; InExec=5.36 ± 1.39; Len=20.54 ± 1.71. AlfWorld-Unseen SR=0.54 ± 0.09; InExec=7.17 ± 1.73; Len=19.64 ± 1.75.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations: varying K (number of retrieved examples) shows SR peaks at K=5 for BossLevel and K=10 for Synth, and declines for larger K due to distracting/irrelevant context; retrieval-level ablation (trajectory-level retrieval) yields slightly worse or similar SR and higher in-executability/episode length compared to interaction-level retrieval; initial memory seeding with expert demonstrations improves performance, but starting from empty memory still allows online learning; removing memory causes a significant drop in performance (see 'without memory' variant).",
            "key_findings": "Storing and retrieving interaction-level experiences (including critics' feedback and rectifications) significantly improves success rates, reduces in-executability and episode length compared to baselines; best performance with modest K (≈5–10); interaction-level retrieval outperforms trajectory-level retrieval; prior expert seeding helps but is not strictly required for eventual improvement.",
            "limitations_or_challenges": "Performance sensitive to number and quality of retrieved examples (too many examples or noisy examples degrade output); relies on quality of embedding/retrieval; memory stores only successful interactions by design (design choice may limit diversity); evaluated only in simulated benchmarks (BabyAI, AlfWorld); added token/compute cost due to retrieval and longer prompts; potential distraction effects of irrelevant context for LLMs.",
            "uuid": "e8206.0",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG-Modulo (trajectory-level retrieval ablation)",
            "name_full": "RAG-Modulo variant: trajectory-level retrieval",
            "brief_description": "An ablation of RAG-Modulo that first finds the most similar task trajectory in memory (by comparing goals) and then extracts the top-K interactions from that single trajectory for in-context use.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG-Modulo (trajectory-level retrieval)",
            "agent_description": "Same architecture as RAG-Modulo except the retrieval step selects a most-similar task trajectory (goal-level similarity) and returns top-K interactions from that single trajectory rather than retrieving across the whole memory by interaction similarity.",
            "model_name": "GPT-4O and TEXT-EMBEDDING-3-LARGE",
            "model_description": "Same LLM/embedding setup as RAG-Modulo experiments.",
            "task_name": "BabyAI (Synth, BossLevel)",
            "task_description": "Same as main experiments (object-centric sequential tasks in gridworld).",
            "task_type": "long-horizon sequential decision-making",
            "memory_used": true,
            "memory_type": "retrieval-augmented (trajectory-constrained) episodic memory",
            "memory_mechanism": "Trajectory-level retrieval: compute cosine similarity between goals to pick most similar stored task, then select top-K interactions from that task's trajectory.",
            "memory_representation": "Same interaction tuples, but retrieval restricted to a single task trajectory.",
            "memory_retrieval_method": "Semantic search over goals to select trajectory, then within-trajectory top-K selection.",
            "performance_with_memory": "BabyAI-Synth SR=0.50 ± 0.10; InExec=5.30 ± 0.93; Len=15.42 ± 2.04. BabyAI-BossLevel SR=0.52 ± 0.10; InExec=4.22 ± 0.76; Len=13.24 ± 1.43.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to interaction-level retrieval, trajectory-level retrieval generally yields slightly higher in-executability and longer episode lengths while maintaining similar or slightly lower SR; suggests interaction-level retrieval gives richer diverse examples.",
            "key_findings": "Retrieving interactions from across memory (interaction-level) generally outperforms restricting retrieval to a single most-relevant trajectory.",
            "limitations_or_challenges": "Less diverse context available; may miss useful corrections present across multiple tasks; slightly worse episode length.",
            "uuid": "e8206.1",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG-Modulo (no prior experience)",
            "name_full": "RAG-Modulo variant: empty initial memory",
            "brief_description": "RAG-Modulo initialized without expert-provided prior interactions to study online accumulation of experience and its effect on performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG-Modulo (no prior experience)",
            "agent_description": "Same as RAG-Modulo but memory starts empty and is filled online only from successful interactions during evaluation/training tasks.",
            "model_name": "GPT-4O and TEXT-EMBEDDING-3-LARGE",
            "model_description": "Same as main experiments.",
            "task_name": "BabyAI (Synth, BossLevel)",
            "task_description": "Same BabyAI evaluation levels.",
            "task_type": "long-horizon sequential decision-making",
            "memory_used": true,
            "memory_type": "retrieval-augmented episodic memory (empty initialization)",
            "memory_mechanism": "Memory begins empty and is appended with successful interaction tuples during task execution; retrieval proceeds as in main method.",
            "memory_representation": "Interaction tuples accumulated online.",
            "memory_retrieval_method": "Semantic top-K retrieval over accumulated interactions.",
            "performance_with_memory": "BabyAI-Synth SR=0.44 ± 0.10; InExec=4.67 ± 0.88; Len=15.92 ± 2.05. BabyAI-BossLevel SR=0.54 ± 0.10; InExec=4.68 ± 0.88; Len=13.16 ± 1.48.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to RAG-Modulo seeded with expert demonstrations, performance is generally lower but still better than the variant without any memory component; demonstrates ability to bootstrap learning online.",
            "key_findings": "Prior expert seeding helps, but an agent starting with empty memory can still improve by collecting experiences online and eventually outperform agents that have no memory mechanism.",
            "limitations_or_challenges": "Longer warm-up period; early performance is weaker without prior examples; depends on ability to generate some initial successful interactions to seed memory.",
            "uuid": "e8206.2",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG-Modulo (without memory)",
            "name_full": "LLM-Modulo-like variant: critics but no interaction memory",
            "brief_description": "An ablation representing the RAG-Modulo framework with the memory/storage/retrieval component removed (i.e., critics + LLM only) to test the utility of memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG-Modulo (without memory)",
            "agent_description": "LLM plus critics (syntax, semantics, low-level) but no mechanism to store or retrieve past interactions; essentially the LLM-Modulo baseline used for ablation.",
            "model_name": "GPT-4O (LLM)",
            "model_description": "Same LLM as used in main experiments.",
            "task_name": "BabyAI (Synth, BossLevel)",
            "task_description": "Same evaluation tasks used for other variants.",
            "task_type": "long-horizon sequential decision-making",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "BabyAI-Synth SR=0.43 ± 0.10; InExec=6.72 ± 1.13; Len=16.23 ± 2.03. BabyAI-BossLevel SR=0.37 ± 0.09; InExec=6.07 ± 0.99; Len=14.48 ± 1.47.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Removing the memory component leads to a significant drop in performance: e.g., ~0.20 decrease in success rate on BabyAI-BossLevel (paper reports) and increases in average episode length by ~1.4–2.0 steps relative to RAG-Modulo with memory.",
            "key_findings": "Memory is a major contributor to improved success rate and efficiency; critics alone (LLM-Modulo) are insufficient to realize the learning benefits shown by RAG-Modulo.",
            "limitations_or_challenges": "No capacity to learn from past successes/failures; worse sample efficiency and longer episodes.",
            "uuid": "e8206.3",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-Modulo",
            "name_full": "LLM-Modulo framework (critic-aided LLM planning)",
            "brief_description": "A prior framework coupling language models with verifiers/critics to generate more executable plans; RAG-Modulo is explicitly an extension of this framework that adds memory.",
            "citation_title": "Llms can't plan, but can help planning in llm-modulo frameworks",
            "mention_or_use": "mention",
            "agent_name": "LLM-Modulo",
            "agent_description": "Framework where an LLM proposes plans/actions and separate verifiers/critics check syntax, semantics, and executability to ground the LLM outputs; by design in prior work it does not incorporate an interaction memory.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": "planning with verification",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "RAG-Modulo extends LLM-Modulo by adding interaction memory; the paper compares RAG-Modulo to a variant without memory (LLM-Modulo-like) and finds memory yields substantial improvements.",
            "key_findings": "LLM-Modulo benefits from critics but lacks mechanisms for continual improvement (learning) from past interactions; adding memory produces measurable learning gains.",
            "limitations_or_challenges": "No built-in mechanism to remember and reuse past interaction-level experiences.",
            "uuid": "e8206.4",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-Planner",
            "name_full": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "brief_description": "A baseline LLM-based online planner that uses grounded replanning and (in some forms) retrieval-augmented generation but does not store interaction-level successes/failures as RAG-Modulo does.",
            "citation_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "mention_or_use": "mention",
            "agent_name": "LLM-Planner",
            "agent_description": "An LLM-based online grounded replanning method that updates plans dynamically in response to observation and action outcomes; uses retrieval-augmented generation in a different manner than RAG-Modulo and does not maintain an expanding database of interaction-level experiences.",
            "model_name": null,
            "model_description": null,
            "task_name": "BabyAI and AlfWorld (as baseline comparisons in this paper)",
            "task_description": "Used as baseline for sequential embodied tasks in BabyAI and AlfWorld benchmarks.",
            "task_type": "long-horizon sequential decision-making",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": "Uses retrieval in planning but not an interaction memory of past successes/failures (per paper's description).",
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "Reported in Table I: BabyAI-Synth SR=0.48 ± 0.11; InExec=2.02 ± 2.17; Len=14.72 ± 2.13. BabyAI-BossLevel SR=0.24 ± 0.08; InExec=13.98 ± 1.48; Len=16.16 ± 1.35. AlfWorld-Seen SR=0.20 ± 0.07; InExec=21.24 ± 1.76; Len=25.65 ± 1.46. AlfWorld-Unseen SR=0.17 ± 0.06; InExec=21.73 ± 1.72; Len=26.36 ± 1.40.",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "RAG-Modulo outperforms LLM-Planner on success rate, in-executability, and episode length in reported benchmarks, attributed to interaction memory and critics.",
            "key_findings": "Dynamic replanning is beneficial, but explicit interaction-level memory with critic feedback (RAG-Modulo) yields better success rates and efficiency.",
            "limitations_or_challenges": "Does not retain past interaction-level failures/successes for future tasks in the way RAG-Modulo does (per paper).",
            "uuid": "e8206.5",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ProgPrompt",
            "name_full": "ProgPrompt: Generating situated robot task plans using large language models",
            "brief_description": "A static LLM-based planner that generates a complete plan offline at the start of a task and uses assertion checks to ground the plan; it does not use a memory of past interactions.",
            "citation_title": "Progprompt: Generating situated robot task plans using large language models",
            "mention_or_use": "mention",
            "agent_name": "ProgPrompt",
            "agent_description": "An offline/static planner that prompts an LLM to produce a full plan before execution and uses assertions to verify grounding; representative of LLM agents that do not learn from experience.",
            "model_name": null,
            "model_description": null,
            "task_name": "BabyAI and AlfWorld (used as baseline in this paper)",
            "task_description": "Benchmark tasks for sequential decision-making; ProgPrompt generates full plans at start rather than online incremental planning with memory.",
            "task_type": "offline planning / long-horizon planning",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "Reported in Table I: BabyAI-Synth SR=0.24 ± 0.08; BabyAI-BossLevel SR=0.11 ± 0.06; AlfWorld results SR≈0.09 ± 0.05 (Seen) and SR≈0.08 ± 0.05 (Unseen) according to table entries (InExec/Len not applicable for offline ProgPrompt).",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "RAG-Modulo achieves higher success rates than ProgPrompt across domains, attributed to RAG-Modulo's memory and critics and its interactive online planning.",
            "key_findings": "Static planning without learning from prior task interactions performs worse in these partially observable, stochastic long-horizon tasks.",
            "limitations_or_challenges": "Cannot adapt or improve from online experience; offline plans may fail when encountering unforeseen interactions.",
            "uuid": "e8206.6",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Bootstrap your own skills: Learning to solve new tasks with large language model guidance",
            "rating": 2,
            "sanitized_title": "bootstrap_your_own_skills_learning_to_solve_new_tasks_with_large_language_model_guidance"
        },
        {
            "paper_title": "Retrieval-augmented web agent planning with llms",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_web_agent_planning_with_llms"
        }
    ],
    "cost": 0.018047,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models
18 Sep 2024</p>
<p>Abhinav Jain abhinav.jain@rice.edu 
Department of Computer Science
Rice University
HoustonTX</p>
<p>Chris Jermaine 
Department of Computer Science
Rice University
HoustonTX</p>
<p>Vaibhav Unhelkar 
Department of Computer Science
Rice University
HoustonTX</p>
<p>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models
18 Sep 2024016A4CC63427B150D32B7460A1205524arXiv:2409.12294v1[cs.AI]This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties.Recent LLM-based decision-making methods (also referred to as LLM-based agents), when paired with appropriate critics, have demonstrated potential in solving complex, long-horizon tasks with relatively few interactions.However, most existing LLM-based agents lack the ability to retain and learn from past interactions-an essential trait of learning-based robotic systems.We propose RAG-Modulo, a framework that enhances LLM-based agents with a memory of past interactions and incorporates critics to evaluate the agents' decisions.The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decisionmaking.Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning.Through experiments in the challenging BabyAI and AlfWorld domains, we demonstrate significant improvements in task success rates and efficiency, showing that the proposed RAG-Modulo framework outperforms state-of-the-art baselines.</p>
<p>I. INTRODUCTION</p>
<p>Solving goal-driven sequential tasks is a core problem in robotics, with a wide array of challenges [1], [2], [3], [4], [5], [6].Due to imperfect actuation, real-world robots operate in stochastic environments.Their sensors often provide only a partial view of the surroundings, requiring decision-making under partial observability and limited knowledge of the world model.To reduce the programming burden for end-users, even complex, long-horizon tasks are frequently defined by sparse reward functions or natural language descriptions of the robot's goal.</p>
<p>Various paradigms and corresponding methods have been explored to address this fundamental challenge [7], [8], [9], [10], [11].The planning paradigm assumes access to a task model, which is often unavailable in real-world applications.While reinforcement learning can operate without a task model, it typically requires a prohibitively large number of exploratory interactions and significant manual effort for reward design.This challenge is further compounded in partially observable environments, where sparse rewards and safety concerns limit the feasibility of extensive exploration.</p>
<p>To complement these long-standing paradigms, language models have recently emerged as promising tools for solving Fig. 1.The RAG-Modulo framework incorporates a language model to generate candidate actions and a set of critics to evaluate them.Importantly, it features mechanisms for storing and retrieving past interactions, which enable learning from experience and improve decision-making over time.</p>
<p>long-horizon tasks in robotics [12], [13], [14], [15], [16], [17], [18], [19].They can approximate world knowledge [20], [21], [22] and use few-shot reasoning to decompose high-level tasks into mid-level plans [23], [24], [25].Additionally, they can function as dynamic planners, adjusting their strategies based on environmental feedback, which is especially useful in partially observable settings [17].Moreover, their performance is shown to improve when integrated with formal systems that evaluate decisions based on criteria such as correctness, executability, and user preferences [26].</p>
<p>Despite their promise, most existing LLM-based decisionmaking methods (also referred to as LLM-based agents) lack the ability to learn from experience.To effectively solve complex, long-horizon tasks, a robotic agent must demonstrate the ability to learn: meaning it should improve its performance over time as it gains more experience in its environment.A prevalent approach to realize such "learning" for LLM-based robotic agents is to tune prompts using in-context examples [27], but this method is constrained by the selection of examples, requires domain knowledge, and demands manual effort.Another option is to fine-tune language models based on past interactions [15], [28], but this approach can be computationally expensive and resource intensive.To address these gaps, we propose RAG-Modulo: a framework which augments a language model with a memory that stores past interactions, retrieving relevant experience at each step of the task to guide robot decision-making.</p>
<p>As shown in Figs. 1 and 2, RAG-Modulo extends the LLM-Modulo framework [26] with memory, where formal verifiers or critics evaluate the feasibility of actions at each step based on criteria like syntax, semantics, and executability.</p>
<p>The interactions, along with feasibility feedback, are stored in memory and retrieved as in-context examples, enabling automatic prompt tuning for future tasks.By leveraging these past interactions, the agent can generalize from its experiences, avoid repeated mistakes, and make more accurate decisions-much like how humans learn from their past errors.In summary, building on the insight of memoryaugmented behavior generation, this paper makes three key contributions:</p>
<p>• RAG-Modulo: A framework with LLM-based agents that learns not through back-propagation, but by building up a database of experiences (Interaction Memory) that it then accesses.</p>
<p>II. PROBLEM FORMULATION</p>
<p>In this section, we formally model the tasks of interest and define the problem, followed by an explanation of how language models can be prompted to function as agents.</p>
<p>A. Task Model</p>
<p>We focus on object-centric, goal-driven sequential robotic tasks that may involve uncertainties in both actions and observations [29].More specifically, we denote S o as the set of all possible objects in the robot's environment and S p as the set of object properties.We formally define the task model with the tuple (S, G, A, O, T, R g , h, γ).Given S o and S p , a state s ∈ S is defined as an assignment of object properties.A is the set of low-level physical actions and G is the set of all goals.A goal g ∈ G is the natural language description of the goal state.O is the set of observations retrieved from states via an observation function O : (S × A) → O, and T : (S × A) → S is the transition function.R g is the goal-conditioned reward function, which = 1 when goal is achieved, else 0. Finally, γ denotes the discount factor and h represents the task horizon.</p>
<p>Following prior work [2], [16], the agent is also equipped with a set of high-level text actions, denoted by C. In reinforcement learning (RL) literature, these can be interpreted as macro actions or options [30], [31].Each action c ∈ C is composed of a function and its corresponding set of arguments, i.e., c = FUNCTION(ARGUMENT), such as OPEN(TYPE.DOOR, COLOR.RED).We assume that the robot can execute this high-level action by breaking it down into a sequence of primitive actions (a 1 , a 2 , . ..), governed by its low-level policy π c , until a termination condition β c is met.For the remainder of the paper, we simply refer to high-level actions as actions.</p>
<p>B. Problem Statement</p>
<p>We can now formally define the problem statement.Given the initial state, s 0 , generate the shortest sequence of actions (c 1 , c 2 , . . ., c t ) to reach the goal state described as g.</p>
<p>C. Language Models as Agents</p>
<p>As shown in recent works [16], [17], large language models (LLM ) can be prompted at each time step to generate a sequence of actions using the following prompt:
PROMPT t = p env ; {g k , o k , c k } K k=1 ; g; {o 1:t−1 ,</p>
<p>III. RELATED WORK</p>
<p>In this section, we discuss related methods that utilize language models as agents, use memory components and incorporate retrieval-augmented generation (RAG).</p>
<p>Language Models as Agents.Recent works have explored using language models as agents for solving long-horizon tasks by generating plans [14], [16], [17], [19], [18].Approaches like ProgPrompt [16], [32] generate static plans offline, which may fail when encountering unforeseen object interactions in a partially observable environment.LLM-Planner-like approaches [17], [19], [18] offer a more online approach, allowing for plan updates if an action fails, but it does not store past successes and failures to guide future decisions.The method in [19] involves a human in the loop to prompt and verify.[18] generates feasible plans but relies on precise model dynamics estimation to assess plan feasibility.More recently, [26] have shown that language models should be coupled with verifiers or critics to generate sound plans.These recent methods have informed our work; however, in contrast to these works, RAG-Modulo stores and retrieves past interactions from memory to inform and improve decision-making.</p>
<p>Learning with Experience.Reinforcement learning agents typically use a replay buffer to store experiences for policy optimization.However, solving complex long-horizon tasks often demands millions of trajectories or environment interactions to learn effectively [1].In contrast, our approach requires only a few hundred experiences to enable meaningful learning.Very recently, some LLM-based approaches have introduced memory modules that store past experiences and expand as the agent interacts with the environment [33], [34], [35], [36], [37].These methods store experiences at the skill level, retrieving them when needed, but lack the ability to track past successes and failures at the interaction level.Moreover, they often require multiple LLMs to reason, relabel and abstract primitive skills into more complex composite ones.</p>
<p>In contrast, the proposed RAG-Modulo stores experiences at the interaction level, removing the need for LLM-guided relabeling, and retrieves these experiences at every decisionmaking step to offer more informed guidance to the language model.Importantly, our work is complementary to these methods, as they tackle different aspects of continual learning -one focuses on learning library of skills, while the other emphasizes learning from past mistakes and successes.</p>
<p>RAG systems for Robotics.Retrieval Augmented Generation (RAG) systems enhance language model predictions by retrieving relevant information from external databases [38], [39].For example, [40] employs RAG to collect exemplars for solving sub-tasks with web agents, while [41] retrieves driving experiences from a database for autonomous vehicle planning.In robotics, [42] explores retrieval for deep RL agents, but it does not use LLMs, limiting its adaptability and scalability.[43] employs a policy retriever to extract robotic policies from a large-scale policy memory.In contrast, our approach integrates a RAG system within an LLM-Modulo framework, where past interactions and feedback from critics is stored and continuously expanded.This enables the retrieval of interaction-level experiences, including mistakes and corrections, providing more detailed and contextaware guidance for sequential decision-making.</p>
<p>IV. PROPOSED APPROACH</p>
<p>We now describe RAG-Modulo, summarized in Alg. 1, which is composed of an LLM, a bank of critics, and an (I k , c k ) ← Retrieve interactions from memory (Eq.2) 7:</p>
<p>PROMPT t ← Construct the prompt (Eq. 1) 8:
c t ← LLM (PROMPT t )
▷ Predict action 9:
f t ← CHECKFEASIBILITY (o t , c t ) 10: if f t is SUCCESS then ▷ Keep track of interactions 11: M ← M ∪ {I t . = (g, c t−1 , f t−1 , o t ), c t } 12:
end if 13: end while 14: if g is satisfied then
15: M ← M ∪ M ▷ Update memory 16: end if 17: return (c 1:t , M)
interaction memory (M) coupled with mechanisms for storing and retrieving interaction experience.At each step t of a task specified by natural language goal g and horizon h, RAG-Modulo first retrieves interactions I from the memory that are relevant to the task and current observation o t , using them to guide the LLM's decision-making (line 6 in Alg. 1).The LLM selects action c t based on this context and receives feedback (lines 8 − 9) from a bank of critics (Alg.2).If feasible, the interaction is stored (lines 10 − 12).Once the
f t ← FAILURE(REASON) 11: return f t
goal is achieved, the interaction memory is updated for future retrieval (lines 14 − 16), enabling learning from experience.</p>
<p>A. Critics and Feedback</p>
<p>Informed by the [26], RAG-Modulo includes a bank of critics (φ syntax , φ semantics , φ low−level ) who provide feedback on actions selected by the LLM.F denotes the set of feedbacks described in natural language.The syntax parser φ syntax : C → F returns feedback based on syntactical correctness.It ensures that the LLM's response adheres to the grammar rules of the environment.The semantics parser φ semantics : (C × O) → F returns feedback based on semantic correctness.It verifies that the predicted action is meaningful and logically consistent with the current observation o, e.g., ensuring the agent has the correct key before opening a door.The low-level policy critic φ low−level :
(C × O) → F checks if c is executable from o.
It runs the execution using π c (o) until β c (o) is satisfied.For example, while traversing a path it can determine if an obstacle is encountered.As summarized in Alg. 2, each critic φ, either returns SUCCESS or FAILURE along with the corresponding REASON.This mimics how programmers receive feedback from compilers during debugging.We now formally define the overall feasibility feedback f ∈ F as a function of the feedback from all critics:
f = SUCCESS, if φ syntax ∧ φ semantics ∧ φ low−level</p>
<p>FAILURE(REASON), otherwise</p>
<p>Given the feedback, the prompt has the following structure:
PROMPT t =(1)p env ; {g k , c k −1 , f k , o k , c k } K k=1 ; g; (o 1:t−1 , c 1:t−1 , f 1:t−1 )
; o t where, in-context examples and history now include the previous action c −1 and its feasibility feedback.</p>
<p>B. Interaction Memory, Storage and Retrieval</p>
<p>RAG-Modulo considers a database of past interactions representing the agent's memory M of solving prior tasks and their outcomes.We represent such interaction with the tuple (I, c), where I = (g, c −1 , f, o).Formally, the memory includes a set of interactions M = {(I 1 , c 1 ), . . ., (I m , c m )}, where m represents the memory size.</p>
<p>Retrieval.At every decision-making step of a given task, RAG-Modulo retrieves from the memory the top-K most relevant interactions {I k , c k } K k=1 that resemble the current task and situation and uses them as in-context examples as shown in Fig. 2. Formally, this is represented as:
I 1:K = argmax K I∈M cos(e(I t ), e(I))(2)
argmax K returns the top-K samples from the memory that have the highest cosine similarity with I t .e(I) represents the fixed-size embedding of I generated by the encoder model e.As detailed in Sec.V, we use OpenAI's TEXT-EMBEDDING-3-LARGE [44] as the encoder model e for realizing RAG-Modulo in our experiments.</p>
<p>Storage.For every successfully completed task, {g, c 1:h , f 1:h , o 1:h , c 1:h )}, RAG-Modulo fills the memory with its interactions (I t , c t ) for which the current option c t is always feasible (i.e.f (c t ) = SUCCESS).Thus, every stored tuple is a successful interaction that includes rectifications when f t−1 = FAILURE, which can be used by the LLM when planning future actions.</p>
<p>V. EXPERIMENTAL SETUP</p>
<p>We evaluate the performance of RAG-Modulo in AlfWorld [2] and BabyAI [1], [15] benchmarks, depicted in Fig. 3.These benchmarks include several features representative of challenges in robot decision-making, thereby making them suitable benchmarks for evaluating RAG-Modulo.For instance, both benchmarks include a suite of sequential tasks that need to be performed by situated agents.The environments are partially observable to the agent, requiring the agent to explore, navigate and interact with objects to complete tasks described in natural language.Solving these tasks require reasoning over long-horizon in presence of a sparse reward signal, which is challenging for planning, RL and LLM-based decision-making algorithms [1], [2], [15].</p>
<p>Tasks.AlfWorld offers a diverse set of household tasks across various difficulty levels.We conduct experiments using the seen and UNSEEN validation sets, which include 140 and 132 task instances, respectively.The SEEN set is designed to measure in-distribution generalization, whereas the unseen set measures out-of-distribution generalization.BabyAI, a 2D grid world environment, features 40 levels of varying complexity.We focus on the Synth and BossLevel levels.The SYNTH level includes single-step instructions, such as "pick up a ball" or "go to the red key," while the BossLevel provides more complex, multi-step instructions, such as "put the yellow ball next to the purple ball, then open the purple door."Each level contains 100 evaluation task instances.</p>
<p>Prompt Design.We represent robot decisions as Python programs [16].Fig. 2 illustrates our prompt.The high-level actions are imported as Python functions.Each action is further defined with the types of arguments it requires.Finally, each argument type is defined as a class whose attributes represent the environment objects and their attributes.The interaction at each step is represented by key variables like feasibility feedback, visible objects and inventory.We task the LLM to predict the next action as the value of the variable action.</p>
<p>Language Model.We use GPT-4O [45] as the large language model LLM for generating actions and OpenAI's TEXT-EMBEDDING-3-LARGE [44] as the embedding model e for encoding instructions into 3072 dimensional vectors.Greedy decoding is applied with a maximum token limit of 200 for the LLM-Planner and 50 for the other approaches.The horizon (h) for high-level actions is set to 30 for AlfWorld, 20 for BabyAI-BossLevel, and 25 for BabyAI-Synth.</p>
<p>Baselines.We consider the following baselines for compar-ison, each using language models as high-level planners: (i) ProgPrompt [16] is a powerful static planner for robotic tasks that generates a complete plan at the start of a task and uses assertion checks to ground the plan to the current state.It is representative of LLM-based agents that do not involve memory or learning from experience.(ii) LLM-Planner [17] is a method that employs grounded replanning, dynamically updating the plan throughout the task.It is a representative approach of more recent LLMbased agents that also utilize retrieval-augmented generation; however, in a different manner than that of RAG-Modulo.For each environment, all baselines have access to 100 training tasks with expert-provided demonstrations.We initialize the memory in RAG-Modulo using these expert demonstrations.We refer to the initial memory as prior experience, which is updated online based on experience of solving new tasks.Metrics.To measure the decision-making performance, we consider three evaluation metrics.(i) Success Rate (SR) measures the fraction of tasks that the planner completed successfully.(ii) Average In-Executability (InExec) is the average number of selected actions that cannot be executed in the environment.(iii) Average Episode Length (Len) is the average number of planning actions that are required to complete a given task.As ProgPrompt is an offline approach, (InExec) and (Len) metrics are not applicable for it.</p>
<p>VI. RESULTS AND DISCUSSION</p>
<p>How does RAG-Modulo compare against other LLM as Agents baselines?In Table I, we report comparison with the baselines.RAG-Modulo demonstrates a higher success rate than ProgPrompt across both domains.This can be attributed to ProgPrompt's lack of memory and critics, which means it does not benefit from interactive learning.By interacting with the environment and retrieving relevant experience, RAG-Modulo enables more informed decision-making.</p>
<p>RAG-Modulo also outperforms LLM-Planner in terms of success rate, in-executability, and average episode length.Notably, the success rate improvements range from +0.33  What is the optimal number of interactions K to use as in-context examples?We ablate the number of interactions retrieved from memory and evaluate performance on BabyAI environments.The results reported in Fig. 4 show that the success rate improves as K increases, peaking at K = 5 for BossLevel and K = 10 for SynthLevel, before beginning to decline.Similarly, in Fig. 5 we observed that in-executability and average episode length decrease initially but start to rise as K continues to grow.The initial boost in performance can be attributed to the inclusion of more informative interactions, enhancing the LLM's decision-making capabilities.The subsequent decline likely stems from the LLM's sensitivity to irrelevant or noisy context [46], [47].As K increases, the chance of introducing less relevant or low-quality interactions also rises, which can distract the model and degrade its output quality [48].These trends suggest retrieving a modest number of interactions (between 5 and 10) while solving tasks using the RAG-Modulo framework.</p>
<p>How does the choice of retrieval function affect performance?We examine how retrieving interactions at different levels of granularity impacts performance.Specifically, we compare against an ablation of our approach that utilizes a trajectory-level retrieval function.This ablation first identifies the most relevant task in the memory by computing the cosine similarity between goals, and then extract top K interactions from that task's trajectory.We represent the performance of this variable in the second row of Table II.We observe that retrieving at the interaction level generally yields better results, with lower in-executability and shorter episode lengths, while maintaining similar or higher success rates across both BabyAI domains.This suggests that retrieving interactions from a diverse set of tasks provides the language model with richer information than simply retrieving interactions from the most relevant single task.</p>
<p>How does the presence of memory affect performance?To study the role of memory, we consider a variant of the proposed approach that does not include any interaction memory.This variant is representative of the LLM-Modulo framework [26], which includes interaction and critics but no mechanisms for storage or retrieval of experience.As reported in the last row of Table II, completely removing the memory component leads to a significant drop in performance, with a 0.20 decrease in success rate on BabyAI-BossLevel and an increase in average episode length by 1.4 to 2.0 steps.This demonstrates that storing and retrieving past interactions and feedback significantly improves the decision-making capabilities of the critic-aided language model.How does prior experience affect performance?Lastly, in the third row of Table II, we report results of RAG-Modulo when it is not seeded with any prior expert-generated experience.Unsurprisingly, we find that prior experience generally helps in sequential decision-making.Interestingly, even starting our approach with an empty memory (third row, Table II) still outperforms the variant that does not include a memory component (fourth row, Table II), as the agent can gradually collect experiences of successes and failures, allowing it to learn and improve its decision-making.</p>
<p>VII. CONCLUSION</p>
<p>This paper introduces RAG-Modulo, a framework for solving sequential decision-making tasks by providing LLM-based agents memory of past interactions.Extending the recent LLM-Modulo framework, RAG-Modulo not only incorporates critic feedback regarding the feasibility of generated actions but also enables agents to remember successes and mistakes and learn from them.RAG-Modulo demonstrates superior performance on the challenging BabyAI and AlfWorld benchmarks, achieving higher success rates while requiring fewer actions to complete sequential tasks.</p>
<p>In future work, we plan to utilize RAG-Modulo to solve tasks in other environments involving physical robots, such as FurnitureBench with the Panda robot [6].We also see potential in integrating RAG-Modulo with existing continual learning frameworks, such as BOSS and Voyager [33], [34], to enable learning from experience at multiple layers of abstractions: namely, skills and interactions.Another avenue is to explore tunable retrieval models that can anticipate future needs to further enhance the agent's performance [49], [50].Finally, we are interested in studying how RAG-Modulo can enhance end-user programming of complex robot behaviors by leveraging user commands, experience and critiques.</p>
<p>Fig. 2 .
2
Fig. 2. (Left) The prompt in RAG-Modulo consists of an environment descriptor, a history of past interactions, and in-context examples to guide the LLM in selecting a feasible action.Here, the agent can be carrying a blue key, which it needs to drop before picking up the green key.The retrieved in-context example shows a similar scenario where the agent is unable to drop an object in an occupied cell.Based on this, the agent generates an action to move to an empty cell before completing the task.(Right) Illustration of how each critic provides feedback for the infeasible action shown on top.</p>
<p>Algorithm 1
1
RAG-Modulo 1: INPUT: (g, h, LLM, M) 2: t ← 1 ▷ Initialize the time-step 3: M ← {} 4: while t ≤ h or (g is satisfied) do 5: o t ← Observe the environment 6:</p>
<p>Fig. 3 .
3
Fig. 3. (Left) AlfWorld Domain where the agent is shown in a household environment.(Right) Execution trace while solving a task from BabyAI.Ticks and Crosses show feasible and infeasible actions respectively.</p>
<p>Bold.Error bars are computed using bootstrapped sampling with 10k trials.The first row presents the results of the complete RAG-Modulo framework.The second row corresponds to a variant of RAG-Modulo with an alternate retrieval function, which retrieves the most similar task trajectory.The third row shows performance of RAG-Modulo when starting with no prior experience.The last row represents a variant that does not involve a memory component.hetuple(g k , c k −1 , f k , o k ).to +0.37 in more challenging environments like BabyAI-BossLevel and AlfWorld-Unseen.Additionally, RAG-Modulo has lower in-executability (approx.7 lower in Synth and 16 in Alfworld-Seen), and achieves shorter average episode lengths.While both systems are interactive and utilize retrievalaugmented generation, the key advantage of RAG-Modulo is the memory of past interactions that includes critics' feedback.By leveraging this memory, RAG-Modulo can avoid actions and accomplish tasks with fewer steps.Additionally, the lower episode length achieved by the RAG-Modulo facilitates a reduction in the overall cost of using LLMs, such as API expenses for closed-source models.</p>
<p>Fig. 4 .Fig. 5 .
45
Fig. 4. Success Rate as a function of K</p>
<p>Algorithm 2 CHECKFEASIBILITY 1: INPUT: o t , c t 2: f t ← SUCCESS, REASON ← NONE 3: try:
4:Parse c t using φ syntax▷ Syntax Critic5:Parse c t using φ semantics▷ Semantics Critic6:repeat7:Execute π ct▷ Low-level Policy Critic8:until β ct is True9: except Exception as REASON:10:</p>
<p>Table I .
I
Baseline comparison on BabyAI (top) and AlfWorld (bottom) environments.↑ denotes higher is better.↓ denotes lower is better.For a fair few-shot comparison, each approach uses 10 in-context examples in BabyAI-Synth and 5 in the remaining environments.The best performing approach is shown in Bold.Error bars are computed using bootstrapped sampling with 10k trials.(−) indicates the metric is not applicable for the approach.
ApproachBabyAI-SynthBabyAI-BossLevelSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓Expert0.960.798.340.980.438.41ProgPrompt0.24 ± 0.08−−0.11 ± 0.06−−LLM-Planner0.48 ± 0.112.02 ± 2.1714.72 ± 2.130.24 ± 0.08 13.98 ± 1.48 16.16 ± 1.35Ours0.48 ± 0.15.18± 1.1814.82 ± 2.140.57 ± 0.103.74 ± 0.7812.48 ± 1.49AlfWorld-SeenAlfWorld-UnseenSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓Expert0.820.1618.180.810.2818.47ProgPrompt0.09 ± 0.05−−0.08 ± 0.05−−LLM-Planner0.2 ± 0.0721.24 ± 1.76 25.65 ± 1.46 0.17 ± 0.0621.73 ± 1.726.36 ± 1.4Ours0.52 ± 0.085.36 ± 1.3920.54 ± 1.710.54 ± 0.097.17 ± 1.7319.64 ± 1.75</p>
<p>Table II .
II
RAG-Modulo 0.48 ± 0.1 5.18 ± 1.18 14.82 ± 2.14 0.57 ± 0.1 3.74 ± 0.78 12.48 ± 1.49 with trajectory-level retrieval 0.50 ±0.1 5.30 ± 0.93 15.42 ± 2.04 0.52 ± 0.1 4.22 ± 0.76 13.24 ± 1.43 without prior experience 0.44 ± 0.1 4.67 ± 0.88 15.92 ± 2.05 0.54 ± 0.1 4.68 ± 0.88 13.16 ± 1.48 without memory 0.43 ± 0.1 6.72 ± 1.13 16.23 ± 2.03 0.37 ± 0.09 6.07 ± 0.99 14.48 ± 1.47 Ablating different components of RAG-Modulo.↑ denotes higher is better.↓ denotes lower is better.The best performing approach is shown in
BabyAI-SynthBabyAI-BossLevelSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓
This work was supported in part by the NSF and Rice University funds.
Babyai: A platform to study the sample efficiency of grounded language learning. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, arXiv:1810.082722018arXiv preprint</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202010749</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, arXiv:2009.122932020arXiv preprint</p>
<p>Vima: Robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, 2023</p>
<p>Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. M Heo, Y Lee, D Lee, J J Lim, Robotics: Science and Systems. 2023</p>
<p>A survey of robot learning from demonstration. B D Argall, S Chernova, M Veloso, B Browning, Robotics and autonomous systems. 5752009</p>
<p>Reinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 32112013</p>
<p>Recent advances in robot learning from demonstration. H Ravichandar, A S Polydoros, S Chernova, A Billard, robotics, and autonomous systems. 312020Annual review of control</p>
<p>Integrated task and motion planning. C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, robotics, and autonomous systems. 412021Annual review of control</p>
<p>Reinforcement learning in robotic applications: a comprehensive survey. B Singh, R Kumar, V P Singh, Artificial Intelligence Review. 5522022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022arXiv preprint</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International conference on machine learning. PMLR2022</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE202311530</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Text2motion: From natural language instructions to feasible plans. K Lin, C Agia, T Migimatsu, M Pavone, J Bohg, Autonomous Robots. 4782023</p>
<p>Chatgpt for robotics: Design principles and model abilities. S H Vemprala, R Bonatti, A Bucker, A Kapoor, IEEE Access. 2024</p>
<p>Language models as knowledge bases. F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.010662019arXiv preprint</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, arXiv:2002.089102020arXiv preprint</p>
<p>How can we know what language models know?. Z Jiang, F F Xu, J Araki, G Neubig, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>How to prompt your robot: A promptbook for manipulation skills with code as policies. M G Arenas, T Xiao, S Singh, V Jain, A Ren, Q Vuong, J Varley, A Herzog, I Leal, S Kirmani, 2024 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Plansformer: Generating symbolic plans using transformers. V Pallagani, B Muppasani, K Murugesan, F Rossi, L Horesh, B Srivastava, F Fabiano, A Loreggia, arXiv:2212.086812022arXiv preprint</p>
<p>Mimicgen: A data generation system for scalable robot learning using human demonstrations. A Mandlekar, S Nasiriany, B Wen, I Akinola, Y Narang, L Fan, Y Zhu, D Fox, Conference on Robot Learning. PMLR2023</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-21999</p>
<p>Probabilistic inference for determining options in reinforcement learning. C Daniel, H Van Hoof, J Peters, G Neumann, Machine Learning. 2016104</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. R Hazra, P Z Martires, L De Raedt, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438133</p>
<p>J Zhang, J Zhang, K Pertsch, Z Liu, X Ren, M Chang, S.-H Sun, J J Lim, arXiv:2310.10021Bootstrap your own skills: Learning to solve new tasks with large language model guidance. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Lifelong robot library learning: Bootstrapping composable and generalizable skills for embodied control with language models. G Tziafas, H Kasaei, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438642</p>
<p>Atlas: Fewshot learning with retrieval augmented language models. G Izacard, P Lewis, M Lomeli, L Hosseini, F Petroni, T Schick, J Dwivedi-Yu, A Joulin, S Riedel, E Grave, Journal of Machine Learning Research. 242512023</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International conference on machine learning. PMLR2022</p>
<p>Rada: Retrieval-augmented web agent planning with llms. M Kim, V Bursztyn, E Koh, S Guo, S.-W Hwang, Findings of the Association for Computational Linguistics ACL 2024. 202413525</p>
<p>Vistarag: Toward safe and trustworthy autonomous driving through retrieval-augmented generation. X Dai, C Guo, Y Tang, H Li, Y Wang, J Huang, Y Tian, X Xia, Y Lv, F.-Y Wang, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Retrievalaugmented reinforcement learning. A Goyal, A Friesen, A Banino, T Weber, N R Ke, A P Badia, A Guez, M Mirza, P C Humphreys, K Konyushova, International Conference on Machine Learning. PMLR2022</p>
<p>Retrieval-augmented embodied agents. Y Zhu, Z Ou, X Mou, J Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202417995</p>
<p>. Openai, 2024</p>
<p>Large language models can be easily distracted by irrelevant context. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, N Schärli, D Zhou, International Conference on Machine Learning. PMLR202331227</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Superposition prompting: Improving and accelerating retrieval-augmented generation. T Merth, Q Fu, M Rastegari, M Najibi, arXiv:2404.069102024arXiv preprint</p>
<p>Replug: Retrieval-augmented black-box language models. S Weijia, M Sewon, Y Michihiro, S Minjoon, J Rich, L Mike, Y Wen-Tau, ArXiv: 2301.126522023</p>
<p>Active retrieval augmented generation. Z Jiang, F F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, arXiv:2305.069832023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7452 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7452</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7452</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-268379355</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.08035v1.pdf" target="_blank">Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7452.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7452.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct concise prompt (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct concise yes/no classification prompt (used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, single-question natural-language prompt that asks the model to answer 'Is the following text hateful or not? Just answer in Yes or No.' with no examples or explanations; used as the primary prompt in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLM (GPT-3.5 series) accessed via API; instruction-following RLHF-tuned variant noted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HateCheck / binary hate speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of short text as hateful or non-hateful using the HateCheck diagnostic dataset (directed vs general hate; includes non-hate examples designed to probe spurious correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language single-question classification prompt (binary yes/no answer)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; single explicit instruction: 'Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations.' No in‑prompt examples; short delimiter text: 'text: {hate_speech}'; outputs mapped to binary labels; manual handling of explanations or guardrail messages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Precision/Recall/F1 for each class; AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy 89% (0.89); Hate F1 0.93; Non-hate F1 0.82; AUROC 0.85</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Context prompt (GPT-3.5): Accuracy 84% (0.84); Chain-of-thought prompt (GPT-3.5): Accuracy 80% (0.80)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+5 percentage points absolute vs Context prompt; +9 percentage points absolute vs COT prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot classification on HateCheck; model version gpt-3.5-turbo-0613; prompt exactly as above; date range of experiments reported: Sept 25–Oct 8, 2023; no temperature/max-tokens reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7452.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context prompt (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual prompt with additional context (used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A longer natural-language prompt that supplies additional context about the classification task before asking for a yes/no label; tested as an alternative to the direct concise prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLM (GPT-3.5 series) accessed via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HateCheck / binary hate speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of text as hateful or non-hateful on HateCheck.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language contextual prompt (longer prompt providing additional task/context information, then asking for yes/no)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; includes extra contextual description preceding the binary question; no in-prompt labeled examples; longer input which requires the model to retrieve relevant instructions from middle sections.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Precision/Recall/F1 for each class; AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy 84% (0.84); Hate F1 0.88; Non-hate F1 0.76; AUROC 0.83</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct concise prompt (GPT-3.5): Accuracy 89% (0.89)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-5 percentage points absolute vs Direct concise prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot on HateCheck; gpt-3.5-turbo-0613; additional context text inserted before the classification instruction; no temperature reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7452.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompt (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought (COT) style prompt requesting step-by-step reasoning before yes/no label</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt variant that asks the model to first describe step-by-step how it evaluates the text, then provide a Yes/No label; tested to evaluate whether eliciting reasoning helps classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLM (GPT-3.5 series).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HateCheck / binary hate speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of text as hateful or non-hateful on HateCheck.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language chain-of-thought prompt (requesting step-by-step process then final Yes/No)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; explicit request for stepwise reasoning then final label; longer prompt with 'Think step-by-step about the task before classifying'; no few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Precision/Recall/F1 for each class; AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy 80% (0.80); Hate F1 0.84; Non-hate F1 0.74; AUROC 0.79</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct concise prompt (GPT-3.5): Accuracy 89% (0.89)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-9 percentage points absolute vs Direct concise prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot on HateCheck; gpt-3.5-turbo-0613; explicit COT instruction; no temperature reported. Paper hypothesizes long/middle-context retrieval issues (cites 'Lost in the middle').</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7452.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 2 (7B) direct prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 7B chat variant used with direct concise prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source instruction/chat-tuned Llama 2 (7B chat variant) evaluated zero-shot on HateCheck using the same direct concise yes/no prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (7B chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction/chat-tuned LLM (Llama 2 family) used in its 7B chat configuration as provided on Hugging Face.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HateCheck / binary hate speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of text as hateful or non-hateful on HateCheck.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language single-question classification prompt (binary yes/no)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; identical direct concise prompt as used with GPT-3.5; no in-prompt examples; outputs mapped to binary labels; manual handling of explanations and guardrail messages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Precision/Recall/F1 for each class; AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy 83% (0.83); Hate F1 0.89; Non-hate F1 0.63; AUROC 0.73</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GPT-3.5 direct prompt: Accuracy 89% (0.89)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-6 percentage points absolute vs GPT-3.5 direct prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot on HateCheck; Llama-2-7b-chat model from Hugging Face; same labeling and caveat-handling as other models; no temperature reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7452.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon (7B) direct prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon 7B chat variant used with direct concise prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Falcon 7B chat variant evaluated zero-shot on HateCheck using the same direct concise yes/no prompt; performed poorly compared to other tested LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon (7B chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction/chat-tuned Falcon 7B model (chat variant) obtained from Hugging Face.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HateCheck / binary hate speech detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of text as hateful or non-hateful on HateCheck.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language single-question classification prompt (binary yes/no)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; direct concise prompt identical to other models; no in-prompt examples; manual label mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Precision/Recall/F1 for each class; AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy 47% (0.47); Hate F1 0.53; Non-hate F1 0.40; AUROC 0.49</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GPT-3.5 direct prompt: Accuracy 89% (0.89)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-42 percentage points absolute vs GPT-3.5 direct prompt</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot on HateCheck; Falcon-7b-instruct chat model from Hugging Face; same prompt and labeling conventions as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7452.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot w/o instructions (Chiu et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting without explicit label instructions (reported by Chiu et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior result reporting that GPT-3's few-shot performance on hate speech detection improved when prompts included few-shot exemplars but omitted explicit task instructions (i.e., examples only), indicating format influences performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting hate speech with gpt-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base GPT-3 next-token-prediction LLM (175B for GPT-3 base as reported elsewhere; Chiu et al. used GPT-3 in few-shot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hate speech detection (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify text as hate/not-hate using few-shot in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context learning (examples provided in prompt) without additional instructional text</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot setting (1-shot, few-shot); study reports better performance when prompts omitted explicit instructions and relied only on exemplars; exact number of shots varied in experiments reported by Chiu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Few-shot with explicit instructions (worse performance reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot in-context learning with GPT-3; comparison between prompt variants with/without instruction text; numerical details not provided in the current paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7452.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot sample count effect (Han & Tang 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing number of labeled in-prompt examples (Han & Tang, 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported observation that increasing the number of labeled examples included in the prompt (few-shot) improves GPT-3 performance on hate speech recognition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Designing of prompts for hate speech recognition with incontext learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Next-token-prediction LLM used in few-shot in-context learning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hate speech detection (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification with varying numbers of in-prompt labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context learning (varying shot count)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Varying few-shot counts; finding: performance increases as more labeled examples are provided in the prompt; details of exact shot counts and numerical improvements are reported in Han & Tang (2022) but not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Lower-shot prompts (worse performance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot in-context evaluation with GPT-3; prompt design experiments; no numerical values reproduced in current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7452.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-tuned models advantage (Del Arco et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Superiority of instruction-tuned LLMs (e.g., FLAN-T5, mT0) in zero-shot hate detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported finding that instruction-tuned models evaluated zero-shot on multiple hate-speech benchmarks outperformed encoder-based models like BERT family, suggesting instruction-tuning and prompt framing influence zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Respectful or toxic? using zero-shot learning with language models to detect hate speech.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5, mT0 (instruction-tuned models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned seq2seq and multitask-finetuned models explicitly trained to follow prompts/instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot hate speech detection (multiple benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot classification of hateful content across 8 benchmark datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction-style prompts</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / model training regime</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Instruction-tuned prompts supplied zero-shot; models trained to follow instructions (instruction-finetuning); reported superior performance vs encoder-only classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Encoder-based LM classifiers (e.g., BERT family) reported worse zero-shot performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot evaluation across eight benchmark datasets; instruction-tuned models (FLAN-T5, mT0) compared to BERT-family encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7452.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-tuning (Luo et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-tuning for enforceable hate speech detection (Luo et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of prompt-tuning (a parameter-efficient approach that freezes model weights and optimizes soft prompt parameters) on RoBERTa-large to improve performance for a newly proposed 'enforceable hate speech' detection task, illustrating that format (soft prompts/prompt-tuning) affects classifier behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards legally enforceable hate speech detection for public forums.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (with prompt-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large encoder-based LM (RoBERTa-large) adapted via prompt-tuning (soft-prompt parameter updates while freezing base weights).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Enforceable hate speech detection (new task variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification of content according to legally enforceable definitions of hate speech.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt-tuning (soft prompts) applied to encoder-based model</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / fine-tuning method</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompt-tuning: model weights frozen, a small set of prompt parameters optimized; used for low-resource adaptation and to alter model behavior for the specific detection definition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard fine-tuning or zero-shot prompting (implied comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt-tuning on RoBERTa-large for a legally-enforceable detection formulation; details in Luo et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7452.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt length/temperature sensitivity (Reiss 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependence of LLM-derived annotations on temperature and prompt complexity (Reiss, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported observation that ChatGPT annotations' reliability depends strongly on generation temperature and possibly other prompt factors such as prompt length and instruction complexity, indicating format and decoding hyperparameters influence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing the reliability of chatgpt for text annotation and classification: A cautionary remark.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-family models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLM(s); study examined sensitivity of annotation outputs to sampling temperature and prompt characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text annotation and classification (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reproducing human annotations for diverse annotation tasks and assessing reliability/sensitivity to prompt hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; variation in prompt length, complexity; generation temperature varied</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / decoding settings</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Findings highlight that annotation outcomes change with temperature and prompt length/complexity; suggests careful control of generation hyperparameters when using LLMs as annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Exploratory analysis of ChatGPT annotations; temperature and prompt complexity varied; details in Reiss (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7452.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7452.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context placement / long-context effect (Liu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lost-in-the-middle long-context retrieval effect (N. F. Liu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior finding that LLMs use information at the beginning or end of long contexts more effectively than information placed in the middle, which the paper cites to explain why complex/long prompts (with task-critical info in the middle) can reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lost in the middle: How language models use long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Study-level observation about transformer-based LLMs' context utilization patterns rather than a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General long-context utilization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis of how positional placement of important information in long input contexts affects retrieval/use by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long-context prompts with task-relevant information at different positions (begin/middle/end)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt structure</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Finding: performance peaks when vital information is at beginning or end, degrades when info is located in the middle of long contexts; used by the paper to interpret why longer/contextual or COT prompts underperformed versus concise prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Analytical/empirical study across models and context lengths; cited for interpretation in present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Detecting hate speech with gpt-3. <em>(Rating: 2)</em></li>
                <li>Designing of prompts for hate speech recognition with incontext learning. <em>(Rating: 2)</em></li>
                <li>Respectful or toxic? using zero-shot learning with language models to detect hate speech. <em>(Rating: 2)</em></li>
                <li>Towards legally enforceable hate speech detection for public forums. <em>(Rating: 2)</em></li>
                <li>Testing the reliability of chatgpt for text annotation and classification: A cautionary remark. <em>(Rating: 2)</em></li>
                <li>Lost in the middle: How language models use long contexts. <em>(Rating: 2)</em></li>
                <li>Chatgpt outperforms crowd-workers for text-annotation tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7452",
    "paper_id": "paper-268379355",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Direct concise prompt (paper)",
            "name_full": "Direct concise yes/no classification prompt (used in this paper)",
            "brief_description": "A zero-shot, single-question natural-language prompt that asks the model to answer 'Is the following text hateful or not? Just answer in Yes or No.' with no examples or explanations; used as the primary prompt in the paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "Instruction-tuned conversational LLM (GPT-3.5 series) accessed via API; instruction-following RLHF-tuned variant noted in paper.",
            "model_size": null,
            "task_name": "HateCheck / binary hate speech detection",
            "task_description": "Binary classification of short text as hateful or non-hateful using the HateCheck diagnostic dataset (directed vs general hate; includes non-hate examples designed to probe spurious correlations).",
            "problem_format": "Natural-language single-question classification prompt (binary yes/no answer)",
            "format_category": "prompt style",
            "format_details": "Zero-shot; single explicit instruction: 'Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations.' No in‑prompt examples; short delimiter text: 'text: {hate_speech}'; outputs mapped to binary labels; manual handling of explanations or guardrail messages.",
            "performance_metric": "Accuracy; Precision/Recall/F1 for each class; AUROC",
            "performance_value": "Accuracy 89% (0.89); Hate F1 0.93; Non-hate F1 0.82; AUROC 0.85",
            "baseline_performance": "Context prompt (GPT-3.5): Accuracy 84% (0.84); Chain-of-thought prompt (GPT-3.5): Accuracy 80% (0.80)",
            "performance_change": "+5 percentage points absolute vs Context prompt; +9 percentage points absolute vs COT prompt",
            "experimental_setting": "Zero-shot classification on HateCheck; model version gpt-3.5-turbo-0613; prompt exactly as above; date range of experiments reported: Sept 25–Oct 8, 2023; no temperature/max-tokens reported in paper.",
            "statistical_significance": null,
            "uuid": "e7452.0",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Context prompt (paper)",
            "name_full": "Contextual prompt with additional context (used in this paper)",
            "brief_description": "A longer natural-language prompt that supplies additional context about the classification task before asking for a yes/no label; tested as an alternative to the direct concise prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "Instruction-tuned conversational LLM (GPT-3.5 series) accessed via API.",
            "model_size": null,
            "task_name": "HateCheck / binary hate speech detection",
            "task_description": "Binary classification of text as hateful or non-hateful on HateCheck.",
            "problem_format": "Natural-language contextual prompt (longer prompt providing additional task/context information, then asking for yes/no)",
            "format_category": "prompt style",
            "format_details": "Zero-shot; includes extra contextual description preceding the binary question; no in-prompt labeled examples; longer input which requires the model to retrieve relevant instructions from middle sections.",
            "performance_metric": "Accuracy; Precision/Recall/F1 for each class; AUROC",
            "performance_value": "Accuracy 84% (0.84); Hate F1 0.88; Non-hate F1 0.76; AUROC 0.83",
            "baseline_performance": "Direct concise prompt (GPT-3.5): Accuracy 89% (0.89)",
            "performance_change": "-5 percentage points absolute vs Direct concise prompt",
            "experimental_setting": "Zero-shot on HateCheck; gpt-3.5-turbo-0613; additional context text inserted before the classification instruction; no temperature reported.",
            "statistical_significance": null,
            "uuid": "e7452.1",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-thought prompt (paper)",
            "name_full": "Chain-of-thought (COT) style prompt requesting step-by-step reasoning before yes/no label",
            "brief_description": "A prompt variant that asks the model to first describe step-by-step how it evaluates the text, then provide a Yes/No label; tested to evaluate whether eliciting reasoning helps classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "Instruction-tuned conversational LLM (GPT-3.5 series).",
            "model_size": null,
            "task_name": "HateCheck / binary hate speech detection",
            "task_description": "Binary classification of text as hateful or non-hateful on HateCheck.",
            "problem_format": "Natural-language chain-of-thought prompt (requesting step-by-step process then final Yes/No)",
            "format_category": "prompt style",
            "format_details": "Zero-shot; explicit request for stepwise reasoning then final label; longer prompt with 'Think step-by-step about the task before classifying'; no few-shot examples.",
            "performance_metric": "Accuracy; Precision/Recall/F1 for each class; AUROC",
            "performance_value": "Accuracy 80% (0.80); Hate F1 0.84; Non-hate F1 0.74; AUROC 0.79",
            "baseline_performance": "Direct concise prompt (GPT-3.5): Accuracy 89% (0.89)",
            "performance_change": "-9 percentage points absolute vs Direct concise prompt",
            "experimental_setting": "Zero-shot on HateCheck; gpt-3.5-turbo-0613; explicit COT instruction; no temperature reported. Paper hypothesizes long/middle-context retrieval issues (cites 'Lost in the middle').",
            "statistical_significance": null,
            "uuid": "e7452.2",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Llama 2 (7B) direct prompt",
            "name_full": "Llama 2 7B chat variant used with direct concise prompt",
            "brief_description": "Open-source instruction/chat-tuned Llama 2 (7B chat variant) evaluated zero-shot on HateCheck using the same direct concise yes/no prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 (7B chat variant)",
            "model_description": "Open-source instruction/chat-tuned LLM (Llama 2 family) used in its 7B chat configuration as provided on Hugging Face.",
            "model_size": "7B",
            "task_name": "HateCheck / binary hate speech detection",
            "task_description": "Binary classification of text as hateful or non-hateful on HateCheck.",
            "problem_format": "Natural-language single-question classification prompt (binary yes/no)",
            "format_category": "prompt style",
            "format_details": "Zero-shot; identical direct concise prompt as used with GPT-3.5; no in-prompt examples; outputs mapped to binary labels; manual handling of explanations and guardrail messages.",
            "performance_metric": "Accuracy; Precision/Recall/F1 for each class; AUROC",
            "performance_value": "Accuracy 83% (0.83); Hate F1 0.89; Non-hate F1 0.63; AUROC 0.73",
            "baseline_performance": "GPT-3.5 direct prompt: Accuracy 89% (0.89)",
            "performance_change": "-6 percentage points absolute vs GPT-3.5 direct prompt",
            "experimental_setting": "Zero-shot on HateCheck; Llama-2-7b-chat model from Hugging Face; same labeling and caveat-handling as other models; no temperature reported.",
            "statistical_significance": null,
            "uuid": "e7452.3",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Falcon (7B) direct prompt",
            "name_full": "Falcon 7B chat variant used with direct concise prompt",
            "brief_description": "Open-source Falcon 7B chat variant evaluated zero-shot on HateCheck using the same direct concise yes/no prompt; performed poorly compared to other tested LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon (7B chat variant)",
            "model_description": "Open-source instruction/chat-tuned Falcon 7B model (chat variant) obtained from Hugging Face.",
            "model_size": "7B",
            "task_name": "HateCheck / binary hate speech detection",
            "task_description": "Binary classification of text as hateful or non-hateful on HateCheck.",
            "problem_format": "Natural-language single-question classification prompt (binary yes/no)",
            "format_category": "prompt style",
            "format_details": "Zero-shot; direct concise prompt identical to other models; no in-prompt examples; manual label mapping.",
            "performance_metric": "Accuracy; Precision/Recall/F1 for each class; AUROC",
            "performance_value": "Accuracy 47% (0.47); Hate F1 0.53; Non-hate F1 0.40; AUROC 0.49",
            "baseline_performance": "GPT-3.5 direct prompt: Accuracy 89% (0.89)",
            "performance_change": "-42 percentage points absolute vs GPT-3.5 direct prompt",
            "experimental_setting": "Zero-shot on HateCheck; Falcon-7b-instruct chat model from Hugging Face; same prompt and labeling conventions as other models.",
            "statistical_significance": null,
            "uuid": "e7452.4",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Few-shot w/o instructions (Chiu et al. 2021)",
            "name_full": "Few-shot prompting without explicit label instructions (reported by Chiu et al., 2021)",
            "brief_description": "Prior result reporting that GPT-3's few-shot performance on hate speech detection improved when prompts included few-shot exemplars but omitted explicit task instructions (i.e., examples only), indicating format influences performance.",
            "citation_title": "Detecting hate speech with gpt-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Base GPT-3 next-token-prediction LLM (175B for GPT-3 base as reported elsewhere; Chiu et al. used GPT-3 in few-shot experiments).",
            "model_size": null,
            "task_name": "Hate speech detection (few-shot)",
            "task_description": "Classify text as hate/not-hate using few-shot in-context examples.",
            "problem_format": "Few-shot in-context learning (examples provided in prompt) without additional instructional text",
            "format_category": "question type / prompt style",
            "format_details": "Few-shot setting (1-shot, few-shot); study reports better performance when prompts omitted explicit instructions and relied only on exemplars; exact number of shots varied in experiments reported by Chiu et al.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": "Few-shot with explicit instructions (worse performance reported)",
            "performance_change": null,
            "experimental_setting": "Few-shot in-context learning with GPT-3; comparison between prompt variants with/without instruction text; numerical details not provided in the current paper's summary.",
            "statistical_significance": null,
            "uuid": "e7452.5",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Few-shot sample count effect (Han & Tang 2022)",
            "name_full": "Effect of increasing number of labeled in-prompt examples (Han & Tang, 2022)",
            "brief_description": "Reported observation that increasing the number of labeled examples included in the prompt (few-shot) improves GPT-3 performance on hate speech recognition tasks.",
            "citation_title": "Designing of prompts for hate speech recognition with incontext learning.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Next-token-prediction LLM used in few-shot in-context learning experiments.",
            "model_size": null,
            "task_name": "Hate speech detection (few-shot)",
            "task_description": "Binary classification with varying numbers of in-prompt labeled examples.",
            "problem_format": "Few-shot in-context learning (varying shot count)",
            "format_category": "prompt style",
            "format_details": "Varying few-shot counts; finding: performance increases as more labeled examples are provided in the prompt; details of exact shot counts and numerical improvements are reported in Han & Tang (2022) but not enumerated in this paper.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": "Lower-shot prompts (worse performance)",
            "performance_change": null,
            "experimental_setting": "Few-shot in-context evaluation with GPT-3; prompt design experiments; no numerical values reproduced in current paper.",
            "statistical_significance": null,
            "uuid": "e7452.6",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Instruction-tuned models advantage (Del Arco et al. 2023)",
            "name_full": "Superiority of instruction-tuned LLMs (e.g., FLAN-T5, mT0) in zero-shot hate detection",
            "brief_description": "Reported finding that instruction-tuned models evaluated zero-shot on multiple hate-speech benchmarks outperformed encoder-based models like BERT family, suggesting instruction-tuning and prompt framing influence zero-shot performance.",
            "citation_title": "Respectful or toxic? using zero-shot learning with language models to detect hate speech.",
            "mention_or_use": "mention",
            "model_name": "FLAN-T5, mT0 (instruction-tuned models)",
            "model_description": "Instruction-tuned seq2seq and multitask-finetuned models explicitly trained to follow prompts/instructions.",
            "model_size": null,
            "task_name": "Zero-shot hate speech detection (multiple benchmarks)",
            "task_description": "Zero-shot classification of hateful content across 8 benchmark datasets.",
            "problem_format": "Zero-shot instruction-style prompts",
            "format_category": "prompt style / model training regime",
            "format_details": "Instruction-tuned prompts supplied zero-shot; models trained to follow instructions (instruction-finetuning); reported superior performance vs encoder-only classifiers.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": "Encoder-based LM classifiers (e.g., BERT family) reported worse zero-shot performance",
            "performance_change": null,
            "experimental_setting": "Zero-shot evaluation across eight benchmark datasets; instruction-tuned models (FLAN-T5, mT0) compared to BERT-family encoders.",
            "statistical_significance": null,
            "uuid": "e7452.7",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Prompt-tuning (Luo et al. 2023)",
            "name_full": "Prompt-tuning for enforceable hate speech detection (Luo et al., 2023)",
            "brief_description": "Use of prompt-tuning (a parameter-efficient approach that freezes model weights and optimizes soft prompt parameters) on RoBERTa-large to improve performance for a newly proposed 'enforceable hate speech' detection task, illustrating that format (soft prompts/prompt-tuning) affects classifier behaviour.",
            "citation_title": "Towards legally enforceable hate speech detection for public forums.",
            "mention_or_use": "mention",
            "model_name": "RoBERTa-large (with prompt-tuning)",
            "model_description": "Large encoder-based LM (RoBERTa-large) adapted via prompt-tuning (soft-prompt parameter updates while freezing base weights).",
            "model_size": null,
            "task_name": "Enforceable hate speech detection (new task variant)",
            "task_description": "Classification of content according to legally enforceable definitions of hate speech.",
            "problem_format": "Prompt-tuning (soft prompts) applied to encoder-based model",
            "format_category": "prompt style / fine-tuning method",
            "format_details": "Prompt-tuning: model weights frozen, a small set of prompt parameters optimized; used for low-resource adaptation and to alter model behavior for the specific detection definition.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": "Standard fine-tuning or zero-shot prompting (implied comparison)",
            "performance_change": null,
            "experimental_setting": "Prompt-tuning on RoBERTa-large for a legally-enforceable detection formulation; details in Luo et al. (2023).",
            "statistical_significance": null,
            "uuid": "e7452.8",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Prompt length/temperature sensitivity (Reiss 2023)",
            "name_full": "Dependence of LLM-derived annotations on temperature and prompt complexity (Reiss, 2023)",
            "brief_description": "Reported observation that ChatGPT annotations' reliability depends strongly on generation temperature and possibly other prompt factors such as prompt length and instruction complexity, indicating format and decoding hyperparameters influence outputs.",
            "citation_title": "Testing the reliability of chatgpt for text annotation and classification: A cautionary remark.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (GPT-family models)",
            "model_description": "Instruction-tuned conversational LLM(s); study examined sensitivity of annotation outputs to sampling temperature and prompt characteristics.",
            "model_size": null,
            "task_name": "Text annotation and classification (general)",
            "task_description": "Reproducing human annotations for diverse annotation tasks and assessing reliability/sensitivity to prompt hyperparameters.",
            "problem_format": "Natural-language prompts; variation in prompt length, complexity; generation temperature varied",
            "format_category": "prompt style / decoding settings",
            "format_details": "Findings highlight that annotation outcomes change with temperature and prompt length/complexity; suggests careful control of generation hyperparameters when using LLMs as annotators.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Exploratory analysis of ChatGPT annotations; temperature and prompt complexity varied; details in Reiss (2023).",
            "statistical_significance": null,
            "uuid": "e7452.9",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Context placement / long-context effect (Liu et al. 2023)",
            "name_full": "Lost-in-the-middle long-context retrieval effect (N. F. Liu et al., 2023)",
            "brief_description": "Prior finding that LLMs use information at the beginning or end of long contexts more effectively than information placed in the middle, which the paper cites to explain why complex/long prompts (with task-critical info in the middle) can reduce performance.",
            "citation_title": "Lost in the middle: How language models use long contexts.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Study-level observation about transformer-based LLMs' context utilization patterns rather than a single model.",
            "model_size": null,
            "task_name": "General long-context utilization",
            "task_description": "Analysis of how positional placement of important information in long input contexts affects retrieval/use by LLMs.",
            "problem_format": "Long-context prompts with task-relevant information at different positions (begin/middle/end)",
            "format_category": "input modality / prompt structure",
            "format_details": "Finding: performance peaks when vital information is at beginning or end, degrades when info is located in the middle of long contexts; used by the paper to interpret why longer/contextual or COT prompts underperformed versus concise prompts.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Analytical/empirical study across models and context lengths; cited for interpretation in present paper.",
            "statistical_significance": null,
            "uuid": "e7452.10",
            "source_info": {
                "paper_title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Detecting hate speech with gpt-3.",
            "rating": 2,
            "sanitized_title": "detecting_hate_speech_with_gpt3"
        },
        {
            "paper_title": "Designing of prompts for hate speech recognition with incontext learning.",
            "rating": 2,
            "sanitized_title": "designing_of_prompts_for_hate_speech_recognition_with_incontext_learning"
        },
        {
            "paper_title": "Respectful or toxic? using zero-shot learning with language models to detect hate speech.",
            "rating": 2,
            "sanitized_title": "respectful_or_toxic_using_zeroshot_learning_with_language_models_to_detect_hate_speech"
        },
        {
            "paper_title": "Towards legally enforceable hate speech detection for public forums.",
            "rating": 2,
            "sanitized_title": "towards_legally_enforceable_hate_speech_detection_for_public_forums"
        },
        {
            "paper_title": "Testing the reliability of chatgpt for text annotation and classification: A cautionary remark.",
            "rating": 2,
            "sanitized_title": "testing_the_reliability_of_chatgpt_for_text_annotation_and_classification_a_cautionary_remark"
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts.",
            "rating": 2,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        },
        {
            "paper_title": "Chatgpt outperforms crowd-workers for text-annotation tasks.",
            "rating": 1,
            "sanitized_title": "chatgpt_outperforms_crowdworkers_for_textannotation_tasks"
        }
    ],
    "cost": 0.020445500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection
12 Mar 2024</p>
<p>Tharindu Kumarage 
Equal Contribution</p>
<p>Amrita Bhattacharjee 
Equal Contribution</p>
<p>Joshua Garland jtgarlan@asu.edu </p>
<p>Arizona State University</p>
<p>Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection
12 Mar 2024AF10CA955D89F98DCAFD8538386B61B1arXiv:2403.08035v1[cs.CL]
Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis.One intriguing application is in text classification.This becomes pertinent in the realm of identifying hateful or toxic speech-a domain fraught with challenges and ethical dilemmas.In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content.Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training.Providing insight into the factors that contribute to an LLM's proficiency (or lack thereof) in discerning hateful content.By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection.</p>
<p>Introduction</p>
<p>Online social media platforms have become important channels of communication and sharing information, opinions, and connecting with other individuals and businesses.However, these platforms are also often used for hateful or toxic content, bullying and intimidation, etc. (Poletto et al., 2021).Given the scale of such platforms, hate speech and toxic content detection is a challenge and performing such detection manually is infeasible.This necessitates the use of automated detection systems Del Vigna12 et al. (2017); Schmidt and Wiegand (2017), which also is a challenge in practice due to the dynamic nature of hate speech Sheth et al. (2023).Hate speech can evolve with time, is highly subjective, and may be dependent on the context in which it is expressed MacAvaney et al. (2019); Sheth et al. (2024).</p>
<p>With the advent of advanced large language models (LLMs), there is growing interest in leveraging these models for content moderation.Specifically, using them to detect harmful and toxic content online by simply prompting the models.Several recent studies have examined the efficacy of GPT-3 Brown et al. (2020) and GPT-3.5 Huang, Kwak, and An (2023)1 st in detecting hate speech, encompassing both explicit and implicit forms.OpenAI has recently presented in-house experiments demonstrating GPT-4's OpenAI (2023) potential as a content moderator2 nd .Similarly, the state-of-the-art open-source model, Llama 2 Touvron et al. (2023), has shown promise in hate speech detection.In this study, our objective is to thoroughly assess these claims and delve into the nuances behind the LLMs' ability to discern hate speech.To achieve this, first explore the space of LLMs as a detector or text classifier, with a focus on the task of hate speech detection.Then, we evaluate several candidate LLMs, spanning both open-source and proprietary models, and address the following research questions:</p>
<p>Q1: How robust are these LLMs in detecting hate speech?We will examine and compare multiple LLMs on various types of hate speech: both general and targeted towards specific minorities.We aim to determine if these LLMs primarily rely on specific keywords, such as profanities, for detection, or if they genuinely discern and characterize the hateful intent of the speech.</p>
<p>Q2: How do various prompting techniques influence the hate speech detection efficacy of LLMs?We will compare different prompting strategies, with varying degrees of complexity, to discern differences in how they affect the hate speech detection capabilities.Based on our findings, we will endeavor to provide insights into the specific elements and nuances of LLMs and best practices surrounding the use of LLMs for this particular task.</p>
<p>LLMs as Text Classifiers or Annotators</p>
<p>Given the availability of several large language models, both open-source and proprietary (albeit via APIs), these technologies are increasingly being used in NLP applications such as text classification.Owing to the success of the more recent larger LLMs (such as Chat-GPT, GPT-4 OpenAI (2023), Llama 2 Touvron et al. ( 2023), etc.), researchers are actively exploring novel use-cases of such models in order to tackle issues such as generalization, data scarcity, etc.In this section we provide a brief overview on how language models (both pre-trained language models, and the more recent large language models) have been used in the task of text classification, first going over the general text classification task, before delving into hate speech specific classifiers.</p>
<p>General Text Classifier or Annotator</p>
<p>In this section, we describe some works that have used language models for the general problem of text classification.We further divide this section into two categories: (i) the pre-LLM era, and (ii) the LLM era.</p>
<p>Pre-LLM Era</p>
<p>In the pre-LLM era, pre-trained language models (PLMs) such as BERT Devlin et al. (2018), RoBERTa Y. Liu et al. (2019), BART Lewis et al. (2019) etc. have been used extensively as language encoders.These PLMs are essentially transformer-based language models that are pre-trained on a large corpus of unlabeled text data (mostly webtext) and often fine-tuned on downstream task datasets to perform classification or detection.Given the extensive pre-training that these language models go through, PLMs are often used as general language encoders in a classification task, with additional classification layers or classification heads added to facilitate task-specific fine-tuning Howard and Ruder (2018); Arslan et al. (2021).</p>
<p>For example, authors in Kant et al. (2018) first pre-train and then fine-tune an encoderdecoder type language model on task specific data for the task of multi-dimensional sentiment classification and compare their method with a pre-trained ELMo Peters et al. (1802), which is then further fine-tuned on their tasks-specific dataset.BERT Devlin et al. (2018), which is a bidirectional transformer-based language model, has shown impressive performance on many natural language understanding tasks.Authors in Sun et al. (2019) in-vestigate the training regimes and different fine-tuning settings to understand how to get the most out of fine-tuning BERT for the task of text-classification.Through their experiments they advise that text classification using BERT can be improved via the following best practices: further pre-training on task-specific in-domain data, multi-task fine-tuning rather than single task fine-tuning etc.</p>
<p>Given the smaller sizes of pre-trained language models as compared to more recent models like ChatGPT or Llama, these models have been used in several other text classification tasks, often with task-specific fine-tuning or in conjunction with other specialized architecture or training regimes Min et al. (2023).Examples of some tasks where such pre-trained language models have been used are toxic comment classification Zhao, Zhang, and Hopfgartner (2021), counter-speech detection Garland et al. (2020Garland et al. ( , 2022) )</p>
<p>LLM Era</p>
<p>Given the impressive performance of newer LLMs such as ChatGPT and GPT-4 OpenAI (2023) on a variety of natural language tasks, that too in a zero-shot manner, researchers are evaluating the possibility of using such LLMs as annotators.This could potentially assuage data scarcity issues in tasks and thereby facilitate or improve training of better models.One recent work Gilardi, Alizadeh, and Kubli (2023) performed a systematic evaluation of the annotation capabilities of ChatGPT especially in comparison to annotations obtained from crowd workers on Amazon Mechanical Turk3 rd .They evaluate the accuracy of ChatGPT and MTurk workers with annotations from trained annotators and show that ChatGPT outperforms the MTurk crowd workers, on a variety of content moderation tasks involving different four datasets of Tweets and news articles.</p>
<p>Another recent study Zhu et al. ( 2023) evaluated the capability of ChatGPT to reproduce human-generated labels on a set of five benchmark text datasets, on tasks such as stance detection, bot detection, sentiment analysis and hate speech detection.Results show that ChatGPT can replicate the human generated labels to a certain extent, achieving an accuracy of 0.609 across the five datasets, but is still far from being a perfect annotator.The authors also find varying performance of ChatGPT across different labels within one specific task.A similar observation has been made by authors in Bhattacharjee and Liu (2023) where ChatGPT was used to distinguish AI-generated text from human-written text, and an asymmetric performance across the two labels was identified.However, exper-iments demonstrate that GPT-4 has superior performance on the task.A similar work uses ChatGPT in automatic genre classification, where the task is to classify a given text into one of several genre categories such as News, Legal, Promotion, etc.The authors evaluate ChatGPT and compare its performance with a fine-tuned XLM-RoBERTa, and they test on both English and Slovenian language data.Interestingly, for the English split, ChatGPT performs better than the fine-tuned XLM-RoBERTa model, even without any labeled data, although the performance drops a bit for the Slovenian one.</p>
<p>Compared to all these works that demonstrate the potential for using LLMs and, in particular ChatGPT as an annotator, one interesting piece of work Reiss (2023) investigates the reliability of ChatGPT-derived annotations, and demonstrates that the annotations rely heavily on the temperature parameters and possibly other factors such as length of the text prompt and complexity of instructions.</p>
<p>Hate Speech Classifiers</p>
<p>In this section, we go over recent works that have used language models in a hate speech classification task, and we divide this section into two categories: (i) the pre-LLM era, and (ii) the LLM era.</p>
<p>Pre-LLM Era</p>
<p>Similar to the general classification, early applications of language models in hate speech detection employed pre-trained language models as rich embeddings or representations for the text.Since hate speech detection is often heavily dependent on language-specific words and phrases such as profanities, there have been many efforts in building hate speech classifiers for specific languages.Among methods that use pre-trained language models in the detection framework, some examples are Plaza-del Arco et al. (2021) for Spanish hate speech detection where they use both multilingual pre-trained LMs like mBERT and XLM Lample and Conneau (2019) as well as a Spanish version of BERT called BETO4 th .Authors in Pham et al. (2020) build a detector for Vietnamese hate speech by using a RoBERTa model, or in particular, a version trained for the Vietnamese language called PhoBERT Dat and Tuan (2020).Similar efforts involving detection using multilingual and monolingual versions of BERT or RoBERTa have also been done for Italian hate speech detection Lavergne et al. (2020), where alongside multilingual models, Italian versions such as AlBERTo, PoliBERT and UmBERTo have been used.Similar efforts for training language-specific hate speech detectors by fine-tuning different variants of the BERT family of models have been used in languages such as Marathi Velankar, Patil, and Joshi (2022), Polish Czapla et al. (2019).</p>
<p>Authors in Stappen, Brunn, and Schuller (2020) use frozen pre-trained language models as feature extractors in a framework for cross-lingual hate speech detection.Alongside comparing various framework designs for the task, authors also evaluate their proposed method in zero-shot and few-shot setting with substantial success.Another interesting work in multi-lingual hate speech detection uses a multi-channel BERT Sohn and Lee (2019), i.e., multiple language-specific pre-trained BERT models in parallel to facilitate transfer learning, The authors also experiment with adding additional classification signals by providing translated versions of the input to the classifier.Given that the lack of labeled data in low-resource languages is a major bottleneck in the development of hate speech detectors for these particular languages, Zia et al. ( 2022) proposed a framework that leverages labeled data from a high-resource language such as English and used a language model based teacher-student framework to perform transfer learning for hate speech detection on a target language, in the absence of target labels.To do this, they first fine-tune a multilingual language model on labeled training data from the source language.Then they use this model to generate pseudo-labels for samples from the target language, by simply predicting in a zero-shot manner.Finally, they use these pseudo-labels to fine-tune a monolingual pre-trained language model to perform hate speech detection on the target language without requiring any labels from the target.</p>
<p>LLM Era</p>
<p>Most of the works discussed above use pre-trained language models of parameter sizes in the range of a few hundred million.However, there is a growing trend towards developing and training larger language models, often with parameter sizes of a few hundred billion.Performance of language models on NLP tasks have shown huge improvements with increase in the scale of these models.These larger models, now often referred to as Large Language Models (LLMs) are trained on huge internet-scale data corpora.Due to their extensive pre-training, LLMs often demonstrate good performance on a variety of tasks even on a zero-shot manner.The standard mode of using these LLMs is via the task of text generation, whereby the user provides a text input as a 'prompt' to the LLM, and the LLM produces some text output conditioned on the input prompt.</p>
<p>Broadly, there are two categories of LLMs: base LLMs -that simply perform the task of next token prediction, essentially performing a text completion task; and instructiontuned LLMs -where LLMs are specifically trained to follow instructions in the prompt.Instruction-tuned LLMs are useful for a variety of tasks.Examples of such instruction-tuned LLMs are ChatGPT, GPT-4, the Llama family of models, etc.An example of a base LLM is GPT-3 Brown et al. (2020) by OpenAI, with 175 billion parameters.</p>
<p>Authors in Chiu, Collins, and Alexander (2021) evaluate the performance of GPT-3 Brown et al. (2020) on hate speech detection in a variety of settings, including zero-shot, one-shot (where a single example is provided in the prompt), few-shot (where a small number of samples are provided in the prompt as examples).The authors also evaluate the fewshot performance along with instructions in the prompt wherein a small instruction is also provided in the prompt, specifying what the possible labels are, such as 'sexist', 'racist' or 'neither'.Interestingly, the study finds that GPT-3 performs the best when prompted without instructions in a few-shot setting.In a similar direction, alongside experimenting with different prompt structures for this task, Han and Tang (2022) shows how increasing the number of labeled samples in the prompt in the few shot setting improves the performance of GPT-3.</p>
<p>Other recent prompt-based detection methods include Luo et al. (2023), where the authors propose a new category of the hate speech detection task: enforceable hate speech detection, where text content is classified as hate speech if it violates at least one legally enforceable definition of hate speech.For the detection method, the authors present various settings of prompt tuning on a RoBERTa-large model.Prompt-tuning is a new parameterefficient fine-tuning method that enables fine-tuning of large language models in lowresource settings, by freezing the model weights and updating a small set of parameters instead.Del Arco, Nozza, and Hovy (2023) evaluates zero-shot hate speech detection by simply prompting instruction-tuned models FLAN-T5 Chung et al. (2022) and mT0 Muennighoff et al. (2022), and compare the performance with encoder-based language models such as the BERT family of models.They perform the evaluation on an extensive collection of 8 benchmark datasets containing online hate speech.Their results show that the instruction-tuned models have superior performance.</p>
<p>Recently, the accessibility and ease of use of ChatGPT, along with its impressive performance has inspired a series of interesting exploratory efforts into using ChatGPT as a detector for many NLP tasks.Along this direction, authors in Huang et al. (2023) have experimented with ChatGPT to understand how well it can detect implicit hate speech in Tweets, and also whether it can provide explanations for the reasoning.Their experiments demonstrate that ChatGPT has the potential to be used for such subjective tasks such as implicit hate speech detection.Furthermore, ChatGPT generated explanations also appear to have more clarity than human-written explanations, although there was no significant difference in informativeness.ChatGPT has also been evaluated for language-specific hate speech detection in Portuguese Oliveira et al. (2023) and results show that even without any fine-tuning, ChatGPT performs well in the detection task.</p>
<p>Empirical Analysis</p>
<p>In this section, we undertake several experiments utilizing representative LLMs to empirically assess their proficiency in identifying hate speech.Through these experiments, we address two primary research questions:</p>
<p>• RQ1: How robust are LLMs in classifying hate speech?</p>
<p>• RQ2: How do various prompting techniques influence the hate speech detection efficacy of LLMs?</p>
<p>Experiment Design</p>
<p>In this subsection, we delve into the details of our experimental design, highlighting the critical decisions made to address the stated research questions.Paramount among these decisions were the choice of LLMs as the hate speech detector(classifier) and the dataset selection to rigorously assess the robustness of the chosen LLMs in detecting hate speech.</p>
<p>LLM Selection</p>
<p>As mentioned in Section 2, numerous advanced LLMs are currently available, encompassing both open-source and proprietary options.From the open-source category, we opted for the Llama-2 model (7B parameters chat variant5 th ) and the Falcon model (7B parameters chat variant6 th ) based on their notable standings on the Open LLM Leaderboard hosted by Hugging Face7 th .For proprietary models, we selected GPT 3.5, which serves as a foundational benchmark for subsequent LLMs, including GPT 4. To enhance the reproducibility of our results, we utilized the specific version "gpt-3.5-turbo-0613."The experiments involving the chosen LLMs-Llama-2, Falcon, and GPT 3.5-were conducted between September 25th and October 8th, 2023.</p>
<p>Dataset Selection</p>
<p>The primary objective of this study is to assess the robustness and reliability of selected LLM classifiers in identifying real-world hate speech.To achieve this, our evaluation cri-teria encompass: i) determining the classifier's capability to recognize general hate speech concepts; ii) gauging its sensitivity to hate speech directed at specific groups, such as women or Muslims; and iii) ascertaining any biases the classifier might exhibit towards extraneous features, like swear words or group identifiers.Consequently, we chose the HateCheck dataset Röttger et al. (2021) as the primary hate speech dataset to evaluate the LLM classifiers.This dataset is meticulously designed to serve as a diagnostic tool for hate speech detection systems, aligning perfectly with our evaluation criteria.It features annotations that categorize hate speech as either 'directed' or 'general' and identifies the target of the hate.Furthermore, the non-hateful examples within this dataset are constructed to probe potential biases in hate speech classifiers towards spurious correlations.Consider the example: "They are building a new dyke near the beach."The aim here is to discern if the classifier exhibits a bias towards the term "dyke" and misclassifies these examples as hate speech.For a comprehensive overview of the dataset statistics, refer to Figure 1.</p>
<p>Hate Speech Detection</p>
<p>The concluding aspect of our experimental design pertains to the methodology detailing our hate speech classification approach using the chosen LLMs.We employed the LLMs in a zero-shot fashion (i.e., without specific training on hate speech datasets), presenting them with examples from the HateCheck dataset and instructing each LLM to determine the hatefulness of the input text.Consequently, it is imperative to discuss both the input prompt utilized to query the LLM and the labeling method adopted to categorize the LLM's response and decide the final classification label.</p>
<p>Classification Prompt</p>
<p>The input prompt is integral to LLM-based classifiers.Its primary objective is to concisely direct the LLM to analyze a provided text and determine the appropriate label, signifying whether the content is hateful or non-hateful.In our study, we incorporated the following prompt: Prompt Is the following text hateful or not?Just answer in Yes or No. Don't provide explanations.</p>
<p>text: {hate speech}</p>
<p>Labeling Method and Caveats</p>
<p>The labeling method is employed to translate the text output of the LLM into binary class labels: 1 ('hate') and 0 ('non-hate').When the LLM explicitly responds with 'Yes' or 'No', the label mapping process is straightforward.However, some scenarios necessitate a more nuanced approach to categorize the output:</p>
<p>• Caveat 1: Deviation from Instructions: LLMs occasionally diverge from the provided directives and offer explanations alongside the label.In these instances, we manually reviewed the diverse, unique outputs, determining the appropriate labels grounded in keywords like 'Yes,' 'hateful,' 'No,' and 'not hateful.'</p>
<p>• Caveat 2: Activation of LLM Guardrails: Certain examples within the HateCheck dataset activate the LLM's built-in guardrails, designed to identify and mitigate hateful or offensive content processing.When these guardrails are triggered, the LLM yields a message indicating the presence of hate or offensive language, leading us to categorize such instances as hate speech.</p>
<p>Experiment Results</p>
<p>RQ1: LLM's Hate Classification Performance</p>
<p>Table 1 displays the efficacy of selected LLMs in classifying hate speech, using data from the HateCheck dataset.The performance metrics, derived from direct prompt outcomes, reveal that both GPT-3.5 and Llama 2 exhibit commendable efficiency, with accuracy and F1 scores ranging between 80-90%.This underscores their proficiency in identifying hate speech.GPT-3.5 outperforms the others, an expected outcome given it has benefited from numerous advanced iterations of Reinforcement Learning from Human Feedback (RLHF) (from November 2022 onwards), and it contains more parameters than the other LLMs we considered.In contrast, Llama 2, despite its smaller 7B parameter model, delivers a performance that nearly matches GPT-3.5.The Falcon model, however, demonstrates inferior classification, performing below the level of random guessing.This disparity in performance between Llama 2 and Falcon can be attributed to the specific tuning conducted to optimize their pre-trained versions for chat compatibility.Another potential explanation is that the Llama 2 authors deliberately retained toxic data during pre-training to enhance downstream task generalization Touvron et al. ( 2023), positioning it as a more adept hate speech classifier than the Falcon model.</p>
<p>Error Analysis</p>
<p>We conducted an error analysis to delve into the challenges the existing LLMs face in identifying hate speech and to pinpoint specific contexts where these models struggle to discern hate speech effectively.For this examination, we utilized the directionality annotations and target annotations from the HateCheck dataset.Within the realm of directionality, we assessed the proportion of misclassified hate speech samples, distinguishing between errors in identifying directed hate speech and those in discerning general hate speech.As shown in Table 1, both Llama 2 and Falcon have equal error rates for directed and general hate speech, suggesting that these models possess comparable proficiency in detecting both types of hate speech.In contrast, GPT 3.5 exhibits a higher error rate for directed hate speech than general hate speech.Subsequently, we assessed the error rates of the LLMs concerning different hate targets.The objective of this segment was to ascertain which target-associated hate speech poses the most significant detection challenges for the LLMs.</p>
<p>As demonstrated in Table 2, the error rates for Llama 2 and Falcon regarding specific targets largely mirror the original distribution of these targets in the dataset.However, GPT 3.5 exhibits a disproportionately elevated error rate when identifying hate speech related to It is crucial to examine whether the notable classification performance of LLMs can be attributed to spurious correlations, such as categorizing a text as hate speech based solely on the presence of swear words or group identifiers, rather than substantive reasoning.This consideration is facilitated by the non-hate examples included in the HateCheck dataset, which contains elements like swear words and group identifiers used in non-hateful contexts.Evaluating the performance of LLMs in classifying these "non-hate" examples is essential to confirm their reliability as hate speech classifiers.As detailed in Table 1, although Llama 2 demonstrates impressive classification accuracy for "hate" content, its performance diminishes in identifying non-hateful content, suggesting a reliance on spurious correlations.Conversely, GPT 3.5 maintains robust performance in classifying both "hate" and "non-hate" content.</p>
<p>We further investigated the specific types of spurious correlations influencing these LLMs using the functionality annotations of the HateCheck dataset.These annotations identify various categories of spurious correlations scenarios evident in non-hateful content, Figure 2: Error analysis on non-hate class</p>
<p>RQ2: Effect of Prompting</p>
<p>The input prompt plays an indispensable role in LLM-based classifiers.Generally, the efficacy of an LLM in classifying text is intrinsically tied to the quality of the input prompt.</p>
<p>In light of this, we conducted an extended experiment involving the top-performing LLM, GPT 3.5, to explore the impact of various prompts on classification performance.As illustrated in Figure 3, we introduced two additional prompt types, referred to as context prompt, and chain-of-thought(COT) prompt.</p>
<p>Table 3 presents the classification results of GPT 3.5 using different prompts employed in our study.Intuitively, we anticipated the performance of the LLM classifier to improve as we transitioned through the prompts from left to right in Figure 3, particularly given the additional context and incorporation of the COT method.However, unexpectedly, the direct concise prompt yielded the most superior performance out of the three prompts.One potential rationale for this result is that an overly complex prompt, paired with the inherently intricate nature of hate speech detection, might obscure the LLM's understanding of the task rather than clarifying it.Another explanation aligns with recent findings on LLMs, suggesting that performance peaks when vital information is positioned at the beginning or end of the input context and diminishes substantially when models must retrieve relevant information from the middle of lengthy contexts N. F. Liu et al. (2023).</p>
<p>Discussion</p>
<p>In addressing the two research questions posed, our findings offer significant insights into the robustness and nuances of LLMs in hate speech classification.</p>
<p>Answering RQ1: LLM's Robustness in Classifying Hate Speech</p>
<p>For RQ1, the GPT-3.5 and Llama 2 models proved their robustness in classifying hate speech, boasting accuracy and F1 scores between 80-90%.Despite its fewer parameters, Llama 2 nearly matches the performance of GPT-3.5, although GPT-3.5 remains superior.We attribute this to its advanced RLHF iterations and larger parameter size.Falcon, conversely, demonstrated subpar performance, indicating its unsuitability for reliable hate speech classification.The error analysis further enriched our understanding.While Llama 2 and Falcon demonstrated equal proficiency in detecting directed and general hate speech, GPT-3.5 showed a higher error rate for directed hate speech.Additionally, it exhibited an increased error rate in identifying hate speech targeted at women, indicating potential areas for improvement in its training and calibration.Llama 2's diminished performance in classifying non-hateful content hinted at its reliance on spurious correlations.Both Llama 2 and GPT-3.5 were challenged in scenarios involving the counteraction of hate speech through referencing or quoting hateful content, pinpointing a need to refine the LLMs' handling of such contexts.</p>
<p>Prompt</p>
<p>Answering RQ2: Influence of Prompting Techniques</p>
<p>As for RQ2, the efficacy of LLMs is notably influenced by the employed prompting techniques.Contrary to our anticipation that more complex prompts (such as context and chainof-thought prompts) would enhance classification performance, the direct concise prompts delivered best results.It suggests that simplicity and conciseness in prompts might facilitate clearer hate speech detection task comprehension for LLMs, leading to more accurate classifications.</p>
<p>Best Practices and Pro Tips</p>
<p>Optimizing LLM Performance</p>
<p>When utilizing LLMs as hate speech classifiers, certain practices can optimize their performance and reliability.</p>
<p>• Select Appropriate LLMs: GPT-3.5 and Llama 2 have shown notable efficacy; however, it's crucial to consider the specific needs and contexts of the application.Evaluate multiple models to identify which offers the best balance of accuracy and computational efficiency.</p>
<p>• Input Prompt: Direct and concise prompts have been shown to be more effective.Avoid overly complex prompts that could potentially confuse the model or dilute the task's clarity.Experiment with various prompt designs to identify which yields optimal performance for the specific LLM and classification task.</p>
<p>• Error Analysis: Conduct detailed error analyses to identify specific areas where the LLM struggles, and consider this information when fine-tuning or selecting models for deployment.</p>
<p>• Labeling Function: The labeling function plays a pivotal role in the performance of LLMs in classification tasks.It's essential to optimize and test various labeling functions to ensure that they are accurate and reliable, avoiding misclassifications especially in complex scenarios like counter-speech.</p>
<p>Mitigating the Influence of Spurious Correlations</p>
<p>The risk of LLMs relying on spurious correlations, as observed with Llama 2, underscores the necessity of specific strategies to mitigate such influences.</p>
<p>• Balanced Fine-tuning: Conduct additional fine-tuning of the LLM with balanced training data that includes diverse examples of hate speech and non-hate speech, reducing the model's reliance on specific words or phrases as indicators of hate speech.</p>
<p>• Functionality Annotations: Leverage functionality annotations to identify and analyze potential spurious correlations, enabling the refinement of the model's classification capabilities.</p>
<p>• Real-world Testing: Test the LLMs in real-world scenarios to assess their performance beyond controlled experiments.Adapt and refine the models continuously based on the emerging data and classification challenges.</p>
<p>Incorporating these insights and practices will be instrumental in enhancing the reliability, accuracy, and fairness of LLMs in hate speech classification, ensuring they are a valuable tool in combating online hate while preserving freedom of expression.</p>
<p>Conclusion</p>
<p>In our study, we provided a detailed look into the progression of language models for hate speech classification, from the days of pre-LLMs to the modern era of sophisticated LLMs like GPT.Earlier language models, often needed significant fine-tuning to work well, but new LLMs, like GPT-3.5 and Llama 2, have shown they can be effective at identifying some forms of hate speech right out of the box, even in zero and few shot settings.</p>
<p>We explored the capabilities of three LLMs, GPT-3.5, Llama 2 and Falcon, on the HateCheck dataset to gain deeper insights into their abilities and challenges in classifying hate speech.From our experiments, a few key points stood out: GPT-3.5 and Llama-2 were quite effective overall with accuracy levels between 80-90%, but Falcon lagged behind considerably.As we discussed, this may be an artifact of what data was used to train Falcon.When we looked into the nuances of hate speech, like understanding who the hate was directed at, all of these models faced challenges and their performance declined considerably.For instance, GPT 3.5 struggled particularly with recognizing hate directed towards women.We also found through experimentation that clear and straightforward prompts worked best, hinting that simplicity of classification instructions may be key for effective classification performance.</p>
<p>Hate speech classification remains a challenging area for many reasons, not just due to its nuanced nature but also the ethical concerns around data collection and especially labeling.LLMs, even in zero and few shot settings, present a potential exciting way forward.While they are promising, there is still much to understand and refine.We hope our findings and recommendations from this study offer a useful guide for those looking to delve further into the capabilities of LLMs for managing online hate.Forging towards a safer, more inclusive digital landscape for everyone.</p>
<p>text mining Zhang et al. (2021), sentiment classification Meng et al. (2020); Rathnayake et al. (2022), etc.</p>
<p>Figure 1: HateCheck dataset statistics</p>
<p>Table 1 :
1LLMHate ClassNon-Hate ClassOverallPRF1PRF1 Accuracy AUROCFalcon 0.69 0.43 0.53 0.3 0.56 0.40.470.49Llama 2 0.80 1.00 0.89 0.99 0.46 0.630.830.73GPT 3.5 0.89 0.98 0.93 0.93 0.73 0.820.890.85
Hate classification results: Precision(P), Recall(R), F1-score(F1) values are recorded for both "Hate" and "Non-Hate" classes.Highest performance under each metric is in bold.</p>
<p>Table 2 :
2
Error analysis: error rate (%) under "directionality" and "hate-target".Highest error rate under each category is in bold.
"women."Performance Attributed to Spurious Correlations Rather Than Proper Reasoning</p>
<p>Given the following text, evaluate if it contains hate speech, specifically speech that promotes violence or discrimination against a group based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.Figure3: Prompt templates used for hate speech classification including "slur", "profanity", "negate hateful statements", "group identifiers", "countering of hate speech through quoting or referencing hate speech examples" and "abuse targeted at objects, individuals, and non-protected groups."Asillustrated in Figure2, Llama 2 exhibits more errors attributed to spurious correlations, further underlining its diminished performance in classifying the 'non-hate' category.Both Llama 2 and GPT 3.5 display heightened inaccuracies in distinguishing examples that counteract hate speech by referencing or quoting hate speech instances.This augmented error rate may be, in part, due to the labeling function, where specific counter-speech scenarios could trigger the LLM guardrails.As a result, the labeling function might mistakenly assume that the LLM's response to these examples implies a hate label.This underscores the significance of adequately addressing such scenarios when integrating LLMs into real-world hate speech detection frameworks.
Context Prompt]Chain-of-thought Prompt]Given the following text, evaluate if it containshate speech, specifically speech that promotes[Direct Prompt]violence or discrimination against a groupIs the following text hateful or not?based on attributes such as race, religion,Just answer in Yes or No. Don'tethnic origin, sexual orientation, disability, orprovide explanations.gender.ProcessFirst describe the step by step process of howtext: <code>{hate_speech}</code>[ Please provide your assessment as labels Yes or No.you evaluate the given text. Then provide your assessment as labels Yes or No.text: <code>{hate_speech}</code>[ text: <code>`{hate_speech}</code>++Additional contextThink step-by-stepabout the taskbefore classifying</p>
<p>Table 3 :
3Hate ClassNon-Hate ClassOverallPRF1PRF1 Accuracy AUROCDirect 0.89 0.98 0.93 0.93 0.73 0.820.890.85Context 0.91 0.85 0.88 0.71 0.82 0.760.840.83COT0.88 0.81 0.84 0.69 0.79 0.740.800.79
GPT 3.5's hate classification results with different prompts: Precision(P), Recall(R), F1-score(F1) values are recorded for both "Hate" and "Non-Hate" classes.Highest performance under each metric is in bold.</p>
<p>st    ChatGPT and GPT-3.5 are used interchangeably here.
nd https://openai.com/blog/using-gpt-4-for-content-moderation
rd https://www.mturk.com/
th https://github.com/dccuchile/beto
th https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
th https://huggingface.co/tiiuae/falcon-7b-instruct
th https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard
and data mining(pp. 626-635).Sohn, H., &amp; Lee, H. (2019).Mc-bert4hate: Hate speech detection using multi-channel bert for different languages and translations.In 2019 international conference on data mining workshops (icdmw) (pp.551-559).Stappen, L.,Brunn, F., &amp; Schuller, B. (2020).Cross-lingual zero-and few-shot hate speech detection utilising frozen transformer language models and axel.arXiv preprint arXiv:2004.13850., C., et al. (2019).How to fine-tune bert for text classification?In Chinese computational linguistics: 18th china national conference, ccl 2019, kunming, china, october18-20, 2019, proceedings 18 (pp. 194-206).Zia, H. B., et al. (2022).Improving zero-shot cross-lingual hate speech detection with pseudo-label fine-tuning of transformer language models.In Proceedings of the international aaai conference on web and social media(Vol. 16, pp. 1435(Vol. 16, pp. -1439)).Sun
A comparison of pre-trained language models for multi-class text classification in the financial domain. Y Arslan, Companion proceedings of the web conference 2021. 2021</p>
<p>Fighting fire with fire: Can chatgpt detect ai-generated text?. A Bhattacharjee, H Liu, arXiv:2308.012842023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, Advances in neural information processing systems. 202033</p>
<p>K.-L Chiu, A Collins, R Alexander, arXiv:2103.12407Detecting hate speech with gpt-3. 2021arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, arXiv:2210.114162022arXiv preprint</p>
<p>Universal language model fine-tuning for polish hate speech detection. P Czapla, Proceedings ofthePolEval2019Workshop. thePolEval2019Workshop2019149</p>
<p>Phobert: Pre-trained language models for vietnamese. Findings of the. N Dat, N Tuan, EMNLP. 2020. 2020Association for Computational Linguistics</p>
<p>Respectful or toxic? using zero-shot learning with language models to detect hate speech. F M P Del Arco, D Nozza, D Hovy, The 7th workshop on online abuse and harms (woah). 2023</p>
<p>Hate me, hate me not: Hate speech detection on facebook. F Del Vigna12, Proceedings of the first italian conference on cybersecurity (itasec17). the first italian conference on cybersecurity (itasec17)2017</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, arXiv:1810.048052018arXiv preprint</p>
<p>Countering hate on social media: Large scale classification of hate and counter speech. J Garland, Proceedings of the fourth workshop on online abuse and harms. the fourth workshop on online abuse and harmsAssociation for Computational Linguistics2020, November</p>
<p>Impact and dynamics of hate and counter speech online. J Garland, EPJ Data Science. 11132022</p>
<p>Chatgpt outperforms crowd-workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, arXiv:2303.150562023arXiv preprint</p>
<p>Designing of prompts for hate speech recognition with incontext learning. L Han, H Tang, 2022 international conference on computational science and computational intelligence (csci). 2022</p>
<p>Universal language model fine-tuning for text classification. J Howard, S Ruder, arXiv:1801.061462018arXiv preprint</p>
<p>F Huang, H Kwak, J An, arXiv:2302.07736Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. 2023arXiv preprint</p>
<p>Practical text classification with large pre-trained language models. N Kant, arXiv:1812.012072018arXiv preprint</p>
<p>G Lample, A Conneau, arXiv:1901.07291Cross-lingual language model pretraining. 2019arXiv preprint</p>
<p>Thenorth@ haspeede 2: Bert-based language model fine-tuning for italian hate speech detection. E Lavergne, 7th evaluation campaign of natural language processing and speech tools for italian. final workshop, evalita. 20202765</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, arXiv:1910.134612019BartarXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, arXiv:2307.031722023arXiv preprint</p>
<p>Y Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Towards legally enforceable hate speech detection for public forums. C F Luo, arXiv:2305.136772023arXiv preprint</p>
<p>Hate speech detection: Challenges and solutions. S Macavaney, PloS one. 148e02211522019</p>
<p>Text classification using label names only: A language model self-training approach. Y Meng, arXiv:2010.072452020arXiv preprint</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. B Min, ACM Computing Surveys. 5622023</p>
<p>N Muennighoff, arXiv:2211.01786Crosslingual generalization through multitask finetuning. 2022arXiv preprint</p>
<p>How good is chatgpt for detecting hate speech in portuguese?. A S Oliveira, Anais do xiv simpósio brasileiro de tecnologia da informac ¸ão e da linguagem humana. 2023</p>
<p>R Openai, Gpt-4 technical report. 2023</p>
<p>M E Peters, arXiv:1802.05365Deep contextualized word representations. 1802. 2018arXiv preprint</p>
<p>From universal language model to downstream task: Improving roberta-based vietnamese hate speech detection. Q H Pham, 2020 12th international conference on knowledge and systems engineering (kse). 2020</p>
<p>Comparing pre-trained language models for spanish hate speech detection. F M Plaza-Del Arco, Expert Systems with Applications. 1661141202021</p>
<p>Resources and benchmark corpora for hate speech detection: a systematic review. F Poletto, Language Resources and Evaluation. 552021</p>
<p>Adapter-based fine-tuning of pre-trained multilingual language models for code-mixed and code-switched text classification. H Rathnayake, Knowledge and Information Systems. 6472022</p>
<p>Testing the reliability of chatgpt for text annotation and classification: A cautionary remark. M V Reiss, arXiv:2304.110852023arXiv preprint</p>
<p>HateCheck: Functional tests for hate speech detection models. P Röttger, 10.18653/v1/2021.acl-longProceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing. the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processingAssociation for Computational Linguistics2021. August1Long papers</p>
<p>A survey on hate speech detection using natural language processing. A Schmidt, M Wiegand, Proceedings of the fifth international workshop on natural language processing for social media. the fifth international workshop on natural language processing for social media2017</p>
<p>Peace: Cross-platform hate speech detection-a causality-guided framework. P Sheth, Joint european conference on machine learning and knowledge discovery in databases. 2023</p>
<p>Causality guided disentanglement for cross-platform hate speech detection. P Sheth, Proceedings of the 17th acm international conference on web search. the 17th acm international conference on web search2024</p>            </div>
        </div>

    </div>
</body>
</html>
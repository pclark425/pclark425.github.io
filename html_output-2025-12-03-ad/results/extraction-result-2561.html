<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2561 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2561</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2561</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-6bbb82dde9c49f9d91c60e93d9ad0c051dca75d7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6bbb82dde9c49f9d91c60e93d9ad0c051dca75d7" target="_blank">A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems</a></p>
                <p><strong>Paper Venue:</strong> Adaptive Agents and Multi-Agent Systems</p>
                <p><strong>Paper TL;DR:</strong> This work conceptualises the ad hoc coordination problem formally as a stochastic Bayesian game in which the behaviour of a player is determined by its type, and derives a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises a set of user-defined types to characterise players based on their observed behaviours.</p>
                <p><strong>Paper Abstract:</strong> The ad hoc coordination problem is to design an ad hoc agent which is able to achieve optimal flexibility and efficiency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. We conceptualise this problem formally as a stochastic Bayesian game in which the behaviour of a player is determined by its type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises a set of user-defined types to characterise players based on their observed behaviours. We evaluate HBA in the level-based foraging domain, showing that it outperforms several alternative algorithms using just a few user-defined types. We also report on a human-machine experiment in which the humans played Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms. The results show that HBA achieved equal efficiency but a significantly higher welfare and winning rate.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2561.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2561.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HBA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harsanyi-Bellman Ad Hoc Coordination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ad hoc coordination agent that combines Bayesian type-based belief tracking with Bellman optimal planning: it maintains posteriors over user-defined behavioural types for other agents and plans actions that maximize expected long-term payoff under those beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Harsanyi-Bellman Ad Hoc Coordination (HBA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decentralized ad hoc coordination agent that (1) accepts a set of user-defined type hypotheses for each other agent, (2) observes state-action histories and computes posteriors over those types, and (3) computes best-response actions by combining the Bayesian expected-type distribution with a Bellman-style long-term payoff estimate (discounted planning). Implementations in the paper include an exact planning variant (Algorithm 3) and a sampling/planning + RL variant integrated with a Q-table and eligibility traces (Algorithm 2). HBA can include opponent-modelling types (conceptual types) and temporally reweighted posteriors to handle changing behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (experiments with 2, 3 players; general N in the model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Ad hoc agent (HBA) — plans and controls one player using type posteriors and Bellman planning; Teammates/opponents — represented as types in Θ*: (a) fixed-behaviour heuristic types H1–H4 (vision-based foraging behaviours), (b) learning agents (JAL, CJAL), (c) human participants in experiments. HBA treats each other agent role separately with its own type-space and posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution and evaluation (planning & decision-making in interactive multiagent tasks); includes online opponent modelling (hypothesis generation/use), adaptation (type inference), and policy planning; does not cover literature review or code implementation phases of scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized, belief-based best-response coordination: each HBA agent independently infers other agents' types from observed actions and then selects actions that maximize expected discounted long-term reward under those inferred type distributions; no centralized controller or explicit pre-coordination required. Coordination emerges by planning under the same model of others' behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit message-passing protocol; coordination is achieved via observation of shared environment state and agents' actions (implicit communication through behaviour). Internally messages are structured probabilistically (posteriors over discrete user-defined types) but there is no explicit inter-agent messaging format.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Agents receive scalar rewards from environment; HBA uses (a) likelihood-based posterior updates over types from observed actions (product posterior or temporally reweighted posterior), (b) Q-style long-term payoff updates in RL implementation with eligibility traces, and (c) planning simulations using user-defined types to compute expected returns. Posteriors act as feedback about others' behaviours and are used to adapt planning.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous / per time-step: HBA updates posteriors and chooses actions every time step based on the latest observed history; planning uses projected trajectories frequently (per decision) with simulated opponent actions sampled from current posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiagent logistics coordination (level-based foraging) and human-machine interactive games (Prisoner's Dilemma, Rock-Paper-Scissors) — general model applicable to interactive multiagent tasks (ad hoc coordination).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Flexibility (probability of task termination/solving) and Efficiency (average payoff per time step in solved tasks). Reported numeric results: HBA variants achieved perfect flexibility = 1.0 in some 2-player foraging tests; in a 10x10, 3-player, 8-food foraging test HBA reported flexibility 0.83 ± 0.01. Human-machine experiments: PD — HBA and CJAL had statistically equivalent total payoffs; HBA achieved (C,C) in ≥50% of final 10 rounds in >28% of games (CJAL: 0%). RPS — HBA winning rate 53.71% vs JAL 43.98%; HBA inferred human behaviour statistics: humans used 4.45 types (PD) with mean duration 4.96 rounds and 8.25 types (RPS) with mean duration 2.46 rounds. In simulated comparisons HBA variants 'Unl' and 'Gtw' were reported to be over 100% and 200% more efficient than alternative algorithms in a larger foraging test (textual claim in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to JAL, CJAL, and WoLF-PHC as algorithmic baselines and to human players: HBA outperformed JAL/CJAL/WoLF-PHC in the level-based foraging domain (higher efficiency and flexibility) and achieved statistically equal or better performance in human-machine PD and RPS experiments (equal total payoff in aggregate, higher welfare in PD, higher winning rate in RPS). Humans sometimes outperformed HBA when humans had greater planning power.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Belief-driven planning enabled better coordination and manipulation of other agents' decisions: (a) higher efficiency (faster task completion / higher per-step payoff) and higher welfare in PD due to HBA's ability to plan effects of its actions on others; (b) faster recognition of behavioural changes (via TR-posteriors) improving dynamic adaptation and win-rate in RPS (53.71% vs 43.98%). Quantitative examples: flexibility improvements to 1.0 in some tests; flexibility 0.83 ± 0.01 vs baselines ~0.734–0.749 in larger test; >100% / >200% efficiency gains claimed for some HBA variants vs alternatives in large simulated test.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires good user-defined type spaces Θ* — performance depends on how well Θ* approximates true types; product-posteriors can permanently zero-out types and fail under dynamic type changes; planning complexity and required planning power can limit performance (humans with larger planning horizon outperformed HBA in one test); specifying types manually is burdensome in complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — comparisons (ablation-like) across posterior formulations and type models: (1) product (normal) posterior (Unl), limited-window posterior (Lim), and temporally reweighted posterior (Gtw) — Gtw significantly improved efficiency and matched 'correct types' agent in dynamic-change experiments; (2) conceptual-type variants (c-types 1–4) tested separately — c-type 2 produced good efficiency while others were less effective; (3) 'Cor' variant (HBA with correct types known) used as upper bound. These comparisons quantify the impact of posterior formulation and type modelling on coordination performance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper recommends temporally reweighted posteriors (general time weight) for dynamic environments and inclusion of conceptual types when types are only approximately known; example parameter settings used in experiments: general time weight f(ξ) parameters a=10, b=0.01 (or b=0.05 in human experiments), c=3; planning/expansion parameters: sampling expansion depth d=20 (or 30 in larger tests) and x expansions per real step (x=3 or x=10); Q-learning / eligibility trace parameters β=0.2, γ=0.9, λ=0.9. No formal proof of universally optimal configuration; the paper lists these as empirically effective settings and highlights open questions about type-space design and posterior choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2561.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2561.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TR-posterior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporally Reweighted Posterior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A posterior update rule that replaces the product of likelihoods over history with a weighted sum of action-likelihoods using a monotonically nonincreasing time-weight f(·), enabling rapid re-assignment of probability to previously-zeroed types and better adaptation to changing behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Temporally Reweighted Posterior (component of HBA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A type-inference mechanism: L(H^t | θ_j) = Σ_{τ=0}^{t-1} f(t−τ) π_j(H^τ, a_j^τ, θ_j) with time weight f(ξ) ≥ 0 and nonincreasing in ξ. The TR-posterior is plugged into HBA's Bayesian update to compute Pr(θ_j | H^t). Implemented with 'general time weight' f(ξ)=max[0, a−b(ξ−1)^c] in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>component used for each other agent; supports arbitrary N</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>N/A (inference component used by the ad hoc agent for modelling all other agents' types)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>online adaptation / evaluation (type inference & model update)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Improves decentralized coordination by allowing the ad hoc planner to re-evaluate and quickly reassign probability mass to types based on recent observations — thereby enabling coordinated responses to behaviour changes.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not applicable (inference mechanism operating on observed actions); uses numerical time-weighted aggregation of observed action likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Uses recent observed actions (with recency-weighted influence) as feedback to reweight type probabilities; these posteriors feed into planning and thereby produce behavioral adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Updated every time-step (continuous per observation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used within HBA across foraging and human-game tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improved efficiency in dynamic/mismatched-type tests: Gtw (TR-posterior with general time weight) significantly outperformed product posterior variants; in a 2-player foraging test with dynamic type switching, Gtw matched performance of 'Cor' (HBA with correct types); in larger foraging tests TR-posterior variant showed up to ~200% efficiency improvement in text-reported comparisons (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to (a) product (multiplicative) posterior (Unl), and (b) limited-window normal posterior (Lim); Gtw outperformed both in experiments with dynamic type changes and partial-type accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Faster recovery when agents revert to previously seen behaviours; avoids permanent elimination of types due to zero likelihood events; improved planning accuracy in dynamic environments leading to higher task efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Choice of time-weight parameters (a,b,c) influences sensitivity and requires tuning; sum-based formulation loses some probabilistic interpretation of likelihoods (but improves adaptability).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — head-to-head comparisons in simulated experiments between Gtw (TR-posterior), Unl (product posterior), and Lim (limited product posterior) showing significant efficiency advantages for Gtw under dynamic type changes and partial-type accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>General time weight parameters used in experiments: a=10, b=0.01, c=3 (simulations); a=10, b=0.05, c=3 (human experiments). These were empirically effective in the paper's domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2561.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2561.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C-type</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Type (opponent-modelling type)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An opponent-modelling type that represents a hypothesised world-conceptualisation rather than explicit state-by-state behaviour: it generalizes observed actions to unseen states via a state-distance function, radius, and time-weight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conceptual Types (c-types) as HBA type hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A tuple (d_j, r, f) where d_j is a symmetric state-distance function, r a radius, and f a time-weight; action probabilities for unseen states are computed by kernel-weighted aggregation of observed same-action instances in similar past states (g(s1,s2)=max[0,1 - d_j(s1,s2)/r]). Conceptual types enable HBA to generalize behaviour across state abstractions and improve planning in rarely visited states.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>used per other agent (arbitrary N)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>N/A (modelling primitives used to represent other agents' behavioural conceptualisations)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>modelling & evaluation (improving opponent models used for planning)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Enables coordination by allowing HBA to predict other agents' actions in unseen or rarely-visited states via state-similarity generalization — thereby improving planned joint behaviour without requiring exhaustive experience.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not applicable (internal modelling structure); conceptual similarity weights are numerical features used in posterior updates.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Uses observed state-action pairs weighted by similarity and recency (time weight f) to produce predicted action distributions; these predictions feed back into HBA's planning.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Updated at every observation/time-step (continuous).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Tested in level-based foraging domain; evaluated four distinct distance functions (c-types 1–4) with different state similarity notions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When using a single c-type in Θ*_j, c-type 2 yielded good efficiency (close to 'Cor' in tests) in 2-player foraging; other c-types were less efficient. Reported flexibility ~0.86 ± 0.01 when single c-type used (per experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to HBA without c-types (user-defined explicit behaviour types) and to baselines JAL/CJAL/WoLF; c-type 2 improved performance in scenarios where explicit types were inaccurate for rarely visited states.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improves generalization of opponent behaviour to unseen states, increasing planning accuracy and efficiency particularly in sparse/large state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Effectiveness depends on choice of distance function and radius; some c-types did not improve efficiency relative to baseline types because learning agents (C/JAL) produced behaviours already captured by explicit heuristics H1/H3.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — separate experiments tested c-types 1–4 individually in Θ*_j; c-type 2 performed best in the tested configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Example settings used: time-weight f(ξ) = [ξ < 10]_1, radius r = 1; four distance functions defined in paper (d^1..d^4) with specific state features (player and food positions, relative distances).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2561.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2561.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Action Learner (JAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiagent reinforcement learning baseline that learns frequency estimates of other players' joint actions in each state and uses them to compute expected action payoffs for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The dynamics of reinforcement learning in cooperative multiagent systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Joint Action Learner (JAL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Opponent-modelling MAL algorithm that estimates action frequencies of each player conditioned on state (learned empirical distributions) and computes expected payoffs for available actions; implemented in the paper as a baseline both in foraging simulations and human-game experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (used as one agent interacting with 1..N opponents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Learner agent that models opponents via tabular frequency counts per state; no explicit specialization beyond being an RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution and evaluation (online learning of opponent action distributions and policy selection); does not include explicit high-level planning using model hypotheses like HBA.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized learning: coordinates implicitly by estimating opponents' empirical action distributions and responding greedily/myopically to those estimates; no explicit belief-based planning over hypothesised types.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Implicit via observation of state and opponent actions; internal representation is tabular frequency counts per state.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Uses observed actions and received rewards to update frequency estimates; updates are slow to reflect behavioural changes across many states because each state-action count must be re-estimated.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step (updates after each observation), but learning is local/state-specific so adaptation to changes is relatively slower.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as baseline in level-based foraging and in RPS human experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In RPS human experiment JAL winning rate = 43.98% (vs HBA 53.71%). In foraging tasks JAL was outperformed by HBA in efficiency and flexibility metrics (exact domain-dependent numbers shown in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared as baseline to HBA, CJAL, WoLF-PHC, and human players — generally underperformed HBA in efficiency/flexibility in foraging experiments and won less often in RPS.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Simple opponent modelling that can converge to NE in some zero-sum games (fictitious play guarantees in certain settings).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Slow to adapt to rapid behaviour changes (requires re-sampling of many states); poor generalization to unseen states; does not plan using hypothesised behavioural types — limits long-horizon strategic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Used as comparative baseline rather than ablation; relative performance measured against HBA variants showing where belief-based planning and TR-posteriors improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper used C/JAL parameters same as HBA where relevant; no new optimal configuration claimed for JAL in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2561.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2561.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CJAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Joint Action Learner (CJAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of JAL that conditions learned opponent action frequencies on the learner's own actions (i.e., it learns joint frequencies conditioned on its chosen action) to improve coordination and reach Pareto outcomes in some games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conditional Joint Action Learner (CJAL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Opponent-modelling multiagent learner that learns conditional action frequencies (opponent action distribution conditioned on the learner's actions) to compute expected payoffs and encourages coordination; used as a baseline in foraging simulations and as preferred baseline for PD in human experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Learner agent that models opponents conditional on its own actions — designed to encourage coordinated outcomes in repeated games.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution and evaluation (online conditioned opponent modelling, policy selection); used to benchmark social-welfare oriented behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized conditioned-frequency learning: implicitly coordinates by assuming a dependency between its own actions and opponents' actions and using that in planning; no explicit belief-over-types.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Implicit via observation; internal tabular conditional frequency representation per state/action pair.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Updates conditional frequency counts from observed joint actions and rewards; can be slower to adapt when many states exist but more directly encourages correlated strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step; conditions require sufficient data for each conditioning action.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as baseline in level-based foraging and as the PD baseline in human experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In human PD experiments CJAL had statistically equivalent total payoff to HBA but achieved 0 games where (C,C) occurred in ≥50% of final 10 rounds (HBA achieved >28% of games), so HBA obtained significantly higher welfare. In simulated foraging, CJAL underperformed HBA in efficiency/flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to HBA, JAL, WoLF-PHC, and humans. CJAL was competitive in PD but HBA achieved higher welfare through belief-based planning and quicker adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Designed to promote Pareto-improving coordinated outcomes in repeated games by conditioning opponent models on the learner's actions.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires lots of conditioned data; less ability to plan for unseen states; slower to react to rapid type changes across many states compared to HBA with TR-posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Used as baseline; compared against HBA variants demonstrating the value of belief-based planning and temporal reweighting for dynamic human behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper used CJAL parameterization consistent with referenced CJAL work; no new global optimal suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2561.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2561.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WoLF-PHC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Win or Learn Fast - Policy Hill-Climbing (WoLF-PHC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiagent learning algorithm that adapts policy learning rates depending on whether the agent is 'winning' (slower) or 'losing' (faster) to stabilize learning in multiagent settings while hill-climbing on policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multiagent learning using a variable learning rate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WoLF-PHC (multiagent learning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Model-free multiagent RL algorithm maintaining a mixed policy π and Q-values; it updates policy with different learning rates depending on performance (win vs learn fast) and does not explicitly model opponents; used as baseline in foraging experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Model-free learner agent maintaining policy and Q-values; does not perform explicit opponent modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution and evaluation (online policy learning and adaptation in multiagent interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Implicit: learning a mixed strategy without explicit modelling of other agents; coordination arises only through adaptation to experienced payoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>None beyond environmental observation; internal representations are Q-values and mixed policy distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Uses scalar reward signals to update Q-values and policy; no explicit feedback about other agents' types or intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step updates to policy/Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as a baseline in level-based foraging experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In foraging tests WoLF-PHC achieved lower efficiency/flexibility compared to HBA; in 10x10 foraging flexibility ~0.744 compared to HBA ~0.83 ± 0.01 in that experiment (paper reports these baseline numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against HBA, JAL, CJAL and humans; HBA outperformed WoLF-PHC on coordination and task-efficiency metrics in simulated logistics domain.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Simple adaptive policy updates that can converge in some self-play settings and are robust without opponent models.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Lacks explicit opponent modelling; poorer generalization to unseen states and poorer adaptation to heterogenous teammates compared to HBA's belief-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Used as baseline in experiments; relative underperformance highlights benefits of HBA components (type modelling + planning + TR-posteriors).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Used standard WoLF-PHC parameters described in Bowling & Veloso (2002) and in this paper's experiments (learning rates and delta_w/delta_l settings given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ad hoc autonomous agent teams: Collaboration without pre-coordination <em>(Rating: 2)</em></li>
                <li>A framework for sequential planning in multiagent settings <em>(Rating: 2)</em></li>
                <li>Multiagent learning using a variable learning rate <em>(Rating: 2)</em></li>
                <li>The dynamics of reinforcement learning in cooperative multiagent systems <em>(Rating: 2)</em></li>
                <li>Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning <em>(Rating: 2)</em></li>
                <li>A framework for sequential planning in multiagent settings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2561",
    "paper_id": "paper-6bbb82dde9c49f9d91c60e93d9ad0c051dca75d7",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "HBA",
            "name_full": "Harsanyi-Bellman Ad Hoc Coordination",
            "brief_description": "An ad hoc coordination agent that combines Bayesian type-based belief tracking with Bellman optimal planning: it maintains posteriors over user-defined behavioural types for other agents and plans actions that maximize expected long-term payoff under those beliefs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Harsanyi-Bellman Ad Hoc Coordination (HBA)",
            "system_description": "Decentralized ad hoc coordination agent that (1) accepts a set of user-defined type hypotheses for each other agent, (2) observes state-action histories and computes posteriors over those types, and (3) computes best-response actions by combining the Bayesian expected-type distribution with a Bellman-style long-term payoff estimate (discounted planning). Implementations in the paper include an exact planning variant (Algorithm 3) and a sampling/planning + RL variant integrated with a Q-table and eligibility traces (Algorithm 2). HBA can include opponent-modelling types (conceptual types) and temporally reweighted posteriors to handle changing behaviours.",
            "number_of_agents": "variable (experiments with 2, 3 players; general N in the model)",
            "agent_specializations": "Ad hoc agent (HBA) — plans and controls one player using type posteriors and Bellman planning; Teammates/opponents — represented as types in Θ*: (a) fixed-behaviour heuristic types H1–H4 (vision-based foraging behaviours), (b) learning agents (JAL, CJAL), (c) human participants in experiments. HBA treats each other agent role separately with its own type-space and posterior.",
            "research_phases_covered": "execution and evaluation (planning & decision-making in interactive multiagent tasks); includes online opponent modelling (hypothesis generation/use), adaptation (type inference), and policy planning; does not cover literature review or code implementation phases of scientific research.",
            "coordination_mechanism": "Decentralized, belief-based best-response coordination: each HBA agent independently infers other agents' types from observed actions and then selects actions that maximize expected discounted long-term reward under those inferred type distributions; no centralized controller or explicit pre-coordination required. Coordination emerges by planning under the same model of others' behaviours.",
            "communication_protocol": "No explicit message-passing protocol; coordination is achieved via observation of shared environment state and agents' actions (implicit communication through behaviour). Internally messages are structured probabilistically (posteriors over discrete user-defined types) but there is no explicit inter-agent messaging format.",
            "feedback_mechanism": "Agents receive scalar rewards from environment; HBA uses (a) likelihood-based posterior updates over types from observed actions (product posterior or temporally reweighted posterior), (b) Q-style long-term payoff updates in RL implementation with eligibility traces, and (c) planning simulations using user-defined types to compute expected returns. Posteriors act as feedback about others' behaviours and are used to adapt planning.",
            "communication_frequency": "Continuous / per time-step: HBA updates posteriors and chooses actions every time step based on the latest observed history; planning uses projected trajectories frequently (per decision) with simulated opponent actions sampled from current posterior.",
            "task_domain": "Multiagent logistics coordination (level-based foraging) and human-machine interactive games (Prisoner's Dilemma, Rock-Paper-Scissors) — general model applicable to interactive multiagent tasks (ad hoc coordination).",
            "performance_metrics": "Flexibility (probability of task termination/solving) and Efficiency (average payoff per time step in solved tasks). Reported numeric results: HBA variants achieved perfect flexibility = 1.0 in some 2-player foraging tests; in a 10x10, 3-player, 8-food foraging test HBA reported flexibility 0.83 ± 0.01. Human-machine experiments: PD — HBA and CJAL had statistically equivalent total payoffs; HBA achieved (C,C) in ≥50% of final 10 rounds in &gt;28% of games (CJAL: 0%). RPS — HBA winning rate 53.71% vs JAL 43.98%; HBA inferred human behaviour statistics: humans used 4.45 types (PD) with mean duration 4.96 rounds and 8.25 types (RPS) with mean duration 2.46 rounds. In simulated comparisons HBA variants 'Unl' and 'Gtw' were reported to be over 100% and 200% more efficient than alternative algorithms in a larger foraging test (textual claim in paper).",
            "baseline_comparison": "Compared to JAL, CJAL, and WoLF-PHC as algorithmic baselines and to human players: HBA outperformed JAL/CJAL/WoLF-PHC in the level-based foraging domain (higher efficiency and flexibility) and achieved statistically equal or better performance in human-machine PD and RPS experiments (equal total payoff in aggregate, higher welfare in PD, higher winning rate in RPS). Humans sometimes outperformed HBA when humans had greater planning power.",
            "coordination_benefits": "Belief-driven planning enabled better coordination and manipulation of other agents' decisions: (a) higher efficiency (faster task completion / higher per-step payoff) and higher welfare in PD due to HBA's ability to plan effects of its actions on others; (b) faster recognition of behavioural changes (via TR-posteriors) improving dynamic adaptation and win-rate in RPS (53.71% vs 43.98%). Quantitative examples: flexibility improvements to 1.0 in some tests; flexibility 0.83 ± 0.01 vs baselines ~0.734–0.749 in larger test; &gt;100% / &gt;200% efficiency gains claimed for some HBA variants vs alternatives in large simulated test.",
            "coordination_challenges": "Requires good user-defined type spaces Θ* — performance depends on how well Θ* approximates true types; product-posteriors can permanently zero-out types and fail under dynamic type changes; planning complexity and required planning power can limit performance (humans with larger planning horizon outperformed HBA in one test); specifying types manually is burdensome in complex domains.",
            "ablation_studies": "Yes — comparisons (ablation-like) across posterior formulations and type models: (1) product (normal) posterior (Unl), limited-window posterior (Lim), and temporally reweighted posterior (Gtw) — Gtw significantly improved efficiency and matched 'correct types' agent in dynamic-change experiments; (2) conceptual-type variants (c-types 1–4) tested separately — c-type 2 produced good efficiency while others were less effective; (3) 'Cor' variant (HBA with correct types known) used as upper bound. These comparisons quantify the impact of posterior formulation and type modelling on coordination performance.",
            "optimal_configurations": "Paper recommends temporally reweighted posteriors (general time weight) for dynamic environments and inclusion of conceptual types when types are only approximately known; example parameter settings used in experiments: general time weight f(ξ) parameters a=10, b=0.01 (or b=0.05 in human experiments), c=3; planning/expansion parameters: sampling expansion depth d=20 (or 30 in larger tests) and x expansions per real step (x=3 or x=10); Q-learning / eligibility trace parameters β=0.2, γ=0.9, λ=0.9. No formal proof of universally optimal configuration; the paper lists these as empirically effective settings and highlights open questions about type-space design and posterior choice.",
            "uuid": "e2561.0",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "TR-posterior",
            "name_full": "Temporally Reweighted Posterior",
            "brief_description": "A posterior update rule that replaces the product of likelihoods over history with a weighted sum of action-likelihoods using a monotonically nonincreasing time-weight f(·), enabling rapid re-assignment of probability to previously-zeroed types and better adaptation to changing behaviours.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Temporally Reweighted Posterior (component of HBA)",
            "system_description": "A type-inference mechanism: L(H^t | θ_j) = Σ_{τ=0}^{t-1} f(t−τ) π_j(H^τ, a_j^τ, θ_j) with time weight f(ξ) ≥ 0 and nonincreasing in ξ. The TR-posterior is plugged into HBA's Bayesian update to compute Pr(θ_j | H^t). Implemented with 'general time weight' f(ξ)=max[0, a−b(ξ−1)^c] in experiments.",
            "number_of_agents": "component used for each other agent; supports arbitrary N",
            "agent_specializations": "N/A (inference component used by the ad hoc agent for modelling all other agents' types)",
            "research_phases_covered": "online adaptation / evaluation (type inference & model update)",
            "coordination_mechanism": "Improves decentralized coordination by allowing the ad hoc planner to re-evaluate and quickly reassign probability mass to types based on recent observations — thereby enabling coordinated responses to behaviour changes.",
            "communication_protocol": "Not applicable (inference mechanism operating on observed actions); uses numerical time-weighted aggregation of observed action likelihoods.",
            "feedback_mechanism": "Uses recent observed actions (with recency-weighted influence) as feedback to reweight type probabilities; these posteriors feed into planning and thereby produce behavioral adaptation.",
            "communication_frequency": "Updated every time-step (continuous per observation).",
            "task_domain": "Used within HBA across foraging and human-game tasks in the paper.",
            "performance_metrics": "Improved efficiency in dynamic/mismatched-type tests: Gtw (TR-posterior with general time weight) significantly outperformed product posterior variants; in a 2-player foraging test with dynamic type switching, Gtw matched performance of 'Cor' (HBA with correct types); in larger foraging tests TR-posterior variant showed up to ~200% efficiency improvement in text-reported comparisons (see paper).",
            "baseline_comparison": "Compared to (a) product (multiplicative) posterior (Unl), and (b) limited-window normal posterior (Lim); Gtw outperformed both in experiments with dynamic type changes and partial-type accuracy.",
            "coordination_benefits": "Faster recovery when agents revert to previously seen behaviours; avoids permanent elimination of types due to zero likelihood events; improved planning accuracy in dynamic environments leading to higher task efficiency.",
            "coordination_challenges": "Choice of time-weight parameters (a,b,c) influences sensitivity and requires tuning; sum-based formulation loses some probabilistic interpretation of likelihoods (but improves adaptability).",
            "ablation_studies": "Yes — head-to-head comparisons in simulated experiments between Gtw (TR-posterior), Unl (product posterior), and Lim (limited product posterior) showing significant efficiency advantages for Gtw under dynamic type changes and partial-type accuracy.",
            "optimal_configurations": "General time weight parameters used in experiments: a=10, b=0.01, c=3 (simulations); a=10, b=0.05, c=3 (human experiments). These were empirically effective in the paper's domains.",
            "uuid": "e2561.1",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "C-type",
            "name_full": "Conceptual Type (opponent-modelling type)",
            "brief_description": "An opponent-modelling type that represents a hypothesised world-conceptualisation rather than explicit state-by-state behaviour: it generalizes observed actions to unseen states via a state-distance function, radius, and time-weight.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Conceptual Types (c-types) as HBA type hypotheses",
            "system_description": "A tuple (d_j, r, f) where d_j is a symmetric state-distance function, r a radius, and f a time-weight; action probabilities for unseen states are computed by kernel-weighted aggregation of observed same-action instances in similar past states (g(s1,s2)=max[0,1 - d_j(s1,s2)/r]). Conceptual types enable HBA to generalize behaviour across state abstractions and improve planning in rarely visited states.",
            "number_of_agents": "used per other agent (arbitrary N)",
            "agent_specializations": "N/A (modelling primitives used to represent other agents' behavioural conceptualisations)",
            "research_phases_covered": "modelling & evaluation (improving opponent models used for planning)",
            "coordination_mechanism": "Enables coordination by allowing HBA to predict other agents' actions in unseen or rarely-visited states via state-similarity generalization — thereby improving planned joint behaviour without requiring exhaustive experience.",
            "communication_protocol": "Not applicable (internal modelling structure); conceptual similarity weights are numerical features used in posterior updates.",
            "feedback_mechanism": "Uses observed state-action pairs weighted by similarity and recency (time weight f) to produce predicted action distributions; these predictions feed back into HBA's planning.",
            "communication_frequency": "Updated at every observation/time-step (continuous).",
            "task_domain": "Tested in level-based foraging domain; evaluated four distinct distance functions (c-types 1–4) with different state similarity notions.",
            "performance_metrics": "When using a single c-type in Θ*_j, c-type 2 yielded good efficiency (close to 'Cor' in tests) in 2-player foraging; other c-types were less efficient. Reported flexibility ~0.86 ± 0.01 when single c-type used (per experiment).",
            "baseline_comparison": "Compared to HBA without c-types (user-defined explicit behaviour types) and to baselines JAL/CJAL/WoLF; c-type 2 improved performance in scenarios where explicit types were inaccurate for rarely visited states.",
            "coordination_benefits": "Improves generalization of opponent behaviour to unseen states, increasing planning accuracy and efficiency particularly in sparse/large state spaces.",
            "coordination_challenges": "Effectiveness depends on choice of distance function and radius; some c-types did not improve efficiency relative to baseline types because learning agents (C/JAL) produced behaviours already captured by explicit heuristics H1/H3.",
            "ablation_studies": "Yes — separate experiments tested c-types 1–4 individually in Θ*_j; c-type 2 performed best in the tested configuration.",
            "optimal_configurations": "Example settings used: time-weight f(ξ) = [ξ &lt; 10]_1, radius r = 1; four distance functions defined in paper (d^1..d^4) with specific state features (player and food positions, relative distances).",
            "uuid": "e2561.2",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "JAL",
            "name_full": "Joint Action Learner (JAL)",
            "brief_description": "A multiagent reinforcement learning baseline that learns frequency estimates of other players' joint actions in each state and uses them to compute expected action payoffs for planning.",
            "citation_title": "The dynamics of reinforcement learning in cooperative multiagent systems",
            "mention_or_use": "use",
            "system_name": "Joint Action Learner (JAL)",
            "system_description": "Opponent-modelling MAL algorithm that estimates action frequencies of each player conditioned on state (learned empirical distributions) and computes expected payoffs for available actions; implemented in the paper as a baseline both in foraging simulations and human-game experiments.",
            "number_of_agents": "variable (used as one agent interacting with 1..N opponents)",
            "agent_specializations": "Learner agent that models opponents via tabular frequency counts per state; no explicit specialization beyond being an RL agent.",
            "research_phases_covered": "execution and evaluation (online learning of opponent action distributions and policy selection); does not include explicit high-level planning using model hypotheses like HBA.",
            "coordination_mechanism": "Decentralized learning: coordinates implicitly by estimating opponents' empirical action distributions and responding greedily/myopically to those estimates; no explicit belief-based planning over hypothesised types.",
            "communication_protocol": "Implicit via observation of state and opponent actions; internal representation is tabular frequency counts per state.",
            "feedback_mechanism": "Uses observed actions and received rewards to update frequency estimates; updates are slow to reflect behavioural changes across many states because each state-action count must be re-estimated.",
            "communication_frequency": "Per time-step (updates after each observation), but learning is local/state-specific so adaptation to changes is relatively slower.",
            "task_domain": "Used as baseline in level-based foraging and in RPS human experiments.",
            "performance_metrics": "In RPS human experiment JAL winning rate = 43.98% (vs HBA 53.71%). In foraging tasks JAL was outperformed by HBA in efficiency and flexibility metrics (exact domain-dependent numbers shown in figures).",
            "baseline_comparison": "Compared as baseline to HBA, CJAL, WoLF-PHC, and human players — generally underperformed HBA in efficiency/flexibility in foraging experiments and won less often in RPS.",
            "coordination_benefits": "Simple opponent modelling that can converge to NE in some zero-sum games (fictitious play guarantees in certain settings).",
            "coordination_challenges": "Slow to adapt to rapid behaviour changes (requires re-sampling of many states); poor generalization to unseen states; does not plan using hypothesised behavioural types — limits long-horizon strategic manipulation.",
            "ablation_studies": "Used as comparative baseline rather than ablation; relative performance measured against HBA variants showing where belief-based planning and TR-posteriors improve results.",
            "optimal_configurations": "Paper used C/JAL parameters same as HBA where relevant; no new optimal configuration claimed for JAL in this work.",
            "uuid": "e2561.3",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "CJAL",
            "name_full": "Conditional Joint Action Learner (CJAL)",
            "brief_description": "A variant of JAL that conditions learned opponent action frequencies on the learner's own actions (i.e., it learns joint frequencies conditioned on its chosen action) to improve coordination and reach Pareto outcomes in some games.",
            "citation_title": "Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning",
            "mention_or_use": "use",
            "system_name": "Conditional Joint Action Learner (CJAL)",
            "system_description": "Opponent-modelling multiagent learner that learns conditional action frequencies (opponent action distribution conditioned on the learner's actions) to compute expected payoffs and encourages coordination; used as a baseline in foraging simulations and as preferred baseline for PD in human experiments.",
            "number_of_agents": "variable",
            "agent_specializations": "Learner agent that models opponents conditional on its own actions — designed to encourage coordinated outcomes in repeated games.",
            "research_phases_covered": "execution and evaluation (online conditioned opponent modelling, policy selection); used to benchmark social-welfare oriented behaviour.",
            "coordination_mechanism": "Decentralized conditioned-frequency learning: implicitly coordinates by assuming a dependency between its own actions and opponents' actions and using that in planning; no explicit belief-over-types.",
            "communication_protocol": "Implicit via observation; internal tabular conditional frequency representation per state/action pair.",
            "feedback_mechanism": "Updates conditional frequency counts from observed joint actions and rewards; can be slower to adapt when many states exist but more directly encourages correlated strategies.",
            "communication_frequency": "Per time-step; conditions require sufficient data for each conditioning action.",
            "task_domain": "Used as baseline in level-based foraging and as the PD baseline in human experiments.",
            "performance_metrics": "In human PD experiments CJAL had statistically equivalent total payoff to HBA but achieved 0 games where (C,C) occurred in ≥50% of final 10 rounds (HBA achieved &gt;28% of games), so HBA obtained significantly higher welfare. In simulated foraging, CJAL underperformed HBA in efficiency/flexibility.",
            "baseline_comparison": "Compared to HBA, JAL, WoLF-PHC, and humans. CJAL was competitive in PD but HBA achieved higher welfare through belief-based planning and quicker adaptation.",
            "coordination_benefits": "Designed to promote Pareto-improving coordinated outcomes in repeated games by conditioning opponent models on the learner's actions.",
            "coordination_challenges": "Requires lots of conditioned data; less ability to plan for unseen states; slower to react to rapid type changes across many states compared to HBA with TR-posteriors.",
            "ablation_studies": "Used as baseline; compared against HBA variants demonstrating the value of belief-based planning and temporal reweighting for dynamic human behaviour.",
            "optimal_configurations": "Paper used CJAL parameterization consistent with referenced CJAL work; no new global optimal suggested.",
            "uuid": "e2561.4",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "WoLF-PHC",
            "name_full": "Win or Learn Fast - Policy Hill-Climbing (WoLF-PHC)",
            "brief_description": "A multiagent learning algorithm that adapts policy learning rates depending on whether the agent is 'winning' (slower) or 'losing' (faster) to stabilize learning in multiagent settings while hill-climbing on policy.",
            "citation_title": "Multiagent learning using a variable learning rate",
            "mention_or_use": "use",
            "system_name": "WoLF-PHC (multiagent learning baseline)",
            "system_description": "Model-free multiagent RL algorithm maintaining a mixed policy π and Q-values; it updates policy with different learning rates depending on performance (win vs learn fast) and does not explicitly model opponents; used as baseline in foraging experiments.",
            "number_of_agents": "variable",
            "agent_specializations": "Model-free learner agent maintaining policy and Q-values; does not perform explicit opponent modelling.",
            "research_phases_covered": "execution and evaluation (online policy learning and adaptation in multiagent interactions).",
            "coordination_mechanism": "Implicit: learning a mixed strategy without explicit modelling of other agents; coordination arises only through adaptation to experienced payoffs.",
            "communication_protocol": "None beyond environmental observation; internal representations are Q-values and mixed policy distributions.",
            "feedback_mechanism": "Uses scalar reward signals to update Q-values and policy; no explicit feedback about other agents' types or intentions.",
            "communication_frequency": "Per time-step updates to policy/Q-values.",
            "task_domain": "Used as a baseline in level-based foraging experiments.",
            "performance_metrics": "In foraging tests WoLF-PHC achieved lower efficiency/flexibility compared to HBA; in 10x10 foraging flexibility ~0.744 compared to HBA ~0.83 ± 0.01 in that experiment (paper reports these baseline numbers).",
            "baseline_comparison": "Compared against HBA, JAL, CJAL and humans; HBA outperformed WoLF-PHC on coordination and task-efficiency metrics in simulated logistics domain.",
            "coordination_benefits": "Simple adaptive policy updates that can converge in some self-play settings and are robust without opponent models.",
            "coordination_challenges": "Lacks explicit opponent modelling; poorer generalization to unseen states and poorer adaptation to heterogenous teammates compared to HBA's belief-based planning.",
            "ablation_studies": "Used as baseline in experiments; relative underperformance highlights benefits of HBA components (type modelling + planning + TR-posteriors).",
            "optimal_configurations": "Used standard WoLF-PHC parameters described in Bowling & Veloso (2002) and in this paper's experiments (learning rates and delta_w/delta_l settings given in text).",
            "uuid": "e2561.5",
            "source_info": {
                "paper_title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
                "publication_date_yy_mm": "2013-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ad hoc autonomous agent teams: Collaboration without pre-coordination",
            "rating": 2
        },
        {
            "paper_title": "A framework for sequential planning in multiagent settings",
            "rating": 2
        },
        {
            "paper_title": "Multiagent learning using a variable learning rate",
            "rating": 2
        },
        {
            "paper_title": "The dynamics of reinforcement learning in cooperative multiagent systems",
            "rating": 2
        },
        {
            "paper_title": "Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning",
            "rating": 2
        },
        {
            "paper_title": "A framework for sequential planning in multiagent settings",
            "rating": 1
        }
    ],
    "cost": 0.0192605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems</h1>
<p>Stefano V. Albrecht<br>School of Informatics<br>University of Edinburgh<br>Edinburgh EH8 9AB, UK<br>s.v.albrecht@sms.ed.ac.uk</p>
<h2>Abstract</h2>
<p>The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.</p>
<h2>1 Introduction</h2>
<p>We are concerned with the ad hoc coordination problem, in which the goal is to design an autonomous agent, called the ad hoc agent, which is able to achieve optimal flexibility and efficiency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. Flexibility describes the ad hoc agent's ability to solve its task with a variety of other agents in the system. Efficiency is the relation between the ad hoc agent's payoffs and time needed to solve the task. No prior coordination means that the ad hoc agent</p>
<h2>Subramanian Ramamoorthy</h2>
<p>School of Informatics
University of Edinburgh
Edinburgh EH8 9AB, UK
s.ramamoorthy@ed.ac.uk
does not know ahead of time who the other agents are and how they behave. In particular, there are no prior agreements on information sharing, communication and action protocols, standards, etc.</p>
<p>This problem is motivated by the fact that there is a growing number of agents, both robotic and virtual, which are employed in an increasing number of areas. Given that a primary goal in agents research is to increase the autonomy and thus lifetime of agents, it can be expected that agents based on different technologies may have to interact in nontrivial ways, without knowing a priori who the other agents are. This motivates both the notion of flexibility, since the other agents could be based on any kind of technology, and efficiency, since there may be no time for long learning periods, especially if interactions are sparse. Human-machine interaction problems (e.g. robots used in rescue scenarios or software agents used in trading markets) can be viewed as a special case of ad hoc coordination, since humans have extremely variable behaviour (flexibility) and expect agents to be able to interact quickly (efficiency), while there may be no prior description of the human's behaviour (no prior coordination).</p>
<p>There have been several attempts to address ad hoc coordination in multiagent systems, e.g. [Bowling and McCracken, 2005, Dias et al., 2006, Stone et al., 2010a]. While all of these works are relevant to ad hoc coordination, the assumptions made by the solutions therein imply that they only address certain aspects of the larger problem. For example, in [Bowling and McCracken, 2005, Dias et al., 2006] it is assumed that all agents follow pre-specified plans which include roles and synchronised action sequences for each role, and in [Stone and Kraus, 2010, Stone et al., 2010b, Barrett et al., 2011, Agmon and Stone, 2012] it is assumed that the other agents' behaviours are fixed and known, and that all agents have common payoffs. We also note that the problem descriptions in these works are of a procedural nature, associated with the specific tasks considered therein. Therefore, there is a need for a formal model</p>
<p>of the ad hoc coordination problem, general enough to accommodate a wide spectrum of problems.</p>
<p>A related problem is known in game theory as the incomplete information game. Therein, each player has some private information relevant to its decision making of which the other players are not aware, which is what relates the incomplete information game to the ad hoc coordination problem. [Harsanyi, 1967] introduced Bayesian games in which the private information of a player is abstractly represented by its type, admitting a solution in the form of the Bayesian Nash equilibrium. Since then, there have been several works on learning in Bayesian games, e.g. [Jordan, 1991, Kalai and Lehrer, 1993, Dekel et al., 2004]. While the notion of private information is useful to describe the ad hoc coordination problem, the learning processes and solutions studied therein are not directly applicable, since the focus has traditionally been on equilibrium considerations but not on efficiency. On the other hand, much work in multiagent systems has focused on efficiency, whilst often making central assumptions about the other agent's behaviours [Albrecht and Ramamoorthy, 2012]. Therefore, it is natural to ask if these fields can be combined to address ad hoc coordination in a useful way.</p>
<p>Inspired by this question, we model the problem using a game-theoretic construct called the stochastic Bayesian game, in which a player's behaviour is determined by its type. Based on this model, we give formal definitions of flexibility and efficiency, and we define ad hoc coordination as the problem of optimising flexibility and efficiency, subject to the constraint that the ad hoc agent is unaware of the players' type spaces, and hence the rules by which their types are assigned. Our model allows for both the definition of Bayesian Nash equilibrium and, since it satisfies the Markov property, the definition of Bellman optimal control [Bellman, 1957], a key result in intelligent agents. We combine these two concepts to obtain a solution which we call HarsanyiBellman Ad Hoc Coordination (HBA). HBA does not rely on a central assumption about the other agents' behaviours. Instead, it allows for the specification of multiple such assumptions which are provided to HBA as a set of user-defined types, each corresponding to a different hypothesis of how an agent might behave. Based on the agents' observed actions, HBA computes probability distributions over the user-defined types, called posteriors, and utilises them in a planning procedure to find optimal actions.</p>
<p>HBA has a number of useful features with respect to ad hoc coordination. The fact that the user-defined types may encapsulate any kind of behaviour means that HBA can potentially deal with a variety of different agents, including agents which maintain beliefs about the behaviour of the HBA agent, or any other type of recursive reasoning. We show this in a human-machine experiment conducted at a public science exhibition, in which HBA was able to manipulate the beliefs of humans in repeated Prisoner's Dilemma such that both ended up cooperating, thus maximising its efficiency. HBA also supports the possibility that agents may switch between different behaviours. We address this by introducing temporally reweighted posteriors which allow HBA to quickly recognise changed types. In our human-machine experiment, this allowed HBA to achieve a significantly higher winning rate in Rock-Paper-Scissors than the human participants and an alternative algorithm.</p>
<p>A central feature of HBA is that it can use the types to plan in the entire state space of the problem (including unseen states) provided that the posteriors and user-defined types are reasonably accurate. To accommodate the case in which none of the user-defined types accurately describe an agent's behaviour, HBA is able to include methods for opponent modelling. We propose an opponent modelling method, called conceptual type, which can be viewed as a kind of type that specifies the conceptualisation underlying a behaviour, rather than specifying the behaviour directly. The conceptualisation is combined with the observed actions of an agent to generalise its actions to unseen states and improve accuracy in rarely visited states. We demonstrate these features in a multiagent logistics domain called level-based foraging, in which HBA is able to achieve significantly higher flexibility and efficiency than three alternative algorithms (JAL [Claus and Boutilier, 1998], CJAL [Banerjee and Sen, 2007], WoLF-PHC [Bowling and Veloso, 2002]), using just a few user-defined types.</p>
<h2>2 Defining Ad Hoc Coordination</h2>
<h3>2.1 Stochastic Bayesian Games</h3>
<p>As discussed earlier, ad hoc coordination can be defined based on the notion of private information in Bayesian games. However, in their original form [Harsanyi, 1967], Bayesian games are not descriptive enough to allow us to model the kinds of problems we are interested in, as they do neither include states nor time. Therefore, we combine Bayesian games with the concept of stochastic games [Shapley, 1953] to obtain a more descriptive model which we call stochastic Bayesian game: ${ }^{1}$
Definition 1. A stochastic Bayesian game (SBG) consists of:</p>
<ul>
<li>discrete state space $S$ with initial state $s^{0} \in S$ and terminal states $\bar{S} \subset S$</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>players $N={1, \ldots, n}$ and for each $i \in N$ :</li>
<li>set of actions $A_{i}$ (where $A=A_{1} \times \ldots \times A_{n}$ )</li>
<li>type space $\Theta_{i}$ (where $\Theta=\Theta_{1} \times \ldots \times \Theta_{n}$ )</li>
<li>payoff function $u_{i}: S \times A \times \Theta_{i} \rightarrow \mathbb{R}$</li>
<li>strategy $\pi_{i}: \mathbb{H} \times A_{i} \times \Theta_{i} \rightarrow[0,1]$</li>
<li>state transition function $T: S \times A \times S \rightarrow[0,1]$</li>
<li>type distribution $\Delta: \mathbb{N}_{0} \times \Theta \rightarrow[0,1]$</li>
</ul>
<p>$\mathbb{H}$ contains all histories $H^{t}=\left\langle s^{0}, a^{0}, s^{1}, a^{1}, \ldots, s^{t}\right\rangle$ with $t \geq 0,\left(s^{\tau}, a^{\tau}\right) \in S \times A$ for $0 \leq \tau&lt;t$, and $s^{t} \in S$.</p>
<p>We also define several classes of type distributions:
Definition 2. A type distribution $\Delta$ is called static if $\forall t, \hat{t} \forall \theta \in \Theta: \Delta(t, \theta)=\Delta(\hat{t}, \theta)$, else it is called dynamic.
Definition 3. A type distribution $\Delta$ is called pure if $\forall t \exists \theta \in \Theta: \Delta(t, \theta)=1$, else it is called mixed.</p>
<p>A SBG starts at time $t=0$ in state $s^{0}$. In state $s^{t}$, the types $\theta_{1}^{t}, \ldots, \theta_{n}^{t}$ are sampled from $\Theta$ with probability $\Delta\left(t,\left(\theta_{1}^{t}, \ldots, \theta_{n}^{t}\right)\right)$, and each player $i \in N$ is only informed about its own type $\theta_{i}^{t}$. Based on the history $H^{t}$, each player $i$ chooses an action $a_{i}^{t} \in A_{i}$ with probability $\pi_{i}\left(H^{t}, a_{i}^{t}, \theta_{i}^{t}\right)$. Given the joint action $a^{t}=\left(a_{1}^{t}, \ldots, a_{n}^{t}\right)$, the game transitions into a successor state $s^{t+1} \in S$ with probability $T\left(s^{t}, a^{t}, s^{t+1}\right)$ and every player $i$ receives an individual payoff given by $u_{i}\left(s^{t}, a^{t}, \theta_{i}^{t}\right)$. This process is repeated until the game reaches a terminal state $s^{t} \in \bar{S}$, after which the game stops.</p>
<p>Our definition of types follows the original definition of [Harsanyi, 1967], which means that a type determines a player's payoffs and strategies. However, since we define strategies with respect to a history of states and actions (rather than just the current state), a type may in fact specify strategies which change over time (such as players who learn or use recursive reasoning), and we thus also refer to it as behaviour. Therefore, our interpretation of types is that of a "programme" which governs the behaviour of a player.
Each player may correspond to a specific role in the game. For instance, if we model a soccer team, player 1 may correspond to the goal keeper. Therefore, in the following sections, we implicitly assume that the ad hoc agent, denoted $\alpha$, controls the player of interest, denoted $i$, by which we mean that $\alpha$ chooses the strategy $\pi_{i}$. Furthermore, $i$ has a fixed type which is known to $\alpha$, and we denote its payoffs by $u_{i}\left(s^{t}, a^{t}, \alpha\right)$.</p>
<h3>2.2 Flexibility \&amp; Efficiency</h3>
<p>Two important aspects of ad hoc coordination are flexibility and efficiency. We now define each of them formally within the SBG model. The definitions rely on the notion of paths and probabilities of paths:</p>
<p>Definition 4. A path $\rho$ in SBG $\Gamma$ is a sequence $\left\langle s_{\rho}^{0}, \theta_{\rho}^{0}, a_{\rho}^{0}, s_{\rho}^{1}, \theta_{\rho}^{1}, a_{\rho}^{1}, \ldots, s_{\rho}^{t_{\rho}}\right\rangle$ where $s_{\rho}^{\tau} \in S, \theta_{\rho}^{\tau} \in \Theta$, $a_{\rho}^{\tau} \in A$, and $s_{\rho}^{0}=s^{0}$. A path $\rho$ is terminating if $s_{\rho}^{t_{\rho}} \in \bar{S}$, otherwise it is non-terminating. Given a type distribution $\Delta$ for $\Gamma$, the probability of path $\rho$ is defined as $\operatorname{Pr}(\rho \mid \Gamma, \Delta)=$</p>
<p>$$
\prod_{\tau=0}^{t_{\rho}-1} \Delta\left(\tau, \theta_{\rho}^{\tau}\right) T\left(s_{\rho}^{\tau}, a_{\rho}^{\tau}, s_{\rho}^{\tau+1}\right) \prod_{k \in N} \pi_{k}\left(H_{\rho}^{\tau},\left(a_{\rho}^{\tau}\right)<em _rho="\rho">{k},\left(\theta</em>\right)
$$}^{\tau}\right)_{k</p>
<p>where $H_{\rho}^{\tau}$ is the history extracted from $\rho$ until time $\tau$.
For $\operatorname{Pr}(\rho \mid \Gamma, \Delta)$ to be well-defined (i.e. there is a set $X$ with $\forall \rho \in X: \operatorname{Pr}(\rho \mid \Gamma, \Delta) \geq 0$ and $\sum_{\rho \in X} \operatorname{Pr}(\rho \mid \Gamma, \Delta)=1$ ), it is important to note the following two implications in the definition of SBGs. Firstly, no path $\rho$ can be prefixed by a terminating path, i.e., there is no $s_{\rho}^{\tau} \in \rho$ such that $\tau&lt;t_{\rho}$ and $s_{\rho}^{\tau} \in \bar{S}$. This is important since otherwise $\operatorname{Pr}(\rho \mid \Gamma, \Delta)$ might assign positive probability to a path which is prefixed by a terminating path and, thus, could never occur. Secondly, the only paths that can occur are either terminating (and hence finite) or non-terminating and infinite (i.e. $t \rightarrow \infty$ ). Thus, if $\Phi$ is the set of all terminating paths and $\Psi$ the set of all infinite nonterminating paths, then $\sum_{\rho \in \Phi \cup \Psi} \operatorname{Pr}(\rho \mid \Gamma, \Delta)=1$.
Based on the notion of paths, we define the flexibility and efficiency of ad hoc agent $\alpha$ as follows:
Definition 5. Let $\Phi$ be the set of all terminating paths in SBG $\Gamma$. Given a set of type distributions $\mathbb{D}$ for $\Gamma$, the flexibility $F(\alpha \mid \Gamma, \mathbb{D})$ and efficiency $E(\alpha \mid \Gamma, \mathbb{D})$ of $\alpha$ in $\Gamma$ with respect to $\mathbb{D}$ are defined as</p>
<p>$$
\begin{aligned}
&amp; F(\alpha \mid \Gamma, \mathbb{D})=\frac{1}{|\mathbb{D}|} \sum_{\Delta \in \mathbb{D}} \sum_{\rho \in \Phi} \operatorname{Pr}(\rho \mid \Gamma, \Delta) \
&amp; E(\alpha \mid \Gamma, \mathbb{D})=\frac{1}{|\mathbb{D}|} \sum_{\Delta \in \mathbb{D}} \sum_{\rho \in \Phi} \overline{\operatorname{Pr}}(\rho \mid \Gamma, \Delta) \frac{\left(\sum_{\tau=0}^{t_{\rho}-1} u_{i}\left(s_{\rho}^{\tau}, a_{\rho}^{\tau}, \alpha\right)\right)^{r_{1}}}{\left(t_{\rho}\right)^{r_{2}}}
\end{aligned}
$$</p>
<p>where $\overline{\operatorname{Pr}}(\rho \mid \Gamma, \Delta)=\frac{\operatorname{Pr}(\rho \mid \Gamma, \Delta)}{\sum_{\rho \in \Phi} \overline{\operatorname{Pr}(\rho \mid \Gamma, \Delta)}}$, and $r_{1}, r_{2} \geq 1$ specify the relative importance between payoff and time.
$F(\alpha \mid \Gamma, \mathbb{D})$ and $E(\alpha \mid \Gamma, \mathbb{D})$ can be interpreted as, respectively, the average probability that $\alpha$ solves a task in $\Gamma$ and the average payoff per time step $\alpha$ received in solved tasks, where $\mathbb{D}$ specifies all constellations of types that can occur. There may be problems in which flexibility is not a relevant metric because termination is guaranteed for some reason. In such cases, the primary metric is efficiency.</p>
<h3>2.3 The Ad Hoc Coordination Problem</h3>
<p>We are now in a position to formally define the ad hoc coordination problem. The core aspect is that there is no prior coordination between the ad hoc agent and the</p>
<p>Algorithm 1 Evaluation procedure
Input: SBG $\Gamma$, set of type distributions $\mathbb{D}$, ad hoc agent $\alpha$, player $i$ (to be controlled by $\alpha$ )
Output: flexibility $F(\alpha \mid \Gamma, \mathbb{D})$, efficiency $E(\alpha \mid \Gamma, \mathbb{D})$
$F \leftarrow 0$
$E \leftarrow 0$
Repeat $K$ times:
Randomly draw type distribution $\Delta \in \mathbb{D}$
Generate path $\rho$ in $\Gamma$ with $\Delta$ ( $\alpha$ controls $i$ )
If $\rho$ terminates do
$F \leftarrow F+1$
$E \leftarrow E+\left(\sum_{\tau=0}^{t_{\rho}-1} u_{i}\left(s_{\rho}^{\tau}, a_{\rho}^{\tau}, \alpha\right)\right)^{r_{1}} *\left(t_{\rho}\right)^{-r_{2}}$
$F(\alpha \mid \Gamma, \mathbb{D}) \leftarrow F / K$
$E(\alpha \mid \Gamma, \mathbb{D}) \leftarrow E / K$
other agents in the system. We express this formally by requiring that the ad hoc agent does not know the type spaces $\Theta_{j}$ of the other players and, therefore, the type distribution $\Delta$ of the game.</p>
<p>Definition 6. Let $\Gamma$ be a SBG with type spaces $\Theta_{j}$, and let $\mathbb{D}$ be a set of type distributions for $\Gamma$. The ad hoc coordination problem is to optimise the flexibility $F(\alpha \mid \Gamma, \mathbb{D})$ and efficiency $E(\alpha \mid \Gamma, \mathbb{D})$ of ad hoc agent $\alpha$, subject to the constraint that $\alpha$ does not know $\Theta_{j}$ (and, therefore, the type distributions $\Delta$ ).</p>
<p>Computing $F(\alpha \mid \Gamma, \mathbb{D})$ and $E(\alpha \mid \Gamma, \mathbb{D})$ exactly is infeasible for all but the simplest games. We propose to approximate these by using the procedure given in Algorithm 1. The procedure generates $K$ samples $F_{k} \sim F(\alpha \mid \Gamma, \mathbb{D})$ and $E_{k} \sim E(\alpha \mid \Gamma, \mathbb{D})$, based on which it approximates $F(\alpha \mid \Gamma, \mathbb{D})=\frac{1}{K} \sum_{k} F_{k}$ and $E(\alpha \mid \Gamma, \mathbb{D})=\frac{1}{K} \sum_{k} E_{k}$. Since all $F_{k}$ and $E_{k}$, respectively, come from the same distribution, by the law of large numbers this will converge to the true values of $F(\alpha \mid \Gamma, \mathbb{D})$ and $E(\alpha \mid \Gamma, \mathbb{D})$ for $K \rightarrow \infty$. The procedure needs some means to determine if a path is non-terminating. This could be done, for instance, by checking if the path reached a state space which contains no terminal states and cannot be left anymore, or by setting a maximum path length.</p>
<h2>3 Harsanyi-Bellman Ad Hoc Coordination</h2>
<p>The problem of incomplete information is solved in Bayesian games by assuming that the type spaces $\Theta_{j}$ and type distribution $\Delta$ are common knowledge. This admits a solution in the form of the Bayesian Nash equilibrium [Harsanyi, 1968], here defined for SBGs:</p>
<p>Definition 7. Let $H^{t}$ be the history at time $t$ and define $\Theta_{-i}=\times_{j \neq i} \Theta_{j}$. A Bayesian Nash equilibrium (BNE) in state $s^{t}$ is a strategy profile $\left(\pi_{1}, \ldots, \pi_{n}\right)$ in
which, for all $i \in N$ and $\theta_{i} \in \Theta_{i}, \pi_{i}$ maximises</p>
<p>$$
\sum_{\hat{\theta}<em -i="-i">{-i} \in \Theta</em>}} \Delta\left(t, \hat{\theta<em i="i">{-i} \mid \theta</em>\right)\right)
$$}\right) \sum_{a \in A} u_{i}\left(s^{t}, a, \theta_{i}\right) \pi\left(H^{t}, a,\left(\theta_{i}, \hat{\theta}_{-i</p>
<p>where</p>
<p>$$
\begin{aligned}
\Delta\left(t, \theta_{-i} \mid \theta_{i}\right) &amp; =\frac{\Delta\left(t,\left(\theta_{i}, \theta_{-i}\right)\right)}{\sum_{\hat{\theta}<em -i="-i">{-i} \in \Theta</em>}} \Delta\left(t,\left(\theta_{i}, \hat{\theta<em N="N" _in="\in" k="k">{-i}\right)\right)} \
\pi\left(H^{t}, a, \theta\right) &amp; =\prod</em>\right)
\end{aligned}
$$} \pi_{k}\left(H^{t}, a_{k}, \theta_{k</p>
<p>In ad hoc coordination problems, the ad hoc agent does not know the type spaces $\Theta_{j}$ and, hence, the type distribution $\Delta$ of the game. Therefore, it cannot compute $\Delta\left(t, \theta_{-i} \mid \theta_{i}\right)$. However, using the history $H^{t}$, it can compute a posterior $\operatorname{Pr}\left(\theta_{-i} \mid H^{t}\right)=\prod_{j \neq i} \operatorname{Pr}\left(\theta_{j} \mid H^{t}\right)$ with $\operatorname{Pr}\left(\theta_{j} \mid H^{t}\right)$ being the probability that player $j$ has type $\theta_{j}$ based on history $H^{t}$</p>
<p>$$
\operatorname{Pr}\left(\theta_{j} \mid H^{t}\right)=\frac{L\left(H^{t} \mid \theta_{j}\right) P\left(\theta_{j}\right)}{\sum_{\hat{\theta}<em j="j">{j} \in \Theta</em>}} L\left(H^{t} \mid \hat{\theta<em j="j">{j}\right) P\left(\hat{\theta}</em>
$$}\right)</p>
<p>where $L\left(H^{t} \mid \theta_{j}\right)=\prod_{\tau=0}^{t-1} \pi_{j}\left(H^{\tau}, a_{\hat{\jmath}}^{\tau}, \theta_{j}\right)$ is the probability of history $H^{t}$ if the type of player $j$ is $\theta_{j}$, and $P\left(\theta_{j}\right)$ is the agent's prior belief that player $j$ has type $\theta_{j}$.
[Kalai and Lehrer, 1993] studied single-state SBGs (with static pure type distributions) with players who choose actions to maximise their expected long-term payoff. They have shown that, if player $i$ maintains a posterior according to (2), and if the type distribution $\Delta$ is $a b$ solutely continuous with respect to the posterior (i.e., $\Delta\left(t,\left(\theta_{i}, \theta_{-i}\right)\right)&gt;0 \Rightarrow \operatorname{Pr}\left(\theta_{-i} \mid H^{t}\right)&gt;0$ ), then player $i$ 's predictions of future play will eventually be correct, regardless of player $i$ 's own strategy (Theorem 1 in [Kalai and Lehrer, 1993]). It follows that, if all players maintain such posteriors (where $\Delta$ is absolutely continuous with each posterior), and if all players choose their strategies according to a modified version of (1) which replaces the immediate payoff with the expected long-term payoff, then play will converge to a Nash equilibrium (NE) of the game (Theorem 2 in [Kalai and Lehrer, 1993]). A similar result was shown by [Jordan, 1991] for myopic players (i.e. maximising immediate payoffs).
While these are encouraging theoretical results, there are several potential objections concerning the use of NE: Firstly, if there are multiple NE, then the players may converge to a sub-optimal equilibrium. Secondly, a NE is incomplete in that it does not specify strategies for off-equilibrium paths. Finally, [Dekel et al., 2004] have shown that if the posteriors of the players are not identical, then they might converge to a solution which is not a NE. However, our main concern with NE is that it makes strong behavioural assumptions about the</p>
<p>players' behaviours (such as perfect rationality) which may be difficult to justify in ad hoc coordination. For instance, there is no guarantee that all players maintain posteriors according to (2). The same arguments hold for solution concepts in extensive form games, such as the perfect Bayesian equilibrium and sequential equilibrium [Fudenberg and Tirole, 1991].
Rather than attempting to converge to NE, it is appealing to use (1) as a best-response rule, since it maximises the expected payoff with respect to what types the ad hoc agent believes the other players to have and their strategies for all types. Based on Theorem 1 in [Kalai and Lehrer, 1993], we know that the agent's beliefs, and hence its expected payoffs, will be correct after some time. However, in its current form, (1) only considers immediate payoffs whereas optimal behaviour may require an agent to take payoffs of future states into account. Therefore, we propose to combine (1) with the Bellman optimality equation [Bellman, 1957] to obtain a best-response rule which we call Harsanyi-Bellman Ad Hoc Coordination. Since ad hoc coordination requires that the agent does not know the type spaces $\Theta_{j}$, we assume instead that the ad hoc agent is provided with user-defined type spaces $\Theta_{j}^{<em>}$, and we sometimes refer to $\Theta_{j}$ as the true type spaces.
Definition 8. Let $\Gamma$ be an ad hoc coordination problem where ad hoc agent $\alpha$ controls player $i$ and has access to user-defined type spaces $\Theta_{-i}^{</em>}=\times_{j \neq i} \Theta_{j}^{*}$. Harsanyi-Bellman Ad Hoc Coordination (HBA) is defined as $a_{i}^{t} \sim \arg \max <em i="i">{a</em>)=$
is the expected long-term payoff for player $i$ of taking action $a_{i}$ in state $s$ after history $\hat{H}\left(a_{i,-i} \triangleq\left(a_{i}, a_{-i}\right)\right)$, and $Q_{a}^{a}(\hat{H})=$
is the expected long-term payoff for player $i$ when joint action $a$ is executed in state $s$ after history $\hat{H}$, with $0 \leq \gamma \leq 1$ being the discount factor.}} E_{s^{t}}^{a_{i}}\left(H^{t}\right)$, where $E_{s}^{a_{i}}(\hat{H</p>
<p>HBA is a modification of (1) which replaces $\Delta\left(t, \theta_{-i} \mid \theta_{i}\right)$ by the posterior $\operatorname{Pr}\left(\theta_{-i} \mid H^{t}\right)(2)$, and in which the immediate payoff $u_{i}$ is replaced by an altered version (3) of the Bellman optimality equation. The actual history $H^{t}$ is used to compute the posterior, and the projected histories $\hat{H}$ are used to generate all future trajectories.
Each user-defined type $\theta_{j}^{<em>} \in \Theta_{j}^{</em>}$ is a hypothesis about the behaviour of player $j$. While this gives HBA great flexibility (as $\Theta_{j}^{<em>}$ may include a variety of behaviours), it is important to note that the accuracy of (3), and
hence efficiency of HBA, depends on how closely the user-defined types capture the players' true types. In this respect, we state two useful properties of HBA:
Proposition 1. Let $\Gamma$ be a SBG with static pure type distribution $\Delta$. If all players $i \in N$ are controlled by an HBA agent $\alpha_{i}$ with user-defined type spaces $\Theta_{j}^{</em>, i}$, and if $\forall j \neq i: \Theta_{j} \subseteq \Theta_{j}^{*, i}$, then play will converge to NE.</p>
<p>This follows from Theorems 1 and 2 in [Kalai and Lehrer, 1993] together with the fact that $\Theta_{j} \subseteq \Theta_{j}^{<em>, i}$ for all $i$ and $j$ (with $i \neq j$ ), which means that the type distribution $\Delta$ is always absolutely continuous with respect to the players' posteriors. Note that, while this proposition does not directly relate to ad hoc coordination, its does guarantee the minimum requirements of convergence and optimality in self-play, as formulated in [Bowling and Veloso, 2002].
For the next proposition, we define the class of $d e$ terministic learners, denoted $\Theta^{D}$, which consists of all types $\theta_{j}$ where, for all times $t$ and histories $H^{t}$, there exists a unique sequence $\left(\chi_{a_{j}}\right)<em j="j">{a</em>\right)$, for all $(a, s) \in A \times S$. In other words, a deterministic learner always learns the same from a given history. By definition, this includes all fixed (i.e. non-changing) behaviours.
Proposition 2. Let $\Gamma$ be a SBG with static pure type distribution $\Delta$, where $\alpha$ controls $i$. If $\forall j \neq i: \Theta_{j} \subseteq$ $\Theta^{D} \wedge \Theta_{j} \subseteq \Theta_{j}^{} \in A_{j}}$ such that $\pi_{j}\left(\left\langle H^{t},(a, s)\right\rangle, a_{j}, \theta_{j}\right)+\chi_{a_{j}}=\pi_{j}\left(H^{t}, a_{j}, \theta_{j</em>}$, then $\alpha$ will be optimally efficient.</p>
<p>This follows from the fact that there is some point after which HBA knows the players' types (Theorem 1 in [Kalai and Lehrer, 1993]) and, since all types are deterministic learners, the expected payoffs (3) are correct. Since HBA chooses actions with maximum expected payoffs, according to the Bellman principle [Bellman, 1957], it follows that it achieves optimal efficiency. Note that HBA is itself a deterministic learner, hence HBA achieves optimal efficiency in self-play.
Both propositions assume that (3) can be implemented directly, which is often infeasible. In Sections 4 and 5, we show how HBA can be implemented as a reinforcement learning procedure and an exact planning procedure.</p>
<h3>3.1 Temporally Reweighted Posteriors</h3>
<p>A potential problem with the posterior defined in (2) is that it assigns zero probability to a type $\theta_{j}$ if $\pi_{j}\left(H^{t}, a_{j}^{t}, \theta_{j}\right)$ is zero for any $t$. This can be problematic for the following reasons: If the game uses a dynamic or mixed type distribution, and if $\operatorname{Pr}\left(\theta_{j} \mid H^{t}\right)=0$ for a type $\theta_{j}$ that is not currently the true type of player $j$, then $\operatorname{Pr}\left(\theta_{j} \mid H^{\tau}\right)=0$ for all times $\tau&gt;t$, even if player $j$ 's type changes to $\theta_{j}$. Furthermore, if we have a user-defined type $\theta_{j}^{<em>}$ which approximates the true type $\theta_{j}$ of player $j$ in a subset $S^{</em>} \subset S$ (i.e. $\pi_{j}\left(H^{t}, a_{j}, \theta_{j}^{*}\right) \approx \pi_{j}\left(H^{t}, a_{j}, \theta_{j}\right)$</p>
<p>for $s^{t} \in S^{<em>}$ ), but not outside $S^{</em>}$, then (2) might assign zero probability to $\theta_{j}^{<em>}$ once player $j$ leaves $S^{</em>}$. However, $\theta_{j}^{<em>}$ may be the best approximation we have for $S^{</em>}$, so it would be useful if (2) was able to quickly reassign positive probability to $\theta_{j}^{<em>}$ once player $j$ returns to $S^{</em>}$. To address these problems, we introduce temporally reweighted posteriors:
Definition 9. A temporally reweighted posterior (TRposterior) is defined as in (2) by redefining</p>
<p>$$
L\left(H^{t} \mid \theta_{j}\right)=\sum_{\tau=0}^{t-1} f(t-\tau) \pi_{j}\left(H^{\tau}, a_{j}^{\tau}, \theta_{j}\right)
$$</p>
<p>where $f(\xi) \geq 0$ and $f(\xi) \geq f(\xi+1)$, for all $\xi \in \mathbb{N}^{+}$.
The function $f$ is called the time weight and can assume various forms. An example of a simple but useful time weight, called the general time weight, is given by $f(\xi)=$ $\max \left[0, a-b(\xi-1)^{c}\right]$ where $a, b, c \in \mathbb{R}_{0}^{+}$. This time weight can be used to produce various behaviours, depending on the parameters $a, b, c$. In particular, it can be used to give greater importance to more recent events, which means that HBA is able to quickly reassign probabilities. However, the crucial aspect of (4) is that it defines a sum rather than a product, which means that the problems described above do not occur.</p>
<h3>3.2 Conceptual Types</h3>
<p>If the user-defined type space $\Theta_{j}^{<em>}$ for player $j$ does not include the true type space $\Theta_{j}$ (i.e. $\Theta_{j} \not \subset \Theta_{j}^{</em>}$ ), then $j$ might assume a type which is unknown to HBA, causing its expected payoffs to be inaccurate. In such cases, it would be useful if HBA was able to learn new types from experience. This opens up the possibility of using methods for opponent modelling (e.g. case-based reasoning [Wendler and Bach, 2004] or recursive modelling [Gmytrasiewicz and Durfee, 2000]) which can be included in $\Theta_{j}^{<em>}$. In this work, we use a combination of case-based reasoning and fictitious play [Brown, 1951], called conceptual types. Conceptual types are based on the observation that behaviour may not be specified on a state-by-state basis but rather on abstractions of state spaces. (An example are the "information sets" in extensive form games.) That is, there may be some world conceptualisation inherent in a behaviour. While the types in $\Theta_{j}^{</em>}$ are used to hypothesise behaviours directly, a conceptual type can be used to hypothesise a world conceptualisation underlying a player's behaviour. Combined with the player's observed actions, this can be used to generalise actions to unseen states and increase accuracy in rarely visited states.
Definition 10. A conceptual type (c-type) $\theta_{j}^{c}$ for player $j$ is a tuple $\left(d_{j}, r, f\right)$, where $d_{j}: S \times S \rightarrow \mathbb{R}_{0}^{+}$is a symmetric distance function for pairs of states, $r \in$
$\mathbb{R}^{+}$is a radius, and $f$ is a time weight (as defined in Section 3.1), with</p>
<p>$$
\pi_{j}\left(H^{t}, a_{j}, \theta_{j}^{c}\right)=\left{\begin{array}{l}
\left|A_{j}\right|^{-1} \text { if } \nexists r<t: g\left(s^{t}, s^{\tau}\right)>0 \text { else } \
\eta \sum_{a^{*} \in H^{t}: a_{j}^{\tau}=a_{j}} f(t-\tau) g\left(s^{t}, s^{\tau}\right)
\end{array}\right.
$$</p>
<p>where $g\left(s_{1}, s_{2}\right)=\max \left[0,1-d_{j}\left(s_{1}, s_{2}\right) r^{-1}\right]$ and $\eta$ is a normalisation constant s.t. $\sum_{a_{j}} \pi_{j}\left(H^{t}, a_{j}, \theta_{j}^{c}\right)=1$.</p>
<p>The function $g$ is the hypothesised world conceptualisation of player $j$, where $d_{j}$ and $r$ specify how similar two states are from the perspective of player $j$ (examples given in Section 4). The time weight $f$ can be used to give greater importance to recent events, which allow c-types to adapt quickly to changing behaviours. Note that we can include multiple c-types in $\Theta_{j}^{*}$, each corresponding to a different world conceptualisation, and the posterior filters out those types which do not fit.</p>
<h2>4 Simulated Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>We evaluated different configurations of HBA in a multiagent logistics domain called level-based foraging (see Figure 1). A level-based foraging problem consists of a rectangular grid with $n$ players and $m$ foods. Each field in the grid is either empty or occupied by one player or one food. All players and foods have a level $\left(\in \mathbb{N}^{+}\right)$ where no food has a level greater than the sum of any 4 players' levels. A player can choose among 5 actions: $N, E, S, W$, and load. The first 4 actions move the player into the corresponding direction if the field is empty and inside the grid. A group of 1 to 4 players can load a food if they are placed on fields next to the food and if the sum of their levels is at least as high as the food's level. A player which successfully loads a food obtains a payoff equal to the level of the loaded food. At all other times, it receives a negative payoff of -0.01 . To avoid conflicts and keep this solvable, the foods are placed such that the Euclidean distance between each of them is greater than 1, and no food is placed at any border of the grid. The players' goal is to
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Level-based foraging domain. Players are marked by circles and foods are marked by squares (the levels are shown inside). Left: Each player can load a food. Right: No player can load a food.</p>
<p>collect all foods in minimal time, while also trying to maximise their own payoffs. Since the players have different abilities (i.e. levels) and are spatially distributed, this requires strong coordination of their behaviours.</p>
<p>We specify 6 classes of types. The first 4 classes contain types with fixed behaviours (i.e. they do not change over time). They each have a parameter $\sigma$ which specifies the radius of their sight: H1 always goes to the closest visible food. H2 goes to the one visible food which is closest to the centre of all visible players. H3 always goes to the closest visible food with compatible level (i.e. it can load it) and H4 goes to the one visible food which is closest to all visible players such that the sum of their and H4's level is sufficient to load the food. H1-4 try to load the food once they are next to it. If they do not see a food, they go into a random direction. The last two classes specify types with learning behaviours: Class 5 contains all instances of JAL and class 6 all instances of CJAL, as specified in the next paragraph.</p>
<p>We evaluated various configurations of HBA and three alternative algorithms: JAL [Claus and Boutilier, 1998] learns the action frequencies of each player in each state (i.e. opponent modelling) and uses them to compute expected action payoffs; CJAL [Banerjee and Sen, 2007] is similar to JAL but learns the frequencies conditioned on its own actions; WoLF-PHC [Bowling and Veloso, 2002] is a hill-climbing method in the space of mixed strategies. All three algorithms behave differently in ad hoc coordination [Albrecht and Ramamoorthy, 2012].
A single framework (Algorithm 2) was used to implement each ad hoc agent. We assume that the ad hoc agent is able to observe the states of the game, each player's actions, and its own payoffs. For simplicity, we also assume that the agent knows the levels of all players and foods. The framework uses a table $Q$ to learn the expected long-term payoffs of joint actions, similar to Q-learning [Watkins and Dayan, 1992]. To accelerate learning, it uses an eligibility trace $e$ (see [Sutton and Barto, 1998]) to connect current payoffs with past actions. We assume that the agent has access to a simulator SimulATE $(s, a)$ which, based on the transition $(T)$ and payoff $\left(u_{i}\right)$ functions of the game, returns a successor state $s^{\prime}$ and payoff $u$ after taking joint action $a$ in state $s$. This simulator is used in a sampling-based planning procedure [Kearns et al., 1999] $\operatorname{EXPAND}(d, s, \hat{e})$ which, starting in state $s$, generates a future trajectory of length $d$ and updates Q using the eligibility trace $\hat{e}$. The function $\operatorname{EXPPAY}\left(Q, s, a_{i}\right)$ computes the expected payoff for taking action $a_{i}$ in state $s$ based on $Q$, and the function $\operatorname{OPPACtions}(s)$ samples actions for all other players $j \neq i$ in state $s$. HBA implements Expand using (1) and its posterior, and OppActions using its posterior and user-defined types. C/JAL implement these functions using their learned action frequencies.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 Reinforcement learning framework
Set \(Q(s, a) \leftarrow 0\) and \(e(s, a) \leftarrow 0\) for all \((s, a) \in S \times A\)
Repeat until \(s^{t} \in \bar{S}\) :
    Observe: current state \(s^{t}\)
    With probability \(1-\epsilon_{1}: a_{i}^{t}=\operatorname{ChOoseAction}\left(s^{t}\right)\),
                        else sample \(a_{i}^{t} \sim A_{i}\)
</code></pre></div>

<p>Observe: joint action $a^{t}$, own payoff $u_{i}^{t}$, next state $s^{t+1}$</p>
<div class="codehilite"><pre><span></span><code>\(\operatorname{UpdateQ}\left(s^{t}, a^{t}, u_{i}^{t}, s^{t+1}, e\right)\)
</code></pre></div>

<p>Repeat $x$ times: $\operatorname{EXPAND}\left(d, s^{t+1}, \operatorname{Copy}(e)\right)$
$\operatorname{EXPAND}(d, s, \hat{e})$ :
Repeat $d$ times or until $s \in \bar{S}$ :
With probability $1-\epsilon_{2}: a_{i}=\operatorname{ChOoseAction}(s)$,
else sample $a_{i} \sim A_{i}$</p>
<p>$$
\begin{aligned}
&amp; a_{-i} \leftarrow \operatorname{OPPACtions}(s) \
&amp; \left(u_{i}, s^{\prime}\right) \leftarrow \operatorname{Simulate}\left(s,\left(a_{i}, a_{-i}\right)\right) \
&amp; \operatorname{UpdateQ}\left(s,\left(a_{i}, a_{-i}\right), u_{i}, s^{\prime}, \hat{e}\right) \
&amp; s \leftarrow s^{\prime}
\end{aligned}
$$</p>
<p>$\operatorname{UpdateQ}\left(s, a, u, s^{\prime}, \hat{e}\right):$
$\delta=\beta\left(u+\gamma \max <em i="i">{\hat{a}</em>}} \operatorname{EXPPAY}\left(Q, s^{\prime}, \hat{a<em _min="\min">{i}\right)-Q(s, a)\right)$
$\hat{e}(s, a) \leftarrow 1$
For all $(\hat{s}, \hat{a}) \in S \times A$ s.t. $\hat{e}(\hat{s}, \hat{a}) \geq e</em>$ do:
$Q(\hat{s}, \hat{a}) \leftarrow Q(\hat{s}, \hat{a})+\delta \hat{e}(\hat{s}, \hat{a})$
$\hat{e}(\hat{s}, \hat{a}) \leftarrow \lambda \hat{e}(\hat{s}, \hat{a})$
$\operatorname{ChOoseAction}(s):$
Return $a_{i} \sim \arg \max <em i="i">{\hat{a}</em>\right)$}} \operatorname{EXPPAY}\left(Q, s, \hat{a}_{i</p>
<p>For WoLF-PHC, the framework defines $Q$ and $e$ on $S \times A_{i}$ (rather than $S \times A$ ) and $\operatorname{EXPPAY}\left(Q, s, a_{i}\right)$ is simply defined as $Q\left(s, a_{i}\right)$. Since WoLF-PHC does not model its opponents, we implement OppActions the same way as in JAL. The function ChooseAction $(s)$ is redefined to $a_{i} \sim \pi(s)$, where $\pi$ is the mixed strategy maintained in WoLF-PHC (cf. Tables 5 and 6 in [Bowling and Veloso, 2002]).
All algorithms used identical parameters: $\beta=.2, \gamma=.9$, $\lambda=.9, e_{\min }=.01, \epsilon_{1}=0, \epsilon_{2}=.2, x=3, d=20$. For WoLF-PHC, we used learning rates $\delta_{w}(t)=(1000+$ $\left.\frac{1}{10}\right)^{-1}$ and $\delta_{l}(t)=2 \delta_{w}(t)$. For HBA, we used uniform prior beliefs $\left(P\left(\theta_{j}^{<em>}\right)=\left[\Theta_{j}^{</em>}\right]^{-1}\right)$ and $a=10, b=.01$, $c=3$ for the general time weight. To obtain estimates of flexibility and efficiency, we used Algorithm 1 with $i=1, r_{1}=r_{2}=1, K=1000$, where we assumed a path to be non-terminating if it reached $t=1000$. The initial states were generated with random positions and levels for all players and foods, with the maximum level being equal to the number of players. All agents were tested on the same sequence of games and random numbers.</p>
<h3>4.2 Results</h3>
<p>We tested the effectiveness of TR-posteriors by simulating the two situations described in Section 3.1. All</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of simulated experiments, averaged over 1000 runs. Markers have the same colour if the difference is statistically insignificant (based on paired t-test with $5 \%$ significance level). "Cor" is HBA with correct types, "Gtw" is HBA using TR-posterior with general time weight, "Unl" is HBA with unlimited normal posterior, and "Lim" is HBA with normal posterior limited to 9 most recent events.
tests were run on a $8 \times 8$ grid with 2 players and 5 foods. In Figure 2a, we used $\Theta_{2}=\Theta_{2}^{<em>}={\mathrm{H} 1-\mathrm{H} 4 \mid \sigma=\infty}$ and a dynamic pure type distribution which changed the type of player 2 after every 10 to 20 time steps. In Figure 2b, we used $\Theta_{2}={\mathrm{H} 1-\mathrm{H} 4 \mid \sigma=3,5,7}$, $\Theta_{2}^{</em>}={\mathrm{H} 1-\mathrm{H} 4 \mid \sigma=\infty}$ (i.e the types in $\Theta_{2}^{<em>}$ were accurate only for subsets $S^{</em>} \subset S$ ) and a static pure type distribution. In both cases, the efficiency of HBA was significantly higher when using a TR-posterior with general time weight (Gtw) compared to both the normal posterior defined in (2) (Unl) and a normal posterior which was limited to the 9 most recent events (Lim), which is the same time frame used in Gtw. In Figure 2a, Gtw even achieved the same efficiency as a version of HBA which always knew the correct type of the other player (Cor). All HBA agents achieved a perfect flexibility of 1.
We tested HBA with 4 conceptual types $\theta_{j}^{c}=\left(d_{j}^{c}, r, f\right)$ where $f(\xi)=\left[\xi&lt;10\right]<em j="j">{1}$ and $r=1$. In the following, we write $s . p</em>$ are
$d_{j}^{1}\left(s_{1}, s_{2}\right)=\left[s_{1} \neq s_{2}\right]}\left(s . f_{k}\right)$ to refer to the position of player $j$ (food $f_{k}$ ) in state $s$, and $f_{k} \in s$ to say that food $f_{k}$ is available in state $s$. The distance functions $d_{j}^{c<em j="j">{1} \infty$
$d</em>\right]}^{2}\left(s_{1}, s_{2}\right)=\left[s_{1} . p_{j}=s_{2} \cdot p_{j} \wedge \forall k: f_{k} \in s_{1} \Leftrightarrow f_{k} \in s_{2<em j="j">{1} \infty$
$d</em>$
$d_{j}^{4}\left(s_{1}, s_{2}\right)=d_{j}^{3}\left(s_{1}, s_{2}\right)+\sum_{v} \phi\left(s_{1} \cdot p_{v}, s_{2} \cdot p_{v}\right) \omega_{v}^{-1.5}$
where $\phi\left(x_{1}, x_{2}\right)=\log \left(1+\psi\left(x_{1}, x_{2}\right)\right) \frac{1}{2}, \mu=s_{1} \cdot p_{j}+\frac{1}{2}\left(s_{2} \cdot p_{j}-\right.$ $\left.s_{1} \cdot p_{j}\right), \omega_{v}=\min \left[\psi\left(s_{1} \cdot p_{v}, \mu\right), \psi\left(s_{2} \cdot p_{v}, \mu\right)\right]$, and $\psi\left(x_{1}, x_{2}\right)$ denotes the Euclidean distance between $x_{1}$ and $x_{2}$. All tests were run on a $8 \times 8$ grid with 2 players and 5 foods, using $\Theta_{2}={\mathrm{H} 1-\mathrm{H} 4, \mathrm{JAL}, \mathrm{CJAL} \mid \sigma=\infty}$ (C/JAL used same parameters as HBA), $\Theta_{2}^{*}=\left{\theta_{2}^{c}\right}$ (each $c=1, \ldots, 4$ tested separately), and a static pure type distribution. The results in Figure 2c show that HBA achieved good efficiency (compared to Cor) using $\theta_{j}^{2}$, while the other c-types were less efficient. All HBA agents achieved statistically equivalent flexibilities of $0.86 \pm 0.01$.}^{3}\left(s_{1}, s_{2}\right)=\phi\left(s_{1} \cdot p_{j}, s_{2} \cdot p_{j}\right)+\sum_{k: f_{k} \in s_{1} \succsim f_{k} \in s_{2}} \psi\left(s_{1} \cdot f_{k}, \mu\right)^{-\frac{3}{2}</p>
<p>Finally, we tested HBA, JAL, CJAL, and WoLF-PHC
on a $10 \times 10$ grid with 3 players and 8 foods, using $\Theta_{2,3}={\mathrm{H} 1-\mathrm{H} 4, \mathrm{JAL}, \mathrm{CJAL} \mid \sigma=5,7,9}$ and $\Theta_{2,3}^{<em>}=$ ${\mathrm{H} 1-\mathrm{H} 4 \mid \sigma=\infty}$. To add more realism, players 2 and 3 were "defective" with probability 0.2 , where a defective player changed its type randomly every 10 to 30 time steps. While the potential of HBA is demonstrated by Cor, it would also be useful to know the optimal solution to the problem. However, with a complex problem such as this one, we were unable to compute optimal solutions. Instead, we had 6 humans play the game in a graphical user interface (each one played the full 1000 runs, distributed over 7 days at their own convenience), where no human was familiar with the technical details of this work. We do not necessarily claim that humans produce optimal solutions, but we expect them to perform consistently well in this setting. To cope with the increased problem size, we set the planning power of the algorithms to $x=10$ and $d=30$ (cf. Algorithm 2).
The results (Figure 2d) show that HBA clearly outperformed all alternative algorithms, with Unl and Gtw being over $100 \%$ and $200 \%$ more efficient, respectively. This is despite the fact that the user-defined types $\Theta_{2,3}^{</em>}$ did not include any true types of the players. We also tested HBA with the c-types $\theta_{j}^{1}$ and $\theta_{j}^{3}$ (added separately to $\Theta_{2,3}^{<em>}$ ) but found that the efficiency of HBA did not improve significantly. This is since C/JAL learned similar behaviours to H 1 and H 3 , which were already covered in $\Theta_{2,3}^{</em>}$. We found that HBA's posteriors often assigned high probabilities to $\mathrm{H} 1 / 3$ when the true type of the player was in fact C/JAL. Since H1/3 ignore other players, this means that C/JAL did not effectively coordinate their behaviours with other players. We found similar results for WoLF-PHC. As was expected, the humans achieved high efficiency (Figure 2d shows the best human) and outperformed even Cor. One reason for this is the fact that the humans had much greater planning power than HBA. Lastly, HBA achieved higher flexibilities $(.83 \pm .01)$ than JAL (.734), CJAL (.749), and WoLF-PHC (.744), while the humans all achieved perfect flexibility (1.0).</p>
<h2>5 Human-Machine Experiment</h2>
<h3>5.1 Experimental Setup</h3>
<p>We conducted a large-scale human-machine experiment at the Royal Society Summer Science Exhibition 2012. Therein, the human participants played repeated Prisoner's Dilemma (PD) and Rock-Paper-Scissors (RPS) against HBA and alternative algorithms, where each game was played for 20 rounds. We collected data from 427 participants, of which 186 played PD and 241 played RPS. The lowest and highest recorded ages were 9 and 72 , respectively, with an average age of about 17 .</p>
<p>A large public exhibition such as this one is an excellent testbed environment for ad hoc agents, since the visitors vary widely in factors such as age, intelligence, and behaviour. However, in order to make statistically relevant comparisons, we required data from many participants. Therefore, the games needed to be simple enough so participants would understand them quickly, yet they also needed to be interesting in terms of coordination strategies. PD and RPS are two widely studied problems in game theory which we believe cover these properties. In PD, the symmetric payoffs are $u_{1}(C, C)=3$, $u_{1}(D, D)=1, u_{1}(C, D)=0, u_{1}(D, C)=5$. The problem here is that the only NE, and hence stable outcome, is at (D,D), while (C,C) is the only outcome that has both the highest welfare (sum of payoffs) and fairness (product of payoffs) but is unstable since the players could deviate to obtain higher immediate payoffs. In RPS, the payoffs are $+1 / 0 /-1$ for won/even/lost games. The only NE is for all players to play randomly. However, even if humans attempt to play randomly, they often fall back to patterns [Wagenaar, 1972] against which the other player can coordinate its actions.</p>
<p>Our hypothesis for the experiment was that the human would switch between several simple behaviours, as opposed to having one complex behaviour. Therefore, we modelled the problem as a SBG with a dynamic mixed type distribution (unknown to us) which governed the type of the human, and we provided HBA with a small set of types (given in Tables 1 and 2) which we believed the human could have. HBA did not use any conceptual types.</p>
<p>The alternative algorithms were CJAL for PD, which was shown to outperform both JAL and WoLF-PHC in PD [Banerjee and Sen, 2007], and JAL for RPS, which is guaranteed to converge to NE in self-play in zero-sum games [Brown, 1951]. We implemented all algorithms using a single framework (Algorithm 3), where we set $l^{<em>}=10$ for PD, $l^{</em>}=1$ for RPS, and $t^{<em>}=20$. The function $\operatorname{OppStrat}\left(s^{\tau}, a^{\tau}\right)$ returns the probability that players $j \neq i$ choose actions $a_{j}^{\tau}$ in state $s^{\tau}$. HBA implements this by averaging over all user-defined types in $\Theta_{j}^{</em>}$ using its current posterior, and C/JAL do this</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3 Exact planning framework
    Repeat:
    Observe current state \(s^{t}\)
    For all \(a_{i} \in A_{i}\) do:
    \(\Omega\left(a_{i}\right)=\left\{\left\langle s^{t}, a^{t}, \ldots, s^{t+l}, a^{t+l}\right\rangle \mid a_{i}^{t}=a_{i}\right\}\)
        where \(l=\min \left[l^{*}, t^{*}-t\right]-1\)
    \(E\left(a_{i}\right)=\sum_{\omega \in \Omega\left(a_{i}\right)}\left[\prod_{\tau=t}^{t+l} \operatorname{OppStrat}\left(s^{\tau}, a^{\tau}\right) \sum_{\tau=t}^{t+l} u_{i}\left(s^{\tau}, a^{\tau}\right)\right]\)
    Sample action \(a_{i}^{t} \sim \arg \max <span class="ge">_{a_</span>{i}} E\left(a_{i}\right)\)
</code></pre></div>

<p>using their learned actions frequencies. While PD and RPS have no states, we found that the performance of C/JAL could be further improved by introducing "artificial" states, which we simply defined as $s^{t}=a^{t-1}$ (in the first round, C/JAL assumed the opponent to play randomly). HBA used uniform prior beliefs and the general time weight with $a=10, b=0.05, c=3$.</p>
<p>The procedure of the experiment was as follows: First, we randomly sampled a participant from the set of visitors which were currently at our exhibit. The participant was then brought to a dedicated table with a chair and a laptop on it. The laptop ran a programme, with an intuitive graphical user interface, which prompted the participant to choose between PD and RPS. The rules of the games were explained both textually in the programme and in person by one of our staff members to make sure the participant understood the rules. The game was then played in two matches, each lasting 20 rounds. One of the matches was against HBA and the other match against C/JAL, but this was hidden from the participant and the order was chosen randomly. The programme displayed the current match, round, and scores of all players, and also allowed to display the rules at any time. At the end of each round, the participant was shown the actions and scores of both players, and at the end of each match, the participant was given a summary of the scores.</p>
<h3>5.2 Results</h3>
<p>In the following, all significance statements are based on paired t-tests with $5 \%$ significance level. Figures 3a and 3 b show the results for PD and RPS, respectively. In both games, the average total payoffs of HBA and C/JAL were statistically equivalent. Since the time was fixed to 20 rounds, it means that they achieved equal efficiency. This is, in fact, a positive result considering that C/JAL are strong candidates in PD/RPS. In addition, as we discuss in the following, HBA behaved very differently from C/JAL, with beneficial side effects.
In PD, the most desirable long-term outcome is (C,C)</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results of the human-machine experiment. Circles and whiskers correspond to mean, minimum, and maximum values, respectively. The welfare plot in (a) shows the median value and 25%/75% percentiles.</p>
<p>since it is both welfare and fairness optimal, and since it is a non-myopic equilibrium [Brams, 1993], meaning that no player has a long-term incentive to deviate. With this in mind, we point out that in over 28% of the games, HBA and the human played (C,C) in at least 50% of the final 10 rounds of the game, while CJAL did not achieve this in any game. Thus, HBA achieved a significantly higher total welfare than CJAL (Figure 3a). This is despite the fact that neither of them was optimised for social welfare. The reason for this is that HBA was planning more accurately than CJAL. When computing the expected payoffs $E(a_i)$, CJAL uses its learned action frequencies to obtain probabilities for each trajectory in $\Omega(a_i)$. However, these probabilities can only be accurate for states that have been visited frequently enough. Moreover, if a player changes its behaviour, CJAL requires new evidence from all states to accurately reflect the change. On the other hand, HBA uses its posterior and types to compute probabilities of trajectories. Therefore, once HBA has an accurate posterior, it can use the types to accurately plan in the entire state space of the game, including unseen states. This also allows HBA to plan the effects of its actions on the other player, which means that HBA may take actions to manipulate the player's decisions. Finally, if a player changes its behaviour, HBA only needs to update its posterior, which requires much less information than the update in CJAL.</p>
<p>In RPS, the crucial questions is whether a player is winning or not. Interestingly, the winning rate of HBA (53.71%) was significantly higher than the winning rate of JAL (43.98%), as shown in Figure 3b. While in PD the good performance of HBA was due to its planning capabilities, in RPS this was not as relevant since the planning horizon was limited to trajectories of length 1. Rather, HBA's good performance was due to the fact that it recognised changed behaviours faster than JAL. Indeed, in a game such as RPS, it can be expected that the human players change frequently between different strategies. This is confirmed by the statistics shown in Figure 3c, which show the average number of types used by the human players and the average duration. The statistics are based on HBA's posteriors, where the number of types for player $i$ in a play corresponds to the number $q$ in $\langle t_0, t_1, \ldots, t_q \rangle$, with $t_0 = 0$ and $t_q = 20$, for which $\arg \max_{\theta_i} \Pr(\theta_i | H^<em>) \subseteq \arg \max_{\theta_i} \Pr(\theta_i | H^{</em>*1})$ for all $t_{q-1} \leq \tau &lt; t_q$ and $y \in {1, \ldots, q}$, and where the average duration is $\frac{1}{q} \sum_q t_q - t_{q-1}$. According to these statistics, the human players had 4.45 types with a duration of 4.96 rounds in PD, and 8.25 types with a duration of 2.46 rounds in RPS. Clearly, with a duration of only 2.46 rounds, planning was not as important as recognising changed types. By using TR-posteriors, HBA was able to do this effectively.</p>
<h2>6 Summary &amp; Open Questions</h2>
<p>This work is concerned with the ad hoc coordination problem, in which the goal is to design an autonomous agent (the ad hoc agent) which can achieve optimal flexibility and efficiency in a multiagent system in which the behaviour of the other agents is not a priori known. We make three important contributions to the ad hoc coordination problem:</p>
<ol>
<li>
<p>We propose a game-theoretic model, SBG, which captures the notion of private information in the form of types. Based in this model, we give formally concise definitions of flexibility, efficiency, and the ad hoc coordination problem. We also provide a procedure which can be used to estimate the ad hoc agent's flexibility and efficiency.</p>
</li>
<li>
<p>From this model, we derive a principled solution, HBA, which utilises a set of user-defined types in a planning procedure to find optimal actions in the sense of Bayesian Nash equilibrium and Bellman optimal control. We also propose two possible extensions which enable HBA to recognise changed types and learn new types.</p>
</li>
<li>
<p>We show how HBA can be implemented as a reinforcement learning and exact planning procedure, and we provide extensive empirical evaluations in a complex multiagent logistics domain and a large-scale human-machine experiment. Our results show that HBA is both more flexible and efficient than alternative methods.</p>
</li>
</ol>
<p>The work presented in this paper provides a rich ground for future research, including the following open questions:</p>
<ul>
<li>A crucial design parameter of HBA are the userdefined type spaces $\Theta_{j}^{<em>}$ provided to it. In this regard, an important direction for future research would be to analyse how closely $\Theta_{j}^{</em>}$ must approximate $\Theta_{j}$ for HBA to be able to achieve optimal flexibility and efficiency.</li>
<li>Another design parameter of HBA is the posterior $\operatorname{Pr}\left(\cdot \mid H^{t}\right)$, and in this work we discussed two different formulations (the product posterior and TR-posteriors). It would be interesting to explore alternative posterior formulations and to analyse the conditions under which they are guaranteed to converge to the type distribution of the game.</li>
<li>The prior belief $P$ can be considered a metaparameter of HBA (it is a parameter of the posterior, which in turn is a parameter of HBA), and in our experiments we assumed that the prior beliefs were uniform. An interesting question in this regard is whether HBA could automatically derive prior beliefs from the user-defined type spaces so as to further maximise its efficiency.</li>
<li>HBA currently assumes that an expert can provide manually specified types for the problem at hand. However, this can be a cumbersome task in complex domains. Future work could investigate how HBA might generate useful types from the problem description so that the burden of having to manually specify types can be alleviated, or perhaps eliminated altogether.</li>
<li>Finally, as we employ HBA in increasingly complex problem domains, it becomes apparent that the type specifications, likewise, become increasingly complex. One way to reduce this type complexity might be to use a hierarchical type specification, in which types are structured into smaller sub-types.</li>
</ul>
<h2>Acknowledgements</h2>
<p>This work was partially supported by grants from the UK Engineering and Physical Sciences Research Council (EP/H012338/1), the European Commission (TOMSY Grant 270436, FP7-ICT-2009.2.1 Call 6) and a Royal Academy of Engineering Ingenious grant.</p>
<h2>References</h2>
<p>[Agmon and Stone, 2012] Agmon, N. and Stone, P. (2012). Leading ad hoc agents in joint action settings with multiple teammates. In 11th International</p>
<p>Conference on Autonomous Agents and Multiagent Systems.
[Albrecht and Ramamoorthy, 2012] Albrecht, S. and Ramamoorthy, S. (2012). Comparative evaluation of MAL algorithms in a diverse set of ad hoc team problems. In 11th International Conference on $A u$ tonomous Agents and Multiagent Systems.
[Banerjee and Sen, 2007] Banerjee, D. and Sen, S. (2007). Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning. $A u$ tonomous Agents and Multiagent Systems, 15(1):91108 .
[Barrett et al., 2011] Barrett, S., Stone, P., and Kraus, S. (2011). Empirical evaluation of ad hoc teamwork in the pursuit domain. In 10th International Conference on Autonomous Agents and Multiagent Systems.
[Bellman, 1957] Bellman, R. (1957). Dynamic Programming. Princeton University Press.
[Bowling and McCracken, 2005] Bowling, M. and McCracken, P. (2005). Coordination and adaptation in impromptu teams. In Proceedings of the National Conference on Artificial Intelligence, volume 20, page 53 .
[Bowling and Veloso, 2002] Bowling, M. and Veloso, M. (2002). Multiagent learning using a variable learning rate. Artificial Intelligence, 136(2):215-250.
[Brams, 1993] Brams, S. (1993). Theory of Moves. Cambridge University Press.
[Brown, 1951] Brown, G. (1951). Iterative solution of games by fictitious play. In Activity Analysis of Production and Allocation. Wiley.
[Claus and Boutilier, 1998] Claus, C. and Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative multiagent systems. In Proceedings of the National Conference on Artificial Intelligence, pages $746-752$.
[Dekel et al., 2004] Dekel, E., Fudenberg, D., and Levine, D. (2004). Learning to play Bayesian games. Games and Economic Behavior, 46(2):282-303.
[Dias et al., 2006] Dias, M., Harris, T., Browning, B., Jones, E., Argall, B., Veloso, M., Stentz, A., and Rudnicky, A. (2006). Dynamically formed human-robot teams performing coordinated tasks. In AAAI Spring Symposium "To Boldly Go Where No Human-Robot Team Has Gone Before".</p>
<table>
<thead>
<tr>
<th>PD type</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>AlwaysC</td>
<td>$a_{i}^{t}=C$</td>
</tr>
<tr>
<td>TitForTat</td>
<td>$a_{i}^{0}=C, a_{i}^{t}=a_{j}^{t-1}$</td>
</tr>
<tr>
<td>TitFor2Tats</td>
<td>$a_{i}^{0,1}=C, a_{i}^{t}=C$ if $a_{j}^{t-1, t-2}=C$ else $D$</td>
</tr>
<tr>
<td>Optimistic</td>
<td>$\pi_{i}\left(C, H^{t}\right)=1$ if $t&lt;2 \vee a_{j}^{t-1}=C \vee \mu=0$ else $0.2+0.8 \sigma$</td>
</tr>
<tr>
<td>Pessimistic</td>
<td>$\pi_{i}\left(D, H^{t}\right)=1$ if $t&lt;2 \vee a_{j}^{t-1}=D$ else $0.2+[\mu&gt;0]_{1} 0.8 \sigma$</td>
</tr>
<tr>
<td></td>
<td>$\mu=\sum_{\tau=0}^{t-2}\left[a_{i}^{\tau}=C\right]<em _tau="0">{1}, \sigma=\frac{1}{\mu} \sum</em>$}^{t-2}\left[a_{i}^{\tau}=a_{j}^{\tau+1}=C\right]_{1</td>
</tr>
</tbody>
</table>
<p>Table 1: PD types. $[b]_{1}=1$ iff. $b$ is true, else 0 .</p>
<table>
<thead>
<tr>
<th>RPS type</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Copycat</td>
<td>$a_{i}^{0} \sim U\left(A_{i}\right), a_{i}^{t}=a_{j}^{t-1}$</td>
</tr>
<tr>
<td>RetryIfWon</td>
<td>$a_{i}^{t} \sim U\left(A_{i}\right)$ if $t=0 \vee u_{i}\left(a^{t-1}\right)&lt;0$ else $a_{i}^{t}=a_{i}^{t-1}$</td>
</tr>
<tr>
<td>$i$-focused $(h)$</td>
<td>$\pi_{i}\left(a_{i}, H^{t}\right)=g\left(a_{i}, x\right) / \sum_{\hat{a}<em i="i">{i} \in A</em>, x\right), x=\min [t, h]$}} g\left(\hat{a}_{i</td>
</tr>
<tr>
<td>$h \in{1,2}$</td>
<td>$g\left(a_{i}, x\right)=\max \left[0, x-\sum_{\tau=1}^{x}\left[a_{i}^{t-\tau}=a_{i}\right]_{1}(x+1-\tau)\right]$</td>
</tr>
<tr>
<td>$j$-focused $(h)$</td>
<td>$a_{i}^{t} \sim \arg \max <em i="i">{a</em>\right)$}} \sum_{a_{j} \in A_{j}} \pi_{j}\left(a_{j}, H^{t}\right) u_{i}\left(a_{i}, a_{j</td>
</tr>
<tr>
<td>$h \in{1,2}$</td>
<td>where $\pi_{j}\left(a_{j}, H^{t}\right)$ is obtained using $i$-focused $(h)$ for $j$</td>
</tr>
</tbody>
</table>
<p>Table 2: RPS types. $U$ is the uniform distribution.
[Fudenberg and Tirole, 1991] Fudenberg, D. and Tirole, J. (1991). Perfect Bayesian equilibrium and sequential equilibrium. Journal of Economic Theory, $53(2): 236-260$.
[Gmytrasiewicz and Doshi, 2005] Gmytrasiewicz, P. and Doshi, P. (2005). A framework for sequential planning in multiagent settings. Journal of Artificial Intelligence Research, 24(1):49-79.
[Gmytrasiewicz and Durfee, 2000] Gmytrasiewicz, P. and Durfee, E. (2000). Rational coordination in multi-agent environments. Autonomous Agents and Multi-Agent Systems, 3(4):319-350.
[Harsanyi, 1967] Harsanyi, J. (1967). Games with incomplete information played by "Bayesian" players. Part I. The basic model. Management Science, $14(3): 159-182$.
[Harsanyi, 1968] Harsanyi, J. (1968). Games with incomplete information played by "Bayesian" players. Part II. Bayesian equilibrium points. Management Science, 14(5):320-334.
[Jordan, 1991] Jordan, J. (1991). Bayesian learning in normal form games. Games and Economic Behavior, $3(1): 60-81$.
[Kalai and Lehrer, 1993] Kalai, E. and Lehrer, E. (1993). Rational learning leads to Nash equilibrium. Econometrica, pages 1019-1045.
[Kearns et al., 1999] Kearns, M., Mansour, Y., and Ng, A. (1999). A sparse sampling algorithm for nearoptimal planning in large Markov decision processes. In International Joint Conference on Artificial Intelligence, volume 16, pages 1324-1331.
[Shapley, 1953] Shapley, L. (1953). Stochastic games. Proceedings of the National Academy of Sciences of the United States of America, 39(10):1095.
[Stone et al., 2010a] Stone, P., Kaminka, G., Kraus, S., and Rosenschein, J. (2010a). Ad hoc autonomous agent teams: Collaboration without pre-coordination. In 24th AAAI Conference on Artificial Intelligence.
[Stone et al., 2010b] Stone, P., Kaminka, G., and Rosenschein, J. (2010b). Leading a best-response teammate in an ad hoc team. In Agent-Mediated Electronic Commerce: Designing Trading Strategies and Mechanisms for Electronic Markets, pages $132-146$.
[Stone and Kraus, 2010] Stone, P. and Kraus, S. (2010). To teach or not to teach? Decision making</p>
<p>under uncertainty in ad hoc teams. In 9th International Conference on Autonomous Agents and Multiagent Systems.
[Sutton and Barto, 1998] Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction. The MIT press.
[Wagenaar, 1972] Wagenaar, W. (1972). Generation of random sequences by human subjects: A critical survey of literature. Psychological Bulletin, 77(1):65.
[Watkins and Dayan, 1992] Watkins, C. and Dayan, P. (1992). Q-learning. Machine learning, 8(3):279-292.
[Wendler and Bach, 2004] Wendler, J. and Bach, J. (2004). Recognizing and predicting agent behavior with case based reasoning. RoboCup 2003: Robot Soccer World Cup VII, pages 729-738.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1} \mathrm{~A}$ related model are I-POMDP, in which agents face incomplete information with respect to the state of the world and the behaviour of other agents [Gmytrasiewicz and Doshi, 2005]. However, I-POMDP are extremely complex and their solution methods are infeasible in most problems.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
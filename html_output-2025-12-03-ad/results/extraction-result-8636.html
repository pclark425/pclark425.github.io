<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8636 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8636</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8636</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273346003</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.09613v1.pdf" target="_blank">Transformer-based Language Models for Reasoning in the Description Logic ALCQ</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order logic sentences with only a few logical operators and quantifiers. We construct the natural language dataset, DELTA$_D$, using the expressive description logic language $\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examples and increases in two dimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we systematically investigate the logical reasoning capabilities of a supervised fine-tuned DeBERTa-based model and two large language models (GPT-3.5, GPT-4) with few-shot prompting. We show that the DeBERTa-based model fine-tuned on our dataset can master the entailment checking task. Moreover, the performance of GPTs can improve significantly even when a small number of samples is provided (9 shots). We open-source our code and datasets.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8636.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8636.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DELTA_M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DELTA_M (DeBERTaV3-large fine-tuned on DELTAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised DeBERTaV3-large model fine-tuned on the DELTAD dataset to perform 3-way entailment checking (True/False/Unknown) over natural-language translations of ALCQ description-logic KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTaV3-large (microsoft/deberta-v3-large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer pretrained (DeBERTaV3) and fine-tuned as a multi-class classifier; input format [CLS] context [SEP] question [SEP]; trained with AdamW, mixed precision (FP16) on two A100 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>304M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DELTAD (DEscription Logics with TrAnsformers) entailment checking</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A synthetic dataset (384K examples) mapping ALCQ description-logic KBs and queries into natural language; task: decide whether a query is entailed (True), contradicted (False), or not provable (Unknown) under the open-world assumption, across multiple inference depths (D ∈ {0,1,2,3,5}) and linguistic complexity levels (L ∈ {0..3}).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning of DeBERTaV3-large on DELTAD; trained variants DELTA_i,j with data up to inference depth i and linguistic complexity j to probe generalization; additional evaluations include zero-shot tests on other distributions, symbolic translations (SoftSymbolic, HardSymbolic), and a tweaked distribution (DELTA_T).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On its own held-out test (D ≤ 5, L ≤ 3) DELTA_M achieved 99.7% accuracy overall; models trained to D ≤ 5 nearly reach 99.5–100% across depths. Generalization: models trained up to D ≤ 2 achieve ~83.5% on D=5 questions; models trained up to D ≤ 3 achieve ~98.5% on D=5. Robustness: DELTA_M scored 99.6% on the tweaked DELTA_T; SoftSymbolic 95.3% (avg); HardSymbolic dropped to 58.5% (avg). On external ProofWriter tests: true 95.2%, false 94.0%, unknown 50.8% (avg 75.7%). Real-world fuel-cell diagnostics zero-shot: 94.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random baseline is 33.3% (balanced 3-way). Ablations: training only to smaller depths/levels yields lower generalization (e.g., D≤2 vs D≤3); DELTA_M (fine-tuned) substantially outperforms zero/few-shot GPTs on full DELTAD; tested robustness vs changed PCFG distributions and vocabulary substitutions (DELTA_T, SoftSymbolic) and compared to performance on ProofWriter and symbolic translations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails or degrades in several targeted cases: (1) significant drop on HardSymbolic (formal DL terminology) to ~58.5% indicating sensitivity to syntactic/formal phrasing, (2) some handcrafted logical-equivalence cases failed (e.g., did not learn A⊓B ⊑ A⊔B and sometimes returned 'unknown' for number-restriction contexts), (3) lower accuracy on 'unknown' labels in external ProofWriter (50.8%) suggesting it may have learned dataset-specific statistical features, (4) dataset generation limited to depth ≤5 and to KBs processible within a 5s delta-closure time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Fine-tuning a modern encoder transformer on a large, carefully generated dataset of expressive logical sentences (ALCQ → NL) enables near-perfect entailment checking and generalization across unseen reasoning depths when training includes examples of depth ≥3; robustness to vocabulary and natural-language phrasing is high, but performance collapses when examples are translated to purely symbolic/formal terminology; suggests that supervised fine-tuning on diverse, expressive synthetic logical NL yields practical reasoning capability, but models may rely on distributional patterns and struggle with strictly symbolic representations and certain logical equivalences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based Language Models for Reasoning in the Description Logic ALCQ', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8636.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8636.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI chat-oriented large language model evaluated in zero-shot and few-shot (up to 9-shot) prompting for entailment checking over DELTAD examples (limited to linguistic complexity L ≤ 1 due to context window limits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained large language model accessed via chat-completion API; evaluated with carefully engineered prompts instructing output 'True', 'False', or 'Unknown'; exact architecture and parameter count not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DELTAD (L ≤ 1) entailment checking; HardSymbolic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same entailment task as DELTAD but experiments constrained to sentences of linguistic complexity L ≤ 1 (context-window limit); additional evaluation on HardSymbolic (symbolic translation) to test behavior on formal terminology.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot prompting (up to 9 shots); prompt design followed OpenAI guidelines and prior deductive reasoning prompts; deterministic settings used (temperature=0, max_tokens=3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On DELTAD (L ≤ 1) GPT-3.5 shows modest zero-shot performance and improves with 9-shot prompting but degrades with increased inference depth (exact per-depth table not printed). On HardSymbolic: 0-shot ~57% average accuracy, 9-shot ~61% average accuracy (100-example averages per depth reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared 0-shot vs 9-shot prompting (9-shot improves performance); compared to DELTA_M (fine-tuned DeBERTa) which achieved much higher accuracies when fine-tuned on the full dataset. Also compared qualitatively to GPT-4 (GPT-4 performs better with same prompting budget).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles as inference depth increases; limited by context-window which prevented evaluation at L ≥ 2; performance drops on symbolic/formal translations (HardSymbolic) and remains sensitive to prompt construction and number of shots; cannot match supervised fine-tuning on large in-distribution data.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Few-shot examples (up to 9) can significantly improve performance on natural-language logical entailment but LLMs like GPT-3.5 still struggle with deeper multi-step logical inferences and with purely symbolic/formal representations; context-window constraints limit evaluation on linguistically complex examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based Language Models for Reasoning in the Description Logic ALCQ', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8636.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8636.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 evaluated in zero-shot and few-shot (9-shot) prompting on DELTAD (L ≤ 1) and on symbolic translations to measure logical reasoning in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained chat-capable language model accessed via API; evaluated deterministically (temperature=0) with brief output constraints; paper does not provide parameter count or internal training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DELTAD (L ≤ 1) entailment checking; HardSymbolic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language entailment checking from ALCQ-derived KBs (restricted to lower linguistic complexity due to window limits) and evaluation on HardSymbolic (formal DL terminology) to probe symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot prompting (9-shot) with an instruction-style system prompt; deterministic settings used to reduce variability (temperature=0, max_tokens=3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On DELTAD (L ≤ 1) with 9-shot prompting GPT-4 achieved substantially improved results; reported 9-shot per-depth accuracy range had a max ≈92% and min ≈77% across depths. On HardSymbolic dataset GPT-4 had ~20% 0-shot and ~65% 9-shot average accuracy (100-example averages).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared 0-shot vs 9-shot (9-shot substantially improves performance). When compared to fine-tuned DeBERTa (DELTA_M), GPT-4 underperforms the supervised model on the full DELTAD distribution, especially when symbolic/formal terminology is used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance degrades with higher inference depth; poor zero-shot performance on symbolic/fomalized inputs (HardSymbolic 0-shot ≈20%); constrained by context window for higher linguistic complexity; still sensitive to prompt quality and shot selection.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4 can reach reasonable accuracy on natural-language logical entailment with a handful of in-context examples (9-shot), but few-shot prompting does not fully bridge the gap to a supervised model trained on large, targeted logical datasets; symbolic/formal terminology and deeper chains of reasoning remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based Language Models for Reasoning in the Description Logic ALCQ', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8636.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8636.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (ProofWriter evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 fine-tuned on ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based seq2seq model fine-tuned on the ProofWriter dataset in prior work to generate entailments and proofs; cited here as a relevant prior result for logical reasoning with transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (text-to-text transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text sequence-to-sequence transformer (Raffel et al., 2020) used in prior work to learn to generate proofs and implications from synthetic NL rule sets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (entailment and proof generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic dataset of implications and proofs under CWA and OWA; tasks include generating implied facts and producing proofs, up to depth 5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning of T5 on the ProofWriter dataset to generate proofs and entailments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prior work (cited) reported high proof-generation accuracy (≈94.8% at depth 5) with a T5-based model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported as state-of-the-art within that prior work for that dataset; this paper cites it as related evidence that TLMs can generate proofs on limited-formal fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in detail in this paper beyond noting that ProofWriter uses different axiom forms and contains elements (e.g., axioms with individual names or negated role assertions) not present in DELTAD training data; cross-dataset generalization issues are implied.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Prior supervised fine-tuning on synthetic reasoning data (ProofWriter) can yield high in-distribution proof-generation performance, but domain/form differences limit cross-dataset generalization; complements this paper's finding that targeted supervised training on expressive, well-designed datasets (like DELTAD) enables strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based Language Models for Reasoning in the Description Logic ALCQ', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8636.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8636.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (evaluated on FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa encoder model was reported (in cited prior work) to perform best among tested TLMs on the FOLIO dataset, though absolute performance remained low, demonstrating limits of transformer models on richer FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FOLIO: natural language reasoning with first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A robustly optimized BERT-style encoder (Liu et al., 2019) fine-tuned for NLI-style logical reasoning tasks in prior work (FOLIO evaluations); parameter counts vary by variant (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO (first-order logic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A small dataset (~1.4K examples) derived from first-order logic sentences (without numeric restrictions) to probe NL-to-FOL reasoning capabilities of TLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning of RoBERTa on the FOLIO benchmark (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prior work concluded RoBERTa performed best among the models they tested on FOLIO but overall performance remained 'low' (no precise numbers given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to other transformer models tested in the FOLIO paper (including generative models), RoBERTa ranked highest in that experiment but still had limited absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Indicates transformer models struggle on richer FOL reasoning tasks; limited dataset size, absence of number restrictions, and restricted expressivity constrain conclusions; referenced here to contextualize DELTAD's greater expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Existing encoder transformers may be the best among tested options on some FOL datasets, but absolute performance is weak, motivating larger and more expressive benchmarks (like DELTAD) and targeted fine-tuning to advance strict logical reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based Language Models for Reasoning in the Description Logic ALCQ', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language. <em>(Rating: 2)</em></li>
                <li>Diagnosing the first-order logical reasoning ability through LogicNLI. <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic. <em>(Rating: 2)</em></li>
                <li>Large language models are in-context semantic reasoners rather than symbolic reasoners. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8636",
    "paper_id": "paper-273346003",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "DELTA_M",
            "name_full": "DELTA_M (DeBERTaV3-large fine-tuned on DELTAD)",
            "brief_description": "A supervised DeBERTaV3-large model fine-tuned on the DELTAD dataset to perform 3-way entailment checking (True/False/Unknown) over natural-language translations of ALCQ description-logic KBs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTaV3-large (microsoft/deberta-v3-large)",
            "model_description": "Encoder-only transformer pretrained (DeBERTaV3) and fine-tuned as a multi-class classifier; input format [CLS] context [SEP] question [SEP]; trained with AdamW, mixed precision (FP16) on two A100 GPUs.",
            "model_size": "304M",
            "reasoning_task_name": "DELTAD (DEscription Logics with TrAnsformers) entailment checking",
            "reasoning_task_description": "A synthetic dataset (384K examples) mapping ALCQ description-logic KBs and queries into natural language; task: decide whether a query is entailed (True), contradicted (False), or not provable (Unknown) under the open-world assumption, across multiple inference depths (D ∈ {0,1,2,3,5}) and linguistic complexity levels (L ∈ {0..3}).",
            "method_or_approach": "Supervised fine-tuning of DeBERTaV3-large on DELTAD; trained variants DELTA_i,j with data up to inference depth i and linguistic complexity j to probe generalization; additional evaluations include zero-shot tests on other distributions, symbolic translations (SoftSymbolic, HardSymbolic), and a tweaked distribution (DELTA_T).",
            "performance": "On its own held-out test (D ≤ 5, L ≤ 3) DELTA_M achieved 99.7% accuracy overall; models trained to D ≤ 5 nearly reach 99.5–100% across depths. Generalization: models trained up to D ≤ 2 achieve ~83.5% on D=5 questions; models trained up to D ≤ 3 achieve ~98.5% on D=5. Robustness: DELTA_M scored 99.6% on the tweaked DELTA_T; SoftSymbolic 95.3% (avg); HardSymbolic dropped to 58.5% (avg). On external ProofWriter tests: true 95.2%, false 94.0%, unknown 50.8% (avg 75.7%). Real-world fuel-cell diagnostics zero-shot: 94.0%.",
            "baseline_comparison": "Random baseline is 33.3% (balanced 3-way). Ablations: training only to smaller depths/levels yields lower generalization (e.g., D≤2 vs D≤3); DELTA_M (fine-tuned) substantially outperforms zero/few-shot GPTs on full DELTAD; tested robustness vs changed PCFG distributions and vocabulary substitutions (DELTA_T, SoftSymbolic) and compared to performance on ProofWriter and symbolic translations.",
            "limitations_or_failures": "Fails or degrades in several targeted cases: (1) significant drop on HardSymbolic (formal DL terminology) to ~58.5% indicating sensitivity to syntactic/formal phrasing, (2) some handcrafted logical-equivalence cases failed (e.g., did not learn A⊓B ⊑ A⊔B and sometimes returned 'unknown' for number-restriction contexts), (3) lower accuracy on 'unknown' labels in external ProofWriter (50.8%) suggesting it may have learned dataset-specific statistical features, (4) dataset generation limited to depth ≤5 and to KBs processible within a 5s delta-closure time budget.",
            "insights_or_conclusions": "Fine-tuning a modern encoder transformer on a large, carefully generated dataset of expressive logical sentences (ALCQ → NL) enables near-perfect entailment checking and generalization across unseen reasoning depths when training includes examples of depth ≥3; robustness to vocabulary and natural-language phrasing is high, but performance collapses when examples are translated to purely symbolic/formal terminology; suggests that supervised fine-tuning on diverse, expressive synthetic logical NL yields practical reasoning capability, but models may rely on distributional patterns and struggle with strictly symbolic representations and certain logical equivalences.",
            "uuid": "e8636.0",
            "source_info": {
                "paper_title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "gpt-3.5-turbo-1106",
            "brief_description": "An OpenAI chat-oriented large language model evaluated in zero-shot and few-shot (up to 9-shot) prompting for entailment checking over DELTAD examples (limited to linguistic complexity L ≤ 1 due to context window limits).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_description": "Pretrained large language model accessed via chat-completion API; evaluated with carefully engineered prompts instructing output 'True', 'False', or 'Unknown'; exact architecture and parameter count not provided in this paper.",
            "model_size": null,
            "reasoning_task_name": "DELTAD (L ≤ 1) entailment checking; HardSymbolic evaluation",
            "reasoning_task_description": "Same entailment task as DELTAD but experiments constrained to sentences of linguistic complexity L ≤ 1 (context-window limit); additional evaluation on HardSymbolic (symbolic translation) to test behavior on formal terminology.",
            "method_or_approach": "Zero-shot and few-shot prompting (up to 9 shots); prompt design followed OpenAI guidelines and prior deductive reasoning prompts; deterministic settings used (temperature=0, max_tokens=3).",
            "performance": "On DELTAD (L ≤ 1) GPT-3.5 shows modest zero-shot performance and improves with 9-shot prompting but degrades with increased inference depth (exact per-depth table not printed). On HardSymbolic: 0-shot ~57% average accuracy, 9-shot ~61% average accuracy (100-example averages per depth reported).",
            "baseline_comparison": "Compared 0-shot vs 9-shot prompting (9-shot improves performance); compared to DELTA_M (fine-tuned DeBERTa) which achieved much higher accuracies when fine-tuned on the full dataset. Also compared qualitatively to GPT-4 (GPT-4 performs better with same prompting budget).",
            "limitations_or_failures": "Struggles as inference depth increases; limited by context-window which prevented evaluation at L ≥ 2; performance drops on symbolic/formal translations (HardSymbolic) and remains sensitive to prompt construction and number of shots; cannot match supervised fine-tuning on large in-distribution data.",
            "insights_or_conclusions": "Few-shot examples (up to 9) can significantly improve performance on natural-language logical entailment but LLMs like GPT-3.5 still struggle with deeper multi-step logical inferences and with purely symbolic/formal representations; context-window constraints limit evaluation on linguistically complex examples.",
            "uuid": "e8636.1",
            "source_info": {
                "paper_title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4 evaluated in zero-shot and few-shot (9-shot) prompting on DELTAD (L ≤ 1) and on symbolic translations to measure logical reasoning in-context.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_description": "Large pretrained chat-capable language model accessed via API; evaluated deterministically (temperature=0) with brief output constraints; paper does not provide parameter count or internal training details.",
            "model_size": null,
            "reasoning_task_name": "DELTAD (L ≤ 1) entailment checking; HardSymbolic evaluation",
            "reasoning_task_description": "Natural-language entailment checking from ALCQ-derived KBs (restricted to lower linguistic complexity due to window limits) and evaluation on HardSymbolic (formal DL terminology) to probe symbolic reasoning.",
            "method_or_approach": "Zero-shot and few-shot prompting (9-shot) with an instruction-style system prompt; deterministic settings used to reduce variability (temperature=0, max_tokens=3).",
            "performance": "On DELTAD (L ≤ 1) with 9-shot prompting GPT-4 achieved substantially improved results; reported 9-shot per-depth accuracy range had a max ≈92% and min ≈77% across depths. On HardSymbolic dataset GPT-4 had ~20% 0-shot and ~65% 9-shot average accuracy (100-example averages).",
            "baseline_comparison": "Compared 0-shot vs 9-shot (9-shot substantially improves performance). When compared to fine-tuned DeBERTa (DELTA_M), GPT-4 underperforms the supervised model on the full DELTAD distribution, especially when symbolic/formal terminology is used.",
            "limitations_or_failures": "Performance degrades with higher inference depth; poor zero-shot performance on symbolic/fomalized inputs (HardSymbolic 0-shot ≈20%); constrained by context window for higher linguistic complexity; still sensitive to prompt quality and shot selection.",
            "insights_or_conclusions": "GPT-4 can reach reasonable accuracy on natural-language logical entailment with a handful of in-context examples (9-shot), but few-shot prompting does not fully bridge the gap to a supervised model trained on large, targeted logical datasets; symbolic/formal terminology and deeper chains of reasoning remain challenging.",
            "uuid": "e8636.2",
            "source_info": {
                "paper_title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "T5 (ProofWriter evaluation)",
            "name_full": "T5 fine-tuned on ProofWriter",
            "brief_description": "A T5-based seq2seq model fine-tuned on the ProofWriter dataset in prior work to generate entailments and proofs; cited here as a relevant prior result for logical reasoning with transformers.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "mention_or_use": "mention",
            "model_name": "T5 (text-to-text transformer)",
            "model_description": "Text-to-text sequence-to-sequence transformer (Raffel et al., 2020) used in prior work to learn to generate proofs and implications from synthetic NL rule sets.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (entailment and proof generation)",
            "reasoning_task_description": "Synthetic dataset of implications and proofs under CWA and OWA; tasks include generating implied facts and producing proofs, up to depth 5.",
            "method_or_approach": "Supervised fine-tuning of T5 on the ProofWriter dataset to generate proofs and entailments.",
            "performance": "Prior work (cited) reported high proof-generation accuracy (≈94.8% at depth 5) with a T5-based model.",
            "baseline_comparison": "Reported as state-of-the-art within that prior work for that dataset; this paper cites it as related evidence that TLMs can generate proofs on limited-formal fragments.",
            "limitations_or_failures": "Not discussed in detail in this paper beyond noting that ProofWriter uses different axiom forms and contains elements (e.g., axioms with individual names or negated role assertions) not present in DELTAD training data; cross-dataset generalization issues are implied.",
            "insights_or_conclusions": "Prior supervised fine-tuning on synthetic reasoning data (ProofWriter) can yield high in-distribution proof-generation performance, but domain/form differences limit cross-dataset generalization; complements this paper's finding that targeted supervised training on expressive, well-designed datasets (like DELTAD) enables strong performance.",
            "uuid": "e8636.3",
            "source_info": {
                "paper_title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RoBERTa (FOLIO)",
            "name_full": "RoBERTa (evaluated on FOLIO)",
            "brief_description": "RoBERTa encoder model was reported (in cited prior work) to perform best among tested TLMs on the FOLIO dataset, though absolute performance remained low, demonstrating limits of transformer models on richer FOL tasks.",
            "citation_title": "FOLIO: natural language reasoning with first-order logic.",
            "mention_or_use": "mention",
            "model_name": "RoBERTa",
            "model_description": "A robustly optimized BERT-style encoder (Liu et al., 2019) fine-tuned for NLI-style logical reasoning tasks in prior work (FOLIO evaluations); parameter counts vary by variant (not specified here).",
            "model_size": null,
            "reasoning_task_name": "FOLIO (first-order logic reasoning)",
            "reasoning_task_description": "A small dataset (~1.4K examples) derived from first-order logic sentences (without numeric restrictions) to probe NL-to-FOL reasoning capabilities of TLMs.",
            "method_or_approach": "Supervised fine-tuning of RoBERTa on the FOLIO benchmark (prior work).",
            "performance": "Prior work concluded RoBERTa performed best among the models they tested on FOLIO but overall performance remained 'low' (no precise numbers given in this paper).",
            "baseline_comparison": "Compared to other transformer models tested in the FOLIO paper (including generative models), RoBERTa ranked highest in that experiment but still had limited absolute accuracy.",
            "limitations_or_failures": "Indicates transformer models struggle on richer FOL reasoning tasks; limited dataset size, absence of number restrictions, and restricted expressivity constrain conclusions; referenced here to contextualize DELTAD's greater expressivity.",
            "insights_or_conclusions": "Existing encoder transformers may be the best among tested options on some FOL datasets, but absolute performance is weak, motivating larger and more expressive benchmarks (like DELTAD) and targeted fine-tuning to advance strict logical reasoning capabilities.",
            "uuid": "e8636.4",
            "source_info": {
                "paper_title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Transformers as soft reasoners over language.",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Diagnosing the first-order logical reasoning ability through LogicNLI.",
            "rating": 2,
            "sanitized_title": "diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic.",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Large language models are in-context semantic reasoners rather than symbolic reasoners.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_incontext_semantic_reasoners_rather_than_symbolic_reasoners"
        }
    ],
    "cost": 0.019428499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformer-based Language Models for Reasoning in the Description Logic ALCQ
12 Oct 2024</p>
<p>Angelos Poulis apoulis@bu.edu 
Dept. of Computer Science
Boston University</p>
<p>Eleni Tsalapati etsalapati@atc.gr 
Athens Technology Center 3 AI Team
Dept. of Informatics and Telecommunications
National and Kapodistrian University of Athens</p>
<p>Manolis Koubarakis koubarak@di.uoa.gr 
Archimedes/Athena RC
Greece</p>
<p>Transformer-based Language Models for Reasoning in the Description Logic ALCQ
12 Oct 202455B89404F256ADEDD9D577AF6532B9AEarXiv:2410.09613v1[cs.CL]
Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities.Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order logic sentences with only a few logical operators and quantifiers.We construct the natural language dataset, DELTAD, using the expressive description logic language ALCQ.DELTAD comprises 384K examples and increases in two dimensions: i) reasoning depth, and ii) linguistic complexity.In this way, we systematically investigate the logical reasoning capabilities of a supervised fine-tuned DeBERTabased model and two large language models (GPT-3.5,GPT-4) with few-shot prompting.We show that the DeBERTabased model fine-tuned on our dataset can master the entailment checking task.Moreover, the performance of GPTs can improve significantly even when a small number of samples is provided (9 shots).We open-source our code and datasets.</p>
<p>Introduction</p>
<p>Description Logic (DL) languages (Baader et al., 2003) are fragments of first-order logic (FOL) that have evolved into one of the main formalisms for the representation of conceptual knowledge in a precise and well-defined manner.An expressive and decidable DL language that supports existential, universal, and numerical constraints besides the standard Boolean operators is ALCQ.For instance, in ALCQ one can formally express sentences like the ones appearing in Fig. 1.</p>
<p>The formal apparatus of DLs allows us to perform deductive reasoning tasks, such as entailment checking, i.e., deciding whether a sentence or a set of sentences, logically implies another.Recent advancements in transformer-based language models (TLMs) have sparked new research into whether TLMs can learn to perform such tasks over contexts expressed in natural language (Clark, Tafjord, and Richardson, 2020;Han et al., 2022;Tang et al., 2023;He et al., 2023).However, in most cases, the contexts used were either Facts Erin eats Dave.Bob admires none.Fiona loves at least three people that are smart or not orange or that eat at most three not cold people or that chase someone not kind.composed of relatively short sentences, simple in structure (i.e., their formal representations contain only a few logical operators and quantifiers) (Clark, Tafjord, and Richardson, 2020;Tian et al., 2021;Saparov and He, 2023), or they were of limited size (Tian et al., 2021;Han et al., 2022).This work aims to answer the fundamental question: "How well can TLMs perform inference over contexts produced from an expressive DL language, like ALCQ?".Following this, a subsequent research question in line with the literature (Clark, Tafjord, and Richardson, 2020;Tafjord, Dalvi, and Clark, 2021) but focusing on higher expressivity, is: "Is the performance of TLMs affected by the reasoning depth required to perform the inference process?".A third research question arises about whether the fragment of the formal language used is sufficient to evaluate the reasoning capabilities of a model.For instance, all sentences in Fig. 1 can be formally expressed within ALCQ, yet some are linguistically more complex than others.It is expected that contexts mostly containing complex sentences would be more challenging to process.Thus, the third research question of this paper is: "Is the performance of TLMs affected by the linguistic complexity of the context?".</p>
<p>As discussed by Madusanka et al. (2023), the most appropriate reasoning problem for assessing the impact of language constructs (like quantifiers and negation) is textual entailment checking.This involves checking entailment in natural language from a purely logical perspective, eliminating the influence of any background or commonsense knowledge.</p>
<p>To answer the research questions posed, we have created the synthetic dataset DELTA D (DEscription Logics with TrAnsformers) of 384K examples (context-questionanswer-depth-linguistic complexity) based on ALCQ, where the question is the statement that we check whether it is logically deduced from the context, under the open world assumption.The synthetic nature of the dataset, aside from isolating commonsense/background knowledge, enables us to systematically examine the performance of TLMs, as DELTA D gradually increases in both reasoning depth and linguistic complexity.Additionally, it allows us to eliminate obvious statistical features, such as the correlation between the answer "False" and the word "not" in the sentence in question.</p>
<p>We conducted a systematic evaluation of the textual entailment checking capabilities of supervised fine-tuned De-BERTa and few-shot prompting on large language models (GPT-3.5,GPT-4) over DELTA D .Our results show that the performance of the DeBERTa-based model, DELTA M , remains consistently high (reaching 99.7% on its test set) when the reasoning depth increases (differently from Tian et al. (2021)) or the linguistic complexity of the sentences increases.To ensure that DELTA M does not overfit on DELTA D , and inspired by Zhang et al. (2022), we changed the probability distributions used for the dataset generation and the accuracy of the model remained equally good.Additionally, tests to similar datasets (Tafjord, Dalvi, and Clark, 2021) returned good results.</p>
<p>To check the impact of semantics on DELTA M , we followed the approach of Tang et al. (2023) generating a symbolic "translation" of DELTA D by replacing the words used in the synthetic dataset to symbols.Zero-shot testings of DELTA M show that in contrast to Tang et al. (2023), its performance remained consistent, while the accuracy of GPT models slightly decreased.This suggests that DELTA M 's performance is not influenced by the dataset's semantics (as expected, given the dataset's nonsensical nature).However, when the dataset was further translated to resemble the language used for describing description logic sentences, the models' accuracy significantly dropped.Finally, successful testing of DELTA M in a real-world scenario (fuel cell system diagnostics) demonstrates the potential of TLMs to be used in rule-based system diagnostics.</p>
<p>Overall, we make the following contributions: (C 1 ) We introduce the first extensive description logic benchmark consisting of 384K examples.This is a significant contribution because building large benchmarks over expressive logic languages, like ALCQ, is a challenging task as it requires performing query answering with logic reasoners, a process that can be very time-consuming (∼ 1 min.for KBs with long subsumption axioms/facts of our dataset).Both the dataset and the code for its generation are openly available1 .</p>
<p>(C 2 ) We show that TLMs can achieve very high accuracy in entailment checking over synthetic natural language contexts generated from ALCQ sentences.This demonstrates the potential of TLMs to be utilized for scalable reasoning tasks over vast KBs, thus bypassing formal representations required by traditional knowledge-based systems.</p>
<p>(C 3 ) We show that the performance of TLMs is not affected by the linguistic complexity of the contexts.</p>
<p>(C 4 ) We show that DeBERTa-based models are not affected by the dataset's vocabulary.</p>
<p>(C5) We show how these contributions can be leveraged in a real-world use-case scenario.</p>
<p>Background on Description Logics</p>
<p>We can use ALCQ (Baader et al., 2003) to represent knowledge about a domain by defining three types of entities: individuals (e.g., John), concepts (e.g., Postdoc, i.e., the concept describing the entities that are postdocs) and roles (e.g., teaches).A concept expression C can be formed using these entities, Boolean constructors (⊓, ⊔, ¬), quantifiers (∀, ∃), and number restrictions (≤, ≥) recursively as follows:
C, D := A | ⊤ | ⊥ | ¬C | C ⊓ D | C ⊔ D | ∀R.C | ∃R.C |≥ nR.C |≤ nR.C
, where A is an atomic concept, R an atomic role, ⊤ the top concept, which has every individual as an instance, and ⊥ the dual of ⊤.In this way, one can represent formally complex concept expressions, such as all entities that "have a Ph.D., teach at most two postgraduate courses and are not academics" (∃hasDegree.PhD⊓ ≤ 2teaches.PostgrCourse ⊓ ¬Academic).Subsumption axioms in ALCQ have the form C ⊑ D and describe relationships between concept expressions.For example, one can describe formally that all postdocs are described by the aforementioned concept as Postdoc ⊑ ∃owns.PhD⊓ ≤ 2teaches.PostgrCourse ⊓ ¬Academic.We denote with LHS (left-hand side) the concept expression that appears on the left of the subsumption symbol (⊑) in a subsumption axiom and with RHS (right-hand side) the concept expression that appears on the right.Assertional axioms or, simply, facts describe knowledge about named individuals, i.e., that are instances of some concept (expression) and have the form C(a) or R(a, b), where a, b individuals.Using complex expressions one can construct very complex facts.An ALCQ knowledge base (KB) is a set of subsumption axioms and a set of facts.</p>
<p>Delta-closure(K, t) of a KB K is the set of subsumption axioms and facts that are inferred from K within time t seconds using the InferredOntologyGenerator 2 , and no more axioms or facts are inferred after t3 .Given a KB K and a subsumption axiom or a fact a, we say that K entails a (subsumption axiom or fact) if every model of K (i.e., if every interpretation that satisfies all subsumption axioms and facts of K) is also a model of a. Entailment checking can be considered as the prototypical reasoning task for querying knowledge: we check whether some statement is necessarily true, presuming the statements of the knowledge base.Following the semantics of DLs, we make the open-world assumption, i.e., missing information is treated as unknown.Supposing that K is transformed in negated normal form, we consider inferrence depth, or simply depth of a with respect to K, depth(a, K), as the size of the justification (Horridge, Parsia, and Sattler, 2008) for a, i.e., the minimum number of subsumption axioms and facts in K that can be used to logically deduce that a is true or false.If none of the two can be deduced, the answer is "unknown" and a is not characterized by any depth.</p>
<p>Dataset Generation</p>
<p>We investigate the ability of transformers to perform textual entailment checking over ALCQ KBs expressed in natural language with respect to two dimensions: i) the depth D of the sentences (i.e., subsumption axioms/facts in question), henceforth mentioned as queries, with respect to the corresponding KB, ii) the linguistic complexity level L (defined in Section 3.1) of the knowledge required to answer the queries.To achieve this, each example in the dataset DELTA D is a 5-tuple ⟨T , Q, A, D, L⟩, where T is the context containing ALCQ axioms (subsumption axioms/facts) expressed in natural language, Q the query expressed in natural language, henceforth mentioned as question, A is the answer which can be either true, false, or unknown, and D the depth of Q, if A is true or false, otherwise it is denoted as na.L is the linguistic complexity of the KB4 .</p>
<p>The pipeline for the generation of the dataset is presented in Fig. 2. For the generation of an example (described in detail in Section 3.1) of linguistic complexity level n (L ≤ n) and depth m (D ≤ m), we first generate a KB K using a specially crafted probabilistic context-free grammar (denoted in Fig. 2 with ALCQ-n PCFG) for producing subsumption axioms and facts of maximum linguistic complexity n.Then, Delta-closure(K, t) is calculated for t = 5 sec.KBs that required more than 5 seconds to calculate the Delta-closure, were discarded.</p>
<p>From the Delta-closure we calculate, as described in Section 3.2, true (answer=true), false (answer=false) and unknown (answer=unknown) queries, which eventually will formulate the sentences in question.A KB is kept only if it can produce queries with all three types of answers at all depths up to m, otherwise a new one is generated.Once this process is completed, the generated queries (subsump-tion axioms/facts) along with the original K, are translated into natural language statements Q and into the context T , respectively, by utilizing a set of natural language templates, as described in Section 3.3.</p>
<p>KB Generation</p>
<p>To create diverse contexts and to avoid overfitting to a specific vocabulary, we have defined two different pools of terms, Pool A and Pool B. Pool A contains 14 atomic concepts, 5 roles, and 8 individuals, mostly taken from Rule-Taker dataset (Clark, Tafjord, and Richardson, 2020)(in RuleTaker the subsumption axioms are simple conjunctive implications, where the concept names are named "attributes", the roles "relations" and the individual names "entities").Pool B contains 8 atomic concepts, 8 roles, and 8 individuals.Both pools can be found in A.1.</p>
<p>From each pool, we generate 20 datasets (40 in total) of 1000 KBs each, of various inference depths and axiom lengths.</p>
<p>To obtain KBs of different linguistic complexity levels, we have manually crafted four types (L = 0, 1, 2, 3) of PCFGs, based on the number of constructors and quantifiers appearing in their axioms.In general, a concept of linguistic complexity L contains L Boolean constructors and at most L + 1 quantifiers.</p>
<p>An L-type PCFG produces KBs of linguistic complexity level L with axioms that their one side (e.g., LHS) is of linguistic complexity L and their other side (e.g., RHS) of at most L − 1, but also contains simpler axioms, of smaller linguistic complexity levels.Specifically, A KB of level:
• L = 0 contains axioms of the form Level 0 ⊑ Level 0 , • L = 1 contains axioms of the form Level 0 ⊑ Level 1 , Level 1 ⊑ Level 0 , Level 1 ⊑ Level 1 , • L = 2 contains axioms of the form Level 0 ⊑ Level 2 , Level 2 ⊑ Level 0 , Level 1 ⊑ Level 2 , Level 2 ⊑ Level 1 , • L = 3 contains axioms of the form Level 0 ⊑ Level 3 , Level 1 ⊑ Level 3 , Level 2 ⊑ Level 3 , Level 3 ⊑ Level 0 , Level 3 ⊑ Level 1 , Level 3 ⊑ Level 2 .
For instance, KBs of level L = 0 contain only very simple facts or subsumption axioms that do not contain any Boolean constructors but can contain one quantifier, such as Enthusiastic ⊑ ∃supports.Enthusiastic (translated in NL as "Enthusiastic people support someone enthusiastic"), but KBs of level L = 3 can contain subsumption axioms as complex as the first one appearing in Fig. 1.</p>
<p>It is important to discern the notion of linguistic complexity of a sentence from its length.We do not focus here only on sentences that contain, for instance, multiple conjunctions but rather on sentences with a more complex structure (with quantifiers as well), leading to increased linguistic complexity.</p>
<p>To keep the KBs processible by the reasoners, the subsumption axioms can contain up to seven atomic concepts and up to two nested quantifiers (e.g., ∃likes.(∃loves.Cat)), which describes the entities that like some entity that loves some cat).All KBs are rather small (with a minimum of 3
ABoxAssertion -&gt; ConceptAssertion | RoleAssertion TBoxAxiom -&gt; ConceptInclusion ConceptInclusion -&gt; InclusionL0 [0.1] | InclusionL1 [0.15] | InclusionL2 [0.15] | InclusionL3 [0.3] | SpecialAxiom [0.3]</p>
<p>Query Generation</p>
<p>For an inference depth D, a true query q is an axiom or fact selected from the Delta-closure of a consistent K, such that depth(q, K) = D.An unknown query (answer=unknown) is generated by creating a random fact or statement (using the corresponding PCFG) such that it does not belong to the Delta-closure of K and is consistent with K.A false query (answer=false) can be generated in three ways:</p>
<p>• From an inconsistent K: for every a ∈ K if K \ {a} is consistent then a is a false query over the KB K \ {a}.• From a consistent K: i) By negating a true query q with depth(q, K) = D (and applying De Morgan's laws).ii) By automatically generating an appropriate axiom or fact a such that K ∪ {a} is inconsistent and depth(a, K) = D.</p>
<p>For instance, suppose that a KB K 1 contains the axioms (∀admires.⊥)(Anne)and ∀admires.⊥⊑ ∀likes.Quiet which in natural language are translated into: "Anne admires none", "All people that admire none like only quiet people".Then, the fact (∃likes.¬Quiet)(Anne)stating that "Anne likes someone who is not quiet" forms a false query for K.</p>
<p>The disadvantage of the first approach is that it requires calling the reasoner multiple times, a time-consuming process, especially in KBs with long axioms (e.g., L=3 KBs).Hence, we used the two latter approaches.</p>
<p>We set the reasoning depth limit to five (i.e., D = 0, 1, 2, 3, 5) following the literature (Clark, Tafjord, and Richardson, 2020).Additionally, extending this further would require longer times for the dataset generation.</p>
<p>Data Translation to NL</p>
<p>The KBs and queries were translated to NL with the use of templates.The templates were created based on the user-friendly Manchester syntax for ALCQ (Horridge et al., 2006).Following this syntax, the intersection (⊓) and union (⊔) operators, are translated as "and" and "or", respectively, the existential (∃) quantifier is translated as "someone" or "something" (depending on whether the pool is about people or things), the universal (∀) as "only", and the number restrictions ≤, ≥ as "at most" and "at least".Also, we use the word "that" for intersections and nested quantifiers.For instance, the fact (∃likes.(∀likes.Kind))(Bob) is translated as "Bob likes someone that likes only kind people".</p>
<p>Following the template-based approach suggested by Tafjord, Dalvi, and Clark (2021), the axioms of the form C ⊑ D are, roughly, translated into NL in four different ways: i)"If C then D"; ii)"People/Things that are C are D", iii)"All people/things that are C are D"; iv) If C = ⊤ and D = ∀R.C ′ this is translated as "Someone/something can R only people/things that are C ′ ".A fact C(a) is translated as "a is C".To ensure that the resulting NL sentences are grammatically correct we have used a grammar checker5 .</p>
<p>The Dataset DELTA D</p>
<p>At the end, the examples of the same depth and level from both pools are merged.This results in 20 datasets of 2000 KBs each, with each resulting dataset containing sentences from both vocabularies.From each KB we generated three queries (true, false, unknown) for each depth (D = {0, 1, 2, 3, 5}), i.e., from each KB we generated 3 × (d + 1), d ∈ D, queries.So, in total, the dataset contains
Σ d∈D 3×(d+1)×2000×(L max +1) = 384K
examples, as we generate KBs for each linguistic complexity level ranging from zero up to L max = 3.</p>
<p>Statistical Features</p>
<p>As it is thoroughly discussed by Zhang et al. (2022), it is impossible to eliminate all statistical features that exist in data, besides, some of them inherently exist in logical reasoning problems.</p>
<p>However, DELTA D is balanced with respect to some of the most obvious features: i) KB size: From the same KB we extract all three types of questions (true, false, unknown); ii) Inference depth: We keep a KB only if it can provide all three types of questions with the same inference depth; iii) Formulation of the question: The translation to natural language is implemented in such a way that the word "not" appears almost equal number of times in true questions (52.39%), false questions (50.71%) and unknown questions (46.60%); iv) Average length in words: True questions 10.85, false questions 8.97, unknown questions 10.34.</p>
<p>Experiments</p>
<p>We systematically tested the entailment checking ability of supervised fine-tuned DeBERTaV3-large, due to its recent advancements in NLU tasks (He, Gao, and Chen, 2023).We also tested in zero-shot and few-shot prompting the models GPT-3.5 (gpt-3.5-turbo-1106)and GPT-4 (gpt-4) from OpenAI, as they have demonstrated strong performance across various reasoning benchmarks (OpenAI, 2023).Our limited resources did not allow us to test the performance of other models, like the Llama family 6 ; we plan to do this in future work.</p>
<p>DeBERTa-based Models</p>
<p>Evaluation Setup We fine-tuned the DeBERTaV3-large to predict true/false/unknown (i.e., multi-class sentence classification) for each example.A context-question pair was supplied to the model as [CLS] context [SEP] question [SEP].We used accuracy as the evaluation metric.The test data has an equal balance of true/false/unknown answers, hence the baseline of random guessing is 33.3%.The specifics of the chosen hyper-parameters, which we maintained consistently throughout our experiments, can be found in A.2.</p>
<p>For each combination of depth and level, we trained different models on subsets of DELTA D .A model DELTA i,j is trained in examples of reasoning depth up to i and of linguistic complexity level up to j.For instance, the model DELTA 3,2 has been trained to depths up to 3 and linguistic complexity levels up to 2. The final model DELTA M has been trained to all depths and all linguistic complexity levels, i.e., DELTA M =DELTA 5,3 .For all datasets, we partitioned the data into 70%/10%/20% splits for train/validation/test sets.</p>
<p>6 https://llama.meta.com/Evaluation Results Table 1 illustrates the performance of DELTA models when trained on up to L ≤ 3 linguistic complexity over the various inference depths (the results for smaller levels are presented in A.3).For instance, the column DELTA 0,3 shows the performance of the model trained on all levels in depth 0. Test (own) represents the (held out) test set of the dataset that the model has been trained on.The D 5,3 dataset has questions from all inference depths (D ≤ 5) of all levels (L ≤ 3)."Depth N/A" refers to the unknown questions, as these are not provable."D = 0" to "D = 5" lines represent subsets of D 5,3 of 0-reasoning depth to 5reasoning depth, respectively.It is observed that models trained even in D ≤ 2 datasets generalize quite well in larger depths (83.5% for questions of D = 5), while when trained in D ≤ 3 datasets they show impressive generalization ability (98.5% for questions of D = 5).Finally, we observe that the model trained in D ≤ 5 datasets almost masters (99.5-100%) the reasoning task for all reasoning depths and linguistic complexity levels.
D = 0 D ≤ 1 D ≤ 2 D ≤ 3 D ≤ 5 L = 0
Table 2 demonstrates the performance of each model DELTA i,j when tested on Test (own).For instance, the cell that corresponds to D = 0, L = 0 shows the accuracy of the model DELTA 0,0 .We observe that for all depths D = 0 to D ≤ 5 the models are robust across levels, hence increasing linguistic complexity does not affect their performance.</p>
<p>We, also, partitioned the dataset to the various depths, i.e., we extracted from DELTA D five datasets which contain only data of depth D = i (of all levels) and not up to i. Additionally, we trained a model on a set of 3, 200 examples specifically at depth 3 for all linguistic complexity levels (L ≤ 3).The accuracy when tested in questions of depth 3 was 97.5%, it slightly dropped when tested in questions of smaller depths (D = 1, 2) to ∼ 94.5%, except for the look-up questions (D = 0), where the accuracy reached 99.0%.The model showed even better performance (∼ 97.8%) in larger depths (D = 4, 5).Differently from the findings of Tian et al. (2021), these results demonstrate the model's capacity for generalization across both smaller and larger reasoning depths than those encountered during training.</p>
<p>Zero-shot Performance of DELTA M on Other Distributions.</p>
<p>Results for Tweaked Dataset.We generated the new dataset DELTA T by changing the probability distributions of the PCFG for L = 3 as follows: We increased the probability of the universal quantifier (∀) from 0.33 to 0.70 and the probability of the disjunction (⊔) from 0.50 to 0.80.DELTA T contains 1, 200 examples of up to reasoning depth D ≤ 5.This tweaking has resulted in sentences with 0.8/sentence disjunctions and 0.62/sentence universals.The accuracy of DELTA M on DELTA T was 100.0% for both true and false questions, 98.9% for unknown questions, and, 99.6% overall.As it is evident, the model is robust according to this tweaked distribution.</p>
<p>Results for ProofWriter Dataset (Tafjord, Dalvi, and Clark, 2021).The reason for choosing this dataset is that it was generated in a similar way (using PCFGs) as DELTA D , it is under the open-world assumption and, partly, we have used the same pool of terms (Pool A).DELTA M demonstrated very high accuracy in true questions (95.2%) and false questions (94.0%) but low accuracy (50.8%) in unknown questions.On average the accuracy was 75.7%.The very high accuracy for true/false questions is a surprising result as although DELTA D and ProofWriter have many common types of subsumption axioms/facts, ProofWriter also contains subsumption axioms that involve individual names (e.g., "If Bob is blue then Erin is red") and negated role assertions (e.g., "Bob does not like Erin"), which are not supported by ALCQ and therefore are not contained in the training set of DELTA M .The low performance of DELTA M in unknown questions can be attributed to the different generation processes among the two datasets.According to the generation process described in Tafjord, Dalvi, and Clark (2021) (the source code is not openly available), the unknown questions in ProofWriter contain terms that appear in the context, whereas, as described in Section 3.2, unknown questions in DELTA D are formulated by choosing random terms from the corresponding pool, thus they can be completely irrelevant to the context.Hence, we can assume this is a statistical feature that DELTA M may have learned.</p>
<p>Results for Use Case Scenario.We utilized the ontology subsumption axioms and facts (generated from lab experiments) from Tsalapati et al. (2021) and generated 1, 500 examples for fuel cell system diagnostics.The context contained subsumption axioms of the form "If a system is in a state that is described by a low voltage value that is a result of an observation made by some voltage sensor that is a reliable sensor then the system is under some flooding" and facts of the form "v1 is a high voltage value".Again, DELTA M performed particularly well (94.0%zero-shot).The full dataset is openly available in the provided URL.</p>
<p>Handcrafted Quality Tests of DELTA D To test the quality of DELTA D on which DELTA M is trained, we created simple test examples based on some of the most important knowledge base equivalences according to Rudolph (2011).</p>
<p>For instance, for the conjunction subsumption axiom, we provided the context: "Anne is red and green" and the two (true) questions: "Anne is red" and "Anne is green".The model performs well overall, answering correctly 24/29 questions, however, it seems that in contexts involving number restrictions, it returned the answer "unknown", which in two out of the six cases was wrong (notice though that the set of questions with number restrictions in DELTA D was balanced with respect to their answers).</p>
<p>Additionally, it failed to learn the property A⊓B ⊑ A⊔B.This became evident through the test: "Context: Anne is red and green.Question: Anne is red or green.Answer: True.Prediction: Unknown".To find where it fails in the reasoning chain, we asked the model the intermediate sentences "Anne is green" and "Anne is red", to which it returned (correctly) the answer "true", but it returned "unknown" to the question "If someone is red and green then they are red or green".Whereas, in the test "Context: Anne is red.Anne is green.Question: Anne is red or green." the predicted answer was, again, falsely, "unknown".The complete set of these tests is available in the provided URL.</p>
<p>GPT Models</p>
<p>Evaluation Setup We tested GPT-3.5-turbo and GPT-4 models from the chat completion API provided by OpenAI.</p>
<p>Our examples were limited to linguistic complexity L ≤ 1, due to the models' context width limit: contexts of L ≥ 2 could not fit in the window for the few-shot setting.For the same reason, the maximum number of training shots was limited to 9. To enforce deterministic behavior to the models we set temperature=0.To make the responses less verbose we set max tokens=3.</p>
<p>As transformer-based language models undergo pretraining through a certain form of language modeling objective, the most common approach to evaluate these models in the zero/few-shot setting is by employing prompt engineering techniques.To formulate our prompt, we used the guidelines7 from OpenAI and our approach was based on the deductive reasoning prompts presented in Tang et al. (2023).The prompt that we concluded in was the following: {"role": "system", "content": "You are an assistant capable of logical reasoning.Answer to the given question with 'True', 'False', or 'Unknown' based on the context."}.</p>
<p>Evaluation Results</p>
<p>In Table 3 we present the average accuracy (over 100 examples) per inference depth of 0-shot and 9-shot prompting for GPT-3.5 and GPT-4.It is noted that GPT-4 has good performance (max 92% and min 77%) with just 9 shots.Also, it is evident that the models consistently struggle in increased inference depths, demonstrating that our dataset is challenging even for L ≤ 1.</p>
<p>Table 3: Average accuracy per inference depth of 0-shot, 9-shot GPT-3.5, GPT-4 on 100 examples from DELTA of linguistic complexity level L ≤ 1. 0-shot GPT-3.5 0-shot GPT-4 9-shot GPT-3.5 9-shot GPT-4
D = N/A</p>
<p>Tests on Symbolic Data</p>
<p>To test the effect of the semantics of the words on the performance of DELTA M we created the dataset SoftSymbolic, generated by replacing consistently in the test set the words appearing in pools A and B with symbols.Specifically, all individuals (e.g., Anna) were replaced with an a i symbol (e.g., a 3 ), all classes (e.g., smart) with an C j symbol (e.g., C 2 ), and all roles (e.g., likes) with an R k symbol (e.g., R 5 ), where i, j, k is some ID number.The average performance of DELTA M over the SoftSymbolic dataset was 95.3%, hence in contrast to Tang et al. (2023), we conclude that DELTA M is not affected by the lack of semantics.</p>
<p>To check if the models can perform over purely logical examples, we generated the HardSymbolic dataset, which resulted from the SoftSymbolic by also utilizing the DL terminology: the word "some" (corresponding to the existential quantifier) was translated as "exists", the "if . . .then . . ." (corresponding to subsumption) as "is subsumed by", etc.</p>
<p>The performance of DELTA M on both Soft and Hard Symbolic datasets is presented in Table 4.We observe that the average performance of DELTA M over the HardSymbolic dataset dropped to 58.5%.This is an expected result as the structure of the tested sentences was very different from the sentences in which DELTA M had been trained.We tested also the GPT models on (100 examples of) the HardSymbolic dataset where they showed similar performance to DELTA M .The results are shown in Table 5.The average accuracy of GPT-3.5 0-shot was 57%, 9-shot 61%; and GPT-4 20%, 65%, respectively.Hence, TLMs seem to struggle with purely logical datasets.Both the Soft and HardSymbolic datasets are openly available in the provided URL.</p>
<p>Related Work</p>
<p>Multiple surveys (Yang et al., 2023;Huang and Chang, 2023;Yu, Zhang, and Wang, 2023) in the literature describe the most recent research developments on the use of transformers for reasoning tasks.One of the first datasets generated for this purpose was from Clark, Tafjord, and Richardson (2020) with RuleTaker, demonstrating the potential of transformers to perform logical question answering under CWA by training TLMs on synthetic datasets.However, their approach was limited to short expressions of simple conjunctive subsumption axioms.Tafjord, Dalvi, and Clark (2021), generated the ProofWriter datasets (under CWA and OWA) and with a T5 (Raffel et al., 2020)based model fined-tuned on ProofWriter showed that TLMs can generate proofs with high accuracy (94.8% for depth 5).We generated DELTA D based on the approach for the generation of the datasets RuleTaker and ProofWriter, i.e., using PCFGs.However, DELTA D is different from these datasets as i) ALCQ is a much more expressive logic language hence we produced new PCFGs; ii) we have defined different PCFGs for each linguistic complexity level (which has not been done for any other dataset in the literature); iii) it is balanced regarding the aspects discussed in Section 3.5.</p>
<p>In more expressive contexts, Ontañón et al. (2022) showed that TLMs perform well (up to 90.5%) over contexts generated by propositional logic and a small subset of FOL.Han et al. (2022), with the FOLIO dataset (1.4K), generated from FOL sentences -but without number restrictions-tested the ability of various TLMs for the same reasoning task and concluded that RoBERTa (Liu et al., 2019) performed best among all tested models (including GPT-3 and Codex) but still, the performance was low.Tian et al. (2021) introduced the much richer synthetic dataset LogicNLI (30K), under OWA for diagnosing TLMs' ability in FOL reasoning, showing that even their best-performing model does not learn to perform reasoning tasks and cannot generalize to different scenarios.Schlegel, Pavlov, and Pratt-Hartmann (2022) generated a very simple dataset (containing a single conjunction) for satisfiability checking and showed that models that perform well on hard problems do not perform equally well on easier ones, concluding that transformers cannot learn the underlying reasoning rules rather than they tend to overfit to patterns in the generated data.Also, Zhang et al. (2022), andTian et al. (2021) achieved similar results.Bang et al. (2023) studied ChatGPT's (Liu et al., 2023) deductive reasoning ability on bAbi task 16 (Weston et al., 2016) and En-tailmentBank (Dalvi et al., 2021), performing merely well.In addition, differently from our results (where the performance decrease was small), Tang et al. (2023) showed that TLMs perform significantly better when using natural language instead of symbolic representations of logical facts and subsumption axioms.Most of the aforementioned benchmarks are composed of short sentences; the ones with longer sentences (avg.13 words/sentence) are small (≤ 40K), while none of them have examples with numerical restrictions.This is better demonstrated with Table 6, where we present the metrics of the datasets that are most relevant to DELTA D (Entailment Bank is omitted as it does not conform to a specific formal language).A work that is close to our research is that of He et al. (2023), who tested the ability of TLMs, and specifically of RoBERTa, to perform natural language inference tasks over existing OWL2 ontologies (e.g., FoodOn, Shema.org).However, the task studied is different: in He et al. (2023), given two concept expressions C and D the TLM is asked to infer if one entails/contradicts the other, while in this work TLMs decide if a sentence can be inferred from a set of subsumption axioms and facts, i.e., a KB.</p>
<p>Relevant to our research is also the work of Madusanka et al. (2023), who investigated the effects of the various types of quantifiers on the performance of TLMs.As the generated dataset is not openly available it is hard to evaluate its complexity and hence its relevance to DELTA D .It is worth noting, though, that they do not investigate systematically the aspects that we have focused on in this work (inference depth, linguistic complexity).</p>
<p>Conclusions and Future Work</p>
<p>We generated the only large dataset (384K) in the literature that targets expressive DLs (namely, ALCQ), enjoys both high expressivity and high linguistic complexity, and is publicly available for further understanding of the functionality of TLMs.We showed that our DeBERTa-based model, DELTA M , can carry out entailment checking over expressive synthetic datasets with very high accuracy, regardless of the linguistic complexity of the context.Differently from recent results in the literature, we showed that our model has learned to generalize on unseen reasoning depths, smaller or greater.Zero-shot tests showed that DELTA M is mostly robust to other distributions.Tests with the GPT family showed that GPT-4 can have significant performance with only a few shots.The high accuracy of zero-shot testings in a real-world scenario demonstrates the potential of TLMs for performing reasoning tasks bypassing the necessity for domain experts to be familiar with formal representations.</p>
<p>Our qualitative tests revealed the need for the develop-ment of systematic evaluation techniques of synthetically generated datasets.Hence, this will be our next step in future work.Furthermore, we plan to explore the upper limit of the expressivity of the logic language so that a TLM will be able to perform reasoning tasks with high accuracy.</p>
<p>Finally, we will expand our evaluation section with other state-of-the-art generative models.</p>
<p>Pool B</p>
<p>• Atomic Concepts: "ambitious", "confident", "creative", "determined", "enthusiastic", "innovative", "logical", "persevering".</p>
<p>• Role Names: "admires", "consults", "guides", "instructs", "leads", "mentors", "supervises", "supports".</p>
<p>• Individual Names: "Ioanna", "Dimitrios", "Eleni", "Maria", "Manolis", "Angelos", "Panos", "Anna".</p>
<p>To generate the KBs, we employ a random sampling technique to select a subset of individuals, roles, and atomic Concepts from the pools mentioned above.An item from each pool has the same probability of being chosen.</p>
<p>All PCFGs can be found in the provided URL.The probabilities in the PCFGs were determined experimentally to generate appropriate KBs that would yield the desired inferences in the minimum amount of time.</p>
<p>KB Sizes</p>
<p>We utilize randomized parameters to control the size of a KB, based on the target reasoning depth of the corresponding dataset.The optimal (as we have found through experimentation) predefined ranges of the subsumption axioms and facts per reasoning depth D are as follows:</p>
<p>•</p>
<p>A.2 Additional Training Details</p>
<p>We used PyTorch 2.0 to set up our training and testing (inferencing).</p>
<p>We use the microsoft/deberta-v3-large model from the transformers8 library, along with the accelerate9 framework.</p>
<p>We fine-tuned the DeBERTaV3-large model (304M parameters) using the AdamW optimizer on two A100 GPUs.We used mixed precision (FP16) for our calculations to save memory and speed up the process.The specific set of hyperparameters used for all our models' training is given in Table 7.The model showed significant performance with this set of hyper-parameters, so there was no reason to proceed with any further hyper-parameter tuning, especially given our limited resources.The model output corresponds to the truth value 0 for False, 1 for True, and 2 for Unknown labels.</p>
<p>A.3 Evaluation Results on Datasets for L 0 , L 1 , L 2</p>
<p>The performance of the intermediate models DELTA i,j , for i ∈ {0, 1, 2, 3, 5}, j ∈ {0, 1, 2} on their corresponding datasets (of D ≤ i and L ≤ j) are illustrated in Tables 8, 9, 10.We observe that the pattern of the models' performance across various linguistic complexity levels is similar.However, as the models progress to higher linguistic complexity levels and, hence are trained on more data, the number of times they achieve perfect accuracy is increased.The models trained on D ≥ 3 show very good generalization on unseen reasoning depths, whereas the performance on unseen reasoning depths of the models trained on D ≤ 2 fluctuates across linguistic complexity levels.This can be attributed to the complexity difference among linguistic levels, affecting models' generalization.</p>
<p>Fiona</p>
<p>are quiet or not nice or they eat more than one people that admire none and they love someone.</p>
<p>Figure 1 :
1
Figure 1: An example from DELTAD, where the context contains three sentences of high linguistic complexity level and the true and false sentences are of reasoning depth 3.</p>
<p>rdf:about="#Charlie"&gt; <likes rdf:resource="#Erin"/> </owl:NamedIndividual> … Delta-closure (K, 5) <q T , True, {}> <q F , False, {}> <q U , Unknown, {}> … <q T , True, e m,T > <q F , False, e m,F > &lt;q U</p>
<p>Figure 2 :
2
Figure 2: Data generation pipeline for examples with n-level context and answers of minimum inference depth ≤ m</p>
<p>Table 5 :
5
Average accuracy per inference depth of 0-shot, 9-shot GPT-3.5, GPT-4 on 100 examples (per depth) of the HardSymbolic dataset.0-shotGPT-3.5 0-shot GPT-4 9-shot GPT-3.5 9-shot GPT-</p>
<p>For D = 0: |sub.axioms| ∈ [3, 8], |facts| ∈ [1, 5] • For D = 1: |sub.axioms| ∈ [3, 8], |facts| ∈ [2, 6] • For D = 2: |sub.axioms| ∈ [3, 8], |facts| ∈ [3, 8] • For D = 3: |sub.axioms| ∈ [4, 8], |facts| ∈ [5, 10] • For D = 5: |sub.axioms| ∈ [6, 14], |facts| ∈[6, 12]</p>
<p>Charlie likes Erin.Charlie chases Erin.If someone admires only people that are cold and not round or kind or that love at least one not white people, then they love someone that is cold or that loves someone kind.If someone loves someone, then they love someone that is not big or not furry and that chases someone.</p>
<p>…<Charlie likes Erin, True, 0 > <Charlie loves Erin, False, 0> <Erin likes Charlie, Unknown, na> … <Dave lives someone that is not big or not furry and that chases someone, True, m> <Erin loves only people that chase someone, False, m> <Anne Dave lives someone that is not big or not furry, Unknown, na></p>
<p>Table 1 :
1
Accuracy of DELTA models on Test (own), on D5,3 dataset, and slices of D5,3 per depth.
DELTA0,3 DELTA1,3 DELTA2,3 DELTA3,3 DELTA5,3Test (own)100.099.899.899.699.7D5,361.290.595.299.399.8D = N/A D = 0 D = 1 D = 2 D = 3 D = 4 D = 5100.0 100.0 43.4 24.5 34.1 29.2 19.399.4 100.0 100.0 73.1 71.3 77.6 76.599.5 100.0 100.0 99.5 99.5 84.5 83.599.2 100.0 100.0 100.0 100.0 99.5 98.599.7 100.0 100.0 100.0 100.0 100.0 99.5</p>
<p>Table 2 :
2
Accuracy of DELTA models on Test (own) across all levels.</p>
<p>Table 4 :
4
Average accuracy per inference depth of DELTAM on SoftSymbolic and HardSymbolic datasets.
60987592D = 0 D = 1 D = 2 D = 3 D = 4 D = 574 68 48 43 44 4680 57 42 32 30 4084 82 60 67 57 5789 86 77 80 83 83Soft. Hard.71.8 100.0 58.3 D = N/A 84.1 D = 0 99.3 39.4 D = 1 96.3 45.5 D = 2 94.9 68.2 D = 3 97.0 66.6 D = 4 D = 5 95.6 60.7</p>
<p>Table 6 :
6
The state-of-the-art benchmarks for deductive reasoning with transformers.</p>
<h1>QuestionsAvg size of sentencesMax size of sentencesFormal LanguageGenerating method</h1>
<p>Table 7 :
7
Detailed specifications of the hyper-parameters used in DeBERTaV3-large training.
Hyper-parameterValueBatch size4Accumulation steps2 (Effective Batch size = 8)Learning rate Warm-up ratio2 × 10 −5 0.06Epochs4Mixed precisionFP16Betas(0.9, 0.999)Weight Decay Text Embedding Size 512 (dimensions) 1 × 10 −4</p>
<p>Table 8 :
8
Accuracy of DELTA models on their own test sets, and the entire, and slices of D ≤ 5, L = 0 dataset.
DELTA0,0 DELTA1,0 DELTA2,0 DELTA3,0 DELTA5,0Test (own)100.099.799.498.998.7D ≤ 5, L = 0 D = N/A D = 0 D = 1 D = 2 D = 3 D = 4 D = 561.4 98.8 99.9 48.9 11.9 34.3 32.1 29.675.3 97.9 100.0 99.6 47.1 49.5 45.0 42.993.2 94.4 100.0 100.0 96.3 75.7 72.0 67.297.7 95.2 99.5 99.5 99.0 99.0 99.0 97.598.8 98.2 100.0 100.0 99.0 99.0 99.0 99.0</p>
<p>Table 9 :
9
Accuracy of DELTA models on their own test sets, and the entire, and slices of D ≤ 5, L ≤ 1 dataset.
DELTA0,1 DELTA1,1 DELTA2,1 DELTA3,1 DELTA5,1Test (own)100.099.799.699.799.5D ≤ 5, L ≤ 1 D = N/A D = 0 D = 1 D = 2 D = 3 D = 4 D = 567.9 99.0 99.9 52.5 27.4 47.9 46.9 39.492.4 98.1 100.0 99.3 81.7 79.5 77.2 66.085.8 99.3 100.0 99.5 97.5 61.5 58.0 53.598.7 98.8 100.0 100.0 99.0 99.0 98.0 96.599.6 99.7 100.0 100.0 99.5 99.5 99.0 99.0</p>
<p>Table 10 :
10
Accuracy of DELTA models on their own test sets, and the entire, and slices of D ≤ 5, L ≤ 2 dataset.
DELTA0,2 DELTA1,2 DELTA2,2 DELTA3,2 DELTA5,2Test (own)99.999.599.799.799.6D ≤ 5, L ≤ 2 D = N/A D = 0 D = 1 D = 2 D = 3 D = 4 D = 555.1 99.6 99.9 37.2 16.1 16.4 17.6 10.089.7 95.6 100.0 99.6 81.7 62.7 63.1 63.485.3 98.7 100.0 100.0 98.5 59.0 62.0 51.099.1 99.2 100.0 100.0 100.0 98.5 99.5 98.099.7 99.7 100.0 99.5 99.0 99.5 100.0 98.5
https://github.com/angelosps/DELTA
https://www.cs.ox.ac.uk/isg/tools/HermiT//download/ 0.9.2/owlapi/javadoc/org/semanticweb/owl/util/ InferredOntologyGenerator.html
Hence, Delta-closure can be calculated only for KBs that InferredOntologyGenerator can calculate all axioms and facts within t.
DELTAD also contains the justification for each answer to be used for future work or by the research community for other downstream tasks, such as proof generation.
https://pypi.org/project/language-tool-python/
https://platform.openai.com/docs/guides/prompt-engineering/ strategy-write-clear-instructions
https://github.com/huggingface/transformers
https://github.com/huggingface/accelerate
AcknowledgementThis work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.* This work was performed while the author was with the Dept. of Informatics and Telecommunications, National and Kapodistrian University of Athens.A AppendixThe appendix consists of the following content:A.1 Dataset GenerationWe generated our synthetic dataset DELTA D , using four probabilistic context-free grammars.To ensure that we will produce datasets with inference depth D &gt; 0, i.e., datasets resulted from some inferencing, we generated a new statement C ⊑ D, if C either appeared in some already generated fact or the RHS of some already generated statement.Probabilistic Context-Free Grammars Each grammar is based on some vocabulary of terms.Pool A and Pool B are defined next.Pool A• Atomic Concepts: "red", "blue", "green", "kind", "nice", "big", "cold", "young", "round", "rough", "orange", "smart", "quiet", "furry".• Role Names: "likes", "loves", "eats", "chases", "admires".• Individual Names: "Anne", "Bob", "Charlie", "Dave", "Erin", "Fiona", "Gary", "Harry".
The Description Logic Handbook: Theory, Implementation, and Applications. F Baader, D Calvanese, D L Mcguinness, D Nardi, Patel-Schneider , P. F.2003Cambridge University Press</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, abs/2302.040232023</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. C Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Explaining answers with entailment trees. B Dalvi, P A Jansen, O Tafjord, Z Xie, H Smith, L Pipatanangkura, P Clark, Conference on Empirical Methods in Natural Language Processing. 2021</p>
<p>FOLIO: natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, D Peng, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, S R Joty, A R Fabbri, W Kryscinski, X V Lin, C Xiong, D Radev, CoRR abs/2209.008402022</p>
<p>Language model analysis for ontology subsumption inference. Y He, J Chen, E Jimenez-Ruiz, H Dong, I Horrocks, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing. P He, J Gao, W Chen, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>The manchester OWL syntax. M Horridge, N Drummond, J Goodwin, A L Rector, R Stevens, H Wang, Proceedings of the OWLED<em>06 Workshop on OWL: Experiences and Directions. CEUR Workshop Proceedings. CEUR-WS.org. B C Grau, P Hitzler, C Shankey, E Wallace, the OWLED</em>06 Workshop on OWL: Experiences and DirectionsAthens, Georgia, USA2006. November 10-11, 2006216</p>
<p>Laconic and precise justifications in OWL. M Horridge, B Parsia, U Sattler, A P Sheth, S Staab, M Dean, M Paolucci, D Maynard, T W Finin, K Thirunarayan, The Semantic Web -ISWC 2008, 7th International Semantic Web Conference, ISWC 2008. Lecture Notes in Computer Science. Karlsruhe, GermanySpringer2008. October 26-30, 20085318</p>
<p>Towards reasoning in large language models: A survey. J Huang, K C Chang, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J L Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, 2019RoBERTa: A robustly optimized BERT pretraining approach. CoRR abs/1907.11692</p>
<p>Summary of ChatGPT-related research and perspective towards the future of large language models. Y Liu, T Han, S Ma, J Zhang, Y Yang, J Tian, H He, A Li, M He, Z Liu, Z Wu, L Zhao, D Zhu, X Li, N Qiang, D Shen, T Liu, B Ge, Meta-Radiology. 121000172023</p>
<p>Not all quantifiers are equal: Probing transformer-based language models' understanding of generalised quantifiers. T Madusanka, I Zahid, H Li, I Pratt-Hartmann, R Batista-Navarro, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>LogicInference: A new dataset for teaching logical inference to seq2seq models. S Ontañón, J Ainslie, V Cvicek, Z Fisher, CoRR abs/2203.150992022</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 21672020</p>
<p>Foundations of Description Logics. S Rudolph, 2011SpringerBerlin, Heidelberg; Berlin Heidelberg</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chainof-thought. A Saparov, H He, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Can transformers reason in fragments of natural language?. V Schlegel, K V Pavlov, I Pratt-Hartmann, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022. December 7-11, 20222022Association for Computational Linguistics</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B Dalvi, P Clark, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, 2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL. Association for Computational Linguistics</p>
<p>Large language models are in-context semantic reasoners rather than symbolic reasoners. X Tang, Z Zheng, J Li, F Meng, S Zhu, Y Liang, M Zhang, CoRR abs/2305.148252023</p>
<p>Diagnosing the first-order logical reasoning ability through LogicNLI. J Tian, Y Li, W Chen, L Xiao, H He, Y Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M Moens, X Huang, L Specia, S W Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021. 2021. 7-11 November, 2021</p>
<p>Enhancing polymer electrolyte membrane fuel cell system diagnostics through semantic modelling. E Tsalapati, W Johnson, T W Jackson, L M Jackson, D Low, B Davies, L Mao, A A West, Expert Syst. Appl. 1631135502021</p>
<p>Towards AI-complete question answering: A set of prerequisite toy tasks. J Weston, A Bordes, S Chopra, T Mikolov, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Y Lecun, San Juan, Puerto Rico2016. May 2-4, 2016Conference Track Proceedings</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Z Yang, X Du, R Mao, J Ni, E Cambria, CoRR abs/2303.120232023</p>
<p>Nature language reasoning, A survey. F Yu, H Zhang, B Wang, 2023. CoRR abs/2303.14725</p>
<p>On the paradox of learning to reason from data. H Zhang, L H Li, T Meng, K Chang, G V Broeck, CoRR abs/2205.115022022</p>            </div>
        </div>

    </div>
</body>
</html>
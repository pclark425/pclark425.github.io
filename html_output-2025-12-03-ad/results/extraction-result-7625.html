<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-264146906</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09478v2.pdf" target="_blank">MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7625",
    "paper_id": "paper-264146906",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0038195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MINIGPT-V2: LARGE LANGUAGE MODEL AS A UNIFIED INTERFACE FOR VISION-LANGUAGE MULTI-TASK LEARNING
26 Oct 2023</p>
<p>Jun Chen 
King Abdullah University of Science and Technology (KAUST</p>
<p>Meta AI Research</p>
<p>Deyao Zhu 
King Abdullah University of Science and Technology (KAUST</p>
<p>Xiaoqian Shen 
King Abdullah University of Science and Technology (KAUST</p>
<p>Xiang Li 
King Abdullah University of Science and Technology (KAUST</p>
<p>Zechun Liu 
Meta AI Research</p>
<p>Pengchuan Zhang 
Meta AI Research</p>
<p>Raghuraman Krishnamoorthi 
Meta AI Research</p>
<p>Vikas Chandra 
Meta AI Research</p>
<p>Yunyang Xiong 
Meta AI Research</p>
<p>Mohamed Elhoseiny 
King Abdullah University of Science and Technology (KAUST</p>
<p>33 45 57 33 45 57 37 49 61 38 44 50 23 37 51 55 57 59 79 82 85 68 71 74 78 81 84</p>
<p>MINIGPT-V2: LARGE LANGUAGE MODEL AS A UNIFIED INTERFACE FOR VISION-LANGUAGE MULTI-TASK LEARNING
26 Oct 2023FFC59099414282BC5F9DB76010C97A4CarXiv:2310.09478v2[cs.CV]
Large language models have shown their remarkable capabilities as a general interface for various language-related applications.Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others.The challenge is to use a single model for performing diverse visionlanguage tasks effectively with simple multi-modal instructions.Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks.We propose using unique identifiers for different tasks when training the model.These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task.After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models.Our model and codes are available at https://minigpt-v2.github.io/.</p>
<p>INTRODUCTION</p>
<p>Multi-modal Large Language Models (LLMs) have emerged as an exciting research topic with a rich set of applications in vision-language community, such as visual AI assistant, image captioning, visual question answering (VQA), and referring expression comprehension (REC).A key feature of multimodal large language models is that they can inherit advanced capabilities (e.g., logical reasoning, common sense, and strong language expression) from the LLMs (OpenAI, 2022;Touvron et al., 2023a;b;Chiang et al., 2023).When tuned with proper vision-language instructions, multi-modal LLMs, specifically vision-language models, demonstrate strong capabilities such as producing detailed image descriptions, generating code, localizing the visual objects in the image, and even performing multi-modal reasoning to better answer complicated visual questions (Zhu et al., 2023b;Liu et al., 2023b;Ye et al., 2023;Wang et al., 2023;Chen et al., 2023b;Dai et al., 2023;Zhu et al., 2023a;Chen et al., 2023a;Zhuge et al., 2023).This evolution of LLMs enables interactions of visual and language inputs across communication with individuals and has been shown quite effective for building visual chatbots.</p>
<p>However, learning to perform multiple vision-language tasks effectively and formulating their corresponding multi-modal instructions present considerable challenges due to the complexities inherent among different tasks.For instance, given a user input "tell me the location of a person", there are many ways to interpret and respond based on the specific task.In the context of the referring expression comprehension task, it can be answered with one bounding box location of the person.For the visual question-answering task, the model might describe their spatial location using human natural language.For the person detection task, the model might identify every spatial location of each human in a given image.To alleviate this issue and towards a unified approach, we propose Figure 1: Our MiniGPT-v2 achieves state-of-the-art performances on a broad range of visionlanguage tasks compared with other generalist models.</p>
<p>a task-oriented instruction training scheme to reduce the multi-modal instructional ambiguity, and a vision-language model, MiniGPT-v2.Specifically, we provide a unique task identifier token for each task.For example, we provide a [vqa] identifier token for training all the data samples from the visual question answering tasks.In total, we provide six different task identifiers during the model training stages.</p>
<p>Our model, MiniGPT-v2, has a simple architecture design.It directly takes the visual tokens from a ViT vision encoder (Fang et al., 2022) and project them into the feature space of a large language model (Touvron et al., 2023b).For better visual perception, we utilize higher-resolution images (448x448) during training.But this will result in a larger number of visual tokens.To make the model training more efficient, we concatenate every four neighboring visual tokens into a single token, reducing the total number by 75%.Additionally, we utilize a three-stage training strategy to effectively train our model with a mixture of weakly-labeled, fine-grained image-text datasets, and multi-modal instructional datasets, with different training focus at each stage.</p>
<p>To evaluate the performance of our model, we conducted extensive experiments on diverse visionlanguage tasks, including (detailed) image/grounded captioning, vision question answering, and visual grounding.The results demonstrate that our MiniGPT-v2 can achieve SOTA or comparable performance on diverse benchmarks compared to previous vision-language generalist models, such as MiniGPT-4 (Zhu et al., 2023b), InstructBLIP (Dai et al., 2023), LLaVA (Liu et al., 2023b) and Shikra (Chen et al., 2023b).For example, our MiniGPT-v2 outperforms MiniGPT-4 by 21.3%, InstructBLIP by 11.3%, and LLaVA by 11.7% on the VSR benchmark (Liu et al., 2023a), and it also performs better than the previously established strong baseline, Shikra, in most validations on RefCOCO, RefCOCO+, and RefCOCOg.Our model establishes new state-of-the-art results on these benchmarks among vision-language generalist models, shown in Fig. 1.</p>
<p>RELATED WORK</p>
<p>We briefly review relevant works on advanced large language models and multi-modal LLMs for visual aligning.</p>
<p>Advanced Large Language Models (LLMs).Early-stage models such as GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2018) are foundation models trained on web-scale text datasets, marking a breakthrough in the NLP field.Following the success of foundation models, LLMs with higher capacity and increased training data are developed, including GPT-3 (Brown et al., 2020), Megatron-turing NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), OPT (Zhang et al., 2022), andBLOOM (Scao et al., 2022).Most recently, the efforts have been focused on refining LLMs to work effectively with human instruction and feedback.Representative works in this direction are InstructGPT (Ouyang et al., 2022) and ChatGPT (OpenAI, 2022), which demonstrate strong capabilities such as answering a diverse range of language questions, engaging in conversations with humans, and learning to perform complex tasks like writing refinement and coding assistant.</p>
<p>Concurrent with these advancements of LLMs is the rise of LLaMA (Touvron et al., 2023a) language models.To enable human instruction following abilities similar to ChatGPT, some works attempt to finetune the LLaMA model with additional high-quality instruction datasets (sha, 2023).Examples of these models include Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), andMPT (Team, 2023).Some other open-sourced language models that learned from the human feedback data, such as Falcon (Penedo et al., 2023) and LLaMA-2 (Touvron et al., 2023b), have also been introduced to the NLP community with impressive performance.</p>
<p>Visual Aligning with LLMs.With the remarkable generalization abilities of LLMs, interesting studies have extended LLMs to multi-modal domains by aligning visual inputs with LLMs.Early works such as VisualGPT (Chen et al., 2022) and Frozen (Tsimpoukelli et al., 2021) used pre-trained language models to improve vision-language models on image captioning and visual question answering.This initial exploration paved the way for subsequent vision-language research such as Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023a).More recently, GPT-4 has been released and demonstrates many advanced multi-modal abilities, e.g., generating website code based on handwritten text instructions.Those demonstrated capabilities inspired other vision-language LLMs, including MiniGPT-4 (Zhu et al., 2023b) and LLaVA (Liu et al., 2023b), which align the image inputs with a large language model, Vicuna Chiang et al. (2023), using proper instructional tuning.These vision-language models also showcase many advanced multi-modal capabilities after the alignment.Recent works, such as Vision-LLM (Wang et al., 2023), Kosmos-2 (Peng et al., 2023), Shikra (Chen et al., 2023b), and our concurrent work, Qwen-VL (Bai et al., 2023), also demonstrate that multi-model LLMs models can also perform visual grounding by generating the text format of bounding boxes through language model.We start by introducing our vision-language model, MiniGPT-v2, then discuss the basic idea of a multi-task instruction template with task identifiers for training, and finally adapt our task identifier idea to achieve task-oriented instruction tuning.</p>
<p>MODEL ARCHITECTURE</p>
<p>Our proposed model architecture, MiniGPT-v2, is shown in Fig. 2. It consists of three components: a visual backbone, a linear projection layer, and a large language model.We describe each component as follows:</p>
<p>Visual backbone.MiniGPT-v2 adapts the EVA (Fang et al., 2022) as our visual backbone model backbone.We freeze the visual backbone during the entire model training.We train our model with the image resolution 448x448, and we interpolate the positional encoding to scale with a higher image resolution.</p>
<p>Linear projection layer.We aim to project all the visual tokens from the frozen vision backbone into the language model space.However, for higher-resolution images such as 448x448, projecting all the image tokens results in a very long-sequence input (e.g., 1024 tokens) and significantly lowers the training and inference efficiency.Hence, we simply concatenate 4 adjacent visual tokens in the embedding space and project them together into one single embedding in the same feature space of the large language model, thus reducing the number of visual input tokens by 4 times.With this operation, our MiniGPT-v2 can process high-resolution images much more efficiently during the training and inference stage.</p>
<p>Large language model.MiniGPT-v2 adopts the open-sourced LLaMA2-chat (7B) (Touvron et al., 2023b) as the language model backbone.In our work, the language model is treated as a unified interface for various vision-language inputs.We directly rely on the LLaMA-2 language tokens to perform various vision-language tasks.For the visual grounding tasks that necessitate the generation of spatial locations, we directly ask the language model to produce textual representations of bounding boxes to denote their spatial positions.</p>
<p>MULTI-TASK INSTRUCTION TEMPLATE</p>
<p>When training a single unified model for multiple different tasks such as visual question answering, image caption, referring expression, grounded image caption, and region identification, the multimodal model might fail to distinguish each task by just aligning visual tokens to language models.For instance, when you ask "Tell me the spatial location of the person wearing a red jacket?", the model can either respond you the location in a bounding box format (e.g., &lt; X lef t &gt;&lt; Y top &gt;&lt; X right &gt;&lt; Y bottom &gt;) or describe the object location using natural language (e.g., upper right corner).To reduce such ambiguity and make each task easily distinguishable, we introduce taskspecific tokens in our designed multi-task instruction template for training.We now describe our multi-task instruction template in more details.</p>
<p>General input format.We follow the LLaMA-2 conversation template design and adapt it for the multi-modal instructional template.The template is denoted as follows,
[INST] <Img> &lt; ImageFeature&gt; </Img> [Task Identifier] Instruction [/INST]
In this template, [INST] is considered as the user role, and [/INST] is considered as the assistant role.We structure the user input into three parts.The first part is the image features, the second part is the task identifier token, and the third part is the instruction input.</p>
<p>Task identifier tokens.Our model takes a distinct identifier for each task to reduce the ambiguity across various tasks.As illustrated in Table 1, we have proposed six different task identifiers for visual question answering, image caption, grounded image captioning, referring expression comprehension, referring expression generation, and phrase parsing and grounding respectively.For vision-irrelevant instructions, our model does not use any task identifier token.</p>
<p>Tasks</p>
<p>MULTI-TASK INSTRUCTION TRAINING</p>
<p>We now adapt our designed multi-task instruction template for instruction training.The basic idea is to take instruction with task-specific identifier token as input for task-oriented instruction training of MiniGPT-v2.When input instructions have task identifier tokens, our model will become more prone to multiple-task understanding during training.We train our model with task identifier instructions for better visual aligment in three stages.The first stage is to help MiniGPT-v2 build broad vision-language knowledge through many weakly-labeled image-text datasets, and highquality fine-grained vision-language annotation datasets as well (where we will assign a high data sampling ratio for weakly-labeled image-text datasets).The second stage is to improve the model with only fine-grained data for multiple tasks.The third stage is to finetune our model with more multi-modal instruction and language datasets for answering diverse multi-modal instructions better and behaving as a multi-modal chatbot.The datasets used for training at each stage are listed in Table 2.  Stage 1: Pretraining.To have broad vision-language knowledge, our model is trained on a mix of weakly-labeled and fine-grained datasets.We give a high sampling ratio for weakly-labeled datasets to gain more diverse knowledge in the first-stage.</p>
<p>For the weakly-labeled datasets, we use LAION (Schuhmann et al., 2021), CC3M (Sharma et al., 2018), SBU (Ordonez et al., 2011), and GRIT-20M from Kosmos v2 (Peng et al., 2023) that built the dataset for referring expression comprehension (REC), referring expression generation (REG), and grounded image captioning.</p>
<p>For fine-grained datasets, we use datasets like COCO caption (Lin et al., 2014) and Text Captions (Sidorov et al., 2020) for image captioning, RefCOCO (Kazemzadeh et al., 2014), Ref-COCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016) for REC.For REG, we restructured the data from ReferCOCO and its variants, reversing the order from phrase → bounding boxes to bounding boxes → phrase.For VQA datasets, our training takes a variety of datasets, such as GQA (Hudson &amp; Manning, 2019), VQA-v2 (Goyal et al., 2017), OCR-VQA (Mishra et al., 2019), OK-VQA (Marino et al., 2019), and AOK-VQA (Schwenk et al., 2022).</p>
<p>Stage 2: Multi-task training.To improve the performance of MiniGPT-v2 on each task, we only focus on using fine-grained datasets to train our model at this stage.We exclude the weakly-supervised datasets such as GRIT-20M and LAION from stage-1 and update the data sampling ratio according to the frequency of each task.This strategy enables our model to prioritize high-quality aligned image-text data for superior performance across various tasks.</p>
<p>Stage 3: Multi-modal instruction tuning.Subsequently, we focus on tuning our model with more multi-modal instruction datasets and enhancing its conversation ability as a chatbot.We continue using the datasets from the second stage and add instructional datasets, including LLaVA (Liu et al., 2023b), Flickr30k dataset (Plummer et al., 2015), our constructed mixing multi-task dataset, and the language dataset, Unnatural Instruction (Honovich et al., 2022).We give a lower data sampling ratio for the fine-grained datasets from stage-2 and a higher data sampling ratio for the new instruction datasets.</p>
<p>-LLaVA instruction data.We add the multi-modal instruction tuning datasets, including the detailed descriptions and complex reasoning from LLaVA (Liu et al., 2023b), with 23k and 58k data examples respectively.</p>
<p>-Flicker 30k.After the second-stage training, our MiniGPT-v2 can effectively generate the grounded image caption.Nevertheless, these descriptions tend to be short and often cover very few number of visual objects.This is because the GRIT-20M dataset from KOSMOS-v2 (Peng et al., 2023) that our model was trained with, features a limited number of grounded visual objects in each caption, and our model lacks proper multi-modal instruction tuning to teach it to recognize Preprint more visual objects.To improve this, we fine-tune our model using the Flickr30k dataset (Plummer et al., 2015), which provides more contextual grounding of entities within its captions.</p>
<p>We prepare the Flickr30k dataset in two distinct formats for training our model to perform grounded image caption and a new task "object parsing and grounding":</p>
<p>1) Grounded image caption.We select captions with a minimum of five grounded phrases, containing around 2.5k samples, and we directly instruct the model to produce the grounded image caption.e.g., a <p>wooden table</p>{<X lef t ><Y top ><X right ><Y bottom >} in the center of the room.</p>
<p>2) Object parsing and grounding.This new task is to parse all the objects from an input caption and then ground each object.To enable this, we use the task identifier [detection] to differentiate this capability from other tasks.Also, we use Flickr30k to construct two types of instruction datasets: caption→ grounded phrases and phrase → grounded phrase, each containing around 2.5k and 3k samples.Then we prompt our model with the instruction: [detection] description, the model will directly parse the objects from the input image description and also ground the objects into bounding boxes.</p>
<p>-Mixing multi-task dataset.After extensive training with single-round instruction-answer pairs, the model might not handle multiple tasks well during multi-round conversations since the context becomes more complex.To alleviate this situation, we create a new multi-round conversation dataset by mixing the data from different tasks.We include this dataset into our third-stage model training.</p>
<p>-Unnatural instruction.The conversation abilities of language model can be reduced after extensive vision-language training.To fix this, we add the language dataset, Unnatural Instruction (Honovich et al., 2022) into our model's third-stage training for helping recover the language generation ability.</p>
<p>EXPERIMENTS</p>
<p>In this section, we present experimental settings and results.We primarily conduct experiments on (detailed) image/grounded captioning, vision question answering, and visual grounding tasks, including referring expression comprehension.We present both quantitative and qualitative results.</p>
<p>Implementation details.Throughout the entire training process, the visual backbone of MiniGPT-v2 remains frozen.We focus on training the linear projection layer and efficient finetuning the language model using LoRA (Hu et al., 2021).With LoRA, we finetune W q and W v via lowrank adaptation.In our implementation, we set the rank, r = 64.We trained the model with an image resolution of 448x448 during all stages.During each stage, we use our designed multi-modal instructional templates for various vision-language tasks during the model training.</p>
<p>Training and hyperparameters.We use AdamW optimizer with a cosine learning rate scheduler to train our model.In the initial stage, we train on 8xA100 GPUs for 400,000 steps with a global batch size of 96 and an maximum learning rate of 1e-4.This stage takes around 90 hours.During the second stage, the model is trained for 50,000 steps on 4xA100 GPUs with a maximum learning rate of 1e-5, adopting a global batch size of 64, and this training stage lasts roughly 20 hours.For the last stage, training is executed for another 35,000 steps on 4xA100 GPUs, using a global batch size of 24 and this training stage took around 7 hours, maintaining the same maximum learning rate of 1e-5.</p>
<p>QUANTITATIVE EVALUATION</p>
<p>Dataset and evaluation metrics.We evaluate our model across a range of VQA and visual grounding benchmarks.For VQA benchmarks, we consider OKVQA (Schwenk et al., 2022), GQA (Hudson &amp; Manning, 2019), visual spatial reasoning (VSR) (Liu et al., 2023a), IconVQA (Lu et al., 2021), VizWiz (Gurari et al., 2018), HatefulMemes and (HM) (Kiela et al., 2020).For visual grounding, we evaluate our model on RefCOCO (Kazemzadeh et al., 2014) and RefCOCO+(Yu et al., 2016), and RefCOCOg (Mao et al., 2016) benchmarks.</p>
<p>To evaluate VQA benchmarks, we use an open-ended approach with a greedy decoding strategy.We evaluate each VQA question with the following instruction template: "[vqa] question".Following the previous method (Dai et al., 2023), we evaluate the performance by matching the model's Table 4: Results on referring expression comprehension tasks.Our MiniGPT-v2 outperforms many VL-generalist models including VisionLLM (Wang et al., 2023), OFA (Wang et al., 2022) and Shikra (Chen et al., 2023b) and reduces the accuracy gap comparing to specialist models including UNINEXT (Yan et al., 2023) and G-DINO (Liu et al., 2023c).</p>
<p>response to the ground-truth and reporting top-1 accuracy.For visual grounding benchmarks, we use the template " [refer] give me the location of Referring expression" for each referring expression comprehension question, and a predicted bounding box is considered as correct for reporting accuracy if its IOU between prediction and ground-truth is higher than 0.5.</p>
<p>Visual question answering results.Table 3 presents our experimental results on multiple VQA benchmarks.Our results compare favorably to baselines including MiniGPT-4 (Zhu et al., 2023b), Shikra (Chen et al., 2023b), LLaVA (Liu et al., 2023b), and InstructBLIP (Dai et al., 2023) across all the VQA tasks.For example, on QKVQA, our MiniGPT-v2 outperforms MiniGPT-4, Shikra, LLaVA, and BLIP-2 by 20.3%, 10.6%, 3.4%, and 11.9%.These results indicate the strong visual question answering capabilities of our model.Furthermore, we find that our MiniGPT-v2 (chat) variant shows higher performance than the version trained after the second stage.On OKVQA, VSR, IconVQA, VizWiz, and HM, MiniGPT-v2 (chat) outperforms MiniGPT-v2 by 0.9%, 2.3%, 4.2%, 20.7%, and 0.6%.We believe that the better performance can be attributed to the improved language skills during the third-stage training, which is able to benefit visual question comprehension and response, especially on VizWiz with 20.7% top-1 accuracy increase.</p>
<p>Referring expression comprehension results.Table 4 compares our model to baselines on REC benchmarks.Our MiniGPT-v2 shows strong REC performance on RefCOCO, RefCOCO+, and RefCOCOg, performing better than other vision-language generalist models.MiniGPT-v2 outperforms OFA-L (Wang et al., 2022) by over 8% accuracy across all tasks of Ref-COCO/RefCOCO+/RefCOCOg.Compared with a strong baseline, Shikra (13B) (Chen et al., 2023b), our model still shows better results, e.g., 84.29% vs 83.96% accuracy in average.These results provide direct evidence for the competing visual grounding capabilities of MiniGPT-v2.Although our model underperforms specialist models, the promising performance indicates its growing competence in visual grounding.</p>
<p>Ablation on task identifier.We conduct ablation studies on the effect of the task identifier on the performance of MiniGPT-v2.We compare our model with the variant without using task iden- The image depicts a large cave with a group of people painted on the walls.The people are engaged in various activities, such as hunting, gathering, and riding horses.There are several horses scattered throughout the scene, with some in the foreground and others in the background.</p>
<p>In addition to the people and horses, there are several other animals depicted in the cave, including a lion, a tiger, and a dinosaur.The cave is also adorned with various objects, such as a bow and arrows, a spear, and a shield.</p>
<p>Overall, the scene appears to be a vibrant and lively depiction of ancient life, showcasing the skills and activities of the people who lived in this cave.</p>
<p>Please describe this image as detailed as possible   Hallucination.We measure the hallucination of our model on image description generation and compare the results with other vision-language baselines, including MiniGPT-4 (Zhu et al., 2023b), mPLUG-Owl (Ye et al., 2023), LLaVA (Liu et al., 2023b), and MultiModal-GPT (Gong et al., 2023).Following the methodology from (Li et al., 2023b), we use CHAIR (Rohrbach et al., 2018) to assess hallucination at both object and sentence levels.As shown in</p>
<p>QUALITATIVE RESULTS</p>
<p>We now provide the qualitative results for a complementary understanding of our model's multimodal capabilities.Some examples can be seen in Fig. 3. Specifically, we demonstrated various abilities in the examples including a) object identification; b) detailed grounded image captioning; c) visual question answering; d) referring expression comprehension; e) visual question answering under task identifier; f) detailed image description; g) object parsing and grounding from an input text.More qualitative results can be found in the Appendix.These results demonstrate that our model has competing vision-language understanding capabilities.Moreover, notice that we train our model only with a few thousand of instruction samples on object parsing and grounding tasks at the third-stage, and our model can effectively follow the instructions and generalize on the new task.This indicates that our model has the flexibility to adapt on many new tasks.</p>
<p>Note that our model still occasionally shows hallucinations when generating the image description or visual grounding.e.g., our model may sometimes produce descriptions of non-existent visual objects or generate inaccurate visual locations of grounded objects.We believe training with more highquality image-text aligned data and integrating with a stronger vision backbone or large language model hold the potential for alleviating this issue.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce MiniGPT-v2, a multi-modal LLM that can serve as a unified interface for various vision-language multi-tasking learning.To develop a single model capable of handling multiple vision-language tasks, we propose using distinct identifiers for each task during the training and inference.These identifiers help our model easily differentiate various tasks and also improve learning efficiency.Our MiniGPT-v2 achieves state-of-the-art results across many visual question answering and referring expression comprehension benchmarks.We also found that our model can efficiently adapt to new vision-language tasks, which suggests that MiniGPT-v2 has many potential applications in the vision-language community.</p>
<p>Preprint</p>
<p>A APPENDIX</p>
<p>In the supplementary, we provide more qualitative results that are generated from our model to demonstrate the vision-language multi-tasking capabilities.</p>
<p>A For each task, we share 4 examples for showing the vision-language capabilities of our model.The results in the demo provide direct evidence for the competing visual understanding capabilities of MiniGPT-v2 on multiple vision-language tasks.For example, in the cases of grounded caption, our model is able to give correct grounded image caption with detailed spatial locations of objects.In the cases of identify, the model also generates our expected object names.MiniGPT-v2 can understand the new scenes and follow the question identifier to respond.But we also need to note that our model still has some hallucination e.g., In Fig. 6, several persons are not grounded accurately, and in Fig. 7, there does not exist a vase in the image.</p>
<p>Figure 2 :
2
Figure 2: Architecture of MiniGPT-v2.The model takes a ViT visual backbone, which remains frozen during all training phases.We concatenate four adjacent visual output tokens from ViT backbone and project them into LLaMA-2 language model space via a linear projection layer.</p>
<p>the right player's hat Who are the people on the right, on the left, and in the middle?The people in the image are: * Barack Obama, the former President of the United States, is on the left side of the image.<em> Joe Biden, the current President of the United States, is in the middle of the image.</em> Donald Trump, the former President of the United States, is on the right side of the image.</p>
<p>Figure 3 :
3
Figure3: Examples for various multi-modal capabilities of MiniGPT-v2.We showcase that our model is capable of completing multiple tasks such as referring expression comprehension, referring expression generation, detailed grounded image caption, visual question answering, detailed image description, and directly parsing phrase and grounding from a given input text.</p>
<p>.1 INSTRUCTION TEMPLATE FOR VARIOUS VISION-LANGUAGE TASKS RefCOCO/RefCOCO+/RefCOCOg: [refer] give me the location of question VizWiz: [vqa] Based on the image, respond to this question with a single word or phrase: question, and reply 'unanswerable' when the provided information is insufficient Hateful Meme: [vqa] This is an image with: question written on it.Is it hateful?Answer: VSR: [vqa] Based on the image, is this statement true or false?question IconQA, GQA, OKVQA: [vqa] Based on the image, respond to this question with a single word or phrase: question A.2 ADDITIONAL QUALITATIVE RESULTSTo study how well our model is able to take visual input and answer questions based on task-oriented identifier, we use our model to perform multiple vision-language tasks including grounded image captioning in Fig.4, Fig. 5, Fig. 6 and Fig. 7; Object parsing and grounding in Fig. 8, Fig. 9, Fig. 10 and Fig. 11; Referring expression comprehension in Fig. 12, Fig. 13, Fig. 14 and Fig. 15; Object identification in Fig. 16, Fig. 17, Fig. 18 and Fig. 19.</p>
<p>Figure 4 :
4
Figure 4: Detail grounded image caption example.</p>
<p>Figure 5 :
5
Figure 5: Detail grounded image caption example</p>
<p>Figure 7 :
7
Figure 7: Detail grounded image caption example</p>
<p>Table 1 :
1
Task identifier tokens for 6 different tasks, including visual question answering, image captioning, grounded image captioning, referring expression comprehension (REC), referring expression generation (REG), and object parsing and grounding (where the model extracts objects from the input text and determines their bounding box locations).location representation.For tasks such as referring expression comprehension (REC), referring expression generation (REG), and grounded image captioning, our model is required to identify the spatial location of the referred objects accurately.We represent the spatial location through the textual formatting of bounding boxes in our setting, specifically: "{&lt; X lef t &gt;&lt; Y top &gt;&lt; X right &gt;&lt; Y bottom &gt;}".Coordinates for X and Y are represented by integer values normalized in the range [0,100].&lt; X lef t &gt; and &lt; Y top &gt; denote the x and y coordinate top-left corner of the generated bounding box, and &lt; X right &gt; and &lt; Y bottom &gt; denote the x and y coordinates of the bottom-right corner.
VQA Caption Grounded CaptionRECREGObject Parsing and GroundingIdentifiers [vqa] [caption][grounding][refer] [identify][detection]Spatial</p>
<p>Table 2 :
2
The training datasets used for our model three-stage training.</p>
<p>Table 3 :
3
Results on multiple VQA tasks.We report top-1 accuracy for each task.Grounding column indicates whether the model incorporates visual localization capability.The best performance for each benchmark is indicated in bold.
PreprintMethodGrounding OKVQA GQAVSR (zero-shot) (zero-shot) (zero-shot) (zero-shot) IconVQA VizWiz HMFlamingo-9B✗44.7-31.8-28.857.0BLIP-2 (13B)✗45.941.050.940.619.653.7InstructBLIP (13B)✗-49.552.144.833.457.5MiniGPT-4 (13B)✗37.530.841.637.6--LLaVA (13B)✗54.441.351.243.0--Shikra (13B)✓47.2-----Ours (7B)✓56.960.360.647.732.958.2Ours (7B)-chat✓57.860.162.951.553.658.8MethodModel typesvalRefCOCO test-A test-BvalRefCOCO+ test-A test-BRefCOCOg val testAvgUNINEXT G-DINO-LSpecialist models92.64 94.33 90.56 93.1991.46 85.24 89.63 79.79 88.73 89.37 88.90 88.24 82.75 88.95 75.92 86.13 87.02 86.60VisionLLM-H-86.70-------OFA-L79.96 83.6776.39 68.29 76.00 61.75 67.57 67.58 72.65Shikra (7B) Shikra (13B)Generalist models87.01 90.61 87.83 91.1180.24 81.60 87.36 72.12 82.27 82.19 82.93 81.81 82.89 87.79 74.41 82.64 83.16 83.96Ours (7B)88.69 91.65 85.33 79.97 85.12 74.45 84.44 84.66 84.29Ours (7B)-chat88.06 91.29 84.30 79.58 85.52 73.32 84.19 84.31 83.70</p>
<p>Table 5 :
5
Task identifier ablation study on VQA benchmarks.With task identifier during the model training can overall improve VQA performances from multiple VQA benchmarks tifiers on VQA benchmarks.Both models were trained on 4xA100 GPUs for 24 hours with an equal number of training steps for multiple vision-language tasks.Results in Table5demonstrate the performance on multiple VQA benchmarks and consistently show that token identifier training benefits the overall performance of MiniGPT-v2.Specifically, our MiniGPT-v2 with task-oriented instruction training achieves 1.2% top-1 accuracy improvement on average.These ablation results Preprint can validate the clear advantage of adding task identifier tokens and support the use of multi-task identifiers for multi-task learning efficiency.
MethodCHAIRI ↓ CHAIRS ↓LenMiniGPT-49.231.5116.2mPLUG-Owl30.276.898.5LLaVA18.862.790.7MultiModal-GPT18.236.245.7MiniGPT-v2 (long)8.725.356.5MiniGPT-v2 (grounded)7.612.518.9MiniGPT-v2 (short)4.47.110.3</p>
<p>Table 6 :
6
Results on hallucination.We evaluate the hallucination of MiniGPT-v2 with different instructional templates and output three versions of captions for evaluation.For the "long" version, we use the prompt generate a brief description of the given image.For the "grounded" version, the instruction is [grounding] describe this image in as detailed as possible.For the "short" version, the prompt is [caption] briefly describe the image.</p>
<p>Table 6
6, we findthat our MiniGPT-v2 tends to generate theimage description with reduced halluci-nation compared to other baselines. Wehave evaluated three types of prompts inMiniGPT-v2. First, we use the promptgenerate a brief description of the givenimage without any specific task identifierwhich tends to produce more detailed im-
age descriptions.Then we provide the instruction prompt [grounding] describe this image in as detailed as possible for evaluating grounded image captions.Lastly, we prompt our model with [caption] briefly describe the image.With these task identifiers, MiniGPT-v2 is able to produce a variety of image descriptions with different levels of hallucination.As a result, all these three instruction variants have lower hallucination than our baseline, especially with the task specifiers of [caption] and [grounding].</p>
<p>Sharegpt. 2023</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 2022</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny, arXiv:2304.04227Video chatcaptioner: Towards the enriched spatiotemporal descriptions. 2023aarXiv preprint</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.151952023barXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Exploring the limits of masked visual representation learning at scale. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao, Eva, arXiv:2211.076362022arXiv preprint</p>
<p>Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, arXiv:2305.04790Multimodal-gpt: A vision and language model for dialogue with humans. 2023arXiv preprint</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Vizwiz grand challenge: Answering visual questions from blind people. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P Bigham, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Unnatural instructions: Tuning language models with (almost) no human labor. Or Honovich, Thomas Scialom, Omer Levy, Timo Schick, arXiv:2212.096892022arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. Preprint Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>The hateful memes challenge: Detecting hate speech in multimodal memes. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, Davide Testuggine, Advances in neural information processing systems. 332020</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023aarXiv preprint</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023barXiv preprint</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023a</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023barXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023carXiv preprint</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu, arXiv:2110.13214Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. 2021arXiv preprint</p>
<p>Generation and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019</p>
<p>Ocr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, 2019 international conference on document analysis and recognition (ICDAR). IEEE2019</p>
<p>. OpenAI. Introducing chatgpt. 2022</p>
<p>Im2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Advances in neural information processing systems. 242011</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Liwei Bryan A Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Preprint Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, arXiv:2112.114462021arXiv preprint</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, arXiv:1809.02156Object hallucination in image captioning. 2018arXiv preprint</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Matthias Franc ¸ois Yvon, Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021arXiv preprint</p>
<p>A-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, European Conference on Computer Vision. Springer2022</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Textcaps: a dataset for image captioningwith reading comprehension. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh, 2020</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Introducing mpt-7b: A new standard for open-source, commercially usable llms. Nlp Mosaicml, Team, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Multimodal few-shot learning with frozen language models. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill, Advances in Neural Information Processing Systems. 202134</p>
<p>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, International Conference on Machine Learning. PMLR2022</p>
<p>Large language model is also an open-ended decoder for vision-centric tasks. Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, arXiv:2305.111752023arXiv preprint</p>
<p>Universal instance perception as object discovery and retrieval. Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Modeling context in referring expressions. Licheng Preprint, Patrick Yu, Shan Poirson, Alexander C Yang, Tamara L Berg, Berg, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 201614Proceedings, Part II</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny, arXiv:2303.065942023aarXiv preprint</p>
<p>Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023barXiv preprint</p>
<p>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Abed Hasan, Kader Al, Vincent Hammoud, Kazuki Herrmann, Irie, arXiv:2305.17066Mindstorms in natural language-based societies of mind. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
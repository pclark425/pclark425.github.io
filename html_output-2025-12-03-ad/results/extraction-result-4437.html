<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4437 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4437</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4437</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-278996064</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22787v1.pdf" target="_blank">Can Large Language Models Match the Conclusions of Systematic Reviews?</a></p>
                <p><strong>Paper Abstract:</strong> Systematic reviews (SR), in which experts summarize and analyze evidence across individual studies to provide insights on a specialized topic, are a cornerstone for evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large language models (LLMs) to automate SR generation. However, the ability of LLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly characterized. We therefore ask: Can LLMs match the conclusions of systematic reviews written by clinical experts when given access to the same studies? To explore this question, we present MedEvidence, a benchmark pairing findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs on MedEvidence, including reasoning, non-reasoning, medical specialist, and models across varying sizes (from 7B-700B). Through our systematic evaluation, we find that reasoning does not necessarily improve performance, larger models do not consistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance tends to degrade as token length increases, their responses show overconfidence, and, contrary to human experts, all models show a lack of scientific skepticism toward low-quality findings. These results suggest that more work is still required before LLMs can reliably match the observations from expert-conducted SRs, even though these systems are already deployed and being used by clinicians. We release our codebase and benchmark to the broader research community to further investigate LLM-based SR systems.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4437.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4437.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedEvidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedEvidence benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-curated benchmark of 284 closed-form questions derived from 100 Cochrane systematic reviews paired with the underlying relevant studies, designed to test LLMs' ability to extract, reason over, and synthesize evidence across multiple papers to match expert SR conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MedEvidence</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A dataset and evaluation pipeline rather than a single LLM system: human annotators convert Cochrane SR conclusions into closed QA items, collect the cited source studies (full-text when available, otherwise abstracts), label evidence certainty (GRADE), compute source concordance, and provide metadata. The benchmark supplies the LLM with the same set of source documents used by human SR authors and evaluates LLM outputs against the SR conclusions using a fixed 5-class taxonomy (Higher, Lower, No Difference, Uncertain Effect, Insufficient Data).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>N/A (benchmark used to evaluate many models; DeepSeek V3 used as a strong baseline in analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Provision of raw study texts (abstracts/full-text) as context with closed-form QA prompts; models produce rationale + a single-class answer. Also single-source prompting was used to compute per-source agreement (source concordance).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>In-context multi-article synthesis via prompting (zero-shot basic prompt and an expert-guided prompt that asks the model to summarize design/population and grade evidence); multi-step chunk refinement (LangChain RefineDocumentsChain) used when inputs exceed context windows. No automated statistical meta-analysis is performed by the benchmark itself.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset assembled from 100 systematic reviews referencing 329 individual articles (114 full-text available); per-question relevant sources limited to <= 12 (annotator filter).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical / clinical systematic reviews across 10 medical specialties</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Closed-form comparative conclusions (Higher, Lower, No Difference, Uncertain Effect, Insufficient Data) plus optional rationale text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact-match accuracy (answer field), bootstrap 95% CIs (N=1000), per-class recall, instruction-following rate, source concordance, stratified analyses by evidence certainty, token length, and number of sources</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used to report LLM performance; e.g., top models (DeepSeek V3) achieved 62.40% (56.35, 68.45) accuracy and GPT-4.1 60.40% (54.30, 66.50); frontier LLMs failed to match SR conclusions in at least 37% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert systematic review conclusions (Cochrane SR conclusions) as ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs frequently disagree with SR conclusions; even best models have > 37% error rate relative to human SR conclusions and show lower performance on low-certainty evidence and uncertain-effect classes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides controlled evaluation showing LLMs struggle to replicate SR conclusions given same source studies: performance falls with longer token contexts, LLMs are overconfident and non-skeptical toward low-quality evidence, medical fine-tuning often degrades performance, and model size yields diminishing returns beyond ~70B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmark isolates synthesis step but does not test full SR pipeline (search/screening/risk-of-bias assessment); selection bias toward SRs with retrievable sources; closed-form labels simplify but do not capture narrative subtleties.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Observed diminishing returns beyond ~70B parameters; accuracy increases with source concordance and evidence certainty but decreases with combined token length of sources; reasoning-mode models did not uniformly outperform non-reasoning counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4437.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large generalist (non-reasoning) LLM evaluated in the paper; used as the strongest-performing model on MedEvidence and also used to estimate per-source agreement by answering questions using single sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepSeek V3 (evaluation usage)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluated by prompting with the set of relevant study texts (abstracts/full text) per question; required output: rationale + one-class answer. Also used in single-source prompting to compute source concordance by answering the question using only one study at a time.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DeepSeek V3 (671B parameters, generalist non-reasoning, 131K context limit per Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Zero-shot QA prompting over provided study texts; rationale-first format with answer field; also single-source QA prompts to assess whether each source 'agrees' with SR answer.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>In-context synthesis across multiple article chunks and iterative chunk-refinement when input exceeds context window; produces natural-language rationale rather than formal meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on MedEvidence which contains up to 12 relevant sources per question (aggregate dataset covers 329 articles across 100 SRs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical systematic review conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Rationale + categorical conclusion (Higher/Lower/No Difference/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact-match answer accuracy, per-class recall, instruction-following rate, bootstrap CIs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Average accuracy 62.40% (56.35, 68.45) on MedEvidence; achieves 92.45% accuracy at 100% source agreement vs. 41.21% at 0% source agreement in stratified analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cochrane SR conclusions (human experts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Does not match SR conclusions in at least ~37% of cases overall; strong dependence on source concordance and evidence certainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top-performing evaluated model but still far from perfect; shows better performance when sources unanimously agree and when evidence certainty is high; recognizes some limited indicators of study quality in select qualitative cases but generally lacks systematic skepticism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Overconfidence in outputs, limited ability to reconcile contradictory numerical evidence or perform formal meta-analysis, performance decline with long token inputs, sensitivity to prompt design (expert-guided prompt sometimes decreased performance).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>As a 671B model it shows strong performance relative to smaller models, but paper finds diminishing returns across families beyond ~70B parameters in general (DeepSeek family included in analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4437.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-capable generalist LLM family member evaluated by the authors (reasoning-mode variant of DeepSeek).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepSeek R1 (evaluation usage)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A reasoning-targeted LLM variant evaluated on MedEvidence; used with the same prompting setups (basic and expert-guided) and multi-step refinement when needed. Compared to DeepSeek V3 to analyze the effect of explicit reasoning incentives.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DeepSeek R1 (671B parameters, reasoning-enabled, 131K context limit per Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Zero-shot QA prompting with rationale + answer over provided study texts; allowed multi-step chunk refinement for long inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>In-context multi-document reasoning as elicited by prompts; no automated statistical meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Same MedEvidence setup (per-question relevant sources, dataset totals as above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical systematic review conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Rationale + categorical conclusion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact-match accuracy, per-class recall, instruction-following rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported but not always superior to non-reasoning counterpart; paper notes reasoning models do not consistently outperform comparable non-reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>DeepSeek V3 and human SR conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>DeepSeek R1 underperforms or does not consistently outperform DeepSeek V3 in this benchmark, illustrating that increased 'reasoning' at test-time does not guarantee better SR-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reasoning-targeted RLHF-style incentives do not necessarily translate to improved cross-document evidence synthesis on MedEvidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Same as general models: overconfidence, failure to weigh study quality appropriately, sensitivity to long inputs and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Part of the observation set showing that specialized reasoning training does not uniformly improve performance; results depend on family and distillation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4437.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clinfo.ai</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clinfo.ai</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source retrieval-augmented large language model system for answering medical questions using scientific literature (referenced in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clinfo.ai (retrieval-augmented LLM system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A retrieval-augmented generation (RAG) system that couples a literature retrieval component with an LLM to answer medical questions using scientific documents as sources; described in a referenced work (listed among related deployments).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (reference describes an open-source RAG pipeline; exact LLMs used are in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented reading of scientific literature (document retrieval + LLM question-answering over retrieved passages)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RAG-style in-context synthesis of retrieved passages into answers; likely includes passage-level summarization and consolidation but details are in the referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here (system described in cited work likely supports retrieval across many documents)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical / medical question answering using scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to medical questions grounded in retrieved literature; likely structured summaries or responses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper; refer to the Clinfo.ai reference for evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of deployed LLM-assisted tools integrated into SR or medical literature QA workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here; general RAG challenges include retrieval quality, grounding/factuality, and handling long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper for Clinfo.ai; RAG systems typically scale with retrieval index size and LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4437.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit (AI research assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI research assistant platform that uses LLMs to assist literature review tasks (mentioned as an example of LLM-assisted tools integrated into SR processes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicit: The ai research assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A commercial/hosted LLM-assisted research assistant used to support systematic review tasks such as literature search, screening, and summarization; referenced as an example of deployed tools that integrate LLMs into SR pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (platform uses proprietary LLM(s) per its documentation)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval over literature databases plus prompt-based summarization and QA over retrieved documents (as described in related-work mentions).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated summarization and aggregation of retrieved evidence into answers or synthesized outputs; specifics are in the product/reference documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here (designed to scale to many retrieved papers)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature with applications in biomedical evidence synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, synthesized answers, assistance for SR stages</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to indicate that LLM-assisted tools are already being incorporated into SR processes, motivating the need for benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here; general concerns include factuality, hallucination, and lack of critical evaluation of evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4437.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open Evidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Evidence (AI-powered medical information platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed AI-powered platform that uses LLMs to provide medical information and has been incorporated by some users into the SR process (mentioned in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open evidence: Ai-powered medical information platform.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Open Evidence</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Commercial/hosted LLM-backed medical information platform referenced as an example of LLMs being incorporated into evidence review workflows; specific architecture/details not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely retrieval-augmented QA and summarization over medical literature (details in product documentation rather than this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Likely in-context synthesis and summarization; explicit methods not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Medical information / evidence synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Medical answers, synthesized over literature</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate real-world deployments of LLM-based systems for medical literature tasks and the urgency for rigorous evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not specified here; general deployment risks highlighted in the paper include incorrect/misleading outputs and lack of skepticism.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4437.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Research (system card)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Research (OpenAI / Google product references)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Named deployed LLM-based research assistant products (Deep Research by OpenAI and Gemini Deep Research by Google) referenced as real-world LLM tools for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep research system card.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Research / Gemini Deep Research (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Commercial research-assistant products that couple LLM capabilities with literature access and retrieval to assist researchers; cited in the paper as examples of recently-deployed LLM systems for scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Commercial LLMs (OpenAI / Google proprietary models) â€” specific versions not enumerated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely retrieval-augmented document retrieval + prompt-driven summarization and QA (product descriptions cited in references).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Product-level in-context aggregation and summarization across retrieved literature; specifics outside the scope of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature / research assistance</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, synthesized research answers, literature assistance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper uses them as evidence that LLM-assisted systems are already integrated into research workflows, motivating the controlled benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in detail here; general concerns include hallucination, lack of skepticism, and need for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4437.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain RefineDocumentsChain (iterative chunk refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step refinement approach used to handle inputs exceeding an LLM's context window by iteratively digesting chunks and refining a consolidated answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangChain RefineDocumentsChain</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A procedural approach (library component) used to break long documents into chunks, have the model summarize or answer on each chunk, and iteratively refine a combined answer across chunks so the model can handle inputs larger than its context window.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used as a chaining mechanism with multiple LLMs in experiments (models in the paper included GPT-4.1, DeepSeek family, Llama variants, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chunking of long texts followed by per-chunk prompting and iterative aggregation/refinement of partial answers into a final synthesized response.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step refinement / progressive summarization that composes chunk-level outputs into a global answer; no explicit meta-analysis is computed by the chain itself.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per-question across the provided relevant sources (up to ~12 per question in MedEvidence); overall dataset uses 329 articles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied to biomedical systematic-review-style QA in this work; general technique applicable across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined natural-language rationale and categorical conclusion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Effectiveness of chain measured indirectly via downstream model accuracy on MedEvidence (no separate metric for chain itself reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used whenever input exceeded model context; multi-step refinement enabled use of long inputs but did not fully overcome performance degradation with very long token lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly against single-pass prompting with limited context; no direct ablation numbers provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Multi-step refinement allowed models to accept longer inputs but overall model accuracy still decreased with combined token length; some models benefited more than others.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chunk-refinement enables processing of longer documents but does not fully solve long-context reasoning and synthesis challenges; performance still declines as token length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Iterative summarization can lose information, and the approach depends on effective chunking and prompt design; models still struggle to use long-context information reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Helps scale to larger document sets but observed per-paper scaling limits remain (performance declines with very large combined token counts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4437.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reason et al. (meta-analysis automation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reason et al. (2024) â€” LLMs to extract numerical data and generate meta-analysis code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study that applied LLMs to extract numerical data from abstracts and to generate executable code to perform meta-analyses (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-assisted numerical extraction + code generation for meta-analysis (as reported by Reason et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach where LLMs are prompted to extract quantitative data from abstracts (numerical counts, effect sizes) and then generate analysis code (e.g., R/Python) to perform meta-analytic computations; presented as small-sample case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in detail here (referenced study contains specifics)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompted numerical extraction from abstracts and generation of executable analysis scripts</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated statistical aggregation via generated code (meta-analysis) rather than purely natural-language synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Small case studies in referenced work (N = 4 case studies noted in paper summary); not large-scale in that prior work</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Evidence synthesis / meta-analysis in biomedical research</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted numeric data and executable meta-analysis code / analytical results</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Case-study comparisons to human-executed meta-analyses (as reported in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described as promising in limited case studies but limited scope; the MedEvidence paper cites it as small-N (4 case studies).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-executed extraction and meta-analytic code</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Promising but limited to a few case studies; not generalized or large-scale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows potential for LLMs to automate numeric extraction and meta-analytic computation but is limited in scale and rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small sample size, reliance on abstracts, need for expert oversight; does not test broad generalization across many SRs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explored broadly in the referenced work; MedEvidence presents larger-scale evaluation to address this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4437.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedREQAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedREQAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed QA dataset designed to probe LLMs' ability to verify claims derived from Cochrane systematic reviews, focusing on recall of internal knowledge rather than retrieval from provided sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MedREQAL: Examining medical knowledge recall of large language models via question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MedREQAL dataset</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-curated closed QA benchmark where models are asked to verify claims derived from Cochrane SRs but the dataset does not provide the original SR sources â€” it assesses models' internal knowledge/memory rather than cross-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>N/A (dataset used to evaluate LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not applicable; dataset does not provide source documents to the model (tests recall).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not applicable (focus on recall / claim verification)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset size reported in reference (not large and no source provisioning in MedEvidence).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical / Cochrane SR claims</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Closed-form QA answers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy of claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here; included as related work to contrast with MedEvidence which supplies sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Internal model knowledge vs. ground-truth SR claims</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Illustrates difference between internal-knowledge benchmarks and retrieval-augmented / source-grounded synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not test cross-document reasoning since sources are not provided; evaluates memorized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4437.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HealthFC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HealthFC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset/benchmark that tasks models with verifying health claims using pre-synthesized analyses from a medical fact-checking source; focuses on verification rather than multi-source synthesis from raw studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HealthFC: Verifying health claims with evidence-based medical fact-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HealthFC dataset/system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A fact-checking benchmark where models verify claims using pre-synthesized analyses from a medical fact-checking site; the evidence provided is already synthesized, so the task relies less on cross-document reasoning over unsynthesized primary studies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>N/A (benchmark used to evaluate LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Verification against pre-synthesized evidence (not direct extraction from raw papers)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not the core focus; uses pre-synthesized evidence rather than requiring the model to synthesize across raw sources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset size described in the referenced work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Consumer health / claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Claim verification labels and explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy of verification, factuality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper; included as related-work contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human fact-checking outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates limitations of prior benchmarks that use synthesized evidence rather than challenging models to synthesize from raw study texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not reflect the complexity of synthesizing unsynthesized primary literature.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4437.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConflictingQA / ClashEval / ConflictBank / KNOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmarks for LLM behavior in presence of conflicting sources (ConflictingQA, ClashEval, ConflictBank, KNOT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of prior benchmarks assessing how models handle conflicting evidence or contradictions, across domains; referenced as related work but generally not specialized to peer-reviewed multi-study synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ConflictingQA; Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence.; ConflictBank; KNOT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conflicting-sources benchmarks (ConflictingQA, ClashEval, ConflictBank, KNOT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmarks that probe LLMs' responses when presented with conflicting information from different sources (some focus on internal vs external conflicts, some on synthetic conflict types); they use curated conflicting evidence to test model robustness to contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Various LLMs in referenced studies (not enumerated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>QA over curated conflicting content or single-source perturbations, not necessarily over peer-reviewed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Conflict-resolution style evaluation; not focused on aggregative meta-analytic synthesis across many primary studies.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Benchmarks vary; typically small-to-moderate size and often synthetic or from web sources rather than journal articles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-domain (some include medical subsets) focusing on conflict resolution</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Model judgments about which side of a conflict is supported or reconciled answers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy on conflict resolution tasks, calibration measures comparing internal priors vs external evidence</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Summarized in referenced works (not given numerically here); cited to show prior work on conflicts but limited scope for SR-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Often compares model internal prior vs external evidence; details in each benchmark paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Varies per benchmark; generally indicates models struggle with conflicting information.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Existing conflict benchmarks often use non-peer-reviewed or single-source conflicts and do not capture full complexity of systematic-review style multi-study synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmarks limited in scale, domain specificity, or realism (often synthetic or non-peer-reviewed sources).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not a central focus of these benchmark papers as referenced here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4437.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4437.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenBioLLM / HuatuoGPT / Llama / Qwen (evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Examples of evaluated LLM models (OpenBioLLM, HuatuoGPT-o1, Llama 3/4, Qwen2.5/3, GPT-4.1, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variety of generalist and medically fine-tuned large language models evaluated on MedEvidence to measure their ability to synthesize conclusions from multiple papers; used as experimental subjects rather than full systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evaluated LLMs (OpenBioLLM; HuatuoGPT-o1; Llama 3/3.1/3.3/4; Qwen2.5/Qwen3; GPT-4.1; OpenThinker2; GPT-o1; others)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These are pre-existing LLMs (some medically fine-tuned, some reasoning-targeted) that were zero-shot prompted (basic prompt) and also evaluated under an expert-guided prompt across MedEvidence. Models received the relevant article texts and produced rationale + a categorical conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Examples include OpenBioLLM (8B, 70B), HuatuoGPT-o1 (7B, 70B, med-reasoning), Llama 3.x/4 variants (8B-405B range), Qwen2.5 (7B-72B), Qwen3 (235B hybrid), GPT-4.1 (commercial), OpenThinker2 (32B reasoning), GPT-o1, etc. (as listed in Table 3 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based QA over provided study texts (abstracts/full text), chunking + LangChain refine when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>In-context synthesis across multiple article chunks; expert-guided prompt asked models to summarize study design/population and grade evidence; no standardized automatic aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Tested across MedEvidence dataset: up to 12 relevant sources per question, 329 articles total across 100 SRs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical systematic review conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Rationale and categorical synthesis answers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact-match accuracy, per-class recall, instruction-following rate, bootstrap CIs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Best models (DeepSeek V3, GPT-4.1) achieved ~60-62% accuracy; many models performed worse; medical fine-tuning often degraded performance; observed diminishing returns beyond 70B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cochrane SR conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>All evaluated models fall short of matching SR conclusions on a substantial fraction of questions (at least 37% failure for frontier models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Medical fine-tuning did not reliably improve SR-matching accuracy and sometimes degraded it; larger size gives gains up to ~70B but yields diminishing returns beyond; models overconfident and poor at expressing uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Instruction-following failures in some models; long-context degradation; inability to properly weigh low-quality evidence; inconsistent benefits from reasoning-targeted variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance improved from 7Bâ†’70B in some families but flattened/diminished beyond ~70B; performance declines with increasing combined token length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Match the Conclusions of Systematic Reviews?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature. <em>(Rating: 2)</em></li>
                <li>Deep research system card. <em>(Rating: 2)</em></li>
                <li>Elicit: The ai research assistant. <em>(Rating: 2)</em></li>
                <li>Open evidence: Ai-powered medical information platform. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models. <em>(Rating: 2)</em></li>
                <li>MedREQAL: Examining medical knowledge recall of large language models via question answering. <em>(Rating: 1)</em></li>
                <li>HealthFC: Verifying health claims with evidence-based medical fact-checking. <em>(Rating: 1)</em></li>
                <li>Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4437",
    "paper_id": "paper-278996064",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "MedEvidence",
            "name_full": "MedEvidence benchmark",
            "brief_description": "A human-curated benchmark of 284 closed-form questions derived from 100 Cochrane systematic reviews paired with the underlying relevant studies, designed to test LLMs' ability to extract, reason over, and synthesize evidence across multiple papers to match expert SR conclusions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MedEvidence",
            "system_description": "A dataset and evaluation pipeline rather than a single LLM system: human annotators convert Cochrane SR conclusions into closed QA items, collect the cited source studies (full-text when available, otherwise abstracts), label evidence certainty (GRADE), compute source concordance, and provide metadata. The benchmark supplies the LLM with the same set of source documents used by human SR authors and evaluates LLM outputs against the SR conclusions using a fixed 5-class taxonomy (Higher, Lower, No Difference, Uncertain Effect, Insufficient Data).",
            "llm_model_used": "N/A (benchmark used to evaluate many models; DeepSeek V3 used as a strong baseline in analyses)",
            "extraction_technique": "Provision of raw study texts (abstracts/full-text) as context with closed-form QA prompts; models produce rationale + a single-class answer. Also single-source prompting was used to compute per-source agreement (source concordance).",
            "synthesis_technique": "In-context multi-article synthesis via prompting (zero-shot basic prompt and an expert-guided prompt that asks the model to summarize design/population and grade evidence); multi-step chunk refinement (LangChain RefineDocumentsChain) used when inputs exceed context windows. No automated statistical meta-analysis is performed by the benchmark itself.",
            "number_of_papers": "Dataset assembled from 100 systematic reviews referencing 329 individual articles (114 full-text available); per-question relevant sources limited to &lt;= 12 (annotator filter).",
            "domain_or_topic": "Biomedical / clinical systematic reviews across 10 medical specialties",
            "output_type": "Closed-form comparative conclusions (Higher, Lower, No Difference, Uncertain Effect, Insufficient Data) plus optional rationale text",
            "evaluation_metrics": "Exact-match accuracy (answer field), bootstrap 95% CIs (N=1000), per-class recall, instruction-following rate, source concordance, stratified analyses by evidence certainty, token length, and number of sources",
            "performance_results": "Used to report LLM performance; e.g., top models (DeepSeek V3) achieved 62.40% (56.35, 68.45) accuracy and GPT-4.1 60.40% (54.30, 66.50); frontier LLMs failed to match SR conclusions in at least 37% of cases.",
            "comparison_baseline": "Human expert systematic review conclusions (Cochrane SR conclusions) as ground truth",
            "performance_vs_baseline": "LLMs frequently disagree with SR conclusions; even best models have &gt; 37% error rate relative to human SR conclusions and show lower performance on low-certainty evidence and uncertain-effect classes.",
            "key_findings": "Provides controlled evaluation showing LLMs struggle to replicate SR conclusions given same source studies: performance falls with longer token contexts, LLMs are overconfident and non-skeptical toward low-quality evidence, medical fine-tuning often degrades performance, and model size yields diminishing returns beyond ~70B parameters.",
            "limitations_challenges": "Benchmark isolates synthesis step but does not test full SR pipeline (search/screening/risk-of-bias assessment); selection bias toward SRs with retrievable sources; closed-form labels simplify but do not capture narrative subtleties.",
            "scaling_behavior": "Observed diminishing returns beyond ~70B parameters; accuracy increases with source concordance and evidence certainty but decreases with combined token length of sources; reasoning-mode models did not uniformly outperform non-reasoning counterparts.",
            "uuid": "e4437.0",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek V3",
            "name_full": "DeepSeek V3",
            "brief_description": "A large generalist (non-reasoning) LLM evaluated in the paper; used as the strongest-performing model on MedEvidence and also used to estimate per-source agreement by answering questions using single sources.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepSeek V3 (evaluation usage)",
            "system_description": "Evaluated by prompting with the set of relevant study texts (abstracts/full text) per question; required output: rationale + one-class answer. Also used in single-source prompting to compute source concordance by answering the question using only one study at a time.",
            "llm_model_used": "DeepSeek V3 (671B parameters, generalist non-reasoning, 131K context limit per Table 3)",
            "extraction_technique": "Zero-shot QA prompting over provided study texts; rationale-first format with answer field; also single-source QA prompts to assess whether each source 'agrees' with SR answer.",
            "synthesis_technique": "In-context synthesis across multiple article chunks and iterative chunk-refinement when input exceeds context window; produces natural-language rationale rather than formal meta-analysis.",
            "number_of_papers": "Evaluated on MedEvidence which contains up to 12 relevant sources per question (aggregate dataset covers 329 articles across 100 SRs).",
            "domain_or_topic": "Biomedical systematic review conclusions",
            "output_type": "Rationale + categorical conclusion (Higher/Lower/No Difference/etc.)",
            "evaluation_metrics": "Exact-match answer accuracy, per-class recall, instruction-following rate, bootstrap CIs",
            "performance_results": "Average accuracy 62.40% (56.35, 68.45) on MedEvidence; achieves 92.45% accuracy at 100% source agreement vs. 41.21% at 0% source agreement in stratified analyses.",
            "comparison_baseline": "Cochrane SR conclusions (human experts)",
            "performance_vs_baseline": "Does not match SR conclusions in at least ~37% of cases overall; strong dependence on source concordance and evidence certainty.",
            "key_findings": "Top-performing evaluated model but still far from perfect; shows better performance when sources unanimously agree and when evidence certainty is high; recognizes some limited indicators of study quality in select qualitative cases but generally lacks systematic skepticism.",
            "limitations_challenges": "Overconfidence in outputs, limited ability to reconcile contradictory numerical evidence or perform formal meta-analysis, performance decline with long token inputs, sensitivity to prompt design (expert-guided prompt sometimes decreased performance).",
            "scaling_behavior": "As a 671B model it shows strong performance relative to smaller models, but paper finds diminishing returns across families beyond ~70B parameters in general (DeepSeek family included in analyses).",
            "uuid": "e4437.1",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-capable generalist LLM family member evaluated by the authors (reasoning-mode variant of DeepSeek).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepSeek R1 (evaluation usage)",
            "system_description": "A reasoning-targeted LLM variant evaluated on MedEvidence; used with the same prompting setups (basic and expert-guided) and multi-step refinement when needed. Compared to DeepSeek V3 to analyze the effect of explicit reasoning incentives.",
            "llm_model_used": "DeepSeek R1 (671B parameters, reasoning-enabled, 131K context limit per Table 3)",
            "extraction_technique": "Zero-shot QA prompting with rationale + answer over provided study texts; allowed multi-step chunk refinement for long inputs.",
            "synthesis_technique": "In-context multi-document reasoning as elicited by prompts; no automated statistical meta-analysis.",
            "number_of_papers": "Same MedEvidence setup (per-question relevant sources, dataset totals as above).",
            "domain_or_topic": "Biomedical systematic review conclusions",
            "output_type": "Rationale + categorical conclusion",
            "evaluation_metrics": "Exact-match accuracy, per-class recall, instruction-following rate",
            "performance_results": "Reported but not always superior to non-reasoning counterpart; paper notes reasoning models do not consistently outperform comparable non-reasoning models.",
            "comparison_baseline": "DeepSeek V3 and human SR conclusions",
            "performance_vs_baseline": "DeepSeek R1 underperforms or does not consistently outperform DeepSeek V3 in this benchmark, illustrating that increased 'reasoning' at test-time does not guarantee better SR-matching.",
            "key_findings": "Reasoning-targeted RLHF-style incentives do not necessarily translate to improved cross-document evidence synthesis on MedEvidence.",
            "limitations_challenges": "Same as general models: overconfidence, failure to weigh study quality appropriately, sensitivity to long inputs and prompt design.",
            "scaling_behavior": "Part of the observation set showing that specialized reasoning training does not uniformly improve performance; results depend on family and distillation variants.",
            "uuid": "e4437.2",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Clinfo.ai",
            "name_full": "Clinfo.ai",
            "brief_description": "An open-source retrieval-augmented large language model system for answering medical questions using scientific literature (referenced in the paper).",
            "citation_title": "Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature.",
            "mention_or_use": "mention",
            "system_name": "Clinfo.ai (retrieval-augmented LLM system)",
            "system_description": "A retrieval-augmented generation (RAG) system that couples a literature retrieval component with an LLM to answer medical questions using scientific documents as sources; described in a referenced work (listed among related deployments).",
            "llm_model_used": "Not specified in this paper (reference describes an open-source RAG pipeline; exact LLMs used are in the cited work).",
            "extraction_technique": "Retrieval-augmented reading of scientific literature (document retrieval + LLM question-answering over retrieved passages)",
            "synthesis_technique": "RAG-style in-context synthesis of retrieved passages into answers; likely includes passage-level summarization and consolidation but details are in the referenced paper.",
            "number_of_papers": "Not specified here (system described in cited work likely supports retrieval across many documents)",
            "domain_or_topic": "Biomedical / medical question answering using scientific literature",
            "output_type": "Answers to medical questions grounded in retrieved literature; likely structured summaries or responses",
            "evaluation_metrics": "Not detailed in this paper; refer to the Clinfo.ai reference for evaluation metrics",
            "performance_results": "Not reported in this paper",
            "comparison_baseline": "Not reported here",
            "performance_vs_baseline": "Not reported here",
            "key_findings": "Cited as an example of deployed LLM-assisted tools integrated into SR or medical literature QA workflows.",
            "limitations_challenges": "Not detailed here; general RAG challenges include retrieval quality, grounding/factuality, and handling long contexts.",
            "scaling_behavior": "Not discussed in this paper for Clinfo.ai; RAG systems typically scale with retrieval index size and LLM capability.",
            "uuid": "e4437.3",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Elicit",
            "name_full": "Elicit (AI research assistant)",
            "brief_description": "An AI research assistant platform that uses LLMs to assist literature review tasks (mentioned as an example of LLM-assisted tools integrated into SR processes).",
            "citation_title": "Elicit: The ai research assistant.",
            "mention_or_use": "mention",
            "system_name": "Elicit",
            "system_description": "A commercial/hosted LLM-assisted research assistant used to support systematic review tasks such as literature search, screening, and summarization; referenced as an example of deployed tools that integrate LLMs into SR pipelines.",
            "llm_model_used": "Not specified in this paper (platform uses proprietary LLM(s) per its documentation)",
            "extraction_technique": "Retrieval over literature databases plus prompt-based summarization and QA over retrieved documents (as described in related-work mentions).",
            "synthesis_technique": "Automated summarization and aggregation of retrieved evidence into answers or synthesized outputs; specifics are in the product/reference documentation.",
            "number_of_papers": "Not specified here (designed to scale to many retrieved papers)",
            "domain_or_topic": "General scientific literature with applications in biomedical evidence synthesis",
            "output_type": "Summaries, synthesized answers, assistance for SR stages",
            "evaluation_metrics": "Not provided in this paper",
            "performance_results": "Not provided in this paper",
            "comparison_baseline": "Not provided in this paper",
            "performance_vs_baseline": "Not provided in this paper",
            "key_findings": "Cited to indicate that LLM-assisted tools are already being incorporated into SR processes, motivating the need for benchmarking.",
            "limitations_challenges": "Not detailed here; general concerns include factuality, hallucination, and lack of critical evaluation of evidence.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4437.4",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Open Evidence",
            "name_full": "Open Evidence (AI-powered medical information platform)",
            "brief_description": "A deployed AI-powered platform that uses LLMs to provide medical information and has been incorporated by some users into the SR process (mentioned in the paper).",
            "citation_title": "Open evidence: Ai-powered medical information platform.",
            "mention_or_use": "mention",
            "system_name": "Open Evidence",
            "system_description": "Commercial/hosted LLM-backed medical information platform referenced as an example of LLMs being incorporated into evidence review workflows; specific architecture/details not provided in the paper.",
            "llm_model_used": "Not specified in this paper",
            "extraction_technique": "Likely retrieval-augmented QA and summarization over medical literature (details in product documentation rather than this paper).",
            "synthesis_technique": "Likely in-context synthesis and summarization; explicit methods not described here.",
            "number_of_papers": "Not specified",
            "domain_or_topic": "Medical information / evidence synthesis",
            "output_type": "Medical answers, synthesized over literature",
            "evaluation_metrics": "Not provided in this paper",
            "performance_results": "Not provided in this paper",
            "comparison_baseline": "Not provided in this paper",
            "performance_vs_baseline": "Not provided in this paper",
            "key_findings": "Cited to illustrate real-world deployments of LLM-based systems for medical literature tasks and the urgency for rigorous evaluation.",
            "limitations_challenges": "Not specified here; general deployment risks highlighted in the paper include incorrect/misleading outputs and lack of skepticism.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4437.5",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Deep Research (system card)",
            "name_full": "Deep Research (OpenAI / Google product references)",
            "brief_description": "Named deployed LLM-based research assistant products (Deep Research by OpenAI and Gemini Deep Research by Google) referenced as real-world LLM tools for literature synthesis.",
            "citation_title": "Deep research system card.",
            "mention_or_use": "mention",
            "system_name": "Deep Research / Gemini Deep Research (referenced)",
            "system_description": "Commercial research-assistant products that couple LLM capabilities with literature access and retrieval to assist researchers; cited in the paper as examples of recently-deployed LLM systems for scientific synthesis.",
            "llm_model_used": "Commercial LLMs (OpenAI / Google proprietary models) â€” specific versions not enumerated in this paper",
            "extraction_technique": "Likely retrieval-augmented document retrieval + prompt-driven summarization and QA (product descriptions cited in references).",
            "synthesis_technique": "Product-level in-context aggregation and summarization across retrieved literature; specifics outside the scope of this paper.",
            "number_of_papers": "Not specified",
            "domain_or_topic": "General scientific literature / research assistance",
            "output_type": "Summaries, synthesized research answers, literature assistance",
            "evaluation_metrics": "Not provided in this paper",
            "performance_results": "Not provided in this paper",
            "comparison_baseline": "Not provided in this paper",
            "performance_vs_baseline": "Not provided in this paper",
            "key_findings": "Paper uses them as evidence that LLM-assisted systems are already integrated into research workflows, motivating the controlled benchmark.",
            "limitations_challenges": "Not discussed in detail here; general concerns include hallucination, lack of skepticism, and need for validation.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4437.6",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LangChain Refine",
            "name_full": "LangChain RefineDocumentsChain (iterative chunk refinement)",
            "brief_description": "A multi-step refinement approach used to handle inputs exceeding an LLM's context window by iteratively digesting chunks and refining a consolidated answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LangChain RefineDocumentsChain",
            "system_description": "A procedural approach (library component) used to break long documents into chunks, have the model summarize or answer on each chunk, and iteratively refine a combined answer across chunks so the model can handle inputs larger than its context window.",
            "llm_model_used": "Used as a chaining mechanism with multiple LLMs in experiments (models in the paper included GPT-4.1, DeepSeek family, Llama variants, etc.).",
            "extraction_technique": "Chunking of long texts followed by per-chunk prompting and iterative aggregation/refinement of partial answers into a final synthesized response.",
            "synthesis_technique": "Multi-step refinement / progressive summarization that composes chunk-level outputs into a global answer; no explicit meta-analysis is computed by the chain itself.",
            "number_of_papers": "Applied per-question across the provided relevant sources (up to ~12 per question in MedEvidence); overall dataset uses 329 articles.",
            "domain_or_topic": "Applied to biomedical systematic-review-style QA in this work; general technique applicable across domains.",
            "output_type": "Refined natural-language rationale and categorical conclusion",
            "evaluation_metrics": "Effectiveness of chain measured indirectly via downstream model accuracy on MedEvidence (no separate metric for chain itself reported).",
            "performance_results": "Used whenever input exceeded model context; multi-step refinement enabled use of long inputs but did not fully overcome performance degradation with very long token lengths.",
            "comparison_baseline": "Compared implicitly against single-pass prompting with limited context; no direct ablation numbers provided here.",
            "performance_vs_baseline": "Multi-step refinement allowed models to accept longer inputs but overall model accuracy still decreased with combined token length; some models benefited more than others.",
            "key_findings": "Chunk-refinement enables processing of longer documents but does not fully solve long-context reasoning and synthesis challenges; performance still declines as token length increases.",
            "limitations_challenges": "Iterative summarization can lose information, and the approach depends on effective chunking and prompt design; models still struggle to use long-context information reliably.",
            "scaling_behavior": "Helps scale to larger document sets but observed per-paper scaling limits remain (performance declines with very large combined token counts).",
            "uuid": "e4437.7",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Reason et al. (meta-analysis automation)",
            "name_full": "Reason et al. (2024) â€” LLMs to extract numerical data and generate meta-analysis code",
            "brief_description": "A prior study that applied LLMs to extract numerical data from abstracts and to generate executable code to perform meta-analyses (cited as related work).",
            "citation_title": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-assisted numerical extraction + code generation for meta-analysis (as reported by Reason et al.)",
            "system_description": "An approach where LLMs are prompted to extract quantitative data from abstracts (numerical counts, effect sizes) and then generate analysis code (e.g., R/Python) to perform meta-analytic computations; presented as small-sample case studies.",
            "llm_model_used": "Not specified in detail here (referenced study contains specifics)",
            "extraction_technique": "Prompted numerical extraction from abstracts and generation of executable analysis scripts",
            "synthesis_technique": "Automated statistical aggregation via generated code (meta-analysis) rather than purely natural-language synthesis",
            "number_of_papers": "Small case studies in referenced work (N = 4 case studies noted in paper summary); not large-scale in that prior work",
            "domain_or_topic": "Evidence synthesis / meta-analysis in biomedical research",
            "output_type": "Extracted numeric data and executable meta-analysis code / analytical results",
            "evaluation_metrics": "Case-study comparisons to human-executed meta-analyses (as reported in the referenced work)",
            "performance_results": "Described as promising in limited case studies but limited scope; the MedEvidence paper cites it as small-N (4 case studies).",
            "comparison_baseline": "Human-executed extraction and meta-analytic code",
            "performance_vs_baseline": "Promising but limited to a few case studies; not generalized or large-scale.",
            "key_findings": "Shows potential for LLMs to automate numeric extraction and meta-analytic computation but is limited in scale and rigor.",
            "limitations_challenges": "Small sample size, reliance on abstracts, need for expert oversight; does not test broad generalization across many SRs.",
            "scaling_behavior": "Not explored broadly in the referenced work; MedEvidence presents larger-scale evaluation to address this gap.",
            "uuid": "e4437.8",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MedREQAL",
            "name_full": "MedREQAL",
            "brief_description": "A closed QA dataset designed to probe LLMs' ability to verify claims derived from Cochrane systematic reviews, focusing on recall of internal knowledge rather than retrieval from provided sources.",
            "citation_title": "MedREQAL: Examining medical knowledge recall of large language models via question answering.",
            "mention_or_use": "mention",
            "system_name": "MedREQAL dataset",
            "system_description": "An LLM-curated closed QA benchmark where models are asked to verify claims derived from Cochrane SRs but the dataset does not provide the original SR sources â€” it assesses models' internal knowledge/memory rather than cross-document synthesis.",
            "llm_model_used": "N/A (dataset used to evaluate LLMs)",
            "extraction_technique": "Not applicable; dataset does not provide source documents to the model (tests recall).",
            "synthesis_technique": "Not applicable (focus on recall / claim verification)",
            "number_of_papers": "Dataset size reported in reference (not large and no source provisioning in MedEvidence).",
            "domain_or_topic": "Biomedical / Cochrane SR claims",
            "output_type": "Closed-form QA answers",
            "evaluation_metrics": "Accuracy of claim verification",
            "performance_results": "Not reported here; included as related work to contrast with MedEvidence which supplies sources.",
            "comparison_baseline": "Internal model knowledge vs. ground-truth SR claims",
            "performance_vs_baseline": "Not detailed in this paper.",
            "key_findings": "Illustrates difference between internal-knowledge benchmarks and retrieval-augmented / source-grounded synthesis tasks.",
            "limitations_challenges": "Does not test cross-document reasoning since sources are not provided; evaluates memorized knowledge.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4437.9",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HealthFC",
            "name_full": "HealthFC",
            "brief_description": "A dataset/benchmark that tasks models with verifying health claims using pre-synthesized analyses from a medical fact-checking source; focuses on verification rather than multi-source synthesis from raw studies.",
            "citation_title": "HealthFC: Verifying health claims with evidence-based medical fact-checking.",
            "mention_or_use": "mention",
            "system_name": "HealthFC dataset/system",
            "system_description": "A fact-checking benchmark where models verify claims using pre-synthesized analyses from a medical fact-checking site; the evidence provided is already synthesized, so the task relies less on cross-document reasoning over unsynthesized primary studies.",
            "llm_model_used": "N/A (benchmark used to evaluate LLMs)",
            "extraction_technique": "Verification against pre-synthesized evidence (not direct extraction from raw papers)",
            "synthesis_technique": "Not the core focus; uses pre-synthesized evidence rather than requiring the model to synthesize across raw sources.",
            "number_of_papers": "Dataset size described in the referenced work (not detailed here).",
            "domain_or_topic": "Consumer health / claim verification",
            "output_type": "Claim verification labels and explanations",
            "evaluation_metrics": "Accuracy of verification, factuality",
            "performance_results": "Not reported in this paper; included as related-work contrast.",
            "comparison_baseline": "Human fact-checking outputs",
            "performance_vs_baseline": "Not detailed here",
            "key_findings": "Demonstrates limitations of prior benchmarks that use synthesized evidence rather than challenging models to synthesize from raw study texts.",
            "limitations_challenges": "Does not reflect the complexity of synthesizing unsynthesized primary literature.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4437.10",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ConflictingQA / ClashEval / ConflictBank / KNOT",
            "name_full": "Benchmarks for LLM behavior in presence of conflicting sources (ConflictingQA, ClashEval, ConflictBank, KNOT)",
            "brief_description": "A collection of prior benchmarks assessing how models handle conflicting evidence or contradictions, across domains; referenced as related work but generally not specialized to peer-reviewed multi-study synthesis.",
            "citation_title": "ConflictingQA; Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence.; ConflictBank; KNOT",
            "mention_or_use": "mention",
            "system_name": "Conflicting-sources benchmarks (ConflictingQA, ClashEval, ConflictBank, KNOT)",
            "system_description": "Benchmarks that probe LLMs' responses when presented with conflicting information from different sources (some focus on internal vs external conflicts, some on synthetic conflict types); they use curated conflicting evidence to test model robustness to contradictions.",
            "llm_model_used": "Various LLMs in referenced studies (not enumerated in this paper)",
            "extraction_technique": "QA over curated conflicting content or single-source perturbations, not necessarily over peer-reviewed literature.",
            "synthesis_technique": "Conflict-resolution style evaluation; not focused on aggregative meta-analytic synthesis across many primary studies.",
            "number_of_papers": "Benchmarks vary; typically small-to-moderate size and often synthetic or from web sources rather than journal articles.",
            "domain_or_topic": "Multi-domain (some include medical subsets) focusing on conflict resolution",
            "output_type": "Model judgments about which side of a conflict is supported or reconciled answers",
            "evaluation_metrics": "Accuracy on conflict resolution tasks, calibration measures comparing internal priors vs external evidence",
            "performance_results": "Summarized in referenced works (not given numerically here); cited to show prior work on conflicts but limited scope for SR-like tasks.",
            "comparison_baseline": "Often compares model internal prior vs external evidence; details in each benchmark paper.",
            "performance_vs_baseline": "Varies per benchmark; generally indicates models struggle with conflicting information.",
            "key_findings": "Existing conflict benchmarks often use non-peer-reviewed or single-source conflicts and do not capture full complexity of systematic-review style multi-study synthesis.",
            "limitations_challenges": "Benchmarks limited in scale, domain specificity, or realism (often synthetic or non-peer-reviewed sources).",
            "scaling_behavior": "Not a central focus of these benchmark papers as referenced here.",
            "uuid": "e4437.11",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "OpenBioLLM / HuatuoGPT / Llama / Qwen (evaluated models)",
            "name_full": "Examples of evaluated LLM models (OpenBioLLM, HuatuoGPT-o1, Llama 3/4, Qwen2.5/3, GPT-4.1, etc.)",
            "brief_description": "A variety of generalist and medically fine-tuned large language models evaluated on MedEvidence to measure their ability to synthesize conclusions from multiple papers; used as experimental subjects rather than full systems.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Evaluated LLMs (OpenBioLLM; HuatuoGPT-o1; Llama 3/3.1/3.3/4; Qwen2.5/Qwen3; GPT-4.1; OpenThinker2; GPT-o1; others)",
            "system_description": "These are pre-existing LLMs (some medically fine-tuned, some reasoning-targeted) that were zero-shot prompted (basic prompt) and also evaluated under an expert-guided prompt across MedEvidence. Models received the relevant article texts and produced rationale + a categorical conclusion.",
            "llm_model_used": "Examples include OpenBioLLM (8B, 70B), HuatuoGPT-o1 (7B, 70B, med-reasoning), Llama 3.x/4 variants (8B-405B range), Qwen2.5 (7B-72B), Qwen3 (235B hybrid), GPT-4.1 (commercial), OpenThinker2 (32B reasoning), GPT-o1, etc. (as listed in Table 3 of the paper).",
            "extraction_technique": "Prompt-based QA over provided study texts (abstracts/full text), chunking + LangChain refine when needed.",
            "synthesis_technique": "In-context synthesis across multiple article chunks; expert-guided prompt asked models to summarize study design/population and grade evidence; no standardized automatic aggregation.",
            "number_of_papers": "Tested across MedEvidence dataset: up to 12 relevant sources per question, 329 articles total across 100 SRs.",
            "domain_or_topic": "Biomedical systematic review conclusions",
            "output_type": "Rationale and categorical synthesis answers",
            "evaluation_metrics": "Exact-match accuracy, per-class recall, instruction-following rate, bootstrap CIs",
            "performance_results": "Best models (DeepSeek V3, GPT-4.1) achieved ~60-62% accuracy; many models performed worse; medical fine-tuning often degraded performance; observed diminishing returns beyond 70B parameters.",
            "comparison_baseline": "Cochrane SR conclusions",
            "performance_vs_baseline": "All evaluated models fall short of matching SR conclusions on a substantial fraction of questions (at least 37% failure for frontier models).",
            "key_findings": "Medical fine-tuning did not reliably improve SR-matching accuracy and sometimes degraded it; larger size gives gains up to ~70B but yields diminishing returns beyond; models overconfident and poor at expressing uncertainty.",
            "limitations_challenges": "Instruction-following failures in some models; long-context degradation; inability to properly weigh low-quality evidence; inconsistent benefits from reasoning-targeted variants.",
            "scaling_behavior": "Performance improved from 7Bâ†’70B in some families but flattened/diminished beyond ~70B; performance declines with increasing combined token length.",
            "uuid": "e4437.12",
            "source_info": {
                "paper_title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature.",
            "rating": 2,
            "sanitized_title": "clinfo_ai_an_opensource_retrievalaugmented_large_language_model_system_for_answering_medical_questions_using_scientific_literature"
        },
        {
            "paper_title": "Deep research system card.",
            "rating": 2,
            "sanitized_title": "deep_research_system_card"
        },
        {
            "paper_title": "Elicit: The ai research assistant.",
            "rating": 2,
            "sanitized_title": "elicit_the_ai_research_assistant"
        },
        {
            "paper_title": "Open evidence: Ai-powered medical information platform.",
            "rating": 2,
            "sanitized_title": "open_evidence_aipowered_medical_information_platform"
        },
        {
            "paper_title": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_to_automate_network_metaanalyses_four_case_studies_to_evaluate_the_potential_application_of_large_language_models"
        },
        {
            "paper_title": "MedREQAL: Examining medical knowledge recall of large language models via question answering.",
            "rating": 1,
            "sanitized_title": "medreqal_examining_medical_knowledge_recall_of_large_language_models_via_question_answering"
        },
        {
            "paper_title": "HealthFC: Verifying health claims with evidence-based medical fact-checking.",
            "rating": 1,
            "sanitized_title": "healthfc_verifying_health_claims_with_evidencebased_medical_factchecking"
        },
        {
            "paper_title": "Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence.",
            "rating": 1,
            "sanitized_title": "clasheval_quantifying_the_tugofwar_between_an_llms_internal_prior_and_external_evidence"
        }
    ],
    "cost": 0.02487325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Match the Conclusions of Systematic Reviews?
28 May 2025</p>
<p>Christopher Polzak 
Stanford University</p>
<p>Alejandro Lozano lozanoe@stanford.edu 
Stanford University</p>
<p>Min Woo Sun 
Stanford University</p>
<p>James Burgess 
Stanford University</p>
<p>Yuhui Zhang 
Stanford University</p>
<p>Kevin Wu 
Stanford University</p>
<p>Serena Yeung-Levy 
Stanford University</p>
<p>Can Large Language Models Match the Conclusions of Systematic Reviews?
28 May 2025E30D8CA0AED3C8CAD9C49B473BA1CE3AarXiv:2505.22787v1[cs.CL]Scientific Skepticism Conflict Resolution
Systematic reviews (SR), in which experts summarize and analyze evidence across individual studies to provide insights on a specialized topic, are a cornerstone for evidence-based clinical decision-making, research, and policy.Given the exponential growth of scientific articles, there is growing interest in using large language models (LLMs) to automate SR generation.However, the ability of LLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly characterized.We therefore ask: Can LLMs match the conclusions of systematic reviews written by clinical experts when given access to the same studies?To explore this question, we present MedEvidence, a benchmark pairing findings from 100 SRs with the studies they are based on.We benchmark 24 LLMs on MedEvidence, including reasoning, non-reasoning, medical specialist, and models across varying sizes (from 7B-700B).Through our systematic evaluation, we find that reasoning does not necessarily improve performance, larger models do not consistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence.Instead, most models exhibit similar behavior: performance tends to degrade as token length increases, their responses show overconfidence, and, contrary to human experts, all models show a lack of scientific skepticism toward low-quality findings.These results suggest that more work is still required before LLMs can reliably match the observations from expert-conducted SRs, even though these systems are already deployed and being used by clinicians.We release our codebase 1 and benchmark 2 to the broader research community to further investigate LLM-based SR systems.</p>
<p>Introduction</p>
<p>As the number of published articles grows exponentially [1], manually synthesizing findings from multiple sources has become highly time-consuming.Thus, there is growing interest in developing automatic tools to process, synthesize, and extract insights from scientific literature [2,3].In particular, large language model (LLM)-based systems could offer a promising solution for supporting and automating tasks such as conducting systematic reviews (SRs), which typically take an average of 67 weeks of intensive human effort [4,5].For example, several LLM-assisted tools such as Deep Research [6,7], Elicit [8], and Open Evidence [9], have already been deployed and can be 1 https://github.com/zy-f/med-evidence 2 https://huggingface.co/datasets/clcp/med-evidence</p>
<p>Studies Studies Studies</p>
<p>Studies Studies Q: Given these 3 studies, is survival to hospital discharge higher, lower, or the same when comparing untrained bystander CPR with continuous chest compression to untrained bystander CPR with chest compression interrupted with pauses for rescue breathing?Q: Given the 2 studies, Is the risk of cryptococcal IRIS events higher, lower, or the same when comparing early ART initiation to delayed ART initiation?</p>
<p>Domain Expertise</p>
<p>Q: Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to percutaneous acetic acid injection (PAI)?Q: Is fatigue severity higher, lower, or the same when comparing doxepin to placebo?Q: Is patient function measured by HAQ score higher, lower, or the same when comparing biologic monotherapy to placebo?</p>
<p>Oncology &amp; Hematology Psychiatry &amp; Neurology Internal Medicine &amp; Subspecialties</p>
<p>Pediatrics &amp; Neonatology Q: Is the incidence of bronchopulmonary dysplasia (BPD) higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?</p>
<p>LLM LLM</p>
<p>Figure 1: Core skills evaluated by MedEvidence including: medical domain expertise across 10 different specialties, synthesizing conflicting evidence, and applying scientific skepticism when studies exhibit a high risk of bias (e.g.due to small sample sizes or insufficient supporting evidence).</p>
<p>incorporated into the SR process to improve efficiency [4].The momentum behind these technologies is further exemplified by the U.S. Food and Drug Administration's launch of an LLM-assisted scientific review pilot on May 2025 [10].</p>
<p>However, despite multiple deployments and efforts assessing scientific synthesis generation, the behavior of LLMs across key variables that influence generation remains poorly understood.In particular, their ability to synthesize findings from multiple studies-each varying in study type, population size, and risk of bias-and to navigate conflicting evidence (as medical findings may contradict one another) is not well-characterized.Understanding these behaviors is essential, as medical knowledge is continually reshaped by new clinical trials, cohort studies, and expert opinions.Thus, like medical professionals do, LLMs must be capable of integrating the latest findings (e.g. via retrieval augmentation) [11], weighing the strength of varying evidence, and applying appropriate skepticism when needed to produce reliable, up-to-date recommendations (as shown in Figure 1).</p>
<p>While prior work has successfully evaluated LLMs on their internal "static" medical knowledge [12,13], assessing LLMs' capability to reason across multiple sources and draw expert-level conclusions remains a significant challenge.Specifically, previous efforts have often evaluated LLMs' ability to generate summaries on a given topic.This approach requires a thorough review of every detail in the generated content and lacks easily verifiable ground truth; therefore, medical experts are typically needed to assess output accuracy [14,15,16,17,18], making evaluation time-consuming and hard to scale.To address this, we remove the complexity of evaluating long-format summaries and retrieving relevant papers to pose an even simpler, but fundamental question: Can LLMs replicate the individual conclusions of expert-written SRs when provided with the same source studies?We explore this question in a controlled setting by collecting open-access SRs along with their associated reference articles.We then extract individual findings and reformat them into a closed question-answering (QA) task to simplify evaluation.To this end, we introduce the following contributions:</p>
<p>â€¢ MedEvidence Benchmark We introduce MedEvidence, a human-curated benchmark of 284 questions curated from the conclusions of 100 open-access SRs across 10 medical specialties.Each question evaluates comparative treatment effectiveness on clinical outcomes.All questions are manually transformed into closed-form question answering to enable large-scale evaluation.</p>
<p>In addition, human annotators extract evidence quality (based on the SR's analysis), determine whether full-text access is necessary, and collect the relevant sources needed to replicate the SR findings.</p>
<p>â€¢ Large-scale evaluation on MedEvidence We leverage MedEvidence to perform an in-depth analysis of 24 LLMs spanning general-domain, medical-finetuned, and reasoning models.By utilizing MedEvidence's metadata, we dissect and examine success and failure modes, helping to identify targeted directions for future work.An overview of related works and the key distinct contributions of our current work are summarized in Table 1.</p>
<p>Related work</p>
<p>LLM-based medical systematic review</p>
<p>Numerous studies have explored the potential of LLMs to automate various aspects of scientific literature review, including literature search, query augmentation, screening, data extraction, bias assessment, narrative synthesis, and answering simple clinical inquiries [19,20].However, larger-scale evaluations of LLM-based SR or meta-analyses generation remain relatively underexplored.Reason et al. [14] examined the ability of LLMs to extract numerical data from abstracts and generate executable code to perform meta-analyses.While their results are promising, the study is limited to just four individual case studies.Schopow et al. [15] and Qureshi et al. [16] investigate LLM usage across a range of systematic review stages, including meta-review and narrative evidence synthesis, but also present findings on a very small-case study scale (N &lt; 10) and rely on comparison to humans.Overall, these investigations have been limited in scope and require substantial amounts of review from medical experts, highlighting the need for automated benchmarks to help evaluate LLMs' progress.</p>
<p>Verification of medical facts derived from systematic reviews Several studies have leveraged SRs to benchmarked LLMs' ability to perform medical fact verification, where a model must decide whether to support or refute a given claim.For instance, MedREQAL [21] is an LLM-curated closed QA dataset designed to investigate how reliably models can verify claims derived from Cochrane SRs.However, it does not provide the sources used by the SRs.Instead, the dataset evaluates models on their internal knowledge, making the task a form of fact recall.HealthFC [22], on the other hand, tasks models with verifying claims analyzed by the medical fact-checking site Medizin Transparent, but it only provides pre-synthesized analysis from the web portal as evidence.In contrast to real SR generation, this task primarily involves retrieving information from a pre-synthesized source, removing the real complexity of reasoning across unsynthesized evidence.Unlike prior work, MedEvidence requires extracting, reasoning over, and synthesizing relevant information across single or multiple sources (each with different levels of evidence) to match the expert-derived conclusion of a SR (without access to the original SR itself).It resembles the intricacies of SR analysis, as the raw sources (articles/abstracts) are directly provided to the model.</p>
<p>LLM Behavior in the</p>
<p>Presence of Conflicting Sources ConflictingQA [23] examines how models respond to conflicting arguments supporting or refuting a claim.However, it focuses on inherently contentious questions without definitive answers, spans domains beyond medicine, and uses diverse online sources rather than peer-reviewed literature.ClashEval [24] investigates conflicts between a model's internal knowledge and external evidence, including a drug-related (medical) subset, but limits evaluation to single-source conflicts with artificially perturbed values.ConflictBank [25] and KNOT [26] assess model performance on specific conflict types-such as temporal inconsistencies,  misinformation, and logic-based contradictions-but rely on factoid-style questions sourced from Wikipedia.These benchmarks only leverage relatively small and synthesized inputs.</p>
<p>To the best of our knowledge, no existing studies or datasets provide richly annotated data to systematically benchmark models' ability to align with the conclusions of medical systematic reviews while using the same underlying research documents as the original medical experts.</p>
<p>Dataset Curation Process</p>
<p>Data provenance We collect open-source systematic reviews, available via PubMed, conducted by Cochrane, an international non-profit organization dedicated to synthesizing evidence on healthcare interventions through contributions from over 30,000 volunteer clinician authors [27].Cochrane is a long-standing and widely respected source of clinical evidence [28,29], offering open-access content and analyses presented in a standardized format.Additionally, for each SR, we collect all the cited studies that are relevant for a given conclusion (we refer to these studies as 'sources').When the source article's full text is available (i.e. the article is open-source), we obtain it using the existing BIOMEDICA dataset [30]; otherwise, abstracts are retrieved directly via PubMed's Entrez API [31].All retrieved full-text articles use a CC-BY 4.0 license, which allows for re-distribution.</p>
<p>Dataset curation pipeline</p>
<p>The core challenge in creating our dataset is ensuring that an LLM is provided with sufficient information to reproduce a given conclusion.To ensure a high-quality dataset, we developed a four-stage pipeline consisting of: (1) systematic review selection, (2) conclusion to questions conversion, (3) relevant study selection, and (4) question feasibility validation (as shown in Figure 2).</p>
<p>Systematic review selection</p>
<p>We use Entrez to retrieve all Cochrane SRs published between January 1, 2014 to April 4, 2024 [32].We only include systematic reviews for which all sourced studies are indexed in PubMed (with at least an abstract available).We additionally retrieve all data and metadata for the sourced studies, including: full-text via BIOMEDICA (when it is available), abstract, mesh terms, title, and publish date.2. Conclusion to question conversion.Cochrane reviews follow a standardized format, allowing for a systematic conversion process.To identify potential questions, we followed the protocol below: Human annotators were instructed to review the SR abstract and examine the "Main Results" subsection (see Appendix Figure 9 for an example) to identify individual conclusive statements that statistically compare an intervention with a control group.These individual statements were then converted into question-answer pairs by the annotators, with answers belonging to a fixed set of classes.To be clear, insufficient data was used for statements by the SR authors explicitly indicating that no study investigated-or included sufficient data to analyze-the combination of treatment, control, and outcome; uncertain effect referred to cases where analysis was performed but definitive conclusions could not be made (see Appendix Section B.2 for more conversion details).Evidence certainty was extracted only when it was explicitly provided by the original SR authors, who use the standardized GRADE framework [33] to assess the quality of evidence in the included studies.This certainty is often stated in the abstract, indicating the strength or quality of each observation.</p>
<p>3.</p>
<p>Relevant study selection To identify relevant studies for a given SR, annotators used the analysis section provided in the appendix, which "weighs" the contributions of sources supporting each conclusion.For questions with insufficient data (where it is not possible to determine weights), reviewers were instructed to include studies cited in the SR that either (1) discuss the specified treatment and control but not the outcome, or (2) evaluate the treatment and outcome but compare against a different control.</p>
<ol>
<li>Question feasibility validation Finally, given the question-answer pair and the source studies, annotators were tasked with determining whether the question was answerable based on the provided information.A question was considered answerable if at least 75% of the total weight in the analysis came from "valid" studies included in the meta-analysis.We define a study as "valid" if it (1) provides numerical data on both the intervention and control groups specified in the question, and (2) includes statistical or numerical details about the difference between the groups on the specified outcome-such as raw counts, p-values, confidence intervals, or risk ratios.The most common reason for discarding conclusions was when review authors pooled outcome data across studies, but the outcome was omitted or discussed without clear statistical detail in the abstracts of relevant studies.</li>
</ol>
<p>In addition to these human-curated metadata, we use an LLMs to assess the percentage of individual source studies whose answer to the question aligns with the final answer provided in the systematic review.Thus, to calculate source-level agreement (which we call 'source concordance') we prompt DeepSeekV3 (the strongest model in our benchmark) to answer the question using only one single relevant source; the source is deemed to 'agree' with the final answer if and only if the LLM's classification with the one source matches the ground truth classification.</p>
<p>Medical domain taxonomy assignment</p>
<p>To identify the relevant medical specialties in our dataset, we extract the Medical Subject Headings (MeSH terms)-a controlled vocabulary used by PubMed to index papers-from the 100 systematic reviews included in our dataset.We then feed this list into DeepSeek to generate a simplified categorization of specialties, resulting in 10 categories.Finally, we prompt DeepSeek to assign each question to the most relevant category, or to an "Other" category if no specific specialization is applicable.with between one and five years of graduate education.Figure 3 shows the dataset distribution stratified by specialty, outcome effect, and source concordance with the expert-assessed treatment outcome effect (i.e. the correct answer).The benchmark covers topics from 10 medical specialties (e.g.public health, surgery, family medicine, etc.), five different outcome effects (higher, lower, no difference, uncertain effect, insufficient data), and three broad levels of concordance between the source paper and the correct answer (full agreement, no agreement, mixed agreement).Additional characteristic distributions of the dataset can be found in Appendix Figure 11.</p>
<p>Dataset Description</p>
<p>Data format.MedEvidence is grouped by question; each question includes core data for evaluation, metadata, as well as the content details for the relevant sources.The core data consists of: a humangenerated question of the form "Is [quantity of medical outcome] higher, lower, or the same when comparing [intervention] to [control]?"; the taxonomized answer to the question (higher, lower, no difference, uncertain effect, insufficient data); and the list of relevant studies (sources) used by the review authors to perform the analysis, identified by their unique PubMed IDs.We additionally provide the following metadata: the systematic review from which the question was extracted; the publication year of the systematic review; the authors' confidence in their analysis, also referred to as the 'evidence certainty' (high, moderate, low, very low, or n/a if not provided); a Boolean identification of whether full-text is available and needed to answer the question; the exact fractional source concordance; and the medical specialty associated with the question.Separately, for each source, we provide the unique PubMed ID, title, publication date if available, and content (full-text if available in PMC-OA, abstract otherwise).An individual data point example is shown in Table 2.</p>
<p>5 Benchmarking LLM performance 5.1 Experimental settings LLM selection We selected 24 LLMs across different configurations, including a variety of sizes (from 7B to 671B), reasoning and non-reasoning capabilities, commercial and non-commercial licensing, and medical fine-tuning.This selection includes GPT-o1 [34], DeepSeek R1 [35], OpenThinker2 [36], GPT-4.1 [37], Qwen3 [38], Llama 4 [39], HuatuoGPT-o1 [40], OpenBioLLM [41], and more (please see Appendix Table 3 to see details of all selected models).This selection is non-exhaustive; rather, it is designed to investigate overarching trends across different model types.(b) Accuracy as a function of source concordance, defined as the percentage of relevant sources that agree with the final systematic review (SR) answer, also exhibits a monotonically increasing trend.
OpenBioLLM 8B Llama 4 Scout Llama 3.0 8B Qwen2.5-7B-Instruct HuatuoGPT-o1-7B OpenBioLLM 70B HuatuoGPT-o1-70B Llama 3.1 8B Llama 3.0 70B Llama 4 Maverick GPT-o1 Llama 3.1 405B Llama 3.3 70B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-235B-A22B-FP8 Llama 3.1 70B GPT-4.1 mini QwQ-32B OpenThinker2-32B Llama 3.3 70B (R1-Distill) DeepSeek R1 GPT-4.1 DeepSeek V3 Non-Reasoning Reasoning Average</p>
<p>Prompting setup</p>
<ol>
<li>Basic prompt We evaluated all models in a zero-shot setting, prompting them to first provide a rationale for their answer, followed by an 'answer' field containing only one option from the list of five valid treatment outcome effects (higher, lower, no difference, uncertain effect, or insufficient data).To assess the models' "natural" behavior, we provided minimal guidance in the prompt beyond specifying the required response format, and supplied the abstracts or full text of the relevant studies as context (see Appendix Figure 12).</li>
</ol>
<p>2.</p>
<p>Expert-guided prompt LLMs may not natively understand how to handle multiple levels of evidence, which can lead to unfair evaluations.To address this, we explicitly design a prompt that instructs the LLM to summarize the study design and study population, and to assign a grade of evidence based on established definitions of grades of recommendation (see Appendix Figure 13 for the full prompt).</p>
<p>For both cases, if the input exceeded the LLM's context window, we used multi-step refinement (via LangChain's RefineDocumentsChain [42]) to iteratively refine the answer based on a sequence of article chunks.All models were evaluated with zero temperature to maximize reproducibility.</p>
<p>LLM evaluation Model performance was evaluated using accuracy based on an exact match between the answer field and the ground truth.Model outputs were lower-cased and stripped of whitespace before comparison.If no 'answer' field was provided, or if its content was not an exact rule-based match with the correct answer, the output was deemed incorrect.Confidence intervals (CIs) were calculated via bootstrap (95%, N=1000) [43].</p>
<p>Compute Environment Experiments were performed in a local on-prem university compute environment using 24 Intel Xeon 2.70GHz CPU cores, 8 Nvidia H200 GPUs, 16 Nvidia A6000 GPUs, and 40 TB of Storage.Large-scale models that could not be run locally in this environment were queried in the cloud using public APIs available from together.aior OpenAI.As shown in Figure 4 (a), even frontier models such as DeepSeek V3 and GPT-4.1 demonstrate relatively low average accuracy of 62.40% (56.35, 68.45) and 60.40% (54.30, 66.50), respectively-far from saturating our benchmark.We identify four key factors that influence model performance on our benchmark:</p>
<p>(1) token length, (2) dependency on treatment outcomes, (3) inability to assess the quality of evidence, and (4) lack of skepticism toward low-quality findings.Additionally, we found that (5) medical finetuning does not improve performance, and ( 6) model size shows diminishing returns beyond 70 billion parameters.We explore each of these factors in more detail below using the basic prompt setup.</p>
<p>Reasoning vs non-reasoning LLMs We highlight that, in general, reasoning models do not consistently outperform non-reasoning models of the same class or size on MedEvidence (Figure 4 (a)), as evidenced by DeepSeek V3 outperforming its reasoning counterpart (DeepSeek R1), while LLaMA 3.3 70B distilled from DeepSeek R1 outperforms the LLaMA 3.3 70B base model.</p>
<p>Model performance decreases as token length increases Generally, performance on MedEvidence drastically reduces as the number of tokens increases (Appendix Figure 15).Naturally, training LLMs on long contexts does not guarantee improved long-context understanding, as models may still struggle to utilize information from lengthy inputs [44,45].</p>
<p>Model performance dependency on treatment outcome effect Figure 4 (b) shows the per-class recall stratified by treatment outcome effect.Overall, all models perform best on questions where the correct answer corresponds to higher or lower effects-cases where a strong stance can be taken.</p>
<p>They are slightly less successful on no difference and insufficient data questions, where a definitive conclusion is available but there is no clear preference for either treatment.Performance is lowest on the most ambiguous class, uncertain effect.Notably, as shown in Appendix Figure 16, models are generally reluctant to express uncertainty, often committing to a more certain outcome that appears plausible.Notably, previous work has observed LLMs are verbally overconfident [46,47] and shown that reinforcement learning via human feedback (RLHF) amplifies this effect [48].</p>
<p>Model performance improves with increasing levels of evidence We leverage the evidence certainty levels reported by experts in each systematic review (SR).As shown in Figure 5(a), the overall ability of models to match SR conclusions improves as the level of evidence increases.We therefore explore whether model performance is also associated with the level of source concordance.As shown in Figure 7: Average model accuracy as a function of model size.We observe diminishing returns beyond 70 billion parameters.Arrows point from base models to their medically-finetuned counterparts (arrow between HuatuoGPT-o1 7B and Qwen2.5 7B omitted due to very similar performance).</p>
<p>Figure 5(b), models' ability to match human conclusions increases as the proportion of sources agreeing with the correct answer increases (e.g., DeepSeek V3 achieves 92.45% accuracy at 100% source agreement vs. 41.21% at 0% source agreement).This suggests that, unlike human experts, current LLMs struggle to critically evaluate the quality of evidence and to remain skeptical of results.We observe that this behavior persists even when models are prompted (using the expert-guided prompt) to consider study design, population, and level of evidence (Appendix Figure 19).</p>
<p>Medical finetuning does not improve performance Figure 6 compares the average performance of medically finetuned models to their base model counterparts.Across all comparisons, medical finetuning fails to improve performance (even for medical-reasoning models) and, in most cases, actually degrades it.Indeed, fine-tuning without proper calibration can harm generalization, sometimes resulting in worse performance than the base model [49,50,51].Similar behavior has been previously reported in long-context medical applications [13].</p>
<p>Model size shows diminishing returns beyond 70B parameters As shown in Figure 7, within the same model families, increasing size from 7B to 70B parameters yields substantial accuracy gains on MedEvidence.However, beyond this point, we observe rapidly diminishing returns, both within specific model families and across our suite of evaluated models more broadly.</p>
<p>Combined, our results suggest that synthesizing information across sources to match individual systematic reviews' conclusions eludes current scaling paradigms.Increasing test-time compute (i.e., reasoning) does not necessarily improve performance, larger models do not consistently yield greater gains, and knowledge-based fine-tuning tends to degrade performance.Instead, most models exhibit similar behavior: model performance tends to degrade as token length increases, their responses show overconfidence, and all models exhibit a lack of scientific skepticism toward low-quality findings.These results suggest that more work is still required before LLMs can reliably match the observations from expert-conducted SRs, even though LLM systems are already deployed and being used by clinicians.</p>
<p>Limitations Our study has several limitations.First, the dataset is subject to selection bias, as we only include a SR if all its sources are available (either full text/abstract).Second, while our benchmark is designed to isolate and provide a controlled environment to test LLMs' ability to reason over the same studies experts used to derive conclusions, it does not assess the full SR pipeline, including literature search, screening, or risk-of-bias assessment.Future work could incorporate multi-expert consensus or update findings based on newer studies to strengthen benchmark reliability.</p>
<p>Conclusion</p>
<p>Benchmarks drive advancements by providing a standard to measure progress and enabling researchers to identify weaknesses in current approaches.While LLMs are already deployed for scientific synthesis, our understanding of their failure modes still requires broader investigation.In this work, we present MedEvidence, a benchmark derived from gold-standard medical systematic reviews.We use MedEvidence to characterize the performance of 24 LLMs and find that, unlike humans, LLMs struggle with uncertain evidence and cannot exhibit skepticism when studies present design flaws.Consequently, given the same studies, frontier LLMs fail to match the conclusions of systematic reviews in at least 37% of evaluated cases.We release MedEvidence to enable researchers to track progress.</p>
<p>We thank Dr. Jeffrey Nirschl and Professor Robert Tibshirani for their invaluable discussions...</p>
<p>Appendix contents
A</p>
<p>P Individual confusion matrices for all models</p>
<p>A Societal impact</p>
<p>The use of large language models to automate systematic reviews offers clear potential to accelerate evidence synthesis in medicine and policy.However, when these systems produce incorrect or misleading results, clinicians and policymakers may base decisions on flawed findings, leading to inappropriate treatments or misguided recommendations.</p>
<p>Our study underscores the urgent need for continued research and cautious deployment.LLM-based systematic review systems need further rigorous validation, transparent uncertainty quantification, and mechanisms to detect and mitigate biases and errors.Only through careful development and oversight can these technologies be harnessed to benefit society without exacerbating existing risks or creating new harms.</p>
<p>B Dataset collection details</p>
<p>Below, we provide additional in-depth details regarding stages in dataset curation process.</p>
<p>B.1 Systematic review selection</p>
<p>MedEvidence is originally derived from 6,709 Cochrane publications extracted via Entrez from PubMed.We first discarded any papers where first References subsection was not both entitled "Studies included in this review" and non-empty, as our initial extraction filter included Cochrane SR protocols and SRs finding no valid studies, which were not of interest.We filter for SRs where all included references have a retrievable abstract and limit to SRs with 12 or less references to reduce annotator burden and improve odds of finding SRs where questions can be validated.On average, the end-to-end creation of a single question requires approximately 20 minutes.Appendix Figure 8 presents a cohort diagram for the materialization of the dataset.(VE), we converted the overall risk ratio (RR) to vaccine effectiveness using the formula: VE = (1 -RR) x 100%.</p>
<p>Main results</p>
<p>Five RCTs, reported in 12 records, with 462,754 participants, met the inclusion criteria.</p>
<p>We identified trials on whole-cell plus recombinant vaccine (WC-rBS vaccine (Dukoral)) from Peru and trials on bivalent whole-cell vaccine (BivWC (Shanchol)) vaccine from India and Bangladesh.We did not identify any trials on other BivWC vaccines (Euvichol/ Euvichol-Plus), or Hillchol.</p>
<p>Two doses of Dukoral with or without a booster dose reduces cases of cholera at two-year follow-up in a general population of children and adults, and at five-month follow-up in an adult male population (overall VE 76%; RR 0. We found no differences in the reporting of adverse events due to vaccination between the vaccine and control/placebo groups.</p>
<p>Authors' conclusions</p>
<p>Two doses of Dukoral reduces cases of cholera at two-year follow-up.</p>
<p>Two doses of Shanchol reduces cases of cholera at five-year follow-up, and a single dose of Shanchol reduces cases of cholera at twoyear follow-up.</p>
<p>Overall, the vaccines were safe and well-tolerated.</p>
<p>We found no trials on other BivWC vaccines (Euvichol/Euvichol-Plus).However, BivWC products (Shanchol, Euvichol/Euvichol-Plus) are considered to produce comparable vibriocidal responses.Therefore, it is reasonable to apply the results from Shanchol trials to the other BivWC products (Euvichol/Euvichol-Plus).</p>
<p>PICOs</p>
<p>B.2 Conclusion to question conversion</p>
<p>Appendix Figure 9 provides a direct example of a SR abstract parsed for manual question creation.We highlight the explicit statements ('conclusions') asserting differences between a treatment and control on an outcome, and the presence of standardized, author-provided assessment of evidence certainty for these individual conclusions.SR abstracts were consistently written in this form, allowing annotators to consistently interpret the conclusion into a question.To define the correct answer to the generated question, annotators obeyed the following criteria:</p>
<p>â€¢ Outcomes, or pairs of treatments and controls, where the authors stated that no studies provided sufficient (or any) evidence to perform analysis were labeled as insufficient data questions.â€¢ Conclusions in which the authors stated that there was "no difference" or "no significant difference" between treatments and controls were labeled as no difference questions.â€¢ Conclusions where the authors stated a difference between outcomes either definitively or with qualification (e.g.'X increases Y' or 'X may reduce Y') were given the appropriate higher or lower label.â€¢ Conclusions where the authors expressed that uncertainty was too great to evaluate a treatment outcome effect were placed in the uncertain effect label class.Conclusions where authors assessed a difference, but then stated that they were very uncertain of their findings were deemed ambiguous and discarded.</p>
<p>B.3 Relevant study selection and question validation</p>
<p>For author conclusions where more than one study was used, SRs provide meta-analyses over all relevant sources (an example meta-analysis is shown in Appendix Figure 10), allowing us to confirm whether the studies used in the original SR contain sufficient information to replicate the conclusions of human analysis.</p>
<p>C Additional dataset distributions</p>
<p>We present additional statistical characteristics of the questions in our MedEvidence dataset in Appendix Figure 11.We highlight that the dataset is balanced with respect to evidence certainty levels, strengthening the reliability of our main observations on the relationship between evidence certainty and model performance.With regard to the joint distribution of correct treatment outcome effect and evidence certainty, we note that the highly concentrated distributions for the insufficient data and uncertain effect classes are inherent to the nature of SR.For example, in the case of the insufficient data class, authors cannot draw definitive conclusions from analyses they were unable to perform; thus, their findings are most uncertain when the quality of evidence is poor.</p>
<p>D Evaluated models and prompts</p>
<p>The full list of 24 models we evaluate on MedEvidence is provided in Appendix Table 3.The exact prompt used to elicit LLM responses for evaluation under the basic prompt regime is provided in Appendix Figure 12.Under the expert-guided prompt regime, models were first instructed to generate a formatted article summmary using the summarization step (using Appendix Figure 13a), then asked to provide answers based on the generated summaries for all relevant articles (via Appendix Figure 13b).In all cases, chunks of original article text or previously-generated summarization were provided with a header line containing the article's title, date of publication (if available), and PubMed ID, allowing the LLM to recognize and assign blocks of content to different sources and synthesize in-context.</p>
<p>Table 3: List of evaluated models with their model size and context length limit we set for our experiments.Precision is 16-bit floating point unless specified otherwise.</p>
<p>Model Model Type Parameter Sizes Context Limit</p>
<p>DeepSeek R1 [35] Generalist Reasoning 671B 131K DeepSeek V3 [52] Generalist Non-Reasoning 671B 131K GPT-4.1 [37] Generalist Non-Reasoning Unknown 1M GPT-4.1 mini [37] Generalist Non-Reasoning Unknown 131K GPT-o1 [34] Generalist Non-Reasoning Unknown 150K HuatuoGPT-o1 [40] Medical Reasoning 7B, 70B 32K, 16K Llama 3.0 [53] Generalist Non-Reasoning 8B, 70B 8K Llama 3.1 [53] Generalist Non-Reasoning 8B, 70B, 405B 131K Llama 3.3 [53] Generalist Non-Reasoning 70B 131K Llama 3.3 (R1-Distill) [35] Generalist Reasoning 70B 131K Llama 4 Maverick [39] Generalist Non-Reasoning 400B (17B active) 500K Llama 4 Scout [39] Generalist Non-Reasoning 109B (17B active) 1M OpenBioLLM [41] Medical Non-Reasoning 8B, 70B 8K OpenThinker2 [36] Generalist Reasoning 32B 131K Qwen2.5 [54] Generalist Non-Reasoning 7B, 32B, 72B 32K Qwen3 [38] Generalist Reasoning (hybrid) 235B (22B active, 8-bit) 32 K QwQ [55] Generalist Reasoning 32B 131K (c) Joint distribution of evidence certainty and correct treatment outcome effect   You are the author of a Cochrane Collaboration systematic review, leveraging statistical analysis and assessing risks of bias in order to rigorously assess the effectiveness of medical interventions.As part of your review process, perform the following task: As a subject expert, (1) summarize the evidence provided by a given ARTICLE as it pertains to a given QUESTION and (2) provide a possible answer.</p>
<p>Otherwise, if the provided article contains relevant information, you must return a list including the following items:</p>
<p>-<strong>Study Design</strong>: Type of study, level of evidence, and grade of recommendation according to the levels of evidence REC  -------------|--------------|----------------------------------------------</p>
<p>E LLM instruction-following rates</p>
<p>The rate at which LLMs provided valid answer output of any kind is presented as part of Figure 4. Precisely, we measured the per-model instruction-following rate, i.e. the percentage of questions for which the full "Answer" field in the model's final output exactly matched one of the defined answer classes (case-insensitive).We note that a substantial portion of models exhibit a high rate of instruction-following failures: OpenBioLLM 8B and 70B; HuatuoGPT-o1 7B and 70B; Llama 4 Maverick and Scout; Llama 3.0 8B; and Llama 3.1 8B all fail to achieve a 60% instruction-following rate, and only Llama 3.3 70B (Instruct and R1-Distill) achieves perfect instruction-following.We highlight that OpenBioLLM 8B has a 0% instruction-following rate.Lastly, we observe that even when significant portion of the outputs are valid, models still have high error rates, with only an average of 58.1(Â±5.0)% of valid model outputs being correct.These results demonstrate that, while a high instruction-following rate may diminish performance in small models, poor performance cannot be attributed to instruction-following errors alone.</p>
<p>F LLM performance as a function of number of relevant sources</p>
<p>As shown in Appendix Figure 14, we find no clear general trend between the number of relevant sources and model performance.Notably, this includes performance with a single source (no model achieves even 60% accuracy), highlighting challenges in LLMs' ability to perform systematic review beyond resolving evidence conflicts.The only exceptions to this are the models with the overall poorest performance (colored in red and orange hues, such as HuatuoGPT-o1 7B and Llama 3.0 8B).</p>
<p>G LLM performance as a function of token length of relevant sources</p>
<p>Given the lack of dependency on the number of sources on average accuracy, we directly investigate the dependency of model performance on the combined token length of all relevant sources; we present these results in Appendix Figure 15.As noted in the main analysis, performance consistently declines at high token counts, except for models with over 100B parameters.Notably, 32B models maintain over 50% average accuracy up to the 80-100% quantile (15K tokens and above).By contrast, 70-72B models fall below 50% accuracy around the 60-80% quantile (11-15K tokens).This decline in the 70-72B range is primarily driven by the underperformance of medically finetuned models (HuatuoGPT-o1 and OpenBioLLM).</p>
<p>H Average confusion matrices for treatment outcome effects</p>
<p>We assess which treatment outcome effect classes are most frequently misclassify by visualizing the confusion matrix averaged across all models.As shown in Figure 16, we observe that models with lower than 40% accuracy significantly skew the confusion matrix toward invalid outputs.However, when considering exclusively models with above 40% performance, we observe two significant trends.First, models are consistently unwilling to predict uncertain effect.Second, models consistently confuse the uncertain effect and no difference classes.</p>
<p>For completeness, we provide all individual confusion matrices in Appendix Section P.</p>
<p>I Performance by review publication year</p>
<p>As shown in Appendix Figure 17, performance steadily declines for more recent publication years, except for 2023 and 2024.These improvements may partially be explained by the fact that the majority of questions from 2024 involve high-or moderate-certainty evidence (as shown in Appendix Figure 11(d)); as a result, these questions are likely easier for models to answer.</p>
<p>J Per-class recall for individual models</p>
<p>We present individual model per-class recall in Appendix Figure 18.Notably, all models, without exception, perform poorly on the uncertain effect class.We highlight that Llama 3.3 70B-Instruct outperforms all other models on the higher and lower classes, but its overall accuracy is held back significantly by its poor performance on the no difference and insufficient data classes.</p>
<p>N Full-text vs abstract sources</p>
<p>We evaluate how model performance differs when using full-text articles versus abstracts alone, using the basic prompt setup in both cases.The results of this experiment are presented in Appendix Figure 23.We find that most models with the poorest overall performance actually experience a boost in accuracy (as high as 24.8% in the case of HuatuoGPT-o1-70B) when given only abstracts, even though abstracts contain less information.This suggests that some models struggle on our dataset because of an inability to handle long context, as full-text articles are much longer than abstracts alone.By contrast, the best-performing models usually perform better when given access to full-text (DeepSeek R1, for instance, gains 4.8% accuracy with full-text access).We note that, due to licensing constraints in scientific publishing, many existing deployments and evaluations of LLM to answer questions using scientific literature rely solely on abstracts [2,9].Our analysis demonstrates that access to full article text benefits frontier models, underscoring the urgent need to expand such access.We highlight initiatives promoting this shift-for instance, beginning in 2025, all U.S. federally-funded research must be made freely available, which could significantly enhance the performance of already-deployed models.</p>
<p>O Qualitative analysis of DeepSeek V3</p>
<p>To provide a better qualitative understanding of success and failure modes on our dataset, we explore the outputs (under the basic prompt setup) of our best-performing model, DeepSeek V3, and provide some commentary on the quality of the model's output.Specifically, we consider four types of questions: (1) Questions where all models get the answer wrong.-<strong>Rationale</strong>:</p>
<p>-Article ID 9190979 reports that the complication rate, including lymphocyst formation, was significantly higher in the drained group ( Group A) compared to the not-drained group (Group B) (P = 0.01).However, sonographic monitoring showed free abdominal fluid (a proxy for lymphocyst formation) in 18% of drained patients vs. 36% of notdrained patients (P = 0.03), suggesting a lower rate in the drained group.This creates a contradiction in the findings.-Article ID 12214830 found no significant difference in lymphocyst formation between the drained and not-drained groups (P = 0.2), with asymptomatic lymphocysts detected only in the not-drained group (though statistically insignificant).</p>
<p>-<strong>Full Answer</strong>: -Article ID 9190979 reports a higher complication rate (including lymphocyst-related morbidity) in the drained group [9190979], but sonographic data suggests a lower rate of free abdominal fluid (a proxy for lymphocyst formation) in the drained group (18% vs. 36%, P = 0.03) [9190979].Article ID 12214830 found no significant difference in lymphocyst formation between groups (P = 0.2) [12214830].The evidence is conflicting, with one study suggesting a lower rate and another showing no difference.</p>
<p>-<strong>Answer</strong>: Uncertain Effect</p>
<p>Remarks: In the SR, authors perform meta-analysis and find no overall significant difference between retroperitoneal drainage and no drainage.DeepSeek V3 does not appear to know how to perform any kind of reconcilliation.We note that Article 12214830 does not specify the raw number of events under both treatments, but does state that no lymphocysts (i.e.0) were detected in the drainage group.</p>
<p>Question ID 198 -0/24 models correct Review PMID: 35514111 Question: Is the rate of any PCR-positive SARS-COV-2 infection higher, lower, or the same when comparing test-based attendance to standard 10-day self-isolation?Correct Answer: uncertain effect PMIDs of Relevant Sources: 34534517 DeepSeek V3 Response:</p>
<p>-<strong>Rationale</strong>: The study compared daily testing (test-based attendance) with standard 10-day self-isolation for school-based contacts of SARS-CoV -2 cases.The primary outcomes included rates of symptomatic PCRconfirmed infections and COVID-19-related absences.The results showed no significant difference in the rate of symptomatic PCR-confirmed infections between the two groups, with the intervention group (daily testing) being non-inferior to the control group (self-isolation).The adjusted incidence rate ratio (aIRR) for symptomatic PCR-confirmed infections was 0.96 (95% CI -0.751.22),indicating no statistically significant difference.</p>
<p>-<strong>Full Answer</strong>: The rate of PCR-positive SARS-CoV-2 infection was not significantly different when comparing test-based attendance (daily testing) to standard 10-day self-isolation (aIRR 0.96, 95% CI -0.751.22)</p>
<p>[34534517].</p>
<p>-<strong>Answer</strong>: No Difference</p>
<p>Remarks: The authors of the SR state that the results are inconclusive for the following reasons: (a) "risk of bias: considerable missing data for PCR-tests for staff (data were available for 76% control schools and 83% intervention schools)" and (b) "imprecision: the confidence intervals are wide and the 95% CI crosses the line of no effect."In this case, the full-text article is open-source, so LLMs theoretically had access to essentially all the same information as the SR authors, but evidently, none of these concerns are addressed in DeepSeek V3's response.Furthermore, the DeepSeek V3 response focuses only on symptomatic PCRconfirmed infections, demonstrating limitations in its knowledge of medical contexts, as the question calls for an assessment of any PCR positive infections, which includes asymptomatic cases.</p>
<p>Question Remarks: This is a relatively challenging question-in the original SR, the authors compare the improvements in FSS in the placebo group directly to the improvements in the treatment group, rather than comparing the two treatment groups separately to their own baselines (which is what the authors of the cited source do).The source's abstract does state that smaller but non-significant improvements were present in the placebo group, albeit without providing numbers.Regardless, the LLM's response reads as a regurgitation of facts and lacks the critical thinking needed to critically assess cited studies.</p>
<p>O. -<strong>Rationale</strong>: The three articles provide mixed evidence on the effectiveness of Transcatheter Device Closure (TDC) compared to medical therapy for stroke prevention in patients with patent foramen ovale (PFO) .Article ID 22417252 found no significant difference in stroke prevention between TDC and medical therapy (P=0.37 for the primary endpoint).Article ID 23514285 also reported no significant reduction in recurrent embolic events or death with TDC (P=0.34 for the primary endpoint).However, Article ID 23514286 showed no significant benefit in the intention-to-treat analysis (P=0.08) but did find a significant reduction in recurrent stroke in the per-protocol and as-treated analyses (P=0.03 and P=0.007, respectively).Given the inconsistency in results across studies and analyses, the overall evidence is not conclusive.</p>
<p>-<strong>Full Answer</strong>: The evidence is mixed.-<strong>Rationale</strong>:</p>
<p>-Article ID 7967990 evaluates the short-term efficacy of two doses of WC/ rBS cholera vaccine without a booster, showing 86% protective efficacy over a mean follow-up of 18 weeks.However, it does not provide data on long-term (two-year) efficacy or the effect of a booster dose.-Article ID 10823767 evaluates the two-year efficacy of the same vaccine, comparing two doses (without booster) to three doses (with booster).After two doses, the vaccine showed no efficacy (VE = -4%), but after the booster dose, efficacy increased to 61%.This suggests that the booster dose significantly improves long-term protection compared to two doses alone.</p>
<ul>
<li>-<strong>Rationale</strong>: The study compared ibuprofen (an NSAID) to acetaminophen and placebo for pain relief in children with acute otitis media over 48 hours.The results showed that ibuprofen was statistically superior to placebo (P &lt; 0.01) in relieving pain, with 7% of children still suffering pain in the ibuprofen group compared to 25% in the placebo group.There was no significant difference between acetaminophen and placebo for pain relief.</li>
</ul>
<p>-<strong>Full Answer</strong>: Pain relief at 48 hours was higher with NSAIDs (ibuprofen) compared to placebo, as evidenced by a statistically significant reduction in pain (7% vs. 25% of children still suffering pain, P &lt; 0.01) [8871138].</p>
<ul>
<li>-<strong>Rationale</strong>: The study compared two groups: one where gastric residual volume (GRV) was measured only in the presence of specific clinical signs (study group) and another where GRV was routinely measured (control group).The primary outcome was time to reach full feeding volumes, but secondary outcomes included necrotizing enterocolitis (NEC).The study found no significant differences in NEC incidence between the two groups, with only 2 cases reported in the control group and none in the study group.However, the small sample size (87 infants) and low event rate limit the ability to draw definitive conclusions about NEC risk.</li>
</ul>
<p>-<strong>Full Answer</strong>: The study found no significant difference in the incidence of necrotizing enterocolitis between the group where gastric residual volume was measured only in the presence of clinical signs (0 cases) and the group where it was routinely measured (2 cases) [29866595].The small sample size and low event rate suggest uncertainty in the effect.</p>
<p>-<strong>Answer</strong>: Uncertain Effect Remarks: Even without explicit prompting, DeepSeek V3 recognizes the weakness of the limited sample size/total number of events-the fact that only DeepSeek V3 gets this question correct shows both the current limitations of models' ability to assess uncertainty, as well as the promise that they may be able to do so consistently in the future.</p>
<p>P Individual confusion matrices for all models</p>
<p>A</p>
<p>Systematic Review Selection B Conclusion ConversionQ: Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing two doses of Dukoral with or without a booster dose to placebo?Shanchol reduces cases of cholera at two-year follow-up (high-certainty)</p>
<p>Figure 2 :
2
Figure 2: Overview of the dataset curation process for MedEvidence.</p>
<p>Figure 3 :
3
Figure 3: Key statistical characteristics of the questions in MedEvidence.(a) shows the dataset distribution stratified by medical specialty.(b) presents the distribution stratified by outcome effect.(c) shows the distribution stratified by source concordance with the expert-assessed treatment outcome effect (i.e. the correct answer).</p>
<p>Figure 5 :
5
Figure 5: (a) Accuracy as a function of evidence certainty, shows a monotonically increasing trend.(b)Accuracy as a function of source concordance, defined as the percentage of relevant sources that agree with the final systematic review (SR) answer, also exhibits a monotonically increasing trend.</p>
<p>Figure 6 :
6
Figure 6: Medically-finetuned models vs their base generalist counterparts.Pairs of medical and base models are adjacent.95% confidence intervals are calculated via bootstrapping with N = 1000.</p>
<p>Societal impact B Dataset collection details B.1 Systematic review selection . . . . . . . . . . . . . . . . . . . . . . . . . . .B.2 Conclusion to question conversion . . . . . . . . . . . . . . . . . . . . . . .B.3 Relevant study selection and question validation . . . . . . . . . . . . . . . .C Additional dataset distributions D Evaluated models and prompts E LLM instruction-following rates F LLM performance as a function of number of relevant sources G LLM performance as a function of token length of relevant sources H Average confusion matrices for treatment outcome effects I Performance by review publication year J Per-class recall for individual models K Model performance under the expert-guided prompt setup L Question correctness across models M Performance by medical specialty N Full-text vs abstract sources O Qualitative analysis of DeepSeek V3 O.1 Questions where all models are wrong . . . . . . . . . . . . . . . . . . . . .O.2 Questions where most models are correct, but DeepSeek V3 is wrong . . . .O.3 Questions where most models are correct, including DeepSeek V3 . . . . . .O.4 Questions where DeepSeek V3 is correct, despite most models being wrong .</p>
<p>Figure 8 :
8
Figure 8: MedEvidence cohort diagram describing selection criteria for Cochrane SRs suitable for use in the MedEvidence dataset.Note that not all available papers in the second-to-last stage were manually reviewed for use in the final stage.</p>
<p>Figure 9 :
9
Figure 9: An example of a "Main Results" section from a Cochrane review used in MedEvidence (DOI: https://doi.org/10.1002/14651858.CD014573).Annotators were instructed to extract conclusions from this standardized sub-section of the SR abstract.</p>
<p>Figure 10 :
10
Figure 10: An example meta-analysis from a Cochrane review (figure from DOI: https://doi.org/10.1002/14651858.CD014573).Notably, the set of relevant studies and their individual weighted contributions to the overall result are available.</p>
<p>Joint distribution of evidence certainty and review publication year</p>
<p>Figure 11 :
11
Figure 11: Additional statistical characteristics of MedEvidence.(a) shows the dataset distribution stratified by evidence certainty.(b) stratifies the questions by number of relevant sources.(c) is a joint distribution of evidence certainty and correct answer label.(d) shows the distribution of evidence certainties by systematic review publication year.</p>
<p>Figure 12 :
12
Figure 12: Prompt used to generate LLM responses to questions under the basic prompt setup.</p>
<p>| | A | 1a | Systematic review and meta-analysis of (homogeneous) randomized controlled trials | | A | 1b | Individual randomized controlled trials (with narrow confidence intervals) | | B | 2a | Systematic review of (homogeneous) cohort studies of 'exposed' and ' unexposed' subjects | | B | 2b | Individual cohort study / low-quality randomized control studies | | B | 3a | Systematic review of (homogeneous) case-control studies | | B | 3b | Individual case-control studies | | C | 4 | Case series, low-quality cohort or case-control studies, or case reports | | D | 5 | Expert opinions based on non-systematic reviews of results or mechanistic studies |" Think step by step.<strong>QUESTION</strong>: {question} <strong>ARTICLE TITLE</strong>: {title} <strong>ARTICLE CONTENT</strong>: {context} (a) Prompt used for the summarization step.You are the author of a Cochrane Collaboration systematic review, leveraging statistical analysis and assessing risks of bias in order to rigorously assess the effectiveness of medical interventions.As part of your review process, perform the following task: Given the ARTICLE SUMMARIES.Provide a concise and precise answer to the provided QUESTION.After you think, return your answer with the following format: -<strong>Rationale</strong>: Your rationale -<strong>Full Answer</strong>: A precise answer, citing each fact with the Article ID in brackets (e.g.[2]).-<strong>Answer</strong>: A final classification exactly matching one of the following options: Higher, Lower, No Difference, Insufficient Data, Uncertain Effect Think step by step.<strong>QUESTION</strong>: {question} <strong>ARTICLE SUMMARIES</strong>: {context} (b) Prompt used for the final answer step.</p>
<p>Figure 13 :
13
Figure 13: Prompts used to generate LLM responses to questions under the expert-guided prompt setup, designed to attempt to explicitly enforce model awareness of evidence quality and strength.19</p>
<p>Figure 14 :
14
Figure 14: Model accuracy as a function of number of relevant sources.</p>
<p>Figure 15 :
15
Figure 15: Model performance as a function of the number of tokens in the relevant studies, separated by model size range.Horizontal axis measures the accuracy by 5-quantiles.</p>
<p>if fe re n c e in s u ff ic ie n t d a ta u n c e rt a in e ff e c t in v a li d Predicted h ig h e r lo w e r n o d if fe re n c e in s u ff ic ie n t d a ta u n c e rt a in e ff e c t</p>
<p>Figure 16 :
16
Figure 16: Average confusion matrices using basic prompts.(a) Average confusion matrix aggregated across all models.(b) Average confusion matrix aggregated across models achieving at least 40% overall accuracy.</p>
<p>Figure 22 :Figure 23 :
2223
Figure 22: Average model accuracy across all models (and 95% confidence interval) stratified by medical specialty.</p>
<p>( 2 )
2
Questions where most models are correct, but DeepSeek V3 is incorrect.(3) Questions where most models are correct, including DeepSeek V3. (4) Questions where most models are wrong, but DeepSeek V3 is correct.O.1 Questions where all models are wrong Question ID 1 -0/24 models correct Review PMID: 28660687 Question: Is the short-term rate of overall lymphocyst formation higher, lower, or the same when comparing retroperitoneal drainage to no drainage?Correct Answer: no difference PMIDs of Relevant Sources: 9190979, 12214830 DeepSeek V3 Response:</p>
<p>Figure 24 :
24
Figure 24: Confusion matrix for DeepSeek R1.</p>
<p>h ig h e r lo w e r n o dFigure 25 :Figure 26 :
2526
Figure 25: Confusion matrix for DeepSeek V3.</p>
<p>Figure 27 :Figure 28 :Figure 29 :
272829
Figure 27: Confusion matrix for GPT-4.1 mini.</p>
<p>Figure 30 :Figure 31 :Figure 32 :Figure 33 :Figure 34 :Figure 35 :Figure 36 :
30313233343536
Figure 30: Confusion matrix for HuatuoGPT-o1-7B.</p>
<p>h ig h e r lo w e r n o dFigure 37 :Figure 38 :
3738
Figure 37: Confusion matrix for Llama 3.3 70B-Instruct.</p>
<p>Figure 39 :Figure 40 :Figure 41 :
394041
Figure 39: Confusion matrix for Llama 4 Scout.</p>
<p>Figure 42 :Figure 43 :Figure 44 :
424344
Figure 42: Confusion matrix for OpenThinker2-32B.</p>
<p>Figure 45 :Figure 46 :Figure 47 :
454647
Figure 45: Confusion matrix for Qwen2.5-72B-Instruct.</p>
<p>Table 1 :
1
Comparison of factuality and evidence reasoning benchmarks with medical focus.We compare MedEvidence to prior datasets across attributes relevant to systematic review-style reasoning.MedEvidence is the only dataset to satisfy all criteria.
Expert-GroundedAutomatedMultipleEvidenceSource-LevelDatasetSizeTopicCurationAnswerEvaluationSourcesQualityConcordanceReason et al.4MedicineHumanâœ“âœ—âœ“âœ—âœ—Schopow et al.1MedicineHumanâœ“âœ—âœ“âœ—âœ—MedREQAL2786MedicineLLMâœ“âœ“âœ—âœ“âœ—HealthFC750 Consumer Health Humanâœ“âœ“âœ—âœ“âœ—ConflictingQA 238Multi-DomainLLMâœ—âœ—âœ“âœ—âœ“MedEvidence284MedicineHumanâœ“âœ“âœ“âœ“âœ“</p>
<p>Table 2 :
2
Sample question from the dataset.Fields marked with an asterisk (<em>) use LLMs to assist the generation.Relevant source details are omitted here for brevity.
QuestionIs stroke prevention higher, lower, or the same when comparing Tran-scatheter Device Closure (TDC) to medical therapy?Answerno differenceRelevant Sources (PubMed IDs)22417252, 23514285, 23514286Systematic Review (PubMed ID)26346232Review Publication Year2015Evidence Certaintyn/aOpen-Access Full-Text Neededno</em>Source Concordance1.0*Medical SpecialtySurgeryMedEvidence contains a total of 284 questions derived from 100 systematic reviews with 329referenced individual articles, of which 114 have full-text available (see Appendix Figure 8 for acohort diagram of the dataset). Questions were systematically collected by three human annotators</p>
<p>TABLE (
(provided Below).-<strong>Study Population</strong>: Study size and patient population.-<strong>Summary</strong>: A concise but comprehensive summary based on the previouslyspecified information, with a focus on the main findings.-<strong>Possible Answer</strong>: A concise feasible answer given the evidence.<strong>REC TABLE </strong>: Levels of Evidence (from strongest [1a] to lowest [5]).| Grade of Recommendation | Level of Evidence | Type of Study ||-</p>
<p>2 Questions where most models are correct, but DeepSeek V3 is wrong
Question ID 6 -16/24 models correctReview PMID: 26346232Question: Is stroke prevention higher, lower, or the same when comparing TranscatheterDevice Closure (TDC) to medical therapy?Correct Answer: no differencePMIDs of Relevant Sources: 22417252, 23514285, 23514286DeepSeek V3 Response:</p>
<p>This is case where DeepSeek V3 seems to fail due to a misinterpretation of the question, although we believe the semantics of this question are unambiguous.DeepSeek V3 does not seem to pick up on the mention of placebo in the question at all.
O.3 Questions where most models are correct, including DeepSeek V3Question ID 160 -21/24 models correctReview PMID: 28829911Question: Is the risk of treatment-related mortality higher, lower, or the same when comparingchemoradiotherapy plus esophagectomy to chemoradiotherapy alone?Correct Answer: higherPMIDs of Relevant Sources: 17401004, 15800321<strong>Full Answer</strong>: DeepSeek V3 Response:-Without a booster dose (two doses only), the vaccine showed no protective -<strong>Rationale</strong>: Both articles compare chemoradiotherapy plus esophagectomy (efficacy (VE = -4%) at two-year follow-up [10823767]. surgery) to chemoradiotherapy alone in patients with locally advanced-With a booster dose (three doses), the vaccine demonstrated 61% esophageal cancer. Article ID 17401004 reports a 3-month mortality rateprotective efficacy at two-year follow-up [10823767]. of 9.3% in the surgery group (arm A) compared to 0.8% in the-Therefore, the risk of cholera at two-year follow-up is <strong>higher</strong> with chemoradiation-alone group (arm B), with a statistically significanttwo doses alone compared to two doses plus a booster. difference (P = .002). Article ID 15800321 similarly reports asignificantly higher treatment-related mortality in the surgery group-<strong>Answer</strong>: Higher (12.8%) compared to the chemoradiotherapy-alone group (3.5%) (P = .03).Both studies consistently demonstrate that adding surgery tochemoradiotherapy increases treatment-related mortality. -<strong>Full Answer</strong>: The risk of treatment-related mortality is higher when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone , as evidenced by significantly higher mortality rates in the surgery groups (9.3% vs. 0.8% [17401004]; 12.8% vs. 3.5% [15800321]). Remarks: Question ID 163 -16/24 models correct -<strong>Answer</strong>: HigherReview PMID: 25739381 Remarks: While this question involves multiple sources, both sources individually agree Question: Is neonatal mortality higher, lower, or the same when comparing community-led with the overall correct conclusion, making it fairly straightforward to answer; consequently, chlorhexidine cord cleansing to community-led dry cord care? Correct Answer: lower almost all models get this question correct.PMIDs of Relevant Sources: 22322124, 16546539, 22322126Question ID 214 -23/24 models correctDeepSeek V3 Response:Review PMID: 37594020 -<strong>Rationale</strong>: The question asks whether neonatal mortality is higher, Question: Is pain relief at 48 hours higher, lower, or the same when comparing NSAIDs to lower, or the same when comparing community-led chlorhexidine cord placebo? cleansing to community-led dry cord care. The three articles provide evidence from randomized trials in different settings (Bangladesh, Nepal, Correct Answer: higherand Pakistan) comparing chlorhexidine cord cleansing with dry cord care. PMIDs of Relevant Sources: 8871138All three studies report reductions in neonatal mortality withchlorhexidine use, though the magnitude and statistical significance vary DeepSeek V3 Response:.-<strong>Full Answer</strong>:-In Bangladesh, neonatal mortality was lower with single chlorhexidinecleansing (22.5 per 1000 livebirths) compared to dry cord care (28.3per 1000 livebirths; RR 0.80 [0.65-0.98]) [22322124].-In Nepal, neonatal mortality was 24% lower with chlorhexidine (RR 0.76[0.55-1.04]) compared to dry cord care, with a significant 34%reduction in infants enrolled within 24 hours (RR 0.66 [0.46-0.95])[16546539].-In Pakistan, chlorhexidine cleansing reduced neonatal mortality (RR 0.62[0.45-0.85]) compared to dry cord care [22322126].-<strong>Answer</strong>: <strong>Lower</strong>Remarks: An example of an instruction-following failure for DeepSeekV3 (even though theoverall analysis leads to a correct solution).</p>
<p><strong>Rationale</strong>: The question asks whether quality of life (QoL) is higher, lower, or the same when comparing computerized cognitive training (CCT) of at least 12 weeks to no intervention.The provided articles focus on cognitive and physical performance improvements but do not explicitly measure or report QoL outcomes.Article ID 23531885 discusses cognitive improvements from CCT but does not mention QoL.Article ID 25511081 examines dual-task gait and divided attention but also does not address QoL.Article ID 20418350 evaluates cognitive performance after mental and physical activity interventions but similarly lacks QoL data.The variability in results suggests that the effect may depend on the type of breast surgery (e.g., mastectomy vs. axillary lymph node dissection) or other factors.While DeepSeek V3 does get this question right, and shows flashes of attempting deeper reasoning (e.g. the remark that the effect "may depend on the type of breast surgery"), but this analysis is not directly supported by similar statements from the original SR authors.Additionally, there is still no formal aggregation or meta-analysis attempted, and the level of detail in the reasoning provided by DeepSeek V3 makes it hard to tell if the model's predicted answer was made via some form of intuition or if the model just got a bit lucky.
one source accurately directly reports on relevant findings without conclusions being revisedby the SR authors' critical analysis.O.4 Questions where DeepSeek V3 is correct, despite most models being wrong Question ID 235 -3/24 models correct Review PMID: 32104914 Question: Is quality of life higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention? Correct Answer: insufficient data PMIDs of Relevant Sources: 23531885, 25511081, 20418350 DeepSeek V3 Response: -<strong>Full Answer</strong>: None of the articles provide data on quality of life (QoL) outcomes when comparing CCT of at least 12 weeks to no intervention [23531885, 25511081, 20418350]. -<strong>Answer</strong>: Insufficient Data Remarks: DeepSeek V3 correctly recognizes that none of the relevant studies provide direct, quantitative analysis on QoL outcomes; given the very low model correctness rate (3/24), other models likely attempted to apply colloquial definitions of "quality of life" rather than the narrowly-defined medical definition requiring explicit quantitative measurement. Question ID 89 -2/24 models correct Review PMID: 31557310 Question: Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to placebo? Correct Answer: lower PMIDs of Relevant Sources: 7480237, 9776150, 23052809, 10873356, 19673597, 2403655, 2403697 DeepSeek V3 Response: -<strong>Rationale</strong>: The question asks about the incidence of surgical site infection (SSI) in breast cancer surgery without reconstruction when comparing prophylactic antibiotics to placebo. The relevant articles provide mixed results: some show a significant reduction in SSI with antibiotics (e.g., [7480237], [2403655]), while others show no antibiotics compared to placebo in some breast cancer surgeries, as shown by significant reductions in infections in studies involving plastic surgery ([7480237]) and certain breast procedures ([2403655]). However, other studies found no significant difference for modified radical mastectomy ([23052809]), clean elective breast surgery ([10873356]), or mastectomy ([2403697]). The overall evidence suggests a context-dependent effect, with some procedures benefiting from prophylaxis. -<strong>Answer</strong>: Lower Remarks: Question ID 173 -1/24 models correct Review PMID: 31425604 Question: Is the risk of necrotizing enterocolitis higher, lower, or the same when comparing monitoring of gastric residual volume and quality to only monitoring of gastric residual quality? Correct Answer: uncertain effect PMIDs of Relevant Sources: 29866595 significant difference (e.-<strong>Full Answer</strong>: The incidence of SSI is <strong>lower</strong> with prophylactic DeepSeek V3 Response:<strong>Answer</strong>: HigherRemarks: A straightforward question that all models (except OpenBioLLM 8B) get correct:
g.,[23052809], [10873356], [2403697]).Notably , [7480237] specifically highlights a significant reduction in infections for breast surgery, while [2403655] also supports a reduction in infections for breast surgery.However, other studies, such as [23052809] and [10873356], found no significant difference.</p>
<p>AcknowledgementsK Model performance under the expert-guided prompt setupTo evaluate the dependency of model performance on prompting quality, we leverage an expertguided prompt setup as described in the main paper and Appendix Section D. Critically, as shown in Appendix Figure19and discussed in the main paper, we find that even with a prompt explicitly designed to encourage models to assess the quality of studies, the dependency of model performance on evidence certainty remains.More broadly, as shown in Appendix Figure20, we find that our more intentionally-designed prompt does not consistently improve model performance; while performance improves for the five models that performed worst under the basic prompt (namely OpenBioLLM 8B, Llama 4 Scout, Llama 3.0 8B, Qwen2.5-7B-Instruct, and HuatuoGPT-o1 7B), we observe that performance actually decreases for several of the models that performed best with the basic prompt, including a nearly 20% drop in performance for DeepSeek V3 (the highest-performing model when using the basic prompt).L Question correctness across modelsAs shown in Appendix Figure21, 53 questions are answered incorrectly by all models, and only 2 are answered correctly by all models (omitting OpenBioLLM 8B, which gets every question wrong).Otherwise, we observe that performance varies significantly across models .A qualitative analysis of these various question types is presented in Appendix Section O.M Performance by medical specialtyAppendix Figure22
Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, RÃ¼diger Haunschild, Mutz, Humanities and Social Sciences Communications. 812242021</p>
<p>Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature. Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, Nigam Shah, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024. World Scientific2023</p>
<p>The emergence of large language models (llm) as a tool in literature reviews: an llm automated systematic review. Dmitry Scherbakov, Nina Hubig, Vinita Jansari, Alexander Bakumenko, Leslie A Lenert, arXiv:2409.046002024arXiv preprint</p>
<p>How to optimize the systematic review process using ai tools. Nicholas Fabiano, Arnav Gupta, Nishaant Bhambra, Brandon Luu, Stanley Wong, Muhammad Maaz, Jess G Fiedorowicz, Andrew L Smith, Marco Solmi, JCPP advances. 42e122342024</p>
<p>Future of evidence synthesis: Automated, living, and interactive systematic reviews and metaanalyses. Syed Irbaz Bin Riaz, Ahmed Arsalan, Bashar Naqvi, Mohammad Hasan, Murad Hassan, Digital Health. 232024Mayo Clinic Proceedings</p>
<p>Deep research system card. Openai, 2025</p>
<p>Gemini deep research -your personal research assistant. Google, 2025</p>
<p>Elicit: The ai research assistant. Elicit, 2025</p>
<p>Open evidence: Ai-powered medical information platform. Openevidence, 2025</p>
<p>Fda announces completion of first ai-assisted scientific review pilot and aggressive agency-wide ai rollout timeline. U S , May 2025Food and Drug Administration. FDA News Release</p>
<p>Yuhe Ke, Liyuan Jin, Kabilan Elangovan, Rizal Hairil, Nan Abdullah, Alex Liu, Heng Tiong, Sia, Rick Chai, Joshua Soh, Min Yi, Jasmine Tung, Ling Chiat, Daniel Ong, Wei Shu, Ting, arXiv:2402.01733Development and testing of retrieval augmented generation in large language models-a case study report. 2024arXiv preprint</p>
<p>Can large language models reason about medical questions?. Valentin LiÃ©vin, Egeberg Christoffer, Andreas Geert Hother, Ole Motzfeldt, Winther, Patterns. 532024</p>
<p>Medalign: A clinician-generated dataset for instruction following with electronic medical records. Alejandro Scott L Fleming, William J Lozano, Jenelle A Haberkorn, Eduardo Jindal, Rahul Reis, Louis Thapa, Julian Z Blankemeier, Ethan Genkins, Ashwin Steinberg, Nayak, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models. Tim Reason, Emma Benbow, Julia Langham, Andy Gimblett, Sven L Klijn, Bill Malcolm, Pharmacoecon Open. 82Mar 2024</p>
<p>Applications of the natural language processing tool chatgpt in clinical practice: Comparative study and augmented systematic review. Nikolas Schopow, Georg Osterhoff, David Baur, JMIR Med Inform. 11e48933Nov 2023</p>
<p>Are chatgpt and large language models "the answer"to bringing us closer to systematic review automation?. Riaz Qureshi, Daniel Shaughnessy, A R Kayden, Karen A Gill, Tianjing Robinson, Eitan Li, Agai, Systematic Reviews. 121722023</p>
<p>Assessing the risk of bias in randomized clinical trials with large language models. Honghao Lai, Long Ge, Mingyao Sun, Bei Pan, Jiajie Huang, Liangying Hou, Qiuyu Yang, Jiayi Liu, Jianing Liu, Ziying Ye, Danni Xia, Weilong Zhao, Xiaoman Wang, Ming Liu, Jhalok Ronjan Talukdar, Jinhui Tian, Kehu Yang, Janne Estill, JAMA Netw Open. 75e2412687May 2024</p>
<p>Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature. Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, arXiv:2501.071712025arXiv preprint</p>
<p>Large language models for conducting systematic reviews: on the rise, but not yet ready for use-a scoping review. Judith-Lisa Lieberum, Markus TÃ¶ws, Maria-Inti Metzendorf, Felix Heilmeyer, Waldemar Siemens, Christian Haverkamp, Daniel BÃ¶hringer, Joerg J Meerpohl, Angelika Eisele-Metzger, Journal of Clinical Epidemiology. 1811117462025</p>
<p>Generative artificial intelligence use in evidence synthesis: A systematic review. Justin Clark, Belinda Barton, Loai Albarqouni, Oyungerel Byambasuren, Tanisha Jowsey, Justin Keogh, Tian Liang, Christian Moro, O' Hayley, Mark Neill, Jones, Research Synthesis Methods. 2025</p>
<p>MedREQAL: Examining medical knowledge recall of large language models via question answering. Juraj Vladika, Phillip Schneider, Florian Matthes, Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024</p>
<p>HealthFC: Verifying health claims with evidence-based medical fact-checking. Juraj Vladika, Phillip Schneider, Florian Matthes, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>What evidence do language models find convincing?. Alexander Wan, Eric Wallace, Dan Klein, 2024</p>
<p>Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence. Kevin Wu, Eric Wu, James Zou, 2025</p>
<p>Conflictbank: A benchmark for evaluating the influence of knowledge conflicts in llm. Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, Yu Cheng, 2024</p>
<p>Untangle the knot: Interweaving conflicting knowledge and reasoning skills in large language models. Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li, 2024</p>
<p>How to write a cochrane systematic review. Jonathan C Lorna K Henderson, Narelle S Craig, David Willis, Angela C Tovey, Webster, Nephrology (Carlton). 156Sep 2010</p>
<p>Quality of cochrane reviews. quality of cochrane reviews is better than that of non-cochrane reviews. Mark Petticrew, Paul Wilson, Kath Wright, Fujian Song, BMJ. 3247336545Mar 2002</p>
<p>What is a cochrane review?. T A Cipriani, Furukawa, Barbui, Epidemiol Psychiatr Sci. 203Sep 2011</p>
<p>Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature. Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy, 2025</p>
<p>Bethesda, National Center for Biotechnology Information (US). 2010</p>
<p>Search strategy used to create the pubmed systematic reviews filter. 2019</p>
<p>Assessment of the strength of recommendation and quality of evidence: Grade checklist. a descriptive study. Camila Torres Bezerra, Antonio JosÃ© Grande, Kelly Vivianny, Douglas GalvÃ£o, Ãlvaro Henrique Marin Dos Santos, Valter Nagib Atallah, Silva, Sao Paulo Medical Journal. 14062022</p>
<p>Openai o1 system card. Openai, 2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>. OpenAI. Gpt-4 technical report. 2024</p>
<p>The llama 4 herd. A I , Meta , 2025</p>
<p>Huatuogpt-o1, towards medical complex reasoning with llms. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang, 2024</p>
<p>Openbiollms: Advancing open-source large language models for healthcare and life sciences. Malaikannan Sankarasubbu, Ankit Pal, 2024</p>
<p>. Langchain, Refinedocumentschain, </p>
<p>An introduction to the bootstrap. Bradley Efron, Robert J Tibshirani, 1994Chapman and Hall/CRC</p>
<p>Long context is not long at all: A prospector of long-dependency data for large language models. Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo, Min Yang, arXiv:2405.179152024arXiv preprint</p>
<p>Long-context llms struggle with long in-context learning. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen, 2024</p>
<p>Large language models are overconfident and amplify human bias. Fengfei Sun, Ningke Li, Kailong Wang, Lorenz Goette, arXiv:2505.021512025arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Taming overconfidence in llms: Reward calibration in rlhf. Jixuan Leng, Chengsong Huang, Banghua Zhu, Jiaxin Huang, arXiv:2410.097242024arXiv preprint</p>
<p>Fine-tuning is fine, if calibrated. Zheda Mai, Arpita Chowdhury, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Advances in Neural Information Processing Systems. 202437</p>
<p>Calibrated language model fine-tuning for in-and out-of-distribution data. Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao Zhang, arXiv:2010.115062020arXiv preprint</p>
<p>Eric Wu, Kevin Wu, James Zou, Finetunebench, arXiv:2411.05059How well do commercial fine-tuning apis infuse knowledge into llms?. 2024arXiv preprint</p>
<p>Deepseek-Ai , Deepseek-v3 technical report. 2025</p>
<p>The llama 3 herd of models. A I , Meta , 2024</p>
<p>Qwen Team, Qwen2.5 technical report. 2025</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, March 2025</p>            </div>
        </div>

    </div>
</body>
</html>
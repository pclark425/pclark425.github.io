<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3092 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3092</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3092</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270357296</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.04947v1.pdf" target="_blank">BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense</a></p>
                <p><strong>Paper Abstract:</strong> This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense. The task aims to evaluate the ability of language models to think creatively. The dataset comprises multi-choice questions that challenge models to think ‘outside of the box’. We fine-tune 2 models, BERT and RoBERTa Large. Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2. Finally, we utilize ReConcile, a technique that employs a ‘round table conference’ approach with multiple agents for zero-shot learning, to generate consensus answers among 3 selected language models. Our best method achieves an overall accuracy of 85 percent on the sentence puzzles subtask.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3092.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3092.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReConcile (round-table)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReConcile: Round-table conference consensus method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent consensus method that runs iterative rounds of answer generation and discussion between multiple LLM agents, aggregating confidence-weighted votes to produce a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reconcile: Round-table conference improves reasoning via consensus among diverse llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral8x7b, GPT-3.5, Claude (ensemble via ReConcile)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An ensemble process combining outputs from three distinct LLMs (Mixtral8x7b, OpenAI GPT-3.5, Anthropic Claude) by exchanging answers and confidences over multiple discussion rounds and selecting the choice with maximum total confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mixtral: 8x7b; GPT-3.5: not specified in paper; Claude: not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['round-table multi-agent discussion', 'confidence-weighted voting', 'iterative multi-round deliberation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Initial Response Generation: each model answers and provides a confidence score (0–1) plus reasoning; Multi-Round Discussion: models are fed peers' answers, reasonings, and confidences and asked to reconsider; Final Answer Generation: sum confidences for each choice across models (TC(c_j) = sum conf(m_i, c_j)) and select argmax.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse — leverages different LLMs' independent reasoning styles and iteratively integrates them; paper determines diversity by selecting three models with roughly similar initial accuracy so they can 'grow together' through rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (SemEval-2024 Task 9)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A multiple-choice lateral-thinking / brainteaser dataset (sentence puzzles) designed to require non-standard, creative reasoning and to avoid solutions based on straightforward commonsense assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Consensus overall accuracy at end of round 2: 0.758 (reported). The consensus result improved by ~1.0% from initial round to round 1 and ~0.8% from round 1 to round 2. The paper reports the consensus is 0.3 to 0.5 points higher than the initial round results of each model (text reports these deltas but does not provide per-model numeric accuracies in-line).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to individual-model initial rounds, the ReConcile consensus either improved or maintained each model's overall accuracy across rounds; models tended to converge on the same answer more often after rounds (Table 7 reports increased agreement counts). The method was run because combining diverse agent outputs produced a higher-consensus accuracy than single-model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReConcile-style multi-agent deliberation yields progressive improvement in overall accuracy and inter-model agreement on lateral-thinking questions; it is most effective when participating models have similar baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper warns that if one agent has substantially lower performance, its contributions (reasoning/confidence) may negatively influence others; no large negative empirical counterexample numbers are provided in the paper (only the cautionary statement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3092.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step intermediate reasoning from LLMs before producing a final answer, applied here in zero-shot mode to lateral-thinking multiple-choice questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Mixtral(8x7b), Claude, GPT-3.5, Llama-2-70b, OpenChat, Microsoft Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT prompting forces models to produce explicit intermediate reasoning steps (step-by-step analysis) prior to producing the final multiple-choice selection; implemented via prompts that request analysis and stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mixtral: 8x7b; Llama-2: 70b; others not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (step-by-step) prompting', 'zero-shot CoT (no fine-tuning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Use of a prompt template that instructs the model to analyze the question and provide step-by-step reasoning before outputting the final choice; same initial-round prompt as used in the ReConcile pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar — the same CoT prompting style was applied across multiple LLMs (i.e., uniform prompting style rather than mixing different prompting paradigms), so the reasoning style is consistent across models though the models' internal reasoning may differ.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>120-row sentence puzzle test set from the BRAINTEASER dataset used to evaluate zero-shot reasoning when prompted to produce chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Among the CoT zero-shot models evaluated, Microsoft Copilot achieved the best performance (exact accuracy not provided in-text for each model). The paper states CoT helps models focus on details and reduce errors but provides no detailed per-model numeric breakdown in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT prompting (applied uniformly) improved behavior relative to plain zero-shot answers (qualitative claim). Microsoft Copilot outperformed other CoT-applied LLMs in this evaluation; no direct ablation of CoT vs non-CoT numeric comparisons is provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT zero-shot is beneficial for lateral-thinking puzzles: it encourages stepwise analysis that helps models avoid simple logical traps; practical gains observed, with Microsoft Copilot notably effective under CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with CoT prompting, models (including Copilot) made errors on calculation questions or sometimes reformulated questions to avoid selecting 'None of the above' (examples in Appendix C); CoT does not guarantee correctness for tricky wordplay or arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3092.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned transformers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning BERT-Base and RoBERTa-Large (supervised multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of transformer-based multiple-choice models (BERT-Base and RoBERTa-Large) on the BRAINTEASER sentence and word puzzle training sets using standard input encoding (Q + choice) and cross-entropy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-Base; RoBERTa-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer encoders adapted for multiple-choice by concatenating question and choice per input sequence (format: [CLS] Q [SEP] Choice [SEP]) and fine-tuning with cross-entropy classification loss; implemented via Hugging Face Trainer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (model names indicate common sizes: BERT-Base, RoBERTa-Large) </td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['supervised fine-tuning for classification (vertical/analytic)', 'input-concatenation of question and choice']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each candidate choice is concatenated with the question into a single input sequence; the pretrained encoder produces logits and the model is fine-tuned end-to-end on labeled multiple-choice data with cross-entropy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — both models use the same fine-tuning procedure and input encoding (same style of supervised learning), representing a homogeneous reasoning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles and Word Puzzles (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Train/test splits for sentence puzzles (train 507, test 120) and word puzzles (train 396, test 96) where each example is a multiple-choice brainteaser requiring lateral thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>RoBERTa-Large achieved the best performance among the two fine-tuned models for both sentence and word puzzles (exact per-model accuracy numbers are not provided in the main text). The paper states their best method achieved 85% overall accuracy on the sentence puzzles subtask (claimed in abstract) but does not explicitly tie that number to a specific model in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RoBERTa outperformed BERT under identical fine-tuning configurations (same hyperparameters). No detailed ablation comparing fine-tuning vs CoT numeric results is presented; the paper instead reports separate evaluations for fine-tuned models and zero-shot LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervised fine-tuning with robust pretrained encoders (RoBERTa-Large) yields strong performance on BRAINTEASER tasks; however, zero-shot and multi-agent strategies provide complementary gains for different question types.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuned models still make errors on lateral-thinking puzzles that require divergent, non-literal interpretations; the dataset includes semantic/context reconstructions to reduce simple memorization, indicating limits to purely supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3092.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Microsoft Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Microsoft Copilot (chat assistant, precise mode)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial chat assistant used in precise mode for zero-shot chain-of-thought prompting that achieved the best CoT zero-shot performance reported in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Microsoft Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary chat assistant (accessed via official Copilot web interface) used with CoT zero-shot prompts; described as providing best zero-shot CoT performance among evaluated models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought prompting', 'step-by-step analysis before answer']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts request that the model analyze the puzzle and present stepwise reasoning; Microsoft Copilot was used in 'precise mode' via its web interface with the same initial prompt used by ReConcile.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — Copilot was evaluated under the CoT prompting style identical to other LLMs in that evaluation (i.e., similar reasoning style across models).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (CoT zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>120-row sentence puzzle test split used for zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported as the best performer among the CoT zero-shot LLMs considered; exact accuracy number for Copilot is not given in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Within the CoT zero-shot group, Copilot outperformed other models qualitatively (text states Copilot 'gives the best performance of this section'); no quantitative per-model table in the main text is supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Microsoft Copilot appears especially adept at recognizing lateral-play/wordplay in brainteasers under CoT prompting and often selects the intended non-literal answer without prior fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Copilot sometimes fails on arithmetic/calculation questions: it can correctly set up equations but solve them incorrectly, and sometimes reframes questions to avoid choosing 'None of the above'. Examples documented in Appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3092.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (8x7b mixture-of-experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open/large LLM (Mixtral of experts) used both in CoT zero-shot prompting and as a member of the ReConcile multi-agent ensemble; reported to have accuracy roughly comparable to GPT-3.5 and Claude in the initial round.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mixtral of experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mixture-of-experts style LLM (referenced as Mixtral 8x7b) accessed via HuggingChat; used for zero-shot CoT and as one of three agents in ReConcile.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7b (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought prompting', 'participation in multi-round ReConcile discussion']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Responds to CoT prompts with step-by-step reasoning and participates in ReConcile rounds providing answers, reasonings, and confidences; in ReConcile its outputs are combined with others by confidence-weighted voting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse when used in ensemble (different model architecture/parameters than other agents); similar when only CoT prompting style applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (zero-shot and ReConcile)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same 120-row sentence puzzle test set for zero-shot evaluation and multiple rounds for ReConcile ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported as having initial-round accuracy roughly similar to GPT-3.5 and Claude (numerical accuracies not given in-text); in ReConcile its confidence values contributed to consensus improvements (e.g., Round 2 confidences reported for examples: Mixtral 0.98).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Mixtral participates in both single-model CoT evaluation and multi-model ReConcile; when combined with peers, consensus performance increased compared to initial round individual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixtral's outputs complemented other agents in multi-agent deliberation; as part of an ensemble of similarly performing models, it helped the group converge on correct lateral-thinking answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit negative results for Mixtral alone are provided beyond the general observation that individual models still make errors on some puzzle types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3092.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (chat interface)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used chat-based LLM (GPT-3.5) used here in CoT zero-shot prompting and as one of three agents in the ReConcile ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's generative chat model accessed via ChatGPT/web interface; prompted with CoT-style prompts and placed into multi-round ReConcile discussions where it provides an answer, stepwise reasoning, and confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought prompting', 'participation in ReConcile multi-round consensus']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Produces step-by-step reasoning under CoT prompts; in ReConcile it submits initial answers/confidences and revises in later rounds after seeing peers' rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar in the CoT evaluations (same prompting), diverse in ReConcile when paired with other model architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (zero-shot & ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Zero-shot CoT on sentence puzzles and iterative ReConcile rounds combining opinions of three LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>In example traces, GPT-3.5 reported high confidences (e.g., 0.9 → 0.95) and agreed with peers in final rounds; no aggregate per-model accuracy numbers are provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>As part of the ReConcile trio, GPT-3.5’s contributions helped drive consensus accuracy upward across rounds; in isolation, its CoT performance was weaker than Microsoft Copilot according to the authors' qualitative ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 is an effective participant in multi-agent deliberation that benefits from exposure to peer reasonings; single-model CoT performance was moderate compared to Copilot in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Paper gives no numerical counterexamples specific to GPT-3.5 but notes that models sometimes maintained incorrect answers across rounds before convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3092.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3092.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude (chat assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-based LLM used in CoT zero-shot experiments and as a member of the ReConcile ensemble; reported to achieve the highest overall accuracy among the three ReConcile models in each round.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's conversational LLM accessed via its official web interface and used with CoT prompting and in multi-round ReConcile discussions; in the reported runs, it had the highest per-round accuracy among the three ensemble participants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought prompting', 'participation in ReConcile multi-round consensus']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Under CoT prompting, Claude produced stepwise reasoning; in ReConcile it reported answers and confidences, and it achieved the highest model-specific accuracy among the three agents in each round (as stated in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse in ensemble (distinct architecture/behavior compared to GPT-3.5 and Mixtral), similar in single-model CoT evaluations (same prompt style).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BRAINTEASER — Sentence Puzzles (zero-shot & ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Lateral-thinking sentence puzzles evaluated both in zero-shot CoT and as part of the ReConcile multi-agent consensus process.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported qualitatively as having the highest overall accuracy among the three ReConcile models in each round (no exact numeric per-round accuracy values provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Claude individually outperformed its ReConcile peers in round-level accuracy but still benefited from participating in the ensemble: nearly every model improved or maintained accuracy across rounds, and the consensus improved overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Claude is a strong individual performer in this lateral-thinking benchmark and contributes positively to ensemble consensus; its participation helps drive higher consensus accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No detailed negative numbers for Claude alone are given; the paper notes general puzzle categories (e.g., calculation questions) where multiple models including Claude sometimes fail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reconcile: Round-table conference improves reasoning via consensus among diverse llms <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>BRAINTEASER: Lateral thinking puzzles for large language models <em>(Rating: 2)</em></li>
                <li>Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles <em>(Rating: 1)</em></li>
                <li>Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge <em>(Rating: 1)</em></li>
                <li>MVP-tuning: Multi-view knowledge retrieval with prompt tuning for commonsense reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3092",
    "paper_id": "paper-270357296",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "ReConcile (round-table)",
            "name_full": "ReConcile: Round-table conference consensus method",
            "brief_description": "A multi-agent consensus method that runs iterative rounds of answer generation and discussion between multiple LLM agents, aggregating confidence-weighted votes to produce a final answer.",
            "citation_title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms",
            "mention_or_use": "use",
            "model_name": "Mixtral8x7b, GPT-3.5, Claude (ensemble via ReConcile)",
            "model_description": "An ensemble process combining outputs from three distinct LLMs (Mixtral8x7b, OpenAI GPT-3.5, Anthropic Claude) by exchanging answers and confidences over multiple discussion rounds and selecting the choice with maximum total confidence.",
            "model_size": "Mixtral: 8x7b; GPT-3.5: not specified in paper; Claude: not specified in paper",
            "reasoning_methods": [
                "round-table multi-agent discussion",
                "confidence-weighted voting",
                "iterative multi-round deliberation"
            ],
            "reasoning_methods_description": "Initial Response Generation: each model answers and provides a confidence score (0–1) plus reasoning; Multi-Round Discussion: models are fed peers' answers, reasonings, and confidences and asked to reconsider; Final Answer Generation: sum confidences for each choice across models (TC(c_j) = sum conf(m_i, c_j)) and select argmax.",
            "diversity_of_methods": "diverse — leverages different LLMs' independent reasoning styles and iteratively integrates them; paper determines diversity by selecting three models with roughly similar initial accuracy so they can 'grow together' through rounds.",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (SemEval-2024 Task 9)",
            "reasoning_task_description": "A multiple-choice lateral-thinking / brainteaser dataset (sentence puzzles) designed to require non-standard, creative reasoning and to avoid solutions based on straightforward commonsense assumptions.",
            "performance_by_method": "Consensus overall accuracy at end of round 2: 0.758 (reported). The consensus result improved by ~1.0% from initial round to round 1 and ~0.8% from round 1 to round 2. The paper reports the consensus is 0.3 to 0.5 points higher than the initial round results of each model (text reports these deltas but does not provide per-model numeric accuracies in-line).",
            "comparison_of_methods": "Compared to individual-model initial rounds, the ReConcile consensus either improved or maintained each model's overall accuracy across rounds; models tended to converge on the same answer more often after rounds (Table 7 reports increased agreement counts). The method was run because combining diverse agent outputs produced a higher-consensus accuracy than single-model outputs.",
            "key_findings": "ReConcile-style multi-agent deliberation yields progressive improvement in overall accuracy and inter-model agreement on lateral-thinking questions; it is most effective when participating models have similar baseline performance.",
            "counter_examples_or_negative_results": "The paper warns that if one agent has substantially lower performance, its contributions (reasoning/confidence) may negatively influence others; no large negative empirical counterexample numbers are provided in the paper (only the cautionary statement).",
            "uuid": "e3092.0",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) zero-shot",
            "name_full": "Chain-of-Thought zero-shot prompting",
            "brief_description": "A prompting strategy that elicits step-by-step intermediate reasoning from LLMs before producing a final answer, applied here in zero-shot mode to lateral-thinking multiple-choice questions.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Applied to Mixtral(8x7b), Claude, GPT-3.5, Llama-2-70b, OpenChat, Microsoft Copilot",
            "model_description": "CoT prompting forces models to produce explicit intermediate reasoning steps (step-by-step analysis) prior to producing the final multiple-choice selection; implemented via prompts that request analysis and stepwise reasoning.",
            "model_size": "Mixtral: 8x7b; Llama-2: 70b; others not specified in paper",
            "reasoning_methods": [
                "chain-of-thought (step-by-step) prompting",
                "zero-shot CoT (no fine-tuning)"
            ],
            "reasoning_methods_description": "Use of a prompt template that instructs the model to analyze the question and provide step-by-step reasoning before outputting the final choice; same initial-round prompt as used in the ReConcile pipeline.",
            "diversity_of_methods": "similar — the same CoT prompting style was applied across multiple LLMs (i.e., uniform prompting style rather than mixing different prompting paradigms), so the reasoning style is consistent across models though the models' internal reasoning may differ.",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (zero-shot evaluation)",
            "reasoning_task_description": "120-row sentence puzzle test set from the BRAINTEASER dataset used to evaluate zero-shot reasoning when prompted to produce chain-of-thought.",
            "performance_by_method": "Among the CoT zero-shot models evaluated, Microsoft Copilot achieved the best performance (exact accuracy not provided in-text for each model). The paper states CoT helps models focus on details and reduce errors but provides no detailed per-model numeric breakdown in the main text.",
            "comparison_of_methods": "CoT prompting (applied uniformly) improved behavior relative to plain zero-shot answers (qualitative claim). Microsoft Copilot outperformed other CoT-applied LLMs in this evaluation; no direct ablation of CoT vs non-CoT numeric comparisons is provided in the paper text.",
            "key_findings": "CoT zero-shot is beneficial for lateral-thinking puzzles: it encourages stepwise analysis that helps models avoid simple logical traps; practical gains observed, with Microsoft Copilot notably effective under CoT prompting.",
            "counter_examples_or_negative_results": "Even with CoT prompting, models (including Copilot) made errors on calculation questions or sometimes reformulated questions to avoid selecting 'None of the above' (examples in Appendix C); CoT does not guarantee correctness for tricky wordplay or arithmetic.",
            "uuid": "e3092.1",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Fine-tuned transformers",
            "name_full": "Fine-tuning BERT-Base and RoBERTa-Large (supervised multiple-choice)",
            "brief_description": "Supervised fine-tuning of transformer-based multiple-choice models (BERT-Base and RoBERTa-Large) on the BRAINTEASER sentence and word puzzle training sets using standard input encoding (Q + choice) and cross-entropy loss.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-Base; RoBERTa-Large",
            "model_description": "Pretrained transformer encoders adapted for multiple-choice by concatenating question and choice per input sequence (format: [CLS] Q [SEP] Choice [SEP]) and fine-tuning with cross-entropy classification loss; implemented via Hugging Face Trainer.",
            "model_size": "not specified in paper (model names indicate common sizes: BERT-Base, RoBERTa-Large) ",
            "reasoning_methods": [
                "supervised fine-tuning for classification (vertical/analytic)",
                "input-concatenation of question and choice"
            ],
            "reasoning_methods_description": "Each candidate choice is concatenated with the question into a single input sequence; the pretrained encoder produces logits and the model is fine-tuned end-to-end on labeled multiple-choice data with cross-entropy loss.",
            "diversity_of_methods": "single/similar — both models use the same fine-tuning procedure and input encoding (same style of supervised learning), representing a homogeneous reasoning approach.",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles and Word Puzzles (supervised fine-tuning)",
            "reasoning_task_description": "Train/test splits for sentence puzzles (train 507, test 120) and word puzzles (train 396, test 96) where each example is a multiple-choice brainteaser requiring lateral thinking.",
            "performance_by_method": "RoBERTa-Large achieved the best performance among the two fine-tuned models for both sentence and word puzzles (exact per-model accuracy numbers are not provided in the main text). The paper states their best method achieved 85% overall accuracy on the sentence puzzles subtask (claimed in abstract) but does not explicitly tie that number to a specific model in the main text.",
            "comparison_of_methods": "RoBERTa outperformed BERT under identical fine-tuning configurations (same hyperparameters). No detailed ablation comparing fine-tuning vs CoT numeric results is presented; the paper instead reports separate evaluations for fine-tuned models and zero-shot LLMs.",
            "key_findings": "Supervised fine-tuning with robust pretrained encoders (RoBERTa-Large) yields strong performance on BRAINTEASER tasks; however, zero-shot and multi-agent strategies provide complementary gains for different question types.",
            "counter_examples_or_negative_results": "Fine-tuned models still make errors on lateral-thinking puzzles that require divergent, non-literal interpretations; the dataset includes semantic/context reconstructions to reduce simple memorization, indicating limits to purely supervised fine-tuning.",
            "uuid": "e3092.2",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Microsoft Copilot",
            "name_full": "Microsoft Copilot (chat assistant, precise mode)",
            "brief_description": "A commercial chat assistant used in precise mode for zero-shot chain-of-thought prompting that achieved the best CoT zero-shot performance reported in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Microsoft Copilot",
            "model_description": "Proprietary chat assistant (accessed via official Copilot web interface) used with CoT zero-shot prompts; described as providing best zero-shot CoT performance among evaluated models in this paper.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "zero-shot chain-of-thought prompting",
                "step-by-step analysis before answer"
            ],
            "reasoning_methods_description": "CoT prompts request that the model analyze the puzzle and present stepwise reasoning; Microsoft Copilot was used in 'precise mode' via its web interface with the same initial prompt used by ReConcile.",
            "diversity_of_methods": "single/similar — Copilot was evaluated under the CoT prompting style identical to other LLMs in that evaluation (i.e., similar reasoning style across models).",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (CoT zero-shot)",
            "reasoning_task_description": "120-row sentence puzzle test split used for zero-shot evaluation.",
            "performance_by_method": "Reported as the best performer among the CoT zero-shot LLMs considered; exact accuracy number for Copilot is not given in the paper text.",
            "comparison_of_methods": "Within the CoT zero-shot group, Copilot outperformed other models qualitatively (text states Copilot 'gives the best performance of this section'); no quantitative per-model table in the main text is supplied.",
            "key_findings": "Microsoft Copilot appears especially adept at recognizing lateral-play/wordplay in brainteasers under CoT prompting and often selects the intended non-literal answer without prior fine-tuning.",
            "counter_examples_or_negative_results": "Copilot sometimes fails on arithmetic/calculation questions: it can correctly set up equations but solve them incorrectly, and sometimes reframes questions to avoid choosing 'None of the above'. Examples documented in Appendix C.",
            "uuid": "e3092.3",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mixtral8x7b",
            "name_full": "Mixtral (8x7b mixture-of-experts)",
            "brief_description": "An open/large LLM (Mixtral of experts) used both in CoT zero-shot prompting and as a member of the ReConcile multi-agent ensemble; reported to have accuracy roughly comparable to GPT-3.5 and Claude in the initial round.",
            "citation_title": "Mixtral of experts",
            "mention_or_use": "use",
            "model_name": "Mixtral8x7b",
            "model_description": "A mixture-of-experts style LLM (referenced as Mixtral 8x7b) accessed via HuggingChat; used for zero-shot CoT and as one of three agents in ReConcile.",
            "model_size": "8x7b (as reported in paper)",
            "reasoning_methods": [
                "zero-shot chain-of-thought prompting",
                "participation in multi-round ReConcile discussion"
            ],
            "reasoning_methods_description": "Responds to CoT prompts with step-by-step reasoning and participates in ReConcile rounds providing answers, reasonings, and confidences; in ReConcile its outputs are combined with others by confidence-weighted voting.",
            "diversity_of_methods": "diverse when used in ensemble (different model architecture/parameters than other agents); similar when only CoT prompting style applied.",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (zero-shot and ReConcile)",
            "reasoning_task_description": "Same 120-row sentence puzzle test set for zero-shot evaluation and multiple rounds for ReConcile ensemble.",
            "performance_by_method": "Reported as having initial-round accuracy roughly similar to GPT-3.5 and Claude (numerical accuracies not given in-text); in ReConcile its confidence values contributed to consensus improvements (e.g., Round 2 confidences reported for examples: Mixtral 0.98).",
            "comparison_of_methods": "Mixtral participates in both single-model CoT evaluation and multi-model ReConcile; when combined with peers, consensus performance increased compared to initial round individual outputs.",
            "key_findings": "Mixtral's outputs complemented other agents in multi-agent deliberation; as part of an ensemble of similarly performing models, it helped the group converge on correct lateral-thinking answers.",
            "counter_examples_or_negative_results": "No explicit negative results for Mixtral alone are provided beyond the general observation that individual models still make errors on some puzzle types.",
            "uuid": "e3092.4",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "OpenAI GPT-3.5 (chat interface)",
            "brief_description": "A widely-used chat-based LLM (GPT-3.5) used here in CoT zero-shot prompting and as one of three agents in the ReConcile ensemble.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI's generative chat model accessed via ChatGPT/web interface; prompted with CoT-style prompts and placed into multi-round ReConcile discussions where it provides an answer, stepwise reasoning, and confidence scores.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "zero-shot chain-of-thought prompting",
                "participation in ReConcile multi-round consensus"
            ],
            "reasoning_methods_description": "Produces step-by-step reasoning under CoT prompts; in ReConcile it submits initial answers/confidences and revises in later rounds after seeing peers' rationale.",
            "diversity_of_methods": "similar in the CoT evaluations (same prompting), diverse in ReConcile when paired with other model architectures.",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (zero-shot & ensemble)",
            "reasoning_task_description": "Zero-shot CoT on sentence puzzles and iterative ReConcile rounds combining opinions of three LLMs.",
            "performance_by_method": "In example traces, GPT-3.5 reported high confidences (e.g., 0.9 → 0.95) and agreed with peers in final rounds; no aggregate per-model accuracy numbers are provided in main text.",
            "comparison_of_methods": "As part of the ReConcile trio, GPT-3.5’s contributions helped drive consensus accuracy upward across rounds; in isolation, its CoT performance was weaker than Microsoft Copilot according to the authors' qualitative ranking.",
            "key_findings": "GPT-3.5 is an effective participant in multi-agent deliberation that benefits from exposure to peer reasonings; single-model CoT performance was moderate compared to Copilot in this study.",
            "counter_examples_or_negative_results": "Paper gives no numerical counterexamples specific to GPT-3.5 but notes that models sometimes maintained incorrect answers across rounds before convergence.",
            "uuid": "e3092.5",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Anthropic Claude (chat assistant)",
            "brief_description": "A chat-based LLM used in CoT zero-shot experiments and as a member of the ReConcile ensemble; reported to achieve the highest overall accuracy among the three ReConcile models in each round.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude",
            "model_description": "Anthropic's conversational LLM accessed via its official web interface and used with CoT prompting and in multi-round ReConcile discussions; in the reported runs, it had the highest per-round accuracy among the three ensemble participants.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "zero-shot chain-of-thought prompting",
                "participation in ReConcile multi-round consensus"
            ],
            "reasoning_methods_description": "Under CoT prompting, Claude produced stepwise reasoning; in ReConcile it reported answers and confidences, and it achieved the highest model-specific accuracy among the three agents in each round (as stated in the paper).",
            "diversity_of_methods": "diverse in ensemble (distinct architecture/behavior compared to GPT-3.5 and Mixtral), similar in single-model CoT evaluations (same prompt style).",
            "reasoning_task_name": "BRAINTEASER — Sentence Puzzles (zero-shot & ensemble)",
            "reasoning_task_description": "Lateral-thinking sentence puzzles evaluated both in zero-shot CoT and as part of the ReConcile multi-agent consensus process.",
            "performance_by_method": "Reported qualitatively as having the highest overall accuracy among the three ReConcile models in each round (no exact numeric per-round accuracy values provided in main text).",
            "comparison_of_methods": "Claude individually outperformed its ReConcile peers in round-level accuracy but still benefited from participating in the ensemble: nearly every model improved or maintained accuracy across rounds, and the consensus improved overall.",
            "key_findings": "Claude is a strong individual performer in this lateral-thinking benchmark and contributes positively to ensemble consensus; its participation helps drive higher consensus accuracy.",
            "counter_examples_or_negative_results": "No detailed negative numbers for Claude alone are given; the paper notes general puzzle categories (e.g., calculation questions) where multiple models including Claude sometimes fail.",
            "uuid": "e3092.6",
            "source_info": {
                "paper_title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms",
            "rating": 2,
            "sanitized_title": "reconcile_roundtable_conference_improves_reasoning_via_consensus_among_diverse_llms"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "BRAINTEASER: Lateral thinking puzzles for large language models",
            "rating": 2,
            "sanitized_title": "brainteaser_lateral_thinking_puzzles_for_large_language_models"
        },
        {
            "paper_title": "Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles",
            "rating": 1,
            "sanitized_title": "lateval_an_interactive_llms_evaluation_benchmark_with_incomplete_information_from_lateral_thinking_puzzles"
        },
        {
            "paper_title": "Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge",
            "rating": 1,
            "sanitized_title": "riddlesense_reasoning_about_riddle_questions_featuring_linguistic_creativity_and_commonsense_knowledge"
        },
        {
            "paper_title": "MVP-tuning: Multi-view knowledge retrieval with prompt tuning for commonsense reasoning",
            "rating": 1,
            "sanitized_title": "mvptuning_multiview_knowledge_retrieval_with_prompt_tuning_for_commonsense_reasoning"
        }
    ],
    "cost": 0.01594275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense</p>
<p>Baktash Ansari baktash_ansari@comp.iust.ac.ir 
Iran University of Science and Technology</p>
<p>Mohammadmostafa Rostamkhani mo_rostamkhani97@comp.iust.ac.ir 
Iran University of Science and Technology</p>
<p>Sauleh Eetemadi sauleh@iust.ac.ir 
Iran University of Science and Technology</p>
<p>BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense
C0FFE87DBA9929C848B0ED2E94FA2F7E
This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense.The task aims to evaluate the ability of language models to think creatively.The dataset comprises multi-choice questions that challenge models to think 'outside of the box'.We fine-tune 2 models, BERT and RoBERTa Large.Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models, such as GPT-3.5,Mixtral, and Llama2.Finally, we utilize ReConcile, a technique that employs a 'round table conference' approach with multiple agents for zero-shot learning, to generate consensus answers among 3 selected language models.Our best method achieves an overall accuracy of 85 percent on the sentence puzzles subtask.</p>
<p>Introduction</p>
<p>Evaluation methods in the NLP community predominantly emphasize Vertical thinking, characterized by sequential, analytical processes based on rationality, logic, and rules.However, SemEval-2024 Task 9, BRAINTEASER (Jiang et al., 2024b), which is based on the original BRAINTEASER dataset (Jiang et al., 2023), aims to introduce a task that promotes lateral thinking (or "thinking outside the box"), a divergent and creative process involving the exploration of new perspectives when addressing problems.The BRAINTEASER QA task consists of two subtasks for the English language: Sentence Puzzles and Word Puzzles.This task is designed to challenge the common sense reasoning capabilities of NLP models and stimulate the development of models that can think laterally.</p>
<p>• Sentence Puzzles: Sentence-type brain teaser where the puzzle-defying commonsense is centered on sentence snippets.</p>
<p>• Word Puzzles: Word-type brain teaser where the answer violates the default meaning of the word and focuses on the letter composition of the target question.</p>
<p>We generate baselines for two attention-based models, BERT (Devlin et al., 2019) and RoBERTa-Large (Liu et al., 2019), as selected in the task paper, to solve these types of multiple-choice problems.Then we fine-tune them with the same configs.After achieving some accuracy through finetuning, we explore zero-shot prompting with various large language models (LLMs).To further improve results, zero-shot prompting is conducted using a Chain of Thought technique (Wei et al., 2023).As illustrated in Figure 1, we compel the model to analyze and provide step-by-step reasoning for its answer instead of simply providing a correct option alone.This approach helps the model focus more on details and answer questions with fewer errors.Communication among multiple agents is fundamentally important in complex decision-making processes.Therefore, as our main strategy, we use the ReConcile technique (Chen et al., 2023), which generates answers by gathering the consensus of multiple models based on their confidence levels, to address these problems.To make this technique compatible with this type of dataset, we extend the application of ReConcile from yes/no questions to the BRAINTEASER questions.In this system, we apply the concept of a society of minds to multiple agents.For round table decision-making, we execute three phases: Initial Response Generation, Multi-Round Discussion, and Final Answer Generation on three language learning models: Mix-tral8x7b (Jiang et al., 2024a), Claude1 , and GPT3.52 .As illustrated in Figure 2, in each phase, we generate specific prompts (see Appendix B) for models to answer the question, along with their confidence level between 0 and 1.We then use their answers for the next round and derive an overall consensus answer for each round.The method that gave us our best result achieved a rank of 11 out of 33 for the sentence puzzles in the task leaderboard.Further details of our implementations are available through our GitHub repository.3 .</p>
<p>Background</p>
<p>Related Works</p>
<p>The exploration of reasoning abilities in large language models, lateral thinking, and common sense reasoning has been the focus of several studies in recent years.The BRAINTEASER is a novel task in this context, requiring a unique blend of these capabilities.In the paper (Zhang et al., 2022), foundational insights into the use of knowledge graphs for self-supervision in common sense reasoning tasks are provided.This work is particularly relevant as it demonstrates how external knowledge can significantly improve an LLM's ability to solve reasoning problems.Furthering the discussion on reasoning, LatEval (Huang et al., 2024) introduces an evaluation benchmark for LLMs based on lateral thinking puzzles.This process is similar to the method in solving the BRAINTEASER, which encourages models to elaborate on their reasoning, leading to improved performance.The paper Rid-dleSense (Lin et al., 2021) offers an analysis of how LLMs handle riddle questions that require linguistic creativity and common sense knowledge.This is directly linked to our approaches, where the ability of an LLM to use analogous examples influences its problem-solving effectiveness.Also, (Dou and Peng, 2022) investigates the inherent knowledge within language models and their ability to function in zero-shot common sense question answering tasks.The integration of external knowledge is particularly relevant to tasks requiring common sense and real-world information, underscoring the importance of knowledge retrieval in the context of the BRAINTEASER.MVP-Tuning (Huang et al., 2023) introduces a novel approach to knowledge retrieval using prompt tuning.This aligns with our aim in solving BRAINTEASER to evaluate the capacity of LLMs to handle questions that necessitate an understanding of the world as humans perceive it.Lastly, ReConcile (Chen et al., 2023) and (Liang et al., 2023) both discuss the use of multiple LLMs to improve reasoning capabilities.These works highlight the potential of using a diverse set of models to solve complex tasks like the BRAINTEASER, further enriching the strategies for solving such tasks with AI systems.</p>
<p>Datasets</p>
<p>The organizers provide datasets for one language: English.As mentioned previously, the dataset consists of two categories: Sentence Puzzles and Word Puzzles.The task providers construct reconstruction versions of the original data in two parallel ways: Semantic Reconstruction and Context Reconstruction.This is done to ensure that the task evaluates lateral thinking ability rather than mere memorization.The Semantic variant reformulates the initial question while preserving its answer, whereas the Context variant retains the misleading commonsense assumption as is and modifies the question and its answer to fit a different situational context.</p>
<p>The dataset is split into two parts for the evaluation phase: train and test sets for each category.In the sentence puzzles category, the train set comprises 507-row samples, while the test set consists of 120-row samples.Similarly, in the word puzzles category, the train set contains 396-row samples, and the test set has 96-row samples.Each sample includes a question with its corresponding answer and three distractors.We utilize both word and sentence puzzle datasets during the training phase, but only the sentence puzzles dataset is used for zero-shot phases.</p>
<p>Evaluation Metrics</p>
<p>The accuracy metric is employed for evaluation as described in the task paper.Performance evaluation is conducted using two accuracy metrics: Instancebased Accuracy and Group-based Accuracy for Original, Semantic, and Context questions.</p>
<p>3 System overview</p>
<p>Preprocessing</p>
<p>In the fine-tuning phase, we employ two transformer-based models, BERT and RoBERTa, for multiple choice tasks.Both models are pretrained on large text corpora.The input to these models is a sequence constructed by concatenating the question with each choice, separated by special tokens.This process is facilitated by the models' tokenizers, which convert the text into a format that the models can understand.For a given question "Q", and choices, the input to the models would be:
Input i = [CLS] Q [SEP]Choice i [SEP] (1)
where "i" represents the index of the choice.Each sequence represents a different choice, and the models' understanding of the context and the choices allows them to predict the correct answer.The fine-tuning process adapts the models to this specific task, optimizing their parameters to minimize the difference between the predicted and actual answers.</p>
<p>Model Training</p>
<p>We utilize BERT-Base and RoBERTa-Large models, along with their respective tokenizers, for word embeddings in the multiple choice task, sourced from the Hugging Face library.These models are finetuned using the Hugging Face trainer.Initially, we load the models and establish a baseline on the test set for both sentence and word puzzles.Subsequently, each model undergoes finetuning with the default Hugging Face Cross-Entropy loss function for classification, and overall accuracy is computed as the metric using the same hyper-parameter configurations as detailed in Table 1.</p>
<p>Hyperparameter Value</p>
<p>Learning</p>
<p>Chain of Thought Prompting</p>
<p>We use 6 LLMs for CoT zero-shot: Mixtral(8x7b), Claude, GPT3.5, Llama-2-70b (Touvron et al., 2023), OpenChat (Wang et al., 2023), and Microsoft Copilot4 .We use graphical web page interfaces for Claude, GPT3.5(chatGPT), and Microsoft</p>
<p>Copilot (precise mode).We use the same prompt as the ReConcile initial round.Microsoft Copilot gives the best performance of this section.</p>
<p>ReConcile Round Table</p>
<p>Models make mistakes in one or more types of questions and cannot provide the correct answer on the first attempt.We need to ask them to pay attention to certain parts of the question or give hints to the model so it can provide the correct answer.To ensure human involvement is minimized and models can help each other, we have employed the ReConcile method.Using this approach, each model complements the other.The process of this system is as follows:</p>
<p>• Initial Response Generation: First, using an initial prompt, we ask each model to provide the answer to the question, provide a reason for the answer, and declare a confidence level 0 and 1.</p>
<p>• Multi-Round Discussion: We give the responses, reasoning, and confidence levels of the three models, along with the initial prompt, as input to the models once again.This enables them to consider both the context of the question and the responses of the three models when making a selection.</p>
<p>• Final Answer Generation.In this stage, we initialize a weight for each of the 4 options of the question, and these weights are summed up with the confidence level of each model.Finally, the option with the highest weight is chosen as the correct option.</p>
<p>Let's denote the confidence of the model m i for its selected choice c j as conf (m i , c j ).Then, the total confidence of each choice can be calculated as:
TC(c j ) = 3 i=1 conf(m i , c j ) (2)
Where the sum is over all models that selected choice c j .Finally, the choice with the highest total confidence is selected as the correct choice:
c correct = arg max c j ∈C T C(c j )(3)
This means that the correct choice is the one that maximizes the total confidence over all choices.</p>
<p>The notable point is that this method should be implemented by models that roughly have equal performance to grow together after several rounds.If a model has much lower performance compared to other models, its reasoning and confidence level may negatively affect others.For this reason, we performed this task on three models: Mixtral8x7b, GPT3.5, and Claude, which have almost similar accuracy in the initial round.This iterative process can be continued until all models reach a consensus and all agree on a specific option for the questions.We repeated this process for two discussion rounds.</p>
<p>Experiments and Results</p>
<p>Experimental Setup</p>
<p>The training and test sets of the sentence and word puzzle datasets are used with a split of 0.8 and 0.2, respectively for fine-tuning.Additionally, we utilize Google Colab's T4 GPU with the hyperparameters as shown in Table 1.For zero-shot prompting, we use the 120-row test set from the sentence puzzles.</p>
<p>We leverage HuggingChat5 for Mixtral8x7b, OpenChat, and Llama2-70b.Furthermore, we utilize the official web interface of Claude, Microsoft Copilot, and GPT3.5.Results for each model and their corresponding training codes are available in the GitHub repository.</p>
<p>Results</p>
<p>As illustrated in Table 2 for the sentence puzzles and Table 3 for the word puzzles, we present the performance of BERT and RoBERTa Large in both their base and fine-tuned versions. 6We load the best model based on Overall Accuracy at the end of each training.The best performance is achieved by RoBERTa for both sentence and word puzzles.As illustrated in Appendix A For every 100 training steps, we log the overall accuracy for two models.</p>
<p>The submission scores computed by the task organizer for CoT zero-shot are available in Table 4.Among these LLMs, Microsoft Copilot achieves the best performance.The success of LLMs in responding to these questions depends on the model's ability to recognize that these questions are tricky and that it doesn't need to provide logical reasoning in many cases; the question merely plays with words.Microsoft Copilot understood this phenomenon in many questions.However, it  For Reconcile, the results of each model in every round, as well as the consensus reached in each round, are presented in Table 5.In the table, we observe that Claude achieves the highest overall accuracy among the models in each round.Nearly every model in the Reconcile system either improves or maintains its best performance in overall accuracy with each round.This suggests that they are all capable of making informed decisions based on the reasoning provided by all agents during the discussion rounds.At the conclusion of round 2, the consensus overall accuracy stands at 0.758, which is 0.3 to 0.5 points higher than the initial round results of all three models.Furthermore, we note that the consensus result improves by approximately 1 percent from the initial round to round 1, and by approximately 0.8 percent from round 1 to round 2. This indicates that after several rounds, the models converge and reach a consensus on the questions.See Appendix D. Also for an example, see Appendix E</p>
<p>Conclusion</p>
<p>In this paper, we present our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense, which challenges models to think creatively beyond conventional reasoning.Through fine-tuning BERT and RoBERTa models, as well as employing zero-shot prompting techniques using various large language models, we achieved notable performance improvements.Particularly, Microsoft Copilot performs the best without being taught beforehand, showing it understands the tricky task really well.Furthermore, our ReConcile Round Table method demonstrates the efficacy of collaborative decision-making among models, leading to a progressive improvement in overall accuracy across multiple rounds of discussion.</p>
<p>A Training logs</p>
<p>B Prompts</p>
<p>C Copilot Examples</p>
<p>As illustrated in Table 6, the model correctly answers the first three samples but provides an incorrect response for the fourth.This model lacks accuracy in calculation questions, often correctly presenting the equation but solving it inaccurately.Additionally, in some cases, the model accurately formulates the equation and even solves it correctly.However, when the desired option is not among the choices, instead of choosing the 'None of the above' option, it attempts to reinterpret the question in a way that one of the numerical options becomes correct.</p>
<p>D Reconcile Convergence</p>
<p>Table 7 indicates the number of questions to which all three models provide the same answer choice in each round.This suggests that after each round, the models achieve a relative convergence.Solution: B: Ana.Reasoning: The question states "Ana's school has 4 students in Alaska: Maria, James, Jonghyun."So, Ana is the fourth student.In a house, there are three major rooms.The first is stuffed with cash.The second is stuffed with crucial documents.The third is stuffed with jewels.One day, all of these rooms caught fire.Which room did the cops initially put out the fire? A. The one with jewelry.B. The one with important papers.C. The one filled with money.D. None of above.Solution: D: None of above.Reasoning: The question is a play on words.It's not the cops' job to put out fires, it's the firefighters' job.</p>
<p>During a 5-day competition, a professional fisherman caught 30 fish.He caught three more fish each day than the day before.On the first day, how many fish did the fisherman catch? A. Two B. One C. Three D. None of above Solution: A: Two.Reasoning: If the fisherman caught 30 fish in 5 days, and each day he caught three more fish than the day before, we can set up the equation: x + (x+3) + (x+6) + (x+9) + (x+12) = 30.Solving for x gives us x = 2. So, the fisherman caught two fish on the first day.</p>
<p>E Reconcile Example</p>
<p>The iterative process of answering the question below is illustrated in Table 8.</p>
<p>If a boat is parked in a marina and tied to a nearby station, how did its owner found the boat a couple of hours later in the middle of the ocean? A. The station wasn't fixed in the marina B. The dolphins untied the knots.C. The waves were powerful, which pushed the boat and the marina together.D. None of above.</p>
<p>Figure 1 :
1
Figure 1: Chain Of Thought Prompting (GPT3.5)</p>
<p>Figure 2 :
2
Figure 2: An Illustration of RECONCILE for Initial Round</p>
<p>Figure 3 :
3
Figure 3: Overall Accuracy of Two Models Logged Every 100 Training Steps on Sentence Puzzles.</p>
<p>Figure 4 :
4
Figure 4: Overall Accuracy of Two Models Logged Every 100 Training Steps on Word Puzzles.</p>
<p>Figure 5 :
5
Figure 5: ReConcile Initial and Discussion Prompts</p>
<p>Table 1 :
1
Hyperparameters Configuration
Rate1 × 10 −5OptimizerAdamβ 1 , β 20.9, 0.999Weight Decay0.01Batch Size1Loss FunctionCross-EntropyLogging Steps100Evaluation Metric AccuracyGlobal Seed255</p>
<p>Table 3 :
3
Models' Performance on Word Puzzles also made mistakes in several questions.For example, consider Appendix C for an illustration.</p>
<p>Question Answer How can you toss a fragile glass ball against a sturdy brick wall from a distance without shattering it?A. No problem, because sturdy bricks are very hard to crack B. You need to throw the glass ball hard.C.You need to throw the glass ball gently.D. None of above.Solution: A: No problem, because sturdy bricks are very hard to crack Reasoning: The question is a play on words.It's not the glass ball that we're worried about cracking, it's the brick wall.And brick walls are indeed very hard to crack.Ana's school has 4 students in Alaska: Maria, James, Jonghyun.What is the name of other student? A. Mark.B. Ana C. Michael D. None of above.</p>
<p>Table 6 :
6
Microsoft Copilot Answers to Some Questions
RoundQuestion CountInitial Round74Round 188Round 2104</p>
<p>Table 7 :
7
ReConcile Models Convergence</p>
<p>Available at https://claude.ai/ .
Available at https://openai.com/ .
Github Repository
Available at https://copilot.microsoft.com/
Available at https://huggingface.co/chat/
S(sentence), ori(original), sem(semantic), con(context) <br />
GPT3.5 Solution AIf the station to which the boat was tied was not properly fixed in the marina, it could have drifted away into the ocean, taking the boat with it.0.9Claude Solution AI agree with Agents two and three that an unsecured floating station drifting away best explains how the boat ended up adrift in the ocean.Mixtral Solution AWhile all options are plausible scenarios for how the boat might have ended up in the ocean, option A provides the most likely explanation given our current understanding of physics and marine environments.Both agent two and agent three suggest that if the station was not fixed properly, it could have been moved by external forces such as wind or tide, thereby pulling the boat along with it.In contrast, option C assumes that the waves were strong enough to move both the marina and the boat simultaneously, an event that seems less probable than the movement of a single object like the station.Option B attributes the boat's displacement to intelligent behavior on part of dolphins, which lack the necessary appendages to manipulate rope knots underwater.Therefore, we can infer that the station must have been unsecured, allowing it to drift out into the ocean while carrying the boat with it.0.95Round 2GPT3.5 Solution AIf the station to which the boat was tied was not properly fixed in the marina, it could have drifted away into the ocean, taking the boat with it.0.95Claude Solution A I agree with Agents two and three that an unsecured floating station drifting away provides the most logical explanation for how the boat ended up adrift.Mixtral Solution AAll three agents came to the conclusion that option A is the most likely scenario because it's more feasible that an unsecured floating station would be moved by external forces such as wind or tide, rather than assuming that powerful waves managed to push the marina and the boat together at once, or attributing the boat's displacement to intelligent behavior from dolphins who wouldn't possess the physical capability to manipulate rope knots underwater.0.98
Justin Chih, -Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2023. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. </p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Zero-shot commonsense question answering with cloze translation and consistency optimization. Zi-Yi Dou, Nanyun Peng, 2022</p>
<p>Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles. Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng, 2024</p>
<p>MVP-tuning: Multi-view knowledge retrieval with prompt tuning for commonsense reasoning. Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, Liwei Wang, 10.18653/v1/2023.acl-long.750Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris</p>
<p>. Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024a. Mixtral of experts</p>
<p>Semeval-2024 task 9: Brainteaser: A novel task defying common sense. Yifan Jiang, Filip Ilievski, Kaixin Ma, Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024). the 18th International Workshop on Semantic Evaluation (SemEval-2024)Mexico City, MexicoAssociation for Computational Linguistics2024b. 1996-2010</p>
<p>BRAINTEASER: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, 10.18653/v1/2023.emnlp-main.885Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, 2023</p>
<p>Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. 2021</p>
<p>. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019Roberta: A robustly optimized bert pretraining approach</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Openchat: Advancing open-source language models with mixed-quality data. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu, arXiv:2309.112352023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>An empirical investigation of commonsense self-supervision with knowledge graphs. Jiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, Alessandro Oltramari, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8184 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8184</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8184</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-279243716</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04565v1.pdf" target="_blank">From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems</a></p>
                <p><strong>Paper Abstract:</strong> Compound Al Systems (CAIS) is an emerging paradigm that integrates large language models (LLMs) with external components, such as retrievers, agents, tools, and orchestrators, to overcome the limitations of standalone models in tasks requiring memory, reasoning, real-time grounding, and multimodal understanding. These systems enable more capable and context-aware behaviors by composing multiple specialized modules into cohesive workflows. Despite growing adoption in both academia and industry, the CAIS landscape remains fragmented, lacking a unified framework for analysis, taxonomy, and evaluation. In this survey, we define the concept of CAIS, propose a multi-dimensional taxonomy based on component roles and orchestration strategies, and analyze four foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents, Multimodal LLMs (MLLMs), and orchestration-centric architectures. We review representative systems, compare design trade-offs, and summarize evaluation methodologies across these paradigms. Finally, we identify key challenges-including scalability, interoperability, benchmarking, and coordination-and outline promising directions for future research. This survey aims to provide researchers and practitioners with a comprehensive foundation for understanding, developing, and advancing the next generation of system-level artificial intelligence.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8184.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8184.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical memory system for LLM-based agents inspired by operating system concepts that combines in-context prompt tokens with an external archival memory and managers to enable recall beyond fixed context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical memory + agent orchestration design that separates main context (prompt tokens) from external memory and includes components such as a queue manager and function executor to handle data flow and function chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context / multi-step task execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General agent/automation tasks that exceed a single LLM context window and require recall, archival storage, and function chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / long-context task execution</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical (main context + external long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>External memory store for recall and archival storage managed by a queue manager and function executor that integrate with the LLM's prompt/context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Main prompt tokens (short-term context) and external memory entries (archived notes/records / retrieval pointers).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Manager-driven retrieval and insertion into prompt context (explicit retrieval and prompt concatenation controlled by queue manager / function executor).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative ablations reported in this survey; described as a systems design to address fixed context windows rather than benchmarked within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hierarchical memory architecture addresses LLM context-window limits by decoupling short-term prompt context from an external archival memory and coordinating access via manager components, enabling longer-horizon tasks and function chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Increased system complexity, engineering overhead, and integration challenges; survey notes scalability, orchestration, and interoperability remain open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8184.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8184.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoALA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoALA (Conceptual cognitive-inspired agent memory architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual, cognitive-science-inspired modular agent architecture that explicitly organizes working memory and multiple long-term memory types (episodic, semantic, procedural) for LLM-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive architectures for language agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CoALA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual framework proposing modular memory subsystems (working memory plus long-term episodic/semantic/procedural stores) to structure agent behavior and retrieval for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General agentic tasks (conceptual / design-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Framework aimed at organizing memory use for a variety of agent tasks (planning, reflection, tool use) rather than a specific benchmarked task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>architectural / multi-task agent design</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory and long-term memory (episodic, semantic, procedural)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Modular memory subsystems; design prescribes that agents maintain separate working memory and long-term stores which the agent accesses during planning, reflection, and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Distinct memory types: episodic traces, semantic facts, procedural knowledge; representation format not prescriptively enforced (conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Design-level retrieval policies (e.g., retrieve relevant episodic/semantic entries into working memory for in-context reasoning); specifics left to implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No empirical ablations reported in the survey; presented as a conceptual design to inform agent implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structuring agent memory into working and long-term (episodic/semantic/procedural) stores provides a useful blueprint for designing memory-augmented LLM agents and clarifies roles of different memory types in reasoning and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Conceptual work without empirical validation here; challenges include how to implement, scale, and evaluate these memory subsystems in deployed agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8184.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8184.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL (LLM agents are experiential learners)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework using a dual-memory system that autonomously collects successful and failed task trajectories, extracts natural-language insights, and retrieves relevant past examples at evaluation time to augment decisions without modifying model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expel: Llm agents are experiential learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that learns from experience by storing task trajectories (successes and failures), extracting reusable natural-language insights, and retrieving those examples to improve future decision-making (in-context augmentation without weight updates).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Agentic decision-making / evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where agents execute sequences of actions; ExpeL collects trajectories across trials to improve future performance via memory retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequential decision-making / agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual memory (successful and failed task trajectory stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Autonomous collection of trajectories, extraction of natural-language insights stored in memory; retrieved during evaluation to augment LLM context for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Natural-language summaries/insights of past trajectories and stored example trajectories (successful/failed).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieve relevant past examples/insights (semantic similarity / example retrieval) and append to prompt for in-context augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports the mechanism but does not present quantitative ablation details; described as improving decision-making without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Experience-driven memory (storing and reusing trajectory insights) enables agents to improve via in-context augmentation rather than parameter updates, making experiential learning feasible for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on quality of stored trajectories and retrieval; engineering retrieval, storage management, and generalization remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8184.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8184.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where LLM agents generate natural-language reflections about past trials which are stored in episodic memory and used to improve future performance via in-context learning rather than parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent loop with Actor, Evaluator, and Self-Reflection modules; verbal reflections (natural-language feedback) are stored in episodic memory and retrieved to guide subsequent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterated trial-based agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Repeated trials where the agent executes tasks, receives evaluative feedback, generates reflections, and uses those stored reflections to improve future trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterative learning / episodic trial tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory (verbal reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store natural-language self-reflections produced after trials; retrieve and include reflections in prompt/context for future in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Natural-language reflections (verbalized feedback and mistakes), stored as episodic entries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieve prior reflections relevant to current task (recency/semantic matching) and include them in prompt context to influence agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey summarizes the method and reports that using reflections can improve performance across trials but does not present numeric ablations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verbal reflections stored in episodic memory can serve as an effective supervision signal for agents, enabling improvements across repeated trials via in-context reuse of reflections rather than weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scalability of storing and retrieving many reflections, ensuring reflection quality, and managing noise or contradictory reflections remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8184.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8184.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-memory (Selfmen / Lift yourself up)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lift yourself up: Retrieval-augmented text generation with self-memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that integrate the model's own outputs as iterative 'self-memory' into retrieval-augmented generation, enabling use of past outputs as a non-parametric memory to guide subsequent generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lift yourself up: Retrieval-augmented text generation with self-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-memory (Selfmen / Lift Yourself Up)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RAG variants that store model outputs (self-generated content) and reuse them as memory in later retrieval/generation steps, forming an iterative self-memory loop for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Retrieval-augmented generation / iterative text generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive text generation tasks where model outputs from prior steps are stored and retrieved to inform subsequent output refinement and multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented generation / iterative multi-step generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>self-memory (iterative model-output memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Append or index model outputs as memory entries that are retrieved in subsequent retrieval rounds (iterative retrieval-generation loop).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past model outputs, generated intermediate results, and compressed summaries of retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Iterative retrieval driven by previous outputs (semantic/nearest-neighbor retrieval or prompt concatenation of past outputs), often used in a multi-round retrieval-generation loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey cites systems that use self-memory to improve iterative refinement and multi-hop reasoning but does not report unified numeric ablations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the model's own outputs as a non-parametric self-memory in RAG loops enables iterative refinement and helps address multi-hop reasoning and long-horizon dependencies without retraining the model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of propagating hallucinations or errors if self-generated outputs are low-quality; requires filtering/refinement and robust retrieval to avoid compounding mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8184.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8184.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PagedAttention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PagedAttention (efficient memory management for LLM serving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-management system for LLM serving that implements a paged key-value cache and a centralized scheduler to dynamically allocate non-contiguous memory blocks, enabling long-context serving with reduced memory waste.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient memory management for large language model serving with pagedattention.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PagedAttention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>System-level mechanism that separates attention computation and dynamically pages the KV cache across workers with a centralized scheduler (gManager) and per-instance resource managers, enabling long-context inference and scalable serving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context LLM serving / inference</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Serving LLMs with large or dynamic context lengths by managing KV cache memory allocation across GPUs to improve throughput and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>systems / inference optimization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>KV-cache paging / memory management (architectural memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Paged key-value cache with dynamic allocation and a centralized scheduler that maps non-contiguous memory blocks across GPU workers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key-value cache segments (attention keys/values) paged across memory workers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Scheduler-driven remapping of KV cache pages to active inference workers; retrieval is at the hardware/cache level rather than semantic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports that paged attention reduces memory waste and improves throughput but does not present numeric ablations here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paged KV-cache management reduces memory waste and enables scalable long-context LLM serving by dynamically allocating and mapping memory across workers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Added system complexity; integration with higher-level agent memory (semantic/episodic) requires additional design work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memgpt: Towards llms as operating systems. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners. <em>(Rating: 2)</em></li>
                <li>Lift yourself up: Retrieval-augmented text generation with self-memory. <em>(Rating: 2)</em></li>
                <li>Self-rag: Learning to retrieve, generate, and critique through self-reflection. <em>(Rating: 2)</em></li>
                <li>Cognitive architectures for language agents. <em>(Rating: 2)</em></li>
                <li>Efficient memory management for large language model serving with pagedattention. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8184",
    "paper_id": "paper-279243716",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT",
            "brief_description": "A hierarchical memory system for LLM-based agents inspired by operating system concepts that combines in-context prompt tokens with an external archival memory and managers to enable recall beyond fixed context windows.",
            "citation_title": "Memgpt: Towards llms as operating systems.",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Hierarchical memory + agent orchestration design that separates main context (prompt tokens) from external memory and includes components such as a queue manager and function executor to handle data flow and function chaining.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-context / multi-step task execution",
            "task_description": "General agent/automation tasks that exceed a single LLM context window and require recall, archival storage, and function chaining.",
            "task_type": "multi-step reasoning / long-context task execution",
            "memory_used": true,
            "memory_type": "hierarchical (main context + external long-term memory)",
            "memory_mechanism": "External memory store for recall and archival storage managed by a queue manager and function executor that integrate with the LLM's prompt/context.",
            "memory_representation": "Main prompt tokens (short-term context) and external memory entries (archived notes/records / retrieval pointers).",
            "memory_retrieval_method": "Manager-driven retrieval and insertion into prompt context (explicit retrieval and prompt concatenation controlled by queue manager / function executor).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative ablations reported in this survey; described as a systems design to address fixed context windows rather than benchmarked within this paper.",
            "key_findings": "Hierarchical memory architecture addresses LLM context-window limits by decoupling short-term prompt context from an external archival memory and coordinating access via manager components, enabling longer-horizon tasks and function chaining.",
            "limitations_or_challenges": "Increased system complexity, engineering overhead, and integration challenges; survey notes scalability, orchestration, and interoperability remain open problems.",
            "uuid": "e8184.0",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "CoALA",
            "name_full": "CoALA (Conceptual cognitive-inspired agent memory architecture)",
            "brief_description": "A conceptual, cognitive-science-inspired modular agent architecture that explicitly organizes working memory and multiple long-term memory types (episodic, semantic, procedural) for LLM-based agents.",
            "citation_title": "Cognitive architectures for language agents.",
            "mention_or_use": "mention",
            "agent_name": "CoALA",
            "agent_description": "Conceptual framework proposing modular memory subsystems (working memory plus long-term episodic/semantic/procedural stores) to structure agent behavior and retrieval for decision-making.",
            "model_name": null,
            "model_description": null,
            "task_name": "General agentic tasks (conceptual / design-level)",
            "task_description": "Framework aimed at organizing memory use for a variety of agent tasks (planning, reflection, tool use) rather than a specific benchmarked task.",
            "task_type": "architectural / multi-task agent design",
            "memory_used": true,
            "memory_type": "working memory and long-term memory (episodic, semantic, procedural)",
            "memory_mechanism": "Modular memory subsystems; design prescribes that agents maintain separate working memory and long-term stores which the agent accesses during planning, reflection, and action selection.",
            "memory_representation": "Distinct memory types: episodic traces, semantic facts, procedural knowledge; representation format not prescriptively enforced (conceptual).",
            "memory_retrieval_method": "Design-level retrieval policies (e.g., retrieve relevant episodic/semantic entries into working memory for in-context reasoning); specifics left to implementations.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No empirical ablations reported in the survey; presented as a conceptual design to inform agent implementations.",
            "key_findings": "Structuring agent memory into working and long-term (episodic/semantic/procedural) stores provides a useful blueprint for designing memory-augmented LLM agents and clarifies roles of different memory types in reasoning and tool use.",
            "limitations_or_challenges": "Conceptual work without empirical validation here; challenges include how to implement, scale, and evaluate these memory subsystems in deployed agents.",
            "uuid": "e8184.1",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ExpeL",
            "name_full": "ExpeL (LLM agents are experiential learners)",
            "brief_description": "An agent framework using a dual-memory system that autonomously collects successful and failed task trajectories, extracts natural-language insights, and retrieves relevant past examples at evaluation time to augment decisions without modifying model weights.",
            "citation_title": "Expel: Llm agents are experiential learners.",
            "mention_or_use": "mention",
            "agent_name": "ExpeL",
            "agent_description": "Agent that learns from experience by storing task trajectories (successes and failures), extracting reusable natural-language insights, and retrieving those examples to improve future decision-making (in-context augmentation without weight updates).",
            "model_name": null,
            "model_description": null,
            "task_name": "Agentic decision-making / evaluation tasks",
            "task_description": "Tasks where agents execute sequences of actions; ExpeL collects trajectories across trials to improve future performance via memory retrieval.",
            "task_type": "sequential decision-making / agent evaluation",
            "memory_used": true,
            "memory_type": "dual memory (successful and failed task trajectory stores)",
            "memory_mechanism": "Autonomous collection of trajectories, extraction of natural-language insights stored in memory; retrieved during evaluation to augment LLM context for decision-making.",
            "memory_representation": "Natural-language summaries/insights of past trajectories and stored example trajectories (successful/failed).",
            "memory_retrieval_method": "Retrieve relevant past examples/insights (semantic similarity / example retrieval) and append to prompt for in-context augmentation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports the mechanism but does not present quantitative ablation details; described as improving decision-making without weight updates.",
            "key_findings": "Experience-driven memory (storing and reusing trajectory insights) enables agents to improve via in-context augmentation rather than parameter updates, making experiential learning feasible for LLM agents.",
            "limitations_or_challenges": "Effectiveness depends on quality of stored trajectories and retrieval; engineering retrieval, storage management, and generalization remain challenges.",
            "uuid": "e8184.2",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A framework where LLM agents generate natural-language reflections about past trials which are stored in episodic memory and used to improve future performance via in-context learning rather than parameter updates.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Agent loop with Actor, Evaluator, and Self-Reflection modules; verbal reflections (natural-language feedback) are stored in episodic memory and retrieved to guide subsequent trials.",
            "model_name": null,
            "model_description": null,
            "task_name": "Iterated trial-based agent tasks",
            "task_description": "Repeated trials where the agent executes tasks, receives evaluative feedback, generates reflections, and uses those stored reflections to improve future trials.",
            "task_type": "iterative learning / episodic trial tasks",
            "memory_used": true,
            "memory_type": "episodic memory (verbal reflections)",
            "memory_mechanism": "Store natural-language self-reflections produced after trials; retrieve and include reflections in prompt/context for future in-context learning.",
            "memory_representation": "Natural-language reflections (verbalized feedback and mistakes), stored as episodic entries.",
            "memory_retrieval_method": "Retrieve prior reflections relevant to current task (recency/semantic matching) and include them in prompt context to influence agent behavior.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey summarizes the method and reports that using reflections can improve performance across trials but does not present numeric ablations in this paper.",
            "key_findings": "Verbal reflections stored in episodic memory can serve as an effective supervision signal for agents, enabling improvements across repeated trials via in-context reuse of reflections rather than weight updates.",
            "limitations_or_challenges": "Scalability of storing and retrieving many reflections, ensuring reflection quality, and managing noise or contradictory reflections remain challenges.",
            "uuid": "e8184.3",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-memory (Selfmen / Lift yourself up)",
            "name_full": "Lift yourself up: Retrieval-augmented text generation with self-memory",
            "brief_description": "Approaches that integrate the model's own outputs as iterative 'self-memory' into retrieval-augmented generation, enabling use of past outputs as a non-parametric memory to guide subsequent generations.",
            "citation_title": "Lift yourself up: Retrieval-augmented text generation with self-memory.",
            "mention_or_use": "mention",
            "agent_name": "Self-memory (Selfmen / Lift Yourself Up)",
            "agent_description": "RAG variants that store model outputs (self-generated content) and reuse them as memory in later retrieval/generation steps, forming an iterative self-memory loop for refinement.",
            "model_name": null,
            "model_description": null,
            "task_name": "Retrieval-augmented generation / iterative text generation",
            "task_description": "Knowledge-intensive text generation tasks where model outputs from prior steps are stored and retrieved to inform subsequent output refinement and multi-hop reasoning.",
            "task_type": "retrieval-augmented generation / iterative multi-step generation",
            "memory_used": true,
            "memory_type": "self-memory (iterative model-output memory)",
            "memory_mechanism": "Append or index model outputs as memory entries that are retrieved in subsequent retrieval rounds (iterative retrieval-generation loop).",
            "memory_representation": "Past model outputs, generated intermediate results, and compressed summaries of retrieved documents.",
            "memory_retrieval_method": "Iterative retrieval driven by previous outputs (semantic/nearest-neighbor retrieval or prompt concatenation of past outputs), often used in a multi-round retrieval-generation loop.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey cites systems that use self-memory to improve iterative refinement and multi-hop reasoning but does not report unified numeric ablations in this paper.",
            "key_findings": "Using the model's own outputs as a non-parametric self-memory in RAG loops enables iterative refinement and helps address multi-hop reasoning and long-horizon dependencies without retraining the model.",
            "limitations_or_challenges": "Risk of propagating hallucinations or errors if self-generated outputs are low-quality; requires filtering/refinement and robust retrieval to avoid compounding mistakes.",
            "uuid": "e8184.4",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "PagedAttention",
            "name_full": "PagedAttention (efficient memory management for LLM serving)",
            "brief_description": "A memory-management system for LLM serving that implements a paged key-value cache and a centralized scheduler to dynamically allocate non-contiguous memory blocks, enabling long-context serving with reduced memory waste.",
            "citation_title": "Efficient memory management for large language model serving with pagedattention.",
            "mention_or_use": "mention",
            "agent_name": "PagedAttention",
            "agent_description": "System-level mechanism that separates attention computation and dynamically pages the KV cache across workers with a centralized scheduler (gManager) and per-instance resource managers, enabling long-context inference and scalable serving.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-context LLM serving / inference",
            "task_description": "Serving LLMs with large or dynamic context lengths by managing KV cache memory allocation across GPUs to improve throughput and scalability.",
            "task_type": "systems / inference optimization",
            "memory_used": true,
            "memory_type": "KV-cache paging / memory management (architectural memory)",
            "memory_mechanism": "Paged key-value cache with dynamic allocation and a centralized scheduler that maps non-contiguous memory blocks across GPU workers.",
            "memory_representation": "Key-value cache segments (attention keys/values) paged across memory workers.",
            "memory_retrieval_method": "Scheduler-driven remapping of KV cache pages to active inference workers; retrieval is at the hardware/cache level rather than semantic retrieval.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports that paged attention reduces memory waste and improves throughput but does not present numeric ablations here.",
            "key_findings": "Paged KV-cache management reduces memory waste and enables scalable long-context LLM serving by dynamically allocating and mapping memory across workers.",
            "limitations_or_challenges": "Added system complexity; integration with higher-level agent memory (semantic/episodic) requires additional design work.",
            "uuid": "e8184.5",
            "source_info": {
                "paper_title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memgpt: Towards llms as operating systems.",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners.",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Lift yourself up: Retrieval-augmented text generation with self-memory.",
            "rating": 2,
            "sanitized_title": "lift_yourself_up_retrievalaugmented_text_generation_with_selfmemory"
        },
        {
            "paper_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Cognitive architectures for language agents.",
            "rating": 2,
            "sanitized_title": "cognitive_architectures_for_language_agents"
        },
        {
            "paper_title": "Efficient memory management for large language model serving with pagedattention.",
            "rating": 2,
            "sanitized_title": "efficient_memory_management_for_large_language_model_serving_with_pagedattention"
        }
    ],
    "cost": 0.02019375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Standalone LLMs to Integrated Intelligence: A Survey of Compound AI Systems
5 Jun 2025</p>
<p>Jiayi Chen 
Guiling Wang gwang@njit.edu </p>
<p>New Jersey Institute of Technology
USA</p>
<p>JUNYI YE
New Jersey Institute of Technology
USA</p>
<p>New Jersey Institute of Technology
USA</p>
<p>New Jersey Institute of Technology
NewarkNew JerseyUSA</p>
<p>New Jersey Institute of Technology
Newark, Guiling WangNew JerseyUSA</p>
<p>New Jersey Institute of Technology
NewarkNew JerseyUSA</p>
<p>From Standalone LLMs to Integrated Intelligence: A Survey of Compound AI Systems
5 Jun 2025684A0A8A133A92B62FE13E48A31E8BCAarXiv:2506.04565v1[cs.MA]2025. Manuscript submitted to ACM Manuscript submitted to ACM Manuscript submitted to ACMCompound AI systemslarge language modelsretrieval-augmented generationLLM agentsmultimodal large language modelsAI orchestrationtool-augmented model
Compound AI Systems (CAIS) is an emerging paradigm that integrates large language models (LLMs) with external components, such as retrievers, agents, tools, and orchestrators, to overcome the limitations of standalone models in tasks requiring memory, reasoning, real-time grounding, and multimodal understanding.These systems enable more capable and context-aware behaviors by composing multiple specialized modules into cohesive workflows.Despite growing adoption in both academia and industry, the CAIS landscape remains fragmented, lacking a unified framework for analysis, taxonomy, and evaluation.In this survey, we define the concept of CAIS, propose a multi-dimensional taxonomy based on component roles and orchestration strategies, and analyze four foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents, Multimodal LLMs (MLLMs), and orchestration-centric architectures.We review representative systems, compare design trade-offs, and summarize evaluation methodologies across these paradigms.Finally, we identify key challenges-including scalability, interoperability, benchmarking, and coordination-and outline promising directions for future research.This survey aims to provide researchers and practitioners with a comprehensive foundation for understanding, developing, and advancing the next generation of system-level artificial intelligence.CCS Concepts:  Computing methodologies  Artificial intelligence;  General and reference  Surveys and overviews.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) based on the Transformer architecture [178] have rapidly evolved from academic prototypes to foundational infrastructure for modern artificial intelligence.These models now support billion-user chat interfaces, enterprise copilots, scientific discovery tools, and automated code generation systems.Flagship LLMs such as GPT-4 [2], Gemini [169], and Claude [24] routinely surpass human baselines in reasoning and natural language processing benchmarks, and the global market for generative AI is projected to exceed $1.3 trillion by 2030 [12].</p>
<p>However, the same properties that make LLMs compelling-massive pretraining on static corpora and autoregressive token prediction-also introduce structural limitations.First, hallucination: LLMs may produce fluent but factually inaccurate output, undermining trust in high-stakes domains such as healthcare, law, and scientific analysis.Second, staleness: LLMs cannot access post-training knowledge, limiting their responsiveness to emerging facts.Third, bounded reasoning: finite context windows and inference budgets constrain multi-hop reasoning and long-horizon task decomposition.These limitations impede the safe and effective deployment of LLMs in dynamic, real-world environments that require recency, factual reliability, and compositional reasoning.</p>
<p>2 Architecture of Compound AI Systems</p>
<p>Definition</p>
<p>The term: Compound AI Systems (CAIS), first appeared in a post from Berkeley Artificial Intelligence Research (BAIR) [212].In this post, the authors illustrate the trend where better performance of AI is increasingly obtained by compound systems, instead of standalone LLMs.In this survey, we provide a comprehensive definition: A CAIS is a framework that integrates LLMs, components, and system designs.Its purpose is to address complex tasks that exceed the capabilities of standalone LLMs.It can be described using four dimensions: Retrieval-Augmented Generation (RAG), Multimodal Large Language Model (MLLM), LLM Agent, and Orchestration of CAIS.Pre-training, Continuous pretraining, and fine-tuning without combining other components are not considered CAIS as they are refining LLM itself.</p>
<p>General Representation of Compound AI Systems</p>
<p>From a general perspective, CAIS can be understood as a system comprising integrated interacting components and core LLMs.Components can be external tools, models, agents, multimodal encoders, prompt engineering techniques, and self-optimization.LLMs can be any kind of LLM, including general-purpose LLMs or fine-tuned LLMs trained for specific tasks.Components and LLMs are two essential parts of CAIS, without either one of them, the Compound AI System is not valid.As such, the general formula of the Compound AI System can be described as:
Compound AI System =  (, , )(1)
Where:</p>
<p>  = {LLM 1 , LLM 2 , . . ., LLM  }: The set of all LLMs in the system.Each LLM  represents an individual LLM, with  ranging from 1 to .   = { 1 ,  2 , . . .,   }: The set of all components in the system.Each component   provides:</p>
<ul>
<li>
<p>: Functionality of the component.</p>
</li>
<li>
<p>: Output generated by the component.</p>
</li>
<li>
<p>: Parameters that control its behavior.</p>
</li>
</ul>
<p> : The system design that defines the architecture and interactions between  (LLMs) and  (components).This includes orchestration, topology, or other high-level design principles.</p>
<p>The function  (, , ) abstracts the interactions and integration of:</p>
<p>(1) LLMs (): The core language models contributing to the AI system.</p>
<p>(2) Components (): Auxiliary modules or subsystems that enhance, extend, or refine the functionality of the LLMs.</p>
<p>(3) Design (): The overarching framework or strategy guiding how LLMs and components are connected, utilized, and coordinated to achieve the desired outcomes.</p>
<p>LLMs.The LLMs are the essential part of CAIS.Any kind of LLM can be incorporated into CAIS.The operation of CAIS includes fine-tuning LLM or directly using a general-purpose LLM.However, CAIS generally doesn't include Pretraining of LLM or Continuous Pretraining of LLM.</p>
<p>Components.Components of CAIS serve as enhancements to the LLMs.It can take many forms, including external tools, Machine learning models, and promotional techniques, among others.CAIS leverages the strengths of various components and orchestrates them to maximize end-to-end performance, accuracy, and efficiency.</p>
<p>Manuscript submitted to ACM LLMs are highly capable but exhibit several limitations, including hallucinations [60], outdated knowledge, unstable contextual understanding, and high computational costs associated with retraining or fine-tuning.RAG addresses these limitations by incorporating external documents, datasets, or search engines as non-parametric memory [79].This approach allows RAG to enhance the performance of LLMs in a cost-efficient manner without requiring extensive retraining on massive datasets.Additionally, RAG provides mechanisms for controlling the quality of the generated responses, thereby improving their reliability and relevance.Some popular RAG libraries, such as LangChain [171],</p>
<p>Haystack [30], and LlamaIndex [170], enable the efficient integration of retrieval with LLMs for scalable and reliable applications.</p>
<p>The RAG framework typically consists of two primary phases: Retrieval and Generation.In the retrieval phase, knowledge documents are segmented into smaller chunks, indexed, and represented as vectors through embedding techniques.Queries are similarly embedded, and the most relevant top-k chunks are selected based on cosine similarity.</p>
<p>In the generation phase, these retrieved documents serve as contextual input, which is combined with the original query and processed by the LLM.The LLM then generates a response that is both accurate and contextually grounded [188].</p>
<p>This survey categorizes the RAG architecture into three key components: the Retriever, the Generator, and RAG Design.The detailed architecture is illustrated in Figure 2. As shown, the Retriever identifies and retrieves relevant documents from external databases or search engines through various methods.The Generator processes the retrieved content to produce retrieval-augmented answers, utilizing either pre-trained or fine-tuned large language models (LLMs).The RAG design encompasses the overall orchestration of the system, characterized by distinct patterns or</p>
<p>Retriever</p>
<p>The primary role of the retriever in RAG systems is to retrieve information and external knowledge.It is essential that the retriever effectively identifies and retrieves relevant context to support accurate and meaningful responses.</p>
<p>The retriever in RAG operates along two dimensions: Phase and Approach.With regard to Phase, the retriever can be categorized into three phases: Pre-Retrieval, Retrieval, and Post-Retrieval, based on the sequence of the retrieval process.</p>
<p>Pre-Retrieval refers to the process of converting external knowledge into vector representations.This phase includes query preprocessing, chunking, embedding, and indexing [11,32].A key area of focus is query preprocessing, which includes query regeneration and query rewriting.Frameworks like DSI-QG regenerate queries to represent documents more effectively, enabling the filtering of the most relevant results [223].Query rewriting involves introducing a trainable rewriter, often a small language model, to modify input queries for improved retrieval [111].Moreover, recent research has introduced models like EMAT [192], which utilize key-value memory structures to encode external knowledge for efficient retrieval and indexing.Similarly, approaches like DSI map document text to unique identifiers (docids) and retrieve documents by generating docids based on the query [168].</p>
<p>Retrieval involves locating documents relevant to a query by measuring similarity.This phase has seen advancements aimed at improving the relevance of retrieved documents.For example, Yu et al. [207] introduced an information retrieval-based assertion retrieval method leveraging Jaccard similarity, while KAPING employs semantic similarity to retrieve relevant triples from knowledge graphs [6].Many other innovative designs and frameworks have also been proposed to improve retrieval relevance and precision, which will be discussed in detail in the section: RAG Design.</p>
<p>Manuscript submitted to ACM</p>
<p>Post-Retrieval focuses on refining the initial retrieval results through additional steps.A second round of retrieval may be conducted based on the documents retrieved in the initial phase to enhance retrieval performance.Techniques such as knowledge filtering reduce the search space by applying specific constraints.For instance, Promptagator employs a round-trip consistency check to filter out low-quality knowledge [28], while FILCO implements fine-grained sentence-wise filtering of retrieved passages [182].Knowledge refinement further enhances the quality, coherence, and usability of retrieved information.Examples include LLM-AMT, which uses a knowledge self-refiner to filter and refine retrieved information for relevance [180], and RECOMP, which compresses retrieved documents into concise textual summaries before appending them as input [194].</p>
<p>Besides knowledge filtering and knowledge refinement, reranking involves reordering and reassessing retrieved results to maximize relevance.Several advanced reranking techniques have been proposed.The approach by Lazaridou et al. [78] employs reranking of multiple generated answers based on probabilistic scoring for improved output quality.</p>
<p>Glass et al. [50] leveraged a BERT-based reranker for score adjustment in their RE2G framework.Lin et al. [89] combines dense retrieval and pair-wise reranking to select a subset of upstream examples that are fine-tuned with the base model to improve performance on unseen tasks.Additionally, reranking techniques further enhance retrieval quality and consistency by refining document selection and ordering [66,138,145].</p>
<p>From a functional perspective, retrievers can be classified into five types: Sparse Retriever, Dense Retriever, Graph Retriever, Hybrid Retriever, and LLM as Retriever.Sparse Retriever uses sparse representations of text (e.g., BM25) based on explicit term matching between queries and documents [62].Dense Retriever leverages dense vector representations (e.g., BERT) to capture semantic similarity between queries and documents [92,100,199].Graph Retriever utilizes graph structures (e.g., knowledge graphs) to locate relevant information by traversing nodes and edges [6,38,46].Hybrid Retriever combines both sparse and dense retrieval approaches to leverage the strengths of explicit term matching and semantic similarity [6,50,105].LLM as Retriever involves the use of LLMs to directly retrieve relevant knowledge based on input queries [112].</p>
<p>Generator</p>
<p>The Generator in RAG systems is essentially an LLM.It can be an original pre-trained language model, such as T5 [136],</p>
<p>FLAN [185] and LLaMA [174], or a black-box pre-trained language model, such as GPT-3 [14], GPT-4 [2], Gemini [169],</p>
<p>Claude [24].Alternatively, the generator can also be a fine-tuned language model specifically tailored for a particular task.For instance, BART [79] and T5 [63] are fine-tuned alongside the retriever, a process commonly referred to as co-training or dual fine-tuning, to enhance the quality and consistency of retrieval [93].In other scenarios, the generator is fine-tuned to effectively filter retrieved results, retaining only relevant documents and discarding irrelevant ones [106,206,217].Furthermore, the Generator can be trained and fine-tuned within a reinforcement learning framework to optimize performance in specific contexts [46].</p>
<p>RAG Design</p>
<p>RAG design is characterized by two key dimensions: pattern and framework.The pattern dimension describes how RAG systems retrieve and generate answers, encompassing iterative, recursive, and adaptive approaches.The framework dimension refers to the structural framework of RAG design, including co-training, prompt construction, and modularity.</p>
<p>Iterative is a design pattern that improves the RAG system through repeated cycles of retrieval and generation to refine outputs incrementally.For example, DSP refines its search state by iteratively retrieving relevant passages through repeated queries [68].Similarly, LLM-Augmenter iteratively optimizes prompts and LLM responses until the Manuscript submitted to ACM outputs meet a predefined utility threshold [125].RepoCoder retrieves code snippets from a broader repository context and iteratively improves them to produce accurate, context-aware completions [215].ITER-RETGEN utilizes outputs from previous iterations to refine retrieval and generation processes, effectively addressing multi-hop reasoning and complex queries [149].Selfmen integrates self-memory into RAG, using the model's outputs as iterative memory for subsequent rounds of generation [23].</p>
<p>Recursive is a design pattern where the retrieval and generation processes are applied in a nested manner to handle complex queries; it breaks down complex queries into simple ones.For instance, IRCoT combines retrieval with iterative chain-of-thought reasoning for knowledge-intensive, multi-step question answering [175].RAFT trains LLMs with a mixture of relevant and irrelevant documents to encourage a focus on pertinent content and generate chain-of-thought reasoning [217].Additionally, LLM-AMT processes lengthy and information-dense passages by splitting them into smaller sections for efficient handling [180].</p>
<p>Adaptive is a design pattern that dynamically adjusts the retrieval or generation strategies based on the context or feedback to optimize performance.For example, SELF-RAG enables LLMs to decide dynamically when to retrieve information, generate responses, and critique their outputs via self-reflection [5].FLARE actively determines retrieval timing and content through anticipatory next-sentence predictions, using low-confidence tokens as retrieval triggers [64].</p>
<p>Similarly, CON generates concise, contextually relevant reading notes for each retrieved document while systematically assessing their relevance and reliability [209].</p>
<p>Co-training is a framework where the retriever and generator are jointly trained to enhance their collaboration.</p>
<p>For example, RAG-end2end updates all components of the RAG model asynchronously during training, including the retriever, generator, and external knowledge base encodings [155].Alternatively, DSI replaces traditional multi-stage retrieval pipelines with a single Transformer-based model that indexes and retrieves documents directly through its parameters [168].</p>
<p>Prompt Construction is a framework that focuses on designing and optimizing prompts to enable the generator to efficiently utilize retrieved information, thereby improving the relevance and accuracy of the final response.For example, Ren et al. [142] present new prompting strategies-priori judgement, which evaluates a question before answering, and posteriori judgement, which assesses the correctness of the answer to explore the impact of retrieval augmentation.</p>
<p>Modular is a framework where RAG itself is designed as independent modules or applications, allowing for flexibility, easy replacement, and integration of different components.For instance, R2A adopts a modular architecture that allows integration with various reinforcement learning algorithms [52].AAR employs a plug-and-play approach, adapting trained retrievers to function with unseen, larger target language models [211].RETROPROMPT enhances input sequences through retrieval mechanisms to guide training and improve predictions [22], while REPLUG augments LLMs by incorporating external retrieval systems without altering the LLM's internal parameters [152].</p>
<p>Limitations and Future Trends</p>
<p>Although RAG systems have developed rapidly with numerous innovative designs, modern RAG systems still face several limitations.For instance, seamlessly integrating retrieved information into the generation process remains a significant challenge, particularly for longer contexts or multi-modal tasks [23].Additionally, issues such as retrieval quality [26], scalability [99], and knowledge conflicts or inconsistencies [175] continue to hinder the effectiveness of modern RAG systems.</p>
<p>Recent findings suggest several promising directions for the future development of RAG systems.One emerging trend is the creation of ready-to-use application frameworks that facilitate direct deployment for domain-specific Manuscript submitted to ACM Chen et al.</p>
<p>tasks [146,177,180,196,199,207]. Another significant direction involves greater integration of multi-modal data, combining text, images, videos, and audio to address complex and multi-dimensional use cases [20,59,104,139,204].</p>
<p>Furthermore, the combination of RAG systems with other machine learning models, such as reinforcement learning and classification models, offers opportunities to improve control and accuracy [52,100,111,180].Finally, leveraging</p>
<p>LLMs in end-to-end retrieval processes is anticipated to produce tightly integrated pipelines, offering an advantage over the traditional retrieval-then-rank approach [112,168].</p>
<p>LLM Agent</p>
<p>LLM agents have emerged as a powerful paradigm for enabling intelligent, autonomous behavior.The ecosystem of LLM agents can be broadly categorized into three interconnected layers: Application Scenario, Agent Framework, and Agent Mechanism, as illustrated in Figure 2. Together, these layers define the operational architecture and capabilities of LLM agents, guiding both their theoretical design and practical implementation.</p>
<p>Application Scenario</p>
<p>At the highest level, LLM agents are applied across a spectrum of real-world and simulated environments, serving as intelligent assistants, embodied agents, or even assistants in scientific experiments.These scenarios reflect the practical embodiments of LLM capabilities, where agents are tasked with executing domain-specific objectives, ranging from simulating human interactions to developing software systems.</p>
<p>4.1.1General Purposes.General-purpose LLM agents are designed to perform a wide array of tasks across diverse domains, offering flexible problem-solving capabilities without requiring domain-specific customization.For example, Gato [141] is a generalist agent trained using a transformer-based model that can handle multiple modalities (text, images, proprioception), tasks (captioning, dialogue, control), and embodiments (robots, Atari).Gato uses a single set of weights to perform over 600 tasks, demonstrating the feasibility of a multi-task, multi-embodiment agent.Another instance is MINEDOJO [42], a novel framework built on Minecraft that combines thousands of natural language-based open-ended tasks and an internet-scale multimodal knowledge base to train generalist embodied agents using large-scale pretraining techniques, enabling agents to perform diverse and natural language-prompted tasks.Fig. 2. A structured overview of LLM agents across three dimensions: application scenarios (e.g., general-purpose, embodied), agent frameworks (single-agent and multi-agent architectures), and agent mechanisms (system orchestration, reasoning loops, and tool use).</p>
<p>particularly GPT-4, to automatically generate reward functions for reinforcement learning (RL) tasks, achieving or exceeding human-level performance across a wide variety of tasks.</p>
<p>LLM Agent can be applied in an autonomous driving scenario.DiLu [186], a framework that instills human-like knowledge into autonomous driving via LLMs, leveraging reasoning, reflection, and memory to improve decisionmaking in complex driving environments.It aims to move beyond traditional data-driven approaches by incorporating a knowledge-driven paradigm based on common-sense reasoning.</p>
<p>LLM Agent assists in scientific experiments as well.Boiko et al., 2023 [13] presents an LLM-based Intelligent Agent system capable of autonomously designing, planning, and executing scientific experiments by combining internet search, documentation parsing, and automation tools, where it introduce a modular system composed of a Planner, Manuscript submitted to ACM Chen et al.</p>
<p>Web Searcher, Docs Searcher, Code Execution, and Automation modules, which coordinate via LLMs (primarily GPT-4)</p>
<p>to autonomously perform scientific tasks.</p>
<p>Agent Framework</p>
<p>The agent framework defines how LLM agents are organized, instantiated, and deployed.This layer encompasses the majority of paradigms in multi-agent frameworks, including multi-agent collaborative frameworks, multi-agent debate frameworks, and the workflow of multi-agent systems.[36] propose a multi-agent debate framework in which multiple instances of language models collaboratively reason, critique, and revise their answers to enhance factuality and reasoning across various tasks.The authors design a system where several LLM agents independently generate responses, then iteratively review and revise their answers based on peer responses through multiple rounds of debate, using only prompting and without model fine-tuning.Furthermore, the Multi-Agent Debate (MAD) framework [86] involves two LLM-based agents (affirmative and negative) debating a topic iteratively with a judge LLM that decides when to stop and which response is correct, enabling exploration of diverse reasoning paths.</p>
<p>Multi-Agent Collaborative</p>
<p>Multi Agent Systems.</p>
<p>Multi-agent systems encompass any architecture involving two or more autonomous agents-LLMs that interact within a shared environment to achieve individual or collective goals.Communication strategy defines the protocols and methods through which LLM agents exchange information, coordinate actions, and negotiate meanings within a multi-agent setting.For example, ChatEval [16], a multi-agent debate framework that uses multiple LLMs with diverse roles to collaboratively evaluate generated text, aiming to simulate the quality and depth of human evaluation.ChatEval enables multiple LLM agents, each with a distinct role prompt (e.g., critic, scientist), to engage in structured discussion through designed communication strategies (one-by-one, simultaneous, or summarization-enhanced) before aggregating a final evaluation via majority vote or averaging.</p>
<p>Role playing assigns specific identities, functions, or perspectives to each agent, enabling structured interactions that reflect diverse viewpoints or specialized expertise.For example, in a multi-agent collaborative framework, agents are assigned roles such as instructor-assistant, where the former is responsible for raising questions and asking</p>
<p>Manuscript submitted to ACM follow-up questions.The latter is responsible for addressing the questions and making any necessary corrections in the conversation's dialogue.For example, Autogen [189] proposes a general-purpose framework for building nextgeneration LLM applications through multi-agent conversations, allowing agents to interact via programmable, flexible chat patterns to solve complex tasks collaboratively.Furthermore, CAMEL [81] is a framework that enables multiple language agents to autonomously cooperate via role-playing, facilitating the scalable generation of high-quality conversational datasets and the analysis of multi-agent LLM behaviors.</p>
<p>A shared message pool serves as a centralized communication hub where agents can post, access, and interpret messages, facilitating asynchronous and collective reasoning.For example, in MetaGPT [56], the shared message pool is a centralized, structured communication system that enables asynchronous collaboration among multiple LLM agents, each assigned a human-like role.Instead of unstructured dialogue, agents publish and subscribe to typed messages (e.g., Requirement, Design, Task, Code) with clearly defined sender, receiver, and content fields.This publish-subscribe model allows agents to coordinate without direct interaction, trace the full workflow, and maintain modular, scalable collaboration.The shared message pool ensures clarity, reduces hallucinations, and supports flexible system extension, forming the backbone of MetaGPT's multi-agent orchestration.</p>
<p>Agent Mechanism</p>
<p>This layer covers the mechanism of the LLM-driven agent.How the system is organized, how the LLM agent reasons, plans, perceives feedback from the environment, takes action, and reflects on failures.Moreover, tool use, one of the most essential features of the LLM agent, is also discussed in this layer.</p>
<p>Orchestration</p>
<p>System.An orchestration system coordinates the components and execution flow of an LLM agent, managing memory, tools, and reasoning steps to ensure coherent and goal-directed behavior.Most of LLM agent systems are prompt-based systems, which means the agent system guides the agent's behavior and reasoning by structuring task instructions, examples, and context directly within the input prompt to the language model, without modifying its internal parameters.Inner Monologue [61], Voyager [179], and ChatEval [16] are some representative examples that leverage the prompt-based method to compose their agent systems.However, there are agent systems that apply fine-tuning as well.For instance, GPT4Tools [200], a method that enables open-source language models to use multi-modal tools by generating a tool-usage instruction dataset via self-instruction from GPT-3.5 and fine-tuning with LoRA.The author fine-tunes models, such as Vicuna-13B, using LoRA for efficient parameter adaptation.Structured memory management is an essential component of the LLM agent system.Based on different use cases, memory storage typically employs a combination of short-term memory, long-term memory, and, in some instances, episodic memory.For example, CoALA [157] is a conceptual framework inspired by cognitive science to organize and design modular, memory-augmented LLM-based agents, where it introduces a modular agent architecture with working and long-term memory (episodic, semantic, procedural).Moreover, ExpeL [219] utilizes dual memory systems, where the agent autonomously collects successful and failed task trajectories, extracts reusable natural language insights, and retrieves relevant past examples to augment its decision-making during evaluation, all without modifying LLM weights.</p>
<p>Interactive Reasoning Loop.</p>
<p>The interactive reasoning loop refers to the iterative process through which an LLM agent perceives input, plans actions, executes tasks, and incorporates feedback to refine its decisions over time.Plan and reason refers to the agent formulating a strategy by analyzing the current context, setting subgoals, and logically determining the following steps to achieve its objective.ReAct [203] sets the paradigm of planning and reasoning for an LLM agent that takes autonomous actions.It's a prompt-based method that enables LLMs to interleave verbal Manuscript submitted to ACM reasoning with action-taking, allowing them to solve complex tasks by dynamically planning, acting in environments, and adjusting based on feedback.ReAct augments the LLM's action space to include both environment-changing actions and language-based reasoning traces, prompting models to alternate between thinking and acting using a unified policy, guided by few-shot demonstrations.Another representative of work exploring the reasoning capacity of LLMs is Tree of Thoughts [202], a framework that enables language models to solve complex problems by reasoning over a search tree of intermediate "thoughts, " allowing for planning, exploration, and backtracking, unlike left-to-right token-level generation.Tree of Thoughts introduces a tree-based problem-solving structure where each node is a "thought" (a coherent reasoning step).The model generates, evaluates, and searches over these thoughts using LLMs and strategies like breadth-first or depth-first search to select promising reasoning paths.</p>
<p>Action refers to an agent executing a task or issuing a command-such as calling a tool, generating output, or interacting with an environment-based on reasoning.The key difference between a standalone LLM and an LLM agent is that an LLM agent can execute autonomous actions without needing textual instructions from a human.Environment feedback refers to the agent receiving signals or data from the external environment, reflecting the outcome of its previous actions and informing future decisions.For example, Voyager [179] employs an iterative prompting loop that refines actions through feedback, execution errors, and self-verification.Moreover, LLM-Planner [156], a high-level planner that leverages LLMs like GPT-3 to perform few-shot planning for embodied agents by generating and dynamically updating task plans grounded in real-world environments.</p>
<p>Self-reflection refers to the agent critically evaluating its reasoning and performance, identifying errors or inefficiencies to improve subsequent behavior.For example, Reflexion [153] is a framework where LLM-based agents learn via selfgenerated verbal feedback rather than weight updates, using natural language reflections stored in episodic memory to improve performance across trials.Reflexion combines an LLM-based Actor, Evaluator, and Self-Reflection model in a loop where agents iteratively generate actions, evaluate performance, and produce verbal feedback for memory-based in-context learning.Another example of self-reflection is Recursive Criticism and Improvement (RCI) [69], which is a prompting method where the LLM iteratively generates an output, critiques it, and then improves upon it.This is applied across three stages of action grounding-task grounding, state grounding, and agent grounding-to ensure outputs are appropriate, feasible, and executable in computer task environments.</p>
<p>Tool Use.</p>
<p>Tool use empowers LLM agents to go beyond language generation by interacting with external services-such as APIs, computational tools, or information sources-to perform complex tasks like data processing, real-time querying, and integration with third-party applications.</p>
<p>Model Context Protocol (MCP) [4]  encoded features into a shared representation space compatible with the language model.Typically implemented as a linear projection, multi-layer perceptron (MLP), or cross-attention mechanism, the visual projector ensures that non-textual features can be effectively processed alongside text embeddings.For example, HoneyBee [15] includes two novel locality-enhanced projectors: C-Abstractor (using convolution) and D-Abstractor (using deformable attention), designed to maintain both locality and flexibility.Another example is SEED [47], a discrete image tokenizer designed to integrate vision into LLMs by enabling both image-to-text and text-to-image tasks.Additionally, BLIP-2 [82] introduces a Querying Transformer (Q-Former) that bridges the modality gap between frozen pre-trained image encoders and LLMs.Q-Former consists of a set of learnable query vectors that extract the most relevant visual features from the frozen image encoder through cross-attention layers.These extracted features serve as a bottleneck representation, ensuring that only the most informative visual information is passed to the LLM.</p>
<p>Fusion module.</p>
<p>Fusion module in MLLMs is a mechanism designed to merge and process data from multiple modalities (e.g., text and images) into a shared representation space.This allows the model to leverage complementary information from different modalities.The goal of the fusion module is to combine and align features from different modalities in a way that enables the model to understand and generate coherent outputs across these modalities.For instance, ShareGPT4V [18], a framework designed to improve MLLMs by leveraging high-quality, detailed captions for better modality alignment.The authors propose that detailed captions, rather than VQA data or brief descriptions, significantly enhance the performance of LMMs.Furthermore, Fact-RLHF [161] addresses the problem of multimodal misalignment in MLLMs by introducing a reinforcement learning-based approach that enhances the reward model using factual information, such as image captions and ground-truth multi-choice options.This method prevents reward hacking and improves multimodal alignment by providing more reliable training signals.</p>
<p>5.1.4Core LLM.LLM is the backbone of the MLLMs, responsible for generating outputs based on the fused multimodal representations.Typically built upon transformer-based architectures, the core LLM processes the integrated features and produces coherent responses, leveraging its extensive pre-trained knowledge.This component ensures that the model retains its language capabilities while incorporating multimodal understanding.In the domain of MLLMs, LLMs can be pre-trained, fine-tuned, or frozen.Pre-trained LLM of MLLMs is a model trained on a large-scale, diverse dataset using self-supervised or weakly supervised learning.The goal is to develop general multimodal representations.For example, PaLM-E [35] injects multimodal data-images, states, and textual descriptions-into an LLM using learned encoders that translate continuous sensor inputs into language tokens.The model, based on the pre-trained PaLM-562B and Vision Transformer (ViT-22B), is trained end-to-end on diverse datasets.Similarly, NExT-GPT [190] is an end-to-end general-purpose MLLM that can handle any-to-any modality conversions, meaning it can process and generate outputs in text, image, video, and audio.It integrates a pre-trained LLM (Vicuna-7B) with multimodal adaptors and diffusion decoders, enabling flexible cross-modal understanding and generation.Fine-tuned LLM in MLLMs is the pre-trained model which further trained on a specific dataset for a narrower, task-specific purpose.The objective of fine-tuning is to optimize performance for a specific downstream task, such as Manuscript submitted to ACM Chen et al.</p>
<p>image captioning and medical report generation.For example, MultiModal-GPT [51] is a multimodal dialogue model fine-tuned from OpenFlamingo and utilizes Low-rank Adapter (LoRA) layers in both the gated-cross-attention and self-attention modules to enhance image-text interaction.Likewise, TimeChat [143], a large vision-language model designed to process long videos while accurately localizing temporal events, where its model is fine-tuned on TimeIT, a novel instruction-tuning dataset with 125K timestamped video instances, ensuring precise event localization and efficient processing of lengthy video content.</p>
<p>In MLLMs, a Frozen LLM refers to an approach where the core language model (LLM) remains fixed (i.e., its weights are not updated during training or fine-tuning).Instead, only the multimodal components (like the vision encoder, visual projector, or fusion module) are trained.The primary motivation for leveraging frozen LLMs is computational efficiency and efficient multimodal adoption.For instance, FROMAGe [74]</p>
<p>Fusion Strategy</p>
<p>MLLMs process and integrate multiple modalities, such as text, vision, and audio, to enhance understanding and reasoning across diverse data sources.A fundamental challenge in MLLMs is modality fusion, which determines how heterogeneous information is combined to form a unified representation.The choice of fusion strategy has a significant impact on model performance, influencing its ability to capture cross-modal interactions, preserve modality-specific semantics, and optimize computational efficiency.Broadly, fusion strategies can be categorized into four types: Early Fusion, Late Fusion, Cross-modal Attention Fusion, and Hybrid Fusion.</p>
<p>Early fusion.</p>
<p>Early fusion refers to the integration of multimodal features at the initial stages of the model pipeline, typically before any independent unimodal processing occurs.In this approach, raw or low-level extracted features from different modalities are concatenated or transformed into a shared representation space.This enables the model to learn cross-modal interactions from the outset, resulting in a tightly coupled representation of multimodal data.For example, the Gemini family of AI models [169] is trained on a large-scale multimodal dataset and leverages post-training techniques, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), to improve performance and alignment, which exhibits its early fusion strategy.</p>
<p>Late fusion.</p>
<p>Late fusion preserves modality-specific feature extraction pathways before merging the outputs at the final prediction or decision-making stage.This approach is advantageous when different modalities contribute complementary information, but their interaction is not essential at an early stage of processing.For instance, Woodpecker [205]  which employs learned query embeddings extracted from a Q-Former, and the Q-Former uses cross-attention layers to extract instruction-aware visual features from the frozen image encoder.These query embeddings act as a compressed representation of the image and are passed as soft prompts to the LLM.</p>
<p>Hybrid fusion.</p>
<p>Hybrid Fusion leverages a combination of early, late, and cross-modal attention fusion strategies to optimize multimodal representation learning.By integrating multiple fusion techniques within the same model, hybrid fusion seeks to balance the strengths and weaknesses of individual strategies.A model using hybrid fusion may first employ early fusion to extract joint feature representations, apply cross-modal interactions for deeper contextual alignment, and finally use late fusion for decision-level integration.This approach offers flexibility and robustness, making it particularly useful for complex multimodal tasks that require both localized and holistic reasoning.For example, KOSMOS-2 [127] employs hybrid fusion approach that combines early fusion with cross-attention mechanisms, discretizes bounding box coordinates into location tokens, combine transformer and text and image features directly into a unified Transformer model, enhance the model's ability to perform phrase grounding, referring expression comprehension, and multimodal referring.Another example of a hybrid fusion strategy is GPT4RoI [216], a regionaware multimodal model that extends large vision-language models to understand fine-grained interactions within region-of-interest (RoI) areas.It employed a hybrid fusion strategy, which involves early fusion with interleaved spatial instruction encoding, RoIAlign, or deformable attention.This strategy then interleaves these RoI embeddings with text embeddings, enabling the model to perform detailed region captioning, reasoning, and interaction beyond traditional image-level vision-language models.</p>
<p>Modality Approaches</p>
<p>MLLMs are designed to process and generate outputs across multiple modalities, integrating information from images, audio, video, and text-rich data.The effectiveness of these models depends on the underlying architectures and representation learning strategies used for each modality.In this section, we explore key approaches for different modalities, including image, audio, video, and text-rich image processing.Variational Autoencoders (VAEs) [71] are a deep generative framework that learns latent variable representations by combining probabilistic graphical models with deep learning.VAEs consist of an encoder (inference network) that approximates the posterior distribution of latent variables given input data and a decoder (generative network) that reconstructs data from these latent variables.In MLLMs, VAEs are utilized for learning a structured latent space, thereby enhancing representation learning across modalities (e.g., text and images) by capturing complex dependencies and generating coherent multimodal outputs.</p>
<p>The Vision Transformer (ViT) [34] is often employed as the visual encoder, extracting rich feature representations from images that can be aligned with textual inputs, thereby enabling cross-modal reasoning and generation tasks.ViT it to handle speech recognition, translation, and voice activity detection.In MLLMs, WHISPER serves as a speech-text interface, enabling models to transcribe and understand spoken language inputs for downstream applications such as audio-based retrieval, multimodal dialogue, and voice-assisted generation.</p>
<p>WavLM [19] is a self-supervised pre-trained model designed to learn universal speech representations for a wide range of speech processing tasks, including speech recognition, speaker verification, speech separation, and diarization.It employs a masked speech denoising and prediction framework that combines masked speech modeling with a denoising mechanism, allowing the model to capture both content and speaker-related information.The Transformer-based architecture incorporates gated relative position bias, enhancing the model's ability to process temporal dependencies.</p>
<p>In MLLMs, WavLM serves as a robust speech encoder, providing high-quality speech representations that improve audiotext alignment and multimodal reasoning, thereby enhancing the performance of tasks such as automatic transcription, spoken language understanding, and cross-modal retrieval.</p>
<p>Manuscript submitted to ACM by treating video as a sequence of frame-level patches, encoding these patches into token embeddings, and then feeding them into a Transformer encoder.In MLLMs, TimeSformer serves as a video encoder, providing high-quality spatiotemporal representations for downstream tasks such as video-text retrieval, video captioning, and video-based question answering, thereby improving the model's ability to reason over sequential visual information.</p>
<p>5.3.4</p>
<p>Text-rich Image.Text-rich images, such as scanned documents, charts, and infographics, necessitate models that can jointly process visual layouts and embedded text.This modality is particularly useful in tasks like document understanding, optical character recognition (OCR), and multimodal reasoning over structured data.</p>
<p>Optical Character Recognition (OCR) [120] has been applied extensively in MLLMs, OCR extracts text from images, enabling MLLMs to process structured documents and handwritten text effectively.On the other hand, OCR-free methods refer to recent advancements that explore direct modeling of text within images without explicit OCR extraction, leveraging transformer-based architectures to capture both textual and visual features simultaneously.These methods aim to improve robustness in noisy or low-resource environments.For example, Donut [70] is an OCR-free Visual Document Understanding (VDU) model designed to directly map document images to structured outputs without relying on Optical Character Recognition (OCR).It employs a Transformer-based architecture with a Swin Transformer encoder for extracting visual features and a BART-style decoder that generates structured text sequences, such as JSON representations, conditioned on the document image.In MLLMs, Donut serves as a document processing backbone, enabling seamless integration of text and layout information from images into multimodal reasoning tasks without the need for intermediate OCR-based pipelines.</p>
<p>Tasks</p>
<p>The capabilities of MLLMs extend beyond traditional unimodal processing, enabling them to tackle a diverse range of tasks that integrate multiple modalities.These tasks can be broadly categorized into those that are intrinsically multimodal and those that leverage multimodal capabilities alongside other applications.</p>
<p>Multimodal-Only Tasks.</p>
<p>MLLMs can process and reason over multiple modalities without additional external components, making them well-suited for core multimodal tasks.</p>
<p>Multimodal Reasoning refers to the ability to infer relationships between different modalities, such as answering questions about an image or performing visual commonsense reasoning.For example, Socratic Models (SMs) [214],</p>
<p>a modular framework that composes multiple large pretrained models via multimodal-informed prompting, enables zero-shot multimodal reasoning without requiring finetuning.The framework utilizes multi-step prompting chains,</p>
<p>Manuscript submitted to ACM</p>
<p>where outputs from one model (e.g., CLIP for vision, GPT-3 for language, and Wav2CLIP for audio) are iteratively processed and refined by other models, enabling joint inference on multimodal tasks without additional training.Cross-Modal Retrieval and Alignment refers to Mapping information across different modalities, such as retrieving relevant images based on textual queries or aligning text with corresponding visual elements.For instance, GILL [73] a model that fuses frozen text-only LLMs with pre-trained image encoders and decoder models, allowing for text generation, image retrieval, and novel image generation within multimodal dialogue.The model is trained using image-caption pairs, with a decision module that determines whether to retrieve an image or generate a new one.Content Generation refers to producing multimodal outputs, such as generating detailed textual descriptions of images, creating multimodal narratives, or synthesizing visuals based on textual prompts.For example, Emu [159] is a multimodal foundation model designed for seamless text and image generation within a unified autoregressive framework.It integrates interleaved image, text, and video data, leveraging a causal transformer to transform visual features into a sequence of latent embeddings, enabling versatile text-to-image generation tasks.</p>
<p>Contextual Understanding refers to comprehending and interpreting multimodal contexts to improve downstream applications, such as understanding an image's scene in relation to provided text.For example, ContextDET [213] is a novel framework for contextual object detection that enhances MLLMs by enabling them to locate, identify, and associate visual objects with language inputs.The model integrates language and vision tokens, using contextual LLM outputs as prior knowledge to improve detection accuracy and flexibility.</p>
<p>5.4.2</p>
<p>Tasks with Other Applications.Beyond purely multimodal tasks, MLLMs are increasingly being integrated with additional AI techniques to enhance their reasoning, retrieval, and interaction capabilities.With Chain-of-Thought (CoT) Prompting enhancing reasoning by combining multimodal understanding with stepby-step reasoning techniques.For instance, DDCoT [220] utilized and enhanced multimodal CoT prompting.DDCoT decomposes complex questions into sub-questions, marking those requiring visual information as "uncertain" to prevent hallucinations.A Visual Question Answering (VQA) model then provides relevant visual inputs, which are integrated with textual reasoning to generate accurate answers.This structured CoT approach significantly enhances zero-shot and fine-tuning performance.</p>
<p>With Retrieval-Augmented Generation (RAG) leveraging external knowledge retrieval to supplement multimodal content generation and question answering.For example, RA-CM3 [204], a retrieval-augmented multimodal model that retrieves relevant text and image data from an external memory and integrates it into a CM3 Transformer-based generator.The retriever is a CLIP-based dense retriever, and the generator is trained with a novel retrieval-augmented objective to effectively leverage retrieved documents.</p>
<p>With Reinforcement Learning (RL) Agents enabling interactive decision-making by integrating MLLMs with RL-based agents for autonomous reasoning, planning, and action-taking in complex environments.For example, ESPER [210] uses reinforcement learning to align multimodal inputs with language model generations.It employs CLIP to provide a cosine similarity-based reward function for optimization and uses proximal policy optimization (PPO) to train a lightweight vision-to-text transformation while keeping the language model parameters frozen.</p>
<p>Embodied AI embeds MLLMs into physical or virtual agents that perceive and act in the real world, enabling tasks such as navigation, object manipulation, and human-AI interaction.For example, PaLM-E [35] integrates vision and language with robotic sensor data to enable grounded reasoning and decision-making for embodied tasks.Similarly, TaPA [193], a framework for embodied task planning, aligns LLMs with visual perception models to generate executable</p>
<p>Orchestration</p>
<p>The orchestration of CAIS is inherently sophisticated, often requiring complex architectures.LLM-serving systems are designed to exceed the performance of standalone LLMs by integrating multiple layers, diverse models, or LLMsupported agents that collaborate to deliver optimal results.We categorize LLM-serving systems into three distinct layers: the structural layer, the mechanism layer, and the objective layer, as illustrated in Figure 4.</p>
<p>Structure Layer</p>
<p>The structure layer represents the architectural organization of components within a system, focusing on how tasks are distributed and coordinated.It includes two primary designs: Hierarchical Structure and Central Structure.This layer provides the foundational framework for the system's operational and interaction dynamics.6.1.1Hierarchical Structure.In a hierarchical structure, tasks are decomposed into subtasks, which are managed by components organized in a tree-like hierarchy with clear dependencies.Typically, these systems comprise multiple layers, with each layer breaking down inputs or requests into smaller, more manageable portions.Components in a hierarchical structure are modular, meaning they are distinct, self-contained, and interact with others in a coordinated manner.An example of hierarchical structure, MemGPT [124], introduces a hierarchical memory system inspired by operating systems to address the limitations of LLMs' fixed context windows.The system comprises a main context Manuscript submitted to ACM Chen et al.</p>
<p>(prompt tokens within the LLM), external memory (for recall and archival storage), and components such as the queue manager and function executor to handle data flow and function chaining.This design ensures seamless integration of dynamic memory management with existing LLM capabilities.Furthermore, Acharya et al. [1] proposed a LLM-based recommendation system, which combines an LLM-based description generator with a GRU-based recommendation model.Descriptions generated by the LLM are embedded using BERT, combined with item ID embeddings, and processed through a GRU layer to predict recommendations.The hierarchical structure of LLM-serving systems has also been widely applied across various domains, including data exploration systems [109], multi-agent operating systems [119], and recommendation systems [181].</p>
<p>Central</p>
<p>Structure.In a central structure, a central manager oversees and coordinates interactions among components to ensure efficient collaboration and coordination.The central manager acts as a scheduler or coordinator, dispatching and allocating resources to the most suitable components or tools based on the nature of the task.For example, PagedAttention [77] utilizes a centralized scheduler and distributed GPU workers.The KV cache manager dynamically allocates and maps non-contiguous memory blocks, facilitating scalable and efficient LLM serving while integrating with various decoding strategies.Moreover, BOLAA [103] organizes the agents in a modular structure, with the controller acting as a central node to route tasks among specialized agents and manage their interactions with the environment.This topology allows scalable and efficient collaboration between agents.Similar to the hierarchical structure, the central structure has been effectively applied to diverse domains, including the Internet of Things (IoT) [27] and multimodal systems [130].</p>
<p>Mechanism Layer</p>
<p>The mechanism layer defines the operational processes that govern how a system handles tasks and produces results.This layer ensures seamless execution and adaptability within the system.Besides input handling and output generation, it includes Task Planning, which determines the steps of tasks required to achieve the desired outcome.For example, LLM-MARS [107], a Multi-Agent Robot Systems, supports dynamic task allocation, environmental feedback, and real-time question-answering using LoRa adapters.Another example is Infinite-LLM [88], where its system integrates a distributed attention mechanism (DistAttention) with a centralized scheduler (gManager) and instance-level resource managers (rManagers).This design decouples attention computation from the main inference pipeline, enabling flexible task allocation across GPUs in a cluster.</p>
<p>Model Communication.</p>
<p>Model Communication is a mechanism that enables different AI models or multiple agents or components within a system to communicate and exchange information.For example, de Zarz et al. [29] proposed a system that integrates a DNN-based adaptive PID controller for optimal gain prediction and a GPT-3.5-turboLLM for real-time feedback.Moreover, TransLLaMa [75], a simultaneous machine translation (SiMT) system, integrates an automatic speech recognition (ASR) model with a fine-tuned LLM.Tan et al. [163] proposed a system that involves multiple expert agents for specific tasks, each working with distinct abstraction spaces.Solutions are generated by combining primitive functions, structured prompting, and iterative feedback loops to refine outputs.6.2.2 Tool Use.Tool Use.A key mechanism in CAIS is enabling LLMs to leverage external tools (e.g., APIs, search engines, databases, code executors) [132,151,183].This overcomes limitations such as static knowledge [187] and a lack of computational precision, allowing LLMs to interact with real-time data and perform specialized tasks, essentially Manuscript submitted to ACM acting as controllers for broader computational resources.For instance, agents like OSAgent use standardized APIs to interact with operating systems [195].</p>
<p>Tool use in LLM agents is generally enabled through two main approaches: training-based and prompt-based methods.Training-based methods involve fine-tuning models to integrate tool use capabilities.Examples include GPT-4's structured function calling mechanism [2] and Toolformer's self-supervised approach, which teaches the model to invoke tools such as search engines or translation services [147].In contrast, prompt-based methods rely on strategic prompt design to guide LLM behavior.For instance, the ReAct framework interleaves reasoning with actions like tool invocation or web search via prompting, enabling complex task decomposition without additional fine-tuning [203].</p>
<p>A specific application is enhancing LLMs with Web Search.This addresses knowledge staleness and improves factuality.</p>
<p>Models can be fine-tuned for web interaction, like WebGPT using a text-based browser to search, navigate, and cite sources [123], or GopherCite using search to find supporting evidence for its claims [121].Prompting techniques, such as ReAct, can also guide LLMs to issue search queries during their reasoning process [203].Orchestration frameworks, such as LangChain and AutoGPT, further help manage complex tool interactions and chaining [189].</p>
<p>Memory Management.</p>
<p>Memory Management is a function or algorithm that is designed to efficiently allocate, manage, and optimize the use of memory resources during tasks.For example, PagedAttention [77] implements a memory management system inspired by paging in operating systems, reducing memory waste and improving throughput by dynamically allocating non-contiguous blocks of memory for key-value caches.Similarly, JungleGPT [144] utilizes caching nodes for low-latency global data access, serving as distributed snapshots to facilitate cost-efficient and scalable operations.</p>
<p>Feedback Loop.</p>
<p>Feedback Loop refers to a system mechanism where outputs or results are cyclically returned as inputs to refine or optimize the process.This iterative approach enables continuous improvement and adaptation by addressing errors and inefficiencies found in previous iterations.For example, in Text-to-SQL systems [114], feedback loops refine SQL query generation based on execution results, improving success rates with each iteration.</p>
<p>Objective Layer</p>
<p>The objective layer outlines the primary objectives that guide the design and operation of Compound AI System architectures.This layer focuses on essential priorities, including maintaining privacy and security, reducing bias, ensuring low response times, and improving cost efficiency.These objectives collectively establish a clear framework to guarantee that the system remains effective, reliable, and aligned with user requirements.</p>
<p>Privacy and Security in CAIS involve mechanisms to mitigate risks associated with sensitive data exposure and system vulnerabilities.For instance, architectures like SECGPT [191] utilize isolated execution environments and hierarchical hub-and-spoke models to manage interactions securely, protecting against risks from untrustworthy thirdparty apps and imprecise natural language interfaces.These designs ensure controlled data exchange, user permission management, and encapsulated app environments to safeguard privacy and security.Additionally, Evertz et al. [40] investigate Low Latency measures the ability to process and respond to requests with minimal delay, ensuring efficient and timely user interactions.For example, LLM-Slice [95] reduces average latency by leveraging dedicated wireless network slices for efficient resource allocation, achieving significant reductions in transmission times for LLM tasks.</p>
<p>Cost-efficiency in LLM serving systems refers to the ability to optimize resource usage to achieve desired outcomes, such as performance and quality, while minimizing costs.For instance, PALIMPZEST [96] is a declarative system that optimizes AI workloads by enabling engineers to define tasks at a high level of abstraction.The system compiles these tasks into optimized execution plans, balancing cost, runtime, and quality through logical and physical optimizations.</p>
<p>Benchmarks and Evaluation Metrics</p>
<p>The rapid growth and advancement of CAIS has led to the development of numerous evaluation benchmarks and datasets.Because CAIS can have different objectives and structures, their respective evaluation benchmarks are diverse.</p>
<p>This chapter introduces the key tasks, datasets, and quantitative metrics used for evaluating CAIS across the domains of RAG, Multimodal LLMs, LLM Agents, and Orchestration, as detailed in Table 1.</p>
<p>Evaluation of RAG</p>
<p>To systematically evaluate RAG systems, various standardized datasets and benchmarks are utilized, tailored to assess both retrieval and generation components across diverse tasks, including question answering, retrieval, summarization, and fact verification.The goal is typically to measure improvements in areas like retrieval quality (using metrics such as MRR and nDCG) and generation quality/factuality (using metrics like EM, F1, ROUGE, BLEU, and Accuracy) compared to standalone LLMs.Table 1 summarizes the primary downstream tasks, associated datasets, and quantitative metrics used for RAG evaluation.</p>
<p>Evaluation of LLM Agents</p>
<p>Evaluating LLM agents, the decision-making cores of many CAIS, requires assessing their agentic behaviors, such as role simulation, interactive reasoning, and tool use, in dynamic environments.Specialized benchmarks assess role fidelity (Role-Consistency Score), reasoning quality (Reasoning Trace Accuracy), and tool-use efficiency (Tool Call Accuracy, Token Efficiency).Specific benchmarks and metrics for these aspects are listed in Table 1.</p>
<p>Evaluation of Multimodal LLMs</p>
<p>With MLLMs, evaluation expands to assess the integration and reasoning over multiple input types, such as images and text.Key evaluation areas include multimodal reasoning (e.g., visual QA), chart and document understanding, safety and alignment, and general multimodal capabilities.Standard NLP and vision metrics (Accuracy, F1, EM, BLEU, and ROUGE) are often used alongside custom safety metrics and specialized benchmarks, as detailed in Table 1.</p>
<p>Evaluation of Orchestration of Compound AI Systems</p>
<p>Evaluating the orchestration of CAIS, which integrates multiple components like retrievers and agents, involves assessing efficiency, reliability, and fairness beyond task accuracy.Evaluation often considers the Structure Layer (infrastructure performance: throughput, Latency), the Mechanism Layer (system coordination: Task Completion Time,</p>
<p>Manuscript submitted to ACM</p>
<p>RAG</p>
<p>Reasoning QA Natural Questions [76], TriviaQA [65], HotpotQA [201], WebQuestions [9] Accuracy, F1 Score, EM Passage Retrieval WikiAsp [54], MS MARCO [7], SQUAD [137], TruthfulQA [91] MRR, nDCG, Precision, Recall, F1 Score</p>
<p>Multi-Doc Summarization</p>
<p>OpenBookQA [122], PopQA [115] ROUGE, BLEU, F1 Score</p>
<p>Open-domain QA Multi-News [41], NarrativeQA [72], MuSiQue [176], BEIR [172],</p>
<p>RealTimeQA [67], UniEval [221] EM, MRR, nDCG, F1 Score, Accuracy Extractive QA RAGAs [39], KILT [129] EM, F1 Score, Precision, Recall Fact Verification FEVER [173], DROP [37], TREC-DL [25] Accuracy, Precision, Recall, F1 Score Evaluation Metrics 2WikiMultihopQA [55], StrategyQA [48] EM, F1 Score, MRR, nDCG Multimodal LLM Reasoning and Commonsense QA MM-Vet [208], MMC-Benchmark [97], ALLaVA [17], SEED-Bench [80] Accuracy, F1 Score, EM, Perplexity Chart and Document Understanding ChartQA [116], DocVQA [117], TextVQA [154] Accuracy, F1 Score, BLEU, ROUGE Safety and Alignment MM-SafetyBench [102] Accuracy, F1 Score, Custom Safety Metrics Multimodal Evaluation MME [87], MultiBench [85] Accuracy, F1 Score, MRR, Perplexity, BLEU, ROUGE</p>
<p>Orchestration of Compound AI Systems</p>
<p>Structure Layer BigDataBench [45],</p>
<p>AIBench Training [165] Throughput, Latency Overhead, Bandwidth Utilization Mechanism Layer AISBench [33], WebArena [222], Long Range Arena [167], ZeroSCROLLS [148] Mean Task Completion Time, Average API Response Time, Cache Hit Rate Objective Layer AI Fairness 360 [8] Fairness-specific metrics</p>
<p>LLM Agent</p>
<p>Role-Playing RoleLLM [184], AgentBench [101] , AgentBoard [108] Role-Consistency Score, Self-Correction Rate</p>
<p>Interactive Reasoning</p>
<p>AgentQuest [49], InfiAgent-DABench [58], CriticBench [94] Reasoning Trace Accuracy, Generalization Score Tool Use ML-Bench [166], Berkeley Function Calling Leaderboard [198] Tool Call Accuracy, Token Efficiency Chen et al.</p>
<p>API Response Time, Cache Hit Rate), and the Objective Layer (alignment with goals: fairness metrics).Representative benchmarks for each layer are provided in Table 1.</p>
<p>These benchmarks and datasets collectively form a framework for evaluating modern AI systems, spanning RAG, Multimodal LLMs, LLM Agents, and Orchestration, capturing task performance alongside efficiency, fairness, and orchestration quality, as summarized in Table 1.</p>
<p>8 Challenges, Limitations, and Opportunities</p>
<p>Challenges and Limitations</p>
<p>Compound AI Systems represent a significant advancement in the capabilities of LLM-based architectures, yet they are not without challenges and limitations.As this field matures, it is crucial to understand the key obstacles that hinder its scalability, efficiency, and safety, while also identifying the most promising future directions.</p>
<p>8.1.1System Complexity and Scalability.Compound AI Systems often involve the integration of multiple components-retrievers, agents, multimodal encoders, memory modules, and orchestration mechanisms.This architectural complexity introduces engineering overhead, increasing the difficulty of deployment, debugging, and system optimization.Moreover, the added complexity can lead to performance bottlenecks and increased inference latency, particularly when coordinating multiple LLMs and tools in real-time.</p>
<p>8.1.2Evaluation and Benchmarking.Evaluating Compound AI Systems is inherently difficult due to the diversity of their components and the dynamic nature of their outputs.Traditional NLP benchmarks do not adequately capture the interactive or multi-modal capabilities of such systems.There is a lack of unified evaluation frameworks that consider system-level performance metrics, such as latency, robustness, and resource usage, alongside task-specific accuracy.between LLMs and external components remains a challenge.Issues such as API misalignment, inconsistent data formats, limited error handling, and brittle tool chaining cause failures in real-world applications.Moreover, many tools and retrievers were not originally designed for interaction with LLMs, leading to interoperability and reliability issues.</p>
<p>Opportunities</p>
<p>Future opportunities for Compound AI Systems lie in developing unified and modular architectures that simplify integration across components, advancing multimodal alignment through improved fusion strategies and grounded reasoning, and enabling end-to-end trainable pipelines for tighter coordination among retrievers, agents, and generators.</p>
<p>Emerging directions also include meta-agent systems with self-adaptive orchestration, enhanced human-AI collaboration via interpretable interfaces and controllable planning, and sustainable design practices focused on resource efficiency and deployment scalability.These advances will pave the way for more robust, flexible, and intelligent</p>
<p>Compound AI Systems capable of operating in dynamic, real-world environments.</p>
<p>Conclusion</p>
<p>The survey highlights Compound AI Systems as a significant advancement in the expansion of LLM capabilities by integrating them with external components such as tools, agents, retrievers, and multimodal encoders.This survey captures the emergence of Compound AI as a paradigm that systematically enhances performance, adaptability, and context-awareness across a wide range of applications.We identify four core dimensions-RAG, LLM Agents, MLLMs, Manuscript submitted to ACM and Orchestration-that structure the landscape of Compound AI Systems, each contributing to a more intelligent and flexible AI ecosystem.Technical integrations with methodologies like planning, memory management, and selfreflection have further broadened their capabilities.While Compound AI Systems show promise in handling complex and multimodal tasks, challenges remain in orchestrating components effectively and ensuring system-level robustness.The scope of Compound AI continues to expand into real-world, multimodal, and interactive environments, underscoring its growing impact on both research and industry.This expanding ecosystem requires improved benchmarking and evaluation strategies to more effectively capture system-level performance.As Compound AI Systems continue to evolve, refining these methods will be key to fully realizing their potential in the next generation of intelligent applications.</p>
<p>Fig. 1 .
1
Fig. 1.Taxonomy of Retrieval-Augmented Generation (RAG) systems.The diagram categorizes the core components into three primary modules: Retriever, Generator, and RAG design.</p>
<ol>
<li>3 . 1
31
Image.Image-based MLLMs process visual data to extract semantic representations and generate textual descriptions, answer questions, or perform visual reasoning.CLIP (Contrastive Language-Image Pre-training)[133] in MLLMs serves as the visual encoder, transforming images into feature embeddings that align with text representations, which Manuscript submitted to ACM are then processed by a core LLM to generate responses, perform reasoning, or enable downstream multimodal tasks.It utilizes a dual-encoder architecture, where an image encoder (such as a Vision Transformer or ResNet) and a text encoder (a Transformer) map images and text into a shared multimodal embedding space.The model is trained using a contrastive learning objective: given a batch of (image, text) pairs, it maximizes the cosine similarity between the embeddings of the correct pairs while minimizing the similarity for incorrect ones.This enables CLIP to generalize to unseen tasks without explicit fine-tuning, allowing for zero-shot classification by comparing image embeddings with text embeddings of class descriptions.</li>
</ol>
<p>applies the Transformer architecture to image recognition by treating images as sequences of patches, rather than using convolutional layers.By applying self-attention mechanisms directly to image patches, ViT can capture long-range dependencies and enhance interpretability in visual tasks.BLIP (Bootstrapping Language-Image Pre-training)[83] is a vision-language pre-training (VLP) framework designed for both vision-language understanding and generation tasks.It introduces a multimodal mixture of encoder-decoder (MED) architecture, which allows it to function flexibly as a unimodal encoder, an image-grounded text encoder, or an image-grounded text decoder.In MLLMs, BLIP enhances cross-modal reasoning and text generation by effectively aligning visual and textual representations, thereby facilitating tasks such as image captioning, visual question answering, and image-text retrieval.5.3.2Audio.The integration of audio into MLLMs enables speech recognition, transcription, and even speech-to-text or text-to-speech generation.WHISPER (Web-scale Supervised Pretraining for Speech Recognition)[134] is a large-scale weakly supervised automatic speech recognition (ASR) model designed to generalize across multiple languages and domains without fine-tuning.The model uses a sequence-to-sequence approach, where the encoder processes log-mel spectrogram representations of speech, and the decoder generates transcriptions in a task-conditioned format, allowing</p>
<ol>
<li>3 . 3
33
Video.Video extends multimodal learning by introducing both temporal and spatial dimensions, requiring models to process sequential frames and synchronize with textual or auditory components.Flamingo[3] is a few-shot visual language model (VLM) designed for open-ended vision-language tasks by integrating pre-trained vision and language models with novel cross-modal architectural components.It employs a Perceiver Resampler to convert high-dimensional visual features into a compact set of tokens, which are then processed by a frozen LLM augmented with trainable Gated Cross-Attention Dense layers that facilitate multimodal reasoning.Within MLLMs, Models like Flamingo handle video by encoding frames using a frozen vision model, such as NFNet, and aggregating them into a fixed number of visual tokens via a Perceiver Resampler.These tokens are then fed into a frozen LLM with trainable Gated Cross-Attention Dense layers, allowing the model to reason about video content in conjunction with textual information.TimeSformer[10] is a convolution-free Transformer-based architecture for video understanding that leverages self-attention mechanisms to model spatiotemporal dependencies.It extends the Vision Transformer (ViT)</li>
</ol>
<p>Fig. 4 .
4
Fig.4.An overview of the Compound AI System orchestration layer, highlighting the relationship between its structure (e.g., hierarchical or centralized component organization), mechanism (such as input handling, task planning, and feedback loops), and objective (including privacy, low latency, and performance).</p>
<p>plans grounded in real-world physical constraints.It constructs a multimodal dataset and employs open-vocabulary object detection to ensure that generated action sequences correspond to actual objects present in the scene.</p>
<ol>
<li>1 . 3
13
Tool and Component Integration.Despite advances in tool-use and agent-based planning, seamless integration</li>
</ol>
<p>[56]ework.A multi-agent collaborative framework enables multiple LLM agents to work together toward a shared objective, often leveraging role specialization, communication protocols, and coordinated planning.For example, MetaGPT[56]is a multi-agent collaborative framework for software development that integrates Standardized Operating Procedures (SOPs) into LLM-based agent workflows to improve coherence and accuracy.</p>
<p>[162]ver, AgentVerse[21]is a dynamic multi-agent collaboration framework inspired by human group problemsolving, where agents can adjust their roles, communicate, and collaborate across various tasks, including software development, consulting, and gaming.AgentVerse models the problem-solving process as a loop of four stages: expert recruitment, collaborative decision-making, action execution, and evaluation, allowing dynamic team adjustment based on feedback.Another example worth mentioning is Talebirad et al., 2023[162], where the authors propose a graph-based multi-agent collaboration framework where agents and plugins form nodes with defined communication channels, allowing role-specific LLM agents to collaborate, dynamically generate new agents, provide feedback, and manage execution.4.2.2 Multi-Agent DebateFramework.In a multi-agent debate framework, agents are intentionally assigned to argumentative roles, utilizing structured dialogue to explore divergent viewpoints, validate reasoning, or reach consensus through deliberation.For instance, Du et al.</p>
<p>[31]rmation retrieval allows agents to access relevant documents or data from structured or unstructured sources to support grounded reasoning and informed responses.For instance,Sun et al.,2023 [160]explored and proposed whether LLMs like ChatGPT and GPT-4 can serve as effective passage re-ranking agents in information retrieval and introduced a novel permutation generation method that allows LLMs to directly output ranked passage lists.Web search enables agents to query the internet in real-time, expanding their knowledge beyond pre-training andallowing them to answer up-to-date or domain-specific questions.For example, paper MIND2WEB[31]proposes MINDACT, a two-stage framework combining a small LM for webpage element ranking and a large LM for action prediction in a multiple-choice QA format, enabling the model to operate efficiently on long and noisy web pages.MLLMs extend the capabilities ofLLMs beyond text-based understanding and generation to multimodal comprehension and content creation.These models are designed to process and generate information across multiple modalities, including text, images, audio, and video.By incorporating cross-modal learning, MLLMs enhance traditional LLMs, enabling them to integrate and reason across different types of data.This section categorizes MLLMs into four aspects: architecture of MLLMs, fusion strategy inside MLLMs, MLLMs' modality approaches, and tasks of MLLMs.From the architecture standpoint, MLLM has four major components: Encoder, Visual Projector, Fusion Module, and Core LLM Model.Fusion strategy is another key aspect of MLLMs; Audio encoder converts raw audio signals into compact feature representations, capturing temporal and spectral characteristics, then processes waveform or spectrogram inputs for downstream multimodal tasks.For example, SALMONN [164], a MLLM designed to understand and process general auditory inputs, including speech, audio events, and music.The model integrates a pre-trained text-based LLM with two audio encoders-Whisper for speech and BEATs for non-speech audio-using a window-level query transformer (Q-Former) for alignment.Multiple encoder system consists of separate encoders for different modalities (e.g., text, vision, audio) that process their respective inputs independently before aligning or fusing them.This approach allows specialized feature extraction per modality, improving performance in multimodal learning.For example, GLaMM [140] consists of an end-to-end multimodal model architecture that includes a Global Image Encoder, Region Encoder, LLM, Grounding Image Encoder, Overview of Multimodal Large Language Models.The diagram categorizes MLLMs by architecture, fusion strategy, modality, and tasks.and Pixel Decoder.The Global Image Encoder extracts holistic image features, while the Region Encoder refines region-specific details using a hierarchical feature pyramid.The Pixel Decoder is based on SAM (Segment Anything Model) and generates segmentation masks from textual queries, enabling seamless grounded conversations.5.1.2Visual projector.Since different modalities have distinct feature spaces, a visual projector is employed to transform
Chen et al.Encoder(1) Vision Encoder, (2) Audio Encoder, (3) Multiple Encoder SystemVisual Projector(1) Visual Tokennizer, (2) Q-FormerArchitectureFusion ModuleModality Alignment(1) Pre-trained,Core LLM(2) Fine-tuned,(3) Frozen5 Multimodal Large Language Models (MLLMs)Decoder(1) Vision Decoder, (2) Audio DecoderEarly FusionLate FusionFusionStrategyCross-Modal Attention FusionLargeMultimodalModelsHybrid FusionImage(1) CLIP, (2) VAE (3) ViT, (4) BLIPAudio(1) Whisper, (2) WavLMModalityApproachesVideo(1) Flamingo (2) TimeSformerText-Rich Image(1) OCR, (2) OCR FreeMultimodal only(1) Multimodal Reasoning, (2) Cross-Modal Retrieval and Alignment, (3) Content Generation, (4) Contextual UnderstandingTasksWith other(1) With CoT Prompting, (2) With RAG, (3)applicationsWith RL Agents (4) Embodied AILLM agents can invoke external services via API Calls, enabling access to dynamic functionalities such as data Fig. 3.processing, computation, or third-party applications. For example, ToolLLM [131] is a framework that empowers LLMsto effectively use over 16,000 real-world APIs by combining a new dataset (ToolBench), a decision-making algorithm(DFSDT), and a trained model (ToolLLaMA) to achieve robust tool-use capabilities.Manuscript submitted to ACM
[118]standardized interface proposed by Anthropic that enables language models to interact with external tools, memory, documents, and user interfaces in a structured and flexible manner.It defines how models receive context and respond with function calls or natural language, facilitating advanced agent behaviors.For example, MCP servers such as AWS KB Retrieval facilitate retrieval from the AWS Knowledge Base using Bedrock Agent Runtime.A server like Brave Search enables web and local search capabilities using Brave's Search API.Moreover, a server like Filesystem provides secure file operations with configurable access controls.Manuscript submitted to ACM different MLLMs employ various fusion strategies, broadly classified into early fusion, late fusion, cross-modal fusion, and hybrid fusion.In terms of modality, MLLMs can be further classified by their focused modality approaches: image, audio, video, or text-rich image.Moreover, based on different tasks and usages, MLLMs can be defined as multimodal or integrated with other techniques, such as MLLMs with Chain-of-Thought (CoT) or MLLMs with RAG.5.1 Architecture 5.1.1Encoder.The Encoder is responsible for extracting meaningful representations from raw input data.Depending on the modality, different encoders are used.The extracted features serve as high-dimensional embeddings, which are further processed for alignment with the language model.Vision encoder extracts high-level feature representations from raw visual inputs, which then transform pixel data into compact embeddings suitable for downstream multimodal processing.For instance, LLaVA[98]is a multimodal model combining a CLIP-based vision encoder with the Vicuna LLM, fine-tuned on GPT-4-generated multimodal instructionfollowing data.Additionally, MM1[118]is a family of multimodal models trained using a carefully optimized mix of image-caption, interleaved image-text, and text-only data.MM1's model architecture includes the image encoder, vision-language connector, and LLM.</p>
<p>is a training-free framework that consists of five stages: extracting key concepts from generated text, formulating verification questions, validating visual knowledge using expert models, generating structured claims about the image, Manuscript submitted to ACM and modifying hallucinated responses while incorporating bounding box evidence.The use of a late fusion strategy ensures that the model's generated text aligns with the image content.
5.2.3 Cross-modal Attention fusion. Cross-modal attention Fusion aims to establish direct interactions between differentmodalities throughout the model's architecture. Unlike early and late fusion, which either merge modalities at theinput or output stages, cross-modal fusion involves multiple interactions between modality-specific representations viaattention mechanisms, gating functions, or co-learning frameworks. This strategy is commonly employed in transformer-based architectures where modalities attend to each other through cross-attention layers. Cross-modal fusion enablesdeeper integration and alignment of multimodal information, improving contextual understanding and generalization intasks such as visual question answering and image captioning. For example, Zhang et al.(2024) [218] propose a methodcalled Multimodal-CoT that integrates both text and vision modalities into a two-stage framework to improve reasoningperformance. The model employed a cross-modal gated fusion strategy, in which a gate mechanism determines theamount of information from vision to be injected into language representations. Another example is BLIVA [57],</p>
<p>[150]dentiality vulnerabilities in systems that integrate LLMs with external tools.It proposes a framework that simulates specific attack scenarios to evaluate and measure risks related to sensitive data leakage when LLMs interact with integrated tools and services.Bias in CAIS refers to the tendencies of models to produce outputs influenced by underlying patterns or training data that may reflect stereotypes, inaccuracies, or a lack of diversity.Sharma et al.[150]investigate the effects of LLM-powered conversational search systems on selective exposure and opinion polarization.It employs studies thatManuscript submitted to ACM compare LLM-supported systems to traditional web search, analyzing biases and user attitudes influenced by LLM opinion alignment.</p>
<p>Table 1 .
1
Summary of evaluation dimensions, benchmarks, and metrics for Compound AI Systems.
DimensionTaskDatasets/BenchmarksEvaluation Metrics
Manuscript submitted to ACM</p>
<p>LLM Based Generation of Item-Description for Recommendation System. Arkadeep Acharya, Brijraj Singh, Naoyuki Onoe, 10.1145/3604915.3610647Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender SystemsSingapore, Singapore; New York, NY, USAAssociation for Computing Machinery2023RecSys '23</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 352022. 2022</p>
<p>Anthropic, Model Context Protocol. 2024</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, arXiv:2310.115112023. 2023arXiv preprint</p>
<p>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, arXiv:2306.041362023. 2023arXiv preprint</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew Mcnamara, Bhaskar Mitra, Tri Nguyen, arXiv:1611.09268Ms marco: A human generated machine reading comprehension dataset. 2016. 2016arXiv preprint</p>
<p>AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. Rachel Ke Bellamy, Kuntal Dey, Michael Hind, Stephanie Samuel C Hoffman, Kalapriya Houde, Pranay Kannan, Jacquelyn Lohia, Sameep Martino, Aleksandra Mehta, Mojsilovi, IBM Journal of Research and Development. 632019. 4/5 (2019</p>
<p>Semantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Is space-time attention all you need for video understanding?. Gedas Bertasius, Heng Wang, Lorenzo Torresani, In ICML. 242021</p>
<p>How to Chunk Documents for Retrieval-Augmented Generation (RAG. Multimodal Blog, 2024</p>
<p>Generative AI to Become a $1.3 Trillion Market by 2032. 2023Bloomberg Intelligence. Research Finds</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Honeybee: Locality-enhanced projector for multimodal llm. Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023. 2023arXiv preprint</p>
<p>Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, arXiv:2402.11684Xiang Wan, and Benyou Wang. 2024. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. 2024arXiv preprint</p>
<p>Sharegpt4v: Improving large multi-modal models with better captions. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin, European Conference on Computer Vision. Springer2025</p>
<p>Wavlm: Large-scale self-supervised pre-training for full stack speech processing. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, IEEE Journal of Selected Topics in Signal Processing. 162022. 2022</p>
<p>Re-imagen: Retrieval-augmented text-to-image generator. Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W Cohen, arXiv:2209.144912022. 2022arXiv preprintManuscript submitted to ACM Chen et al.</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.108482023. 202326arXiv preprint</p>
<p>Decoupling knowledge from memorization: Retrieval-augmented prompt learning. Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Lift yourself up: Retrieval-augmented text generation with self-memory. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>TREC deep learning track: Reusable test collections in the large data regime. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees, Ian Soboroff, Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>The power of noise: Redefining retrieval for rag systems. Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Llmind: Orchestrating ai and iot with llm for complex task execution. Hongwei Cui, Yuyang Du, Qun Yang, Yulin Shao, Soung Chang Liew, IEEE Communications Magazine. 2024. 2024</p>
<p>Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith B Guu, Ming-Wei Hall, Chang, arXiv:2209.11755Promptagator: Few-shot dense retrieval from 8 examples. 2022. 2022arXiv preprint</p>
<p>Llm adaptive pid control for b5g truck platooning systems. Irene De Zarz, Joachim De Curt, Gemma Roig, Carlos T Calafate, Sensors. 2358992023. 2023</p>
<p>Haystack GitHub Repository. Deepset, Team, 2024</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Introduction to Indexing in Retrieval-Augmented Generation (RAG. 2024DataStax Documentation</p>
<p>AISBench: an performance benchmark for AI server systems. Jian Dong, Wei Bao, Xiaoqi Cao, Yang Xu, Yuze Yang, Binbin Li, Qi Zhang, Heng Ye, The Journal of Supercomputing. 812025. 2025</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, arXiv:2010.119292020. 2020arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023. 2023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning. 2023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.001612019. 2019arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024. 2024arXiv preprint</p>
<p>Ragas: Automated evaluation of retrieval augmented generation. Shahul Es, Jithin James, Luis Espinosa Anke, Steven Schockaert, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European Chapterthe Association for Computational Linguistics: System Demonstrations2024</p>
<p>Jonathan Evertz, Merlin Chlosta, Lea Schnherr, Thorsten Eisenhofer, arXiv:2402.06922Whispers in the Machine: Confidentiality in LLM-integrated Systems. 2024. 2024arXiv preprint</p>
<p>Irene Alexander R Fabbri, Tianwei Li, Suyi She, Li, Dragomir R Radev, arXiv:1906.01749Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. 2019. 2019arXiv preprint</p>
<p>Minedojo: Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah, arXiv:2504.19678From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review. 2025. 2025arXiv preprint</p>
<p>Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, arXiv:1802.08254Bigdatabench: A scalable and unified big data and ai benchmark suite. 2018. 2018arXiv preprint</p>
<p>Iseeq: Information seeking question generation using dynamic metainformation retrieval and knowledge graphs. Manas Gaur, Kalpa Gunaratna, Vijay Srinivasan, Hongxia Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Planting a seed of vision in large language model. Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan, arXiv:2307.080412023. 2023arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Agentquest: A modular benchmark framework to measure progress and improve llm agents. Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence, arXiv:2404.064112024. 2024arXiv preprint</p>
<p>Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, and Alfio Gliozzo. Michael Glass, Gaetano Rossiello, arXiv:2207.06300Re2G: Retrieve, rerank, generate. 2022. 2022arXiv preprint</p>
<p>Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, arXiv:2305.04790Multimodal-gpt: A vision and language model for dialogue with humans. 2023. 2023arXiv preprint</p>
<p>Retrieval-augmented reinforcement learning. Anirudh Goyal, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, International Conference on Machine Learning. PMLR2022</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024. 2024arXiv preprint</p>
<p>Wikiasp: A dataset for multi-domain aspect-based summarization. Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, Graham Neubig, Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, arXiv:2011.01060Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. 2020. 2020arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Metagpt: Meta programming for multi-agent collaborative framework. 2023. 202336arXiv preprint</p>
<p>Bliva: A simple multimodal llm for better handling of text-rich visual questions. Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, arXiv:2401.05507Infiagent-dabench: Evaluating agents on data analysis tasks. 2024. 2024arXiv preprint</p>
<p>Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory. Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, Alireza Fathi, 10.1109/CVPR52729.2023.022382023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, ACM Transactions on Information Systems. 2023. 2023</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022. 2022arXiv preprint</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, arXiv:2007.012822020. 2020arXiv preprint</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, Journal of Machine Learning Research. 242023. 2023</p>
<p>Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2305.06983Active retrieval augmented generation. 2023. 2023arXiv preprint</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017. 2017arXiv preprint</p>
<p>Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Realtime qa: What's the answer right now?. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, Kentaro Inui, Advances in neural information processing systems. 362023. 2023</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022. 2022arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Ocr-free document understanding transformer. Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park, European Conference on Computer Vision. Springer2022</p>
<p>An introduction to variational autoencoders. Max Diederik P Kingma, Welling, Foundations and Trends in Machine Learning. 122019. 2019</p>
<p>The narrativeqa reading comprehension challenge. Tom Koisk, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gbor Melis, Edward Grefenstette, Transactions of the Association for Computational Linguistics. 62018. 2018</p>
<p>Generating images with multimodal language models. Jing Yu Koh, Daniel Fried, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 362024. 2024Manuscript submitted to ACM</p>
<p>Grounding language models to images for multimodal inputs and outputs. Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, International Conference on Machine Learning. PMLR2023</p>
<p>Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura, arXiv:2402.04636Transllama: Llm-based simultaneous translation system. 2024. 2024arXiv preprint</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019. 2019</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Internet-augmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022. 2022arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, K " Heinrich, Mike Uttler, Wen-Tau Lewis, Tim Yih, Rockt "aschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.16125Seed-bench: Benchmarking multimodal llms with generative comprehension. 2023. 2023arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>A Review of Prominent Paradigms for LLM-Based Agents: Tool Use, Planning (Including RAG), and Feedback Learning. Xinzhe Li, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational Linguistics2025</p>
<p>Multibench: Multiscale benchmarks for multimodal representation learning. Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A Lee, Yuke Zhu, Advances in neural information processing systems 2021. 2021. 202111</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, arXiv:2305.191182023. 2023arXiv preprint</p>
<p>A Survey of Multimodel Large Language Models. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, Ke Liu, Proceedings of the 3rd International Conference on Computer. the 3rd International Conference on Computer2024</p>
<p>Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache. Bin Lin, Chen Zhang, Tao Peng, Hanyu Zhao, Wencong Xiao, Minmin Sun, Anmin Liu, Zhipeng Zhang, Lanbo Li, Xiafei Qiu, arXiv:2401.026692024. 2024arXiv preprint</p>
<p>Unsupervised cross-task generalization via retrieval augmentation. Kangmin Bill Yuchen Lin, Chris Tan, Beiwen Miller, Xiang Tian, Ren, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu, arXiv:2410.16392LLM-based Optimization of Compound AI Systems: A Survey. 2024. 2024arXiv preprint</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.07958Truthfulqa: Measuring how models mimic human falsehoods. 2021. 2021arXiv preprint</p>
<p>Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-Tau Yih, Xilun Chen, arXiv:2302.07452How to train your dragon: Diverse augmentation towards generalizable dense retrieval. 2023. 2023arXiv preprint</p>
<p>Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, arXiv:2310.01352Ra-dit: Retrieval-augmented dual instruction tuning. 2023. 2023arXiv preprint</p>
<p>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang, arXiv:2402.14809Criticbench: Benchmarking llms for critique-correct reasoning. 2024. 2024arXiv preprint</p>
<p>Llm-slice: Dedicated wireless network slicing for large language models. Boyi Liu, Jingwen Tong, Proceedings of the 22nd ACM Conference on Embedded Networked Sensor Systems. the 22nd ACM Conference on Embedded Networked Sensor SystemsJun Zhang. 2024</p>
<p>Chunwei Liu, Matthew Russo, Michael Cafarella, Lei Cao, Zui Peter Baille Chen, Michael Chen, Tim Franklin, Samuel Kraska, Gerardo Madden, Vitagliano, arXiv:2405.14696A Declarative System for Optimizing AI Workloads. 2024. 2024arXiv preprint</p>
<p>Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, Dong Yu, arXiv:2311.10774Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. 2023. 2023arXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 362024. 2024</p>
<p>Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian, arXiv:2310.15556Tcra-llm: Token compression retrieval augmented large language model for inference cost reduction. 2023. 2023arXiv preprint</p>
<p>WebGLM: Towards an efficient web-enhanced question answering system with human preferences. Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, Jie Tang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023. 2023arXiv preprint</p>
<p>Mm-safetybench: A benchmark for safety evaluation of multimodal large language models. Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, Yu Qiao, European Conference on Computer Vision. Springer2024</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, arXiv:2308.05960Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. 2023. 2023arXiv preprint</p>
<p>Retrieval Augmented Classification for Long-Tail Visual Recognition. Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Pulak Vu Nguyen, Ravi Purkait, Alan Garg, Chunhua Blair, Anton Shen, Van Den, Hengel, 10.1109/CVPR52688.2022.006832022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Seung-won Hwang, and Alexey Svyatkovskiy. Shuai Lu, Nan Duan, Hojae Han, Daya Guo, arXiv:2203.07722Reacc: A retrieval-augmented code completion framework. 2022. 2022arXiv preprint</p>
<p>Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, James Glass, arXiv:2305.15225Sail: Search-augmented instruction learning. 2023. 2023arXiv preprint</p>
<p>Llm-mars: Large language model for behavior tree generation and nlp-enhanced dialogue in multi-agent robot systems. Artem Lykov, Maria Dronova, Nikolay Naglov, Mikhail Litvinov, Sergei Satsevich, Artem Bazhenov, Vladimir Berman, Aleksei Shcherbak, Dzmitry Tsetserukou, arXiv:2312.093482023. 2023arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.131782024. 2024arXiv preprint</p>
<p>InsightPilot: An LLM-empowered automated data exploration system. Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, Dongmei Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2023</p>
<p>Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, Xuanjing Huang, arXiv:2402.02101Are large language models good prompt optimizers?. 2024. 2024arXiv preprint</p>
<p>Query rewriting for retrieval-augmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, arXiv:2305.142832023. 2023arXiv preprint</p>
<p>Fine-tuning llama for multi-stage text retrieval. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Zhu, arXiv:2310.12931Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large language models. 2023arXiv preprint</p>
<p>End-to-end Text-to-SQL Generation within an Analytics Insight Engine. Karime Maamari, Amine Mhedhbi, arXiv:2406.121042024. 2024arXiv preprint</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105112022. 2022arXiv preprint</p>
<p>Ahmed Masry, Xuan Do, Jia Long, Shafiq Qing Tan, Enamul Joty, Hoque, arXiv:2203.10244Chartqa: A benchmark for question answering about charts with visual and logical reasoning. 2022. 2022arXiv preprint</p>
<p>Docvqa: A dataset for vqa on document images. Minesh Mathew, Dimosthenis Karatzas, Jawahar, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2021</p>
<p>MM1: methods, analysis and insights from multimodal LLM pre-training. Brandon Mckinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, European Conference on Computer Vision. Springer2024</p>
<p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang, arXiv:2403.16971Llm agent operating system. 2024. 2024arXiv preprint</p>
<p>Handwritten optical character recognition (OCR): A comprehensive systematic literature review (SLR). Jamshed Memon, Maira Sami, Rizwan Ahmed Khan, Mueen Uddin, IEEE access. 82020. 2020</p>
<p>Teaching language models to support answers with verified quotes. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, Nat Mcaleese, arXiv:2203.11147[cs.CL2022</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.027892018. 2018arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, arXiv:2112.09332[cs.CLWebGPT: Browser-assisted question-answering with human feedback. 2022</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023. 2023arXiv preprintManuscript submitted to ACM</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, arXiv:2302.128132023. 2023arXiv preprint</p>
<p>Sida Peng, Eirini Kalliamvakou, Peter Cihon, Mert Demirer, arXiv:2302.06590The impact of ai on developer productivity: Evidence from github copilot. 2023. 2023arXiv preprint</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.148242023. 2023arXiv preprint</p>
<p>A I Perplexity, Perplexity AI. 2025</p>
<p>KILT: a benchmark for knowledge intensive language tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, arXiv:2009.022522020. 2020arXiv preprint</p>
<p>Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen, arXiv:2401.10061Diffusiongpt: LLM-driven text-to-image generation system. 2024. 2024arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023. 2023arXiv preprint</p>
<p>Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, arXiv:2405.17935Tool Learning with Large Language Models: A Survey. 2024. 2024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, International conference on machine learning. PMLR2023</p>
<p>use-of-ai-to-analyze-chest-ct-shortens-turnaround-times-in-russia. Radlogics, Use of AI to Analyze Chest CT Shortens Turnaround Times in Russia. 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 212020. 2020</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250Squad: 100,000+ questions for machine comprehension of text. 2016. 2016arXiv preprint</p>
<p>In-context retrievalaugmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, Transactions of the Association for Computational Linguistics. 112023. 2023</p>
<p>Smallcap: Lightweight Image Captioning Prompted with Retrieval Augmentation. Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva, 10.1109/CVPR52729.2023.002782023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Glamm: Pixel grounding large multimodal model. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Eric Rao M Anwer, Ming-Hsuan Xing, Fahad S Yang, Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, arXiv:2205.06175A generalist agent. 2022. 2022arXiv preprint</p>
<p>Investigating the factual knowledge boundary of large language models with retrieval augmentation. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Hao Tian, Ji-Rong Wu, Haifeng Wen, Wang, arXiv:2307.110192023. 2023arXiv preprint</p>
<p>Timechat: A time-sensitive multimodal large language model for long video understanding. Linli Shuhuai Ren, Shicheng Yao, Xu Li, Lu Sun, Hou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Sherry Ruan, Tian Zhao, arXiv:2407.00038JungleGPT: Designing and Optimizing Compound AI Systems for E-Commerce. 2024. 2024arXiv preprint</p>
<p>Improving passage retrieval with zero-shot question generation. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-Tau Yih, Joelle Pineau, Luke Zettlemoyer, arXiv:2204.074962022. 2022arXiv preprint</p>
<p>Explaining legal concepts with augmented large language models. Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, Huihui Xu, arXiv:2306.095252023. 2023arXiv preprintgpt-4</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>ZeroSCROLLS: A zero-shot benchmark for long text understanding. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, Omer Levy, arXiv:2305.141962023. 2023arXiv preprint</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2305.152942023. 2023arXiv preprint</p>
<p>Generative Echo Chamber? Effect of LLM-Powered Search Systems on Diverse Information Seeking. Nikhil Sharma, Ziang Vera Liao, Xiao, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Zhuocheng Shen, arXiv:2409.18807Llm with tools: A survey. 2024. 2024arXiv preprint</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Towards vqa models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, Suranga Nanayakkara, Transactions of the Association for Computational Linguistics. 112023. 2023</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Cognitive architectures for language agents. Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas Griffiths, Transactions on Machine Learning Research. 2023. 2023</p>
<p>Generative multimodal models are in-context learners. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang, arXiv:2307.05222Generative pretraining in multimodality. 2023. 2023arXiv preprint</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, arXiv:2304.09542Is ChatGPT good at search? investigating large language models as re-ranking agents. 2023. 2023arXiv preprint</p>
<p>Aligning large multimodal models with factually augmented rlhf. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, arXiv:2309.145252023. 2023arXiv preprint</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Yashar Talebirad, Amirhossein Nadiri, arXiv:2306.033142023. 2023arXiv preprint</p>
<p>Large language model (llm) as a system of multiple expert agents: An approach to solve the abstraction and reasoning corpus (arc) challenge. John Chong, Min Tan, Mehul Motani, arXiv:2310.051462023. 2023arXiv preprint</p>
<p>Salmonn: Towards generic hearing abilities for large language models. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang, arXiv:2310.132892023. 2023arXiv preprint</p>
<p>AIBench training: Balanced industry-standard AI training benchmarking. Fei Tang, Wanling Gao, Jianfeng Zhan, Chuanxin Lan, Xu Wen, Lei Wang, Chunjie Luo, Zheng Cao, Xingwang Xiong, Zihan Jiang, 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE2021</p>
<p>Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, arXiv:2311.09835ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code. 2023. 2023arXiv preprint</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler, arXiv:2011.04006Long range arena: A benchmark for efficient transformers. 2020. 2020arXiv preprint</p>
<p>Transformer memory as a differentiable search index. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023. 2023arXiv preprint</p>
<p>A I Langchain, Team, LangChain GitHub Repository. 2024</p>
<p>Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, Iryna Gurevych, arXiv:2104.086632021. 2021arXiv preprint</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, arXiv:1803.053552018. 2018arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092022. 2022arXiv preprint</p>
<p>MuSiQue: Multihop Questions via Single-hop Question Composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language Model. Yunda Tsai, Mingjie Liu, Haoxing Ren, Proceedings of the 61st ACM/IEEE Design Automation Conference. the 61st ACM/IEEE Design Automation Conference2024</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, 2017. 201730Attention is all you need. Manuscript submitted to ACM</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023. 2023arXiv preprint</p>
<p>Augmenting black-box llms with medical textbooks for clinical question answering. Yubo Wang, Xueguang Ma, Wenhu Chen, arXiv:2309.022332023. 2023arXiv preprint</p>
<p>Llm4msr: An llm-enhanced paradigm for multi-scenario recommendation. Yuhao Wang, Yichao Wang, Zichuan Fu, Xiangyang Li, Wanyu Wang, Yuyang Ye, Xiangyu Zhao, Huifeng Guo, Ruiming Tang, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Learning to filter context for retrieval-augmented generation. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig, arXiv:2311.083772023. 2023arXiv preprint</p>
<p>Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig, arXiv:2403.15452What are tools anyway? a survey from the language model perspective. 2024. 2024arXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023. 2023arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021. 2021arXiv preprint</p>
<p>Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao, arXiv:2309.16292Dilu: A knowledge-driven approach to autonomous driving with large language models. 2023. 2023arXiv preprint</p>
<p>Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering. Zhihua Wen, Zhiliang Tian, Zexin Jian, Zhen Huang, Pei Ke, Yifu Gao, Minlie Huang, Dongsheng Li, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Retrieval-augmented generation. December-2024Wikipedia contributors. n.d18</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, arXiv:2308.081552023. 2023arXiv preprint</p>
<p>Next-gpt: Any-to-any multimodal llm. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, arXiv:2309.055192023. 2023arXiv preprint</p>
<p>SecGPT: An execution isolation architecture for llm-based systems. Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal, arXiv:2403.049602024. 2024arXiv preprint</p>
<p>An efficient memory-augmented transformer for knowledge-intensive nlp tasks. Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, arXiv:2210.167732022. 2022arXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023. 2023arXiv preprint</p>
<p>Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, arXiv:2310.044082023. 2023arXiv preprint</p>
<p>OSAgent: Copiloting Operating System with LLM-based Agent. Jiaming Xu, Kaibin Guo, Wuxuan Gong, Runyu Shi, 2024 International Joint Conference on Neural Networks (IJCNN). IEEE2024</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, arXiv:2305.165042023. 2023arXiv preprint</p>
<p>Beyond self-talk: A communicationcentric survey of llm-based multi-agent systems. Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, Chaozhuo Li, arXiv:2502.143212025. 2025arXiv preprint</p>
<p>Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie, Tianjun Ji, Zhang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, Berkeley Function Calling Leaderboard. 2024</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, Animashree Anandkumar, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Gpt4tools: Teaching large language model to use tools via self-instruction. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.096002018. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in neural information processing systems. 362023. 2023</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2211.12561Retrieval-augmented multimodal language modeling. 2022. 2022arXiv preprint</p>
<p>Woodpecker: Hallucination correction for multimodal large language models. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen, Science China Information Sciences. 672201052024. 2024</p>
<p>Making retrieval-augmented language models robust to irrelevant context. Tomer Ori Yoran, Ori Wolfson, Jonathan Ram, Berant, arXiv:2310.015582023. 2023arXiv preprint</p>
<p>Automated assertion generation via information retrieval and its integration with deep learning. Hao Yu, Yiling Lou, Ke Sun, Dezhi Ran, Tao Xie, Dan Hao, Ying Li, Ge Li, Qianxiang Wang, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023. 2023arXiv preprint</p>
<p>Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu, arXiv:2311.09210Chain-of-note: Enhancing robustness in retrievalaugmented language models. 2023. 2023arXiv preprint</p>
<p>Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jaesung Park, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Le Ronan, Gunhee Bras, Kim, arXiv:2205.12630Multimodal knowledge alignment with reinforcement learning. 2022. 2022arXiv preprint</p>
<p>Augmentation-adapted retriever improves generalization of language models as generic plug-in. Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu, arXiv:2305.173312023. 2023arXiv preprint</p>
<p>Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared , Heather Miller, Chris Potts, James Zou, Micha , Jonathan Frankle, Naveen Rao, Ali Ghodsi, The Shift from Models to Compound AI Systems. 2024</p>
<p>Contextual object detection with multimodal large language models. Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy, International Journal of Computer Vision. 2024. 2024</p>
<p>Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.00598Socratic models: Composing zero-shot multimodal reasoning with language. 2022. 2022arXiv preprint</p>
<p>Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, arXiv:2303.12570Repocoder: Repository-level code completion through iterative retrieval and generation. 2023. 2023arXiv preprint</p>
<p>Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo, arXiv:2307.03601Gpt4roi: Instruction tuning large language model on region-of-interest. 2023. 2023arXiv preprint</p>
<p>Tianjun Zhang, G Shishir, Naman Patil, Sheng Jain, Matei Shen, Ion Zaharia, Joseph E Stoica, Gonzalez, arXiv:2403.10131Raft: Adapting language model to domain specific rag. 2024. 2024arXiv preprint</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.009232023. 2023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 19632-1964238</p>
<p>Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.071972022. 2022arXiv preprint</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023. 2023arXiv preprint</p>
<p>Bridging the gap between indexing and retrieval for differentiable search index with query generation. Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, Daxin Jiang, arXiv:2206.101282022. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
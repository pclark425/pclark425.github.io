<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9045 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9045</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9045</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-266741775</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.01519v3.pdf" target="_blank">Exploring the frontiers of LLMs in psychological applications: a comprehensive review</a></p>
                <p><strong>Paper Abstract:</strong> This review explores the frontiers of large language models (LLMs) in psychological applications. Psychology has undergone several theoretical changes, and the current use of artificial intelligence (AI) and machine learning, particularly LLMs, promises to open up new research directions. We aim to provide a detailed exploration of how LLMs are transforming psychological research. We discuss the impact of LLMs across various branches of psychology—including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology—highlighting their ability to model patterns, cognition, and behavior similar to those observed in humans. Furthermore, we explore the ability of such models to generate coherent, contextually relevant text, offering innovative tools for literature reviews, hypothesis generation, experimental designs, experimental subjects, and data analysis in psychology. We emphasize the importance of addressing technical and ethical challenges, including data privacy, the ethics of using LLMs in psychological research, and the need for a deeper understanding of these models’ limitations. Researchers should use LLMs responsibly in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, this review provides a comprehensive overview of the current state of LLMs in psychology, exploring the potential benefits and challenges. We hope it can serve as a call to action for researchers to responsibly leverage LLMs’ advantages while addressing the associated risks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9045.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9045.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hagendorff Deception/CRT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hagendorff et al. — Deception abilities and cognitive reflection testing of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental evaluation of modern LLMs on deception tasks (simple and complex) and cognitive-reflection / semantic-illusion tests to probe System 1 vs System 2–like behavior and emergent deceptive capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deception abilities emerged in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and earlier LLMs / ChatGPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLMs (GPT-family), varying by model size and alignment; tested for reasoning/chain-of-thought and behavior in deception scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Deception scenarios (simple and complex), Cognitive Reflection Test (CRT), semantic illusion tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Deception tasks: induced false beliefs and Machiavellian behavior in simple vs complex scenarios; CRT and semantic illusion tasks probe susceptibility to intuitive but wrong answers (System 1) vs reflective reasoning (System 2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Deception: GPT-4 succeeded in 99.16% of simple scenarios and 71.46% of complex scenarios; CRT/semantic-illusion: as model size and capability increased, models more often produced human-like intuitive (System 1) errors (no single numeric accuracy reported for CRT in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported quantitatively in review for these exact task variants; CRT/semantic-illusion comparisons discussed qualitatively (human normative patterns invoked).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On deception tasks GPT-4 shows very high success (near-perfect on simple, high but lower on complex); on CRT-style tasks, larger models show human-like intuitive errors — i.e., they replicate some human error patterns rather than clearly outperforming humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors manipulated ability to use chain-of-thought reasoning (e.g., preventing chain-of-thought) to probe differences; tasks included both simple and complex deception scenarios and semantic-illusion items; comparisons drawn across model sizes and alignment variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Review notes that emergence of deceptive capacities does not imply intent or consciousness; performance depends on prompt framing and on whether chain-of-thought reasoning is enabled; caveats about equating LLM 'intuitive' errors with human cognition are emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9045.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kosinski ToM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kosinski — Evaluating large language models in theory of mind tasks (false-belief tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic evaluation of multiple LLMs on false-belief (Theory of Mind) tasks to compare developmental-like performance to human children.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models in theory of mind tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs (GPT family) tested across many false-belief task variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>40 false-belief tasks (Theory of Mind battery)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>False-belief tasks probe the ability to attribute others' beliefs that diverge from reality — canonical test for ToM in developmental psychology.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 achieved 75% accuracy on the 40 false-belief tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Review states GPT-4 performance was 'comparable to the performance of a six-year-old child' (no exact numeric human baseline provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 approximates young child (around 6-year-old) performance on false-belief tasks (i.e., below adult normative ToM performance).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>40 false-belief tasks required success in multiple related scenarios per task; comparison made across model versions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The review highlights developmental analogies but cautions that matching behavioral answers on ToM tasks does not imply human-like mental state representations; task framing and prompt wording strongly affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9045.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Marjieh Perceptual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Marjieh et al. — LLMs predict human sensory judgments across modalities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3, GPT-3.5, and GPT-4 on multiple psychophysical datasets and multilingual color-naming tasks to test whether LLMs recover perceptual structure from language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of transformer LLMs differing in scale and training data; assessed for alignment between language-derived representations and human perceptual judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Six psychophysical datasets + multilingual color-naming task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Psychophysical datasets probe perceptual judgments (e.g., color, possibly other sensory modalities); color-naming task assesses cross-linguistic perceptual variation and recovery of perceptual representations (e.g., color wheel).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 'aligns closely with human perceptual data' and recovers representations such as the color wheel (no single numeric accuracy given in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human perceptual datasets used as ground truth; exact human metrics not restated in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 shows close alignment to human perceptual judgments (i.e., strong correspondence), better than earlier GPT variants according to review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multilingual prompts and psychophysical stimulus descriptions were used; outputs compared to human datasets and cross-linguistic patterns examined.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Recovery of perceptual structure from text does not imply sensory experience; alignment depends on distributional signals present in language corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9045.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loconte Neuropsych</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Loconte et al. — Neuropsychological investigation of prefrontal functioning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of LLMs (GPT-3.5, GPT-4 and others) on neuropsychological tests tapping planning, semantic processing, and Theory of Mind to probe prefrontal-like functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Challenging ChatGPT's "intelligence" with human tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4 (also Claude2, Llama2 discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLMs of varying alignment and update status; tested as if they were subjects on neuropsychological tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Neuropsychological tests targeting prefrontal functions (planning, semantic understanding, Theory of Mind)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks designed to probe executive functions associated with prefrontal cortex, semantic comprehension and ToM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 'generally meets normative human standards' on many tasks; Claude2 and Llama2 showed variable and often limited abilities (no specific accuracies reported in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Normative human standards referenced (i.e., clinical/neuropsych norms) but no specific numeric baselines provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 often approaches normative human performance on these neuropsychological measures; other models perform worse and inconsistently.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tests adapted from human neuropsychological batteries; authors compared model outputs against normative responses and clinical expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Review emphasizes difficulty equating model outputs to human neurological function; passing some tests may reflect pattern-matching rather than genuine executive function.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9045.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dhingra GPT-4 Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dhingra et al. — GPT-4's performance on cognitive-psychology-related benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 on several established NLP/problem-solving benchmarks (CommonsenseQA, SuperGLUE, MATH, HANS) to assess integration of cognitive processes and context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mind meets machine: Unravelling GPT-4's cognitive psychology.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal/advanced GPT-family model with enhanced reasoning and instruction-following capabilities; trained on broad web-scale corpora and fine-tuned for instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CommonsenseQA, SuperGLUE, MATH, HANS (benchmark datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmarks probe commonsense reasoning, general language understanding, formal math problem solving, and heuristics/entailment (HANS tests).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 demonstrates 'high accuracy' on these cognitive-psychology-relevant benchmarks and surpasses prior models (no single numeric scores provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Benchmarks have human baselines in original tasks but explicit human baseline numbers are not restated in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 surpasses prior state-of-the-art models on these benchmarks and shows significant reasoning capability, though not equated directly to human-level across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used standard benchmark formulations; analyses considered how GPT-4 integrates context and cognitive process proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Benchmarks are not direct cognitive-psychology experiments; strong benchmark performance does not guarantee human-like cognitive processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9045.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binz & Schulz Bandit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binz & Schulz — Using cognitive-psychology tools to probe GPT-3 (including multiarmed bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigations where GPT-3 was tested on canonical cognitive experiments (decision-making, information search, causal reasoning) including the multiarmed bandit to compare model decision strategies with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer language model (GPT-3) tested on cognitive task analogs; few-shot or instruction prompts used to operationalize tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3 is commonly 175B parameters) - not explicitly stated in review</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Canonical cognitive psychology experiments (including multiarmed bandit task)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Decision-making and reinforcement-learning analogs (multiarmed bandit) probe exploration-exploitation and rational choice behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 'made better decisions than humans' and outperformed humans on the multiarmed bandit task (no numeric accuracy provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants' performance referenced qualitatively (GPT-3 outperforming humans) but no numeric baseline provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 outperformed humans on the reported multiarmed bandit experiments; however, struggled with task perturbations, directed exploration, and causal reasoning in other paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors applied canonical cognitive paradigms to prompted LLMs, sometimes fine-tuning on multiple tasks to improve prediction of human behavior in unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Results depend on how tasks are framed/prompts; LLMs may exploit distributional regularities instead of implementing the same cognitive mechanisms as humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9045.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stevenson AUT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stevenson et al. — GPT-3 evaluated on Guilford's Alternative Uses Test (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of GPT-3's creative generation on the AUT with expert ratings and semantic-distance analyses compared against human data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Putting GPT-3's Creativity to the (Alternative Uses) Test.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM evaluated for generative creativity on divergent-thinking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3 typical, not restated explicitly in review)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Guilford's Alternative Uses Test (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Measures divergent thinking/creativity by asking participants to list unusual uses for a common object; scored on originality, flexibility, fluency, and elaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 produced responses evaluated for originality, usefulness, surprise, and flexibility; review states humans currently outperform GPT-3 in creativity (no numeric scores provided).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants scored higher on creativity measures overall (no numerical baseline provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline on creativity measures, though shows potential and may close gap with future models/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used expert human raters and semantic-distance metrics to quantify originality and related dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Evaluation of creativity involves subjective judgments; LLMs may produce superficially novel combinations that lack human-like conceptual depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9045.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sap SocialIQa/ToMi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sap et al. — Evaluation of social intelligence / Theory of Mind datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLMs on SocialIQa (social intents/reactions) and ToMi (Theory of Mind inference) showing LLMs perform below human levels on these social-cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs assessed on social-intelligence benchmarks probing intent understanding and mental-state inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SocialIQa and ToMi</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>SocialIQa: questions about social intents and reactions; ToMi: tasks requiring inference about mental states and different realities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLMs including GPT-4 scored ~55% on SocialIQa and ~60% on ToMi according to review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Review states these model scores are 'below human levels' (exact human percentages not restated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs perform substantially below human baseline on these social-intelligence / ToM benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Benchmarks applied in standard format; interpretation contextualized via pragmatics theories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Scaling alone does not yield human-level ToM/social intelligence; performance limited by lack of person-centric training and interactive social grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9045.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schaaff Emotion/Empathy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schaaff et al. — Evaluation of ChatGPT's empathic abilities and emotion recognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessed GPT (ChatGPT variants) on emotion recognition tasks, conversational empathy measures, and multiple empathy questionnaires, comparing to human norms and clinical groups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring ChatGPT's Empathic Abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (unnumbered in summary) / GPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLMs evaluated for affective understanding and empathic response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Emotion recognition tasks, conversational analysis, five empathy-related questionnaires</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Emotion identification accuracy tasks and standardized empathy questionnaires (comparisons to normative human samples and clinical groups).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Emotion identification: 91.7% accuracy; 'parallel emotions' in 70.7% of cases; empathy questionnaire scores below average human but above individuals with Asperger syndrome (no further numeric detail in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average human empathy scores (not numerically restated); clinical comparison group (Asperger syndrome) used as lower-bound reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM shows high emotion-recognition accuracy but overall empathy scores below typical human averages while above certain clinical groups.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Combination of direct emotion-label tasks and conversational evaluations; multiple standardized empathy scales administered to model via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High accuracy in labeling emotions does not equal genuine empathic understanding; questionnaire-style probing of models relies on prompt formulation and cannot capture affective experience.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9045.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vzorinab / Patel X. Wang EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vzorinab et al. / Patel & Fan / X. Wang et al. — Emotional intelligence and emotion understanding benchmarks for GPT-4 and other LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple psychometric-style assessments of emotional intelligence and emotion understanding applied to GPT-4 and other LLMs, benchmarking vs hundreds of human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Emotional Intelligence of the GPT-4 Large Language Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (compared to Bard, GPT-3.5, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned LLM with advanced text understanding and generation capabilities; evaluated with psychometric emotion-understanding instruments (e.g., MSCEIT-style tasks, TAS-20, EQ-60 proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MSCEIT-like emotional intelligence tasks / TAS-20 / EQ-60 / custom psychometric Emotion Understanding assessment</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks probe emotion perception, understanding, management and reflective/empathic reasoning — domains of emotional intelligence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 'approached human-level emotional intelligence' and in one psychometric assessment 'scored higher than 89% of humans'; GPT-4 outperformed Bard and GPT-3.5 which showed alexithymic tendencies; MSCEIT evaluation indicated GPT-4 strong on understanding/managing but limited on reflective analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Benchmarked against >500 human participants in one assessment (exact human score distributions not restated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 at or above large majority of human participants on some emotion-understanding measures (e.g., >89th percentile) but shows qualitative differences in reflective reasoning and lacks human-like reflective emotional processing.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Text-based prompting to elicit model responses on standardized emotional scenarios and psychometric items; comparisons against human normative datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High psychometric scores reflect pattern-matching on text; reflective, introspective, and affective aspects differ from human emotional processing; variation across tests and instruments noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9045.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9045.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D'Souza Clinical Vignettes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D'Souza et al. — Appraising ChatGPT in psychiatry using clinical case vignettes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of ChatGPT-3.5 on 100 clinical psychiatric vignettes graded by expert psychiatrists across categories (diagnosis, management, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 evaluated for clinical reasoning on psychiatric case vignettes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>100 clinical psychiatric vignettes (graded across 10 categories)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Clinical case scenarios assessing diagnostic reasoning, management recommendations, and clinical judgement domains.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ChatGPT-3.5 earned 'top grades' in 61% of cases (no further numeric breakdown provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Expert psychiatrists used as gold-standard graders; exact human performance baseline numbers not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGPT-3.5 performed strongly (top grades in majority of cases) but showed minor discrepancies in more complex scenarios; not considered a replacement for clinicians.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Expert psychiatrists graded model outputs across multiple categories for each vignette; models compared qualitatively to professional standards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Vignette-based evaluation restricts complexity and multimodal cues present in clinical practice; model may be overly pessimistic or optimistic depending on variant (other studies noted GPT-3.5 pessimism vs GPT-4/Bard/Claude alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the frontiers of LLMs in psychological applications: a comprehensive review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Turning large language models into cognitive models. <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand GPT-3. <em>(Rating: 2)</em></li>
                <li>Deception abilities emerged in large language models. <em>(Rating: 2)</em></li>
                <li>Evaluating large language models in theory of mind tasks. <em>(Rating: 2)</em></li>
                <li>Large language models predict human sensory judgments across six modalities. <em>(Rating: 2)</em></li>
                <li>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. <em>(Rating: 2)</em></li>
                <li>Mind meets machine: Unravelling GPT-4's cognitive psychology. <em>(Rating: 2)</em></li>
                <li>Putting GPT-3's Creativity to the (Alternative Uses) Test. <em>(Rating: 1)</em></li>
                <li>Exploring ChatGPT's Empathic Abilities. <em>(Rating: 2)</em></li>
                <li>Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9045",
    "paper_id": "paper-266741775",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Hagendorff Deception/CRT",
            "name_full": "Hagendorff et al. — Deception abilities and cognitive reflection testing of LLMs",
            "brief_description": "Experimental evaluation of modern LLMs on deception tasks (simple and complex) and cognitive-reflection / semantic-illusion tests to probe System 1 vs System 2–like behavior and emergent deceptive capabilities.",
            "citation_title": "Deception abilities emerged in large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (and earlier LLMs / ChatGPT variants)",
            "model_description": "Large transformer-based LLMs (GPT-family), varying by model size and alignment; tested for reasoning/chain-of-thought and behavior in deception scenarios.",
            "model_size": null,
            "test_battery_name": "Deception scenarios (simple and complex), Cognitive Reflection Test (CRT), semantic illusion tasks",
            "test_description": "Deception tasks: induced false beliefs and Machiavellian behavior in simple vs complex scenarios; CRT and semantic illusion tasks probe susceptibility to intuitive but wrong answers (System 1) vs reflective reasoning (System 2).",
            "llm_performance": "Deception: GPT-4 succeeded in 99.16% of simple scenarios and 71.46% of complex scenarios; CRT/semantic-illusion: as model size and capability increased, models more often produced human-like intuitive (System 1) errors (no single numeric accuracy reported for CRT in review).",
            "human_baseline_performance": "Not reported quantitatively in review for these exact task variants; CRT/semantic-illusion comparisons discussed qualitatively (human normative patterns invoked).",
            "performance_comparison": "On deception tasks GPT-4 shows very high success (near-perfect on simple, high but lower on complex); on CRT-style tasks, larger models show human-like intuitive errors — i.e., they replicate some human error patterns rather than clearly outperforming humans.",
            "experimental_details": "Authors manipulated ability to use chain-of-thought reasoning (e.g., preventing chain-of-thought) to probe differences; tasks included both simple and complex deception scenarios and semantic-illusion items; comparisons drawn across model sizes and alignment variants.",
            "limitations_or_caveats": "Review notes that emergence of deceptive capacities does not imply intent or consciousness; performance depends on prompt framing and on whether chain-of-thought reasoning is enabled; caveats about equating LLM 'intuitive' errors with human cognition are emphasized.",
            "uuid": "e9045.0",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Kosinski ToM",
            "name_full": "Kosinski — Evaluating large language models in theory of mind tasks (false-belief tasks)",
            "brief_description": "Systematic evaluation of multiple LLMs on false-belief (Theory of Mind) tasks to compare developmental-like performance to human children.",
            "citation_title": "Evaluating large language models in theory of mind tasks.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (and other LLMs)",
            "model_description": "Transformer-based LLMs (GPT family) tested across many false-belief task variants.",
            "model_size": null,
            "test_battery_name": "40 false-belief tasks (Theory of Mind battery)",
            "test_description": "False-belief tasks probe the ability to attribute others' beliefs that diverge from reality — canonical test for ToM in developmental psychology.",
            "llm_performance": "GPT-4 achieved 75% accuracy on the 40 false-belief tasks.",
            "human_baseline_performance": "Review states GPT-4 performance was 'comparable to the performance of a six-year-old child' (no exact numeric human baseline provided in review).",
            "performance_comparison": "GPT-4 approximates young child (around 6-year-old) performance on false-belief tasks (i.e., below adult normative ToM performance).",
            "experimental_details": "40 false-belief tasks required success in multiple related scenarios per task; comparison made across model versions.",
            "limitations_or_caveats": "The review highlights developmental analogies but cautions that matching behavioral answers on ToM tasks does not imply human-like mental state representations; task framing and prompt wording strongly affect results.",
            "uuid": "e9045.1",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Marjieh Perceptual",
            "name_full": "Marjieh et al. — LLMs predict human sensory judgments across modalities",
            "brief_description": "Evaluation of GPT-3, GPT-3.5, and GPT-4 on multiple psychophysical datasets and multilingual color-naming tasks to test whether LLMs recover perceptual structure from language.",
            "citation_title": "Large language models predict human sensory judgments across six modalities.",
            "mention_or_use": "mention",
            "model_name": "GPT-3, GPT-3.5, GPT-4",
            "model_description": "Series of transformer LLMs differing in scale and training data; assessed for alignment between language-derived representations and human perceptual judgments.",
            "model_size": null,
            "test_battery_name": "Six psychophysical datasets + multilingual color-naming task",
            "test_description": "Psychophysical datasets probe perceptual judgments (e.g., color, possibly other sensory modalities); color-naming task assesses cross-linguistic perceptual variation and recovery of perceptual representations (e.g., color wheel).",
            "llm_performance": "GPT-4 'aligns closely with human perceptual data' and recovers representations such as the color wheel (no single numeric accuracy given in review).",
            "human_baseline_performance": "Human perceptual datasets used as ground truth; exact human metrics not restated in review.",
            "performance_comparison": "GPT-4 shows close alignment to human perceptual judgments (i.e., strong correspondence), better than earlier GPT variants according to review summary.",
            "experimental_details": "Multilingual prompts and psychophysical stimulus descriptions were used; outputs compared to human datasets and cross-linguistic patterns examined.",
            "limitations_or_caveats": "Recovery of perceptual structure from text does not imply sensory experience; alignment depends on distributional signals present in language corpora.",
            "uuid": "e9045.2",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Loconte Neuropsych",
            "name_full": "Loconte et al. — Neuropsychological investigation of prefrontal functioning in LLMs",
            "brief_description": "Assessment of LLMs (GPT-3.5, GPT-4 and others) on neuropsychological tests tapping planning, semantic processing, and Theory of Mind to probe prefrontal-like functions.",
            "citation_title": "Challenging ChatGPT's \"intelligence\" with human tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language Model",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5, GPT-4 (also Claude2, Llama2 discussed)",
            "model_description": "Large transformer LLMs of varying alignment and update status; tested as if they were subjects on neuropsychological tasks.",
            "model_size": null,
            "test_battery_name": "Neuropsychological tests targeting prefrontal functions (planning, semantic understanding, Theory of Mind)",
            "test_description": "Tasks designed to probe executive functions associated with prefrontal cortex, semantic comprehension and ToM reasoning.",
            "llm_performance": "GPT-4 'generally meets normative human standards' on many tasks; Claude2 and Llama2 showed variable and often limited abilities (no specific accuracies reported in review).",
            "human_baseline_performance": "Normative human standards referenced (i.e., clinical/neuropsych norms) but no specific numeric baselines provided in review.",
            "performance_comparison": "GPT-4 often approaches normative human performance on these neuropsychological measures; other models perform worse and inconsistently.",
            "experimental_details": "Tests adapted from human neuropsychological batteries; authors compared model outputs against normative responses and clinical expectations.",
            "limitations_or_caveats": "Review emphasizes difficulty equating model outputs to human neurological function; passing some tests may reflect pattern-matching rather than genuine executive function.",
            "uuid": "e9045.3",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Dhingra GPT-4 Benchmarks",
            "name_full": "Dhingra et al. — GPT-4's performance on cognitive-psychology-related benchmark datasets",
            "brief_description": "Evaluation of GPT-4 on several established NLP/problem-solving benchmarks (CommonsenseQA, SuperGLUE, MATH, HANS) to assess integration of cognitive processes and context.",
            "citation_title": "Mind meets machine: Unravelling GPT-4's cognitive psychology.",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large multimodal/advanced GPT-family model with enhanced reasoning and instruction-following capabilities; trained on broad web-scale corpora and fine-tuned for instruction-following.",
            "model_size": null,
            "test_battery_name": "CommonsenseQA, SuperGLUE, MATH, HANS (benchmark datasets)",
            "test_description": "Benchmarks probe commonsense reasoning, general language understanding, formal math problem solving, and heuristics/entailment (HANS tests).",
            "llm_performance": "GPT-4 demonstrates 'high accuracy' on these cognitive-psychology-relevant benchmarks and surpasses prior models (no single numeric scores provided in review).",
            "human_baseline_performance": "Benchmarks have human baselines in original tasks but explicit human baseline numbers are not restated in the review.",
            "performance_comparison": "GPT-4 surpasses prior state-of-the-art models on these benchmarks and shows significant reasoning capability, though not equated directly to human-level across all tasks.",
            "experimental_details": "Evaluation used standard benchmark formulations; analyses considered how GPT-4 integrates context and cognitive process proxies.",
            "limitations_or_caveats": "Benchmarks are not direct cognitive-psychology experiments; strong benchmark performance does not guarantee human-like cognitive processing.",
            "uuid": "e9045.4",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Binz & Schulz Bandit",
            "name_full": "Binz & Schulz — Using cognitive-psychology tools to probe GPT-3 (including multiarmed bandit)",
            "brief_description": "Investigations where GPT-3 was tested on canonical cognitive experiments (decision-making, information search, causal reasoning) including the multiarmed bandit to compare model decision strategies with humans.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large pre-trained transformer language model (GPT-3) tested on cognitive task analogs; few-shot or instruction prompts used to operationalize tasks.",
            "model_size": "175B (GPT-3 is commonly 175B parameters) - not explicitly stated in review",
            "test_battery_name": "Canonical cognitive psychology experiments (including multiarmed bandit task)",
            "test_description": "Decision-making and reinforcement-learning analogs (multiarmed bandit) probe exploration-exploitation and rational choice behavior.",
            "llm_performance": "GPT-3 'made better decisions than humans' and outperformed humans on the multiarmed bandit task (no numeric accuracy provided in review).",
            "human_baseline_performance": "Human participants' performance referenced qualitatively (GPT-3 outperforming humans) but no numeric baseline provided in review.",
            "performance_comparison": "GPT-3 outperformed humans on the reported multiarmed bandit experiments; however, struggled with task perturbations, directed exploration, and causal reasoning in other paradigms.",
            "experimental_details": "Authors applied canonical cognitive paradigms to prompted LLMs, sometimes fine-tuning on multiple tasks to improve prediction of human behavior in unseen tasks.",
            "limitations_or_caveats": "Results depend on how tasks are framed/prompts; LLMs may exploit distributional regularities instead of implementing the same cognitive mechanisms as humans.",
            "uuid": "e9045.5",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Stevenson AUT",
            "name_full": "Stevenson et al. — GPT-3 evaluated on Guilford's Alternative Uses Test (AUT)",
            "brief_description": "Assessment of GPT-3's creative generation on the AUT with expert ratings and semantic-distance analyses compared against human data.",
            "citation_title": "Putting GPT-3's Creativity to the (Alternative Uses) Test.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large transformer LLM evaluated for generative creativity on divergent-thinking tasks.",
            "model_size": "175B (GPT-3 typical, not restated explicitly in review)",
            "test_battery_name": "Guilford's Alternative Uses Test (AUT)",
            "test_description": "Measures divergent thinking/creativity by asking participants to list unusual uses for a common object; scored on originality, flexibility, fluency, and elaboration.",
            "llm_performance": "GPT-3 produced responses evaluated for originality, usefulness, surprise, and flexibility; review states humans currently outperform GPT-3 in creativity (no numeric scores provided).",
            "human_baseline_performance": "Human participants scored higher on creativity measures overall (no numerical baseline provided in review).",
            "performance_comparison": "LLM below human baseline on creativity measures, though shows potential and may close gap with future models/fine-tuning.",
            "experimental_details": "Evaluation used expert human raters and semantic-distance metrics to quantify originality and related dimensions.",
            "limitations_or_caveats": "Evaluation of creativity involves subjective judgments; LLMs may produce superficially novel combinations that lack human-like conceptual depth.",
            "uuid": "e9045.6",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Sap SocialIQa/ToMi",
            "name_full": "Sap et al. — Evaluation of social intelligence / Theory of Mind datasets",
            "brief_description": "Evaluation of LLMs on SocialIQa (social intents/reactions) and ToMi (Theory of Mind inference) showing LLMs perform below human levels on these social-cognitive tasks.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (and other LLMs)",
            "model_description": "Transformer LLMs assessed on social-intelligence benchmarks probing intent understanding and mental-state inference.",
            "model_size": null,
            "test_battery_name": "SocialIQa and ToMi",
            "test_description": "SocialIQa: questions about social intents and reactions; ToMi: tasks requiring inference about mental states and different realities.",
            "llm_performance": "LLMs including GPT-4 scored ~55% on SocialIQa and ~60% on ToMi according to review summary.",
            "human_baseline_performance": "Review states these model scores are 'below human levels' (exact human percentages not restated in review).",
            "performance_comparison": "LLMs perform substantially below human baseline on these social-intelligence / ToM benchmarks.",
            "experimental_details": "Benchmarks applied in standard format; interpretation contextualized via pragmatics theories.",
            "limitations_or_caveats": "Scaling alone does not yield human-level ToM/social intelligence; performance limited by lack of person-centric training and interactive social grounding.",
            "uuid": "e9045.7",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Schaaff Emotion/Empathy",
            "name_full": "Schaaff et al. — Evaluation of ChatGPT's empathic abilities and emotion recognition",
            "brief_description": "Assessed GPT (ChatGPT variants) on emotion recognition tasks, conversational empathy measures, and multiple empathy questionnaires, comparing to human norms and clinical groups.",
            "citation_title": "Exploring ChatGPT's Empathic Abilities.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (unnumbered in summary) / GPT variants",
            "model_description": "Instruction-tuned conversational LLMs evaluated for affective understanding and empathic response generation.",
            "model_size": null,
            "test_battery_name": "Emotion recognition tasks, conversational analysis, five empathy-related questionnaires",
            "test_description": "Emotion identification accuracy tasks and standardized empathy questionnaires (comparisons to normative human samples and clinical groups).",
            "llm_performance": "Emotion identification: 91.7% accuracy; 'parallel emotions' in 70.7% of cases; empathy questionnaire scores below average human but above individuals with Asperger syndrome (no further numeric detail in review).",
            "human_baseline_performance": "Average human empathy scores (not numerically restated); clinical comparison group (Asperger syndrome) used as lower-bound reference.",
            "performance_comparison": "LLM shows high emotion-recognition accuracy but overall empathy scores below typical human averages while above certain clinical groups.",
            "experimental_details": "Combination of direct emotion-label tasks and conversational evaluations; multiple standardized empathy scales administered to model via prompts.",
            "limitations_or_caveats": "High accuracy in labeling emotions does not equal genuine empathic understanding; questionnaire-style probing of models relies on prompt formulation and cannot capture affective experience.",
            "uuid": "e9045.8",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Vzorinab / Patel X. Wang EI",
            "name_full": "Vzorinab et al. / Patel & Fan / X. Wang et al. — Emotional intelligence and emotion understanding benchmarks for GPT-4 and other LLMs",
            "brief_description": "Multiple psychometric-style assessments of emotional intelligence and emotion understanding applied to GPT-4 and other LLMs, benchmarking vs hundreds of human participants.",
            "citation_title": "The Emotional Intelligence of the GPT-4 Large Language Model.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (compared to Bard, GPT-3.5, etc.)",
            "model_description": "Large instruction-tuned LLM with advanced text understanding and generation capabilities; evaluated with psychometric emotion-understanding instruments (e.g., MSCEIT-style tasks, TAS-20, EQ-60 proxies).",
            "model_size": null,
            "test_battery_name": "MSCEIT-like emotional intelligence tasks / TAS-20 / EQ-60 / custom psychometric Emotion Understanding assessment",
            "test_description": "Tasks probe emotion perception, understanding, management and reflective/empathic reasoning — domains of emotional intelligence.",
            "llm_performance": "GPT-4 'approached human-level emotional intelligence' and in one psychometric assessment 'scored higher than 89% of humans'; GPT-4 outperformed Bard and GPT-3.5 which showed alexithymic tendencies; MSCEIT evaluation indicated GPT-4 strong on understanding/managing but limited on reflective analysis.",
            "human_baseline_performance": "Benchmarked against &gt;500 human participants in one assessment (exact human score distributions not restated in review).",
            "performance_comparison": "GPT-4 at or above large majority of human participants on some emotion-understanding measures (e.g., &gt;89th percentile) but shows qualitative differences in reflective reasoning and lacks human-like reflective emotional processing.",
            "experimental_details": "Text-based prompting to elicit model responses on standardized emotional scenarios and psychometric items; comparisons against human normative datasets.",
            "limitations_or_caveats": "High psychometric scores reflect pattern-matching on text; reflective, introspective, and affective aspects differ from human emotional processing; variation across tests and instruments noted.",
            "uuid": "e9045.9",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "D'Souza Clinical Vignettes",
            "name_full": "D'Souza et al. — Appraising ChatGPT in psychiatry using clinical case vignettes",
            "brief_description": "Assessment of ChatGPT-3.5 on 100 clinical psychiatric vignettes graded by expert psychiatrists across categories (diagnosis, management, etc.).",
            "citation_title": "Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (GPT-3.5)",
            "model_description": "Instruction-tuned GPT-3.5 evaluated for clinical reasoning on psychiatric case vignettes.",
            "model_size": null,
            "test_battery_name": "100 clinical psychiatric vignettes (graded across 10 categories)",
            "test_description": "Clinical case scenarios assessing diagnostic reasoning, management recommendations, and clinical judgement domains.",
            "llm_performance": "ChatGPT-3.5 earned 'top grades' in 61% of cases (no further numeric breakdown provided in review).",
            "human_baseline_performance": "Expert psychiatrists used as gold-standard graders; exact human performance baseline numbers not provided in review.",
            "performance_comparison": "ChatGPT-3.5 performed strongly (top grades in majority of cases) but showed minor discrepancies in more complex scenarios; not considered a replacement for clinicians.",
            "experimental_details": "Expert psychiatrists graded model outputs across multiple categories for each vignette; models compared qualitatively to professional standards.",
            "limitations_or_caveats": "Vignette-based evaluation restricts complexity and multimodal cues present in clinical practice; model may be overly pessimistic or optimistic depending on variant (other studies noted GPT-3.5 pessimism vs GPT-4/Bard/Claude alignment).",
            "uuid": "e9045.10",
            "source_info": {
                "paper_title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Turning large language models into cognitive models.",
            "rating": 2,
            "sanitized_title": "turning_large_language_models_into_cognitive_models"
        },
        {
            "paper_title": "Using cognitive psychology to understand GPT-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Deception abilities emerged in large language models.",
            "rating": 2,
            "sanitized_title": "deception_abilities_emerged_in_large_language_models"
        },
        {
            "paper_title": "Evaluating large language models in theory of mind tasks.",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_in_theory_of_mind_tasks"
        },
        {
            "paper_title": "Large language models predict human sensory judgments across six modalities.",
            "rating": 2,
            "sanitized_title": "large_language_models_predict_human_sensory_judgments_across_six_modalities"
        },
        {
            "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs.",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Mind meets machine: Unravelling GPT-4's cognitive psychology.",
            "rating": 2,
            "sanitized_title": "mind_meets_machine_unravelling_gpt4s_cognitive_psychology"
        },
        {
            "paper_title": "Putting GPT-3's Creativity to the (Alternative Uses) Test.",
            "rating": 1,
            "sanitized_title": "putting_gpt3s_creativity_to_the_alternative_uses_test"
        },
        {
            "paper_title": "Exploring ChatGPT's Empathic Abilities.",
            "rating": 2,
            "sanitized_title": "exploring_chatgpts_empathic_abilities"
        },
        {
            "paper_title": "Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes.",
            "rating": 1,
            "sanitized_title": "appraising_the_performance_of_chatgpt_in_psychiatry_using_100_clinical_case_vignettes"
        }
    ],
    "cost": 0.02307825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review</p>
<p>Luoma Ke 
Department of Psychological and Cognitive Sciences
Tsinghua University</p>
<p>Song Tong tong.song.53w@kyoto-u.jp 
Department of Psychological and Cognitive Sciences
Tsinghua University</p>
<p>Department of Psychology
Beijing Normal University</p>
<p>Peng Cheng pengkp@tsinghua@edu.cn 
School of Social Science
Tsinghua University</p>
<p>Kaiping Peng 
Department of Psychological and Cognitive Sciences
Tsinghua University</p>
<p>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review
B823EF5C03FD75526729B2AF0DD34967
This review explores the frontiers of large language models (LLMs) in psychological applications.Psychology has undergone several theoretical changes, and the current use of artificial intelligence (AI) and machine learning, particularly LLMs, promises to open up new research directions.We aim to provide a detailed exploration of how LLMs are transforming psychological research.We discuss the impact of LLMs across various branches of psychology-including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology-highlighting their ability to model patterns, cognition, and behavior similar to those observed in humans.Furthermore, we explore the ability of such models to generate coherent, contextually relevant text, offering innovative tools for literature reviews, hypothesis generation, experimental designs, experimental subjects, and data analysis in psychology.We emphasize the importance of addressing technical and ethical challenges, including data privacy, the ethics of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations.Researchers should use LLMs responsibly in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas.Overall, this review provides a comprehensive overview of the current state of LLMs in psychology, exploring the potential benefits and challenges.We hope it can serve as a call to action for researchers to responsibly leverage LLMs' advantages while addressing the associated risks.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) has a history spanning nearly seven decades, beginning with the 1956 Dartmouth Conference.The field has recently been revolutionized with the advent of large language models (LLMs) such as ChatGPT, Google's Bard, and Meta's LLaMA.Among them, GPT-4, in particular, could signify a paradigm shift given its impressive capabilities (e.g., solving difficult tasks in math, coding, vision, medicine, law, and psychology) (Bubeck et al., 2023), exemplifying the concept of "AI for science" (Wang et al., 2023).LLMs mark a critical juncture in the evolution of machine learning and AI, propelled by their expansive size and sophisticated neural architectures that incorporate attention mechanisms (Vaswani et al., 2017).These models incorporate cognitive principles (Binz &amp; Schulz, 2023a) and exhibit emergent properties comparable to those seen in complex physical systems (Wei et al., 2022).This has enhanced their ability to process and represent concepts and high-level semantics (J.Li et al., 2022) while also deepening our insights into human cognitive processes (Sejnowski, 2022).In psychological applications, these developments are reshaping interactions among data, language, and the environment (De Bot et al., 2007;Demszky et al., 2023), contributing significantly to various fields, including clinical (Thirunavukarasu et al., 2023), developmental (Frank, 2023;Hagendorff, 2023), and social psychology (Hardy et al., 2023;J. Zhang et al., 2023).Moreover, LLMs have had profound effects on psychological research methods, offering novel approaches and tools for exploration and analysis.</p>
<p>The LLM concept: From machine learning to capability emergence</p>
<p>Generative AI evolved from advances in pattern recognition capability.While convolutional neural networks (CNNs) excelled at recognizing objects and concepts, the next challenge was to use this recognition capability for a generation.For example, if a CNN can identify "age" in portraits, we can use that understanding to modify "age" in any portrait.This generative approach first succeeded in computer vision through models such as generative adversarial networks (Goodfellow et al., 2020) and deconvolution (Zeiler, 2014), which could create realistic images based on learned patterns.The same generative principles were then applied to language, leading to LLMs that could generate contextually relevant text.LLMs represent a particularly significant leap in the capabilities of generative AI.These models are designed to process natural language text and generate contextually relevant text.LLMs like GPT-4, LLaMA, Claude, and Gemini leverage the transformer architecture (Vaswani et al., 2017), which employs sophisticated neural networks and attention mechanisms to revolutionize natural language processing.Each model is optimized uniquely to enhance performance across diverse tasks.For instance, LLaMA focuses on efficient training processes (Touvron et al., 2023), Claude emphasizes safety and alignment (Li et al., 2024), and Gemini integrates advanced reasoning capabilities (Rane et al., 2024).LLaMA's open-source nature allows local deployment and efficient training, making it suitable for psychological studies needing rapid iteration or customization, such as behavioral modeling (Binz and Schulz (2023a).Claude, designed for safety and alignment, is less commonly used in psychology research and more oriented toward knowledge-based tasks (Li et al., 2024).</p>
<p>GPT-4, with its large-scale parameters and broad training data, supports a wide range of tasks, including cognitive simulations and clinical assessments.These differences guide model selection based on research needs like accessibility, task specificity, or data scale.</p>
<p>While these models highlight the versatility of LLMs, it is essential to distinguish between specific products designed for particular interactions, such as ChatGPT for conversational applications, and the broader capabilities of LLMs that extend beyond chat interfaces to include text generation, summarization, translation, and embedding extraction.This range of applications demonstrates that LLMs' capabilities are emergent, manifesting new abilities as the model size increases.Performance improvements on log-log scales sometimes experience "breaks" where unexpected capabilities emerge from complex interactions within the models (Wei et al., 2022).</p>
<p>At the heart of LLMs is the transformer architecture, a deep neural network with an attention mechanism that efficiently processes sequential data in parallel (Vaswani et al., 2017); this works in a manner somewhat similar to human brain functions.This architecture has revolutionized the field of natural language processing.The self-attention mechanism of the transformer architecture captures contextual relationships in textual data, allowing for more sophisticated language understanding.Notably, the "large" in LLM refers to the many parameters and massive amounts of training data used to fine-tune these models, typically billions of parameters and terabytes of text (Binz &amp; Schulz, 2023b), in addition to "mastering the world" (Yildirim &amp; Paul, 2023).</p>
<p>The process of large language modeling, from machine learning to the emergence of competence, can be divided into several key stages.(1) Pretraining: LLMs are pretrained on large amounts of textual data to learn intricate linguistic, syntactic, and textual structures, where the model learns to predict the next token through unsupervised learning, resulting in a base model that captures the statistical patterns of language (P.Liu et al., 2023).(2) Alignment: Supervised learning is used to create a foundation model that can better interact with users in the intended ways, which typically involves instruction tuning and reinforcement learning based on human feedback.After the foundation model is available, domain-specific fine-tuning can adapt the model for particular applications (Liu et al., 2022).This fine-tuning process ensures the model can generate contextually relevant responses and engage in meaningful conversations or tasks.Through these stages of development, LLMs demonstrate increasingly sophisticated text-generation capabilities, including response generation, content summarization, translation, and compositional text generation (Bubeck et al., 2023).The ability to effectively process and represent context is a critical factor underlying the observed emergence of advanced capabilities in these models.Finally, LLMs exhibit "observed capability emergence" when integrated into various applications and systems, in addition to performing tasks that require a deep understanding of language and context, thus often achieving human-like or superhuman performance in specific experimental tasks, such as analogical reasoning (Webb et al., 2023), creativity (Stevenson et al., 2022), and emotion recognition (Patel &amp; Fan, 2023).</p>
<p>Therefore, LLMs can provide valuable insights into how such technologies can simulate or augment processes traditionally associated with human cognition.Specifically, LLMs maintain a balance between logical processing and the use of cognitive shortcuts (heuristics), and they adapt their reasoning strategies to optimize between accuracy and effort.This aligns with the principles of resource-rational human cognition, as discussed in dual-process theory (Mukherjee &amp; Chang, 2024).For instance, LLMs generate and process natural language, demonstrating structural and functional parallels with certain aspects of human linguistic and cognitive mechanisms (Goertzel, 2023).These parallels allow for the exploration of AI applications in areas such as cognitive psychology (Sartori &amp; Orrù, 2023), language acquisition (Jungherr, 2023), and even mental health (Lamichhane, 2023).Moreover, the study of LLMs contributes to our understanding of the human mind, offering a computational perspective on language processing, decision-making (Sha et al., 2023), and learning mechanisms (Hendel et al., 2023).The fusion of such disciplines could drive advancements in AI and provide a computational framework for investigating processes related to human cognition.</p>
<p>Psychology and AI</p>
<p>Psychology, as a science that explores the human mind and behavior, has undergone significant theoretical changes since the late nineteenth century, with psychoanalysis and behaviorism extending to cognitive psychology (Hothersall &amp; Lovett, 2022).This history marks a shift in the focus of psychology research, reflecting the academic trend of moving from observing behavioral manifestations to exploring indepth psychological connotations.Each of these phases has led to a deepening understanding of the psychocognitive processes of human beings.</p>
<p>Understanding human psycho-cognitive processes is therefore crucial for psychology.In clinical and counseling psychology, research on cognitive psychology supports diagnosing and treating psychological disorders.It deepens our understanding of the psychological mechanisms underlying emotions, stress, and human behavior.Psychotherapies such as cognitive-behavioral therapy (Hofmann et al., 2012) and psychodynamic therapy have become essential tools for promoting mental health and emotional regulation.</p>
<p>In educational and developmental psychology, the development of cognitive psychology has fostered a deeper understanding of the roles of perceptual and affective factors in learning processes (Glaser, 1984), which has led to innovations in teaching methods and learning strategies.In social and cultural psychology, cognitive psychology research helps explain individuals' behaviors and mental processes in different social and cultural contexts, exploring how cultural differences affect cognitive patterns, values, and behavioral norms, especially in the context of globalization, interaction, and integration.In social psychology, meanwhile, cognitive psychology research on group behavior, social influence, prejudice, and discrimination holds great value for promoting social harmony and mutual understanding (Park &amp; Judd, 2005).</p>
<p>AI is becoming an increasingly influential tool in psycho-cognitive research.Simon (1979) was among the first to recognize the potential of computational models to simulate aspects of human cognitive processes.</p>
<p>Currently, LLMs can process and generate human-like texts and perform certain tasks in a manner similar to human cognition (Bubeck et al., 2023).LLMs also offer a unique computational perspective for the study of human cognition.For example, GPT-3 can solve vignette-based tasks similar to or better than human subjects and can perform rational decision-making based on descriptions, outperforming humans in the multiarmed bandit task (Binz &amp; Schulz, 2023b).Furthermore, after extensive testing, GPT-3 is able to solve complex analogical problems at levels comparable to human performance, and analogical reasoning is an essential hallmark of human intelligence (Webb et al., 2023).Moreover, fine-tuning across multiple tasks can allow LLMs to predict human behavior in previously unseen tasks-that is, LLMs can be adapted to generalpurpose cognitive models (Binz &amp; Schulz, 2023a), potentially opening up new research directions that could transform cognitive psychology and behavioral science in general.Newell (1990) offered a structured framework for analyzing human behavior, categorizing cognitive and behavioral processes into four distinct layers based on their time scales (Fig. 1a).At the biological level, the focus is on physiological and neural processes occurring at rapid time scales, ranging from milliseconds to one second.This level can include neural responses and sensory processing, which form the foundation of human cognition.The cognitive level pertains to mechanisms such as attention, perception, and short-term memory, which operate at intermediate time scales, typically between one second and one minute.These processes enable fundamental cognitive functions.At the rational level, the framework considers more complex cognitive activities such as problem-solving, planning, and decision-making.Such activities occur over longer time scales, spanning several minutes to a few hours, and involve sustained cognitive engagement.Finally, the social level considers behaviors shaped by social interaction and cultural influence, operating at the longest time scale, ranging from hours to days or longer.This level concerns the effects of social communication, group behavior, and cultural influences on cognition.It underscores the multifaceted nature of human behavior, highlighting the relationship between rapid physiological processes and the more prolonged, socially influenced aspects of human cognition.Figure 1 integrates this framework by mapping psychological domains (e.g., cognitive, social) onto these timescales, demonstrating LLMs' ability to simulate behaviors-from short-term processes like memory retrieval to long-term phenomena like cultural trends.Emergent properties (e.g., cognitive simulation) connect these domains to practical research tools (e.g., stimuli generation), with bidirectional influence refining both applications and properties.</p>
<p>Therefore, by analyzing LLM application across these four levels (Fig. 1a), it is possible to further explore their potential for modeling and studying human cognition and behavior (Fig. 1b), as well as their unique role in psycho-cognitive processes.Recent research has revealed significant advancements in LLMs' ability to perform complex human-like cognitive and social tasks (Grossmann et al., 2023;Marjieh et al., 2023;Orru et al., 2023;Pal et al., 2023;Stevenson et al., 2022;Webb et al., 2023).For instance, Grossmann et al. (2023) and Marjieh et al. (2023) demonstrated LLMs' proficiency in simulating human social interactions and perceptual processing, respectively.Orru et al. (2023) and (Webb et al., 2023) highlighted LLMs' capabilities in complex problem-solving and reasoning while Hagendorff et al. (2023) focused on decision-making processes.Stevenson et al. (2022)  As general-purpose cognitive models (Binz &amp; Schulz, 2023a), LLMs offer new perspectives and approaches for research in the fields of cognitive and behavioral psychology, clinical and counseling psychology, educational and developmental psychology, and social and cultural psychology, at different time scales of human behavior (Fig. 1a).LLMs can also be used as research aids (Fig. 1c) to help psychologists with everything from literature reviews (Aydın &amp; Karaarslan, 2022;Qureshi et al., 2023), experimental subjects (Dillion et al., 2023;Hutson, 2023), and data analysis (Patel &amp; Fan, 2023;Peters &amp; Matz, 2023;Rathje et al., 2023) to promote scholarly communication: academic writing (Dergaa et al., 2023;Stokel-Walker, 2022) or peer review (Chiang &amp; Lee, 2023;Van Dis et al., 2023).Thus, LLMs can potentially become research assistants for psychologists, helping them improve their research efficiency.</p>
<p>Objectives and significance of the present review</p>
<p>This review aims to provide a comprehensive analysis of the applications and effects of LLMs in psychological research.To ensure a systematic and rigorous review, we established specific inclusion and exclusion criteria.The review focuses on literature published between 2020 and 2024, sourced from relevant academic databases, including Google Scholar, arXiv, and Web of Science.Our initial keyword selection-"GPT-3", "ChatGPT", "GPT-4", "large language models", and "psychology"-was determined during the literature collection in October 2023, when GPT-based models were predominant in psychological research (e.g., Binz &amp; Schulz, 2023b;Bubeck et al., 2023).At the time, open-source models like LLaMA (Touvron et al., 2023) and proprietary models like Claude had limited psychology-specific applications.To maintain a focused scope, we did not retrospectively expand search terms but included diverse LLMs via manual screening.To reflect recent developments, an updated search incorporated studies from 2024.</p>
<p>To bolster the integrity of our data extraction process, two interdisciplinary researchers (male, 33 and inclusion criteria required the selected studies to (1) explore the application or analysis of LLMs in psychological contexts; (2) be peer-reviewed journal articles or high-impact conference proceedings; and (3) present empirical data, theoretical discussions, or methodological advancements.We selectively included preprint articles if they addressed emerging trends or filled notable gaps in the literature.Articles without a psychological focus or those addressing non-LLM-based AI systems were excluded.The study selection involved screening 191 identified studies, analyzing 100 full-text articles, and ultimately including 47 studies categorized into various psychological subfields.Each of these studies met stringent inclusion criteria, ensuring they contributed meaningfully to our understanding of LLMs in psychological research.</p>
<p>In this review, we systematically examine the use of LLMs in various psychological domains by analyzing their application at different behavioral time scales.The rest of the paper is structured as follows.</p>
<p>In section 2, we explore LLMs in cognitive and behavioral psychology.In section 3, the roles of LLMs in clinical and counseling psychology are discussed.Subsequently, educational and developmental psychology are addressed in section 4, followed by social and cultural psychology in section 5, outlining LLMs' contributions to each area.While psychological techniques are occasionally utilized to assess the capabilities of LLMs, this approach is employed to enhance understanding of their suitability and potential as instruments for psychological research.The primary focus of this review is how LLMs facilitate and advance psychological research across these domains.For a deeper understanding of the effect of LLMs on psychological research, an overview of LLMs' potential as tools for scientific research is given in section 6.</p>
<p>In section 7, the challenges and future research directions with regard to applying LLMs to psychological contexts are provided.Finally, conclusions are presented in section 8, with a summary of LLMs' applications in psychology and recommendations for future work.Importantly, we propose strategies for integrating LLMs into psychological research and provide insights into interpreting such models from a psychological standpoint, contributing to their safety and interpretability.</p>
<p>LLMs in cognitive and behavioral psychology</p>
<p>Within the multilevel time scales of human behavior (Newell, 1990), cognitive and behavioral psychology has primarily focused on the study of cognitive processes at sub-hourly time scales, which encompass human engagement in perception, memory, thinking, decision-making, problem-solving, and conscious planning.Cognitive and behavioral psychology typically uses experimental methods to study these cognitive processes, controlling and observing behaviors and responses under specific conditions.The recent emergence of LLMs has reinvigorated the discussion on whether such models might exhibit patterns resembling human cognitive processes; if so, it may be possible to study the "cognitive processes" of LLMs, which could provide valuable insights into human cognitive phenomena and serve as a valuable addition to existing research methods in cognitive psychology.The foundational technology underlying large language models (LLMs) is the generative pre-trained transformer (GPT) architecture, which employs deep neural networks to process and generate human-like text.GPT models function through mechanisms, such as attention mechanisms and token prediction, enabling them to capture complex linguistic patterns and generate contextually coherent outputs.These foundational technologies have transformed natural language processing (NLP) by expanding the capacity for both comprehension and generation of text across diverse applications, from conversational agents to content creation (Brown et al., 2020;Vaswani et al., 2017).The incorporation of such architectures into psychological research has initiated discussions regarding their potential to simulate cognitive phenomena.Binz and Schulz (2023a) found that fine-tuning multiple tasks enabled an LLM to predict human behavior in previously unseen tasks, suggesting that LLMs can be adapted to become generalist cognitive models.In another study, the same authors tested GPT-3 using tools from cognitive psychology and showed that it made better decisions than humans and outperformed them in the multiarmed bandit task (Binz &amp; Schulz, 2023b).Other studies have shown that LLMs can display perceptual judgment (Marjieh et al., 2023), reasoning (Webb et al., 2023), and decision-making abilities (Hagendorff et al., 2023), as well as creativity (Stevenson et al., 2022) and problem-solving (Orru et al., 2023).One study found that an LLM had the mental ability of a seven-year-old child based on a false-belief task (which is considered the gold standard for testing theory of mind in humans) (Kosinski, 2024).Exploring the reasoning capabilities and decisionmaking processes of LLMs, Hagendorff et al. (2023) designed a series of semantic illusion and cognitive reflection tests designed to elicit intuitive but erroneous responses (these are conventionally used to study human reasoning and decision-making) and then ran the tests for LLMs.They conducted an analysis of model performance on a Cognitive Reflection Test (CRT) task and a semantic illusion task to elucidate their cognitive processes, drawing upon System 1 and System 2 thinking, as conceptualized by Daniel Kahneman in his seminal work Thinking, Fast, and Slow (Kahneman, 2011), which represent fundamental constructs for understanding human cognitive processes.System 1 refers to intuitive and automatic thinking, whereas System 2 involves rational, deliberate decision-making processes.This framework provides a theoretical basis for interpreting how LLMs simulate human-like cognitive behaviors during these tasks.They observe how these models show correct responses in these tasks and avoid pitfalls.The performance of the models in the CRT task were further evaluated by preventing them from chain-thinking to reason.The results showed that as model size and language capability increased, the LLMs increasingly exhibited human-like intuitive thinking (System 1) and the associated cognitive errors.Table 1 provides a summary of the applications of LLMs to cognitive and behavioral psychology.GPT-3's responses to AUT were evaluated for originality, usefulness, surprise, and flexibility, using expert ratings and semantic distance analysis, and compared with human data.</p>
<p>Humans currently outperform GPT-3 in creativity, but GPT-3 shows potential to close the gap in future, raising questions about AI creativity and its evaluation.</p>
<p>Marjieh et al. (2023)</p>
<p>Can LLMs recover perceptual information from language, and how do they reflect cross-linguistic variations?</p>
<p>GPT-3, GPT-3.5, GPT-4 were tested on six psychophysical datasets and a multilingual color-naming task to compare their outputs with human perceptual data.</p>
<p>LLMs like GPT-4 align closely with human perceptual data, recover representations such as the color wheel, and reflect cross-linguistic perceptual variations, demonstrating their ability to extract perceptual information from language.</p>
<p>Loconte et al. (2023)</p>
<p>How do various LLMs perform on neuropsychological tests assessing prefrontal functions compared to human cognitive abilities?GPT-3.5, GPT-4,were evaluated on tasks related to planning, semantic understanding, and Theory of Mind.</p>
<p>Findings indicate that GPT-4 generally meets normative human standards, whereas Claude2, and Llama2 show variable and often limited abilities, particularly in planning and Theory of Mind, underscoring the challenges in mimicking complex human cognitive functions.</p>
<p>Dhingra et al. (2023)</p>
<p>How does GPT-4 perform on cognitive psychology tasks compared to prior state-of-the-art models?</p>
<p>GPT-4 was evaluated on cognitive psychology datasets (CommonsenseQA, SuperGLUE, MATH, HANS) to analyze its integration of cognitive processes with contextual information.</p>
<p>GPT-4 demonstrates high accuracy on cognitive psychology tasks, surpassing prior models, and showcases significant potential to bridge human and machine reasoning.</p>
<p>Hagendorff (2024)</p>
<p>Can modern LLMs understand and apply deception strategies?</p>
<p>Experiments tested LLMs on inducing false beliefs, using chainof-thought reasoning, and displaying Machiavellian behavior in simple and complex deception scenarios.</p>
<p>GPT-4 demonstrates advanced deceptive behavior, succeeding in 99.16% of simple and 71.46% of complex scenarios, highlighting the emergence of sophisticated deception abilities absent in earlier models.DALL-E 2 was employed to generate realistic visual stimuli of car-free urban environments, which were then presented to participants to measure attitudes toward sustainable policies.</p>
<p>Experimental Methodologies for Cognitive Research Using LLMs</p>
<p>By using DALL-E 2, the study demonstrated that generative AI tools can enhance the design process of experimental stimuli, offering greater control, diversity, and scalability, thereby effectively influencing participants' attitudes.</p>
<p>Note:</p>
<p>The AUT is a psychological test that measures creativity by asking participants to think of as many uses as possible for a common object; DALL-E 2 is developed by OpenAI that generates detailed and realistic images from textual descriptions to explore AI's potential in creative fields.</p>
<p>Beyond theoretical evaluations, LLMs have demonstrated practical value in experimental psychology, particularly in stimulus generation and experimental design (Zhuang et al., 2023).Dubey et al. (2024), for instance, used DALL-E 2 1 to create realistic visual stimuli depicting car-free urban environments, which influenced participants' attitudes toward sustainable policies.Such tools streamline the stimulus design process by providing control, diversity, and scalability.Similarly, LLMs have been employed in hardware testing to generate tailored stimuli, outperforming traditional methods in specific scenarios (Z.Zhang et al., 2023).Charness et al. (2023) further demonstrated the use of LLMs for enhancing experimental workflows by refining task instructions, ensuring consistency, and monitoring participant engagement.By leveraging their flexibility and scalability, LLMs can provide novel methods for advancing experimental psychology.</p>
<p>These applications facilitate the exploration of complex cognitive phenomena and the development of innovative research designs while also complementing traditional psychological research frameworks (Srinivasan et al., 2023).However, the interpretation of LLM outputs requires careful contextualization to avoid overstating their capabilities or equating them with human cognitive processes.</p>
<p>LLMs in clinical and counseling psychology</p>
<p>Clinical and counseling psychology focuses on assessing, diagnosing, treating, and preventing mental health problems.These processes often involve medium-to long-term periods.In the multilevel time scales of human behavior (Newell, 1990), clinical and counseling psychology involves assessing everyday behavioral acts (about a few hours to a day), habitual thinking (about a day to a few months), and psychological disorders (a few months to many years), among others (Fig. 1).The application of LLMs in clinical and counseling psychology can be broadly divided into two categories: psychological assessment and psychological intervention.Psychological assessment focuses on improving the ecological validity, scalability, and accuracy of measuring mental health states while psychological interventions consider how LLMs can be used for scalable and personalized mental health support, such as life coaching.According to related reports, there has been a public rush to use LLMs such as GPT for mental health screening and treatment (Demszky et al., 2023).LLMs are expected to be used in clinical psychology and counseling 1 Note: Although DALL-E 2 is not an LLM, we included this study due to its reliance on Transformerbased semantic understanding, a cornerstone of LLM research, and its demonstrated utility in generating controlled visual stimuli for psychological experiments.</p>
<p>because they can parse human language and generate human-like responses, categorize text, and flexibly adapt conversational styles representing different theoretical orientations (Stade et al., 2023).This leads to the following question: How do LLMs work in psychotherapy, and can they replace human psychotherapists?</p>
<p>An LLM is a basic generalized model with the ability to learn from small samples (Brown et al., 2020), which allows it to quickly become an "expert" in the clinical and counseling domain with only a small amount of data to learn from.For example, LLMs trained on clinical content can identify more specific factors of change that can help psychologists understand the process of clinical intervention, thus opening the black box of psychotherapy (Schueller &amp; Morris, 2023).Regarding psychological assessment, studies have demonstrated that LLMs can effectively recognize emotions (Sharma et al., 2023) and respond appropriately.They can also perform complex mental health evaluations (Patel &amp; Fan, 2023;Schaaff et al., 2023), such as suicide risk assessment and schizophrenia prognosis.For example, Elyoseph and Levkovich (2024) found that GPT-4, Google Bard, and Claude produced evaluations consistent with professional benchmarks in treated schizophrenia cases, though GPT-3.5 exhibited overly pessimistic predictions.Other research has shown that GPT-3.5 excels in clinical psychiatric cases, achieving top grades in diagnosis and management across 61% of cases, with only minor discrepancies in more complex scenarios (D 'Souza et al., 2023).</p>
<p>In psychological interventions, LLMs have shown significant potential for delivering scalable and personalized mental health support (Blyler &amp; Seligman, 2023a, 2023b).For example, Blyler and Seligman (2023a) demonstrated that GPT-4 can generate personalized therapeutic strategies by analyzing narrative identities.These strategies were found to be valid and reasonable, highlighting GPT-4's utility as a supportive tool in therapy and coaching.For peer-to-peer mental health support, Sharma et al. (2023) (Schueller &amp; Morris, 2023), hold promise for addressing insufficient capacity in the mental health care system and offering more individualized treatment services, even potentially fully automating psychotherapy in the future (Stade et al., 2023).It is essential to ensure that LLM is safe and privacyprotective in psychotherapy.</p>
<p>LLMs in educational and developmental psychology</p>
<p>Educational and developmental psychology is concerned with learning processes, knowledge accumulation, skill development, and changes in individual psychology in educational environments.</p>
<p>Educational and developmental psychology is mainly positioned at the relatively medium-to long-term level (Newell, 1990), reflecting the ongoing learning and development that characterize the educational process.</p>
<p>A national survey found that only 3 months after the public release of GPT, 40% of US teachers used it weekly for lesson planning, highlighting the growing impact of LLMs in education.</p>
<p>Table 3 summarizes the applications of LLMs in educational and developmental psychology, which can be broadly divided into two categories: developmental research with LLMs and using LLMs for education and learning applications.Here, developmental research seeks to determine whether LLMs can simulate human developmental processes (e.g., theory of mind and emotional reasoning) and how such capabilities might advance our understanding of human cognitive and emotional development.Kosinski (2024), for instance, tested different LLMs in 40 false-belief tasks and found that GPT-4 achieved 75% accuracy, comparable to the performance of a six-year-old child, while older models performed significantly worse.Vzorinab et al. (2024) used the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) to evaluate GPT-4's emotional intelligence.While GPT-4 excelled at understanding and managing emotions, its reflective analysis resembled the early developmental stages of human emotional reasoning.</p>
<p>The study of using LLMs for education and learning applications focuses on leveraging LLMs to address challenges in education, such as providing personalized learning and improving learning motivation.LLMs learn from massive amounts of data taken from books and the Internet (Binz &amp; Schulz, 2023b) and can be used as more knowledgeable learning aids (Stojanov, 2023), provide personalized learning experiences (Kasneci et al., 2023), and enhance the motivation to learn (Ali et al., 2023).Baillifard et al. (2024), for instance, found that an AI tutor powered by GPT-3 improved academic performance by up to 15 percentile points through personalized learning strategies.Stojanov (2023) used the following approach to explore GPT's potential as a learning tool: First, he set learning objectives and had "conversations" with GPT about its functionality for 4 hours.For the next 3 hours, he continued the discussion with GPT and watched some relevant videos on YouTube.He experienced positive feedback from his interactions with GPT and found it to be a motivating and relevant learning experience.GPT-4 was evaluated using the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) through text-based prompts.</p>
<p>GPT-4 excels in understanding and managing emotions but shows limited reflective analysis, resembling early developmental stages in human emotional reasoning.Trott et al. (2023) How does exposure to language influence the development of theory of mind in humans and AI?</p>
<p>A linguistic False Belief Task was presented to humans and GPT-3 to assess belief attribution abilities.</p>
<p>GPT-3's partial success suggests that while language exposure contributes to belief reasoning, other developmental mechanisms unique to humans are crucial for fully developing theory of mind.(2023) How effective is GPT as a learning aid in scaffolding understanding of a specific topic?</p>
<p>LLMs for Education and Learning Applications</p>
<p>Stojanov</p>
<p>An autoethnographic study exploring the author's personal experience using ChatGPT(3.5)The study compared responses from ChatGPT to 13 psychological research method scenarios against ratings by subject matter experts.</p>
<p>ChatGPT's responses correlated strongly with expert evaluations, suggesting its potential as an educational tool in psychology, though its usage should be approached with caution.</p>
<p>Baillifard et al. (2024)</p>
<p>Can AI tutors improve academic performance through personalized learning strategies?</p>
<p>A semester-long study with 51 psychology students using a GPT-3powered AI tutor for personalized retrieval practice and progress modeling.</p>
<p>Active AI tutor use improved grades by up to 15 percentile points, with strong alignment between AI predictions and exam results.</p>
<p>Note： Theory of mind (ToM) is the cognitive ability to attribute mental states to oneself and others； the Fogg Behavior Model (FBM) explains behavior as a product of motivation, ability, and prompts.</p>
<p>LLMs in social and cultural psychology</p>
<p>Social and cultural psychology explores how individuals interact with and are influenced by their social and cultural environments.It focused on interpersonal dynamics, group behavior, social cognition, and the long-term formation and transformation of attitudes and norms (Tajfel, 1982).Such phenomena occur at various time scales, from immediate social interactions to cultural changes evolving over several years (Newell, 1990).LLMs provide valuable tools for advancing social and cultural psychology.By analyzing textual datasets, simulating social interactions, and modeling human-like behaviors, LLMs can provide insights into the dynamics of social cognition, group processes, and cultural norms (Salah et al., 2023).Their scalability and ability to quantify patterns across time scales make them powerful instruments for examining human interactions in diverse contexts.</p>
<p>Research on LLMs in social and cultural psychology can be categorized into three main areas: cultural and cognitive understanding, social interactions and behavioral simulations, and practical applications.First, LLMs have many similarities with humans regarding social cognition.For example, research has found that LLMs reflect a variety of typical human cognitive biases in judgment and decision-making, such as the anchoring effect, the representativeness heuristic, and base-rate neglect (Talboy &amp; Fuller, 2023).In addition, cultural psychology research has identified significant differences in the cognitive processes of Easterners and Westerners when processing information and making judgments (Nisbett et al., 2001); in this regard, LLM consistently favors holistic Eastern ways of thinking (Jin et al., 2023).Second, LLMs have been shown to characterize human groups in social interaction settings.It has been shown that LLMs can replicate the results of Milgram's electroshock experiments (Aher et al., 2023), show better gaming abilities in specific games (Akata et al., 2023), and exhibit different risk-taking and prosocial behaviors under different emotional states (Yukun et al., 2023).</p>
<p>Third, LLMs are increasingly used as proxies for human participants in psychological research.One study, for example, explored the potential of LLMs to serve as valid proxies for specific human subgroups in social science research; it found that LLMs contained information that went far beyond superficial similarity, reflecting the complex interplay between ideas, attitudes, and sociocultural contexts that characterizes human attitudes (Argyle et al., 2022).In addition, LLM has been tested for personality and values, obtaining results comparable to those for human samples, indicating their potential as psychological research tools (Miotto et al., 2022).Within this broader perspective, industrial and organizational psychology has increasingly employed LLMs, particularly in employee selection and workplace optimization, demonstrating their broader utility for understanding human behavior in structured environments.For example, LLMs have been shown to improve the accuracy and efficiency of recruitment systems in terms of assessing candidate fit and simulating workplace behaviors (Du et al., 2024).This approach can help mitigate biases and expand accessibility to a broader range of candidates.LLMs have also been integrated into systems such as PALR (personalization-aware LLMs for recommendation) to dynamically align individual capabilities with organizational needs.Such systems significantly reduce inefficiencies in hiring processes and enhance predictions about job performance by identifying nuanced compatibility factors in resumes and cover letters (Yang et al., 2023).Beyond individual applications, LLMs have contributed to understanding broader organizational cultures and transformational dynamics by providing insights into how group interactions and leadership styles influence workplace outcomes (Noy &amp; Zhang, 2023).In the context of employee productivity, experiments using LLMs have revealed substantial benefits.For instance, professionals using ChatGPT in workplace writing tasks improved productivity by reducing task completion time by 40% and enhancing output quality by 18%, indicating its potential to effectively augment mid-level professional tasks (Noy &amp; Zhang, 2023).Similarly, research on creativity has demonstrated LLMs' ability to help solve organizational problems requiring innovative thinking (Lee &amp; Chung, 2024).Table 4 summarizes the applications of LLMs to social and cultural psychology.</p>
<p>LLMs have many applications in social and cultural psychology, allowing us to test theories and hypotheses about human behavior in social and cultural interaction settings.Zhao et al. (2024), for instance, examined whether AI chatbots can adjust their financial decisions and prosocial behaviors based on emotional cues, similar to humans.It was hypothesized that bots would take fewer risks when exposed to fear cues and more risks with joy cues.Emotional primes (fear, joy, or neutral) were applied, and investment decisions were analyzed.Additionally, prosocial responses, such as donating to a sick friend, were measured to assess how LLMs adapt behaviorally under emotional influences.These findings highlight LLMs' ability to model complex social dynamics and cultural influences.The next section broadens this perspective, exploring LLMs' potential as versatile research tools for psychologists.GPT was evaluated using cognitive and value judgment scales.</p>
<p>GPT leans towards Eastern holistic thinking in cognitive tasks but shows no cultural bias in value judgments, likely influenced by its training data and methods.</p>
<p>Schaaff et al. (2023)</p>
<p>How empathetic is GPT compared to humans?GPT's empathy was evaluated through emotion recognition tasks, conversational analysis, and five empathy-related questionnaires.</p>
<p>GPT accurately identified emotions in 91.7% of cases, showed parallel emotions in 70.7%, and scored below average humans but above individuals with Asperger syndrome on empathy measures.</p>
<p>Patel and Fan (2023)</p>
<p>Can LLMs like Bard, GPT-3.5, and GPT-4 match human empathy and emotion identification?</p>
<p>Empathy and emotional understanding were assessed using TAS-20 (Toronto Alexithymia Scale-20) and EQ-60 (Emotional Quotient Inventory-60), comparing LLM responses to human benchmarks.</p>
<p>GPT-4</p>
<p>approached human-level emotional intelligence, outperforming Bard and GPT-3.5, which showed alexithymic tendencies.X. Wang et al. (2023) How do LLMs compare to humans in emotional intelligence?</p>
<p>A psychometric assessment focusing on Emotion Understanding was developed and applied to mainstream LLMs, benchmarking them against over 500 human participants.</p>
<p>GPT-4 scored higher than 89% of humans in emotional intelligence, with LLMs showing above-average emotional intelligence but using nonhuman mechanisms influenced by model design.</p>
<p>X. Li et al. (2022) Are LLMs psychologically safe, and how can fine-tuning improve their safety?LLMs were assessed using the Short Dark Triad (SD-3), Big Five Inventory (BFI), and well-being tests to evaluate personality traits and the impact of fine-tuning.</p>
<p>LLMs exhibit elevated dark traits but show improved well-being and psychological safety with targeted fine-tuning.</p>
<p>Miotto et al. (2022)</p>
<p>What are GPT-3's personality traits and values as assessed by validated psychological tools?</p>
<p>Administered validated personality and values measurement tools to GPT-3, including a model response memory to assess value alignment.</p>
<p>GPT-3 exhibits personality traits and values similar to human samples, providing initial evidence of psychological assessment in LLMs.</p>
<p>Social Interactions and Behavioral Simulations</p>
<p>Zhao et al. (2024)</p>
<p>Can LLMs like GPT adapt responses to emotional primes in decision-making?</p>
<p>Tested GPT-4 and 3.5 with scenarios eliciting positive, negative, or neutral emotions.</p>
<p>GPT-4 showed distinct emotional response patterns, exceeding GPT-3.5, indicating advanced modulation but no true emotions.</p>
<p>Aher et al. (2023)</p>
<p>Can</p>
<p>LLMs accurately simulate human behaviors, and what biases emerge in their simulations?</p>
<p>Introduced Turing Experiments to evaluate LLMs against findings from classic behavioral studies.</p>
<p>LLMs replicated most findings but showed "hyper-accuracy distortion," raising concerns for applications in education and the arts.</p>
<p>Abramski et al. (2023)</p>
<p>Do LLMs exhibit biases toward math and STEM, and how do these biases compare across models and with humans?</p>
<p>Using Behavioral Forma Mentis Networks, biases in GPT-3, GPT-3.5, GPT-4, and high school students were analyzed through a language generation task.</p>
<p>Newer LLMs (GPT-4) show reduced negative bias and richer semantic associations toward math and STEM compared to older models and humans, suggesting advancements in reducing stereotypes.</p>
<p>Almeida et al. (2024)</p>
<p>How do state-of-the-art LLMs reason about moral and legal issues, and how do their responses align with human judgments?</p>
<p>Eight experimental psychology studies were replicated using Google's Gemini Pro, Anthropic's Claude 2.1, GPT-4, and Meta's Llama 2 Chat 70b.Model responses were compared to human responses to assess alignment and systematic differences.</p>
<p>GPT-4 showed the best human alignment among LLMs but exaggerated effects and reduced variance, highlighting biases that limit their suitability as substitutes for human participants in psychological research.Four studies tested GPT's responses to prompts designed to assess cognitive biases (anchoring, representativeness, availability heuristic, framing effect, endowment effect) and compared them to human participant responses.</p>
<p>Practical Applications with</p>
<p>GPT demonstrated biases consistent with human heuristics across all studies, suggesting that language patterns alone may contribute to these effects, independent of human cognitive and affective processes.Park et al. (2022) How can designers predict and refine social behaviors in large-scale social computing systems before deployment?Developed "social simulacra," an LLM-driven simulation that generates realistic community interactions based on design inputs (goals, rules, personas), allowing scenario testing and iterative design refinement.</p>
<p>Social simulacra accurately mimicked real community behavior, supported "what if?" scenario exploration, and helped designers improve system designs before large-scale deployment.</p>
<p>Sap et al. (2022)</p>
<p>Can LLMs demonstrate social intelligence and Theory of Mind (ToM)?LLMs were evaluated using SocialIQa (social intents and reactions) and ToMi (mental states and realities), with results contextualized through pragmatics theories.</p>
<p>LLMs, including GPT-4, perform below human levels (55% on SocialIQa, 60% on ToMi), indicating that scaling alone does not yield ToM, highlighting the need for personcentric NLP approaches.Argyle et al. (2022) Can GPT-3 reliably emulate human subpopulations for social science research?</p>
<p>GPT-3 was conditioned on sociodemographic backstories from U.S. surveys, creating "silicon samples," which were compared to human survey data.</p>
<p>GPT-3 exhibits nuanced, demographically aligned biases, indicating its potential as a tool for studying human behavior and societal dynamics.P. S. Park et al. (2024) Can GPT-3.5 simulate human participants and replicate social science study results?Replicated 14 Many Labs 2 studies using GPT-3.5, analyzing response patterns and the "correct answer" effect through pre-registered and exploratory studies.GPT-3.5 replicated 37.5% of study results but exhibited uniform responses ("correct answer" effect) and skewed conservative in moral foundation surveys, questioning its reliability and diversity as a human participant substitute.</p>
<p>Note: WEIRD (Western, Educated, Industrialized, Rich, Democratic) refers to societies that represent a minority of the global population but are often overrepresented in psychological research.</p>
<p>LLMs as research tools in psychology</p>
<p>Sections 2-5 illustrate LLMs' applications across cognitive, clinical, educational, and social psychology, revealing their potential to transform research practices.Together, these advancements speed up psychological research with new tools, encourage collaboration with fields like computer science and linguistics, and improve theoretical models through behavioral simulationkey ways LLMs advance psychology.Building on these foundations, this section explores LLMs as versatile research tools in psychology, supporting diverse tasks such as systematic reviews, literature review, and experimental design (Table 5).By reducing subjective bias and minimizing human variability in tasks like stimulus generation (Section 2), standardized assessments (Section 3), and data interpretation (Section 4), LLMs enhance objectivity and efficiency across these applications.</p>
<p>For instance, LLMs can automate systematic reviews and meta-analyses, revolutionizing evidence synthesis and provide actionable insights for psychologists, as grounded in cognitive and behavioral principles (e.g., in Sections 2 and 3).This capacity extends to enhancing psychologists' workflows, building on productivity improvements noted in Section 5, through tools like literature review, hypothesis generation, experimental design, experimental subjects, and data analysis (Table 5).</p>
<p>Table 5. LLMs as research tools in psychology study.</p>
<p>Topic</p>
<p>Related study</p>
<p>Literature review</p>
<p>LLMs can summarize the researched literature (Dis, Bollen, Zuidema, Rooij, &amp; Bockting, 2023), complete literature review tasks (Qureshi et al., 2023), and create literature review articles (Aydın &amp; Karaarslan, 2022), at the same time, there are LLM that has been specially trained to accomplish systematic literature reviews (Taylor et al., 2022)。</p>
<p>Hypothesis generation</p>
<p>LLMs can generate hypotheses from scientific literature, make inferences based on scientific data, and then clarify their conclusions through interpretation (Zheng et al., 2023), and can quickly and automatically test these research hypotheses and learn from mistakes.</p>
<p>Experimental design</p>
<p>LLMs provide text-based material for experimental design, thereby optimizing the research process and reducing experimental complexity.By employing these models, researchers can easily create experimental stimuli, develop test items, and even simulate interactive sessions in controlled environments (Aher, Arriaga, &amp; Kalai, 2022;Akata et al., 2023), providing a high degree of control and precision to the experimental process.</p>
<p>Experimental subjects</p>
<p>LLMs can simulate some human behaviors and responses, which provides an opportunity to test theories and hypotheses about human behavior (Grossmann et al., 2023), their use in place of human participation in experiments saves time and costs and can be applied to some experiments where human participation is not appropriate (Hutson, 2023), they can be combined with factors such as the specific research topic, the task, and the sample, and the use of LLM as an alternative to research participants where appropriate (Dillion et al., 2023).</p>
<p>Data analysis</p>
<p>LLMs can efficiently analyze massive amounts of textual data to gain insights into human behavior and emotions at an unprecedented scale (Patel &amp; Fan, 2023), can analyze textual data in multiple languages, and accurately detect mental structures within it (Rathje et al., 2023), can draw mental profiles from social media data (Peters &amp; Matz, 2023)。</p>
<p>Scholarly Communication</p>
<p>LLMs can also help humans in writing (Dergaa et al., 2023;Stokel-Walker, 2022;Van Dis et al., 2023).LLMs were used in two natural language processing tasks and a human expert to assess the quality of the text, and the results of the assessment were consistent with those of the human expert (Chiang &amp; Lee, 2023), LLMs offer the opportunity to get things done quickly, from Ph.D. students struggling to finish their dissertations, to peer reviewers submitting analyses under time pressure (Van Dis et al., 2023).</p>
<p>Automated literature review and meta-analysis</p>
<p>Conducting a literature review meta-analysis is a complex, arduous process that requires significant time and expertise (Michelson &amp; Reuter, 2019).Nature reported that researchers have used GPT as a research assistant to summarize literature (Dis et al. 2023).In one study, researchers used GPT to complete certain systematic literature review tasks (Qureshi et al., 2023).In another study, a literature review article was created using GPT with the application of digital twins in the health field; the results showed that knowledge compilation and representation were accelerated with the help of LLMs.However, their academic validity needs to be further verified (Aydın &amp; Karaarslan, 2022).Researchers have also specifically trained LLMs to support the practical needs of scientific research (Taylor et al., 2022), including the ability to perform systematic literature reviews.</p>
<p>Recent studies have highlighted how LLMs can efficiently support meta-analysis.For instance, Luo et al. ( 2024) demonstrated that LLMs can screen literature, extract data, and generate statistical codes for metaanalyses, significantly reducing workload while maintaining recall rates comparable to manual curation.</p>
<p>Similarly, Tong et al. (2024) used LLMs to extract causal pairs from 43,312 psychology articles, achieving an 86.98% success rate in pair extraction through adaptive prompting.As discussed in section 3, LLMs have shown strong capabilities in extracting causal relationships from large textual datasets, underscoring their potential to streamline evidence synthesis for systematic reviews and meta-analyses.Nevertheless, while</p>
<p>LLMs excel in organizing qualitative data and identifying conceptual patterns, they face challenges in extracting the precise numerical data necessary for meta-analyses.For example, although LLM-based tools can retrieve and summarize outcome measures, manual validation remains essential to ensure accuracy, especially when processing complex figures or tables.</p>
<p>In summary, LLMs can speed up the process of literature review and meta-analysis.Researchers can use such models to systematically review and synthesize existing research, improving the efficiency of evidence-based psychology.</p>
<p>Hypothesis generation and experimental design</p>
<p>Hypothesis-driven research is at the core of scientific activity.LLMs can generate hypotheses from scientific literature, make inferences based on data, and then clarify conclusions through interpretation (Banker et al., 2024;Zheng et al., 2023).Although LLMs are capable of becoming "hypothesis machines," their logical and mathematical derivation capabilities still need improvement to eliminate factual errors, quickly test hypotheses, and learn from mistakes (Y.J. Park et al., 2024) .As innovative tools, LLMs have great potential for use in psychological experiments, given their ability to provide text-based material for experimental designs, thus optimizing the research process and reducing experimental complexity.Using such models researchers can easily create experimental stimuli, develop test items, and even simulate interactive sessions in controlled environments (Aher, Arriaga, &amp; Kalai, 2022;Akata et al., 2023), providing a high degree of control and precision in the experimental process.</p>
<p>In conclusion, LLMs provide powerful, flexible tools for psychological research, from hypothesis generation to experimental design, which can help researchers achieve more precise, efficient research goals.</p>
<p>LLMs as subjects in psychological experiments</p>
<p>Although LLMs can simulate some human behaviors and responses-which provides an opportunity to test theories and hypotheses about human behavior (Grossmann et al., 2023)-there is still some controversy on whether LLMs can be used as a substitute for human subjects in psychological research.</p>
<p>While recognizing that certain problems persist (e.g., biases and insufficiently trained data), some researchers have suggested that LLMs can be used as substitutes for human participants to save time and cost and can be applied to experiments that are not suitable for human participation (Hutson,2023).Others have proposed using LLMs as an alternative method of studying participants when appropriate, based on their performance in conjunction with factors such as specific research topics, tasks, and samples (Dillion et al., 2023).However, it is also believed that although LLMs can significantly affect scientific research, they are unlikely to replace human participants in any meaningful way (Harding et al., 2023).At the same time, some studies of LLMs as subjects have shown that LLMs perform similarly to humans (Orru et al., 2023;P. S. Park et al., 2024), which might indicate LLMs' potential to replace humans as subjects.</p>
<p>In conclusion, although LLMs can simulate human judgment, their simulation of human thinking remains limited, and their output should be validated and interpreted with caution when used as psychological subjects.</p>
<p>Tools for data analysis</p>
<p>Various forms of AI have long been used to analyze psychological data, such as flight data for pilot screening (Ke et al., 2023).Machine learning algorithms facilitate the processing of large datasets, identifying patterns and correlations that might otherwise be overlooked.However, LLMs take this capability to a new level; they can efficiently analyze massive amounts of textual data on an unprecedented scale to derive insights into human behavior and emotions (Patel &amp; Fan, 2023).For psychological research, this means faster and more comprehensive data analysis, leading to more reliable, nuanced findings.LLMs can analyze textual data in multiple languages, accurately detect psychological structures within them (Rathje et al., 2023), and generate psychological profiles from social media data (Peters &amp; Matz, 2023).</p>
<p>LLMs have also demonstrated a degree of competence in the medical field; LLMs can, for example, predict the optimal neuroradiographic imaging modality for a given clinical presentation.Yet, LLMs cannot outperform experienced neuroradiologists, suggesting the need for continued improvement in the medical context (Nazario-Johnson et al., 2023).These findings demonstrate the great potential of LLMs for evaluating and analyzing data.</p>
<p>Promoting scholarly communication</p>
<p>Scholarly communication is a cornerstone of academic research, encompassing the processes of creating, evaluating, and disseminating knowledge.It includes writing research papers, conducting peer reviews, and ensuring that findings are communicated transparently and ethically.In psychology, this process is particularly complex owing to the field's diverse theoretical frameworks and methodological approaches, ranging from experimental to qualitative research.The discipline's focus on human behavior and its intersection with technology demands precise and ethical communication practices.</p>
<p>It has been argued that LLMs currently cannot completely replace human writing and instead can only answer questions and generate naturally fluent and informative content but with no real intelligence-i.e., text based on patterns of previously seen words (Stokel-Walker, 2022).In one study, students used GPT as an aid in their writing.The experimental group that used GPT was found to be similar to the control group in terms of writing quality, speed, and authenticity; the authors suggested that this could be because experienced researchers can better guide GPT to produce high-quality information.By contrast, studentswho have less writing experience than researchers-found that GPT did not perform as effectively (Bašić et al., 2023).Another article discussed the prospects and potential threats of GPT in academic writing, emphasizing that using GPT in academic research should prioritize peer-reviewed scholarly sources.Yet, GPT's potential advantages for academic research, including the handling of large amounts of textual data and the automatic generation of abstracts and research questions (Dergaa et al., 2023), were highlighted.</p>
<p>Furthermore, LLMs can potentially be used for peer review (Van Dis et al., 2023).The decisions/judgments of LLMs in a text-evaluation task were found to be consistent with those of human experts (Chiang &amp; Lee, 2023).</p>
<p>In conclusion, LLMs such as GPT are potent tools for scholarly communication in psychology, capable of processing large amounts of textual data and automating tasks that were previously done manually.They can be used to scan academic papers and extract essential details, generate objective and unbiased abstracts, and create research questions in social psychology (Banker et al., 2023;Tong et al., 2024).However, researchers must exercise caution when using them as they can also introduce false or biased information into papers, leading to unintentional plagiarism and the misattribution of concepts (Van Dis et al., 2023).</p>
<p>Challenges and future directions</p>
<p>Challenges and limitations</p>
<p>LLMs have enormous potential to simulate complex cognitive processes, providing researchers with new tools to explore the mechanisms of human cognition and behavior for wide-ranging application in various fields, including clinical and counseling psychology, educational and developmental psychology, and social and cultural psychology.However, LLM output should not be mistaken for the presence of thought but instead viewed as complex pattern matching based on probabilistic modeling (Floridi &amp; Chiriatti, 2020).</p>
<p>Although LLMs show impressive performance, this differs from consciousness or genuine understanding.</p>
<p>The interpretation of LLMs' capabilities must be based on an understanding of their limitations and the nature of their operations, which might differ fundamentally from human cognition.It is essential, then, to focus on the potential of LLMs in psychological research while also acknowledging the technical and ethical challenges that might arise.</p>
<p>First, despite the emergence of LLM competence (Wei et al., 2022), its internal working mechanism remains a black box from a cognitive and behavioral psychology perspective.For example, LLMs perform impressively on tasks requiring formal linguistic competence (including knowledge of the rules and patterns of a particular language) but fail many tests requiring functional competence (the set of cognitive abilities needed to understand and use language in the real world) (Mahowald et al., 2023).They excel in analogical and moral reasoning tasks but perform poorly on spatial reasoning tasks (Agrawal, 2023).</p>
<p>Second, while LLMs have accelerated the use of AI in clinical and counseling psychotherapy, privacy and ethical issues might arise (Graber-Stiehl, 2023).For example, gatekeepers, patients, and even mental health professionals who use GPT to assess suicide risk or improve decision-making might receive inaccurate assessments that underestimate risk (Elyoseph &amp; Levkovich, 2023) or bias clinician decisionmaking, which can lead to healthcare inequities (Pal et al., 2023).In addition, LLMs in psychiatry research and practice have been associated with potential bias and privacy violations (Zhong et al., 2023).</p>
<p>Third, LLMs face application challenges in fields such as educational, developmental, and social and cultural psychology.It is evident that when applied in education, LLMs have the potential for output bias and misuse (Kasneci et al., 2023).One study found that texts generated by GPT were not always consistent or logical and sometimes even contradictory (Stojanov, 2023).In the field of social and cultural psychology, LLMs exhibit cognitive biases (Talboy &amp; Fuller, 2023) and cultural biases (Atari et al., 2023) similar to those of humans, in addition to implicitly darker personality patterns (X.Li et al., 2022).Bender et al. (2021) suggested that training data for LLMs might reflect social biases that continue to be perpetuated in research settings.</p>
<p>Finally, LLMs have some limitations as aids to scientific research.With regard to writing, for example, LLMs currently cannot fully replace humans.Instead, they answer questions and generate naturally flowing, informative content lacking real intelligence (Stokel-Walker, 2022).Although macrolanguage models can simulate human judgment when used as experimental subjects, there are still limits to their "understanding" of human thought (Dillion et al., 2023).Van Dis et al. (2023) noted that LLMs might accelerate innovation, shorten publication times, and increase scientific diversity and equality.However, they might also reduce the quality and transparency of research and fundamentally alter scientists' autonomy as human researchers.</p>
<p>In summary, while LLMs offer extraordinary capabilities for psychological research, they also present challenges related to bias, ethical issues, data security, transparency, and technical expertise.Researchers should be fully aware of these challenges when using LLMs and adopt the following steps for ethical use:</p>
<p>First, disclose model details and methods transparently to ensure reproducibility.Second, verify outputs against literature or experts to address inaccuracies and misinformation.Third, use diverse training data to reduce cultural or gender biases.Fourth, in sensitive areas like mental health, limit use to assist-not replace-judgment and train users to interpret outputs critically.These steps, supported by recent studies (Abdurahman et al., 2024;Guo et al., 2024;Porsdam Mann et al., 2024), address ethical concerns in psychological research.Table 6 summarizes the challenges and limitations of LLMs in psychological applications.</p>
<p>Table 6 Challenges and limitations of LLMs in psychological applications.</p>
<p>Challenges Author Details</p>
<p>Cognitive and Behavioral Psychology</p>
<p>Lack of Real-World Understanding</p>
<p>Mitchell (2023)</p>
<p>LLMs lack real-world understanding, abstract reasoning, and intent comprehension.</p>
<p>Lack of Metaknowledge Stella et al. (2023) LLMs fabricate information (hallucination) and lack curiosity/meta-knowledge.</p>
<p>Causal Reasoning and Creativity Sartori and Orrù (2023) Poor causal reasoning, dependence on biased training data, lack of creativity and imagination.</p>
<p>Multi-Step Reasoning Limitations</p>
<p>Goertzel (2023)</p>
<p>Poor multi-step reasoning, lack of autonomy, poor real-world understanding.</p>
<p>Common</p>
<p>Sense Reasoning Peng et al. (2023) Forgetting knowledge in new tasks, poor common-sense reasoning, inconsistent problemsolving.</p>
<p>Model</p>
<p>Behavior Challenges Holtzman et al. (2023) Lack of interpretability and formal behavioral descriptions makes systematic analysis difficult.</p>
<p>Psycholinguistic Features Seals and Shalin (2023) GPT and human-generated analogies differed in these stylistic dimensions, these lexical features, their choice of words for these features and these devices that help readers understand text.GPT may lack human cognitive and psycholinguistic features when generating analogies.</p>
<p>Clinic and Counseling Psychology</p>
<p>Technical Limitations&amp; Patient Connection Issues Stade et al. (2023) Difficulty assessing suicide risk, substance abuse, safety issues, and interpreting nonverbal cues&amp; Problems forming therapeutic relationships, interpreting nonverbal behaviors.</p>
<p>Education and Development Psychology</p>
<p>Integrity and Ethics Issues Li et al. (2023) Academic integrity concerns, misinformation, data privacy, and impact on critical thinking.</p>
<p>Bias</p>
<p>and Over-Reliance&amp; Multilingual Support Challenges Kasneci et al. (2023) Insufficient personalization, bias in teaching, over-reliance on models reduces creativity.&amp;Limited support for diverse languages and equitable access.</p>
<p>Social and Culture Psychology</p>
<p>Liability and Privacy Issues Fecher et al. (2023) Liability issues: challenging traditional mechanisms of authorship and liability.Bias issues: affecting the objectivity and impartiality of science.Privacy and data protection issues: may be privacy issues with the training data of LLMs.Intellectual property issues: potential legal disputes.Environmental issues: generating large amounts of carbon emissions, which can have a negative impact on the environment.</p>
<p>Global</p>
<p>Diversity Ignorance Atari et al. (2023) Ignoring global psychological diversity (e.g., tend to favor the psychological characteristics of WEIRD societies) and which can lead to prejudice and discrimination against people of other cultures and backgrounds.Differences in values and moral judgments and which can lead to problems of communication and understanding in multicultural societies.Self-identity and perceived social roles and which may lead to stereotypes and misconceptions about non-WEIRD populations).</p>
<p>Cultural and Ethical Tensions P.S. Park et al. (2024) Reduced innovation and development, bias and discrimination, culture clash and conflict, differences in values and morals and entrenchment of the status quo.</p>
<p>Social</p>
<p>Context Limitations Salah et al. (2023) Limited understanding of social context: Although GPT performs well in syntax and general semantics, it still has limitations in capturing the nuances of social language.Ethical challenges: AI-generated fake content can lead to ethical issues including digital personhood, informed consent, potential manipulation, and the implications of using AI to simulate human interactions.</p>
<p>Bias and Misleading</p>
<p>Misuse of language modeling: GPT-3 may be used to generate fake news, spread extremist ideas, conduct cyber-attacks and other malicious uses.Fairness, bias, and representation: GPT-3 may carry bias against gender, race, and religion, among others, sparking related controversies.News generation: News generated by GPT-3 may be difficult to distinguish from real news, leading to confusing and misleading information.</p>
<p>Research Tools</p>
<p>Plagiarism and Copyright Issues Sallam (2023) Plagiarism: content generated by GPT may be considered plagiarized, violating academic norms.Copyright issues: Is the generated content owned by GPT or by the user?Transparency issues: The workings of GPT may not be transparent, making it difficult for users to understand the source of generated content.Liability issues: who is responsible for GPT when generating incorrect content?Transparency Limitations Gupta et al. (2023) Transparency and Explanation: The working mechanism of generative AI models may be difficult to explain, which may lead users to doubt the credibility of the generated content.Legal and Ethical Issues: Generative AI models may involve intellectual property, privacy, and ethical issues, requiring attention to compliance with relevant laws and regulations during use.</p>
<p>Academic</p>
<p>Integrity Concerns Dergaa et al. (2023) Integration of erroneous or biased information.Problems with citing original sources and authors.Impact on academic integrity and quality.Increased inequity and inequality: Difficulty in recognizing AI-generated content.Academic evaluation and recognition issues.Direct replacement for academic researchers: GPT is not a complete replacement for academic researchers as it has limitations in certain types of academic research.</p>
<p>Privacy and Bias Risks</p>
<p>Peters and Matz (2023)</p>
<p>User privacy: LLMs can infer psychological traits from a user's social media data, which may violate the user's privacy.Potential bias: LLMs may create potential bias in the inference process, which may lead to unfair treatment of specific groups (e.g., gender, age, etc.).Data security: if the inferential power of LLMs is used maliciously, it may lead to data leakage, with serious implications for users' mental health.</p>
<p>Misconduct and Limitations</p>
<p>Y. Liu et al. (2023) Academic misconduct: GPT may be used for academic cheating, such as generating false papers or assignments.Challenges in the medical field: GPT has limitations in medical image analysis, which may lead to wrong diagnosis and jeopardize patients' health.</p>
<p>Future directions and emergent trends</p>
<p>Currently, LLMs are used in different areas of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology.As the capabilities of LLMs are further enhanced, their potential applications in psychology will continue to develop.</p>
<p>First, in the field of cognitive and behavioral psychology, with the emergence of multimodal LLMs (OpenAI, 2023), it is possible to combine visual and auditory information with textual data to better understand and model emotions, behaviors, and mental states for cognition.However, neuroimaging data can be used to inform the architectures and parameters of LLMs and integrate that information with traditional textual data to create more accurate and biologically sound models of human language and thought.</p>
<p>Second, in the field of clinical and counseling psychology, on the one hand, personal data, such as social media posts, medical records, or wearable device data, can be used to create tailored, personalized LLMs that provide more accurate and relevant insights into an individual's state of mind.At the same time, the strengths of human clinical and counseling expertise can be combined with the scalability and computational power of LLMs to create new diagnostic treatment and intervention tools.In addition, in educational and developmental psychology and social and cultural psychology, it is essential to build ethical LLMs and ensure they are designed and deployed in a way that respects privacy and uses data fairly and responsibly.</p>
<p>Ultimately, LLMs represent a systematic project whose future development cannot be achieved without the interdisciplinary collaboration of researchers in diverse fields such as psychology, computer science, and linguistics.For psychology researchers, accessible open-source LLM frameworks and tools might become an integral part of their future research efforts.Table 7 summarizes LLMs' future directions and emergent trends in psychological application.Explore ways to make LLMs more stable and robust in the face of descriptive tasks.</p>
<p>Investigate whether LLMs can learn to explore purposefully and how to better utilize causal knowledge in tasks.Analyze the performance of LLMs in different tasks and contexts to see if they can adapt like humans.Explore how LLMs develop and refine their cognitive abilities during natural interactions with humans.</p>
<p>Huang and Chang (2022)</p>
<p>Improve the reasoning ability of LLMs to encourage reasoning by optimizing training data, model architecture, and optimization goals.Develop more appropriate evaluation methods and benchmarks to measure the reasoning ability of LLMs to better reflect the true reasoning ability of the models.Investigate the potential of LLMs in different applications (e.g., problem solving, decision making and planning tasks).Explore other forms of reasoning (e.g., inductive and retrospective reasoning).</p>
<p>Clinic and Counseling</p>
<p>Abd-Alrazaq et al. (2019)</p>
<p>Develop more chatbots for people with mental illness, especially for those with disorders such as schizophrenia, obsessive-compulsive disorder and bipolar disorder.</p>
<p>Implement more chatbots in developing countries to address the shortage of mental health professionals.Conduct more randomized controlled trials to evaluate the effectiveness of chatbots in mental health.Stade et al. (2023) Developing new therapeutic techniques and evidence-based practices (EBPs).Focus on evidence-based practices first: to create meaningful clinical impact in the short term, clinical LLM applications based on existing evidence-based psychotherapies and techniques will have the greatest chance of success.Involve interdisciplinary collaboration.Focuses on therapist and patient trust and usability.Criteria for designing effective clinical LLMs.Demszky et al. (2023) Development of high-quality cornerstone datasets: these datasets need to encompass populations and psychological constructs of interest and be associated with psychologically important outcomes (e.g., actual behaviors, mindfulness, health, and mental well-being).Focus on future research directions in consumer neuroscience and clinical neuroscience: research in these areas may involve the neural systems of marketing-related behaviors, decision neuroscience, neuroeconomics, and more.(2023) Developmental psychology: examining how LLMs develop cognitively, socially, and emotionally over the lifespan and how these models can be optimized for specific tasks and situations.Learning psychology: studying how LLMs acquire and retain knowledge and skills, and how to optimize these models to improve learning.Sap et al. (2022) Explore more interactive and empirical training methods to help LLMs acquire true social intelligence and theoretical mental abilities.Investigate ways to combine static text with rich social intelligence and interaction data to improve social intelligence in LLMs.Investigate the theoretical-psychological abilities of LLMs in more naturalistic settings to reveal their performance in real-world scenarios.Argyle et al. (2022) Investigate the algorithmic fidelity of the GPT-3 model and how appropriate conditioning can allow the model to accurately simulate the response distributions of various human subgroups.Created "in silico samples" by conditioning on the socio-demographic backgrounds of real human participants in multiple large U.S. surveys.</p>
<p>Education and Development</p>
<p>Hagendorff</p>
<p>Society and Culture</p>
<p>Schaaff et al. (2023)</p>
<p>Developing more advanced models: to more accurately capture the emotional context of conversations and improve emotional understanding and expression.Measuring the emotional capabilities of bots: to investigate how to assess the emotional capabilities of chatbots in order to better understand how they behave when interacting with humans.Explore the use of GPT as a support tool: investigate how GPT can be used to support people more empathetically and improve human well-being.</p>
<p>Ziems et al. (2023)</p>
<p>Cross-cultural CSS research: future research should separately consider the utility of LLMs for cross-cultural CSS in order to better serve social science research in different cultural contexts.Future research could explore contrastive or causal explanations in LLMs.New paradigms for social science and AI collaboration.Van Dis et al. (2023) Invest in truly open LLMs: develop and implement open-source AI technologies to increase transparency and democratic control.Embrace the advantages of AI: utilize AI to accelerate innovation and breakthroughs at all academic stages, while focusing on issues of ethics and human autonomy.Broaden the discussion: organize international forums to discuss the development and responsible use of LLMs in research, including issues of diversity and inequality.Fecher et al. (2023) Analyzing the risks and opportunities of LLMs for science systems.Examining how LLMs affect academic quality assurance mechanisms, academic misconduct, and scientific integrity.Exploring the impact of LLMs on academic reputation, evaluation systems, and knowledge dissemination.Examining how to balance the potential benefits from LLMs with adherence to scientific principles.</p>
<p>Research Tools</p>
<p>Conclusion</p>
<p>With the rapid development of AI technologies, especially the continuous advancement of LLMs, machine learning has reached the point where it can recognize and generate human language.This development is not simply a technological breakthrough for the field of psychology, but it opens the door to a range of potential applications.</p>
<p>First, in the field of cognitive and behavioral psychology, LLMs are excelling in a variety of cognitive tasks.Although there are still limitations in causal cognition and planning, these models resurrect the principle of association, demonstrating the ability to associate at a distance and reason in complex ways.At the same time, the ability to adapt LLMs to cognitive models is a significant strength of psychological research, allowing for new explorations of human cognitive and behavioral processing mechanisms.</p>
<p>Second, in clinical and counseling psychology, LLMs can be used as preliminary diagnostic tools for mental health.While traditional mental health diagnosis relies on the experience of professionals and direct interaction with patients, LLMs can quickly identify potential mental health problems, such as depression and anxiety, by analyzing an individual's verbal expressions and textual content.Importantly, while such diagnoses cannot wholly replace professional psychological assessment, they can serve as an effective adjunct to help psychologists understand a patient's condition more quickly, or play a role in primary mental health interventions.Meanwhile, personalized psychological intervention is another critical application direction for LLMs.By combining information about an individual's health data and lifestyle habits, these models can provide tailored psychological advice and intervention programs.Such personalized approaches could be crucial for improving the effectiveness of psychological interventions.</p>
<p>Third, LLMs have the same potential for application in both educational and developmental psychology and social and cultural psychology.For example, LLMs provide interactive and personalized learning experiences or generate research tasks based on real-life applications that increase motivation and enhance learning.In addition, by analyzing large amounts of social media data, these models can help researchers track and analyze public sentiment changes to better understand psycho-social dynamics.</p>
<p>Finally, in psychological research, LLMs can drastically improve research efficiency.Researchers can use these models to quickly organize and analyze large amounts of literature, thus saving time.These models can also assist with experimental design, data analysis, and even promoting scholarly communication, making psychological research more efficient and precise.</p>
<p>In light of the above, LLMs have promising applications for psychology, such as research support, cognitive modeling, individualized intervention, and personalized learning.LLMs also have the potential to dramatically improve our understanding of human communication, thought processes, and behaviors, leading to the development of more comprehensive theories of mind and cognitive science.At the same time, it is important to be aware of the related risks and challenges and to ensure adherence to ethical standards, especially with regard to individual privacy and data security.It is also important to recognize that no matter how technologically advanced they are, LLMs can only partially replace the judgment and experience of human professionals.Therefore, such models should be viewed as an aid rather than an all-in-one solution.</p>
<p>documented LLMs' potential for creativity, and Patel and Fan (2023) demonstrated their emotion-recognition abilities.Taken together, such findings highlight the expanding role of LLMs in representing and augmenting human cognitive and social functions, marking significant progress in AI research.</p>
<p>Fig. 1
1
Fig. 1 LLMs in Psychological Research Across Timescales.(a) Domains (e.g., Cognitive &amp; Behavioral, Social &amp; Cultural) mapped to timescales of behavior; (b) Emergent properties (e.g., cognitive simulation) enabling domain-specific modeling; (c) LLMs as research tools (e.g., stimuli generation).Double-sided arrows indicate that emergent properties bridge domains and tools, supporting applications (e.g., memory retrieval) and refining properties through usage.</p>
<p>designed an AI system offering real-time empathic feedback, which improved overall empathy by 19.6% and significantly boosted self-efficacy among users struggling to provide support.Furthermore, J. M.Liu et al. (2023)    evaluated ChatCounselor, a model trained on a domain-specific dataset of psychologist-client conversations, and found it outperformed open-source models, thereby demonstrating the importance of domain-specific training for improving counseling capabilities.</p>
<p>Table 1
1
Applications of large language models (LLMs) in cognitive and behavioral psychology study.
Binz Schulz (2023b)andHow does GPT-3 perform on cognitive psychology tasks, including decision-making, information search, and causal reasoning?GPT-3 was tested using canonical cognitive psychology experiments and compared to human performance.GPT-3 excels in decision-making and reinforcement learning but struggles with task perturbations, directed exploration, and causal reasoning.Can GPT-3 generate creativeStevenson etsolutions comparable to humans inal. (2022)Guilford's Alternative Uses Test(AUT)?</p>
<p>Table 2 summarizes the applications of LLMs to clinical and counseling psychology.</p>
<p>The above research cases, which demonstrate LLMs' ability to provide clinicians with adequate mental health support</p>
<p>Table 2
2
Applications of LLMs in clinical and counseling psychology studies.
ReferencesResearch questionResearch methodKey findingPsychological Assessment Using LLMsThe study evaluated ChatGPT'sThe study found that ChatGPT consistentlyElyoseph and Levkovich (2023)How effective and accurate are ChatGPT in assessing suicide risk?analysis of a text vignette depicting a hypothetical patient with varied psychological states, comparing its assessments to those of mental healthunderestimated suicide risk and mental resilience compared to mental health professionals, suggesting that reliance on ChatGPT for suicide risk assessment couldprofessionals.lead to inaccurately low evaluations.Elyoseph and Levkovich (2024)How do LLMs compare to mental health professionals in assessing schizophrenia prognosis and treatment outcomes?Vignettes were used to compare the assessments of four LLMs (GPT-3.5, GPT-4, Bard, Claude) against professional and public benchmarks.GPT-4, Bard, and Claude aligned with professional views on treated cases, while GPT-3.5 was overly pessimistic.D'Souza et al. (2023)How effective is ChatGPT (3.5) in addressing clinical psychiatric cases and supporting mental health care?ChatGPT was tested on 100 clinical psychiatric vignettes, and expert psychiatrists graded its responses across 10 categories.ChatGPT excelled in management and diagnosis, earning top grades in 61% of cases, with no major errors but minor discrepancies in complex cases.Sufyan et al. (2024)How intelligence (SI) levels of do the social LLMs compare to human psychologists?Social intelligence scores of ChatGPT (4), Bard, and Bing were compared with 180 counseling psychology students (bachelor's and PhD levels).ChatGPT surpassed all psychologists in social intelligence, Bing outperformed most bachelor's and some PhDs, while Bard aligned with bachelor's students but fell behind PhDs.Psychological Interventions with LLMsBlyler Seligman (2023a)andCan personalized GPT-4 strategies based on narrative generate therapeutic identity?GPT-4 analyzed five narrative identities to recommend tailored interventions.GPT-4 effectively crafted personalized strategies, demonstrating its potential as a supportive tool for therapy and coaching.BlylerandCan accurate and insightful GPT-4 generateGPT-4 processed 50 stream-of-consciousness thoughts from 2696% of participants rated the narratives as accurate, and 73% reported gaining new self-Seligmanpersonalnarrativestoparticipants to create personalizedinsights, suggesting GPT-4's potential for(2023b)support self-discovery innarratives, which participants evaluatedenhancing self-discovery in therapeutictherapy and coaching?for accuracy, surprise, and self-insight.contexts.Sharma et al. (2023)Can AI enhance empathy in peer-to-peer mental health support?Tested HAILEY which based on GPT-2, which offering real-time empathic feedback, in a trial with 300 peer supporters on TalkLife.HAILEY improved empathy by 19.6% overall and 38.9% for those struggling with support, boosting self-efficacy without creating reliance.J. M. Liu et al. (2023)How to improve LLM in providing mental health support compared to other models?ChatCounselor, trained on the Psych8k dataset of 260 psychologist-client conversations, was evaluated using the Counseling Bench with real-world counseling questions and psychological metrics.ChatCounselor ChatGLM and performance, highlighting the impact of outperforms LLaMA, approaches GPT-4's domain-specific training on counseling capabilities.</p>
<p>Table 3
3
Applications of LLMs in educational and developmental psychology studies.
ReferencesResearch questionResearch methodKey findingDevelopmental Research with LLMsKosinski (2024)Can LLMs demonstrate theory of mind (ToM) abilities?Eleven different LLMs were tested on 40 false-belief tasks, requiring success in eight related scenarios per task. Performance was compared across model versions.GPT-4 comparable to 6-year-old children, while achieved 75% accuracy, older models performed significantly worse.Vzorinab etHow does GPT-4's emotional intelligence align withal. (2024)developmental patterns inhuman emotional reasoning?</p>
<p>to learn about its technical aspects.
ChatGPT supports learning throughmotivating feedback but often providessuperficial,inconsistent,andcontradictoryresponses,riskingoverestimation of knowledge.Jyothy et al. (2024)What factors influence the adoption of LLMs like ChatGPT in learning, teaching, and research?The Fogg Behavior Model (FBM) was applied to analyze the motivations, abilities, and perceptions of students, teachers, and researchers toward LLM use.User motivation and ability drive LLM adoption, but limitations like teacher hesitance and technical challenges hinder broader integration.Logacheva etCan personalized GPT-4generate programmingGPT-4-generated evaluated inexercises an introductory wereGPT-4 effectively produced high-quality, engaging exercises, offeringal. (2024)exercises to enhance studentprogramming course by students andpersonalized and scalable practiceengagement and learning?instructors for quality and engagement.materials for programming education.Machin et al. (2024)Can psychological GPT comparable to subject matter demonstrate literacy experts (SMEs) in psychology research methods?GPT rated 13 research scenarios, and its responses were statistically compared to SME evaluations.GPT showed strong alignment with SME ratings (r = .73-.80), indicating its potential to match SME-level psychological literacy.GhafouriCan a ChatGPT-based rapport-building protocol (CGRBP)A 16-week experimental study compared 30 EFL learners (15 experimental, 15CGRBP significantly improved L2 grit, demonstrating its potential to foster(2024)enhance L2 (Second Language)control) using pre-test post-test ANCOVAemotionalsupportandlearninggrit in English learners?analysis.motivation.Ghafouri etCan ChatGPT match expertal. (2024)psychological evaluating research methods? literacy in</p>
<p>Table 4
4
Applications of LLMs in social and cultural psychology studies.
ReferencesResearch questionResearch methodKey findingCultural and Cognitive Understanding with LLMsDo LLMs exhibit biasesLLMs closely align with WEIRDAtari et al. (2023)toward WEIRD (Western, Educated, Industrialized, Rich, Democratic) societies inLLMs' responses on psychological measures were compared to cross-cultural human data.cognitive patterns but show declining accuracy with non-WEIRD populations (r = -0.70), revealing apsychological tasks?WEIRD bias.Jin et al. (2023)Does GPT exhibit cultural cognitive traits aligned with Eastern or Western thinking?</p>
<p>HayesPotential biases: if the training data contain biases, LLMs may learn and replicate them.Data Text generated using LLMs may involve data privacy and consent issues.Output may be non-humanly understandable: although LLMs generate text that closely resembles human language, they do not truly understand the content and may generate absurd or misleading responses.Bias and discrimination: LLMs may be affected by biases in the training data, which can produce unfair results, such as reinforcing sexism in the translation of job advertisements.Responsibility and control: Due to the complexity of language models, it is difficult to determine who is responsible for the model's output, which can lead to attribution of problems and lack of controls.In order to better align models with human values, algorithmic improvements are needed to increase factual accuracy and robustness against adversarial samples.In addition, appropriate values need to be made explicit for different usage scenarios.Societal Impact: Widespread use of LLMs may lead to problems such as information leakage and amplification of bias.
Outputs privacy and consent issues: Training Data Bias (2023) Miotto et al.(2022)Propagation of HarmBender etPotential Harm: LLMs may lead to the propagation of harmful ideas such as stereotyping,al. (2021)discrimination, and extremism, and may lead to misinformation and bullying when generatingtext. Data bias and unfairness: leading to potential harm to marginalized communities.Automating bias: exacerbating existing biases and discrimination. Enhancement of authoritativeviewpoints: LLMs may reinforce dominant viewpoints in the training data, further underminingmarginalized people.Alignment Challenges Alignment: Misuse of LLMs Tamkin et al. (2021) Brown et al.</p>
<p>Table 7
7
Future directions and emergent trends of LLMs in psychological applications.Focus on the costs of adopting alternative human narratives in cognitive science research, such as masking the human labor behind them and the impact on human well-being.Concern about the impact of technological developments on scientific work and human understanding to ensure that cognitive scientists remain proactive in technological advances.
AuthorFuture directions and emergent trendsCognition and BehaviorD'Oria (2023)Delving into Human-Computer Interaction (HCI) to understand AI's ability to mimic human behavior. Exploring howAI language modeling can be applied in the human sciences to improve research efficiency and qualityCrockett andMesseri (2023)BinzandSchulz(2023b)</p>
<p>An overview of the features of chatbots in mental health: A scoping review. A A Abd-Alrazaq, M Alajlani, A A Alalwan, B M Bewick, P Gardner, M Househ, 10.1016/j.ijmedinf.2019.103978International Journal of Medical Informatics. 1321039782019</p>
<p>Perils and opportunities in using large language models in psychological research. S Abdurahman, M Atari, F Karimi-Malekabadi, M J Xue, J Trager, P S Park, . . Dehghani, M , PNAS nexus. 37e2452024</p>
<p>Cognitive network science reveals bias in gpt-3, gpt-3.5 turbo, and gpt-4 mirroring math anxiety in high-school students. K Abramski, S Citraro, L Lombardi, G Rossetti, M Stella, Big Data and Cognitive Computing. 731242023</p>
<p>S Agrawal, 10.48550/arxiv.2303.12810Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. 2023arXiv preprint</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. G Aher, R I Arriaga, A T Kalai, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningHonolulu, Hawaii, USA2023</p>
<p>Playing repeated games with Large Language Models. E Akata, L Schulz, J Coda-Forno, S J Oh, M Bethge, E Schulz, 10.48550/arXiv.2305.168672023arXiv preprint</p>
<p>Impact of ChatGPT on Learning Motivation. J K M Ali, M A A Shamsan, T A Hezam, A A Q Mohammed, 10.56540/jesaf.v2i1.51Journal of English Studies in Arabia Felix. 212023</p>
<p>Exploring the psychology of LLMs' moral and legal reasoning. G F Almeida, J L Nunes, N Engelmann, A Wiegmann, M De Araújo, Artificial intelligence. 3331041452024</p>
<p>Out of One, Many: Using Language Models to Simulate Human Samples. L P Argyle, E C Busby, N Fulda, J Gubler, C Rytting, D Wingate, 10.48550/arXiv.2209.068992022arXiv preprint</p>
<p>. M Atari, M J Xue, P S Park, D E Blasi, J Henrich, 10.31234/osf.io/5b26t2023Which Humans? PsyArXiv preprint</p>
<p>OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare. Ö Aydın, E Karaarslan, 10.2139/ssrn.4308687Emerging Computer. 22022</p>
<p>Effective learning with a personal AI tutor: A case study. A Baillifard, M Gabella, P B Lavenex, C S Martarelli, Education and Information Technologies. 2024</p>
<p>Machine-Assisted Social Psychology Hypothesis Generation. S Banker, P Chatterjee, H Mishra, A Mishra, 10.31234/osf.io/kv6f72023PsyArXiv preprint</p>
<p>Machine-assisted social psychology hypothesis generation. S Banker, P Chatterjee, H Mishra, A Mishra, American psychologist. 7967892024</p>
<p>ChatGPT-3.5 as writing assistance in students' essays. Ž Bašić, A Banovac, I Kružić, I Jerković, 10.1057/s41599-023-02269-7Humanities and Social Sciences Communications. 1102023</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event, Canada. the 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event, Canada2021</p>
<p>Turning large language models into cognitive models. M Binz, E Schulz, 10.48550/arXiv.2306.039172023aarXiv preprint</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 10.1073/pnas.22185231202023b120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>AI assistance for coaches and therapists. A P Blyler, M E P Seligman, The Journal of Positive Psychology. 2023a</p>
<p>Personal narrative and stream of consciousness: an AI approach. A P Blyler, M E P Seligman, 10.1080/17439760.2023.2257666The Journal of Positive Psychology. 2023b</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, D …amodei, 10.48550/arXiv.2005.14165Language Models are Few-Shot Learners. 2020arXiv preprint</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E K Horvitz, E Kamar, P Lee, Y T Lee, Y.-F Li, S M Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, 10.48550/arXiv.2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv preprint. 2023</p>
<p>Generation next: Experimentation with ai. G Charness, B Jabarian, J A List, 2023</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. C.-H Chiang, H.-Y Lee, 10.48550/arXiv.2305.019372023arXiv preprint</p>
<p>Should large language models replace human participants?. M Crockett, L Messeri, 10.31234/osf.io/4zdx92023PsyArXiv preprint</p>
<p>Can AI Language Models Improve Human Sciences Research? A Phenomenological Analysis and Future Directions. M D'oria, 10.6092/issn.1825-8670/16554Encyclopaideia. 27662023</p>
<p>Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes. D' Souza, R F Amanullah, S Mathew, M Surapaneni, K , Asian Journal of Psychiatry. 891037702023</p>
<p>A Dynamic Systems Theory approach to second language acquisition. De Bot, K Lowie, W Verspoor, M , 10.1017/S1366728906002732Bilingualism: Language and Cognition. 1012007</p>
<p>Using large language models in psychology. D Demszky, D Yang, D S Yeager, C J Bryan, M Clapper, S Chandhok, J C Eichstaedt, C Hecht, J Jamieson, M Johnson, M Jones, D Krettek-Cobb, L Lai, N Jonesmitchell, D C Ong, C S Dweck, J J Gross, J W Pennebaker, 10.1038/s44159-023-00241-5Nature Reviews Psychology. 2112023</p>
<p>From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing. I Dergaa, K Chamari, P Zmijewski, H Ben Saad, 10.5114/biolsport.2023.125623Biology of Sport. 4022023</p>
<p>S Dhingra, M Singh, V Sb, N Malviya, S Singh Gill, 10.48550/arXiv.2303.11436arXiv:2303.11436Mind meets machine: Unravelling GPT-4's cognitive psychology. 2023arXiv preprint</p>
<p>Can AI language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, 10.1016/j.tics.2023.04.008Trends in Cognitive Sciences. 2772023</p>
<p>AI-generated visuals of car-free US cities help improve support for sustainable policies. Y Du, D Luo, R Yan, X Wang, H Liu, H Zhu, Y Song, J Zhang, R Dubey, M D Hardy, T L Griffiths, R Bhui, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 20247Enhancing job recommendation through llm-based generative adversarial networks</p>
<p>Beyond human expertise-the promise and limitationsof ChatGPT in suicide risk assessment. Z Elyoseph, I Levkovich, 10.3389/fpsyt.2023.1213141Frontiers in Psychiatry. 142023</p>
<p>Comparing the perspectives of generative AI, mental health experts, and the general public on schizophrenia recovery: case vignette study. Z Elyoseph, I Levkovich, Jmir Mental Health. 11e530432024</p>
<p>Friend or foe? Exploring the implications of large language models on the science system. B Fecher, M Hebing, M Laufer, J Pohle, F Sofsky, 10.1007/s00146-023-01791-12023AI &amp; Society</p>
<p>GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines. L Floridi, M Chiriatti, 10.1007/s11023-020-09548-1202030</p>
<p>Baby steps in evaluating the capacities of large language models. M C Frank, 10.1038/s44159-023-00211-xNature Reviews Psychology. 282023</p>
<p>ChatGPT: The catalyst for teacher-student rapport and grit development in L2 class. M Ghafouri, System. 1201032092024</p>
<p>From virtual assistant to writing mentor: Exploring the impact of a ChatGPT-based writing instruction protocol on EFL teachers' self-efficacy and learners' writing skill. M Ghafouri, J Hassaskhah, A Mahdavi-Zafarghandi, 2024. 13621688241239764Language Teaching Research</p>
<p>Education and thinking: The role of knowledge. R Glaser, 10.48550/arXiv.2309.10371arXiv:2309.10371Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs. B Goertzel, 1984. 202339arXiv preprint</p>
<p>Generative adversarial networks. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Communications of the ACM. 63112020</p>
<p>IS THE WORLD READY FOR AI-POWERED THERAPY?. I Graber-Stiehl, 10.1038/d41586-023-01473-4Nature. 6172023</p>
<p>AI and the transformation of social science research. I Grossmann, M Feinberg, D C Parker, N A Christakis, P E Tetlock, W A Cunningham, 10.1126/science.adi1778Science. 38066502023</p>
<p>Large language models for mental health applications: Systematic review. Z Guo, A Lai, J H Thygesen, J Farrington, T Keen, K Li, JMIR mental health. 111e574002024</p>
<p>From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. M Gupta, C Akiri, K Aryal, E Parker, L Praharaj, 10.1109/access.2023.3300381IEEE Access. 112023</p>
<p>T Hagendorff, 10.48550/arXiv.2303.13988Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. 2023arXiv preprint</p>
<p>Deception abilities emerged in large language models. T Hagendorff, Proceedings of the National Academy of Sciences. 12124e23179671212024</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNature Computational Science. 3102023</p>
<p>AI language models cannot replace human research participants. J Harding, W D'alessandro, N G Laskowski, R Long, 10.1007/s00146-023-01725-x2023AI &amp; Society</p>
<p>Large language models meet cognitive science: Llms as tools, models, and participants. M Hardy, I Sucholutsky, B Thompson, T Griffiths, 10.31235/osf.io/yms8pConversing" with Qualitative Data: Enhancing Qualitative Research through Large Language Models (LLMs). Hayes, A.2023. 2023Proceedings of the annual meeting of the cognitive science society. PsyArXiv preprint</p>
<p>R Hendel, M Geva, A Globerson, 10.48550/arXiv.2310.15916arXiv:2310.15916Context Learning Creates Task Vectors. 2023arXiv preprint</p>
<p>The Efficacy of Cognitive Behavioral Therapy: A Review of Meta-analyses. S G Hofmann, A Asnaani, I J Vonk, A T Sawyer, A Fang, 10.1007/s10608-012-9476-1Cognit Ther Res. 3652012</p>
<p>A Holtzman, P West, L Zettlemoyer, 10.48550/arXiv.2308.00189Generative Models as a Complex Systems Science: How can we make sense of large language model behavior? arXiv preprint. 2023</p>
<p>D Hothersall, B J Lovett, History of psychology. Cambridge University Press2022</p>
<p>J Huang, K C Chang, -C , 10.48550/arXiv.2212.10403Towards Reasoning in Large Language Models: A Survey. 2022arXiv preprint</p>
<p>Doing research with human subjects is costly and cumbersome.Can AI chatbots replace them?. M Hutson, 10.1126/science.adj6791Science. 38166542023</p>
<p>The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?. C Jin, S Zhang, T Shu, Z Cui, 10.48550/arXiv.2308.142422023arXiv preprint</p>
<p>Using ChatGPT and Other Large Language Model (LLM) Applications for Academic Paper Assignments. A Jungherr, 2023</p>
<p>Exploring large language models as an integrated tool for learning, teaching, and research through the Fogg Behavior Model: a comprehensive mixed-methods analysis. S Jyothy, V K Kolil, R Raman, K Achuthan, Cogent Engineering. 11123534942024</p>
<p>Thinking, fast and slow. D Kahneman, 2011Farrar, Straus and Giroux</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, G …kasneci, 10.1016/j.lindif.2023.102274Learning and Individual Differences. 1032023</p>
<p>Pilot Selection in the Era of Virtual Reality: Algorithms for Accurate and Interpretable Machine Learning Models. L Ke, G Zhang, J He, Y Li, Y Li, X Liu, P Fang, 10.3390/aerospace10050394Aerospace. 5102023</p>
<p>Evaluating large language models in theory of mind tasks. M Kosinski, Proceedings of the National Academy of Sciences. 12145e24054601212024</p>
<p>Evaluation of ChatGPT for NLP-based Mental Health Applications. B Lamichhane, 10.48550/arXiv.2303.15727arXiv:2303.157272023arXiv preprint</p>
<p>An empirical investigation of the impact of ChatGPT on creativity. B C Lee, J Chung, Nature Human Behaviour. 8102024</p>
<p>Pretrained Language Models for Text Generation: A Survey. J Li, T Tang, W X Zhao, J.-Y Nie, J.-R Wen, 10.48550/arXiv.2201.05273arXiv:2201.052732022arXiv preprint</p>
<p>Ethical implications of ChatGPT in higher education: A scoping review. M Li, A Enkhtur, F Cheng, B A Yamamoto, 10.48550/arXiv.2311.143782023arXiv preprint</p>
<p>Scisafeeval: a comprehensive benchmark for safety alignment of large language models in scientific tasks. T Li, J Lu, C Chu, T Zeng, Y Zheng, M Li, H Huang, B Wu, Z Liu, K Ma, arXiv:2410.037692024arXiv preprint</p>
<p>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. X Li, Y Li, L Liu, L Bing, S Joty, 10.48550/arXiv.2212.105292022arXiv preprint</p>
<p>ChatCounselor: A Large Language Models for Mental Health Support. J M Liu, D Li, H Cao, T Ren, Z Liao, J Wu, 10.48550/arXiv.2309.15461arXiv:2309.154612023arXiv preprint</p>
<p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Computing Surveys. 5592023</p>
<p>P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. X Liu, K Ji, Y Fu, W Tam, Z Du, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandShort Papers20222</p>
<p>Summary of ChatGPT-Related research and perspective towards the future of large language models. Y Liu, T Han, S Ma, J Zhang, Y Yang, J Tian, H He, A Li, M He, Z Liu, Z Wu, L Zhao, D Zhu, X Li, N Qiang, D Shen, T Liu, B Ge, 10.1016/j.metrad.2023.100017Meta-Radiology. 122023</p>
<p>Challenging ChatGPT's "intelligence" with human tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language Model. R Loconte, G Orrù, M Tribastone, P Pietrini, G Sartori, 10.2139/ssrn.44718292023SSRN preprint</p>
<p>Potential Roles of Large Language Models in the Production of Systematic Reviews and Meta-Analyses. E Logacheva, A Hellas, J Prather, S Sarsa, J Leinonen, X Luo, F Chen, D Zhu, L Wang, Z Wang, H Liu, M Lyu, Y Wang, Q Wang, Y Chen, Proceedings of the 2024 ACM Conference on International Computing Education Research. the 2024 ACM Conference on International Computing Education Research2024. 20241e56780Evaluating Contextually Personalized Programming Exercises Created with Generative AI</p>
<p>Comparing ChatGPT With Experts' Responses to Scenarios that Assess Psychological Literacy. M A Machin, T M Machin, N Gasson, Psychology Learning &amp; Teaching. 2024. 14757257241241592</p>
<p>K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, 10.48550/arXiv.2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Large language models predict human sensory judgments across six modalities. R Marjieh, I Sucholutsky, P V Rijn, N Jacoby, T L Griffiths, 10.48550/arXiv.2302.013082023arXiv preprint</p>
<p>The significant cost of systematic reviews and meta-analyses: A call for greater involvement of machine learning to assess the promise of clinical trials. M Michelson, K Reuter, 10.1016/j.conctc.2019.100443Contemporary Clinical Trials Communications. 161004432019</p>
<p>Who is GPT-3? An Exploration of Personality, Values and Demographics. M Miotto, N Rossberg, B Kleinberg, 10.48550/arXiv.2209.143382022arXiv preprint</p>
<p>AI's challenge of understanding the world. M Mitchell, 10.1126/science.adm8175Science. 66713822023</p>
<p>Use of large language models to predict neuroimaging. L Nazario-Johnson, H A Zaki, G A Tung, 10.1016/j.jacr.2023.06.008Journal of the American College of Radiology. 20102023</p>
<p>Unified theories of cognition. A Newell, 1990Harvard University Press</p>
<p>Culture and systems of thought: holistic versus analytic cognition. R E Nisbett, K Peng, I Choi, A Norenzayan, 10.1037/0033-295X.108.2.291Psychological review. 10822001</p>
<p>Experimental evidence on the productivity effects of generative artificial intelligence. S Noy, W Zhang, Science. 38166542023</p>
<p>. Openai, 10.48550/arXiv.2303.087742023arXiv preprint</p>
<p>Human-like problem-solving abilities in large language models using ChatGPT. G Orru, A Piarulli, C Conversano, A Gemignani, 10.3389/frai.2023.1199350Frontiers in Artificial Intelligence. 611993502023</p>
<p>Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping by Large Language Models. R Pal, H Garg, S Patel, T Sethi, 10.1101/2023.03.22.232875852023medRxiv preprint</p>
<p>Rethinking the Link Between Categorization and Prejudice Within the Social Cognition Perspective. B Park, C M Judd, 10.1207/s15327957pspr0902_2Personality and Social Psychology Review. 922005</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. J S Park, L Popowski, C Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Diminished diversity-of-thought in a standard large language model. P S Park, P Schoenegger, C Zhu, Behavior Research Methods. 2024</p>
<p>Can ChatGPT be used to generate scientific hypotheses. Y J Park, D Kaplan, Z Ren, C.-W Hsu, C Li, H Xu, S Li, J Li, Journal of Materiomics. 1032024</p>
<p>Identification and Description of Emotions by Current Large Language Models. S C Patel, J Fan, 10.1101/2023.07.17.549421bioRxiv preprint. 2023</p>
<p>The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions. Y Peng, J Han, Z Zhang, L Fan, T Liu, S Qi, X Feng, Y Ma, Y Wang, S.-C Zhu, 10.1016/j.eng.2023.07.0062023</p>
<p>Large Language Models Can Infer Psychological Dispositions of Social Media Users. H Peters, S Matz, 10.48550/arXiv.2309.086312023arXiv preprint</p>
<p>Guidelines for ethical use and acknowledgement of large language models in academic writing. S Porsdam Mann, A A Vazirani, M Aboy, B D Earp, T Minssen, I G Cohen, J Savulescu, Nature Machine Intelligence. 2024</p>
<p>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation?. R Qureshi, D Shaughnessy, K A R Gill, K A Robinson, T Li, E Agai, 10.1186/s13643-023-02243-zSystematic Reviews. 121722023</p>
<p>Gemini versus ChatGPT: applications, performance, architecture, capabilities, and implementation. Performance, Architecture, Capabilities, and Implementation. N Rane, S Choudhary, J Rane, 2024. February 13, 2024</p>
<p>GPT is an effective tool for multilingual psychological text analysis. S Rathje, D.-M Mirea, I Sucholutsky, R Marjieh, C Robertson, J J V Bavel, 10.31234/osf.io/sekf52023PsyArXiv preprint</p>
<p>May the force of text data analysis be with you: Unleashing the power of generative AI for social psychology research. M Salah, H Al Halbusi, F Abdelfattah, 10.1016/j.chbah.2023.100006Computers in Human Behavior: Artificial Humans. 122023</p>
<p>M Sallam, 10.3390/healthcare11060887ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns. 202311</p>
<p>M Sap, R Lebras, D Fried, Y Choi, 10.48550/arXiv.2210.13312Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. 2022arXiv preprint</p>
<p>Language models and psychological sciences. G Sartori, G Orrù, 10.3389/fpsyg.2023.1279317Frontiers in Psychology. 142023</p>
<p>Exploring ChatGPT's Empathic Abilities. K Schaaff, C Reinig, T Schlippe, 10.48550/arXiv.2308.035272023arXiv preprint</p>
<p>Clinical science and practice in the age of large language models and generative artificial intelligence. S M Schueller, R R Morris, 10.1037/ccp0000848Journal of Consulting and Clinical Psychology. 91102023</p>
<p>Long-form analogies generated by chatGPT lack human-like psycholinguistic properties. S M Seals, V L Shalin, 10.48550/arxiv.2306.045372023arXiv preprint</p>
<p>Large Language Models and the Reverse Turing Test. T Sejnowski, 10.48550/arxiv.2207.143822022arXiv preprint</p>
<p>LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. H Sha, Y Mu, Y Jiang, L Chen, C Xu, P Luo, Eben Li, S Tomizuka, M Zhan, W Ding, M , 10.48550/arXiv.2310.03026arXiv:2310.030262023arXiv preprint</p>
<p>Human-AI collaboration enables more empathic conversations in text-based peer-to-peer mental health support. A Sharma, I W Lin, A S Miner, D C Atkins, T Althoff, 10.1038/s42256-022-00593-2Nature Machine Intelligence. 512023</p>
<p>Information Processing Models of Cognition. H A Simon, 10.1146/annurev.ps.30.020179.002051Annual Review of Psychology. 3011979</p>
<p>Leveraging Cognitive Science for Testing Large Language Models. R Srinivasan, H Inakoshi, K Uchino, 2023 IEEE International Conference On Artificial Intelligence Testing (AITest). 2023</p>
<p>Large Language Models Could Change the Future of Behavioral Healthcare: A Proposal for Responsible Development and Evaluation. E C Stade, S W Stirman, L Ungar, C L Boland, H A Schwartz, D B Yaden, J Sedoc, R J Derubeis, R Willer, J C Eichstaedt, 10.31234/osf.io/cuzvr2023PsyArXiv preprint</p>
<p>Using cognitive psychology to understand GPT-like models needs to extend beyond human biases. M Stella, T T Hills, Y N Kenett, 10.1073/pnas.23129111202023120e2312911120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>C Stevenson, I Smal, M Baas, R Grasman, H V Maas, 10.48550/arXiv.2206.08932Putting GPT-3's Creativity to the (Alternative Uses) Test. 2022arXiv preprint</p>
<p>Learning with ChatGPT 3.5 as a more knowledgeable other: an autoethnographic study. A Stojanov, 10.1186/s41239-023-00404-7International Journal of Educational Technology in Higher Education. 2012023</p>
<p>AI bot ChatGPT writes smart essays -should professors worry?. C Stokel-Walker, 10.1038/d41586-022-04397-7Nature. 2022</p>
<p>Artificial intelligence and social intelligence: preliminary comparison study between AI models and psychologists. N S Sufyan, F H Fadhel, S S Alkhathami, J Y Mukhadi, Frontiers in Psychology. 1513530222024</p>
<p>Do large language models show decision heuristics similar to humans? A case study using GPT-3.5. G Suri, L R Slater, A Ziaee, M Nguyen, Journal of Experimental Psychology: General. 2024</p>
<p>Social psychology of intergroup relations. H Tajfel, Annual Review of Psychology. 3311982</p>
<p>Challenging the appearance of machine intelligence. A N Talboy, E Fuller, 10.48550/arXiv.2304.01358Cognitive bias in LLMs. 2023arXiv preprint</p>
<p>Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models. A Tamkin, M Brundage, J Clark, D Ganguli, 10.48550/arXiv.2102.025032021arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, 10.1038/s41591-023-02448-8Nature Medicine. 2982023</p>
<p>Automating psychological hypothesis generation with AI: when large language models meet causal graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 10.1057/s41599-024-03407-5Humanities and Social Sciences Communications. 18962024</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Do large language models know what humans know?. S Trott, C Jones, T Chang, J Michaelov, B Bergen, Cognitive Science. 477e133092023</p>
<p>ChatGPT: five priorities for research. E A Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, 10.1038/d41586-023-00288-7Nature. 61479472023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>The Emotional Intelligence of the GPT-4 Large Language Model. G D Vzorinab, A M Bukinichac, A V Sedykha, I I Vetrovab, E A Sergienkob, 202417Psychology in Russia: State of the art</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, A Anandkumar, K Bergen, C P Gomes, S Ho, P Kohli, J Lasenby, J Leskovec, T Y Liu, A Manrai, M …zitnik, 10.1038/s41586-023-06221-2Nature. 62079722023</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, 10.1038/s41562-023-01659-wNature Human Behaviour. 792023</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 10.48550/arXiv.2206.07682Emergent Abilities of Large Language Models. 2022arXiv preprint</p>
<p>Palr: Personalization aware llms for recommendation. F Yang, Z Chen, Z Jiang, E Cho, X Huang, Y Lu, arXiv:2305.076222023arXiv preprint</p>
<p>I Yildirim, L A Paul, 10.48550/arXiv.2310.04276arXiv:2310.04276From task structures to world models: What do LLMs know? arXiv preprint. 2023</p>
<p>AI chatbot responds to emotional cuing. Z Yukun, L Xu, Z Huang, K Peng, M Seligman, E Li, F Yu, 10.31234/osf.io/9ymfz2023PsyArXiv preprint</p>
<p>Visualizing and Understanding Convolutional Networks. M Zeiler, 2014</p>
<p>J Zhang, X Xu, S Deng, 10.48550/arXiv.2310.02124arXiv:2310.02124Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View. 2023arXiv preprint</p>
<p>Z Zhang, G Chadwick, H Mcnally, Y Zhao, R Mullins, arXiv:2310.04535Llm4dv: Using large language models for hardware test stimuli generation. 2023arXiv preprint</p>
<p>Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. Y Zhao, Z Huang, M Seligman, K Peng, Scientific Reports. 14170952024</p>
<p>Large Language Models for Scientific Synthesis, Inference and Explanation. Y Zheng, H Y Koh, J Ju, A T N Nguyen, L T May, G I Webb, S Pan, 10.48550/arXiv.2310.079842023arXiv preprint</p>
<p>The Artificial intelligence large language models and neuropsychiatry practice and research ethic. Y Zhong, Y J Chen, Y Zhou, Y A Lyu, J J Yin, Y J Gao, 10.1016/j.ajp.2023.103577Asian Journal of Psychiatry. 841035772023</p>
<p>Y Zhuang, Q Liu, Y Ning, W Huang, R Lv, Z Huang, G Zhao, Z Zhang, Q Mao, S Wang, E Chen, 10.48550/arXiv.2306.10512Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. 2023arXiv preprint</p>
<p>C Ziems, W Held, O Shaikh, J Chen, Z Zhang, D Yang, 10.48550/arXiv.2305.03514Can Large Language Models Transform Computational Social Science? arXiv preprint. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-977 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-977</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-977</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-ee45636e012703cf90e7c367e9db6348f2d8a39e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ee45636e012703cf90e7c367e9db6348f2d8a39e" target="_blank">Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A Probabilistic programming library, LLaMPPL, is presented, for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers, to facilitate experimentation with SMC steering.</p>
                <p><strong>Paper Abstract:</strong> Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (https://github.com/probcomp/hfppl), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e977.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e977.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMC steering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Monte Carlo (SMC) Transformer Steering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time decoding method that frames constrained language generation as posterior inference in probabilistic programs and approximates the posterior with a specialized sequential Monte Carlo algorithm to steer LLM outputs toward syntactic and semantic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SMC Transformer Steering</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SMC steering treats constrained generation tasks as posterior inference in a class of probabilistic sequence models (Feynman-Kac Transformer models). It maintains N weighted particles (partial token sequences), repeatedly expands each active particle K times using Markov kernels M_t (parameterized by an LLM), reweights with potential functions G_t encoding constraints or likelihood terms, and performs a without-replacement resampling step to produce a new set of N particles. Implementation optimizations include a shared CachedTransformer with a token trie and KV-caching to avoid re-evaluating Transformer contexts across particles and time-steps.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic sequence model / Feynman-Kac Transformer (probabilistic program over token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States are token sequences (prefixes and EOS-terminated strings). Transitions are defined by Markov kernels M_t that sample next-token(s) using Transformer logits; potentials G_t score transitions (e.g., enforce hard/soft constraints, compute importance weights). The overall model defines filtering posteriors P_t and a posterior over complete EOS-terminated strings. The model is explicitly probabilistic (Monte Carlo particle representation of posterior distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Provides next-token conditional distributions (transition proposals) and scores used in potentials; defines priors/proposals that the SMC algorithm samples and corrects via importance weights.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMA-family (as used via LLaMPPL / LLaMA Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>LLM predictive uncertainty and state/posterior uncertainty over token sequences (belief over completions)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Sequential Monte Carlo (particle filtering), importance weights (potentials), without-replacement resampling; produces unbiased estimates of marginal likelihood (partition function) and posterior expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Sequential Monte Carlo (particle filtering / SMC with tailored resampling and importance weighting) — not a classical search planner (e.g., not A*/MCTS), but a probabilistic search over sequence-space.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>General text generation/problem specification tasks (prompt intersection, infilling, constrained generation)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Not a single standard benchmark; the paper demonstrates SMC steering on tasks framed as posterior inference over text: constrained generation (syntactic/lexical constraints), infilling templates with known fragments, and prompt intersection (completions jointly likely under multiple prompts). These are text-only generation tasks rather than interactive text-game benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased marginal likelihood estimates (partition function) and E[log \hat{Z}] across runs; qualitative assessment of samples and comparison of estimator tightness across proposal choices and particle counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively and via E[log \hat{Z}] to beam search, greedy decoding/token-masking, and to different choices of Feynman-Kac proposal (e.g., single-prompt proposal vs. product-of-experts proposal); no numeric baseline success rates on standardized text-based planning benchmarks are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper compares two alternative Feynman-Kac formulations (different M_t proposals and corresponding G_t potentials) for the prompt-intersection task and shows the proposal closer to the posterior (using logits from all prompts) yields better E[log \hat{Z}] and qualitatively better samples; this is an analysis of proposal design rather than an ablation specifically labeled as 'uncertainty modeling on/off.'</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SMC steering can enforce hard and soft constraints and sample from posteriors defined by probabilistic programs mixing LLMs and symbolic logic, with compute overhead similar to beam search; choice of proposal distribution (M_t) critically affects inference efficiency and accuracy, and SMC integrates LLM uncertainty by maintaining a particle belief over completions and using importance weights (potentials) to correct proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e977.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e977.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMPPL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMPPL (LLama Probabilistic Programming Library)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python probabilistic programming library for specifying language generation tasks as probabilistic programs that invoke LLaMA-family Transformers, and automating SMC steering for those programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLaMPPL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLaMPPL provides a Model subclass API where the user implements a step method manipulating a string-valued state self.s and uses primitives like sample(dist[, proposal]), observe(dist, val), condition(bool), transformer(context), and finish(). A LLaMPPL program implicitly defines a Feynman-Kac Transformer model (initial state s0, Markov kernels M_t induced by sample/proposal/transformer calls, and potentials G_t induced by importance weights, observes, and conditions). LLaMPPL integrates a CachedTransformer for efficient multi-particle inference and automates SMC steering.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic program implementing Feynman-Kac Transformer models (probabilistic symbolic sequence/world model over token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Programs manipulate an explicit token-sequence state self.s. Sampling statements (with optional proposals) induce M_t transitions; observe/condition statements and importance-weighted proposals define potentials G_t. The representation is probabilistic; the program specifies symbolic constraints and likelihoods over sequences, and the overall posterior is computed via sequential Monte Carlo.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Parameterized Transformer (LLaMA-family) supplies next-token distributions used by sample/transformer primitives and by potential computations; LLaMPPL treats the LLM as a probabilistic primitive inside the program.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMA-family Transformers (as the default backend in the paper's implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>LLM predictive uncertainty and posterior (belief) over textual completions induced by the probabilistic program</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Monte Carlo (SMC) particle approximation of posteriors, importance weights from proposals and observes/conditions, unbiased marginal-likelihood estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Uses SMC steering for inference/decision-making over sequences; proposals can be augmented by heuristics or LLM prompts (the paper notes potential for chain-of-thought proposals and auxiliary prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Program-defined text-generation tasks (no single standard benchmark in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>LLaMPPL targets general generation tasks expressed as probabilistic programs: constrained generation, infilling, prompt intersection, and other compositional constrained text generation tasks; these are not interactive text-game benchmarks but are flexible text environments expressible via the library's primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Quality of posterior approximation (e.g., E[log \hat{Z}]), qualitative sample quality, computational cost comparable to beam search given caching optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to beam search, token-masking greedy decoding, and implementations using stateless APIs (e.g., GenGPT3.jl) where efficient one-token-at-a-time inference is impractical; empirical numeric baselines on standardized text-planning benchmarks are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper shows that different choices of proposals in LLaMPPL programs (i.e., different M_t) substantially affect estimator variance and sample quality; no explicit ablation toggling 'uncertainty modeling' on/off is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLaMPPL makes it straightforward to express constrained generation and composition of prompts as probabilistic programs, and when paired with SMC steering and caching it enables tractable, posterior-aware constrained generation using LLMs; efficient proposals that approximate the posterior yield much better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e977.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e977.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feynman-Kac Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feynman-Kac Transformer Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal probabilistic model for constrained generation where a Transformer-parameterized Markov kernel M_t and potential functions G_t (both using Transformer scores) define a Feynman-Kac model over token sequences whose posterior is targeted by SMC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Feynman-Kac Transformer model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mathematical formalism: tuples (s0, {M_t}, {G_t}) where s0 is initial string, M_t(s_t | s_{t-1}, f_theta) are Transformer-parameterized Markov kernels that extend strings, and G_t(s_{t-1}, s_t, f_theta) are non-negative potentials encoding constraints/likelihoods; together they define filtering posteriors P_t and a posterior over complete EOS-terminated strings. Multiple equivalent M_t/G_t factorizations can represent the same posterior (allowing importance-sampling-style proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic symbolic sequence/world model (Feynman-Kac formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>State space: token sequences (prefixes and EOS-terminated strings). Actions/transitions: token-addition kernels M_t, parameterized by Transformer logits (probabilistic). Potentials G_t assign nonnegative scores to transitions to encode constraints or to correct proposals (importance weights). The model is explicitly probabilistic; the posterior over full sequences is obtained by reweighting trajectories of the base Markov chain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>LLM f_theta parameterizes transition kernels M_t and contributes to potentials G_t used for scoring and importance weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMA-family (in the paper's experiments / implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Posterior/belief uncertainty over sequence completions and predictive uncertainty from the Transformer logits</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Belief represented via sequential filtering/posterior distributions; approximated by SMC particle sets with importance weights and resampling.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Posterior sampling via Sequential Monte Carlo (SMC); formulation supports proposal design to reduce variance (akin to guided planning), but is not a classical symbolic planner (e.g., PDDL-based).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Task-specified text generation problems (constrained generation, infilling, prompt intersection)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Abstract textual environments defined by programmatic constraints and observations; complexity is user-specified via potentials and program logic rather than a pre-defined benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Quality of posterior approximation (E[log \hat{Z}]), sample diversity and constraint satisfaction; computational cost relative to beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasted with token-masking (greedy) decoding and beam search approaches — emphasizes that SMC aims to sample from the posterior (not optimize) and can avoid greedy dead-ends and length bias of beam search; no numeric standardized benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Analysis shows that choosing M_t proposals closer to the posterior reduces variance and yields better E[log \hat{Z}] and sample quality (illustrated in prompt-intersection experiment comparing two formulations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Feynman-Kac formulation provides a flexible, compositional way to express constraints and composite prompt objectives as probabilistic (symbolic) models over sequences; it makes explicit how to design proposals and potentials to trade off computational efficiency and posterior accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e977.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e977.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenGPT3.jl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenGPT3.jl (GPT-3 as a generative function in Gen.jl)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A library integrating OpenAI's GPT-3 into the Gen.jl probabilistic programming system, enabling use of Gen's posterior inference tools for models that include GPT-3 as a generative primitive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GenGPT3.jl: GPT-3 as a generative function in Gen.jl</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GenGPT3.jl (Gen.jl integration)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Wraps GPT-3 as a generative function callable from Gen.jl, allowing probabilistic programs to include GPT-3-sampled texts and to apply Gen's posterior inference methods (e.g., for structured infilling and constrained semantic parsing). The paper notes GenGPT3.jl includes examples that perform posterior inference but that SMC steering's caching and one-token-at-a-time efficiency would be difficult to implement with the stateless OpenAI API used by GenGPT3.jl.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic program with LLM primitives (Gen probabilistic programs over string-valued variables)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Probabilistic programs manipulate string-valued random variables returned by GPT-3; Gen's inference machinery can perform posterior conditioning/inference over program variables. Representation is probabilistic; not expressed in PDDL/PPDDL but as programmatic probabilistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>GPT-3 acts as the generative primitive producing candidate text that the probabilistic program reasons about; Gen performs posterior inference conditioned on observations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Predictive uncertainty of GPT-3 outputs and posterior uncertainty over program variables induced by observed data</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Gen.jl's posterior inference algorithms (various programmable inference strategies); the underlying LLM is treated as stochastic primitive.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>General-purpose probabilistic inference in Gen.jl (not a specialized planner); can be used to infer latent text fragments or other structured variables.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Structured infilling and constrained semantic parsing examples (as reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Examples are program-defined inference tasks over text (e.g., inferring which prompt caused an answer), not interactive text-game environments; the OpenAI API's statelessness limits efficient one-token-at-a-time posterior inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GenGPT3.jl demonstrates that LLMs can be embedded as primitives in probabilistic programs and that Gen's inference machinery can perform posterior inference in such models; however, API constraints (stateless calls) make certain SMC-style one-token incremental inference inefficient without caching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e977.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e977.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lampp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lampp: Language models as probabilistic priors for perception and action</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work that uses Transformer-based language models to provide probabilistic priors (world knowledge) for downstream tasks such as perception, semantic segmentation, and household navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lampp: Language models as probabilistic priors for perception and action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lampp</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Proposes using posterior distributions of probabilistic generative models that incorporate Transformers as components; LLMs provide priors or score proposals for tasks like semantic segmentation and navigation so that general world knowledge informs probabilistic inference over state/action variables. The paper references this as an example of using LLM-informed priors for perception and action.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic generative models combining LLM priors with task-specific state/action models (probabilistic, not classical symbolic PDDL/PPDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Combines LLM-derived priors over textual or semantic variables with probabilistic models for task-specific states and transitions (e.g., perception outputs or navigation state estimates); representational specifics vary by application and are probabilistic rather than classical PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Provides general-world knowledge priors or proposal/score information to inform probabilistic inference over perception and action variables.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State uncertainty and LLM predictive uncertainty used to form priors and posterior estimates over task variables (perception/action).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Probabilistic inference (posterior computation) where LLMs provide prior probabilities or scoring factors; exact inference method depends on the downstream task and is probabilistic (e.g., Bayesian inference).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Perception and household navigation tasks (cited as examples)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Not strictly text-based interactive environments; examples include semantic segmentation and household navigation where LLM priors are used to inform probabilistic models for perception and action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that posterior distributions from models that incorporate Transformers can be useful beyond pure text generation (e.g., perception and navigation), illustrating the potential of probabilistic programming tools like LLaMPPL to make such approaches scalable and accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging unstructured statistical knowledge in a probabilistic language of thought <em>(Rating: 2)</em></li>
                <li>GenGPT3.jl: GPT-3 as a generative function in Gen.jl <em>(Rating: 2)</em></li>
                <li>Lampp: Language models as probabilistic priors for perception and action <em>(Rating: 2)</em></li>
                <li>Language model cascades <em>(Rating: 1)</em></li>
                <li>Planning with large language models for code generation <em>(Rating: 1)</em></li>
                <li>On-line inference for hidden markov models via particle filters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-977",
    "paper_id": "paper-ee45636e012703cf90e7c367e9db6348f2d8a39e",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "SMC steering",
            "name_full": "Sequential Monte Carlo (SMC) Transformer Steering",
            "brief_description": "An inference-time decoding method that frames constrained language generation as posterior inference in probabilistic programs and approximates the posterior with a specialized sequential Monte Carlo algorithm to steer LLM outputs toward syntactic and semantic constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SMC Transformer Steering",
            "system_description": "SMC steering treats constrained generation tasks as posterior inference in a class of probabilistic sequence models (Feynman-Kac Transformer models). It maintains N weighted particles (partial token sequences), repeatedly expands each active particle K times using Markov kernels M_t (parameterized by an LLM), reweights with potential functions G_t encoding constraints or likelihood terms, and performs a without-replacement resampling step to produce a new set of N particles. Implementation optimizations include a shared CachedTransformer with a token trie and KV-caching to avoid re-evaluating Transformer contexts across particles and time-steps.",
            "world_model_type": "Probabilistic sequence model / Feynman-Kac Transformer (probabilistic program over token sequences)",
            "world_model_description": "States are token sequences (prefixes and EOS-terminated strings). Transitions are defined by Markov kernels M_t that sample next-token(s) using Transformer logits; potentials G_t score transitions (e.g., enforce hard/soft constraints, compute importance weights). The overall model defines filtering posteriors P_t and a posterior over complete EOS-terminated strings. The model is explicitly probabilistic (Monte Carlo particle representation of posterior distributions).",
            "uses_llm": true,
            "llm_role": "Provides next-token conditional distributions (transition proposals) and scores used in potentials; defines priors/proposals that the SMC algorithm samples and corrects via importance weights.",
            "llm_model_name": "LLaMA-family (as used via LLaMPPL / LLaMA Transformers)",
            "uncertainty_modeling": true,
            "uncertainty_type": "LLM predictive uncertainty and state/posterior uncertainty over token sequences (belief over completions)",
            "uncertainty_method": "Sequential Monte Carlo (particle filtering), importance weights (potentials), without-replacement resampling; produces unbiased estimates of marginal likelihood (partition function) and posterior expectations.",
            "planning_algorithm": "Sequential Monte Carlo (particle filtering / SMC with tailored resampling and importance weighting) — not a classical search planner (e.g., not A*/MCTS), but a probabilistic search over sequence-space.",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "General text generation/problem specification tasks (prompt intersection, infilling, constrained generation)",
            "text_environment_description": "Not a single standard benchmark; the paper demonstrates SMC steering on tasks framed as posterior inference over text: constrained generation (syntactic/lexical constraints), infilling templates with known fragments, and prompt intersection (completions jointly likely under multiple prompts). These are text-only generation tasks rather than interactive text-game benchmarks.",
            "performance_metric": "Unbiased marginal likelihood estimates (partition function) and E[log \\hat{Z}] across runs; qualitative assessment of samples and comparison of estimator tightness across proposal choices and particle counts.",
            "performance_value": null,
            "baseline_comparison": "Compared qualitatively and via E[log \\hat{Z}] to beam search, greedy decoding/token-masking, and to different choices of Feynman-Kac proposal (e.g., single-prompt proposal vs. product-of-experts proposal); no numeric baseline success rates on standardized text-based planning benchmarks are reported.",
            "has_ablation_uncertainty": false,
            "ablation_results": "The paper compares two alternative Feynman-Kac formulations (different M_t proposals and corresponding G_t potentials) for the prompt-intersection task and shows the proposal closer to the posterior (using logits from all prompts) yields better E[log \\hat{Z}] and qualitatively better samples; this is an analysis of proposal design rather than an ablation specifically labeled as 'uncertainty modeling on/off.'",
            "key_findings": "SMC steering can enforce hard and soft constraints and sample from posteriors defined by probabilistic programs mixing LLMs and symbolic logic, with compute overhead similar to beam search; choice of proposal distribution (M_t) critically affects inference efficiency and accuracy, and SMC integrates LLM uncertainty by maintaining a particle belief over completions and using importance weights (potentials) to correct proposals.",
            "uuid": "e977.0",
            "source_info": {
                "paper_title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLaMPPL",
            "name_full": "LLaMPPL (LLama Probabilistic Programming Library)",
            "brief_description": "A Python probabilistic programming library for specifying language generation tasks as probabilistic programs that invoke LLaMA-family Transformers, and automating SMC steering for those programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLaMPPL",
            "system_description": "LLaMPPL provides a Model subclass API where the user implements a step method manipulating a string-valued state self.s and uses primitives like sample(dist[, proposal]), observe(dist, val), condition(bool), transformer(context), and finish(). A LLaMPPL program implicitly defines a Feynman-Kac Transformer model (initial state s0, Markov kernels M_t induced by sample/proposal/transformer calls, and potentials G_t induced by importance weights, observes, and conditions). LLaMPPL integrates a CachedTransformer for efficient multi-particle inference and automates SMC steering.",
            "world_model_type": "Probabilistic program implementing Feynman-Kac Transformer models (probabilistic symbolic sequence/world model over token sequences)",
            "world_model_description": "Programs manipulate an explicit token-sequence state self.s. Sampling statements (with optional proposals) induce M_t transitions; observe/condition statements and importance-weighted proposals define potentials G_t. The representation is probabilistic; the program specifies symbolic constraints and likelihoods over sequences, and the overall posterior is computed via sequential Monte Carlo.",
            "uses_llm": true,
            "llm_role": "Parameterized Transformer (LLaMA-family) supplies next-token distributions used by sample/transformer primitives and by potential computations; LLaMPPL treats the LLM as a probabilistic primitive inside the program.",
            "llm_model_name": "LLaMA-family Transformers (as the default backend in the paper's implementation)",
            "uncertainty_modeling": true,
            "uncertainty_type": "LLM predictive uncertainty and posterior (belief) over textual completions induced by the probabilistic program",
            "uncertainty_method": "Monte Carlo (SMC) particle approximation of posteriors, importance weights from proposals and observes/conditions, unbiased marginal-likelihood estimation.",
            "planning_algorithm": "Uses SMC steering for inference/decision-making over sequences; proposals can be augmented by heuristics or LLM prompts (the paper notes potential for chain-of-thought proposals and auxiliary prompts).",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Program-defined text-generation tasks (no single standard benchmark in paper)",
            "text_environment_description": "LLaMPPL targets general generation tasks expressed as probabilistic programs: constrained generation, infilling, prompt intersection, and other compositional constrained text generation tasks; these are not interactive text-game benchmarks but are flexible text environments expressible via the library's primitives.",
            "performance_metric": "Quality of posterior approximation (e.g., E[log \\hat{Z}]), qualitative sample quality, computational cost comparable to beam search given caching optimizations.",
            "performance_value": null,
            "baseline_comparison": "Compared conceptually to beam search, token-masking greedy decoding, and implementations using stateless APIs (e.g., GenGPT3.jl) where efficient one-token-at-a-time inference is impractical; empirical numeric baselines on standardized text-planning benchmarks are not provided.",
            "has_ablation_uncertainty": false,
            "ablation_results": "Paper shows that different choices of proposals in LLaMPPL programs (i.e., different M_t) substantially affect estimator variance and sample quality; no explicit ablation toggling 'uncertainty modeling' on/off is reported.",
            "key_findings": "LLaMPPL makes it straightforward to express constrained generation and composition of prompts as probabilistic programs, and when paired with SMC steering and caching it enables tractable, posterior-aware constrained generation using LLMs; efficient proposals that approximate the posterior yield much better performance.",
            "uuid": "e977.1",
            "source_info": {
                "paper_title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Feynman-Kac Transformer",
            "name_full": "Feynman-Kac Transformer Model",
            "brief_description": "A formal probabilistic model for constrained generation where a Transformer-parameterized Markov kernel M_t and potential functions G_t (both using Transformer scores) define a Feynman-Kac model over token sequences whose posterior is targeted by SMC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Feynman-Kac Transformer model",
            "system_description": "Mathematical formalism: tuples (s0, {M_t}, {G_t}) where s0 is initial string, M_t(s_t | s_{t-1}, f_theta) are Transformer-parameterized Markov kernels that extend strings, and G_t(s_{t-1}, s_t, f_theta) are non-negative potentials encoding constraints/likelihoods; together they define filtering posteriors P_t and a posterior over complete EOS-terminated strings. Multiple equivalent M_t/G_t factorizations can represent the same posterior (allowing importance-sampling-style proposals).",
            "world_model_type": "Probabilistic symbolic sequence/world model (Feynman-Kac formulation)",
            "world_model_description": "State space: token sequences (prefixes and EOS-terminated strings). Actions/transitions: token-addition kernels M_t, parameterized by Transformer logits (probabilistic). Potentials G_t assign nonnegative scores to transitions to encode constraints or to correct proposals (importance weights). The model is explicitly probabilistic; the posterior over full sequences is obtained by reweighting trajectories of the base Markov chain.",
            "uses_llm": true,
            "llm_role": "LLM f_theta parameterizes transition kernels M_t and contributes to potentials G_t used for scoring and importance weighting.",
            "llm_model_name": "LLaMA-family (in the paper's experiments / implementation)",
            "uncertainty_modeling": true,
            "uncertainty_type": "Posterior/belief uncertainty over sequence completions and predictive uncertainty from the Transformer logits",
            "uncertainty_method": "Belief represented via sequential filtering/posterior distributions; approximated by SMC particle sets with importance weights and resampling.",
            "planning_algorithm": "Posterior sampling via Sequential Monte Carlo (SMC); formulation supports proposal design to reduce variance (akin to guided planning), but is not a classical symbolic planner (e.g., PDDL-based).",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Task-specified text generation problems (constrained generation, infilling, prompt intersection)",
            "text_environment_description": "Abstract textual environments defined by programmatic constraints and observations; complexity is user-specified via potentials and program logic rather than a pre-defined benchmark.",
            "performance_metric": "Quality of posterior approximation (E[log \\hat{Z}]), sample diversity and constraint satisfaction; computational cost relative to beam search.",
            "performance_value": null,
            "baseline_comparison": "Contrasted with token-masking (greedy) decoding and beam search approaches — emphasizes that SMC aims to sample from the posterior (not optimize) and can avoid greedy dead-ends and length bias of beam search; no numeric standardized benchmark comparisons.",
            "has_ablation_uncertainty": false,
            "ablation_results": "Analysis shows that choosing M_t proposals closer to the posterior reduces variance and yields better E[log \\hat{Z}] and sample quality (illustrated in prompt-intersection experiment comparing two formulations).",
            "key_findings": "Feynman-Kac formulation provides a flexible, compositional way to express constraints and composite prompt objectives as probabilistic (symbolic) models over sequences; it makes explicit how to design proposals and potentials to trade off computational efficiency and posterior accuracy.",
            "uuid": "e977.2",
            "source_info": {
                "paper_title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GenGPT3.jl",
            "name_full": "GenGPT3.jl (GPT-3 as a generative function in Gen.jl)",
            "brief_description": "A library integrating OpenAI's GPT-3 into the Gen.jl probabilistic programming system, enabling use of Gen's posterior inference tools for models that include GPT-3 as a generative primitive.",
            "citation_title": "GenGPT3.jl: GPT-3 as a generative function in Gen.jl",
            "mention_or_use": "mention",
            "system_name": "GenGPT3.jl (Gen.jl integration)",
            "system_description": "Wraps GPT-3 as a generative function callable from Gen.jl, allowing probabilistic programs to include GPT-3-sampled texts and to apply Gen's posterior inference methods (e.g., for structured infilling and constrained semantic parsing). The paper notes GenGPT3.jl includes examples that perform posterior inference but that SMC steering's caching and one-token-at-a-time efficiency would be difficult to implement with the stateless OpenAI API used by GenGPT3.jl.",
            "world_model_type": "Probabilistic program with LLM primitives (Gen probabilistic programs over string-valued variables)",
            "world_model_description": "Probabilistic programs manipulate string-valued random variables returned by GPT-3; Gen's inference machinery can perform posterior conditioning/inference over program variables. Representation is probabilistic; not expressed in PDDL/PPDDL but as programmatic probabilistic models.",
            "uses_llm": true,
            "llm_role": "GPT-3 acts as the generative primitive producing candidate text that the probabilistic program reasons about; Gen performs posterior inference conditioned on observations.",
            "llm_model_name": "GPT-3 (OpenAI)",
            "uncertainty_modeling": true,
            "uncertainty_type": "Predictive uncertainty of GPT-3 outputs and posterior uncertainty over program variables induced by observed data",
            "uncertainty_method": "Gen.jl's posterior inference algorithms (various programmable inference strategies); the underlying LLM is treated as stochastic primitive.",
            "planning_algorithm": "General-purpose probabilistic inference in Gen.jl (not a specialized planner); can be used to infer latent text fragments or other structured variables.",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Structured infilling and constrained semantic parsing examples (as reported in cited work)",
            "text_environment_description": "Examples are program-defined inference tasks over text (e.g., inferring which prompt caused an answer), not interactive text-game environments; the OpenAI API's statelessness limits efficient one-token-at-a-time posterior inference.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "GenGPT3.jl demonstrates that LLMs can be embedded as primitives in probabilistic programs and that Gen's inference machinery can perform posterior inference in such models; however, API constraints (stateless calls) make certain SMC-style one-token incremental inference inefficient without caching.",
            "uuid": "e977.3",
            "source_info": {
                "paper_title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Lampp",
            "name_full": "Lampp: Language models as probabilistic priors for perception and action",
            "brief_description": "A work that uses Transformer-based language models to provide probabilistic priors (world knowledge) for downstream tasks such as perception, semantic segmentation, and household navigation.",
            "citation_title": "Lampp: Language models as probabilistic priors for perception and action",
            "mention_or_use": "mention",
            "system_name": "Lampp",
            "system_description": "Proposes using posterior distributions of probabilistic generative models that incorporate Transformers as components; LLMs provide priors or score proposals for tasks like semantic segmentation and navigation so that general world knowledge informs probabilistic inference over state/action variables. The paper references this as an example of using LLM-informed priors for perception and action.",
            "world_model_type": "Probabilistic generative models combining LLM priors with task-specific state/action models (probabilistic, not classical symbolic PDDL/PPDDL)",
            "world_model_description": "Combines LLM-derived priors over textual or semantic variables with probabilistic models for task-specific states and transitions (e.g., perception outputs or navigation state estimates); representational specifics vary by application and are probabilistic rather than classical PDDL.",
            "uses_llm": true,
            "llm_role": "Provides general-world knowledge priors or proposal/score information to inform probabilistic inference over perception and action variables.",
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State uncertainty and LLM predictive uncertainty used to form priors and posterior estimates over task variables (perception/action).",
            "uncertainty_method": "Probabilistic inference (posterior computation) where LLMs provide prior probabilities or scoring factors; exact inference method depends on the downstream task and is probabilistic (e.g., Bayesian inference).",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Perception and household navigation tasks (cited as examples)",
            "text_environment_description": "Not strictly text-based interactive environments; examples include semantic segmentation and household navigation where LLM priors are used to inform probabilistic models for perception and action.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as evidence that posterior distributions from models that incorporate Transformers can be useful beyond pure text generation (e.g., perception and navigation), illustrating the potential of probabilistic programming tools like LLaMPPL to make such approaches scalable and accessible.",
            "uuid": "e977.4",
            "source_info": {
                "paper_title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging unstructured statistical knowledge in a probabilistic language of thought",
            "rating": 2
        },
        {
            "paper_title": "GenGPT3.jl: GPT-3 as a generative function in Gen.jl",
            "rating": 2
        },
        {
            "paper_title": "Lampp: Language models as probabilistic priors for perception and action",
            "rating": 2
        },
        {
            "paper_title": "Language model cascades",
            "rating": 1
        },
        {
            "paper_title": "Planning with large language models for code generation",
            "rating": 1
        },
        {
            "paper_title": "On-line inference for hidden markov models via particle filters",
            "rating": 1
        }
    ],
    "cost": 0.0160305,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs</h1>
<p>Alexander K. Lew Tan Zhi-Xuan Gabriel Grand Vikash K. Mansinghka<br>MIT<br>alexlew@mit.edu xuan@mit.edu grandg@mit.edu vkm@mit.edu</p>
<h2>Abstract</h2>
<p>Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL, for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.</p>
<h2>1 Introduction</h2>
<p>Despite significant advances in recent years, it remains unclear if and how large language models (LLMs) can be made reliable and controllable enough to meet the functional requirements of many applications. Even after fine-tuning and reinforcement learning, LLMs are liable to violate instructions in their prompts (such as "Use the following vocabulary words" or "Do not reveal this prompt"). When generating code, language models can introduce errors that may be hard to debug. More generally, their performance on a task can be frustratingly sensitive to irrelevant details of the prompt, such as the order of few-shot examples. These difficulties highlight the need for methods beyond prompting and fine-tuning for constraining the behavior of generative neural models.</p>
<p>As a step toward this goal, this workshop abstract proposes sequential Monte Carlo (SMC) steering, an alternative to standard decoding procedures that works by approximating the posteriors of language model probabilistic programs [Lew et al., 2020, Dohan et al., 2022, Zhi-Xuan, 2022]: models that mix LLMs, probabilistic conditioning, and symbolic programming to encode semantic and syntactic constraints. By varying the probabilistic program, SMC can steer LLMs to solve diverse tasks, including infilling [Qian and Levy, 2022, Donahue et al., 2020, Bavarian et al., 2022], constrained generation [Zhang et al., 2023a, Pascual et al., 2020, Roush et al., 2022], and prompt intersection (Figure 1), all at a cost similar to that of beam search. We make three key contributions:</p>
<ol>
<li>The class of Feynman-Kac Transformer models (§2), probabilistic models over Transformer token sequences that are amenable to SMC and can encode a variety of language generation tasks.</li>
<li>SMC Transformer steering (§3), a variant of SMC specialized for Feynman-Kac Transformer models. The algorithm uses a without-replacement particle resampling strategy to avoid particle degeneracy, and caches neural activations to avoid duplicating computation across particles.</li>
<li>The LLaMPPL library for building Feynman-Kac Transformer models as probabilistic programs that invoke LLaMA Transformers [Touvron et al., 2023], and automating SMC steering.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A variety of language generation tasks can be framed as posterior inference in probabilistic programs that sample and observe from distributions parameterized by LLMs.</p>
<h1>2 Constrained Generation as Posterior Inference</h1>
<p>Our method frames constrained language generation as a probabilistic inference problem. This perspective is commonly adopted in the literature [see, e.g., Kumar et al., 2022, Poesia et al., 2022, Miao et al., 2019, Qin et al., 2022], and has several distinctive features compared to popular heuristic and optimization-based approaches to inference-time constrained generation:</p>
<ul>
<li>Global vs. local constraint following. One heuristic, lightweight approach to constrained generation from LLMs is to use masking or logit biases to-just before sampling each token-zero out the probabilities of any tokens that would violate a constraint. Unfortunately, this local or greedy decoding policy can get stuck, yielding unnatural completions:</li>
</ul>
<p>PROMPT: "The Fed says"
CONSTRAINT: No word with more than five letters
TOKEN MASKING: " the cost of a 30-yr fixed mortg...", " US infl- ation is back. Are they right?", " it will take at least 12 more meet... (read more)"
The tokens " mortg", " infl" and " meet" are sampled because they do not yet violate the 5-letter constraint: the algorithm cannot see that they make future violations hard to avoid. By contrast, conditioning the LLM on the constraint causes global reallocation of probability mass, yielding a posterior that upweights early tokens which make it easier to satisfy the constraint later. By targeting this posterior, SMC steering avoids greedy dead ends:</p>
<p>SMC STEERING: " it will buy $\$ 100$ B in debt per month. Is this the top of a wave or just the start? Might the Fed think twice about this move?"</p>
<ul>
<li>Sampling vs. optimization. Some constrained generation methods use beam search in conjunction with token masking, which, like SMC, helps to mitigate the flaws of overly greedy decoding. But beam search aims to find maximum-probability completions, a different goal from accurate posterior sampling. Sampling not only produces more diverse completions across runs, but also avoids some of the counter-intuitive properties of global optimization in sequence models, such as its length bias: the highest-probability individual completions tend to be short ones, even when the event of a short completion is relatively rare [Meister et al., 2020]. This is particularly pronounced at high beam sizes, which can perform more effective optimization:</li>
</ul>
<p>APPROXIMATE OPTIMIZATION (HIGH BEAM SIZE): "[The Fed says] no"</p>
<p>This two-token completion (including an invisible EOS token) has log probability $-11.74$ under the LLaMA-7b model—i.e., it is actually quite unlikely. But it is more likely than any particular long completion, because the substantial probability mass that LLaMA assigns to longer completions in general must be divided across a huge number of specific possibilities (many of which are quite similar). Reducing beam size can reduce length bias [Murray and Chiang, 2018, Meister et al., 2020], because it handicaps beam search as an optimizer—but this also makes it more likely to enter the dead ends that plague greedy decoding approaches. By contrast, increasing computation in posterior inference algorithms makes them better approximations to the posterior, helping to account for constraints without collapsing to a length-biased mode.</p>
<p>In this section, we first propose a broad class of probabilistic models, based on Del Moral [2004]'s Feynman-Kac formulae, for constrained language generation, designed to admit tractable sequential Monte Carlo approximation (§2.1). We then give several examples to illustrate the breadth of the tasks expressible in this framework (§2.2). Finally, we show how language model probabilistic programs can be used to concisely and compositionally specify such tasks, enabling new combinations of hard and soft constraints, and new ways of composing prompts and language models (§2.3). We illustrate this flexibility via preliminary demonstrations of prompt intersection, the task of generating a completion that is simultaneously likely under multiple prompts (Figures 1 and 4).</p>
<h1>2.1 Mathematical Setting: Feynman-Kac Transformer Models</h1>
<p>Let $\mathcal{V}$ be the vocabulary of a Transformer model, and $\mathcal{S}=\mathcal{V}^{*}$ the set of multi-token strings. We assume $\mathcal{V}$ contains an end-of-sequence token EOS, and write $\mathcal{F} \subseteq \mathcal{S}$ for the set of EOS-terminated strings. A Feynman-Kac Transformer model is a tuple $\left(s_{0},\left{M_{t}\right}<em t="t">{t \geq 1},\left{G</em>\right)$, where:}\right}_{t \geq 1</p>
<ul>
<li>$s_{0} \in \mathcal{S}$ is an initial state, which we will often take to be the empty string $\epsilon$.</li>
<li>$M_{t}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)$ is a Markov kernel (i.e., conditional probability distribution) from $s_{t-1} \in \mathcal{F}^{c}$ to $s_{t} \in \mathcal{S}$, parameterized by a Transformer network $f_{\theta}: \mathcal{F}^{c} \rightarrow \mathbb{R}^{|\mathcal{V}|}$ mapping non-EOS-terminated strings to vectors of logits.</li>
<li>$G_{t}\left(s_{t-1}, s_{t}, f_{\theta}\right)$ is a potential function, mapping a pair $\left(s_{t-1}, s_{t}\right) \in \mathcal{F}^{c} \times \mathcal{S}$ to a real-valued non-negative score. It is also parameterized by a Transformer network $f_{\theta}$.</li>
</ul>
<p>Given a Transformer $f_{\theta}$, the Markov kernels $M_{t}$ define a Markov chain $\mathbb{M}$ on the random variables $S_{t} \in \mathcal{S}(t \geq 0)$, where $S_{0}$ is deterministically $s_{0}$, and the distribution of $S_{t}$ given $S_{t-1}$ is $M_{t}(\cdot \mid$ $\left.s_{t-1}, f_{\theta}\right)$ if $s_{t-1} \in \mathcal{F}^{c}$, or $\delta_{s_{t-1}}$ if $s_{t-1} \in \mathcal{F}$. That is, starting at $s_{0}, \mathbb{M}$ continually modifies the string according to $M_{t}$ until a string ending in EOS is reached, at which point it is never modified again. We write $T$ for the stopping time of the chain, a random variable equal to the first time $t$ that $S_{t} \in \mathcal{F}$. We assume that $M_{t}$ and $f_{\theta}$ are such that $T$ is finite with probability 1.
Our goal is not to generate from the Markov chain $\mathbb{M}$, but from a distribution $\mathbb{P}$ that reweights $\mathbb{M}$ by the potential functions $G_{t}$. We first define the step-t filtering posteriors,</p>
<p>$$
\mathbb{P}<em t="t">{t}\left(s</em>}\right)=\frac{\mathbb{E<em i="1">{\mathbb{M}}\left[\prod</em>}^{t \wedge T} G_{i}\left(S_{i-1}, S_{i}, f_{\theta}\right) \cdot\left[S_{t}=s_{t}\right]\right]}{\mathbb{E<em i="1">{\mathbb{M}}\left[\prod</em>
$$}^{t \wedge T} G_{i}\left(S_{i-1}, S_{i}, f_{\theta}\right)\right]</p>
<p>Then, because $T$ is almost surely finite, we can define the overall posterior $\mathbb{P}(s)=\lim <em t="t">{t \rightarrow \infty} \mathbb{P}</em>(s)$.</p>
<h3>2.2 Examples</h3>
<p>To build intuition for the sorts of tasks that can be specified using Feynman-Kac Transformer models, we now develop several examples.</p>
<p>Hard constraints. Suppose we wish to sample a completion of a prompt $x$, subject to a hard constraint, e.g., that every generated word is shorter than 5 letters. We write $\mathcal{C}<em _mathcal_F="\mathcal{F">{\mathcal{F}} \subseteq \mathcal{F}$ for the set of full strings satisfying the constraint, and $\mathcal{C}=\left{s \mid \exists s^{\prime} . s s^{\prime} \in \mathcal{C}</em>}}\right}$ for the set of all prefixes of strings in $\mathcal{C<em t="t">{\mathcal{F}}$. Our Markov kernel $M</em>$ to append a single token at a time:}$ just uses the Transformer $f_{\theta</p>
<p>$$
M_{t}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)=\sum_{w_{t} \in \mathcal{V}} \operatorname{softmax}\left(f_{\theta}\left(x s_{t-1}\right)\right)<em t="t">{w</em>\right]
$$}} \cdot\left[s_{t}=s_{t-1} w_{t</p>
<p>Our potential functions then enforce that we have not yet violated the constraint:</p>
<p>$$
G_{t}\left(s_{t-1}, s_{t}, f_{\theta}\right)=\left[s_{t} \in \mathcal{C}\right]=1_{\mathcal{C}}\left(s_{t}\right)
$$</p>
<p>Writing $P_{\left(f_{\theta}, x\right)}(S)$ for the distribution over EOS-terminated strings given by standard temperature-1 decoding from $f_{\theta}$ with prompt $x$, we can see that the overall posterior $\mathbb{P}$ of our Feynman-Kac model is precisely $\mathbb{P}(s)=P_{\left(f_{\theta}, x\right)}\left(S=s \mid S \in \mathcal{C}<em 0="0">{\mathcal{F}}\right)$.
There are in fact multiple Feynman-Kac models $\left(s</em>\right}},\left{M_{t<em t="t">{t \geq 1},\left{G</em>\right}<em t="t">{t \geq 1}\right)$ that yield the same overall posterior $\mathbb{P}$. For example, we could have set $M</em>$ to generate only tokens that do not violate the constraint, by token masking:</p>
<p>$$
M_{t}^{\prime}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)=\frac{\sum_{w_{t} \in \mathcal{V}} \operatorname{softmax}\left(f_{\theta}\left(x s_{t-1}\right)\right)<em t="t">{w</em>\right)\right)}} \cdot 1_{\mathcal{C}}\left(s_{t-1} w_{t}\right) \cdot\left[s_{t}=s_{t-1} w_{t}\right]}{\sum_{w \in \mathcal{V}} \operatorname{softmax}\left(f_{\theta}\left(x s_{t-1<em _mathcal_C="\mathcal{C">{w} \cdot 1</em>
$$}}\left(s_{t-1} w\right)</p>
<p>Then we recover the same posterior $\mathbb{P}^{\prime}=\mathbb{P}$ so long as we also change our potential functions to</p>
<p>$$
G_{t}^{\prime}\left(s_{t-1}, s_{t}, f_{\theta}\right)=\sum_{w \in \mathcal{V}} \operatorname{softmax}\left(f_{\theta}\left(x s_{t-1}\right)\right)<em _mathcal_C="\mathcal{C">{w} \cdot 1</em> w\right)
$$}}\left(s_{t-1</p>
<p>This can be seen as an importance weight: on the support of $M_{t}^{\prime}$, it is equal to $\frac{M_{t}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)}{M_{t}^{\prime}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)}$, the ratio of the original sampling distribution to our modified sampling distribution.
The reader may wonder why we do not just set the potential $G_{t}^{\prime}\left(s_{t-1}, s_{t}, f_{\theta}\right)=1$, since the Markov kernels $M_{t}^{\prime}$ now enforce the constraint, and $G_{t}^{\prime}$ (intuitively speaking) has nothing left to "check." The issue is that the Markov kernels enforce the constraint greedily, sampling early tokens with no regard for whether they will make it more or less difficult to satisfy the constraint later. The potential functions $G_{t}^{\prime}$ penalize the string so far $\left(s_{t-1}\right)$ based on how difficult it was to continue it by one token without violating the constraint. As such, they implement a form of "hindsight" and recover the global posterior $\mathbb{P}^{\prime}(s)=\mathbb{P}(s)=P_{\left(f_{\theta}, x\right)}\left(S=s \mid S \in \mathcal{C}<em t="t">{\mathcal{F}}\right)$.
Although these two formulations specify the same task (generating from the posterior given the hard constraint), we will see in $\S 3$ that given bounded compute resources, our steering algorithm's performance depends on which formulation is used. A general rule of thumb is that inference will be more efficient (i.e., require less compute to achieve accurate results) if each $M</em>^{}$ and $G_{t}$ are chosen to reduce the variance of the potential $G_{t}\left(S_{t-1<em>}, S_{t}^{\circ}, f_{\theta}\right)$ for $S_{t-1}^{</em>} \sim \mathbb{P}<em t="t">{t-1}$ and $S</em>\right)$.
Infilling. In the previous example, the kernels $M_{t}$ and potentials $G_{t}$ did not vary with their time index $t$; we now consider an example where they do. Consider the task of infilling a template with holes, such as "To tell the truth, every[BLANK] he[BLANK] to[BLANK] another[BLANK]." Let $x_{0}, \ldots, x_{n}$ be the known fragments, with $x_{n} \in \mathcal{F}$; then our goal is to generate a string $s=$ $x_{0} h_{1} x_{1} \ldots h_{n} x_{n}$ that "fills the blanks" between each of the known fragments $x_{i}$ with completions $h_{i}$. We take $s_{0}=x_{0}$, and choose the Markov kernels $M_{t}$, for time $1 \leq t \leq n$, to append a geometrically distributed number of new sampled tokens, followed by the next known fragment $x_{t}$ :}^{\circ} \sim M_{t}\left(\cdot \mid S_{t-1}^{*}, f_{\theta</p>
<p>$$
M_{t}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)=\sum_{h_{t} \in \mathcal{S}}\left(2^{-\left|h_{t}\right|-1} \prod_{i=1}^{\left|h_{t}\right|} \operatorname{softmax}\left(f_{\theta}\left(s_{t-1} h_{t}^{1: i-1}\right)\right)<em t="t">{h</em>\right]\right)
$$}^{i}} \cdot\left[s_{t}=s_{t-1} h_{t} x_{t</p>
<p>The potential corrects for the length bias from the geometrically sampled number of tokens, and scores a proposed completion by how well it explains the fragment $x_{t}$ :</p>
<p>$$
G_{t}\left(s_{t-1}, s_{t}, f_{\theta}\right)=\sum_{h_{t} \in \mathcal{S}}\left(2^{\left|h_{t}\right|+1} \prod_{i=1}^{\left|x_{t}\right|} \operatorname{softmax}\left(f_{\theta}\left(s_{t-1} h_{t} x_{t}^{1: i-1}\right)\right)<em t="t">{x</em>\right]\right)
$$}^{i}} \cdot\left[s_{t}=s_{t-1} h_{t} x_{t</p>
<p>The resulting posterior $\mathbb{P}$ can be seen to be $\mathbb{P}(s)=P_{\left(f_{\theta}, e\right)}\left(S=s \mid \exists h_{1: n} . S=x_{0} h_{1} x_{1} \ldots h_{n} x_{n}\right)$. If we are looking for completions to be on the shorter side, we can remove the $2^{\left|h_{t}\right|+1}$ correction term from the formula for $G_{t}$ : no longer divided out, the geometric distribution in $M_{t}$ would then behave as a prior on the lengths of the completions $h_{t}$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<h1>Feynman-Kac Transformer Model</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: A LLaMPPL program for prompt intersection, and the model it implicitly defines.</p>
<p>Prompt intersection. As a final example, we look at how to encode the task of prompt intersection, where the goal is to generate a completion $s$ that is likely under multiple distinct prompts $x_{0}, \ldots, x_{m}$. We take $s_{0}=\epsilon$ and set $M_{t}$ to generate according to the first prompt $x_{0}$ :</p>
<p>$$
M_{t}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)=\sum_{w_{t} \in \mathcal{V}} \operatorname{softmax}\left(f_{\theta}\left(x_{0} s_{t-1}\right)\right)<em t="t">{w</em>\right]
$$}} \cdot\left[s_{t}=s_{t-1} w_{t</p>
<p>The potential then scores by the remaining prompts:</p>
<p>$$
G_{t}\left(s_{t-1}, s_{t}, f_{\theta}\right)=\sum_{w_{t} \in \mathcal{V}} \prod_{i=1}^{m} \operatorname{softmax}\left(f_{\theta}\left(x_{i} s_{t-1}\right)\right)<em t="t">{w</em>\right]
$$}} \cdot\left[s_{t}=s_{t-1} w_{t</p>
<p>This defines a product-of-experts posterior, with $\mathbb{P}(s) \propto \prod_{i=0}^{m} P_{\left(f_{\theta}, s_{i}\right)}(S=s)$. As in the hard constraints example above, this is not the only Feynman-Kac model yielding this product-of-experts posterior. Just as we previously changed $M_{t}$ to intelligently (but greedily) sample tokens based on a constraint, here we can change $M_{t}$ to intelligently select tokens based on every prompt at once:</p>
<p>$$
M_{t}^{\prime}\left(s_{t} \mid s_{t-1}, f_{\theta}\right)=\sum_{w_{t} \in \mathcal{V}} \frac{\prod_{i=0}^{m} \operatorname{softmax}\left(f_{\theta}\left(x_{i} s_{t-1}\right)\right)<em t="t">{w</em>\right)\right)}}}{\sum_{w \in \mathcal{V}} \prod_{i=0}^{m} \operatorname{softmax}\left(f_{\theta}\left(x_{i} s_{t-1<em t="t">{w}} \cdot\left[s</em>\right]
$$}=s_{t-1} w_{t</p>
<p>However, just as in the hard constraints example, we also need to change the potentials, to preserve the global product-of-experts posterior (and avoid greedily sampling into dead ends):</p>
<p>$$
G_{t}^{\prime}\left(s_{t-1}, s_{t}, f_{\theta}\right)=\left(\sum_{w_{t} \in \mathcal{V}}\left[s_{t}=s_{t-1} w_{t}\right]\right) \cdot\left(\sum_{w \in \mathcal{V}} \prod_{i=0}^{m} \operatorname{softmax}\left(f_{\theta}\left(x_{i} s_{t-1}\right)\right)_{w}\right)
$$</p>
<h3>2.3 Language Model Probabilistic Programming</h3>
<p>To make SMC steering more accessible and automated, we have implemented a Python probabilistic programming library, LLaMPPL, for building language model probabilistic programs backed by Meta's LLaMA family of models [Touvron et al., 2023]. We now briefly describe the library, and define the Feynman-Kac Transformer model associated to a LLaMPPL program.
A LLaMPPL program is a Python subclass of our Model class. The key method implemented by a LLaMPPL program is step, which performs an update on a string-valued instance variable self.s, and has access to the following special methods:</p>
<ul>
<li>self.sample $($ dist $[$, proposal $]$ ): generate a sample $v$ from the distribution dist. If provided, generate from proposal instead, but incorporate the importance weight $\frac{\operatorname{dist}(v)}{\text { proposal }(v)}$ into the potential.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Example trie of prompts generated in the first few steps of an SMC algorithm on the constraint model from Figure 1. Our system maintains such a trie and at each node, caches nexttoken logits and layerwise key/value vectors for the token-in-context.</p>
<ul>
<li>self.condition(bool): constrain a Boolean expression to be true.</li>
<li>self.observe(dist, val): equivalent to running $v=\operatorname{sample}($ dist, dirac(val $))$, and immediately conditioning on the expression $v=$ val.</li>
<li>self.transformer(context): a distribution over next tokens, to pass to sample or observe.</li>
<li>self.finish(): terminate self.s with EOS.</li>
</ul>
<p>The user's subclass implicitly defines a Feynman-Kac transformer model $\left(s_{0},\left{M_{t}\right}<em t="t">{t \geq 1},\left{G</em>\right}<em 0="0">{t \geq 1}\right)$. The initial state $s</em>$ by overriding the }$ is the value of self.s for a newly constructed instance of the class. (A user can customize $s_{0<strong>init</strong> method.) We then consider the Markov chain induced on $\mathcal{S}$ by repeatedly applying the step method, with $S_{t}$ the value of self.s after $t$ applications. In particular, the Markov kernels $M_{t}$ are the transitions induced by calling step, ignoring condition and observe statements, and generating from proposal at each sample statement (when it is provided, or dist when it isn't). ${ }^{1}$ The potential $G_{t}$ for the transition is the product of: (1) for each encountered sample statement, the importance weight $\frac{\operatorname{dist}(v)}{\text { proposal }(v)}$ (or 1 if proposal is not supplied); (2) for each encountered observe statement, dist(val); and (3) for each encountered condition statement, 1 if the Boolean is true and 0 if it is not. ${ }^{2}$</p>
<h1>3 Sequential Monte Carlo Steering</h1>
<p>Probabilistic programs specify posterior inference tasks, but to generate posterior samples, we require an inference algorithm. Our proposal is to use a variant of sequential Monte Carlo (SMC) specialized to our setting (Algorithm 1). SMC maintains a collection of weighted particles, which in our case store realizations of the state variables $S_{t}$ of a Feynman-Kac Transformer model. The algorithm alternates between extending the particles using the Markov kernels $M_{t}$, reweighting them using the potential functions $G_{t}$, and resampling to clone promising particles and cull low-likelihood ones. We highlight the key differences between Algorithm 1 and standard SMC implementations:</p>
<ul>
<li>Shared Transformer cache. Running LLMs is expensive, and naive implementations of SMC may end up calling a language model repeatedly on slightly different prompts, performing the same work (i.e., processing the same tokens in the same order) many times. For example, Figure 3 shows a collection of prompts generated in the first few steps of SMC, applied to the constraints model from Figure 1. Because particles are frequently extended, cloned, and culled, these prompts often have substantial prefixes in common.
To avoid duplicated work, both across time steps and across particles, Algorithm 1 instantiates a shared CachedTransformer which handles all LLM queries, and maintains a trie of tokens (as in Figure 3) as a cache layer to mediate requests from $M_{t}$ and $G_{t}$ to the Transformer model itself. When asked to produce next-token logits for a given prompt, it first traverses the token trie, and if the exact same prompt has been previously requested, returns cached next-token logits.</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Algorithm 1 Sequential Monte Carlo Transformer Steering
1: Input: $N$ (# particles), $K$ (factor), Feynman-Kac Transformer model $\left(s_{0},\left{M_{t}\right}<em t="t">{t \geq 1},\left{G</em>\right}<em i="i">{t \geq 1}\right)$
2: Output: Weighted particle approximation $\left(x</em>\right)}, w_{i<em S="S" _mathbb_S="\mathbb{S">{i=1, \ldots, N}$ of the posterior $\mathbb{P}$
3: Output: Unbiased estimate $\hat{Z}$ of the partition function $Z=\mathbb{E}</em>\right)\right]$
4: Initialize $f_{\theta} \leftarrow$ CachedTransformer()
5: Initialize $\left(x_{i}, w_{i}\right) \leftarrow\left(s_{0}, 1\right)$ for $i=1, \ldots, N$
6: Initialize $t \leftarrow 1$
7: while $x_{i} \notin \mathcal{F}$ for some $i \in{1, \ldots, N}$ do
8: $\quad$ Set $K_{i} \leftarrow K\left(1-1_{\mathcal{F}}\left(x_{i}\right)\right)+1_{\mathcal{F}}\left(x_{i}\right)$ for $i=1, \ldots, N$
9: $\quad$ Set $N^{\prime} \leftarrow \sum_{i=1}^{N} K_{i}$
10: for $i \in{1, \ldots, N}$ do
11: if $x_{i} \in \mathcal{F}$ then
12: $\quad$ Set $\left(x_{(i, 1)}, w_{(i, 1)}\right) \leftarrow\left(x_{i}, w_{i} \cdot \frac{N^{\prime}}{N}\right)$
13: else
14: $\quad$ Generate $x_{(i, k)} \sim M_{t}\left(\cdot \mid x_{i}, f_{\theta}\right)$ for $k=1, \ldots, K$
15: $\quad$ Set $w_{(i, k)} \leftarrow \frac{N^{\prime}}{K N} \cdot w_{i} \cdot G_{t}\left(x_{i}, x_{(i, k)}, f_{\theta}\right)$ for $k=1, \ldots, K$ 16: end if
17: end for
18: Set normalized weights $\hat{w}}}\left[\prod_{t=1}^{T} G_{t}\left(s_{t-1}, s_{t}, f_{\theta<em _i_="(i," k_="k)">{(i, k)} \leftarrow \frac{w</em>$
19: Set $c^{}}{\sum_{j=1}^{N} \sum_{i=1}^{K_{j}} w_{(j, l)}}$ for $i=1, \ldots, N$ and $k=1, \ldots, K_{i<em>} \leftarrow \inf \left{c \in \mathbb{R}<em i="1">{&gt;0} \mid \sum</em>}^{N} \sum_{k=1}^{K_{i}}\left(1 \wedge c \hat{w<em _det="{det" _text="\text">{(i, k)}\right)&gt;N\right}$
20: $\quad$ Set $\left(I</em>\right) \leftarrow\left(\left{(i, k) \mid c^{}}, I_{\text {stoch }}, I_{\text {strat }</em>} \hat{w}<em l="1">{(i, k)} \geq 1\right},\left{(i, k) \mid c^{<em>} \hat{w}<em I__text="I_{\text" _in="\in" _stoch="{stoch" i="i">{(i, k)}&lt;1\right},{ }\right)$
21: $\quad$ Set $\alpha \leftarrow \frac{\sum</em>}}} \hat{w<em _det="{det" _text="\text">{i}}{N-\left|I</em>([0, \alpha])$
22: for $i \in I_{\text {stoch }}$ do
23: $\quad$ Set $U \leftarrow U-\hat{w}}}\right|}$ and generate $U \sim \operatorname{Uniform<em _strat="{strat" _text="\text">{i}$
24: if $U&lt;0$ then
25: $\quad$ Set $I</em> \cup{i}$
26: $\quad$ Set $U \leftarrow U+\alpha$
27: end if
28: end for
29: Set particles $\left(x_{i}, w_{i}\right)}} \leftarrow I_{\text {strat }<em _det="{det" _text="\text">{i=1, \ldots,\left|I</em>\right}$
30: Set particles $\left(x_{i}, w_{i}\right)}}\right|} \leftarrow\left{\left(x_{j}, w_{j} \cdot \frac{N}{N^{\prime}}\right) \mid j \in I_{\text {det }<em _det="{det" _text="\text">{i=\left|I</em>{c^{}}\right|+1, \ldots, N} \leftarrow\left{\left(x_{j}, \frac{N</em>} N^{\prime}} \sum</em>\right}$
31: end while
32:
33: return $\left(\left(x_{i}, w_{i}\right)}^{N} \sum_{k=1}^{K_{l}} w_{(l, k)}\right) \mid j \in I_{\text {strat }<em i="1">{i=1, \ldots, N}, \hat{Z}=\frac{1}{N} \sum</em>\right)$}^{N} w_{i</p>
<p>Otherwise, it runs the Transformer model on any new prompt tokens only. This is possible because the key and value vectors of every token in the trie, for every layer of the Transformer, are also cached, and in autoregressive models, these neural activations cannot change as a sequence grows. We note that caching these activations is a common Transformer optimization (called " KV caching") in single-particle settings, e.g. to enable conversational interfaces without re-evaluating the entire conversation history with each new message. But we have found that extending it to the multi-particle setting makes inference in language model probabilistic programs significantly cheaper, compared to previous approaches to integrating Transformer models into probabilistic programs [Lew et al., 2020, Dohan et al., 2022, Zhi-Xuan, 2022].</p>
<ul>
<li>Without-replacement resampling. Standard sequential Monte Carlo implementations maintain a fixed number of particles throughout their execution, and use randomized resampling strategies to clone some particles and cull others. Because of this, it is common for the same state $S_{t}=s_{t}$ to be represented multiple times within a particle collection. To maintain better particle diversity, we instead apply a resampling strategy that more closely resembles beam search, while maintaining the unbiasedness and posterior consistency properties we expect of SMC. At each step, any active particles (i.e., those not already terminated by EOS) are cloned $K$ times, and each clone is independently extended using $M_{t}$. This larger collection of particles is then down-sampled back to size $N$, using a without-replacement down-sampling algorithm to ensure uniqueness of the $N$ resampled particles. The down-sampling algorithm is close to that of Fearnhead and Clifford [2003], except</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Results of SMC steering on the prompt intersection task from §2.2, modified to emit EOS after one sentence. Left: We plot mean values of $\log \hat{Z}$ across 10 runs of SMC steering, with varying numbers of particles $N$, fixed expansion factor $K=3$, and the two Feynman-Kac models for prompt intersection given in §2.2. In the first model, the Markov kernel $M_{t}$ proposes tokens according to only the first prompt ("My favorite writer is probably"), and the potential $G_{t}$ conditions on agreement from the second prompt. In the second model, the Markov kernel $M_{t}$ samples a locally optimal proposal distribution based on logits from both prompts, and $G_{t}$ serves as an importance weight. Right: Higher $\mathbb{E}[\log \hat{Z}]$ corresponds to qualitatively better samples. Indeed, by Jensen's inequality, $\mathbb{E}[\log \hat{Z}]$ is a lower bound on $\log Z$, and the gap is itself an upper bound on the KL divergence between SMC steering's sampling distribution and the true posterior $\mathbb{P}$.
that (1) we update the weights slightly differently to ensure they remain unbiased estimates of the partition function $Z$, and (2) we include new weight corrections to account for the fact that in our setting, some particles are stopped and thus not cloned during the expansion step.</p>
<p>Accuracy. Under mild assumptions, SMC is consistent in the sense that as the number of particles grows, the marginal distribution over returned particles approaches the true posterior. It also produces unbiased estimates of marginal likelihoods, and of unnormalized posterior integrals: for any $f$, the expected value of the weighted average $\frac{1}{N} \sum w_{i} f\left(x_{i}\right)$ is the posterior expectation of $f$, times a normalizing constant that does not depend on $f$. However, the variance of these estimates, and the accuracy of posterior sampling, depend on the Feynman-Kac model. Figure 4 illustrates this phenomenon in the prompt intersection task from §2.2. Recall that we formulated two Feynman-Kac models for this task, which targeted the same posterior $\mathbb{P}$ but made distinct choices of $M_{t}$ and $G_{t}$. One way of understanding their differences is that they encode different proposal distributions for the task: the first model proposes according to a single prompt, whereas the second model proposes based on logits from all the prompts. As Figure 4 shows, the second outperforms the first.
A general design principle is that the proposal (i.e., the distribution from which $M_{t}$ samples) should be as close to the posterior as possible. Nearly any heuristics for solving the generation task can be incorporated into the proposal, so long as the potentials $G_{t}$ are appropriately modified to perform the proper importance weighting. One approach to developing better proposals for challenging generation tasks could be to propose tokens from an LLM with an auxiliary prompt that describes the task in natural language (where the prompt itself could be automatically generated via inference, as in <em>Zhi-Xuan [2022]</em> and <em>Zhou et al. [2023]</em>); then SMC steering would be responsible only for correcting mistakes made by the prompted proposal, rather than solving the task from scratch. Another approach could be to use "chain-of-thought" proposals, i.e., LLMs prompted to perform some reasoning before proposing tokens to solve the task <em>[Wei et al., 2022]</em>. Because the marginal probability of proposing a token would not be tractable for such a proposal, the potentials $G_{t}$ would not be able to incorporate exact importance weights, and would instead need to approximate them unbiasedly. Future versions of LLaMPPL could incorporate recently introduced probabilistic programming techniques for automating this unbiased proposal density estimation <em>[Lew et al., 2022, 2023]</em>.</p>
<h1>4 Related Work and Discussion</h1>
<p>Probabilistic programming with language models. To our knowledge, the idea of integrating language models as primitives into a probabilistic programming system was first proposed by Lew et al. [2020], who showed that in certain verbal reasoning tasks, the posteriors of such programs were better models of human behavior than unconstrained language models. More recently, Dohan et al. [2022] proposed unifying various approaches to "chaining" LLMs by understanding them as graphical models or probabilistic programs with string-valued random variables. But in the "chain-of-thought"-style applications they explore, there are typically no unknown variables with non-trivial likelihood terms, so no inference algorithm is required-"forward" or "ancestral" sampling suffices.
Our examples, by contrast, induce non-trivial posteriors that require more powerful inference algorithms to sample. Zhi-Xuan [2022]'s GenGPT3.jl library integrates OpenAI's GPT-3 models into the Gen.jl probabilistic programming system [Cusumano-Towner et al., 2019], and includes examples of using Gen.jl's posterior inference machinery to perform structured infilling (e.g., inferring which of a set of questions was likely to lead an observed answer, similar to automatic prompt engineering Zhou et al. [2023]) and constrained semantic parsing. However, the sequential Monte Carlo algorithms we describe here would be difficult to implement efficiently using GenGPT3.jl. One challenge is that the OpenAI API is stateless, and so "one-token-at-a-time" generation and conditioning would require prohibitively many calls (each with growing numbers of context tokens).</p>
<p>Steering language models with programs. Beurer-Kellner et al. [2022] recently coined the term language model programming, to refer to the use of specialized programming languages to guide the behavior of LLMs. Several such programming languages have since been introduced, including their SQL-inspired language model query language (LMQL), Microsoft and Lundberg [2023]'s Guidance language, and Normal Computing and Louf [2023]'s Outlines language. All three of these provide high-level DSLs that make chaining multiple calls to LLMs more ergonomic, and Outlines and LMQL also expose some features for generation subject to hard constraints. However, they do not support sampling the posterior given these constraints, only beam search (in the case of LMQL) and greedy decoding, using token masking to enforce the constraints. Furthermore, the constraint DSLs supported by these languages are limited, and cannot, for example, encode our prompt intersection task. That said, they support many features that would be useful to include in future versions of LLaMPPL (or higher-level DSLs built on it): a unified frontend to a variety of Transformer backends, automated computation of token masks for enforcing common constraints, and high-level syntax for chaining prompts together that does not require explicit token-by-token processing logic.
Controlled generation and probabilistic inference in language models. Many recent papers have proposed methods for more controlled text generation from language models, either through the lens of optimization [Kumar et al., 2021] or probabilistic inference [Kumar et al., 2022]. Approaches applied during fine-tuning include direct preference optimization [Rafailov et al., 2023], reinforcement learning from human feedback [Ouyang et al., 2022], and generation with distributional control [Khalifa et al., 2021], all of which can be viewed as forms of variational Bayesian inference due to their use of KL-divergence penalties [Korbak et al., 2022]. Finetuning methods have the benefit of avoiding increased runtime during decoding, but they typically cannot handle hard constraints, motivating the use of controlled generation at decoding time.
Among decoding-time approaches, many are focused on optimization, either through beam search [Meister et al., 2020] and heuristic search [Lu et al., 2021, Zhang et al., 2023b], or through gradientbased optimization in embedding space [Dathathri et al., 2019, Kumar et al., 2021]. Other approaches focus on sampling from a constrained or modified distribution [Zhang et al., 2023a], including naive rejection sampling [Poesia et al., 2022], but also more sophisticated Markov Chain Monte Carlo (MCMC) samplers [Miao et al., 2019, Hie et al., 2022] that make use of specialized proposal distributions [Zhang et al., 2020] or the gradients of continuous embeddings [Qin et al., 2022, Kumar et al., 2022]. However, a downside of MCMC methods is that they require potentially many iterations before producing a sample from the desired distribution, limiting their speed and usefulness. In contrast, SMC steering maintains the autoregressive nature of both regular decoding and beam search while still allowing constraints to be applied. As such, our method achieves the same overhead as beam search, while continuing to sample from the desired distribution instead of optimizing. This enables SMC steering to generate a diversity of constrained completions without the need for additional machinery [Vijayakumar et al., 2016].</p>
<p>Researchers have also proposed using the posterior distributions of probabilistic generative models defined in part using Transformers for tasks beyond constrained generation, e.g. semantic segmentation and household navigation, where the general world knowledge learned by the LLM is used to inform priors [Li et al., 2023]. Probabilistic programming tools like LLaMPPL, which support building models that use LLMs and performing efficient inference in them, could help make such approaches more accessible and scalable.</p>
<h1>References</h1>
<p>Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.</p>
<p>Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: A query language for large language models. arXiv preprint arXiv:2212.06094, 2022.</p>
<p>Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. Gen: a general-purpose probabilistic programming system with programmable inference. In Proceedings of the 40th acm sigplan conference on programming language design and implementation, pages 221-236, 2019.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019.</p>
<p>Pierre Del Moral. Feynman-kac formulae. Springer, 2004.
David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. arXiv preprint arXiv:2005.05339, 2020.</p>
<p>Paul Fearnhead and Peter Clifford. On-line inference for hidden markov models via particle filters. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(4):887-899, 2003.</p>
<p>Brian Hie, Salvatore Candido, Zeming Lin, Ori Kabeli, Roshan Rao, Nikita Smetanin, Tom Sercu, and Alexander Rives. A high-level programming language for generative protein design. bioRxiv, pages 2022-12, 2022.</p>
<p>Muhammad Khalifa, Hady Elsahar, and Marc Dymetman. A distributional approach to controlled text generation. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=jWkw45-9AbL.</p>
<p>Tomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed as Bayesian inference. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1083-1091, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.77.</p>
<p>Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. Controlled text generation as continuous optimization with multiple constraints. Advances in Neural Information Processing Systems, 34:14542-14554, 2021.</p>
<p>Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Constrained sampling from language models via Langevin dynamics in embedding spaces. arXiv preprint arXiv:2205.12558, 2022.</p>
<p>Alexander K Lew, Michael Henry Tessler, Vikash K Mansinghka, and Joshua B Tenenbaum. Leveraging unstructured statistical knowledge in a probabilistic language of thought. In Proceedings of the Annual Conference of the Cognitive Science Society, 2020.</p>
<p>Alexander K Lew, Marco Cusumano-Towner, and Vikash K Mansinghka. Recursive monte carlo and variational inference with auxiliary variables. In Uncertainty in Artificial Intelligence, pages 1096-1106. PMLR, 2022.</p>
<p>Alexander K Lew, Matin Ghavamizadeh, Martin Rinard, and Vikash K Mansinghka. Probabilistic programming with stochastic probabilities. In Proceedings of the 44th ACM SIGPLAN Conference on Programming Language Design and Implementation, 2023.</p>
<p>Belinda Z Li, William Chen, Pratyusha Sharma, and Jacob Andreas. Lampp: Language models as probabilistic priors for perception and action. arXiv e-prints, pages arXiv-2302, 2023.</p>
<p>Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic A* esque decoding: Constrained text generation with lookahead heuristics. arXiv preprint arXiv:2112.08726, 2021.</p>
<p>Clara Meister, Tim Vieira, and Ryan Cotterell. If beam search is the answer, what was the question? arXiv preprint arXiv:2010.02650, 2020.</p>
<p>Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. CGMH: Constrained sentence generation by metropolis-hastings sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6834-6842, 2019.</p>
<p>Microsoft and Scott Lundberg. Guidance: A guidance language for controlling large language models., 2023. URL https://github.com/microsoft/guidance.</p>
<p>Kenton Murray and David Chiang. Correcting length bias in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 212-223, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/ W18-6322. URL https://aclanthology.org/W18-6322.</p>
<p>Normal Computing and Remi Louf. Outlines: Generative model programming, May 2023. URL https://github.com/normal-computing/outlines.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.</p>
<p>Damian Pascual, Beni Egressy, Florian Bolli, and Roger Wattenhofer. Directed beam search: Plug-and-play lexically constrained language generation. arXiv preprint arXiv:2012.15416, 2020.</p>
<p>Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227, 2022.</p>
<p>Peng Qian and Roger Levy. Flexible generation from fragmentary linguistic input. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8176-8196, 2022.</p>
<p>Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. COLD decoding: Energy-based constrained text generation with Langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Allen Roush, Sanjay Basu, Akshay Moorthy, and Dmitry Dubovoy. Most language models can be poets too: An ai writing assistant and constrained text generation studio. In Proceedings of the Second Workshop on When Creative AI Meets Conversational AI, pages 9-15, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMe3B_J.</p>
<p>Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den Broeck. Tractable control for autoregressive language generation. arXiv preprint arXiv:2304.07438, 2023a.</p>
<p>Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue. Language generation via combinatorial constraint satisfaction: A tree search enhanced Monte-Carlo approach. arXiv preprint arXiv:2011.12334, 2020.</p>
<p>Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510, 2023b.</p>
<p>Tan Zhi-Xuan. GenGPT3.jl: GPT-3 as a generative function in Gen.jl, November 2022. URL https://github.com/probcomp/GenGPT3.jl.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=92gvk82DE-.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In all our examples, the distribution of $S_{t}$ depends only on the value of $S_{t-1}$ and the time step $t$ itself, so $M_{t}$ is well-defined. LLaMPPL does support programs that maintain and depend on other state, which could be formalized as Feynman-Kac models on extended state spaces.
${ }^{2}$ In all our examples, this product is uniquely determined by $t, s_{t-1}$, and $s_{t}$, so $G_{t}$ is well-defined. But as in the previous footnote, LLaMPL does support the more general case, which could be formalized by extending the state space of the Feynman-Kac model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
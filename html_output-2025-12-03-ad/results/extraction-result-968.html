<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-968 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-968</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-968</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-e1c0662d089c5dc2e44f7c59a94db7f86ac0797f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e1c0662d089c5dc2e44f7c59a94db7f86ac0797f" target="_blank">CausalAgents: A Robustness Benchmark for Motion Forecasting</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This paper constructs a new benchmark for evaluating and improving model robustness by applying perturbations to existing data, and conducts an extensive labeling effort to identify causal agents in the Waymo Open Motion Dataset (WOMD), and uses these labels to perturb the data by deleting non-causal agents from the scene.</p>
                <p><strong>Paper Abstract:</strong> As machine learning models become increasingly prevalent in motion forecasting for autonomous vehicles (AVs), it is critical to ensure that model predictions are safe and reliable. In this paper, we examine the robustness of motion forecasting to non-causal perturbations. We construct a new benchmark for evaluating and improving model robustness by applying perturbations to existing data. Specifically, we conduct an extensive labeling effort to identify causal agents, or agents whose presence influences human drivers’ behavior, in the Waymo Open Motion Dataset (WOMD), and we use these labels to perturb the data by deleting non-causal agents from the scene. We evaluate a diverse set of state-of-the-art deep-learning models on our proposed benchmark and find that all evaluated models exhibit large shifts under non-causal perturbation: we observe a surprising 25-38% relative change in minADE as compared to the original. In addition, we investigate techniques to improve model robustness, including increasing the training dataset size and using targeted data augmentations that randomly drop non-causal agents throughout training. Finally, we release the causal agent labels as an extension to WOMD and the robustness benchmarks to aid the community in building more reliable and safe deep-learning models for motion forecasting 1.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e968.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e968.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human causal labels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-labeled causal agent identification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators label agents in Waymo Open Motion Dataset (WOMD) scenes as causal (would influence a human driver's behavior) or non-causal; labels are used to construct interventions (agent deletions) and to evaluate model sensitivity to spurious/contextual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Human causal agent labeling</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Five human annotators inspected 20 s driving segments and marked any agent (vehicle, pedestrian, cyclist) whose presence could plausibly modify human driver behavior at any time in the segment as 'causal'. Labels are aggregated conservatively (an agent is causal if any labeler marked it) to avoid false negatives. These labels define which agents may be treated as distractors (non-causal) for downstream deletion interventions and training augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Waymo Open Motion Dataset (WOMD) validation segments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-world recorded autonomous-driving scenes (20 s segments, used to create 9.1 s examples); not an interactive simulator but allows deterministic 'virtual interventions' by editing inputs (deleting agents) while keeping ground-truth AV trajectories unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Manual variable selection via conservative human annotation (label as causal when in doubt) to identify distractors (non-causal agents) for deletion or data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/irreproducible context agents (non-causal agents), irrelevant static context (parked cars), features correlating with trajectories but not causally linked.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Human judgment: labelers decide which agents could influence behavior; then model sensitivity to deleting those agents is measured (Abs(Δ), IoU) to detect dependence on distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Use labeled non-causal set to perform data augmentation (randomly drop non-causal agents during training) to reduce model reliance on those variables.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventional deletion: remove labeled causal/non-causal agents from inputs and measure whether predicted trajectories change; if predictions change despite non-causal deletion, this refutes model invariance and indicates spurious dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Not applicable as a predictive method; using labels to train with non-causal-drop augmentation yields reduced sensitivity (example: MP++ Drop Non-causal: original minADE 0.373, perturbed 0.389, Abs(Δ)=0.138 vs baseline MP++ 0.395/0.408 Abs(Δ)=0.150).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline labeled-agnostic training shows higher sensitivity; e.g., MP++ baseline (no non-causal augmentation) original 0.395 perturbed 0.408 Abs(Δ)=0.150 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Dataset-level: on average 13% of agents are causal (≈87% non-causal), most scenes have <30% causal agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conservative human labels provide a practical, high-coverage way to identify likely distractors; intervening on these labeled agents (deletion) reveals that state-of-the-art models often rely on many non-causal agents, and training with augmentations that drop labeled non-causal agents reduces model sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e968.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e968.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-deletion perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent deletion perturbation suite (RemoveNoncausal / RemoveNoncausalEqual / RemoveStatic / RemoveCausal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of controlled virtual interventions implemented by marking an object's valid flag false across time to delete it from the input scene; used to probe causal reliance of forecasting models and to refute spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Agent-deletion perturbations (RemoveNoncausal, RemoveNoncausalEqual, RemoveStatic, RemoveCausal)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Four deterministic input interventions: RemoveNoncausal deletes all non-causal (human-labeled) agents; RemoveNoncausalEqual deletes a random subset of non-causal agents equal in count to causal agents; RemoveStatic deletes agents with negligible motion (thresholded by L2 displacement); RemoveCausal deletes all causal agents. Deletions are implemented by setting the object's valid mask to false at all time steps so models ignore the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Waymo Open Motion Dataset (WOMD) validation scenes (virtual interventions applied at input level)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Recorded, multi-agent driving scenes; not an active simulator but supports deterministic per-example input edits that preserve the original ground-truth AV trajectory (so interventions are 'counterfactual' in the input representation only).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant contextual agents, static parked objects, correlated-but-noncausal scene features that models may use to infer modes.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detect spurious reliance by measuring changes in model outputs under deletions: per-example absolute change in minADE (Abs(Δ)) and IoU-based trajectory set changes quantify sensitivity to deletions of presumed distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventional refutation: if deleting non-causal agents (a non-causal perturbation by definition) changes the model's predicted distribution, this refutes the model's invariance and indicates it exploited spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used as a testbed for robustness techniques, models trained with targeted augmentations show reduced Abs(Δ): e.g., augmenting training with Drop Static Context reduced MP++-All Abs(Δ) from 0.226 to 0.183 (Table 3) under RemoveNoncausal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Across models, average Abs(Δ) under RemoveNoncausal ≈ 0.131 (mean across architectures) and performance degradation relative to original minADE ranged ~25–38% (Table 9 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Per-scene: varies; dataset average fraction non-causal ≈ 87% of agents; sensitivity correlates with fraction removed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Virtual deletions of labeled non-causal agents reveal substantial model brittleness: although most examples minimally change, a long tail exhibits large shifts (>1 m); deleting more non-causal agents increases sensitivity; removing labeled causal agents produces larger sensitivity than removing equal-count non-causal subsets, validating label usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e968.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e968.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Drop-aug training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training data augmentations: Drop Context / Drop Static Context / Drop Non-causal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Random-context-dropping augmentations applied during training that randomly remove context agents (either any context agent, only static ones, or only labeled non-causal agents) to reduce model reliance on spurious/distractor agents and improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Random agent-dropping data augmentation (Drop Context, Drop Static Context, Drop Non-causal)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At training time, context agents are randomly dropped with a fixed probability (e.g., 0.1). Variants include Drop Context (any context agent), Drop Static Context (only agents with <0.1 m displacement), and Drop Non-causal (drop only those agents labeled non-causal). These augmentations expose the model to scenes with fewer context agents and reduce overfitting to spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WOMD training set (models trained on modified datasets with augmented examples)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Recorded driving dataset augmented offline by creating perturbed copies (agent deletions) and merging them into the training set; not an interactive active-experiment environment but allows many virtual interventions to be added to training data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Randomized input dropout to reduce model reliance on contextual/distractor variables; targeted dropout for likely distractors (static or labeled non-causal).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Static irrelevant agents, labeled non-causal agents, context features correlating with target trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Effect measured empirically by comparing robustness metrics (Abs(Δ), minADE) on held-out perturbed test sets (RemoveNoncausal).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Stochastic removal of agent inputs during training to reduce their effective weight/importance in learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Not formal causal refutation; improves invariance empirically by training on examples with distractors absent; robustness validated via intervening deletions at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Examples from paper: MP++-All Drop Static Context: original minADE 0.819, perturbed 0.837, Abs(Δ)=0.183 (std ±0.26), relative Abs(Δ)/orig ≈22.3% vs baseline MP++-All original 0.900 perturbed 0.945 Abs(Δ)=0.226 (25.1%). MP++ Drop Non-causal: original 0.373 perturbed 0.389 Abs(Δ)=0.138 vs MP++ baseline original 0.395 perturbed 0.408 Abs(Δ)=0.150 (Table 3 & 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline models without these augmentations show larger Abs(Δ) and worse perturbed minADE (see tables above).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Augmentation magnitude is controlled by sampling probability (e.g., drop prob 0.1) and by the number of available non-causal agents per scene (dataset average ~87% non-causal).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Heuristic augmentations that preferentially drop agents likely to be non-causal (static agents) provide better robustness gains than indiscriminate context dropping; targeted augmentations using labeled non-causal agents further reduce sensitivity and improve minADE on perturbed test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e968.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e968.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AbsDelta & IoU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-example Abs(Δ) minADE metric and IoU-based trajectory-set metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two metrics designed to detect and quantify model sensitivity to input perturbations: Abs(Δ) measures per-example absolute change in minADE between original and perturbed scenes; the IoU-based metric measures geometric overlap between sets of predicted trajectories (voxelized into top-down grid).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Abs(Δ) per-example minADE change and IoU trajectory-set metric</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Abs(Δ) = average per-example |perturbed_minADE - original_minADE| across dataset; reports mean, std, and percent relative to original minADE. The IoU metric upsamples each predicted trajectory to 100 Hz, voxelizes trajectories on 2D top-down grid (0.5 m resolution), and computes intersection-over-union between original and perturbed predicted trajectory sets (ignoring probabilities/speeds) to capture geometric change.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WOMD perturbed vs original predictions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to deterministic predictions from forecasting models on original and perturbed (agent-deleted) recorded scenes; enables detection of spurious reliance without assuming the ground truth trajectory is the unique valid outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Detects reliance on irrelevant/contextual agents manifested as changed predictions under non-causal deletions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Abs(Δ) flags examples with large per-example changes in minADE; IoU identifies geometric change between predicted trajectory sets even when minADE to ground truth might improve.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Used as evidence: significant Abs(Δ) or low IoU after non-causal deletions refutes the claim that the model ignores non-causal agents.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used as evaluation: average Abs(Δ) across models and perturbations reported (e.g., average Abs(Δ) for RemoveNoncausal across models = 0.131 m; RemoveCausal = 0.145 m). IoU distributions show greater sensitivity for RemoveCausal and RemoveNoncausal than for RemoveNoncausalEqual.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Abs(Δ) and IoU reveal complementary aspects of sensitivity: Abs(Δ) quantifies changes relative to ground truth error, whereas IoU captures geometric distributional changes in outputs; both detect substantial model dependence on non-causal inputs not evident from average minADE alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e968.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e968.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Planner-difference influence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent influence approximation via planner-difference (from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related algorithmic approach that approximates an agent's influence by comparing planner outputs when the agent is included versus excluded; cited as prior work and noted to have limitations in multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent prioritization for autonomous navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Planner-difference agent influence approximation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An influence metric approximated by running a planner (in simulation) with and without a specific agent and measuring the difference between the two resulting plans; intended to estimate the agent's impact on the ego plan and prioritize processing resources accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulation-based planner evaluation (as used in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Uses a planner in simulation to evaluate pairwise differences in planned trajectories when toggling agent presence; operates in a simulated planning environment rather than purely recorded data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Attempts to quantify agent relevance; may fail to capture multi-agent redundancy where removing a single agent does not change planner outcomes though the group is influential.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects influence by computing plan differences when an agent is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Not described in detail here; the paper notes this approach can be unreliable in scenes where multiple agents jointly affect behavior (e.g., two pedestrians jointly blocking a path).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an algorithmic method for agent relevance/prioritization but noted to potentially miss joint multi-agent effects; motivates the use of human labels in this work to avoid such failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e968.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e968.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional behavior prediction (CBP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional behavior prediction / deep interactivity quantification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior deep-learning based method that quantifies interactivity between agents using learned conditional models; cited as related work and noted to possibly suffer robustness issues similar to other algorithmic relevance estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Identifying driver interactions via conditional behavior prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Conditional behavior prediction (deep interactivity quantification)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A deep learning model trained to quantify interactivity by predicting other agents' behaviors conditioned on potential actions; yields interaction scores but can inherit robustness problems and may misestimate importance in complex scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Driving datasets / behavior prediction benchmarks (as used by cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Supervised learning on recorded driving data featuring multiple agents; not necessarily interactive in the experimental design sense.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>May misattribute importance due to dataset correlations and complex multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an algorithmic approach to quantify agent interactivity; authors note such learned scores can be unreliable in complex multi-agent scenes, supporting their choice of human labels to define causal agents for robust evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalAgents: A Robustness Benchmark for Motion Forecasting', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agent prioritization for autonomous navigation <em>(Rating: 2)</em></li>
                <li>Identifying driver interactions via conditional behavior prediction <em>(Rating: 2)</em></li>
                <li>Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning <em>(Rating: 2)</em></li>
                <li>Pedestrian motion model using non-parametric trajectory clustering and discrete transition points <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-968",
    "paper_id": "paper-e1c0662d089c5dc2e44f7c59a94db7f86ac0797f",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Human causal labels",
            "name_full": "Human-labeled causal agent identification",
            "brief_description": "Human annotators label agents in Waymo Open Motion Dataset (WOMD) scenes as causal (would influence a human driver's behavior) or non-causal; labels are used to construct interventions (agent deletions) and to evaluate model sensitivity to spurious/contextual inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Human causal agent labeling",
            "method_description": "Five human annotators inspected 20 s driving segments and marked any agent (vehicle, pedestrian, cyclist) whose presence could plausibly modify human driver behavior at any time in the segment as 'causal'. Labels are aggregated conservatively (an agent is causal if any labeler marked it) to avoid false negatives. These labels define which agents may be treated as distractors (non-causal) for downstream deletion interventions and training augmentations.",
            "environment_name": "Waymo Open Motion Dataset (WOMD) validation segments",
            "environment_description": "Real-world recorded autonomous-driving scenes (20 s segments, used to create 9.1 s examples); not an interactive simulator but allows deterministic 'virtual interventions' by editing inputs (deleting agents) while keeping ground-truth AV trajectories unchanged.",
            "handles_distractors": true,
            "distractor_handling_technique": "Manual variable selection via conservative human annotation (label as causal when in doubt) to identify distractors (non-causal agents) for deletion or data augmentation.",
            "spurious_signal_types": "Irrelevant/irreproducible context agents (non-causal agents), irrelevant static context (parked cars), features correlating with trajectories but not causally linked.",
            "detection_method": "Human judgment: labelers decide which agents could influence behavior; then model sensitivity to deleting those agents is measured (Abs(Δ), IoU) to detect dependence on distractors.",
            "downweighting_method": "Use labeled non-causal set to perform data augmentation (randomly drop non-causal agents during training) to reduce model reliance on those variables.",
            "refutation_method": "Interventional deletion: remove labeled causal/non-causal agents from inputs and measure whether predicted trajectories change; if predictions change despite non-causal deletion, this refutes model invariance and indicates spurious dependence.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Not applicable as a predictive method; using labels to train with non-causal-drop augmentation yields reduced sensitivity (example: MP++ Drop Non-causal: original minADE 0.373, perturbed 0.389, Abs(Δ)=0.138 vs baseline MP++ 0.395/0.408 Abs(Δ)=0.150).",
            "performance_without_robustness": "Baseline labeled-agnostic training shows higher sensitivity; e.g., MP++ baseline (no non-causal augmentation) original 0.395 perturbed 0.408 Abs(Δ)=0.150 (Table 4).",
            "has_ablation_study": true,
            "number_of_distractors": "Dataset-level: on average 13% of agents are causal (≈87% non-causal), most scenes have &lt;30% causal agents.",
            "key_findings": "Conservative human labels provide a practical, high-coverage way to identify likely distractors; intervening on these labeled agents (deletion) reveals that state-of-the-art models often rely on many non-causal agents, and training with augmentations that drop labeled non-causal agents reduces model sensitivity.",
            "uuid": "e968.0",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Agent-deletion perturbations",
            "name_full": "Agent deletion perturbation suite (RemoveNoncausal / RemoveNoncausalEqual / RemoveStatic / RemoveCausal)",
            "brief_description": "A set of controlled virtual interventions implemented by marking an object's valid flag false across time to delete it from the input scene; used to probe causal reliance of forecasting models and to refute spurious correlations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Agent-deletion perturbations (RemoveNoncausal, RemoveNoncausalEqual, RemoveStatic, RemoveCausal)",
            "method_description": "Four deterministic input interventions: RemoveNoncausal deletes all non-causal (human-labeled) agents; RemoveNoncausalEqual deletes a random subset of non-causal agents equal in count to causal agents; RemoveStatic deletes agents with negligible motion (thresholded by L2 displacement); RemoveCausal deletes all causal agents. Deletions are implemented by setting the object's valid mask to false at all time steps so models ignore the agent.",
            "environment_name": "Waymo Open Motion Dataset (WOMD) validation scenes (virtual interventions applied at input level)",
            "environment_description": "Recorded, multi-agent driving scenes; not an active simulator but supports deterministic per-example input edits that preserve the original ground-truth AV trajectory (so interventions are 'counterfactual' in the input representation only).",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Irrelevant contextual agents, static parked objects, correlated-but-noncausal scene features that models may use to infer modes.",
            "detection_method": "Detect spurious reliance by measuring changes in model outputs under deletions: per-example absolute change in minADE (Abs(Δ)) and IoU-based trajectory set changes quantify sensitivity to deletions of presumed distractors.",
            "downweighting_method": null,
            "refutation_method": "Interventional refutation: if deleting non-causal agents (a non-causal perturbation by definition) changes the model's predicted distribution, this refutes the model's invariance and indicates it exploited spurious signals.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When used as a testbed for robustness techniques, models trained with targeted augmentations show reduced Abs(Δ): e.g., augmenting training with Drop Static Context reduced MP++-All Abs(Δ) from 0.226 to 0.183 (Table 3) under RemoveNoncausal.",
            "performance_without_robustness": "Across models, average Abs(Δ) under RemoveNoncausal ≈ 0.131 (mean across architectures) and performance degradation relative to original minADE ranged ~25–38% (Table 9 and Table 2).",
            "has_ablation_study": true,
            "number_of_distractors": "Per-scene: varies; dataset average fraction non-causal ≈ 87% of agents; sensitivity correlates with fraction removed.",
            "key_findings": "Virtual deletions of labeled non-causal agents reveal substantial model brittleness: although most examples minimally change, a long tail exhibits large shifts (&gt;1 m); deleting more non-causal agents increases sensitivity; removing labeled causal agents produces larger sensitivity than removing equal-count non-causal subsets, validating label usefulness.",
            "uuid": "e968.1",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Drop-aug training",
            "name_full": "Training data augmentations: Drop Context / Drop Static Context / Drop Non-causal",
            "brief_description": "Random-context-dropping augmentations applied during training that randomly remove context agents (either any context agent, only static ones, or only labeled non-causal agents) to reduce model reliance on spurious/distractor agents and improve robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Random agent-dropping data augmentation (Drop Context, Drop Static Context, Drop Non-causal)",
            "method_description": "At training time, context agents are randomly dropped with a fixed probability (e.g., 0.1). Variants include Drop Context (any context agent), Drop Static Context (only agents with &lt;0.1 m displacement), and Drop Non-causal (drop only those agents labeled non-causal). These augmentations expose the model to scenes with fewer context agents and reduce overfitting to spurious correlations.",
            "environment_name": "WOMD training set (models trained on modified datasets with augmented examples)",
            "environment_description": "Recorded driving dataset augmented offline by creating perturbed copies (agent deletions) and merging them into the training set; not an interactive active-experiment environment but allows many virtual interventions to be added to training data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Randomized input dropout to reduce model reliance on contextual/distractor variables; targeted dropout for likely distractors (static or labeled non-causal).",
            "spurious_signal_types": "Static irrelevant agents, labeled non-causal agents, context features correlating with target trajectories.",
            "detection_method": "Effect measured empirically by comparing robustness metrics (Abs(Δ), minADE) on held-out perturbed test sets (RemoveNoncausal).",
            "downweighting_method": "Stochastic removal of agent inputs during training to reduce their effective weight/importance in learned policies.",
            "refutation_method": "Not formal causal refutation; improves invariance empirically by training on examples with distractors absent; robustness validated via intervening deletions at test time.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Examples from paper: MP++-All Drop Static Context: original minADE 0.819, perturbed 0.837, Abs(Δ)=0.183 (std ±0.26), relative Abs(Δ)/orig ≈22.3% vs baseline MP++-All original 0.900 perturbed 0.945 Abs(Δ)=0.226 (25.1%). MP++ Drop Non-causal: original 0.373 perturbed 0.389 Abs(Δ)=0.138 vs MP++ baseline original 0.395 perturbed 0.408 Abs(Δ)=0.150 (Table 3 & 4).",
            "performance_without_robustness": "Baseline models without these augmentations show larger Abs(Δ) and worse perturbed minADE (see tables above).",
            "has_ablation_study": true,
            "number_of_distractors": "Augmentation magnitude is controlled by sampling probability (e.g., drop prob 0.1) and by the number of available non-causal agents per scene (dataset average ~87% non-causal).",
            "key_findings": "Heuristic augmentations that preferentially drop agents likely to be non-causal (static agents) provide better robustness gains than indiscriminate context dropping; targeted augmentations using labeled non-causal agents further reduce sensitivity and improve minADE on perturbed test sets.",
            "uuid": "e968.2",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "AbsDelta & IoU",
            "name_full": "Per-example Abs(Δ) minADE metric and IoU-based trajectory-set metric",
            "brief_description": "Two metrics designed to detect and quantify model sensitivity to input perturbations: Abs(Δ) measures per-example absolute change in minADE between original and perturbed scenes; the IoU-based metric measures geometric overlap between sets of predicted trajectories (voxelized into top-down grid).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Abs(Δ) per-example minADE change and IoU trajectory-set metric",
            "method_description": "Abs(Δ) = average per-example |perturbed_minADE - original_minADE| across dataset; reports mean, std, and percent relative to original minADE. The IoU metric upsamples each predicted trajectory to 100 Hz, voxelizes trajectories on 2D top-down grid (0.5 m resolution), and computes intersection-over-union between original and perturbed predicted trajectory sets (ignoring probabilities/speeds) to capture geometric change.",
            "environment_name": "WOMD perturbed vs original predictions",
            "environment_description": "Applied to deterministic predictions from forecasting models on original and perturbed (agent-deleted) recorded scenes; enables detection of spurious reliance without assuming the ground truth trajectory is the unique valid outcome.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Detects reliance on irrelevant/contextual agents manifested as changed predictions under non-causal deletions.",
            "detection_method": "Abs(Δ) flags examples with large per-example changes in minADE; IoU identifies geometric change between predicted trajectory sets even when minADE to ground truth might improve.",
            "downweighting_method": null,
            "refutation_method": "Used as evidence: significant Abs(Δ) or low IoU after non-causal deletions refutes the claim that the model ignores non-causal agents.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Used as evaluation: average Abs(Δ) across models and perturbations reported (e.g., average Abs(Δ) for RemoveNoncausal across models = 0.131 m; RemoveCausal = 0.145 m). IoU distributions show greater sensitivity for RemoveCausal and RemoveNoncausal than for RemoveNoncausalEqual.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Abs(Δ) and IoU reveal complementary aspects of sensitivity: Abs(Δ) quantifies changes relative to ground truth error, whereas IoU captures geometric distributional changes in outputs; both detect substantial model dependence on non-causal inputs not evident from average minADE alone.",
            "uuid": "e968.3",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Planner-difference influence",
            "name_full": "Agent influence approximation via planner-difference (from prior work)",
            "brief_description": "Related algorithmic approach that approximates an agent's influence by comparing planner outputs when the agent is included versus excluded; cited as prior work and noted to have limitations in multi-agent interactions.",
            "citation_title": "Agent prioritization for autonomous navigation",
            "mention_or_use": "mention",
            "method_name": "Planner-difference agent influence approximation",
            "method_description": "An influence metric approximated by running a planner (in simulation) with and without a specific agent and measuring the difference between the two resulting plans; intended to estimate the agent's impact on the ego plan and prioritize processing resources accordingly.",
            "environment_name": "Simulation-based planner evaluation (as used in cited prior work)",
            "environment_description": "Uses a planner in simulation to evaluate pairwise differences in planned trajectories when toggling agent presence; operates in a simulated planning environment rather than purely recorded data.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Attempts to quantify agent relevance; may fail to capture multi-agent redundancy where removing a single agent does not change planner outcomes though the group is influential.",
            "detection_method": "Detects influence by computing plan differences when an agent is removed.",
            "downweighting_method": null,
            "refutation_method": "Not described in detail here; the paper notes this approach can be unreliable in scenes where multiple agents jointly affect behavior (e.g., two pedestrians jointly blocking a path).",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as an algorithmic method for agent relevance/prioritization but noted to potentially miss joint multi-agent effects; motivates the use of human labels in this work to avoid such failure modes.",
            "uuid": "e968.4",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Conditional behavior prediction (CBP)",
            "name_full": "Conditional behavior prediction / deep interactivity quantification",
            "brief_description": "Prior deep-learning based method that quantifies interactivity between agents using learned conditional models; cited as related work and noted to possibly suffer robustness issues similar to other algorithmic relevance estimators.",
            "citation_title": "Identifying driver interactions via conditional behavior prediction",
            "mention_or_use": "mention",
            "method_name": "Conditional behavior prediction (deep interactivity quantification)",
            "method_description": "A deep learning model trained to quantify interactivity by predicting other agents' behaviors conditioned on potential actions; yields interaction scores but can inherit robustness problems and may misestimate importance in complex scenes.",
            "environment_name": "Driving datasets / behavior prediction benchmarks (as used by cited work)",
            "environment_description": "Supervised learning on recorded driving data featuring multiple agents; not necessarily interactive in the experimental design sense.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "May misattribute importance due to dataset correlations and complex multi-agent interactions.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Referenced as an algorithmic approach to quantify agent interactivity; authors note such learned scores can be unreliable in complex multi-agent scenes, supporting their choice of human labels to define causal agents for robust evaluation.",
            "uuid": "e968.5",
            "source_info": {
                "paper_title": "CausalAgents: A Robustness Benchmark for Motion Forecasting",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agent prioritization for autonomous navigation",
            "rating": 2
        },
        {
            "paper_title": "Identifying driver interactions via conditional behavior prediction",
            "rating": 2
        },
        {
            "paper_title": "Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning",
            "rating": 2
        },
        {
            "paper_title": "Pedestrian motion model using non-parametric trajectory clustering and discrete transition points",
            "rating": 1
        }
    ],
    "cost": 0.015200499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CausalAgents: A Robustness Benchmark for Motion Forecasting</h1>
<p>Rebecca Roelofs<em> ${ }^{</em> \dagger}$ Liting Sun<em> ${ }^{</em> \circ}$ Ben Caine ${ }^{\dagger}$ Khaled S. Refaat ${ }^{\circ}$<br>Ben Sapp ${ }^{\circ}$ Scott Ettinger ${ }^{\circ}$ Wei Chai ${ }^{\circ}$<br>October 10,2022</p>
<h4>Abstract</h4>
<p>As machine learning models become increasingly prevalent in motion forecasting for autonomous vehicles (AVs), it is critical to ensure that model predictions are safe and reliable. However, exhaustively collecting and labeling the data necessary to fully test the long tail of rare and challenging scenarios is difficult and expensive. In this work, we construct a new benchmark for evaluating and improving model robustness by applying perturbations to existing data. Specifically, we conduct an extensive labeling effort to identify causal agents, or agents whose presence influences human drivers' behavior in any format, in the Waymo Open Motion Dataset (WOMD), and we use these labels to perturb the data by deleting non-causal agents from the scene. We evaluate a diverse set of state-of-the-art deep-learning model architectures on our proposed benchmark and find that all models exhibit large shifts under even non-causal perturbation: we observe a $25-38 \%$ relative change in minADE as compared to the original. We also investigate techniques to improve model robustness, including increasing the training dataset size and using targeted data augmentations that randomly drop non-causal agents throughout training. Finally, we release the causal agent labels as an additional attribute to WOMD and the robustness benchmarks to aid the community in building more reliable and safe deep-learning models for motion forecasting ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Machine learning models are increasingly prevalent in trajectory prediction and motion planning tasks for autonomous vehicles (AVs) $[5,6,7,10,38,30,20,16,31,39,23,18,21]$. To safely deploy such models, they must have reliable, robust predictions across a diverse range of scenarios and they must be insensitive to spurious features, or patterns in the data that fail to generalize to new environments. However, collecting and labeling the required data to both evaluate and improve model robustness is often expensive and difficult, in part due to the long tail of rare and difficult scenarios [22].
In this work, we propose perturbing existing data via agent deletions to evaluate and improve model robustness to spurious features. To be useful in our setting, the perturbations must preserve the correct labels and not change the ground truth trajectory of the AV. Since generating such perturbations requires high-level scene understanding as well as causal reasoning, we propose using human labelers to identify irrelevant agents. Specifically, we define a non-causal agent as an agent whose deletion does not cause the ground truth trajectory of a given target agent to change. We then construct a robustness evaluation dataset that consists of perturbed examples where we remove all non-causal agents from each scene, and we study model behavior under alternate perturbations, such as removing causal agents, removing a subset of non-causal agents, or removing stationary agents.</p>
<p>Using our perturbed datasets, we then conduct an extensive experimental study exploring how factors such as model architecture, dataset size, and data augmentation effect model sensitivity. We also propose novel</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Trajectory prediction is sensitive to removing non-causal agents. We show a top-down view of a scene from the WOMD (left) and a perturbed version of the scene where we delete all non-causal agents (right). The AV and its predicted trajectories via the Scene Transformer model [25] are shown in blue, the ground truth trajectory of the AV is grey, and the ground truth of other agents is green. The perturbation causes a large shift in minADE because the model fails to predict the ground truth mode (a right turn), which indicates the brittleness of the model to such perturbations.</p>
<p>metrics to quantify model sensitivity, including one that captures per-example absolute changes between predicted and ground truth trajectories and another that directly reflects how the model outputs change under perturbation via IoU (intersection-over-union) without referring to the ground truth trajectory. The second metric helps to address the issue that the ground truth trajectory is one sample from a distribution of many possibly correct trajectories. Additionally, we visualize scenes with large model sensitivity to understand why performance degrades under perturbations.</p>
<p>Our results show that existing motion forecasting models are sensitive to deleting non-causal agents and can have pathological behavior dependencies on faraway or distant agents. For example, Figure 1 illustrates an original (left) and perturbed (right) scenes with non-causal agents removed. In the perturbed example, the model's prediction misses the right-turn mode, which corresponds to the ground-truth trajectory. Such brittleness could lead to serious consequences in autonomous driving systems if we rely on deep-learning models without further safety assurance from other techniques such as optimization and robotics algorithms. The main contributions of our work are as follows:</p>
<ol>
<li>We contribute a new robustness benchmark for the WOMD for evaluating trajectory prediction models' sensitivity to spurious correlations. We release the causal agent labels from human labelers as additional attributes to WOMD so that researchers can utilize the causal relationships between the agents for robustness evaluation and for other tasks such as agents relevance or ranking [29, 37].</li>
<li>We introduce two metrics to quantify the robustness of motion forecasting models to perturbations, including absolute per-example change in minADE and a trajectory set metric that measures sensitivity without using the ground truth as a reference.</li>
<li>We evaluate the robustness of several state-of-the-art motion forecasting models, including Multipath++ [38], Wayformer [24], and SceneTransformer [25]. We show that the absolute per-example change in minADE can range from 0.07-0.23 m (a significant 25 - 38% change relative to the original minADE). We find that all models are sensitive to deleting non-causal agents, and the model with the best overall performance (in terms of regular metrics used to quantify the trajectory prediction performance such as minADE) is not necessarily the most robust.</li>
<li>We show that increasing training dataset size and targeted data augmentations that remove non-causal agents can help improve model robustness.</li>
</ol>
<p>Overall, this is the first work focusing on the robustness of trajectory prediction models to perturbations based on human labels. Such robustness is critical for models deployed in a self-driving car where the reliability and safety requirements are of utmost importance. Ultimately, our goal is to provide a robustness benchmark which can aid the community to better evaluate model reliability, detect possible spurious correlations in deep-learning-based trajectory prediction models, and facilitate the development of more robust models or other mitigation techniques such as optimization and traditional robotic algorithms as complementary solutions to minimize safety risks.</p>
<h1>2 Related work</h1>
<p>Robustness evaluation on perturbations. Machine learning models are known to have brittle predictions under distribution shift. Across multiple domains, researchers have proposed robustness evaluation protocols that move beyond a fixed test set $[28,3,35,15,13,33,36]$. Evaluation can be broadly categorized into three types: (i) slicing, i.e. existing test data is sliced over multiple dimensions, (ii) perturbations, i.e. existing test data is modified via transformations, or (iii) dataset shift, i.e. new test data is drawn from a different distribution. Our work focuses on perturbations, which have previously been explored in both computer vision and NLP. In computer vision, researchers perturb images via pixel level noise corruptions [12, 15], spatial transformations [9, 11], and adversarial modifications [4, 35]. Such synthetic shifts are easy to apply to arbitrary images, but limited in that they do not test model invariance to more complex modifications such as deleting or modifying irrelevant parts of the image. In trajectory prediction, perturbations are potentially more valuable, since the models train on discrete inputs, namely, the agents and the roadgraph. Because of the structure of the problem, it is easier to reliably construct perturbations that do not modify the ground truth labels. This situation mirrors that of NLP, where sentences composed of discrete words can be modified in ways that do not change the prediction task, and indeed, such transformations have proven valuable for testing the robustness of models and identifying possible biases [8].
Robustness evaluation for trajectory prediction. The three types of robustness evaluation (slicing, perturbations, and dataset shift) described above also characterize the trajectory prediction literature. Slicing. The most common approach is to slice model performance along different hyperparameters and buckets, such as duration of the historical trajectories [26], size of the training data [17, 25], sampling frequency [2], number of agents in the scene [30, 25], criticality / interactivity of the scenarios [19, 10], and speed of the AV [25]. Perturbations. Another thread of related work focuses on the robustness of the algorithms to perturbations in both training and test data. For example, [2] introduced synthetic sensor noises into both the training and test process to evaluate the model's accuracy against sensor noises. [14] introduced $30 \%$ anomalies into the training data (with extra labels), and evaluated the robustness of the algorithm to anomalies in the training process. Dataset shift. Less work has focused on dataset shift due to the difficulty of collecting, annotating, and releasing entirely new data. Examples include training and testing in different locations or routes [32, 34], weather, time of day, and sensor noise [34].
Unlike prior work, we evaluate trajectory prediction models trained on the original dataset on "non-causal" domain shifts instead of hard domain shifts (such as weather or locations) since we manually transform our test set by leveraging the causal relationships among agents in the scene. Because these non-causal perturbations are closer to the original validation dataset than the hard domain shifts, the discrepancies we observe are in some ways a more immediate priority for improving model robustness. We focus on evaluating models' sensitivity to agent interactions because that is a complicated component of trajectory prediction and is crucially important for safety.
Agent relevance for autonomous driving. Since trajectory prediction in autonomous driving systems must reason about other agents in the scene, researchers have attempted to efficiently rank agents according to their impact on the AV. The main motivation of this line of work is to determine which agents to allocate computational resources to for processing in real time. In particular, [1] proposed a driver's saliency prediction model which incorporates an attention mechanism to understand salient features for driving context. [29] approximated an agent's influence by looking at the difference between two plans (a planner is running in</p>
<p>simulation) when a given agent is accounted for versus not. However, removing one agent at a time does not account for certain situations where multiple agents may be influencing the car in the same way i.e. two pedestrians are blocking the path of the car and removing one of the pedestrians has no influence on the car. In similar work, [37] quantifies interactivity using a deep learning model which can suffer from the same robustness issues. More generally, algorithmically defined importance/relevance or interaction scores can be unreliable, especially in scenes with complex interactions between the AV and surrounding agents. In this work, we use human labeling to decide which agents are important, and our motivation is to use these labels to test model robustness. In the future, our causal agent labels can be used to verify algorithmic definitions of agent importance or relevance.</p>
<p>Causal reasoning in autonomous driving. In a similar line of work, [27] collect causal annotations using human labelers for the Honda Research Institute Driving Dataset and they slice performance of an object detector over scenes with varying causal attributes. Our work instead uses causal labels to evaluate model robustness for trajectory prediction on WOMD, and we provide more fine-grained per-agent measurements of causality.</p>
<h1>3 Methods</h1>
<h3>3.1 Labeling causal agents in WOMD</h3>
<p>The objective of the labeling task is to identify all agents - cars, cyclists, or pedestrians - that are causal to the AV at any time during a driving segment. Although we are more interested in removing non-causal agents from each scene, we ask labelers to identify causal agents since there are typically fewer of them and they tend to be closer to the AV, making them easier for labelers to identify.</p>
<p>Data. We focus on labeling the WOMD validation data because our primary goal is to evaluate the robustness of models trained on the original dataset. Each example in WOMD is 9.1 seconds in length ( 91 steps at 10 Hz ) and is generated in overlapping windows from a 20 -second video segment. We label the 20 -second segments to give labelers access to a longer time horizon and to not waste resources on labeling overlapping scenes. Moreover, both the regular and interactive WOMD validation sets are generated from the same 20-second segments of data, hence, our causal labels can be used for both.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Camera images from a randomly chosen scene in the labeling UI. The causal agents are circled.
Labeling policy and UI. Causality is an inherently subjective label since human drivers may vary in their judgements of which agents in the scene affect their decisions. Therefore, we want to be overly conservative and identify as many causal agents as possible to maximize the likelihood that removed agents are actually non-causal. If human labelers are unsure if an agent is causal or not, we instruct them to include it as causal. We emphasize that false positives (identifying an agent as causal when it is truly non-causal) are acceptable to a certain extent, but we should avoid false negatives (failing to identify a truly causal agent). (Appendix A includes the exact instructions given to labelers.) That said, in ambiguous situations, we did not expect labelers to reason about chained causality relationships. For example, if the AV is driving behind a queue of 5 cars and the first car were to brake, it could eventually cause the car in front of the AV to brake. In this situation we would only expect the labeler to identify the car directly in front as causal.</p>
<p>The labeling UI is a web-based 3D view of the AV and its surroundings in the 20 -second segmented videos. An example is shown in Fig. 2 where the camera images from a randomly selected scene overlaid with the causal annotations provided by the human labelers.</p>
<p>Human annotations. To maximize coverage and avoid false negatives, each scene is annotated by 5 human labelers and we designate causal agents as all agents that any labeler identified as causal. Appendix B shows the distribution over causal agents for the number of human labelers who selected the agent as causal. The majority of causal agents are selected by all 5 labelers, but a significant portion ( $24 \%$ ) are selected by only 1 labeler.</p>
<h1>3.2 Causal agent statistics</h1>
<p>To understand the properties of causal agents, we compute several statistics of causal agents in the WOMD validation dataset, including the percentage of causal agents (Figure 3a), the distribution of the relative distance between the AV and the causal agents versus all surrounding agents (Figure 3b), and the breakdown of causal versus all surrounding agents by agent type (vehicle, pedestrian, or cyclist) (Figure 3c). Figure 3a
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Causal agent statistics. Causal agents are less frequent than non-causal agents (on average $13 \%$ of agents are causal), and, compared to typical agents, they tend to be closer to the AV. Cyclists are relatively more likely to be causal agents than pedestrians or vehicles.
shows that the majority of agents are actually non-causal: on average, only $13 \%$ of the total agents in the scene are labeled as causal, and $93 \%$ of scenes have less than $30 \%$ of the agents labeled as causal. Figure 3 b shows that causal agents are typically closer to the AV than non-causal agents; causal agents are an average distance of 28.4 m from the AV , compared to an average of 49.4 m over all agents. Figure 3 c shows the likelihood that an agent of a given type (Vehicle, Ped, or Cyclist) is causal. Surprisingly, cyclists are more likely to be causal agents than any other agents, and vehicles are more likely to be causal than pedestrians. We hypothesize that this is because cyclists usually share the road with the AV and have a strong prior of not respecting road boundaries like a car, whereas there are many parked cars that are not necessarily interacting with the AV and similarly pedestrians can be off the road on sidewalks.</p>
<h3>3.3 Perturbed datasets</h3>
<p>In this work, we consider perturbations that modify the scene by deleting agents. While it is possible to create more complex perturbations, such as adding noise to the xyz position of the agents, we start with deletion since it directly reflects the models' robustness regarding the causal relationships of agents in the scene. Object track states in the WOMD consist of the object's states (e.g., 3D center point, velocity vector, heading), as well as a valid flag to indicate which time steps have valid measurements. To delete an agent from the scene, we set its valid mask to false throughout all time steps (and we double check for each model implementation that all agent state is ignored if the valid bit is false). We consider four different perturbations:</p>
<ol>
<li>RemoveNoncausal: Removes all non-causal agents in the dataset.</li>
<li>
<p>RemoveNoncausalEqual: Removes an equal number of randomly selected non-causal agents as there are causal agents in the scene For example, if a scene has 5 causal agents, we randomly remove 5 non-causal agents. RemoveNoncausalEqual is meant to be a less aggressive form of RemoveNoncausal since it deletes fewer agents and it allows us to compare to RemoveCausal when controlling for the number of agents deleted.</p>
</li>
<li>
<p>RemoveStatic: Removes agents whose xyz positions do not change above a certain threshold (e.g. parked cars). We use a threshold of .1 m on the L2 distance of the agent's xyz state to account for sensor noise. Not all static agents are non-causal.</p>
</li>
<li>RemoveCausal: Removes all causal agents; the complement of RemoveNoncausal.</li>
</ol>
<p>Among them, we categorize both RemoveNoncausal and RemoveNoncausalEqual as "non-causal" perturbations. Specifically, to define non-causal perturbations, let us assume $X$ is a scenario representation, $Y$ is the ground truth trajectory of the AV, and $f$ is the ground-truth model that gives the relationship between $X$ and $Y$. If a perturbation $\Delta X$ satisfies $f(X+\Delta X)=f(X)=Y$, we define it as non-causal perturbation since it does not impact the relationship between $X$ and $Y$. We define a deep learning model $\hat{f}$ to be robust to non-causal perturbations if $\hat{f}(X+\Delta X)=\hat{f}(X)=\hat{Y} \forall$ non-causal $\Delta X$, where $\hat{Y}$ is the predicted trajectory from the model. Additionally, we consider RemoveStatic as an important baseline that does not require the human labels. We can thus apply it to the training dataset, which we explore in Section 4.3. Finally, we include the RemoveCausal perturbation as a sanity to ensure models are sensitive to deleting causal agents.</p>
<h1>3.4 Evaluation</h1>
<p>Since we only have camera and LiDAR data from the AV perspective, we only collect causal labels and evaluate model predictions for the AV trajectory. We report the average minADE (following the definition from [10]) over 3,5 and 8 seconds on both the original and perturbed datasets. In all instances, we use the top 6 trajectories for each model $(\mathrm{K}=6)$.</p>
<p>Robustness Metrics. Since we found in our results that the perturbed minADE often improves for a large fraction of the examples, averaging over examples cancels out some of the effects we would like to measure. Thus, we introduce a metric to measure the per-example absolute change in minADE:</p>
<p>$$
\operatorname{Abs}(\Delta)=\frac{1}{n} \sum_{i=1}^{n}|\text { perturbed_minADE }(i)-\text { original_minADE }(i)|
$$</p>
<p>We report $\operatorname{Abs}(\Delta)$, the standard deviation of $\operatorname{Abs}(\Delta)$, and the the relative percentage change in $\operatorname{Abs}(\Delta)$ with respect to the original minADE. Finally, since the ground truth may represent only one of several correct ways to drive, in Section 4.2 we also consider pairwise differences between the original and perturbed predictions to measure model sensitivity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Coord. System</th>
<th style="text-align: center;"># Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MultiPath++</td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">agent-centric</td>
<td style="text-align: center;">125 M</td>
</tr>
<tr>
<td style="text-align: center;">SceneTransformer</td>
<td style="text-align: center;">factorized attention transformer</td>
<td style="text-align: center;">global</td>
<td style="text-align: center;">15 M</td>
</tr>
<tr>
<td style="text-align: center;">Wayformer</td>
<td style="text-align: center;">early fusion attention transformer</td>
<td style="text-align: center;">agent-centric</td>
<td style="text-align: center;">42 M</td>
</tr>
</tbody>
</table>
<p>Table 1: We evaluate on a diverse set of models.
Models. We select three representative deep learning models for evaluation: MultiPath++ [38], Scene Transformer [25], and Wayformer [24]. Importantly, we only consider non-ensembled models (Multipath++ reports ensemble results in their paper and on the WOMD leaderboard). Table 1 reviews the architectural differences and parameter counts of the models. Since we only evaluate on the AV, we typically only train the models to predict the AV, but for MultiPath++ and SceneTransformer we also train models on all agents (which we indicate by appending -All to the model name). Additionally, for SceneTransformer-All, we include both the marginal and joint models (these models are the same when training on only the AV.)</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Model sensitivity to different perturbation types. We plot the per-example perturbed versus original minADE for all perturbations for the MP++ model. The example frequency is shown with a log color scale where yellow is the most frequent. The majority of examples show minimal change from the perturbation and lie close to the $y=x$ axis. However, across all perturbation types, there is a long tail of examples that show relatively large change in $\min A D E(&gt;1 m)$. Surprisingly, even for the RemoveCausal perturbation, the model performance often improves on the perturbed examples. Comparing RemoveNoncausal and RemoveNoncausalEqual indicates that the model is more sensitive to removing larger numbers of non-causal agents).</p>
<h1>4 Results</h1>
<h3>4.1 Model sensitivity to non-causal perturbations</h3>
<p>In order to understand model sensitivity on a per-example level, Figure 4 plots the perturbed versus original minADE across each perturbation for MultiPath++ (see Appendix H for other models). For each perturbation type, we observe that the majority of examples show minimal change (i.e. are clustered around the $y=x$ axis), but a long tail of outlier examples experience a large change $(&gt;1 m)$. Among perturbation types, the model is most sensitive to RemoveCausal, which is expected since removing causal agents can change the correct ground-truth trajectory. Interestingly, models are significantly more robust to RemoveNoncausalEqual than RemoveNoncausal, which means removing more agents increases model sensitivity. When comparing RemoveCausal and RemoveNoncausalEqual, which controls for the number of agents removed, we see that the model is significantly more sensitive to removing causal agents than removing non-causal agents.</p>
<p>Surprisingly, across all perturbation types, including RemoveCausal, the model sees a large portion of examples where minADE improves: $42.7 \%$ of examples show an improvement under the RemoveCausal perturbation, $43.0 \%$ for RemoveNoncausal, $49.6 \%$ for RemoveNoncausalEqual and $51.0 \%$ for RemoveStatic. This finding is counter-intuitive and motivated us to measure model sensitivity in terms of $\operatorname{Abs}(\Delta)$, defined in Equation 1. Across all models, the average $\operatorname{Abs}(\Delta)$ is 0.1450 for RemoveCausal, 0.131 for RemoveNoncausal, 0.051 for RemoveNoncausalEqual, and 0.089 for RemoveStatic. Appendix C reports $\operatorname{Abs}(\Delta)$ for each individual model and perturbation type.</p>
<p>Comparing models. Focusing on the RemoveNoncausal perturbation, in Table 2, we evaluate each model architectures and report the original minADE, perturbed minADE, $\operatorname{Abs}(\Delta)$, the standard deviation of $\operatorname{Abs}(\Delta)$, and $\frac{\operatorname{Abs}(\Delta)}{\min A D E_{\text {Ori }}}$ (see Appendix C for other perturbations.) The SceneTransformer Marginal model shows the lowest average absolute sensitivity to the perturbation, while the MultiPath++-All model shows the lowest sensitivity relative to original minADE. In general, $\operatorname{Abs}(\Delta)$ decreases with the original minADE, but there is no clear relationship between relative $\operatorname{Abs}(\Delta)$ and minADE. Unexpectedly, the marginal SceneTransformer is more robust than the joint (we hypothesize that jointly modeling agents in the scene causes the model to pay more attention to non-causal agents and thus lead to relatively bad robustness). In Appendix G, we report aggregate results for minFDE, overlap rate, miss rate, and mAP.</p>
<p>Slicing the robustness metric. We further slice the robustness of the models $(\operatorname{Abs}(\Delta))$ along several dimensions: AV's current speed, the percentage of removed non-causal agents (the number of removed non-causal agents divided by the number of all context agents), and the minimum distance from the AV to removed non-causal agents. The full results are given in Figure 12 in Appendix E. We see that, across all</p>
<p>Model comparison, RemoveNoncausal, minADE, $\Delta=$ Perturbed - Original</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Perturbed</th>
<th style="text-align: center;">$\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\min \operatorname{ADE}_{\text {Obs }}}$ (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MultiPath++</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$37.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SceneTransformer Marginal</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$26.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wayformer</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$25.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MultiPath++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$25.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SceneTransformer-All Joint</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$34.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SceneTransformer-All Marginal</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$26.6 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Model sensitivity to the RemoveNoncausal perturbation. The SceneTransformer Marginal model shows the lowest average absolute sensitivity to the perturbation, while the MultiPath++-All model shows the lowest sensitivity relative to original minADE. Original and Perturbed are the average minADE across the whole dataset. $\operatorname{Abs}(\Delta)$ is the average per-example absolute difference between perturbed and original minADE.
models, model sensitivity increases when we drop a larger fraction of non-causal agents and when the speed of the AV is greater. We also see that model sensitivity typically decreases when we drop agents that are farther away from the AV, though the SceneTransformer models have much noisy robustness measurements when dropping far away agents.
Visualizing examples. We also visualize some examples with the largest output changes under the RemoveNoncausal perturbation in Appendix E. The findings are discussed in Section 5.</p>
<h1>4.2 Sensitivity via an IoU-based trajectory set metric</h1>
<p>To directly measure the magnitude of model output changes with and without perturbation, in this section we introduce a simple IoU (intersection-over-union) based metric to compare the sensitivity across models to different perturbations.</p>
<p>The IoU-based metric. The IoU-based trajectory metric is computed as follows: given two predicted trajectory sets (with and without perturabtion), we first upsample all predicted trajectories ( 6 of them in each set) to 100 Hz , and then voxelize them into a 2 D top down grid with resolution of 0.5 meters. We then count the number of voxels both sets occupy, divided by the total number of voxels either output set occupies. To simplify computation, we explicitly ignore the probabilities and speeds of trajectories. This measure quantifies "how geometrically different the trajectories look". An IoU of 1 means the trajectories did not meaningfully change, and an IoU of 0 means the trajectories do not overlap at all. While more complicated versions of this metric could be computed (e.g. earth movers distance), we found this metric intuitive and useful for finding interesting shifts due to perturbation.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Density distribution of the per-scene trajectory set IoU values for AV-only models under perturbations (RemoveCausal, RemoveNoncausal, and RemoveNoncausalEqual): models are least sensitive to RemoveNoncausalEqual, and more sensitive to RemoveCausal and RemoveNoncausal.
Results. The results of three AV-only models under perturbations RemoveCausal, RemoveNoncausal and RemoveNoncausalEqual are shown in Figure 5. We find that models are least sensitive to RemoveNoncausalE-</p>
<p>qual, and much more sensitive to RemoveCausal and RemoveNoncausal. This is consistent with our finding in Section 4.1, indicating the model is more sensitive to large perturbations since there are more non-causal agents than causal ones in most examples.</p>
<h1>4.3 Training with data augmentations improves model robustness</h1>
<p>We experiment with two types of data augmentation: 1) data augmentations that use a heuristic definition of non-causal agents, such as randomly dropping any static context agent ${ }^{2}$, and 2) robustness-targeted data augmentations that directly drop only non-causal agents using a labeled portion of the val set.</p>
<p>Heuristic data augmentation. The benefit of using a heuristic definition of non-causal agents for data augmentations is that it can be applied without collecting causal labels. We implement 2 types of heuristicbased data augmentation in the training set of WOMD: Drop Context (randomly dropping context agents) as a baseline, and Drop Static Context (randomly dropping static context agents). We use the MultiPath++-All model and we set the probability of dropping an agent to 0.1 (the best one among $0.1,0.5$, and 0.8 ). Table 3 summarizes the results for the RemoveNoncausal perturbation (for the per-scene distribution of model sensitivity, see Appendix K). Models with data augmentation show less sensitivity to the perturbations, and, in particular, Drop Static Context shows a significant improvement in minADE and $\operatorname{Abs}(\Delta)$ over Drop Context. We hypothesize that Drop Static Context does better because the static context agents are less likely to be causal. Overall, the results for Drop Static Context imply that dropping non-causal agents via data augmentation in training can improve model robustness to such perturbations at test time.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Heuristic Data Augs, RemoveNoncausal, minADE, $\Delta=$ Perturbed - Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Perturbed</td>
<td style="text-align: center;">$\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\min \operatorname{ADE}_{\text {Obs }}}(\%)$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$25.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All Drop Context</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">$\pm 0.31$</td>
<td style="text-align: center;">$22.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All Drop Static Context</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$22.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Heuristic data augmentations. We compare the MP++-All baseline model to the same model trained with either dropping context agents or dropping static context agents, finding that data augmentations that drop agents that are more likely to be non-causal can improve robustness.
Non-causal data augmentations. Motivated by our results that dropping static context agents improves model robustness, we further explore using non-causal perturbations as a data augmentation strategy during training. We randomly sample approximately $70 \%$ of the original validation dataset (i.e. 30 k scenes), perturb multiple copies of them via the causal labels, and add the perturbed versions into the training dataset. We leave the remaining $30 \%$ of the validation set as a holdout for evaluation. We then train a baseline model on the new training dataset as well as a model that randomly drops non-causal agents (when possible) with probability 0.1 . In Table 4, we see that similarly dropping non-causal agents helps improve minADE as well as model robustness.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Noncausal Augs, RemoveNoncausal, minADE, $\Delta=$ Perturbed - Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Perturbed</td>
<td style="text-align: center;">$\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\min \operatorname{ADE}_{\text {Obs }}}(\%)$</td>
</tr>
<tr>
<td style="text-align: left;">MP++ Baseline</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">$\pm 0.226$</td>
<td style="text-align: center;">$38.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++ Drop Non-causal</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">$\pm 0.194$</td>
<td style="text-align: center;">$37.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Noncausal data augmentation. We fold a portion of the WOMD validation dataset into the original training dataset and apply data augmentations that drop non-causal agents. On held-out validation data, we find significant improvements in model robustness across all three $\operatorname{Abs}(\Delta)$ metrics.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.4 Larger dataset size improves model robustness</h1>
<p>We also evaluate model robustness with increasing training data. We randomly select $10 \%, 20 \%, 50 \%, 80 \%$ of the training dataset and train separate models on each split. We sample three datasets for each split and average the performance and robustness of each model. Appendix D summarizes the results. As we increase the training data size, the model performance improves (minADE decreases) and both the absolute and relative robustness improves. Interestingly, previously when varying model architectures, we found that the model with the lowest minADE did not always have the best relative robustness. Here, we see a strong trend: for a fixed model architecture, lowering the minADE by increasing the training data results in lower relative sensitivity.</p>
<h2>5 Discussion</h2>
<p>We now discuss a few hypotheses and initial supporting evidence for why models are not robust to the non-causal perturbations. There may be other reasons we have yet to find strong evidence for, but we hope future work can utilize the labels to explore these ideas further.</p>
<p>Overfitting. One reason models may fail to generalize to the non-causal perturbations is that they overfit to spurious correlations in the training data (i.e. features that correlate with certain ground truth trajectories but fail to generalize). In our experiments, we observe that models that overfit on the original training dataset (as measured by increasing minADE on the original validation dataset) are more sensitive to the non-causal perturbations (see Fig. 11 in Appendix E). Thus, the more the model overfits to spurious features like the number of parked cars in a faraway parking lot, the less well it generalizes to examples where these features are absent. Data augmentation and increasing dataset size may improve robustness by protecting against overfitting.</p>
<p>Distribution shift. Models may fail to generalize to perturbations that are significantly different from any data seen during training. In our results, we observe that the more non-causal agents we remove, the less robust models are. Perhaps certain types of scenes with few agents are relatively rare in the training dataset and the model does not generalize well to the distribution shift. By evaluating on the perturbations, we essentially expose the model to rare scenarios not seen in training. One reason that training with data augmentations via dropping (static) context agents or non-causal agents improves robustness could be that it exposes the model to similar scenes during training.</p>
<p>Over-reliance on agents instead of roadmap. A third possible reason that models fail to generalize is that they utilize the non-causal agents to infer the drivable areas instead of using the mapping information in the input (we serve high-definition maps and traffic control signals as input features for all models). Our evidence comes from visualizing examples where dropping non-causal agents creates predictions that disobey the roadgraph rules (see Fig. 10 in Appendix E).</p>
<p>Data-dependent modes. Finally, many of the state of the art models (e.g. [25, 38]) utilize modes (e.g. straight, left, u-turn, etc.) learned from the data distribution, where the input data influences how the model will utilize its K predictions to minimize its loss function. While effective at minimizing minADE-like metrics, these methods provide no coverage guarantees, and can encourage the model to predict multiple speed profiles for the same mode instead of diverse modes. When we triage examples (see Fig. 9 in Appendix E), we find some of the largest failures come from agent deletions that influence which modes the model predicts, demonstrating a weakness of this approach and the metrics they perform well on.</p>
<h2>6 Conclusions</h2>
<p>We establish a benchmark and metrics for evaluating the robustness of several state-of-the-art models for trajectory prediction for autonomous driving. We find that most state-of-the-art models (with different model architectures and coordination systems) show significant levels of sensitivity to perturbations that remove</p>
<p>non-causal agents, with higher sensitivity when removing a greater number of them. While most examples show minimal change in minADE ( $\leq 0.1 \mathrm{~m}$ ), there is a long tail of examples that can have large changes ( $\geq 1 \mathrm{~m}$ and sometimes up to 8 m ). Surprisingly, removing either causal or non-causal agents can cause a significant fraction of examples to improve their minADE. We also find that increasing dataset size and data augmentation can help improve the model robustness. Overall, our results indicate that current machine learning models for trajectory prediction may not be reliable enough on their own, and careful thought needs to be given to how to integrate such models with non-learning components to make a safe system. Finally, we will publish the causal agent labels as complementary attributes to the WOMD to aid future researchers in building more robust models.</p>
<h1>7 Acknowledgement</h1>
<p>We thank Rami Al-Rfou, Zhifeng Chen, Anca Dragan, Carlton Downey, Aleksandra Faust, Nigamaa Nayakanti, Jiquan Ngiam, Jon Shlens, Mukund Sundararajan, and Vijay Vasudevan for the valuable discussions and inputs during the course of this research.</p>
<h1>References</h1>
<p>[1] Ekrem Aksoy, Ahmet Yazıcı, and Mahmut Kasap. See, attend and brake: An attention-based saliency map prediction model for end-to-end driving. arXiv preprint arXiv:2002.11020, 2020.
[2] Aniket Bera, Sujeong Kim, Tanmay Randhavane, Srihari Pratapa, and Dinesh Manocha. Glmp-realtime pedestrian path prediction using global and local movement patterns. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 5528-5535. IEEE, 2016.
[3] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 2018.
[4] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pages 387-402. Springer, 2013.
[5] Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. Spagnn: Spatially-aware graph neural networks for relational behavior forecasting from sensor data. In 2020 IEEE International Conference on Robotics and Automation, ICRA 2020, Paris, France, May 31 - August 31, 2020, pages 9491-9497. IEEE, 2020.
[6] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In Conference on Robot Learning, 2019.
[7] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous driving using deep convolutional networks. In 2019 International Conference on Robotics and Automation (ICRA), pages 2090-2096. IEEE, 2019.
[8] Kaustubh D Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, et al. Nl-augmenter: A framework for task-sensitive natural language augmentation. arXiv preprint arXiv:2112.02721, 2021.
[9] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Exploring the landscape of spatial robustness. In International Conference on Machine Learning, pages 1802-1811. PMLR, 2019.
[10] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9710-9719, 2021.
[11] Alhussein Fawzi and Pascal Frossard. Manitest: Are classifiers really invariant? arXiv preprint arXiv:1507.06535, 2015.
[12] Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018.
[13] Keren Gu, Brandon Yang, Jiquan Ngiam, Quoc Le, and Jonathon Shlens. Using videos to evaluate image model robustness. arXiv preprint arXiv:1904.10076, 2019.
[14] Yutao Han, Rina Tse, and Mark Campbell. Pedestrian motion model using non-parametric trajectory clustering and discrete transition points. IEEE Robotics and Automation Letters, 4(3):2614-2621, 2019.
[15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations (ICLR), 2019.</p>
<p>[16] Joey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions. In CVPR, 2019.
[17] Manh Huynh and Gita Alaghband. Trajectory prediction by coupling scene-lstm with human movement lstm. In International Symposium on Visual Computing, pages 244-259. Springer, 2019.
[18] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if motion prediction for autonomous driving. arXiv preprint arXiv:2008.10587, 2020.
[19] Julian FP Kooij, Fabian Flohr, Ewoud AI Pool, and Dariu M Gavrila. Context-based path prediction for targets with switching dynamics. International Journal of Computer Vision, 127(3):239-262, 2019.
[20] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 336-345, 2017.
[21] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning lane graph representations for motion forecasting. In European Conference on Computer Vision (ECCV), 2020 .
[22] Osama Makansi, Özgün Cicek, Yassine Marrakchi, and Thomas Brox. On exposing the challenging long tail in future prediction of traffic actors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13147-13157, 2021.
[23] Jean Mercat, Thomas Gilles, Nicole Zoghby, Guillaume Sandou, Dominique Beauvois, and Guillermo Gil. Multi-head attention for joint multi-modal vehicle motion forecasting. In IEEE International Conference on Robotics and Automation, 2020.
[24] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple \&amp; efficient attention networks. arXiv preprint arXiv:2207.05844, 2022 .
[25] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al. Scene transformer: A unified multi-task model for behavior prediction and planning. arXiv e-prints, pages arXiv-2106, 2021.
[26] Noha Radwan, Wolfram Burgard, and Abhinav Valada. Multimodal interaction-aware motion prediction for autonomous street crossing. The International Journal of Robotics Research, 39(13):1567-1598, 2020.
[27] Vasili Ramanishka, Yi-Ting Chen, Teruhisa Misu, and Kate Saenko. Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7699-7707, 2018.
[28] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389-5400, 2019.
[29] Khaled S Refaat, Kai Ding, Natalia Ponomareva, and Stéphane Ross. Agent prioritization for autonomous navigation. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2060-2067. IEEE, 2019.
[30] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals in visual multi-agent settings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2821-2830, 2019.
[31] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamicallyfeasible trajectory forecasting with heterogeneous data. arXiv preprint arXiv:2001.03093, 2020.</p>
<p>[32] Christoph Schöller, Vincent Aravantinos, Florian Lay, and Alois Knoll. The simpler the better: Constant velocity for pedestrian motion prediction. arXiv preprint arXiv:1903.07933, 5(6):7, 2019.
[33] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? arXiv preprint arXiv:1906.02168, 2019.
[34] Liting Sun, Xiaogang Jia, and Anca D Dragan. On complementing end-to-end human behavior predictors with planning. In Proceedings of the Robotics: Science and Systems, 2021.
[35] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2013.
[36] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583-18599, 2020.
[37] Ekaterina V. Tolstaya, Reza Mahjourian, Carlton Downey, Balakrishnan Varadarajan, Benjamin Sapp, and Drago Anguelov. Identifying driver interactions via conditional behavior prediction. 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 3473-3479, 2021.
[38] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction. arXiv preprint arXiv:2111.14973, 2021.
[39] Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu. Multi-agent tensor fusion for contextual trajectory prediction. In CVPR, pages $12126-12134,2019$.</p>
<p>**For review purposes, we provide a copy of the causal labels annotations in the supplementary materials. The causal agent labels are released as a TFRecord of causal labels protos (see the file 'causal_label.proto' in the supplementary material). The proto maps scenario id to labeler id to a list of agent ids identified by that labeler.</p>
<h1>A Labeling Policy</h1>
<p>Below is the exact text given to labelers to define causal agents:</p>
<p>The objective is to identify all agents - cars, cyclists, or pedestrians - that are causal to the AV at any time. A causal agent is one whose presence would modify or influence human driver behavior in any way.
Causality is an inherently subjective label. If you are unsure if an agent is causal or not, please err on the side of including it. In other words, false positives (identifying an agent as causal when it is truly non-causal) are okay, but we should avoid false negatives (failing to identify a truly causal agent).
If the behavior of a human driver would be modified because of a potential action that an agent is likely to take, then that agent should be causal. On the other hand, if the human driver would drive the same regardless of whether the agent is there or not, the agent is non-causal.</p>
<p>The labeling policy also included several examples scenarios with causal agents identified such as Figure 6.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Example from the labeling policy. Causal agents are circled in green and a subset of non-causal agents are circled in red.</p>
<h1>B Labeler Agreement Statistics</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Labeler agreement statistics. We plot the distribution of labeler agreement (i.e. number of labelers who selected a given agent) across all agents in the WOMD validation set. The majority of agents are selected by more than one labeler.</p>
<h2>C Model sensitivity to various perturbation types</h2>
<p>In this section, we summarize the robustness metrics across different model architectures for each of the perturbation types (RemoveCausal, RemoveNoncausal, RemoveNoncausalEqual, RemoveStatic). We report the model's original minADE, perturbed minADE, average absolute difference between perturbed and original minADE computed per-example $(\operatorname{Abs}(\Delta))$, standard deviation of $\operatorname{Abs}(\Delta)$, and the relative $\%$ change ( $\operatorname{Abs}(\Delta)$ divided by the original minADE). Each table below shows the results for a different perturbation dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">RemoveCausal minADE, $\Delta=$ Perturbed - Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Perturbed</td>
<td style="text-align: center;">$\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\operatorname{Ort}}(\%)$</td>
</tr>
<tr>
<td style="text-align: left;">MP++</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$40.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST Marginal</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$27.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wayformer</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">$\pm 0.20$</td>
<td style="text-align: center;">$31.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">$\pm 0.34$</td>
<td style="text-align: center;">$25.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Marginal</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$29.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Joint</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">$\pm 0.31$</td>
<td style="text-align: center;">$42.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Model sensitivity for RemoveCausal, minADE.
RemoveNoncausal minADE, $\Delta=$ Perturbed - Original</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Perturbed</th>
<th style="text-align: center;">$\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\operatorname{Ort}}(\%)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MP++</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$37.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST Marginal</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$26.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wayformer</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$25.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$25.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Marginal</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$26.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Joint</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$34.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Model sensitivity for RemoveNoncausal, minADE.</p>
<p>RemoveNoncausalEqual minADE, $\Delta=$ Perturbed - Original</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Perturbed</th>
<th style="text-align: center;">$\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\mathrm{Oit}}(\%)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MP++</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$16.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST Marginal</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$9.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wayformer</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$10.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">$\pm 0.20$</td>
<td style="text-align: center;">$11.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Marginal</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$8.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Joint</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$10.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Model sensitivity for RemoveNoncausalEqual, minADE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">RemoveStatic minADE, $\Delta=$ Perturbed - Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Perturbed</td>
<td style="text-align: center;">$\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">Std. $\operatorname{Abs}(\Delta)$</td>
<td style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\mathrm{Oit}}(\%)$</td>
</tr>
<tr>
<td style="text-align: center;">MP++</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.094</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$25.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ST Marginal</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$17.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Wayformer</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$13.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MP++-All</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">$17.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ST-All Marginal</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">$20.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ST-All Joint</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">$24.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Model sensitivity for RemoveStatic, minADE.</p>
<p>To make it easier to compare across perturbation types, we also report the average $\mathrm{Abs}($ Perturbed - Original) minADE for each model and perturbation type in Table 9.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Abs(Perturbed - Original) minADE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">R.Causal</td>
<td style="text-align: center;">R.Noncausal</td>
<td style="text-align: center;">R.NoncausalEqual</td>
<td style="text-align: center;">R.Static</td>
</tr>
<tr>
<td style="text-align: left;">MP++</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.094</td>
</tr>
<tr>
<td style="text-align: left;">ST Marginal</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.043</td>
</tr>
<tr>
<td style="text-align: left;">Wayformer</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.054</td>
</tr>
<tr>
<td style="text-align: left;">MP++-All</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.161</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Marginal</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.063</td>
</tr>
<tr>
<td style="text-align: left;">ST-All Joint</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.118</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 3 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8 9}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Abs(Perturbed-Original) across different perturbation types and models. We report the average absolute difference between the per-example perturbed and original minADE for each model and perturbation type. The model sensitivity for RemoveCausal and RemoveNoncausal is similar, with RemoveCausal resulting in a slightly larger average absolute change. However, when we control for the number of agents and compare RemoveCausal to RemoveNoncausalEqual, we see that the model is significantly more sensitive to removing causal agents than non-causal agents.</p>
<h1>D Increasing dataset size</h1>
<p>Train \%, minADE, RemoveNoncausal, $\Delta=$ Ptb - Ori</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Train(\%)</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Perturbed</th>
<th style="text-align: center;">$\operatorname{Abs}(\Delta)$</th>
<th style="text-align: center;">Std. Abs( $\Delta$ )</th>
<th style="text-align: center;">$\frac{\operatorname{Abs}(\Delta)}{\text { Ori }}(\%)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">1.222</td>
<td style="text-align: center;">1.309</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">$\pm 0.69$</td>
<td style="text-align: center;">$37.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">1.039</td>
<td style="text-align: center;">1.117</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">$\pm 0.53$</td>
<td style="text-align: center;">$37.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">$\pm 0.45$</td>
<td style="text-align: center;">$28.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$26.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$25.1 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: Increasing training data improves robustness.</p>
<h1>E Example visualization</h1>
<h2>E. 1 Failure (non-robust) cases under non-causal perturbation</h2>
<p>We have triaged several top sensitive examples under the RemoveNoncausal perturbation. Among these examples, we have found three failure patterns: 1) predictions under the perturbation violate traffic rules, as shown in Figure 8; 2) predictions under the perturbation missed to capture the ground-truth mode, as shown in Figure 9; and 3) predictions under the perturbation violates the causality, for instance, unnecessary slows down when the road becomes more empty due to the removal of non-causal agents, such as the bottom example in Figure 11. Meanwhile, we also have identified examples where the predictions under the non-causal perturbation becomes better, as shown in Figure 10.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: An sensitive example from MP++ under non-causal perturbation: Left side is inference on the original validation data, and right side is inference on the RemoveNoncausal data, where all non-causal agents are removed from the scene, but some of predicted outputs weirdly turn right in the middle of a straight road.</p>
<h2>E. 2 An evidence example for non-robustness due to overfitting</h2>
<p>In this section, we show an example scenario that showcases overfitting is one potential reason for poor robustness. We have trained the MP++ with 1M iterations, which overfitted at 210 k iterations. We then visualize the predictions of a same example with two different checkpoints, one at 210 k iteration and another at 1 M . The results are shown in Figure 11. We can see that the robustness of the model under non-causal perturbation becomes bad when the model over-fits.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Context agents are agents for which no prediction is required in the WOMD leaderboard.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
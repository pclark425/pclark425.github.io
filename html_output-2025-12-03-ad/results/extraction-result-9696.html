<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9696 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9696</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9696</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-268351639</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.06749v1.pdf" target="_blank">Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies</a></p>
                <p><strong>Paper Abstract:</strong> . Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9696.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9696.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic / Metric-based Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated assessment methods proposed for tasks amenable to formal verification (e.g., text-to-SQL), using formal correctness, syntactic/semantic match to ground-truth, and conciseness metrics (e.g., query length).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (evaluation of generated analyses and hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated metric comparison of generated artefacts (e.g., SQL queries, declarative constraints) against ground-truth or executable verification, measuring formal correctness and conciseness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Formal accuracy (syntactic and semantic correctness), conciseness (e.g., query length), executability, and correctness of produced constraints/queries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No single PM-specific dataset presented; proposed to use existing text-to-SQL benchmarks (e.g., SPIDER) and domain-specific event logs for automated checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper recommends automatic evaluation as particularly suited to text-to-SQL and formally-checkable outputs; no quantitative results reported in this survey paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Many PM tasks are not fully formalizable; ground-truth for higher-level insights/hypotheses is often unavailable, limiting automated evaluation applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to human evaluation, automatic evaluation is scalable and objective for formal tasks but insufficient for subjective/higher-level reasoning or hypothesis novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use automated checks where possible (e.g., execute generated SQL against databases), measure formal correctness and conciseness, and complement with human review for broader tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9696.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Expert Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual assessment by domain experts for tasks such as anomaly detection and hypothesis generation, focusing on recall and precision of insights and the effectiveness of iterative validation cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (human assessment of model-generated analyses and hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human experts review model outputs (e.g., detected anomalies, suggested root causes and hypotheses) and score or annotate outputs for correctness, relevance, recall and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Recall (ability to identify expected/known insights), precision (correctness of findings), usefulness of explanations, and effectiveness of feedback cycles in validating/refining hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Human-labeled event logs, annotated anomalies and curated PM scenarios; no single PM-specific benchmark established in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper stresses human evaluation as essential for direct querying and hypothesis generation; no quantitative human-study results presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human evaluation is costly, subjective, and can be inconsistent across annotators; requires domain expertise and clear evaluation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluation is the traditional gold standard for process analysis; LLM evaluation should include human judgment especially for high-level reasoning and novelty assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use human evaluation to assess recall/precision on curated scenarios, combine with automated verification where possible, and define clear annotation guidelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9696.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-eval techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation and Ensemble Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of LLM-internal or multi-model strategies to detect and reduce hallucinations including chain-of-thought, confidence scoring, ensembling (majority vote), and self-reflection where a model reviews its own or peers' outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (robustness and trustworthiness of generated insights)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Model-internal self-checks (confidence scores, chain-of-thought) and cross-session ensembling to corroborate outputs; self-reflection prompts for error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration of confidence (correspondence between reported confidence and correctness), consistency across ensemble runs, and ability of chain-of-thought/self-reflection to surface reasoning steps and errors.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No dedicated PM dataset; techniques evaluated conceptually and supported by literature (e.g., self-evaluation, ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper recommends these techniques to mitigate hallucinations—confidence thresholds to discard low-confidence outputs and ensembling to confirm anomalies—no empirical metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Confidence scores can be miscalibrated (confidence-competence gap); ensembling and multiple runs increase computational cost; self-reflection can propagate initial errors without external grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Self-eval/ensembling can approach human-like error checking but should be used alongside human review and external data checks for critical decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Apply confidence thresholds, ensemble multiple runs, prompt for chain-of-thought where transparency helps, and use self-reflection policies to flag questionable outputs for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9696.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve transparency and reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / Process Mining reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Encourage the model to produce step-wise reasoning (chain-of-thought) and evaluate the resulting explanations for coherence and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality and correctness of intermediate steps, improvement in final-answer accuracy, and usefulness of explanations for human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Referenced as a general technique; not tied to a PM dataset in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper cites chain-of-thought as a technique to enhance explanations and reduce hallucinations; no PM-specific empirical results in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Chain-of-thought increases verbosity and may expose incorrect intermediate reasoning; it does not guarantee factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides human-readable reasoning akin to expert explanations, aiding human evaluation and trustworthiness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use chain-of-thought for tasks where reasoning transparency aids validation, but combine with grounding checks (data or executable verification).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9696.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confidence Scores / Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLM-reported confidence or calibrated probability measures to judge which outputs to trust or discard, mitigating hallucination in anomaly detection and hypothesis proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (anomaly detection, hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Obtain model confidence for each generated insight and apply thresholds to filter low-confidence items; analyze calibration between confidence and empirical correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration (how well confidence predicts correctness), precision/recall tradeoffs when filtering by confidence, and downstream impact of removing low-confidence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No explicit PM dataset provided for calibration tests in this paper; conceptually applied to event-log analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper recommends using confidence scores to exclude doubtful anomalies; no calibration experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM confidence is often miscalibrated (confidence-competence gap); requires empirical calibration and may discard correct but low-confidence novel findings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human experts naturally provide calibrated judgments; model confidence can supplement human review but should not replace it.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Calibrate model confidence empirically on held-out PM scenarios and use conservative thresholds combined with human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9696.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembling / Multi-session Consensus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregating multiple model outputs (different runs or models) and using majority voting or consensus mechanisms to increase reliability and reduce single-run hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (robust detection and hypothesis corroboration)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run multiple independent LLM sessions or models on the same prompt and aggregate results via majority voting or confidence-weighted fusion to identify stable findings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Inter-run agreement, improved precision at fixed recall, and robustness of identified anomalies/hypotheses across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Conceptual; no PM-specific ensemble benchmark provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper suggests ensembling as a way to increase reliability (confirm detections across sessions); no empirical ensemble results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Increased computational cost and potential correlated errors across models; majority voting can suppress minority but correct novel hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Analogous to consulting multiple experts; increases confidence but still benefits from adjudicating disagreements with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use ensembling to corroborate anomalies/hypotheses and combine with confidence scoring and human adjudication for final decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9696.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGIEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose benchmark that assesses LLMs via standardized exam-like prompts and human-centered evaluation, focusing on broad capabilities including domain knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (relevant to domain knowledge and reasoning required in PM)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmark LLMs on exam-like, standardized prompts, often assessed with human scoring or automated grading where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on exam items, reasoning quality, and instructional/dialog capabilities depending on test design.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>AGIEval: collection of standardized exam-style questions across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper lists AGIEval as a relevant traditional benchmark for choosing LLMs for PM tasks; no AGIEval results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exam-style benchmarks may not measure PM-specific capabilities (visual comprehension, long-context processing, text-to-SQL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>AGIEval approximates human exam performance; useful as a general indicator but insufficient alone for PM suitability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use AGIEval as part of a battery of tests but supplement with PM-specific and multimodal benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9696.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XIEZHI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-knowledge benchmark assessing LLM knowledge across multiple specialized fields (economics, science, engineering), useful for evaluating domain expertise needed in PM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General domain knowledge; relevant for Process Mining where domain context matters</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Test models on domain-specific knowledge questions; evaluate factual accuracy and depth of domain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coverage and accuracy across domains, recall of factual information, and reasoning over domain concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>XIEZHI: cross-domain knowledge benchmark (ever-updating collection).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper references XIEZHI as relevant for assessing domain knowledge for PM tasks; no evaluation numbers given here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Domain benchmarks may not capture PM-specific modalities (visual logs, SQL generation) and are not tailored to PM workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Domain benchmarks approximate specialist knowledge testing; human experts remain necessary for nuanced process understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include domain-knowledge benchmarks like XIEZHI in the evaluation suite and augment with PM-specific tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9696.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMBench / MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMBench & MM-Vet: Multimodal Visual Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks designed to test multimodal LLM capabilities on image understanding (MMBench) and integrated multimodal tasks including OCR and recognition (MM-Vet); relevant for interpreting PM visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MMBench: Is Your Multi-modal Model an Allaround Player?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multimodal evaluation (visual understanding relevant to Process Mining visualizations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Assess model performance on image recognition, OCR, and other multimodal tasks; evaluate answers to visual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on visual understanding tasks, OCR quality, and ability to interpret plot-specific features (line orientation, point color/size).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MMBench and MM-Vet: suites of multimodal tasks and datasets for image+text evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper notes these benchmarks as relevant but insufficient for PM-specific visuals (e.g., dotted charts, performance spectrum) since they may not test fine-grained plot feature interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Existing multimodal benchmarks may not evaluate fine-grained visualization interpretation needed in PM (line slopes, point aggregation patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human analysts better interpret complex PM visualizations; multimodal benchmarks are a step toward automation but require PM-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multimodal benchmarks to screen for visual capabilities, and develop PM-specific visualization tests to complement them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9696.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-to-SQL benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPIDER & APPS (Text-to-SQL and Code Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks used to evaluate LLM abilities to translate natural language to SQL (SPIDER) and broader code-generation competence (APPS), both relevant for PM tasks requiring query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring Coding Challenge Competence With APPS.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Text-to-SQL and program generation for Process Mining data extraction and analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure syntactic and semantic correctness of generated SQL/code, executability, and ability to express complex joins and temporal calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact-match, execution accuracy, generalization to unseen schemas, and conciseness of produced queries or code.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SPIDER (text-to-SQL dataset for complex and cross-domain queries) and APPS (coding challenge benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper suggests using text-to-SQL benchmarks to assess LLM ability to generate executable queries for PM workflows; no performance numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SPIDER and APPS evaluate general code/sql skills but not PM-specific schema translations (event-log schemas) or PM semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated text-to-SQL evaluation can approximate correctness of queries that a human would handcraft, but humans are needed to validate domain intent.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use SPIDER/APPS to evaluate baseline code generation, and create PM-specific text-to-SQL tests that reflect common event-log schemas and PM operators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9696.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecodingTrust</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark covering trustworthiness aspects including toxicity, bias, robustness, privacy, ethics and fairness, cited as relevant for assessing fairness in PM applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Trustworthiness and fairness evaluation applicable to Process Mining use-cases (bias detection in processes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Assess models across multiple trust dimensions (bias, toxicity, privacy, robustness) using curated tests and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Metrics for bias detection, fairness across groups, robustness under distributional shifts, and privacy leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DecodingTrust: suite of tests for trustworthiness attributes in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper lists DecodingTrust as a fairness/trust benchmark relevant to PM (e.g., detecting bias in recruitment processes); no empirical application in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General trustworthiness benchmarks may not capture PM-specific fairness subtleties (process-level disparities) without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides automated diagnostics complementing human audits for fairness, but human oversight remains necessary for remediation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include trustworthiness benchmarks like DecodingTrust in evaluation pipelines and adapt fairness tests to PM scenarios (attribute-level analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9696.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9696.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation of Hypotheses Generation (needs PM-specific benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper highlights the absence of PM-specific benchmarks for autonomous hypothesis generation and points to related literature on scientific hypothesis discovery as partial guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Process Mining (automated scientific/hypothesis discovery over event data)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Combination of human evaluation (precision/recall of generated hypotheses), automated verification via executable SQL checks against event data, and iterative feedback cycles to validate/refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty, correctness (verifiable against data), usefulness for process improvement, and ability to suggest verifiable SQL for checks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No established PM-specific hypothesis-generation benchmark; related work in scientific hypothesis discovery suggested as starting points (e.g., 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery').</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper asserts a gap: while LLMs can generate hypotheses, there is a lack of PM-specific benchmarks to systematically evaluate them; no quantitative results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lack of ground-truth hypotheses, difficulty in measuring novelty and usefulness objectively, and risk of hallucinated causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Humans traditionally generate and test hypotheses via experiments and data queries; LLMs can propose candidates but need formal verification and human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Develop PM-specific benchmarks for hypothesis generation, require accompanying executable verification queries (e.g., SQL), and combine automated checks with human assessment of novelty and actionability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. <em>(Rating: 2)</em></li>
                <li>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. <em>(Rating: 2)</em></li>
                <li>ARB: Advanced Reasoning Benchmark for Large Language Models. <em>(Rating: 2)</em></li>
                <li>MMBench: Is Your Multi-modal Model an Allaround Player?. <em>(Rating: 2)</em></li>
                <li>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. <em>(Rating: 2)</em></li>
                <li>Measuring Coding Challenge Competence With APPS. <em>(Rating: 2)</em></li>
                <li>Evaluating the Text-to-SQL Capabilities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. <em>(Rating: 2)</em></li>
                <li>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9696",
    "paper_id": "paper-268351639",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Automatic evaluation",
            "name_full": "Automatic / Metric-based Evaluation",
            "brief_description": "Automated assessment methods proposed for tasks amenable to formal verification (e.g., text-to-SQL), using formal correctness, syntactic/semantic match to ground-truth, and conciseness metrics (e.g., query length).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (evaluation of generated analyses and hypotheses)",
            "evaluation_method": "Automated metric comparison of generated artefacts (e.g., SQL queries, declarative constraints) against ground-truth or executable verification, measuring formal correctness and conciseness.",
            "evaluation_criteria": "Formal accuracy (syntactic and semantic correctness), conciseness (e.g., query length), executability, and correctness of produced constraints/queries.",
            "benchmark_or_dataset": "No single PM-specific dataset presented; proposed to use existing text-to-SQL benchmarks (e.g., SPIDER) and domain-specific event logs for automated checks.",
            "results_summary": "Paper recommends automatic evaluation as particularly suited to text-to-SQL and formally-checkable outputs; no quantitative results reported in this survey paper.",
            "limitations_or_challenges": "Many PM tasks are not fully formalizable; ground-truth for higher-level insights/hypotheses is often unavailable, limiting automated evaluation applicability.",
            "comparison_to_human_or_traditional": "Compared to human evaluation, automatic evaluation is scalable and objective for formal tasks but insufficient for subjective/higher-level reasoning or hypothesis novelty.",
            "recommendations_or_best_practices": "Use automated checks where possible (e.g., execute generated SQL against databases), measure formal correctness and conciseness, and complement with human review for broader tasks.",
            "uuid": "e9696.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human evaluation",
            "name_full": "Human Expert Evaluation",
            "brief_description": "Manual assessment by domain experts for tasks such as anomaly detection and hypothesis generation, focusing on recall and precision of insights and the effectiveness of iterative validation cycles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (human assessment of model-generated analyses and hypotheses)",
            "evaluation_method": "Human experts review model outputs (e.g., detected anomalies, suggested root causes and hypotheses) and score or annotate outputs for correctness, relevance, recall and precision.",
            "evaluation_criteria": "Recall (ability to identify expected/known insights), precision (correctness of findings), usefulness of explanations, and effectiveness of feedback cycles in validating/refining hypotheses.",
            "benchmark_or_dataset": "Human-labeled event logs, annotated anomalies and curated PM scenarios; no single PM-specific benchmark established in paper.",
            "results_summary": "Paper stresses human evaluation as essential for direct querying and hypothesis generation; no quantitative human-study results presented here.",
            "limitations_or_challenges": "Human evaluation is costly, subjective, and can be inconsistent across annotators; requires domain expertise and clear evaluation protocols.",
            "comparison_to_human_or_traditional": "Human evaluation is the traditional gold standard for process analysis; LLM evaluation should include human judgment especially for high-level reasoning and novelty assessments.",
            "recommendations_or_best_practices": "Use human evaluation to assess recall/precision on curated scenarios, combine with automated verification where possible, and define clear annotation guidelines.",
            "uuid": "e9696.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-eval techniques",
            "name_full": "Self-Evaluation and Ensemble Techniques",
            "brief_description": "Set of LLM-internal or multi-model strategies to detect and reduce hallucinations including chain-of-thought, confidence scoring, ensembling (majority vote), and self-reflection where a model reviews its own or peers' outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (robustness and trustworthiness of generated insights)",
            "evaluation_method": "Model-internal self-checks (confidence scores, chain-of-thought) and cross-session ensembling to corroborate outputs; self-reflection prompts for error detection.",
            "evaluation_criteria": "Calibration of confidence (correspondence between reported confidence and correctness), consistency across ensemble runs, and ability of chain-of-thought/self-reflection to surface reasoning steps and errors.",
            "benchmark_or_dataset": "No dedicated PM dataset; techniques evaluated conceptually and supported by literature (e.g., self-evaluation, ensembling).",
            "results_summary": "Paper recommends these techniques to mitigate hallucinations—confidence thresholds to discard low-confidence outputs and ensembling to confirm anomalies—no empirical metrics reported in this paper.",
            "limitations_or_challenges": "Confidence scores can be miscalibrated (confidence-competence gap); ensembling and multiple runs increase computational cost; self-reflection can propagate initial errors without external grounding.",
            "comparison_to_human_or_traditional": "Self-eval/ensembling can approach human-like error checking but should be used alongside human review and external data checks for critical decisions.",
            "recommendations_or_best_practices": "Apply confidence thresholds, ensemble multiple runs, prompt for chain-of-thought where transparency helps, and use self-reflection policies to flag questionable outputs for human review.",
            "uuid": "e9696.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve transparency and reasoning quality.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "General reasoning / Process Mining reasoning",
            "evaluation_method": "Encourage the model to produce step-wise reasoning (chain-of-thought) and evaluate the resulting explanations for coherence and correctness.",
            "evaluation_criteria": "Quality and correctness of intermediate steps, improvement in final-answer accuracy, and usefulness of explanations for human validation.",
            "benchmark_or_dataset": "Referenced as a general technique; not tied to a PM dataset in this paper.",
            "results_summary": "Paper cites chain-of-thought as a technique to enhance explanations and reduce hallucinations; no PM-specific empirical results in this survey.",
            "limitations_or_challenges": "Chain-of-thought increases verbosity and may expose incorrect intermediate reasoning; it does not guarantee factual grounding.",
            "comparison_to_human_or_traditional": "Provides human-readable reasoning akin to expert explanations, aiding human evaluation and trustworthiness checks.",
            "recommendations_or_best_practices": "Use chain-of-thought for tasks where reasoning transparency aids validation, but combine with grounding checks (data or executable verification).",
            "uuid": "e9696.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Confidence scoring",
            "name_full": "Confidence Scores / Calibration",
            "brief_description": "Using LLM-reported confidence or calibrated probability measures to judge which outputs to trust or discard, mitigating hallucination in anomaly detection and hypothesis proposals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (anomaly detection, hypothesis generation)",
            "evaluation_method": "Obtain model confidence for each generated insight and apply thresholds to filter low-confidence items; analyze calibration between confidence and empirical correctness.",
            "evaluation_criteria": "Calibration (how well confidence predicts correctness), precision/recall tradeoffs when filtering by confidence, and downstream impact of removing low-confidence outputs.",
            "benchmark_or_dataset": "No explicit PM dataset provided for calibration tests in this paper; conceptually applied to event-log analyses.",
            "results_summary": "Paper recommends using confidence scores to exclude doubtful anomalies; no calibration experiments reported here.",
            "limitations_or_challenges": "LLM confidence is often miscalibrated (confidence-competence gap); requires empirical calibration and may discard correct but low-confidence novel findings.",
            "comparison_to_human_or_traditional": "Human experts naturally provide calibrated judgments; model confidence can supplement human review but should not replace it.",
            "recommendations_or_best_practices": "Calibrate model confidence empirically on held-out PM scenarios and use conservative thresholds combined with human oversight.",
            "uuid": "e9696.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Ensembling",
            "name_full": "Ensembling / Multi-session Consensus",
            "brief_description": "Aggregating multiple model outputs (different runs or models) and using majority voting or consensus mechanisms to increase reliability and reduce single-run hallucinations.",
            "citation_title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (robust detection and hypothesis corroboration)",
            "evaluation_method": "Run multiple independent LLM sessions or models on the same prompt and aggregate results via majority voting or confidence-weighted fusion to identify stable findings.",
            "evaluation_criteria": "Inter-run agreement, improved precision at fixed recall, and robustness of identified anomalies/hypotheses across sessions.",
            "benchmark_or_dataset": "Conceptual; no PM-specific ensemble benchmark provided in this paper.",
            "results_summary": "Paper suggests ensembling as a way to increase reliability (confirm detections across sessions); no empirical ensemble results in this paper.",
            "limitations_or_challenges": "Increased computational cost and potential correlated errors across models; majority voting can suppress minority but correct novel hypotheses.",
            "comparison_to_human_or_traditional": "Analogous to consulting multiple experts; increases confidence but still benefits from adjudicating disagreements with human experts.",
            "recommendations_or_best_practices": "Use ensembling to corroborate anomalies/hypotheses and combine with confidence scoring and human adjudication for final decisions.",
            "uuid": "e9696.5",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "AGIEval",
            "name_full": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
            "brief_description": "A general-purpose benchmark that assesses LLMs via standardized exam-like prompts and human-centered evaluation, focusing on broad capabilities including domain knowledge and reasoning.",
            "citation_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "General (relevant to domain knowledge and reasoning required in PM)",
            "evaluation_method": "Benchmark LLMs on exam-like, standardized prompts, often assessed with human scoring or automated grading where possible.",
            "evaluation_criteria": "Accuracy on exam items, reasoning quality, and instructional/dialog capabilities depending on test design.",
            "benchmark_or_dataset": "AGIEval: collection of standardized exam-style questions across domains.",
            "results_summary": "Paper lists AGIEval as a relevant traditional benchmark for choosing LLMs for PM tasks; no AGIEval results reported here.",
            "limitations_or_challenges": "Exam-style benchmarks may not measure PM-specific capabilities (visual comprehension, long-context processing, text-to-SQL).",
            "comparison_to_human_or_traditional": "AGIEval approximates human exam performance; useful as a general indicator but insufficient alone for PM suitability.",
            "recommendations_or_best_practices": "Use AGIEval as part of a battery of tests but supplement with PM-specific and multimodal benchmarks.",
            "uuid": "e9696.6",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "XIEZHI",
            "name_full": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
            "brief_description": "A domain-knowledge benchmark assessing LLM knowledge across multiple specialized fields (economics, science, engineering), useful for evaluating domain expertise needed in PM tasks.",
            "citation_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "General domain knowledge; relevant for Process Mining where domain context matters",
            "evaluation_method": "Test models on domain-specific knowledge questions; evaluate factual accuracy and depth of domain reasoning.",
            "evaluation_criteria": "Coverage and accuracy across domains, recall of factual information, and reasoning over domain concepts.",
            "benchmark_or_dataset": "XIEZHI: cross-domain knowledge benchmark (ever-updating collection).",
            "results_summary": "Paper references XIEZHI as relevant for assessing domain knowledge for PM tasks; no evaluation numbers given here.",
            "limitations_or_challenges": "Domain benchmarks may not capture PM-specific modalities (visual logs, SQL generation) and are not tailored to PM workflows.",
            "comparison_to_human_or_traditional": "Domain benchmarks approximate specialist knowledge testing; human experts remain necessary for nuanced process understanding.",
            "recommendations_or_best_practices": "Include domain-knowledge benchmarks like XIEZHI in the evaluation suite and augment with PM-specific tests.",
            "uuid": "e9696.7",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MMBench / MM-Vet",
            "name_full": "MMBench & MM-Vet: Multimodal Visual Benchmarks",
            "brief_description": "Benchmarks designed to test multimodal LLM capabilities on image understanding (MMBench) and integrated multimodal tasks including OCR and recognition (MM-Vet); relevant for interpreting PM visualizations.",
            "citation_title": "MMBench: Is Your Multi-modal Model an Allaround Player?.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Multimodal evaluation (visual understanding relevant to Process Mining visualizations)",
            "evaluation_method": "Assess model performance on image recognition, OCR, and other multimodal tasks; evaluate answers to visual prompts.",
            "evaluation_criteria": "Accuracy on visual understanding tasks, OCR quality, and ability to interpret plot-specific features (line orientation, point color/size).",
            "benchmark_or_dataset": "MMBench and MM-Vet: suites of multimodal tasks and datasets for image+text evaluation.",
            "results_summary": "Paper notes these benchmarks as relevant but insufficient for PM-specific visuals (e.g., dotted charts, performance spectrum) since they may not test fine-grained plot feature interpretation.",
            "limitations_or_challenges": "Existing multimodal benchmarks may not evaluate fine-grained visualization interpretation needed in PM (line slopes, point aggregation patterns).",
            "comparison_to_human_or_traditional": "Human analysts better interpret complex PM visualizations; multimodal benchmarks are a step toward automation but require PM-specific tasks.",
            "recommendations_or_best_practices": "Use multimodal benchmarks to screen for visual capabilities, and develop PM-specific visualization tests to complement them.",
            "uuid": "e9696.8",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Text-to-SQL benchmarks",
            "name_full": "SPIDER & APPS (Text-to-SQL and Code Generation)",
            "brief_description": "Benchmarks used to evaluate LLM abilities to translate natural language to SQL (SPIDER) and broader code-generation competence (APPS), both relevant for PM tasks requiring query generation.",
            "citation_title": "Measuring Coding Challenge Competence With APPS.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Text-to-SQL and program generation for Process Mining data extraction and analysis",
            "evaluation_method": "Measure syntactic and semantic correctness of generated SQL/code, executability, and ability to express complex joins and temporal calculations.",
            "evaluation_criteria": "Exact-match, execution accuracy, generalization to unseen schemas, and conciseness of produced queries or code.",
            "benchmark_or_dataset": "SPIDER (text-to-SQL dataset for complex and cross-domain queries) and APPS (coding challenge benchmark).",
            "results_summary": "Paper suggests using text-to-SQL benchmarks to assess LLM ability to generate executable queries for PM workflows; no performance numbers provided.",
            "limitations_or_challenges": "SPIDER and APPS evaluate general code/sql skills but not PM-specific schema translations (event-log schemas) or PM semantics.",
            "comparison_to_human_or_traditional": "Automated text-to-SQL evaluation can approximate correctness of queries that a human would handcraft, but humans are needed to validate domain intent.",
            "recommendations_or_best_practices": "Use SPIDER/APPS to evaluate baseline code generation, and create PM-specific text-to-SQL tests that reflect common event-log schemas and PM operators.",
            "uuid": "e9696.9",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "DecodingTrust",
            "name_full": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
            "brief_description": "A benchmark covering trustworthiness aspects including toxicity, bias, robustness, privacy, ethics and fairness, cited as relevant for assessing fairness in PM applications.",
            "citation_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "mention_or_use": "mention",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Trustworthiness and fairness evaluation applicable to Process Mining use-cases (bias detection in processes)",
            "evaluation_method": "Assess models across multiple trust dimensions (bias, toxicity, privacy, robustness) using curated tests and metrics.",
            "evaluation_criteria": "Metrics for bias detection, fairness across groups, robustness under distributional shifts, and privacy leakage.",
            "benchmark_or_dataset": "DecodingTrust: suite of tests for trustworthiness attributes in LLMs.",
            "results_summary": "Paper lists DecodingTrust as a fairness/trust benchmark relevant to PM (e.g., detecting bias in recruitment processes); no empirical application in this paper.",
            "limitations_or_challenges": "General trustworthiness benchmarks may not capture PM-specific fairness subtleties (process-level disparities) without adaptation.",
            "comparison_to_human_or_traditional": "Provides automated diagnostics complementing human audits for fairness, but human oversight remains necessary for remediation.",
            "recommendations_or_best_practices": "Include trustworthiness benchmarks like DecodingTrust in evaluation pipelines and adapt fairness tests to PM scenarios (attribute-level analyses).",
            "uuid": "e9696.10",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Hypothesis generation evaluation",
            "name_full": "Evaluation of Hypotheses Generation (needs PM-specific benchmarks)",
            "brief_description": "Paper highlights the absence of PM-specific benchmarks for autonomous hypothesis generation and points to related literature on scientific hypothesis discovery as partial guidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Process Mining (automated scientific/hypothesis discovery over event data)",
            "evaluation_method": "Combination of human evaluation (precision/recall of generated hypotheses), automated verification via executable SQL checks against event data, and iterative feedback cycles to validate/refine hypotheses.",
            "evaluation_criteria": "Novelty, correctness (verifiable against data), usefulness for process improvement, and ability to suggest verifiable SQL for checks.",
            "benchmark_or_dataset": "No established PM-specific hypothesis-generation benchmark; related work in scientific hypothesis discovery suggested as starting points (e.g., 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery').",
            "results_summary": "Paper asserts a gap: while LLMs can generate hypotheses, there is a lack of PM-specific benchmarks to systematically evaluate them; no quantitative results provided.",
            "limitations_or_challenges": "Lack of ground-truth hypotheses, difficulty in measuring novelty and usefulness objectively, and risk of hallucinated causal claims.",
            "comparison_to_human_or_traditional": "Humans traditionally generate and test hypotheses via experiments and data queries; LLMs can propose candidates but need formal verification and human adjudication.",
            "recommendations_or_best_practices": "Develop PM-specific benchmarks for hypothesis generation, require accompanying executable verification queries (e.g., SQL), and combine automated checks with human assessment of novelty and actionability.",
            "uuid": "e9696.11",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "bamboo_a_comprehensive_benchmark_for_evaluating_long_text_modeling_capacities_of_large_language_models"
        },
        {
            "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.",
            "rating": 2,
            "sanitized_title": "xiezhi_an_everupdating_benchmark_for_holistic_domain_knowledge_evaluation"
        },
        {
            "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models.",
            "rating": 2,
            "sanitized_title": "arb_advanced_reasoning_benchmark_for_large_language_models"
        },
        {
            "paper_title": "MMBench: Is Your Multi-modal Model an Allaround Player?.",
            "rating": 2,
            "sanitized_title": "mmbench_is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.",
            "rating": 2,
            "sanitized_title": "mmvet_evaluating_large_multimodal_models_for_integrated_capabilities"
        },
        {
            "paper_title": "Measuring Coding Challenge Competence With APPS.",
            "rating": 2,
            "sanitized_title": "measuring_coding_challenge_competence_with_apps"
        },
        {
            "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "evaluating_the_texttosql_capabilities_of_large_language_models"
        },
        {
            "paper_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "rating": 2,
            "sanitized_title": "decodingtrust_a_comprehensive_assessment_of_trustworthiness_in_gpt_models"
        },
        {
            "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        }
    ],
    "cost": 0.0163835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 2024</p>
<p>Alessandro Berti alessandro.berti@fit.fraunhofer.de 0000-0002-3279-4795
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Humam Kourani humam.kourani@fit.fraunhofer.de 0000-0003-2375-2152
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Hannes Häfke hannes.haefke@fit.fraunhofer.de 0000-0002-2845-3998
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Chiao-Yun Li chiao-yun.li@fit.fraunhofer.de 0009-0002-3767-7915
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Daniel Schuster daniel.schuster@fit.fraunhofer.de 0000-0002-6512-9580
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 20242EF1C7951936BFA3FE2194CB50117E45arXiv:2403.06749v3[cs.DB]Large Language Models (LLMs)Output EvaluationBenchmarking Strategies
Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks?The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
<p>Introduction</p>
<p>Process mining (PM) is a data science field focusing on deriving insights about business process executions from event data recorded by information systems [1].Several types of PM exist, including process discovery (learning process models from event data), conformance checking (comparing event data with process models), and process enhancement (adding frequency/performance metrics to process models).Although many automated methods exist for PM, human analysts usually handle process analysis due to the need for domain knowledge.Recently, LLMs have emerged as conversational interfaces trained on extensive data [28], achieving near-human performance in various general tasks [38].Their potential in PM lies in embedded domain knowledge useful for generating database queries and insights [21], logical and temporal reasoning capabilities [2,16], inference abilities over structured data [12].Prior research has asserted the usage of LLMs for PM tasks [3,4].However, a comprehensive discussion on Fig. 1: Outline of the contributions of this paper.necessary capabilities for PM, LLMs' suitability evaluation for process analytics, and assessment of LLMs' outputs in the PM context is lacking.</p>
<p>The three main contributions of this paper are summarized in Fig. 1.First, building upon prior work [3,4] proposing textual abstractions of process mining artifacts and an experimental evaluation of LLMs' responses, the essential capabilities that LLMs must have for PM tasks are derived in Section 3.1.The aforementioned capabilities allow us to narrow down the field of LLMs to those that meet these requirements.Next, evaluation benchmarks for selecting suitable LLMs are introduced in Section 3.2, incorporating both process-miningspecific and general criteria such as reasoning, visual understanding, factuality, and trustworthiness.Finally, we suggest automatic, human, and self-assessment methods for evaluating LLMs' outputs on specific tasks in Section 3.3, aiming to establish a comprehensive PM benchmark and enhance confidence in LLMs' usage, addressing potential issues like hallucination.</p>
<p>This paper provides an orientation to process mining researchers investigating the usage of LLMs, i.e., this paper aims to facilitate PM-on-LLMs research.</p>
<p>Background</p>
<p>LLMs enhance PM with superior capabilities, handling complex tasks through data understanding and natural language processing.This section covers PM tasks with LLMs (Section 2.1) and the adopted implementation paradigms (Section 2.2) along with the provision of additional domain knowledge.</p>
<p>Process Mining Tasks for LLMs</p>
<p>This subsection explores a range of PM tasks in which LLMs have already been adopted for process mining research.LLMs facilitate the automation of generating textual descriptions from process data, handling inputs such as event logs or formal process models [4].They also generate process models from textual descriptions, with studies showing LLMs creating BPMN models and declarative constraints from text [7].In the realm of anomaly detection, LLMs play a crucial role in identifying process data anomalies, including unusual activities and performance bottlenecks, offering context-aware detection that adapts to new patterns through prompt engineering.This improves versatility over traditional methods [3,4].For root cause analysis, LLMs analyze event logs to suggest causes of anomalies or inefficiencies, linking delays to specific conditions or events.This goes beyond predefined logic, employing language processing for context-aware analysis [3,4] In ensuring fairness, LLMs identify and mitigate bias in processes, suggesting adjustments.They analyze processes like recruitment to detect disparities in rejection rates or delays by gender or nationality, aiding in fair decision-making [22,4].LLMs can also interpret and explain visual data, including complex visualizations, by describing event flows in dotted charts and identifying specific patterns, such as batch processing.For process improvement, after PM tasks identify and analyze problems, LLMs can suggest actions and propose new process constraints [22,4].</p>
<p>Implementation Paradigms of Process Mining on LLMs</p>
<p>To effectively employ LLMs for PM tasks, specific implementation paradigms are required [3,4].This section outlines key approaches for implementing LLMs in PM tasks.We distinguish three main strategies:</p>
<p>-Direct provision of insights: A prompt is generated that merges data abstractions with a query about the task.Also, interactive dialogue between the LLM and the user is possible for step-by-step analysis.The user starts with a query and refines or adjusts it based on the LLM's feedback, continuing until achieving the desired detail or accuracy, such as pinpointing process inefficiencies.For instance, to have LLMs identifying unusual behavior in an event log, we combine a textual abstraction of the log (such as the directly-follows graph or list of process variants) with a question like "Can you analyze the log to detect any unusual behavior?"-Code generation: LLMs can be used to create structured queries, like SQL, for advanced PM tasks [11].Rather than directly asking LLMs for answers, users command LLMs to craft database queries from natural language.These queries are then executed on the databases holding PM information.It is applicable to PM tasks that can be converted into database queries, such as filtering event logs or computing the average duration of process steps.Also, LLMs can be used to generate executable programs that use existing PM libraries to infer insights over the event data [9].-Automated hypotheses generation: Combining the previous strategies by using textual data abstraction to prompt LLMs for autonomous hypotheses generation [3,4].The hypotheses are accompanied by SQL queries for verification against event data.Results confirm or refute these hypotheses, with potential for LLM-suggested refinements of hypotheses.</p>
<p>LLMs may require additional knowledge about processes and databases to implement PM tasks, for example, in anomaly detection and crafting accurate database queries.Some strategies are used to equip LLMs with this additional domain knowledge [14], including fine-tuning and prompt engineering.</p>
<p>Evaluating LLMs in Process Mining</p>
<p>This section introduces criteria for selecting LLMs that are suitable for PM tasks.Moreover, we introduce criteria for evaluating their outputs.First, in Section 3.1, we discuss the fundamental capabilities needed for PM (long context window, acceptance of visual prompts, coding, factuality).Then, we introduce in Section 3.2 general-purpose and process-mining-specific benchmarks to measure the different LLMs on process-mining-related tasks.To foster the development of process-mining-specific benchmarks and to be able to evaluate a given output, we propose in Section 3.3 different methods to evaluate the output of an LLM.</p>
<p>LLMs Capabilities Needed for Process Mining</p>
<p>In this section, we discuss four important capabilities of LLMs for PM tasks:</p>
<p>-Long Context Window : Event logs in PM often include a vast amount of cases and events, challenging the context window limit of LLMs, which restricts the token count in a prompt [13].Moreover, also the textual specification of process models requires a significant amount of information.The context window limit can be severe in many currently popular LLMs. 3 Even simple abstractions like the ones introduced in [3] (directly-follows graph, list of process variants) may exceed this limitation.The context window, which is set during model training, must be large enough for the data size.Recent efforts aim to extend this limit, though quality may decline [13,20].-Accepting Visual Prompts: Visualizations in PM, such as the dotted chart and the performance spectrum [15], summarizing process behavior, empower analysts to spot interesting patterns not seen in tables.Interpreting visual prompts is key for semi-automated PM.Large Visual Models (LVMs) use architectures similar to language models trained on annotated image datasets [31].They perform tasks like object detection and image synthesis, recognizing patterns, textures, shapes, colors, and spatial relations. 4Coding (Text-to-SQL) Capabilities: With the context window limit preventing full event log inclusion in prompts, generating scripts and database queries is crucial for analyzing event data.As discussed in Section 2.2, text-to-SQL assists in filtering and analyzing event data.Key requirements for text-to-SQL in PM include understanding database schemas, performing complex joins, using database-specific operators (e.g., for calculating date differences), and translating PM concepts into queries.Overall, modern LLMs offer excellent coding capabilities [3].</p>
<p>-Factuality: LLM hallucination involves generating incorrect or fabricated information [24].Factuality measures an LLM's ability to cross-check its outputs against real facts or data, crucial for PM tasks like anomaly detection and root cause analysis.This may involve leveraging external databases [19], knowledge bases, or internet search [32] for validation.For instance, verifying the sequence Cancel Order" followed by Deliver Order" against public data in anomaly detection.LLMs with web browsing can access up-to-date information, enhancing factuality. 5</p>
<p>Relevant LLMs Benchmarks</p>
<p>After identifying the required capabilities for LLMs in PM, benchmarking strategies are essential to measure the quality of the textual outputs returned by the LLMs satisfying such capabilities.</p>
<p>Considering the wide array of available benchmarks for assessing LLMs behavior, we focus on identifying those most relevant to PM capabilities.In [5], a comprehensive collection of benchmarks is introduced.This section aims to select and utilize some of these benchmarks to evaluate various aspects of LLMs' performance in PM contexts.</p>
<p>-Traditional benchmarks: Textual prompts are crucial for LLMs evaluation in PM.Benchmarks like AGIEval assess models via standardized exams [37], and MT-Bench focuses on conversational and instructional capabilities [36].</p>
<p>Another benchmark evaluates LLMs on prompts of long size [6].-Domain knowledge benchmarks: Domain knowledge is essential for LLMs in PM to identify anomalies using metrics and context.Benchmarks like XIEZHI assess knowledge across different fields (economics, science, engineering) [8], while ARB evaluates expertise in areas like mathematics and natural sciences [26].-Visual benchmarks: Understanding PM visualizations, such as dotted charts, is essential (c.f.Section 3.1).LLMs must accurately process queries on these visualizations.MMBench tests models on image tasks [17], and MM-Vet assesses recognition, OCR, among others [35].Yet, they may not fully meet PM visualization analysis needs, particularly in evaluating line orientations and point size/color.-Benchmarks for Text-to-SQL: In PM, generating SQL from natural language is key for tasks like event log filtering.Benchmarks such as SPIDER and SPIDER-realistic test LLMs on text-to-SQL conversion [23].The APPS benchmark evaluates broader code generation abilities [10].-Fairness benchmarks: they evaluate LLM fairness in PM by analyzing group treatment and bias detection.DecodingTrust measures LLM trustworthiness, covering toxicity, bias, robustness, privacy, ethics, and fairness [30].-Benchmarking the generation of hypotheses: LLMs' ability to generate hypotheses from event data is vital to implement semi-autonomous PM agents.
X X Process Modeling X X X X X X X Anomaly Detection X X X X X X Root Cause Analysis X X X X X X Ensuring Fairness X X X X X X X Expl. and Interpreting X X X Visualizations Process Improvement X X X X X X X X
While specific benchmarks for hypothesis generation are lacking, related studies like [29] and [34] evaluate LLMs using scientific papers.</p>
<p>In Table 1, we link process mining (PM) tasks to implementation paradigms and benchmarks.We discuss these tasks:</p>
<p>-Process description requires understanding technical terms relevant to the domain, crucial for accurately describing processes.-Process modeling involves generating models from text, using SQL for declarative and BPMN XML for procedural models.LLMs should offer various model hypotheses.-Anomaly detection and root cause analysis need domain knowledge to analyze process sequences or identify event attribute combinations causing issues.-Fairness involves detecting biases by analyzing event attributes and values, necessitating hypothesis generation by LLMs.-Explaining and interpreting visualizations requires extracting features from images and texts, offering contextual insights, like interpreting performance spectrum visualization [15].-Process improvement entails suggesting text proposals or new constraints to enhance current models, leveraging code generation capabilities and understanding process limitations.</p>
<p>While general-purpose benchmarks are already developed and are easily accessible, they are not entirely suited for the task of PM-on-LLMs.In particular, visual capabilities (explaining and interpreting PM visualizations) and autonomous hypotheses generation require more PM-specific benchmarks.However, little research exists on PM-specific benchmarks [3,4].</p>
<p>How to Evaluate LLMs Outputs</p>
<p>This section outlines criteria for assessing the quality of outputs generated by LLMs in PM tasks, serving two primary objectives.The first objective is to as-sist users in identifying and addressing hallucinations and inaccuracies in LLMs' outputs.The second aim is to establish criteria for developing an extensive benchmark specifically tailored to PM applications of LLMs.The strategies follow:</p>
<p>-Automatic evaluation is particularly suited for text-to-SQL tasks.In this context, the formal accuracy and conciseness (indicated by the length of the produced query) of the SQL queries generated can be efficiently assessed.</p>
<p>Additionally, the creation of declarative constraints, designed to enhance process execution, can also be evaluated in terms of their formal correctness.-Human evaluation is essential for LLM tasks like direct querying and hypothesis generation.For direct querying tasks such as anomaly detection and root cause analysis, important criteria are recall (the model's ability to identify expected insights) and precision (the correctness of insights).These criteria also apply to hypothesis generation.Additionally, evaluating the feedback cycle's effectiveness in validating original hypotheses is crucial for these tasks.-Self-evaluation in LLMs tackles hallucinations, as noted by [24].Techniques include chain-of-thought, where LLMs detail their reasoning, enhancing explanations [33].Confidence scores let LLMs assess their insights' reliability, discarding uncertain outputs for quality [27].Ensembling, or using results from multiple LLM sessions, increases accuracy via majority voting or confidence checks [18].Self-reflection, an LLM reviewing its or another's output, detects errors [25].In anomaly detection, using confidence scores to exclude doubtful anomalies and ensembling to confirm detections across sessions improves reliability.</p>
<p>Conclusion</p>
<p>This paper examines LLM applications in PM, offering three main contributions: identification of necessary LLM capabilities for PM, review of benchmarks from literature, and strategies for evaluating LLM outputs in PM tasks.These strategies aim to build confidence in LLM use and establish benchmarks to assess LLM effectiveness across PM implementations.</p>
<p>Our discussion centers on current generative AI capabilities within PM, anticipating advancements like deriving event logs from videos.Despite future enhancements, the criteria discussed here should remain pertinent.Benchmarking for PM tasks on large language models (LLMs) will evolve, including both general and PM-specific benchmarks, yet the foundational aspects and methodologies are expected to stay consistent.</p>
<p>Table 1 :
1
Implementation paradigms and benchmarks for LLMs in the context of different PM tasks.
TaskParadigmsBenchmarks ClassesDirect ProvisionCode GenerationHypotheses GenerationTraditionalDomain KnowledgeVisual PromptsText-to-SQLFairnessHypotheses GenerationProcess DescriptionX
https://community.openai.com/t/are-the-full-8k-gpt-4-tokens-available-on-chatgpt/237999
 and Google Bard/Gemini are popular models supporting both visual and textual prompts.
https://cointelegraph.com/news/chat-gpt-ai-openai-browse-internet-no-longer-limited-info-2021</p>
<p>W M P Van Der Aalst, Process Mining -Data Science in Action. ess Mining -Data Science in ActionSpringer2016Second Edition</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Y Bang, S Cahyawijaya, 10.48550/arXiv.2302.040232023</p>
<p>Leveraging Large Language Models (LLMs) for Process Mining. A Berti, M S Qafari, 10.48550/arXiv.2307.127012023Technical Report</p>
<p>A Berti, D Schuster, W M P Van Der Aalst, 10.48550/arXiv.2307.02194Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study. 2023</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, 10.48550/arXiv.2307.031092023</p>
<p>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. Z Dong, T Tang, 10.48550/arXiv.2309.133452023</p>
<p>Large Language Models Can Accomplish Business Process Management Tasks. M Grohs, L Abb, BPM 2023 International Workshops. Lecture Notes in Business Information Processing. Springer2023492</p>
<p>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. Z Gu, X Zhu, 10.48550/arXiv.2306.057832023</p>
<p>Conceptual model interpreter for Large Language Models. F Härer, ER 2023. CEUR Workshop Proceedings. 20233618</p>
<p>Measuring Coding Challenge Competence With APPS. D Hendrycks, S Basart, NeurIPS Datasets and Benchmarks. 20212021</p>
<p>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. U Jessen, M Sroka, D Fahland, 10.48550/arXiv.2307.099092023</p>
<p>StructGPT: A General Framework for Large Language Model to Reason over Structured Data. J Jiang, K Zhou, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. H Jin, X Han, 10.48550/arXiv.2401.013252024</p>
<p>T Kampik, C Warmuth, 10.48550/arXiv.2309.00900Large Process Models: Business Process Management in the Age of Generative AI. 2023</p>
<p>Performance Mining for Batch Processing Using the Performance Spectrum. E L Klijn, D Fahland, BPM 2019 International Workshops. Lecture Notes in Business Information Processing. Springer2019362</p>
<p>H Liu, R Ning, 10.48550/arXiv.2304.03439Evaluating the Logical Reasoning Ability of Chat-GPT and GPT-4. 2023</p>
<p>MMBench: Is Your Multi-modal Model an Allaround Player?. Y Liu, H Duan, 10.48550/arXiv.2307.062812023</p>
<p>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. K Lu, H Yuan, 10.48550/arXiv.2311.086922023</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, 10.48550/arXiv.2306.083022023</p>
<p>YaRN: Efficient Context Window Extension of Large Language Models. B Peng, J Quesnelle, 10.48550/arXiv.2309.000712023</p>
<p>F Petroni, T Rocktäschel, Language Models as Knowledge Bases? In: EMNLP-IJCNLP 2019. Association for Computational Linguistics2019</p>
<p>M S Qafari, W M P Van Der Aalst, Fairness-Aware Process Mining. Lecture Notes in Computer Science. Springer2019. 201911877</p>
<p>Evaluating the Text-to-SQL Capabilities of Large Language Models. N Rajkumar, R Li, D Bahdanau, 10.48550/arXiv.2204.004982022</p>
<p>The Troubling Emergence of Hallucination in Large Language Models -An Extensive Definition, Quantification, and Prescriptive Remediations. V Rawte, S Chakraborty, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>Self-Evaluation Improves Selective Generation in Large Language Models. J Ren, Y Zhao, 2023</p>
<p>ARB: Advanced Reasoning Benchmark for Large Language Models. T Sawada, D Paleka, 10.48550/arXiv.2307.136922023</p>
<p>The Confidence-Competence Gap in Large Language Models: A Cognitive Study. A K Singh, S Devkota, 10.48550/arXiv.2309.161452023</p>
<p>Welcome to the Era of ChatGPT et al. T Teubner, C M Flath, Bus. Inf. Syst. Eng. 6522023</p>
<p>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 2023</p>
<p>B Wang, W Chen, 10.48550/arXiv.2306.11698DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2023</p>
<p>Review of Large Vision Models and Visual Prompt Engineering. J Wang, Z Liu, 10.48550/arXiv.2307.008552023</p>
<p>L Wang, C Ma, 10.48550/arXiv.2308.11432A Survey on Large Language Model based Autonomous Agents. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, NeurIPS. 20222022</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Z Yang, X Du, 10.48550/arXiv.2309.027262023</p>
<p>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. W Yu, Z Yang, 10.48550/arXiv.2308.024902023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, W Chiang, 10.48550/arXiv.2306.056852023</p>
<p>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. W Zhong, R Cui, 10.48550/arXiv.2304.063642023</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, ICLR 2023. OpenReview.net2023</p>            </div>
        </div>

    </div>
</body>
</html>
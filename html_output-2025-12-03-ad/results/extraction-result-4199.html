<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276961662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.09894v2.pdf" target="_blank">What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines. Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work. Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive. Structured representations offer a natural complement -- enabling systematic analysis across the whole corpus. Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs. By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole. Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts. To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology. The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code: https://github.com/chiral-carbon/kg-for-science.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surveyor pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based structured concept extraction pipeline (this work; demo: abby101/surveyor-0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic pipeline that uses a schema of nine concept categories and an LLM to extract categorized concepts sentence-by-sentence from paper titles/abstracts, store them in a SQL database, and build co-occurrence knowledge graphs for analyzing methodological patterns and trends across disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Surveyor LLM-based structured concept extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Processes paper titles and abstracts sentence-by-sentence using few-shot prompting to tag text with a 9-category scientific schema (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments). Prompt engineering used 3 demonstration papers (few-shot) and a 17-paper development set; outputs can be human-readable or JSON and are stored in a SQL database (tables: papers, predictions). Extracted concepts are visualized as co-occurrence graphs (nodes = concepts, edges = co-occurrence within a paper) using a force-directed layout (d3-force). The pipeline supports SQL queries to compute aggregate statistics (e.g., modality distributions, temporal trends) and was designed to enable systematic discovery of relationships and patterns across large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics, fluid dynamics, evolutionary biology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>29,980 (final extractions: 9,980 astrophysics + 10,000 fluid dynamics + 10,000 evolutionary biology)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>co-occurrence relationships, methodological and temporal usage patterns, extraction of numeric properties (e.g., ages, velocities) and empirical/statistical relationships across papers; the system does not report automated derivation of new physical/scaling laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The pipeline extracts numeric properties and quantitative descriptors from abstracts (examples present in corpus: 'age = 181 ± 25 years', 'velocity ≈ 334 km s⁻¹', 'acceleration ≈ -0.64 m s⁻²'); it supports queries to compute modality distributions and track adoption trends and to examine how similar mathematical models are applied across fields, but no new closed-form physical laws or scaling laws are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Sentence-level few-shot prompting with manually curated demonstrations and iterative prompt engineering; outputs in human-readable or JSON formats; stored in SQL. Co-occurrence graphs built from concept co-occurrence within papers. Comparison vs. RAKE baseline performed on a 300-paper sample.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Development/evaluation on 20 manually annotated astrophysics papers (3 for few-shot examples, 17 for prompt development) with exact-match precision, recall and F1 used as directional metrics; qualitative analysis and comparison to RAKE baseline (both quantitative counts and qualitative semantic organization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-readable output: precision = 44% ± 12%, recall = 31% ± 11%, processing time ≈ 2.8 seconds per sentence. JSON output: precision similar (~44% ± 12%), recall ≈ 40% ± 12%, processing time ≈ 4 seconds per sentence. Aggregate extraction counts: average ~32 concepts per paper in a 300-paper sample (versus RAKE ~69).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No explicit percentage for 'extracted laws' validity because the system extracts categorized concepts and co-occurrence patterns rather than deriving new formal laws; concept extraction precision/recall reported above (precision ~44%, recall 31–40%).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Moderate extraction precision and recall; noise and concept hallucination; frequent ambiguity across domains (e.g., distinguishing named entities from generic concepts), limited relation types (co-occurrence only), reliance on titles/abstracts (not full text) which can miss details, sentence-level granularity may miss cross-sentence relationships, and remaining need for improved disambiguation and domain expert curation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to RAKE (unsupervised keyword extraction) on a 300-paper sample: RAKE produced ~69 average keywords vs. ~32 for this method (Table 2); RAKE yields more keywords but lacks semantic categorization and yields more fragmented/noisy phrases. The paper also situates itself relative to prior LLM+KG and domain-specific KG embedding approaches but does not perform a head-to-head comparison to human expert extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rapid Automatic Keyword Extraction (RAKE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised statistical keyword extraction algorithm (based on word frequency and co-occurrence) used as a baseline in this work to compare traditional keyword extraction with an LLM-powered, schema-based method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAKE (rake-nltk implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Extracts ranked keyword phrases from titles and abstracts using frequency and co-occurrence heuristics (rake-nltk library). Applied to a random 300-paper sample (100 per domain) and compared to the schema-based LLM extraction in counts, semantic organization, and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics, fluid dynamics, evolutionary biology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>300 (sample used for the RAKE vs. LLM comparison; 100 per domain)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>keyword extraction (no explicit quantitative law extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Produces ranked keyword phrases (examples listed in Appendix A.3 for sample papers), but does not distill explicit quantitative relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Unsupervised frequency and co-occurrence analysis on titles and abstracts using the rake-nltk implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Direct quantitative comparison of keyword counts and qualitative comparison of semantic usefulness versus the schema-based LLM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Average number of extracted keywords per paper (RAKE): ~69.23 across sampled domains versus ~32.12 for the LLM-based method (Table 2). RAKE yields higher counts but more fragmented/noisy outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not applicable for 'laws'; as a keyword extractor RAKE is efficient but produces substantially more noisy keywords than the schema-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lacks semantic depth and concept-role categorization, extracts fragmented phrases, and produces substantially more noise compared to the LLM-based schema method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the main baseline in this paper: RAKE gave larger flat keyword lists but lacked the structured, semantically categorized outputs of the LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (AI4Science study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced preprint that studies the impact of GPT-4 on scientific discovery tasks; cited in related work as an example of LLMs applied to scientific discovery, but no experimental details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as prior work investigating the use of a large LLM (GPT-4) for scientific discovery; the current paper references it in related work but does not report methods or results from that study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>not specified in this paper (study broadly about LLM impact on scientific discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Only cited in related work; no method or evaluation details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4199.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4199.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced 2024 preprint proposing/arguing for fully automated open-ended scientific discovery systems; cited as related work in the context of LLMs for discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as an example of research toward autonomous systems for scientific discovery; this paper references it but provides no implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>not specified here (general open-ended scientific discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Referenced only; details must be obtained from the cited preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4199.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4199.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG + LLM astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced preprint that combines knowledge graphs and LLMs in astronomy to quantify driving forces in interdisciplinary discovery; cited as related work showing use of LLMs + KG approaches to quantify relationships in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + Knowledge Graph (referenced astronomy work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as prior work combining LLMs with knowledge graph methods to quantify driving forces in interdisciplinary scientific discovery; the current paper acknowledges similarity in goals (structuring literature for analysis) but differs in being domain-agnostic and schema-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy/astrophysics (context of cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>quantifying driving/relationship forces in interdisciplinary discovery (as reported in cited work) — specifics not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Only cited; details not included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4199.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4199.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predicting research trends (Krenn & Zeilinger)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicting research trends with semantic and neural networks with an application in quantum physics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited PNAS paper that uses semantic and neural-network techniques to predict research trends; referenced as related work on using computational methods to find patterns and trends across scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predicting research trends with semantic and neural networks with an application in quantum physics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic + neural network trend prediction (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of methods that predict research trends using semantic and neural-network approaches; referenced to situate this paper among works that computationally analyze literature for patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>quantum physics (application in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>trend prediction and pattern discovery in literature (no explicit physical law extraction described here).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Referenced for context only; details not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Predicting research trends with semantic and neural networks with an application in quantum physics <em>(Rating: 2)</em></li>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4199",
    "paper_id": "paper-276961662",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "Surveyor pipeline",
            "name_full": "LLM-based structured concept extraction pipeline (this work; demo: abby101/surveyor-0)",
            "brief_description": "A domain-agnostic pipeline that uses a schema of nine concept categories and an LLM to extract categorized concepts sentence-by-sentence from paper titles/abstracts, store them in a SQL database, and build co-occurrence knowledge graphs for analyzing methodological patterns and trends across disciplines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Surveyor LLM-based structured concept extraction pipeline",
            "system_description": "Processes paper titles and abstracts sentence-by-sentence using few-shot prompting to tag text with a 9-category scientific schema (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments). Prompt engineering used 3 demonstration papers (few-shot) and a 17-paper development set; outputs can be human-readable or JSON and are stored in a SQL database (tables: papers, predictions). Extracted concepts are visualized as co-occurrence graphs (nodes = concepts, edges = co-occurrence within a paper) using a force-directed layout (d3-force). The pipeline supports SQL queries to compute aggregate statistics (e.g., modality distributions, temporal trends) and was designed to enable systematic discovery of relationships and patterns across large corpora.",
            "model_name": "Llama-3 70B Instruct",
            "model_size": "70B",
            "scientific_domain": "astrophysics, fluid dynamics, evolutionary biology",
            "number_of_papers": "29,980 (final extractions: 9,980 astrophysics + 10,000 fluid dynamics + 10,000 evolutionary biology)",
            "law_type": "co-occurrence relationships, methodological and temporal usage patterns, extraction of numeric properties (e.g., ages, velocities) and empirical/statistical relationships across papers; the system does not report automated derivation of new physical/scaling laws.",
            "law_examples": "The pipeline extracts numeric properties and quantitative descriptors from abstracts (examples present in corpus: 'age = 181 ± 25 years', 'velocity ≈ 334 km s⁻¹', 'acceleration ≈ -0.64 m s⁻²'); it supports queries to compute modality distributions and track adoption trends and to examine how similar mathematical models are applied across fields, but no new closed-form physical laws or scaling laws are reported in this paper.",
            "extraction_method": "Sentence-level few-shot prompting with manually curated demonstrations and iterative prompt engineering; outputs in human-readable or JSON formats; stored in SQL. Co-occurrence graphs built from concept co-occurrence within papers. Comparison vs. RAKE baseline performed on a 300-paper sample.",
            "validation_approach": "Development/evaluation on 20 manually annotated astrophysics papers (3 for few-shot examples, 17 for prompt development) with exact-match precision, recall and F1 used as directional metrics; qualitative analysis and comparison to RAKE baseline (both quantitative counts and qualitative semantic organization).",
            "performance_metrics": "Human-readable output: precision = 44% ± 12%, recall = 31% ± 11%, processing time ≈ 2.8 seconds per sentence. JSON output: precision similar (~44% ± 12%), recall ≈ 40% ± 12%, processing time ≈ 4 seconds per sentence. Aggregate extraction counts: average ~32 concepts per paper in a 300-paper sample (versus RAKE ~69).",
            "success_rate": "No explicit percentage for 'extracted laws' validity because the system extracts categorized concepts and co-occurrence patterns rather than deriving new formal laws; concept extraction precision/recall reported above (precision ~44%, recall 31–40%).",
            "challenges_limitations": "Moderate extraction precision and recall; noise and concept hallucination; frequent ambiguity across domains (e.g., distinguishing named entities from generic concepts), limited relation types (co-occurrence only), reliance on titles/abstracts (not full text) which can miss details, sentence-level granularity may miss cross-sentence relationships, and remaining need for improved disambiguation and domain expert curation.",
            "comparison_baseline": "Compared to RAKE (unsupervised keyword extraction) on a 300-paper sample: RAKE produced ~69 average keywords vs. ~32 for this method (Table 2); RAKE yields more keywords but lacks semantic categorization and yields more fragmented/noisy phrases. The paper also situates itself relative to prior LLM+KG and domain-specific KG embedding approaches but does not perform a head-to-head comparison to human expert extraction.",
            "uuid": "e4199.0",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAKE",
            "name_full": "Rapid Automatic Keyword Extraction (RAKE)",
            "brief_description": "An unsupervised statistical keyword extraction algorithm (based on word frequency and co-occurrence) used as a baseline in this work to compare traditional keyword extraction with an LLM-powered, schema-based method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RAKE (rake-nltk implementation)",
            "system_description": "Extracts ranked keyword phrases from titles and abstracts using frequency and co-occurrence heuristics (rake-nltk library). Applied to a random 300-paper sample (100 per domain) and compared to the schema-based LLM extraction in counts, semantic organization, and noise.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "astrophysics, fluid dynamics, evolutionary biology",
            "number_of_papers": "300 (sample used for the RAKE vs. LLM comparison; 100 per domain)",
            "law_type": "keyword extraction (no explicit quantitative law extraction)",
            "law_examples": "Produces ranked keyword phrases (examples listed in Appendix A.3 for sample papers), but does not distill explicit quantitative relationships.",
            "extraction_method": "Unsupervised frequency and co-occurrence analysis on titles and abstracts using the rake-nltk implementation.",
            "validation_approach": "Direct quantitative comparison of keyword counts and qualitative comparison of semantic usefulness versus the schema-based LLM approach.",
            "performance_metrics": "Average number of extracted keywords per paper (RAKE): ~69.23 across sampled domains versus ~32.12 for the LLM-based method (Table 2). RAKE yields higher counts but more fragmented/noisy outputs.",
            "success_rate": "Not applicable for 'laws'; as a keyword extractor RAKE is efficient but produces substantially more noisy keywords than the schema-based approach.",
            "challenges_limitations": "Lacks semantic depth and concept-role categorization, extracts fragmented phrases, and produces substantially more noise compared to the LLM-based schema method.",
            "comparison_baseline": "Used as the main baseline in this paper: RAKE gave larger flat keyword lists but lacked the structured, semantically categorized outputs of the LLM pipeline.",
            "uuid": "e4199.1",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4 (AI4Science study)",
            "name_full": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4",
            "brief_description": "A referenced preprint that studies the impact of GPT-4 on scientific discovery tasks; cited in related work as an example of LLMs applied to scientific discovery, but no experimental details are provided in this paper.",
            "citation_title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4",
            "mention_or_use": "mention",
            "system_name": "GPT-4 (referenced study)",
            "system_description": "Cited as prior work investigating the use of a large LLM (GPT-4) for scientific discovery; the current paper references it in related work but does not report methods or results from that study.",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "law_type": "not specified in this paper (study broadly about LLM impact on scientific discovery)",
            "law_examples": "",
            "extraction_method": "",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Only cited in related work; no method or evaluation details are provided in this paper.",
            "comparison_baseline": "",
            "uuid": "e4199.2",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "The AI Scientist",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "brief_description": "A referenced 2024 preprint proposing/arguing for fully automated open-ended scientific discovery systems; cited as related work in the context of LLMs for discovery.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist (referenced work)",
            "system_description": "Mentioned in related work as an example of research toward autonomous systems for scientific discovery; this paper references it but provides no implementation details.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "law_type": "not specified here (general open-ended scientific discovery)",
            "law_examples": "",
            "extraction_method": "",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Referenced only; details must be obtained from the cited preprint.",
            "comparison_baseline": "",
            "uuid": "e4199.3",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "KG + LLM astronomy",
            "name_full": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "brief_description": "A referenced preprint that combines knowledge graphs and LLMs in astronomy to quantify driving forces in interdisciplinary discovery; cited as related work showing use of LLMs + KG approaches to quantify relationships in literature.",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "LLM + Knowledge Graph (referenced astronomy work)",
            "system_description": "Cited as prior work combining LLMs with knowledge graph methods to quantify driving forces in interdisciplinary scientific discovery; the current paper acknowledges similarity in goals (structuring literature for analysis) but differs in being domain-agnostic and schema-driven.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "astronomy/astrophysics (context of cited work)",
            "number_of_papers": null,
            "law_type": "quantifying driving/relationship forces in interdisciplinary discovery (as reported in cited work) — specifics not given here.",
            "law_examples": "",
            "extraction_method": "",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Only cited; details not included in this paper.",
            "comparison_baseline": "",
            "uuid": "e4199.4",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Predicting research trends (Krenn & Zeilinger)",
            "name_full": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "brief_description": "A cited PNAS paper that uses semantic and neural-network techniques to predict research trends; referenced as related work on using computational methods to find patterns and trends across scientific literature.",
            "citation_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "mention_or_use": "mention",
            "system_name": "Semantic + neural network trend prediction (referenced)",
            "system_description": "Cited as an example of methods that predict research trends using semantic and neural-network approaches; referenced to situate this paper among works that computationally analyze literature for patterns.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "quantum physics (application in cited work)",
            "number_of_papers": null,
            "law_type": "trend prediction and pattern discovery in literature (no explicit physical law extraction described here).",
            "law_examples": "",
            "extraction_method": "",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Referenced for context only; details not provided in this manuscript.",
            "comparison_baseline": "",
            "uuid": "e4199.5",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "rating": 2,
            "sanitized_title": "knowledge_graph_in_astronomical_research_with_large_language_models_quantifying_driving_forces_in_interdisciplinary_scientific_discovery"
        },
        {
            "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "rating": 2,
            "sanitized_title": "predicting_research_trends_with_semantic_and_neural_networks_with_an_application_in_quantum_physics"
        },
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "rating": 1,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        }
    ],
    "cost": 0.01529275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 2025</p>
<p>Abhipsha Das abhipsha.das@nyu.edu 
Polymathic AI</p>
<p>Flatiron Institute</p>
<p>Nicholas Lourie 
Polymathic AI</p>
<p>New York University</p>
<p>Siavash Golkar 
Polymathic AI</p>
<p>New York University</p>
<p>Mariel Pettee 
Polymathic AI</p>
<p>Lawrence Berkeley National Laboratory</p>
<p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 202553D0D06EF33E949974E5F358664A809BarXiv:2503.09894v2[cs.CL]
The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines.Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work.Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive.Structured representations offer a natural complement-enabling systematic analysis across the whole corpus.Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs.By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole.Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts.To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology.The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge.Demo: abby101/surveyor-0 on HF Spaces.</p>
<p>Introduction</p>
<p>Consider a researcher seeking to build a multimodal foundation model for astrophysics.They might begin by asking: What are the most important data modalities to support-the most common § Video: YouTube Link ones in the field?How would such a researcher go about answering these questions today?</p>
<p>One might hope that LLMs could easily answer such a question, but while they have created unprecedented opportunities for accelerating scientific discovery, they struggle to aggregate reliable statistics and generate systematic analyses across the breadth of scientific literature.Manually reviewing papers or consulting domain experts are not scalable approaches when there are thousands of papers to investigate.</p>
<p>Most current approaches rely on unstructured methods like retrieval augmented generation (RAG) [9,11,3].While these methods excel at broad information retrieval and synthesis, they make it difficult to analyze specific patterns in research across large bodies of literature.The limitations become particularly evident when researchers try to understand how research in a field evolves.They need to track new instruments, identify research problems that require methodological innovation, and understand how theoretical models get validated across disciplines.Unstructured representations struggle to systematically capture these relationships.</p>
<p>Some efforts explore semistructured representations such as: keyphrase extractions based on statistical patterns [5] and LLM-based concept extraction combined with constructing vector-similaritybased knowledge graphs [15].While valuable, these methods typically treat all concepts uniformly without distinguishing between their functional roles in scientific work, or rely on semantic similarity of concepts using embedding models.This limits the utility of extracted knowledge when a researcher needs a quantitative analysis.</p>
<p>To address this challenge, we introduce a novel approach using LLMs that extracts categorized concepts from scientific papers using a general schema covering key research entities like objects, datasets, methods, and modalities.Our system combines structured knowledge representation with an interactive query interface to enable researchers to analyze methodological patterns, track research evolution, and understand relationships between different aspects of scientific work at scale.We visualize the extracted structured information using knowledge graphs that provide key insights into concept co-occurrence in scientific research.The main contributions of this work are: 1) a generic schema for categorizing scientific concepts across different fields, 2) a scalable LLM-based extraction pipeline to mine concepts from papers, 3) an interactive system for querying scientific information, 4) informative knowledge graphs built from the extracted concepts to represent scientific fields.</p>
<p>Method</p>
<p>Our system consists of three key components: a schema defining different kinds of scientific concepts, a pipeline for prompting LLMs to extract these concepts, and a database to store this structured knowledge for efficient queries and analysis.</p>
<p>Schema</p>
<p>Through an iterative discussion process of example selection, comparing manual annotations between the authors, and examining scientific papers across different domains, we developed an annotation schema capturing nine fundamental categories of scientific concepts: models, tasks, datasets, fields, modalities, methods, objects, properties, and instruments.In designing the schema, we aimed for categories that apply across scientific disciplines.This schema intentionally uses broad category definitions that are immediately understandable to scientists without requiring study of specialized class or type of data/observations with similar or the same structure method:</p>
<p>approach, technique or procedure to complete a task object:</p>
<p>entity that can be studied property:</p>
<p>quantitative or qualitative descriptor, or an inherent attribute of an entity, data, modality or method instrument: device or system used for making measurements Table 1: Definitions for a schema of scientific concepts.</p>
<p>and intricate taxonomies.We opted for coarser categories to avoid the ambiguity and complexity that arises when making subtle distinctions between closely related concepts, even though this means some concepts could be described by multiple tags.</p>
<p>Rather than implementing complex disambiguation tests, our simple tagging schema allows us to maintain scalability; however, there is always a trade-off between coverage and precision.</p>
<p>Original Sentence</p>
<p>We present an analysis of a new Australia Telescope Compact Array (ATCA) radiocontinuum observation of supernova remnant (SNR) G1.9+0.3, which at an age of 181±25 years is the youngest known in the Galaxy.</p>
<p>Tagged Sentence</p>
<p>We present an analysis of a new <dataset> <instrument>Australia Telescope Compact Array (ATCA)</instrument> <modality>radio-continuum</modality> observation</dataset> of <object>supernova remnant (SNR) G1.9+0.3 </object>, which at an <property>age</property> of 181±25 years is the youngest known in the <object>Galaxy</object>.</p>
<p>We implemented our extraction pipeline using the open-source Llama-3 70B Instruct model [4], employing few-shot learning to guide concept extraction.For our prompt optimization experiments, we manually annotated 20 papers, using 3 demonstration papers for few-shot examples and the remaining 17 as a development set to iteratively refine the prompts.Above is an example of the manual annotation process sentence-by-sentence.</p>
<p>The pipeline processes the language content sentence-by-sentence using manually annotated examples to demonstrate the target structure (see Fig. 2).</p>
<p>Pipeline</p>
<p>To optimize extraction reliability, we conducted systematic prompt engineering experiments on the manually annotated set, varying: (i) number and selection of few-shot examples, (ii) structure and ordering of the prompt, (iii) granularity of input text (sentence vs. paragraph), (iv) format of extracted concepts (JSON vs. human-readable).</p>
<p>To guide this iteration, we used a comprehensive set of metrics calculated on our development set, including: precision, recall and F-1 scores, for exact matches.In addition, we also considered the processing time and efficiency of the different approaches.During the annotation process, we found that even simple and broad concepts, such as a modality, encounter ambiguities when applied across scientific fields.As a result, it is likely that no method can achieve complete agreement with the development set annotations.Rather than as an absolute benchmark, we used the development set as a directional signal-a way to see if a given change improved the extraction process.Ultimately, our optimized prompt configuration consisting of instruction, schema and  from each of the 3 demonstration papers) shows promising results.The final results on our development set were: precision of 44% ± 12% and recall of 31% ± 11% in human-readable response format, with processing times averaging around 2.8 seconds per sentence.When using JSON output format, we observed similar precision levels with slightly higher recall rates of 40% ± 12% and processing times of around 4 seconds per sentence.While some noise persisted in the extracted data, the results were sufficient to explore using such structured knowledge from the scientific literature in order to discover relationships and systematically analyze complex statistical questions.</p>
<p>Database</p>
<p>The extracted concepts and their relationships are stored in a SQL database for scientific analysis queries which enables fast computation of aggregate statistics.The SQL database contains 2 tables: papers and predictions (see Fig. 1).The papers table contains metadata, raw text, author and category information about the papers, and the predictions table contains the information about the tagged concepts, the tag type and the papers they come from.Storing both the extracted information and relevant metadata about where the extractions came from facilitates analyses such as the evolution of methodological approaches over time or the adoption patterns of new experimental techniques across fields.</p>
<p>Demo</p>
<p>Visualization</p>
<p>To explore the relationships between scientific concepts, we built a dynamic visualization using force-directed graph layouts.In this representation, nodes represent individual scientific concepts V (e.g., specific methods, instruments, or objects of study), while edges represent co-occurrences within the same paper E in a graph G = (V, E).A physics-based spring layout algorithm determines the spatial arrangement, with frequently cooccurring concepts drawn closer together.</p>
<p>Our visualization system supports interactive exploration through: 1. Tag-type filtering to focus on specific concept categories (e.g., only methods or instruments), 2. Node highlighting to emphasize specific concepts and their immediate connections, 3. Depth-based exploration to reveal n-hop neighborhoods around concepts of interest, 4. Dynamic force-directed layout updates to reflect filtered subgraphs.</p>
<p>This graph-based approach enables both targeted investigation of specific concept relationships and broader analysis of methodological patterns across domains.For example, researchers can identify clusters of related experimental techniques, trace the adoption of methods across different subfields, or visualize isolated clusters of objects to understand how they are studied.</p>
<p>Query Interface</p>
<p>The query interface supports SQL queries for scientific concept exploration, with predefined queries demonstrating use cases like modality distribution analysis and temporal trends.The interface allows researchers to ask increasingly sophisticated questions by leveraging the structured database.For example, a researcher building an astrophysics foundation model could analyze most-used modalities, examine their current coverage, track usage trends over 5 years, and estimate coverage gains from adding new modalities.This approach helps scientists make data-driven research decisions that would be difficult to achieve through other means.</p>
<p>Results</p>
<p>Dataset.We collected the titles and abstracts from 30,000 articles from arXiv comprising 10,000 papers each from astrophysics, fluid dynamics and evolutionary biology, in order test across a breadth of scientific disciplines.Titles and abstracts (i.e.article metadata) were used instead of the full text in order to optimize processing efficiency while maintaining representativeness, since the titles and abstracts of papers are likely to be more information dense than the papers' bodies.After setting aside 20 astrophysics papers for prompt development, we used the optimized prompts refined through this development process to extract concepts from the remaining 9,980 astrophysics papers (from the original 10,000), as well as all 10,000 papers from each of the other two fields (fluid dynamics and evolutionary biology).Our final extractions and subsequent knowledge graph visualizations include results for these 29,980 papers.</p>
<p>Graph-based Exploration</p>
<p>Fig. 3 demonstrates the interconnected nature of scientific concepts through co-occurrence knowledge graphs from our analyzed domains.†</p>
<p>Demonstrating Example Queries</p>
<p>We examine several key questions to demonstrate the interface's capability for both exploratory research and targeted investigation in scientific discovery.</p>
<p>Related Work</p>
<p>Large language models (LLMs) [2,7,13,16,4] have recently demonstrated remarkable capabilities in language tasks, particularly through advances in prompting strategies [2,18].These advances have inspired building LLM-based pipelines to engage with complex scientific discovery tasks [8,1,10].Despite these advancements, the problem of hallucinations in LLMs persists [6].</p>
<p>To overcome this, a popular approach is to augment LLMs with unstructured, external knowledge [11,9], but while RAG excels in retrieving broad context information at scale, it is limited in providing precise information and avenues for systematic analysis, which can be efficiently realized through structured knowledge representations.Some prior work applies semistructured knowledge representations to the scientific literature, such as Gu and Krenn [5]'s SciMuse system which combines LLMs with the RAKE algorithm to extract concepts as keyword phrases.[15] extended this to astronomy, using LLMs to extract concepts from scientific texts and construct knowledge graphs, grouping the concepts with a vectorbased semantic similarity.In biomedicine, [12] integrated domain-specific language models with knowledge graph embeddings, showing improved performance but requiring field-specific finetuning.</p>
<p>Our work differs from these approaches by providing a domain-agnostic framework that combines LLM-powered semantic understanding with queryable structured knowledge curation, and enhanced by graph visualizations.Unlike previous methods, our approach introduces generalizable categorization schemes that enable cross-domain concept extraction, relationship mapping, and question answering.</p>
<p>Discussion</p>
<p>Our tool enables quantitative analysis of research methodologies across scientific domains by organizing concepts into distinct categories like methods, instruments, and data modalities, allowing researchers to systematically investigate patterns that would be difficult to discover through traditional literature review or citation analysis.The combination of SQL queries and graph visualization proves especially valuable for exploring methodological connections.For instance, answering how similar mathematical models get applied across different fields is hard with existing approaches but readily solvable by our system.</p>
<p>While the system shows promise, its current extraction precision leaves room for improvement.At times, the LLM can struggle to distinguish specific named entities (like "Melbourne wind tunnel") from generic concepts (like "wind-tunnel data"), introducing noise into the extracted relationships.Future work could address these limitations through improved prompting strategies, post-training of models by gathering insights from domain experts, and extracting more sophisticated relationships between concepts beyond co-occurrence.</p>
<p>Conclusion</p>
<p>This work demonstrates how combining LLMs with structured knowledge representation can enable systematic analysis of scientific literature.Our four key contributions are: a domain-agnostic schema for categorizing scientific concepts, a scalable LLM-based extraction pipeline, a queryable interactive system, and informative knowledge graphs built from the extracted concepts.The results show that even with modest extraction accuracy, our approach can reveal valuable insights about cross-disciplinary connections and research evolution that would be difficult to discover through traditional literature review, opening up new possibilities for navigating scientific research.support provided by the Simons Foundation and Schmidt Sciences, LLC.</p>
<p>A Comparison of Keyword Extraction Methods</p>
<p>A.1 Methodology</p>
<p>In this appendix we compare two approaches to scientific concept extraction: Rapid Automatic Keyword Extraction (RAKE) [14] and our method.RAKE is an unsupervised statistical method based on word frequency and co-occurrence, while our schema-based approach leverages Llama-3 70B for semantic understanding.We randomly sampled 300 scientific papers (100 each from astrophysics, fluid dynamics, and evolutionary biology) from our dataset and applied RAKE extractions on the title and abstract text, performed using the rake-nltk library [17].The extracted keywords were then compared to those generated by our method.Our approach consistently extracted fewer concepts than RAKE but organized them into semantic categories that reveal their functional roles within the scientific discourse.In table 3, we see that across all domains, object was the predominant concept type, followed by property.</p>
<p>A.2 Quantitative Comparison</p>
<p>A.3 Qualitative Analysis</p>
<p>A.3.1 Astrophysics</p>
<p>A Detailed Analysis of a Magnetic Island Observed by WISPR on Parker Solar Probe</p>
<p>We present the identification and physical analysis of a possible magnetic island feature seen in white-light images observed by the Wide-field Imager for Solar Probe (WISPR) on board the Parker Solar Probe (Parker).The island is imaged by WISPR during Parker's second solar encounter on 2019 April 06, when Parker was 38 solar radii from the Sun center.We report that the average velocity and acceleration of the feature are approximately 334 km s and -0.64 m s-2.The kinematics of the island feature, coupled with its direction of propagation, indicate that the island is likely entrained in the slow solar wind.The island is elliptical in shape with a density deficit in its center, suggesting the presence of a magnetic guide field.We argue that this feature is consistent with the formation of this island via reconnection in the current sheet of the streamer.The feature's aspect ratio (calculated as the ratio of its minor to major axis) evolves from an elliptical to a more circular shape that approximately doubles during its propagation through WISPR's field of view.The island is not distinct in other white-light observations from the Solar and Heliospheric Observatory (SOHO) and the Solar Terrestrial Relations Observatory (STEREO) coronagraphs, suggesting that this is a comparatively faint heliospheric feature and that viewing perspective and WISPR's enhanced sensitivity are key to observing the magnetic island.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (17.67, "possible magnetic island feature seen"), (13.71, "solar terrestrial relations observatory"), (13.33, "comparatively faint heliospheric feature"), (9.00, "2019 april 06"), (8.71, "slow solar wind"), (8.71, "second solar encounter"), (8.71, "38 solar radii"), (8.50, "light images observed"), (8.50, "approximately 334 km"), (8.33, "magnetic guide field"), (8.00, "island via reconnection"), (6.96, "parker solar probe"), (6.00, "heliospheric observatory"), (5.33, "magnetic island"), (5.21, "solar probe"), (4.50, "light observations"), (4.50, "approximately doubles"), (4.33, "island feature"), (4.00, "viewing perspective"), (4.00, "physical analysis"), (4.00, "major axis"), (4.00, "likely entrained"), (4.00, "field imager"), (4.00, "enhanced sensitivity"), (4.00, "density deficit"), (4.00, "current sheet"), (4.00, "average velocity"), (3.75, "parker ()"), (3.50, "sun center"), (3.50, "circular shape"), (3.50, "aspect ratio"), (2.71, "solar"), (2.33, "feature") * 3, (2.00, "island") * 4, (2.00, "field"), (1.75, "parker") * 2, (1.50, "shape"), (1.50, "ratio"), (1.50, "center"), (1.00, "wispr") * 4, (1.00, "wide"), (1.00, "white") * 2, (1.00, "view"), (1.00, "suggesting") * 2, (1.00, "streamer"), (1.00, "stereo"), (1.00, "soho"), (1.00, "report"), (1.00, "propagation") * 2, (1.00, "present"), (1.00, "presence"), (1.00, "observing"), (1.00, "minor"), (1.00, "kinematics"), (1.00, "key"), (1.00, "indicate"), (1.00, "imaged"), (1.00, "identification"), (1.00, "formation"), (1.00, "evolves"), (1.00, "elliptical") * 2, (1.00, "distinct"), (1.00, "direction"), (1.00, "coupled"), (1.00, "coronagraphs"), (1.00, "consistent"), (1.00, "calculated"), (1.00, "board"), (1.00, "argue"), (1.00, "acceleration"), (1.00, "64"), (1.00, "2"), (1.00, "0")</p>
<p>Concepts (by type):</p>
<p>• object: Parker Solar Probe, magnetic island feature, white-light images, island, Parker, Sun, feature, island feature, slow solar wind, streamer, WISPR's field of view, magnetic island, heliospheric feature</p>
<p>• instrument: WISPR, Wide-field Imager for Solar Probe (WISPR), Parker Solar Probe (Parker), Solar and Heliospheric Observatory (SOHO), Solar Terrestrial Relations Observatory (STEREO)</p>
<p>• property: solar radii, velocity, acceleration, density deficit, shape, aspect ratio, faint</p>
<p>The RAKE extraction produces a flat list of keywords with associated scores.In contrast, our approach organizes concepts by semantic role, such as instruments (WISPR, SOHO), objects (Sun, slow solar wind), and their properties (solar radii, acceleration).</p>
<p>A.3.2 Fluid Dynamics</p>
<p>Approximation of sea surface velocity field by fitting surrogate two-dimensional flow to scattered measurements</p>
<p>In this paper, a rapid approximation method is introduced to estimate the sea surface velocity field based on scattered measurements.The method uses a simplified two-dimensional flow model as a surrogate model, which mimics the real submesoscale flow.The proposed approach treats the interpolation of the flow velocities as an optimization problem, aiming to fit the flow model to the scattered measurements.To ensure consistency between the simulated velocity field and the measured values, the boundary conditions in the numerical simulations are adjusted during the optimization process.Additionally, the relevance of quantity and quality of the scattered measurements is assessed, emphasizing the importance of the measurement locations within the domain as well as explaining how these measurements contribute to the accuracy and reliability of the sea surface velocity field approximation.The proposed methodology has been successfully tested in both synthetic and real-world scenarios, leveraging measurements obtained from Global Positioning System (GPS) drifters and high-frequency (HF) radar systems.The adaptability of this approach for different domains, measurement types, and conditions implies that it is suitable for real-world submesoscale scenarios where only an approximation of the sea surface velocity field is sufficient.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (22.83, "sea surface velocity field based"), (20.83, "sea surface velocity field approximation"), (17.83, "sea surface velocity field"), (11.50, "simulated velocity field"), (9.00, "global positioning system"), (8.50, "rapid approximation method"), (8.50, "measurement locations within"), (8.20, "leveraging measurements obtained"), (8.00, "world submesoscale scenarios"), (7.83, "dimensional flow model"), (7.50, "proposed approach treats"), (7.17, "real submesoscale flow"), (5.00, "world scenarios"), (4.83, "flow model"), (4.50, "proposed methodology"), (4.50, "method uses"), (4.50, "measurement types"), (4.50, "flow velocities"), (4.33, "surrogate model"), (4.20, "scattered measurements") * 3, (4.20, "measurements contribute"), (4.00, "successfully tested"), (4.00, "simplified two"), (4.00, "radar systems"), (4.00, "optimization process"), (4.00, "optimization problem"), (4.00, "numerical simulations"), (4.00, "measured values"), (4.00, "ensure consistency"), (4.00, "different domains"), (4.00, "conditions implies"), (4.00, "boundary conditions"), (3.00, "approximation"), (2.00, "approach"), (1.67, "real") * 2, (1.00, "well"), (1.00, "synthetic"), (1.00, "suitable"), (1.00, "sufficient"), (1.00, "reliability"), (1.00, "relevance"), (1.00, "quantity"), (1.00, "quality"), (1.00, "paper"), (1.00, "mimics"), (1.00, "introduced"), (1.00, "interpolation"), (1.00, "importance"), (1.00, "high"), (1.00, "hf"), (1.00, "gps"), (1.00, "frequency"), (1.00, "fit"), (1.00, "explaining"), (1.00, "estimate"), (1.00, "emphasizing"), (1.00, "drifters"), (1.00, "domain"), (1.00, "assessed"), (1.00, "aiming"), (1.00, "adjusted"), (1.00, "additionally"), (1.00, "adaptability"), (1.00, "accuracy") Concepts (by type): Our method's extraction effectively distinguishes between physical objects of study (porous media), models (two-dimensional flow model), methodological approaches (rapid approximation method), and instruments (Global Positioning System (GPS) drifters).</p>
<p>A.3.3 Evolutionary Biology</p>
<p>Complexity-stability relationships in disordered dynamical systems</p>
<p>Robert May famously used random matrix theory to predict that large, complex systems cannot admit stable fixed points.However, this general conclusion is not always supported by empirical observation: from cells to biomes, biological systems are large, complex and, often, stable.In this paper, we revisit May's argument in light of recent developments in both ecology and random matrix theory.Using a non-linear generalization of the competitive Lotka-Volterra model, we show that there are, in fact, two kinds of complexity-stability relationships in disordered dynamical systems: if self-interactions grow faster with density than cross-interactions, complexity is destabilizing; but if cross-interactions grow faster than self-interactions, complexity is stabilizing.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (40.50, "robert may famously used random matrix theory"), (40.00, "complex systems cannot admit stable fixed points"), (15.00, "random matrix theory"), (10.00, "disordered dynamical systems"), (8.00, "interactions grow faster") * 2, (6.50, "revisit may"), (6.00, "biological systems"), (4.00, "volterra model"), (4.00, "two kinds"), (4.00, "stable"), (4.00, "stability relationships"), (4.00, "recent developments"), (4.00, "linear generalization"), (4.00, "general conclusion"), (4.00, "empirical observation"), (4.00, "complex"), (4.00, "competitive lotka"), (4.00, "always supported"), (2.00, "interactions") * 2, (1.00, "using"), (1.00, "stabilizing"), (1.00, "show"), (1.00, "self") * 2, (1.00, "predict"), (1.00, "paper"), (1.00, "often"), (1.00, "non"), (1.00, "light"), (1.00, "large") * 2, (1.00, "however"), (1.00, "fact"), (1.00, "ecology"), (1.00, "destabilizing"), (1.00, "density"), (1.00, "cross") * 2, (1.00, "complexity") * 3, (1.00, "cells"), (1.00, "biomes"), (1.00, "argument")</p>
<p>Concepts (by type):</p>
<p>• object: disordered dynamical systems, systems, cells, biomes, biological systems</p>
<p>• property: complexity-stability relationships, large, complex, stable, complexity-stability, density</p>
<p>• method: random matrix theory</p>
<p>• field: ecology</p>
<p>• model: non-linear generalization of the competitive Lotka-Volterra model Our method's extraction captures biological entities (cells, biomes, systems) and their properties, while also identifying the specific models and methods used.</p>
<p>A.4 Insights From The Comparison</p>
<p>RAKE is efficient, language-independent, and quantitatively ranks keyword importance, but it lacks semantic depth, extracts fragmented phrases, and does not distinguish between concept types.Our method, while computationally more demanding and occasionally prone to concept hallucination, provides structured semantic categorization, generates coherent concepts, and captures domainspecific nuances more effectively.</p>
<p>Both methods introduce some noise, though RAKE produces significantly more.The structured semantic representation in our approach offers a more meaningful and organized summary compared to RAKE's flat keyword list, making it more useful for domain experts.</p>
<p>"Figure 1 :
1
Figure 1: Illustration of the structured concept extraction pipeline: i) the corpus used, ii) running optimized prompt on full corpus, iii) storing model's outputs and corpus metadata in SQL database.</p>
<p>9 few-shot examples (3 sentences with annotated extractions Illustration: Prefix + Prompt The following schema is provided to tag the title and abstract of a given scientific paper as shown in the examples: $SCHEMA Sentence: This magnetic field strength implies a minimum total energy of the synchrotron radiation of E min ≈ 1.8×10 48 ergs.Extractions: property: magnetic field strength, energy object: synchrotron radiation ... (Total 9 few-shot examples) ... Sentence: We present HATNet observations of XO-5b, confirming its planetary nature based on evidence beyond that described in the announcement of Burke et al Ground Truth Tags: dataset: HATNet observations instrument: HATNet object: XO-5b Predicted Tags: dataset: HATNet observations object: XO-5b, planetary nature</p>
<p>Figure 2 :
2
Figure 2: Expanded prompt illustration with schema and few-shot examples, along with the sentence to predict tags for.</p>
<p>(a) Analysis of galaxy images: various galactic entities, measurement objects, clouds, instruments, and properties.(b) COVID-19 cluster analysis: geographic distribution of pandemic research and immune health terms indicating research focus areas.(c) Fluid dynamics clusters centered around "flow," branching into related flow and turbulence-based physical phenomena.(d) Astrophysical objects and phenomena spanning multiple scales, from individual stars and binaries to galactic structures and clusters.</p>
<p>Figure 3 :
3
Figure 3: Co-occurrence graphs: astrophysics (a, d), epidemiology (b), fluid dynamics (c).</p>
<p>Table 2
2
reveals significant differences in the average count of extracted concepts by each method across domains for the subset of 300 papers.
DomainRAKE Ours Ratio(avg)(avg) (avg)Astrophysics69.533.82.05Fluid Dynamics70.332.62.15Evolutionary Biology67.729.72.28Overall69.232.12.16</p>
<p>Table 2 :
2
Average number of extracted concepts by domain
DomainOur method: concept typesAstrophysicsobject (56.1%), property (24.7%),instrument (5.4%), method (3.6%),modality (3.5%), model (3.1%), task(1.5%), field (1.0%), dataset (0.6%)Fluid Dynamicsobject (44.7%), property (31.0%),method (10.0%), model (5.3%), modal-ity (3.3%), field (1.9%), task (1.9%),instrument (1.2%), dataset (0.5%)Evolutionary Biology object (51.2%), property (25.7%),model (9.2%), method (5.4%), task(3.3%), field (2.5%), modality (1.7%),dataset (0.4%), instrument (0.2%)</p>
<p>Table 3 :
3
Distribution of concept types by domain</p>
<p>†  We use the d3-force library; see documentation for more information on the spring-layout implementation.
AcknowledgmentsThe computations in this work were, in part, run at facilities supported by the Scientific Computing Core at the Flatiron Institute, a division of the Simons Foundation.Polymathic AI acknowledgeshttps://github.com/chiral-carbon/kgfor-science.§
arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. Preprint. </p>
<p>others. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, arXiv:2005.14165Language Models are Few-Shot Learners. 202012Preprint</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024Preprint</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.21783The Llama 3 Herd of Models. 2024Preprint</p>
<p>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders. Xuemei Gu, Mario Krenn, arXiv:2405.170442024Preprint</p>
<p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.1145/3703155ACM Transactions on Information Systems. 2024</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, 10.1073/pnas.1914370116Proceedings of the National Academy of Sciences. 11742020</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024Preprint</p>
<p>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023Preprint</p>
<p>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021Preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Anadkat, arXiv:2303.08774Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff BelgumPreprintand 262 others. 2024. GPT-4</p>
<p>Automatic keyword extraction from individual documents. Stuart J Rose, Dave W Engel, Nick Cramer, Wendy Cowley, 2010</p>
<p>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery. Zechang Sun, Yuan-Sen, Yaobo Ting, Nan Liang, Song Duan, Zheng Huang, Cai, arXiv:2406.013912024Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Wenyin Fu, and 49 others. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint</p>
<p>rake-nltk: Python implementation of the rapid automatic keyword extraction algorithm using nltk. B V Vishwas, 2017</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
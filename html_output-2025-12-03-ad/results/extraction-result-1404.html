<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1404 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1404</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1404</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51" target="_blank">Efficiently Modeling Long Sequences with Structured State Spaces</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.</p>
                <p><strong>Paper Abstract:</strong> A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1404.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1404.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State Space Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-time latent-state linear dynamical model x'(t)=Ax(t)+Bu(t), y(t)=Cx(t)+Du(t) used as the fundamental operator mapping input signals to outputs via a learned latent state; treated here as a trainable module inside deep sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>State Space Model (SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Linear continuous-time latent-state model parameterized by matrices A, B, C, (D). In the paper SSMs are used as trainable modules in deep sequence networks and may be viewed in three equivalent representations: continuous-time ODE, discrete-time recurrence (via bilinear/bilinear discretization), and a convolutional view (SSM convolution kernel K). The SSM represents the world/state by an N-dimensional continuous latent vector x(t) that evolves linearly and is projected to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent continuous-time linear dynamical model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general sequence modeling (images flattened to sequences, raw audio, language, time-series forecasting, classification, generative modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task performance metrics (accuracy, perplexity, bits-per-dim), ability to capture long-range dependencies (empirical accuracy on LRA tasks), and numerical stability of matrix transforms; indirectly measured by downstream loss / predictive likelihood and benchmark accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Varies by instantiation; with HiPPO-structured A matrices SSMs can memorize long history and yield high accuracy on long-range tasks (e.g., when used in S4, see S4-specific metrics). Standalone naive SSMs without HiPPO perform poorly (example: random A gave poor generalization in CIFAR-10 ablations). No single MSE/Metrics for a generic SSM given.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: the latent state x(t) is a linear dynamical system so its modes/eigenstructure correspond to temporal basis functions; convolutional kernel K (the impulse response) is explicitly computable and can be visualized (the paper visualizes K reshaped to images). But A, B, C learned as matrices are generally black-box unless structured (e.g., HiPPO) or visualized.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of learned convolutional kernels (SSM filter K), analysis of matrix structure (eigen-decomposition, conjugation), and theoretical analysis (HiPPO provides principled matrices with known memory properties).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Naive discrete-time recurrence: O(N^2 L) operations and O(N L) memory to compute the full kernel; training via convolutional view requires computing K (the SSM filter) which is non-trivial. Costs depend on parameterization and algorithm used.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Naive SSM implementations (LSSL) are orders of magnitude more expensive than optimized S4 parameterization; prior naive LSSL requires O(N^2 L) operations whereas the S4 parameterization reduces this to ~Õ(N+L) (see S4 entry).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When combined with HiPPO structure and S4 parameterization, SSM-based layers achieve strong performance across tasks (see S4 numbers). Unstructured/trained-from-random SSMs achieve perfect training accuracy but much worse validation/generalization on sequential CIFAR-10 in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SSMs are especially useful for long-range-dependency (LRD) tasks because their continuous-time formulation with appropriate A (HiPPO) mathematically enables memorization of input history; however, their raw computational representation can make them impractical unless reparameterized (S4). Task-relevant performance depends critically on A initialization/structure and algorithmic parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High theoretical fidelity for LRD requires structured A (HiPPO) but naïve use yields prohibitive compute/memory; diagonalization of A is numerically unstable for HiPPO (very ill-conditioned eigenvectors), so there is a tradeoff between choosing expressive A and maintaining numeric/compute efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of continuous-time SSM formulation, choice of discretization (bilinear method), and whether A is structured (HiPPO) vs random vs diagonal; parameterization choices (NPLR/DPLR) dramatically affect efficiency and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to RNN/CNN/Transformer families, SSMs conceptually unify continuous-time, recurrent, and convolutional views; without special parameterizations they are less practical (compute/memory) than CNNs/RNNs/Transformers, but with HiPPO + S4 they can surpass many baselines on LRD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using HiPPO-structured A matrices (to enable long memory) combined with the NPLR/DPLR parameterization and computing the filter via the frequency-space generating function (Woodbury correction and Cauchy kernel evaluation) to balance fidelity and efficiency; specific optimal hyperparameters (N, Δ) depend on task and are not universally prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficiently Modeling Long Sequences with Structured State Spaces', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1404.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1404.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear State Space Layer (LSSL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior instantiation of deep SSMs that used HiPPO-structured A matrices to achieve strong accuracy on LRD tasks but with prohibitive computation and memory cost due to naive state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Linear State Space Layer (LSSL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deep-layer implementation of SSMs that directly uses HiPPO-structured A matrices and computes the discrete-time recurrence / convolution, demonstrating that structured SSMs can capture long-range dependencies when trained, but implemented with a naive expansion leading to high computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent linear state-space layer (deep SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>sequence modeling tasks emphasizing long-range dependencies (e.g., sequential MNIST, sCIFAR, LRA tasks, speech/data used in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task accuracy / loss (e.g., classification accuracy on sequential CIFAR, LRA accuracy), empirical ability to memorize long history; numerical condition of intermediate linear algebra (stability of polynomial inverses/eigen-decompositions) used to assess feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Shown in prior work to improve performance dramatically on some tasks (e.g., sequential MNIST improved from ~60% to ~98% when switching to HiPPO A), but specific numbers depend on task and setting; in this paper LSSL serves as baseline and matches earlier reported performance but with worse resource usage.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Similar to SSM: interpretable to the extent A is structured (HiPPO gives theoretical memory properties); however, LSSL's large intermediate matrices and unstable diagonalizations make inspection and stable interpretation difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Theoretical analysis of HiPPO matrices and their eigenstructure; but their diagonalization is numerically infeasible (entries exponentially large), limiting practical interpretability via eigenvectors.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Naive LSSL: O(N^2 L) operations and O(N L) memory. Empirically in Table 2: for state dim 256, training step time 20.6 ms and memory alloc ~1685 MB (examples from paper).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>S4 is substantially more efficient: paper reports S4 being up to 30x faster and up to 400x less memory usage compared to LSSL (Table 2 shows e.g., for dim 512 LSSL step 140.7 ms vs S4 4.75 ms; memory 1685 MB vs S4 12.6 MB at dim 256 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>LSSL demonstrated strong accuracy on several LRD tasks historically (e.g., sequential MNIST) but was impractical for large N or long L due to resource use; in this paper it's used as an expensive baseline and S4 exceeds or matches its empirical performance while being far more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>LSSL shows that SSMs with HiPPO matrices have high task utility for LRDs in principle, but its computational inefficiency prevents wide applicability; thus high-fidelity representations did not translate to practical task utility until S4 addressed computational bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>LSSL achieves strong fidelity for LRDs but at the cost of quadratic (in N) computational operations and linear-in-N times sequence-length memory; attempts to accelerate (diagonalization or polynomial tricks) are numerically unstable due to ill-conditioned matrices, so there is a tradeoff between theoretical speed-ups and numerical stability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Directly use HiPPO A in discrete-time SSM and expand powers of A to compute kernel K (naive powering); no NPLR parameterization; uses bilinear discretization but lacks stable conjugation/diagonalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to S4, LSSL is less efficient (orders of magnitude slower and more memory) though conceptually similar; compared to Transformers/CNNs/RNNs it is less practical at scale due to computational/memory overheads.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper argues that the LSSL is not a practical optimal configuration for large-scale sequence modeling; instead the recommended configuration is the S4 parameterization (HiPPO initialization + NPLR/DPLR + frequency-space kernel computation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficiently Modeling Long Sequences with Structured State Spaces', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1404.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1404.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured State Space sequence model (S4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new parameterization and algorithm for SSMs that decomposes A into a Normal plus Low-Rank (NPLR) term, computes the truncated generating function in frequency space, uses Woodbury corrections, and reduces kernel computation to Cauchy kernel evaluations to achieve near-linear time and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S4 (Structured State Space sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An SSM-based deep sequence layer where the continuous-time state matrix A is parameterized as A = Lambda - P Q* (unitary-conjugate DPLR form). Key algorithmic moves: (1) conjugate HiPPO matrices into diagonal-plus-low-rank (DPLR) via unitary V, (2) compute truncated generating function in frequency domain evaluated at roots of unity, (3) apply Woodbury identity to correct low-rank part, reducing core computation to Cauchy kernel evaluations, and (4) obtain time-domain kernel via iFFT. The model is used in deep stacks (H features) and combined with pointwise mixing and nonlinearities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent continuous-time state-space model with structured (NPLR/DPLR) parameterization; neural sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-range sequence tasks: Long Range Arena (text, retrieval, image, pathfinder, Path-X), raw speech classification (SC10), sequential CIFAR-10 and sMNIST/pMNIST, autoregressive generative modeling (CIFAR-10 density), language modeling (WikiText-103), time-series forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Standard downstream metrics for tasks: classification accuracy (%), perplexity (ppl) for language, bits-per-dim (bpd) for density estimation, MSE/MAE for forecasting; also benchmark-specific accuracy (e.g., LRA tasks). Numerical stability of algorithms (conditioning) also used as a fidelity proxy for numerical correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported strong empirical performance: LRA average 86.09% (Table 4); Path‑X 96.35% (solves length‑16384 task); SC10 raw speech accuracy 98.32% (0.5x sampling: 96.30%); sequential CIFAR sCIFAR 91.13% (Table 6); CIFAR density estimation bpd 2.85 (S4 large), generation speed 20.84 images/sec (~65.1x faster than Transformer baseline); WikiText-103 perplexity 20.95 (tokens/sec 48K, ~60x faster generation than standard Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: the convolutional impulse response K is explicit and visualizable; the paper visualizes K reshaped into images showing interpretable spatial filters (lower-layer local filters, higher-layer global column-aggregating filters). The underlying parameterization (NPLR) gives structural insight into the system dynamics (diagonalizable normal part plus low-rank adjustments).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of SSM convolution kernels (filter K), inspection of learned filters across layers, analysis of NPLR decomposition and spectral properties, examination of HiPPO-informed initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Asymptotically Õ(N+L) operations and O(N+L) memory for computing kernel via Cauchy multiplies (Theorem 3); recurrent step O(N) operations (Theorem 2). Empirical timings: Table 2 shows S4 training step ms (dim 256: 3.07 ms) and memory allocations (dim 256: 12.6 MB). Generation throughput: CIFAR images/sec 20.84 (65x faster than Transformer baseline at 0.32 images/sec); WikiText tokens/sec 48K (60x faster than Transformer baseline 0.8K).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to naive LSSL: up to ~30x faster and up to ~400x less memory in reported experiments (Table 2). Compared to Transformers and efficient Transformer variants, S4 is competitive or faster in speed and much more memory efficient on long sequences (Table 3 reports S4 speed 1.58x vs Transformer at length 1024 and 5.19x at length 4096, with memory fractions reported).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>State-of-the-art on Long Range Arena tasks in reported comparisons (Table 4), solved Path-X (length 16384) with 96.35% accuracy; sCIFAR 91.13% sequential classification; SC10 raw speech 98.32% accuracy; CIFAR autoregressive density estimation competitive (2.85 bpd) and much faster generation; WikiText-103 perplexity 20.95 (close to Transformer 20.51).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity (via HiPPO initialization) combined with S4's efficient algorithms translates into strong task performance across diverse domains requiring long-range reasoning; S4's efficient recurrent view provides orders-of-magnitude faster autoregressive generation while maintaining competitive predictive performance, showing practical utility of the world-model-like SSM for both discriminative and generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>S4 trades some modeling simplicity for algorithmic complexity: the NPLR/DPLR parameterization and frequency-domain kernel computation add algorithmic machinery but achieve large efficiency gains. There remain domains (some language modeling settings) where Transformers still slightly outperform S4 in perplexity, indicating a fidelity vs inductive-bias tradeoff. Numerical stability issues can occur when eigenvalues are in right half-plane; follow-up fix to parameterization (Λ - P P*) mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices: initialize A using HiPPO matrices (to enable long memory), represent A as Normal plus Low-Rank (NPLR) and conjugate to DPLR, compute truncated generating function in frequency space and evaluate at roots of unity, apply Woodbury identity to handle low-rank correction, reduce to Cauchy kernel evaluations and iFFT to obtain time-domain K, broadcast across H features with position-wise mixing. Optionally replace Q*P by P P* for numeric stability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Against LSSL: far more computationally efficient while matching or exceeding accuracy. Against Transformers and efficient Transformer variants: S4 outperforms them on LRA long-range tasks and is much faster at autoregressive generation (≈60x) though Transformers sometimes still have an edge on certain language-model perplexity metrics. Against CNN/RNN baselines on speech and sequential CIFAR, S4 matches or outperforms specialized architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends HiPPO initialization for A plus the NPLR/DPLR parameterization and frequency-domain kernel computation as the practical optimal configuration for balancing fidelity (long-range memory), interpretability (explicit kernel), computational efficiency (Õ(N+L)), and task utility; also notes a minor parameterization change (Λ - P P*) to avoid instabilities when eigenvalues lie in the right half-plane.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficiently Modeling Long Sequences with Structured State Spaces', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1404.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1404.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiPPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiPPO (High-order Polynomial Projection Operators) matrices / framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical framework that prescribes special continuous-time state matrices A (HiPPO matrices) that enable continuous-time memorization by projecting input histories into polynomial bases, giving SSMs the ability to capture long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hippo: Recurrent memory with optimal polynomial projections</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HiPPO-structured SSM (HiPPO matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A class of specially constructed A matrices derived from continuous-time memorization theory; when used to parameterize SSMs, they produce states that optimally encode recent input history in orthogonal polynomial bases (Legendre/Laguerre variants). HiPPO defines specific dense, structured A matrices (e.g., HiPPO-LegT/LegS/LagT) with provable memory properties.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>structured operator / parameterization for latent dynamical models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>long-range sequence modeling, memory tasks (sequential MNIST, LRA tasks), speech/time-series where long temporal context matters</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task performance and formal continuous-time memorization guarantees (theoretical properties of basis projections); empirically measured via improved validation accuracy on long-range tasks when used as initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Empirically, replacing random A with HiPPO A improved sequential MNIST from ~60% to ~98% in prior work; in this paper, HiPPO initialization is shown essential for S4's generalization (ablation: HiPPO-initialized SSMs generalize much better than random initializations). Exact numerical gains depend on task.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High theoretical interpretability: matrices correspond to projections onto polynomial bases (Legendre, Laguerre) and have known analytic structure. However, direct eigen-decomposition is numerically ill-conditioned (eigenvector matrix entries exponentially large), limiting naive spectral interpretability in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Mathematical analysis of the matrix form and its projection properties; analytic derivation of eigenvectors/eigenvalues (paper shows explicit diagonalization with combinatorial entries), and leveraging that HiPPO matrices are NPLR (normal plus low-rank) enabling structured conjugation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>HiPPO matrices themselves are dense and when used naively incur the same O(N^2 L) or O(N L) memory costs as general dense A; the paper shows they can be represented as NPLR (rank r=1 or 2), which S4 exploits to reduce computational cost to near-linear.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Naive handling of HiPPO A makes prior SSM implementations infeasible; S4's NPLR parameterization demonstrates that HiPPO matrices can be handled efficiently by transforming them into unitary-diagonal-plus-low-rank form and using Woodbury/Cauchy algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Critical enabler for high performance on LRD tasks: ablation experiments in paper show HiPPO initialization substantially improves validation accuracy versus random or diagonal initializations; HiPPO+S4 achieves SoTA across LRA and other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>HiPPO provides the principled inductive bias (optimal polynomial memory) necessary for SSMs to capture long histories; however, HiPPO's dense structure requires algorithmic work (NPLR/DPLR) to be practically useful.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>HiPPO gives strong theoretical memory/fidelity but is numerically delicate (diagonalization yields exponentially large factors) and computationally expensive unless expressed in NPLR form; tradeoff between theoretical desirability and practical numeric/compute constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use HiPPO matrices as initialization for A to inject long-range-memory inductive bias; combine with NPLR parameterization and stable conjugation (unitary V) to make practical.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Better principled long-memory behavior than random A or naive diagonal A initializations; diagonal A is computationally cheaper but empirically underperforms in generalization compared to HiPPO-initialized SSMs in the ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends HiPPO initialization together with S4's NPLR/DPLR parameterization (unitary conjugation, low-rank correction) as the practical configuration to achieve both the memorization properties and computational tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficiently Modeling Long Sequences with Structured State Spaces', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer <em>(Rating: 2)</em></li>
                <li>Hippo: Recurrent memory with optimal polynomial projections <em>(Rating: 2)</em></li>
                <li>Legendre memory units: Continuous-time representation in recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Parallelizing legendre memory unit training <em>(Rating: 1)</em></li>
                <li>Fast approximate computations with cauchy matrices and polynomials <em>(Rating: 1)</em></li>
                <li>It's raw! audio generation with state-space models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1404",
    "paper_id": "paper-ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "SSM",
            "name_full": "State Space Model",
            "brief_description": "A continuous-time latent-state linear dynamical model x'(t)=Ax(t)+Bu(t), y(t)=Cx(t)+Du(t) used as the fundamental operator mapping input signals to outputs via a learned latent state; treated here as a trainable module inside deep sequence models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "State Space Model (SSM)",
            "model_description": "Linear continuous-time latent-state model parameterized by matrices A, B, C, (D). In the paper SSMs are used as trainable modules in deep sequence networks and may be viewed in three equivalent representations: continuous-time ODE, discrete-time recurrence (via bilinear/bilinear discretization), and a convolutional view (SSM convolution kernel K). The SSM represents the world/state by an N-dimensional continuous latent vector x(t) that evolves linearly and is projected to outputs.",
            "model_type": "latent continuous-time linear dynamical model",
            "task_domain": "general sequence modeling (images flattened to sequences, raw audio, language, time-series forecasting, classification, generative modeling)",
            "fidelity_metric": "Task performance metrics (accuracy, perplexity, bits-per-dim), ability to capture long-range dependencies (empirical accuracy on LRA tasks), and numerical stability of matrix transforms; indirectly measured by downstream loss / predictive likelihood and benchmark accuracies",
            "fidelity_performance": "Varies by instantiation; with HiPPO-structured A matrices SSMs can memorize long history and yield high accuracy on long-range tasks (e.g., when used in S4, see S4-specific metrics). Standalone naive SSMs without HiPPO perform poorly (example: random A gave poor generalization in CIFAR-10 ablations). No single MSE/Metrics for a generic SSM given.",
            "interpretability_assessment": "Moderately interpretable: the latent state x(t) is a linear dynamical system so its modes/eigenstructure correspond to temporal basis functions; convolutional kernel K (the impulse response) is explicitly computable and can be visualized (the paper visualizes K reshaped to images). But A, B, C learned as matrices are generally black-box unless structured (e.g., HiPPO) or visualized.",
            "interpretability_method": "Visualization of learned convolutional kernels (SSM filter K), analysis of matrix structure (eigen-decomposition, conjugation), and theoretical analysis (HiPPO provides principled matrices with known memory properties).",
            "computational_cost": "Naive discrete-time recurrence: O(N^2 L) operations and O(N L) memory to compute the full kernel; training via convolutional view requires computing K (the SSM filter) which is non-trivial. Costs depend on parameterization and algorithm used.",
            "efficiency_comparison": "Naive SSM implementations (LSSL) are orders of magnitude more expensive than optimized S4 parameterization; prior naive LSSL requires O(N^2 L) operations whereas the S4 parameterization reduces this to ~Õ(N+L) (see S4 entry).",
            "task_performance": "When combined with HiPPO structure and S4 parameterization, SSM-based layers achieve strong performance across tasks (see S4 numbers). Unstructured/trained-from-random SSMs achieve perfect training accuracy but much worse validation/generalization on sequential CIFAR-10 in ablations.",
            "task_utility_analysis": "SSMs are especially useful for long-range-dependency (LRD) tasks because their continuous-time formulation with appropriate A (HiPPO) mathematically enables memorization of input history; however, their raw computational representation can make them impractical unless reparameterized (S4). Task-relevant performance depends critically on A initialization/structure and algorithmic parameterization.",
            "tradeoffs_observed": "High theoretical fidelity for LRD requires structured A (HiPPO) but naïve use yields prohibitive compute/memory; diagonalization of A is numerically unstable for HiPPO (very ill-conditioned eigenvectors), so there is a tradeoff between choosing expressive A and maintaining numeric/compute efficiency.",
            "design_choices": "Use of continuous-time SSM formulation, choice of discretization (bilinear method), and whether A is structured (HiPPO) vs random vs diagonal; parameterization choices (NPLR/DPLR) dramatically affect efficiency and stability.",
            "comparison_to_alternatives": "Compared to RNN/CNN/Transformer families, SSMs conceptually unify continuous-time, recurrent, and convolutional views; without special parameterizations they are less practical (compute/memory) than CNNs/RNNs/Transformers, but with HiPPO + S4 they can surpass many baselines on LRD tasks.",
            "optimal_configuration": "Paper recommends using HiPPO-structured A matrices (to enable long memory) combined with the NPLR/DPLR parameterization and computing the filter via the frequency-space generating function (Woodbury correction and Cauchy kernel evaluation) to balance fidelity and efficiency; specific optimal hyperparameters (N, Δ) depend on task and are not universally prescribed.",
            "uuid": "e1404.0",
            "source_info": {
                "paper_title": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "LSSL",
            "name_full": "Linear State Space Layer (LSSL)",
            "brief_description": "A prior instantiation of deep SSMs that used HiPPO-structured A matrices to achieve strong accuracy on LRD tasks but with prohibitive computation and memory cost due to naive state representations.",
            "citation_title": "Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer",
            "mention_or_use": "use",
            "model_name": "Linear State Space Layer (LSSL)",
            "model_description": "A deep-layer implementation of SSMs that directly uses HiPPO-structured A matrices and computes the discrete-time recurrence / convolution, demonstrating that structured SSMs can capture long-range dependencies when trained, but implemented with a naive expansion leading to high computational cost.",
            "model_type": "latent linear state-space layer (deep SSM)",
            "task_domain": "sequence modeling tasks emphasizing long-range dependencies (e.g., sequential MNIST, sCIFAR, LRA tasks, speech/data used in comparisons)",
            "fidelity_metric": "Downstream task accuracy / loss (e.g., classification accuracy on sequential CIFAR, LRA accuracy), empirical ability to memorize long history; numerical condition of intermediate linear algebra (stability of polynomial inverses/eigen-decompositions) used to assess feasibility.",
            "fidelity_performance": "Shown in prior work to improve performance dramatically on some tasks (e.g., sequential MNIST improved from ~60% to ~98% when switching to HiPPO A), but specific numbers depend on task and setting; in this paper LSSL serves as baseline and matches earlier reported performance but with worse resource usage.",
            "interpretability_assessment": "Similar to SSM: interpretable to the extent A is structured (HiPPO gives theoretical memory properties); however, LSSL's large intermediate matrices and unstable diagonalizations make inspection and stable interpretation difficult.",
            "interpretability_method": "Theoretical analysis of HiPPO matrices and their eigenstructure; but their diagonalization is numerically infeasible (entries exponentially large), limiting practical interpretability via eigenvectors.",
            "computational_cost": "Naive LSSL: O(N^2 L) operations and O(N L) memory. Empirically in Table 2: for state dim 256, training step time 20.6 ms and memory alloc ~1685 MB (examples from paper).",
            "efficiency_comparison": "S4 is substantially more efficient: paper reports S4 being up to 30x faster and up to 400x less memory usage compared to LSSL (Table 2 shows e.g., for dim 512 LSSL step 140.7 ms vs S4 4.75 ms; memory 1685 MB vs S4 12.6 MB at dim 256 etc.).",
            "task_performance": "LSSL demonstrated strong accuracy on several LRD tasks historically (e.g., sequential MNIST) but was impractical for large N or long L due to resource use; in this paper it's used as an expensive baseline and S4 exceeds or matches its empirical performance while being far more efficient.",
            "task_utility_analysis": "LSSL shows that SSMs with HiPPO matrices have high task utility for LRDs in principle, but its computational inefficiency prevents wide applicability; thus high-fidelity representations did not translate to practical task utility until S4 addressed computational bottlenecks.",
            "tradeoffs_observed": "LSSL achieves strong fidelity for LRDs but at the cost of quadratic (in N) computational operations and linear-in-N times sequence-length memory; attempts to accelerate (diagonalization or polynomial tricks) are numerically unstable due to ill-conditioned matrices, so there is a tradeoff between theoretical speed-ups and numerical stability.",
            "design_choices": "Directly use HiPPO A in discrete-time SSM and expand powers of A to compute kernel K (naive powering); no NPLR parameterization; uses bilinear discretization but lacks stable conjugation/diagonalization.",
            "comparison_to_alternatives": "Compared to S4, LSSL is less efficient (orders of magnitude slower and more memory) though conceptually similar; compared to Transformers/CNNs/RNNs it is less practical at scale due to computational/memory overheads.",
            "optimal_configuration": "Paper argues that the LSSL is not a practical optimal configuration for large-scale sequence modeling; instead the recommended configuration is the S4 parameterization (HiPPO initialization + NPLR/DPLR + frequency-space kernel computation).",
            "uuid": "e1404.1",
            "source_info": {
                "paper_title": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "S4",
            "name_full": "Structured State Space sequence model (S4)",
            "brief_description": "A new parameterization and algorithm for SSMs that decomposes A into a Normal plus Low-Rank (NPLR) term, computes the truncated generating function in frequency space, uses Woodbury corrections, and reduces kernel computation to Cauchy kernel evaluations to achieve near-linear time and memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "S4 (Structured State Space sequence model)",
            "model_description": "An SSM-based deep sequence layer where the continuous-time state matrix A is parameterized as A = Lambda - P Q* (unitary-conjugate DPLR form). Key algorithmic moves: (1) conjugate HiPPO matrices into diagonal-plus-low-rank (DPLR) via unitary V, (2) compute truncated generating function in frequency domain evaluated at roots of unity, (3) apply Woodbury identity to correct low-rank part, reducing core computation to Cauchy kernel evaluations, and (4) obtain time-domain kernel via iFFT. The model is used in deep stacks (H features) and combined with pointwise mixing and nonlinearities.",
            "model_type": "latent continuous-time state-space model with structured (NPLR/DPLR) parameterization; neural sequence model",
            "task_domain": "Long-range sequence tasks: Long Range Arena (text, retrieval, image, pathfinder, Path-X), raw speech classification (SC10), sequential CIFAR-10 and sMNIST/pMNIST, autoregressive generative modeling (CIFAR-10 density), language modeling (WikiText-103), time-series forecasting",
            "fidelity_metric": "Standard downstream metrics for tasks: classification accuracy (%), perplexity (ppl) for language, bits-per-dim (bpd) for density estimation, MSE/MAE for forecasting; also benchmark-specific accuracy (e.g., LRA tasks). Numerical stability of algorithms (conditioning) also used as a fidelity proxy for numerical correctness.",
            "fidelity_performance": "Reported strong empirical performance: LRA average 86.09% (Table 4); Path‑X 96.35% (solves length‑16384 task); SC10 raw speech accuracy 98.32% (0.5x sampling: 96.30%); sequential CIFAR sCIFAR 91.13% (Table 6); CIFAR density estimation bpd 2.85 (S4 large), generation speed 20.84 images/sec (~65.1x faster than Transformer baseline); WikiText-103 perplexity 20.95 (tokens/sec 48K, ~60x faster generation than standard Transformer).",
            "interpretability_assessment": "Partially interpretable: the convolutional impulse response K is explicit and visualizable; the paper visualizes K reshaped into images showing interpretable spatial filters (lower-layer local filters, higher-layer global column-aggregating filters). The underlying parameterization (NPLR) gives structural insight into the system dynamics (diagonalizable normal part plus low-rank adjustments).",
            "interpretability_method": "Visualization of SSM convolution kernels (filter K), inspection of learned filters across layers, analysis of NPLR decomposition and spectral properties, examination of HiPPO-informed initializations.",
            "computational_cost": "Asymptotically Õ(N+L) operations and O(N+L) memory for computing kernel via Cauchy multiplies (Theorem 3); recurrent step O(N) operations (Theorem 2). Empirical timings: Table 2 shows S4 training step ms (dim 256: 3.07 ms) and memory allocations (dim 256: 12.6 MB). Generation throughput: CIFAR images/sec 20.84 (65x faster than Transformer baseline at 0.32 images/sec); WikiText tokens/sec 48K (60x faster than Transformer baseline 0.8K).",
            "efficiency_comparison": "Compared to naive LSSL: up to ~30x faster and up to ~400x less memory in reported experiments (Table 2). Compared to Transformers and efficient Transformer variants, S4 is competitive or faster in speed and much more memory efficient on long sequences (Table 3 reports S4 speed 1.58x vs Transformer at length 1024 and 5.19x at length 4096, with memory fractions reported).",
            "task_performance": "State-of-the-art on Long Range Arena tasks in reported comparisons (Table 4), solved Path-X (length 16384) with 96.35% accuracy; sCIFAR 91.13% sequential classification; SC10 raw speech 98.32% accuracy; CIFAR autoregressive density estimation competitive (2.85 bpd) and much faster generation; WikiText-103 perplexity 20.95 (close to Transformer 20.51).",
            "task_utility_analysis": "High fidelity (via HiPPO initialization) combined with S4's efficient algorithms translates into strong task performance across diverse domains requiring long-range reasoning; S4's efficient recurrent view provides orders-of-magnitude faster autoregressive generation while maintaining competitive predictive performance, showing practical utility of the world-model-like SSM for both discriminative and generative tasks.",
            "tradeoffs_observed": "S4 trades some modeling simplicity for algorithmic complexity: the NPLR/DPLR parameterization and frequency-domain kernel computation add algorithmic machinery but achieve large efficiency gains. There remain domains (some language modeling settings) where Transformers still slightly outperform S4 in perplexity, indicating a fidelity vs inductive-bias tradeoff. Numerical stability issues can occur when eigenvalues are in right half-plane; follow-up fix to parameterization (Λ - P P*) mitigates this.",
            "design_choices": "Key design choices: initialize A using HiPPO matrices (to enable long memory), represent A as Normal plus Low-Rank (NPLR) and conjugate to DPLR, compute truncated generating function in frequency space and evaluate at roots of unity, apply Woodbury identity to handle low-rank correction, reduce to Cauchy kernel evaluations and iFFT to obtain time-domain K, broadcast across H features with position-wise mixing. Optionally replace Q*P by P P* for numeric stability.",
            "comparison_to_alternatives": "Against LSSL: far more computationally efficient while matching or exceeding accuracy. Against Transformers and efficient Transformer variants: S4 outperforms them on LRA long-range tasks and is much faster at autoregressive generation (≈60x) though Transformers sometimes still have an edge on certain language-model perplexity metrics. Against CNN/RNN baselines on speech and sequential CIFAR, S4 matches or outperforms specialized architectures.",
            "optimal_configuration": "Paper recommends HiPPO initialization for A plus the NPLR/DPLR parameterization and frequency-domain kernel computation as the practical optimal configuration for balancing fidelity (long-range memory), interpretability (explicit kernel), computational efficiency (Õ(N+L)), and task utility; also notes a minor parameterization change (Λ - P P*) to avoid instabilities when eigenvalues lie in the right half-plane.",
            "uuid": "e1404.2",
            "source_info": {
                "paper_title": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "HiPPO",
            "name_full": "HiPPO (High-order Polynomial Projection Operators) matrices / framework",
            "brief_description": "A theoretical framework that prescribes special continuous-time state matrices A (HiPPO matrices) that enable continuous-time memorization by projecting input histories into polynomial bases, giving SSMs the ability to capture long-range dependencies.",
            "citation_title": "Hippo: Recurrent memory with optimal polynomial projections",
            "mention_or_use": "use",
            "model_name": "HiPPO-structured SSM (HiPPO matrices)",
            "model_description": "A class of specially constructed A matrices derived from continuous-time memorization theory; when used to parameterize SSMs, they produce states that optimally encode recent input history in orthogonal polynomial bases (Legendre/Laguerre variants). HiPPO defines specific dense, structured A matrices (e.g., HiPPO-LegT/LegS/LagT) with provable memory properties.",
            "model_type": "structured operator / parameterization for latent dynamical models",
            "task_domain": "long-range sequence modeling, memory tasks (sequential MNIST, LRA tasks), speech/time-series where long temporal context matters",
            "fidelity_metric": "Downstream task performance and formal continuous-time memorization guarantees (theoretical properties of basis projections); empirically measured via improved validation accuracy on long-range tasks when used as initialization.",
            "fidelity_performance": "Empirically, replacing random A with HiPPO A improved sequential MNIST from ~60% to ~98% in prior work; in this paper, HiPPO initialization is shown essential for S4's generalization (ablation: HiPPO-initialized SSMs generalize much better than random initializations). Exact numerical gains depend on task.",
            "interpretability_assessment": "High theoretical interpretability: matrices correspond to projections onto polynomial bases (Legendre, Laguerre) and have known analytic structure. However, direct eigen-decomposition is numerically ill-conditioned (eigenvector matrix entries exponentially large), limiting naive spectral interpretability in practice.",
            "interpretability_method": "Mathematical analysis of the matrix form and its projection properties; analytic derivation of eigenvectors/eigenvalues (paper shows explicit diagonalization with combinatorial entries), and leveraging that HiPPO matrices are NPLR (normal plus low-rank) enabling structured conjugation.",
            "computational_cost": "HiPPO matrices themselves are dense and when used naively incur the same O(N^2 L) or O(N L) memory costs as general dense A; the paper shows they can be represented as NPLR (rank r=1 or 2), which S4 exploits to reduce computational cost to near-linear.",
            "efficiency_comparison": "Naive handling of HiPPO A makes prior SSM implementations infeasible; S4's NPLR parameterization demonstrates that HiPPO matrices can be handled efficiently by transforming them into unitary-diagonal-plus-low-rank form and using Woodbury/Cauchy algorithms.",
            "task_performance": "Critical enabler for high performance on LRD tasks: ablation experiments in paper show HiPPO initialization substantially improves validation accuracy versus random or diagonal initializations; HiPPO+S4 achieves SoTA across LRA and other tasks.",
            "task_utility_analysis": "HiPPO provides the principled inductive bias (optimal polynomial memory) necessary for SSMs to capture long histories; however, HiPPO's dense structure requires algorithmic work (NPLR/DPLR) to be practically useful.",
            "tradeoffs_observed": "HiPPO gives strong theoretical memory/fidelity but is numerically delicate (diagonalization yields exponentially large factors) and computationally expensive unless expressed in NPLR form; tradeoff between theoretical desirability and practical numeric/compute constraints.",
            "design_choices": "Use HiPPO matrices as initialization for A to inject long-range-memory inductive bias; combine with NPLR parameterization and stable conjugation (unitary V) to make practical.",
            "comparison_to_alternatives": "Better principled long-memory behavior than random A or naive diagonal A initializations; diagonal A is computationally cheaper but empirically underperforms in generalization compared to HiPPO-initialized SSMs in the ablations.",
            "optimal_configuration": "Paper recommends HiPPO initialization together with S4's NPLR/DPLR parameterization (unitary conjugation, low-rank correction) as the practical configuration to achieve both the memorization properties and computational tractability.",
            "uuid": "e1404.3",
            "source_info": {
                "paper_title": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer",
            "rating": 2
        },
        {
            "paper_title": "Hippo: Recurrent memory with optimal polynomial projections",
            "rating": 2
        },
        {
            "paper_title": "Legendre memory units: Continuous-time representation in recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Parallelizing legendre memory unit training",
            "rating": 1
        },
        {
            "paper_title": "Fast approximate computations with cauchy matrices and polynomials",
            "rating": 1
        },
        {
            "paper_title": "It's raw! audio generation with state-space models",
            "rating": 2
        }
    ],
    "cost": 0.019473499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Efficiently Modeling Long Sequences with Structured State Spaces</h1>
<p>Albert Gu, Karan Goel, and Christopher Ré<br>Department of Computer Science, Stanford University<br>{albertgu, krng}@stanford.edu, chrismre@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) $x^{\prime}(t)=A x(t)+$ $B u(t), y(t)=C x(t)+D u(t)$, and showed that for appropriate choices of the state matrix $A$, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning $A$ with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) $91 \%$ accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60 \times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16 k that all prior work fails on, while being as efficient as all competitors. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>A central problem in sequence modeling is efficiently handling data that contains long-range dependencies (LRDs). Real-world time-series data often requires reasoning over tens of thousands of time steps, while few sequence models address even thousands of time steps. For instance, results from the long-range arena (LRA) benchmark [40] highlight that sequence models today perform poorly on LRD tasks, including one (Path-X) where no model performs better than random guessing.
Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed to address them. Modern examples include orthogonal and Lipschitz RNNs [1, 13] to combat vanishing gradients, dilated convolutions to increase context size [3, 28], and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8, 22]. Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification [18].
An alternative approach to LRDs was recently introduced based on the state space model (SSM) (Fig. 1). SSMs are a foundational scientific model used in fields such as control theory, computational neuroscience, and many more, but have not been applicable to deep learning for concrete theoretical reasons. In particular, Gu et al. [18] showed that deep SSMs actually struggle even on simple tasks, but can perform exceptionally</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (Left) State Space Models (SSM) parameterized by matrices $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}, \boldsymbol{D}$ map an input signal $u(t)$ to output $y(t)$ through a latent state $x(t)$. (Center) Recent theory on continuous-time memorization derives special $\boldsymbol{A}$ matrices that allow SSMs to capture LRDs mathematically and empirically. (Right) SSMs can be computed either as a recurrence (left) or convolution (right). However, materializing these conceptual views requires utilizing different representations of its parameters (red, blue, green) which are very expensive to compute. S4 introduces a novel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of tasks, be efficient at both training and inference, and excel at long sequences.
well when equipped with special state matrices $\boldsymbol{A}$ recently derived to solve a problem of continuous-time memorization [16, 45]. Their Linear State Space Layer (LSSL) conceptually unifies the strengths of CTM, RNN and CNN models, and provides a proof of concept that deep SSMs can address LRDs in principle.
Unfortunately, the LSSL is infeasible to use in practice because of prohibitive computation and memory requirements induced by the state representation. For state dimension $N$ and sequence length $L$, computing the latent state requires $O\left(N^{2} L\right)$ operations and $O(N L)$ space - compared to a $\Omega(L+N)$ lower bound for both. Thus for reasonably sized models (e.g. $N=256$ in Gu et al. [18]), the LSSL uses orders of magnitude more memory than comparably-sized RNNs or CNNs. Although theoretically efficient algorithms for the LSSL were proposed, we show that these are numerically unstable. In particular, the special $\boldsymbol{A}$ matrix is highly non-normal in the linear algebraic sense, which prevents the application of conventional algorithmic techniques. Consequently, although the LSSL showed that SSMs have strong performance, they are currently computationally impractical as a general sequence modeling solution.
In this work, we introduce the Structured State Space (S4) sequence model based on the SSM that solves the critical computational bottleneck in previous work. Technically, S4 reparameterizes the structured state matrices $\boldsymbol{A}$ appearing in Gu et al. [16], Voelker et al. [45] by decomposing them as the sum of a low-rank and normal term. Additionally, instead of expanding the standard SSM in coefficient space, we compute its truncated generating function in frequency space, which can be simplified into a multipole-like evaluation. Combining these two ideas, we show that the low-rank term can be corrected by the Woodbury identity while the normal term can be diagonalized stably, ultimately reducing to a well-studied and theoretically stable Cauchy kernel [29, 30]. This results in $\tilde{O}(N+L)$ computation and $O(N+L)$ memory usage, which is essentially tight for sequence models. Compared to the LSSL, S4 is up to $30 \times$ faster with $400 \times$ less memory usage, while exceeding the LSSL's performance empirically.
Empirically, S4 significantly advances the state-of-the-art for LRD. On the LRA benchmark for efficient sequence models, S4 is as fast as all baselines while outperforming them by $20+$ points on average. S4 is the first model to solve the difficult LRA Path-X task (length-16384), achieving $\mathbf{8 8 \%}$ accuracy compared to $\mathbf{5 0 \%}$ random guessing for all prior work. On speech classification with length-16000 sequences, S4 halves the test error (1.7\%) of specialized Speech CNNs - by contrast, all RNN and Transformer baselines fail to learn ( $\geq 70 \%$ error).</p>
<p>Towards a general-purpose sequence model. Beyond LRD, a broad goal of machine learning is to develop a single model that can be used across a wide range of problems. Models today are typically</p>
<p>specialized to solve problems from a particular domain (e.g. images, audio, text, time-series), and enable a narrow range of capabilities (e.g. efficient training, fast generation, handling irregularly sampled data). This specialization is typically expressed via domain-specific preprocessing, inductive biases, and architectures. Sequence models provide a general framework for solving many of these problems with reduced specialization - e.g. Vision Transformers for image classification with less 2D information [12]. However, most models such as Transformers generally still require substantial specialization per task to achieve high performance.
Deep SSMs in particular have conceptual strengths that suggest they may be promising as a general sequence modeling solution. These strengths include a principled approach to handling LRDs, as well as the ability to move between continuous-time, convolutional, and recurrent model representations, each with distinct capabilities (Fig. 1). Our technical contributions enable SSMs to be applied successfully to a varied set of benchmarks with minimal modification:</p>
<ul>
<li>Large-scale generative modeling. On CIFAR-10 density estimation, S4 is competitive with the best autoregressive models ( 2.85 bits per dim). On WikiText-103 language modeling, S4 substantially closes the gap to Transformers (within 0.8 perplexity), setting SoTA for attention-free models.</li>
<li>Fast autoregressive generation. Like RNNs, S4 can use its latent state to perform $60 \times$ faster pixel/token generation than standard autoregressive models on CIFAR-10 and WikiText-103.</li>
<li>Sampling resolution change. Like specialized CTMs, S4 can adapt to changes in time-series sampling frequency without retraining, e.g. at $0.5 \times$ frequency on speech classification.</li>
<li>Learning with weaker inductive biases. With no architectural changes, S4 surpasses Speech CNNs on speech classification, outperforms the specialized Informer model on time-series forecasting problems, and matches a 2-D ResNet on sequential CIFAR with over $90 \%$ accuracy.</li>
</ul>
<h1>2 Background: State Spaces</h1>
<p>Sections 2.1 to 2.4 describe the four properties of SSMs in Fig. 1: the classic continuous-time representation, addressing LRDs with the HiPPO framework, the discrete-time recurrent representation, and the parallelizable convolution representation. In particular, Section 2.4 introduces the SSM convolution kernel $\overline{\boldsymbol{K}}$, which is the focus of our theoretical contributions in Section 3.</p>
<h3>2.1 State Space Models: A Continuous-time Latent State Model</h3>
<p>The state space model is defined by the simple equation (1). It maps a 1-D input signal $u(t)$ to an $N$-D latent state $x(t)$ before projecting to a 1-D output signal $y(t)$.</p>
<p>$$
\begin{aligned}
x^{\prime}(t) &amp; =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \
y(t) &amp; =\boldsymbol{C} x(t)+\boldsymbol{D} u(t)
\end{aligned}
$$</p>
<p>SSMs are broadly used in many scientific disciplines and related to latent state models such as Hidden Markov Models (HMM). Our goal is to simply use the SSM as a black-box representation in a deep sequence model, where $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}, \boldsymbol{D}$ are parameters learned by gradient descent. For the remainder of this paper, we will omit the parameter $\boldsymbol{D}$ for exposition (or equivalently, assume $\boldsymbol{D}=0$ ) because the term $\boldsymbol{D} u$ can be viewed as a skip connection and is easy to compute.</p>
<h3>2.2 Addressing Long-Range Dependencies with HiPPO</h3>
<p>Prior work found that the basic SSM (1) actually performs very poorly in practice. Intuitively, one explanation is that linear first-order ODEs solve to an exponential function, and thus may suffer from gradients scaling exponentially in the sequence length (i.e., the vanishing/exploding gradients problem [32]). To address this</p>
<p>problem, the LSSL leveraged the HiPPO theory of continuous-time memorization [16]. HiPPO specifies a class of certain matrices $\boldsymbol{A} \in \mathbb{R}^{N \times N}$ that when incorporated into (1), allows the state $x(t)$ to memorize the history of the input $u(t)$. The most important matrix in this class is defined by equation (2), which we will call the HiPPO matrix. For example, the LSSL found that simply modifying an SSM from a random matrix $\boldsymbol{A}$ to equation (2) improved its performance on the sequential MNIST benchmark from $60 \%$ to $98 \%$.</p>
<p>$$
\text { (HiPPO Matrix) } \quad \boldsymbol{A}_{n k}=-\left{\begin{array}{ll}
(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} &amp; \text { if } n&gt;k \
n+1 &amp; \text { if } n=k \
0 &amp; \text { if } n&lt;k
\end{array}\right.
$$</p>
<h1>2.3 Discrete-time SSM: The Recurrent Representation</h1>
<p>To be applied on a discrete input sequence $\left(u_{0}, u_{1}, \ldots\right)$ instead of continuous function $u(t),(1)$ must be discretized by a step size $\Delta$ that represents the resolution of the input. Conceptually, the inputs $u_{k}$ can be viewed as sampling an implicit underlying continuous signal $u(t)$, where $u_{k}=u(k \Delta)$.
To discretize the continuous-time SSM, we follow prior work in using the bilinear method [43], which converts the state matrix $\boldsymbol{A}$ into an approximation $\overline{\boldsymbol{A}}$. The discrete SSM is</p>
<p>$$
\begin{array}{ll}
x_{k}=\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_{k} &amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) \
y_{k}=\overline{\boldsymbol{C}} x_{k} &amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B} \quad \overline{\boldsymbol{C}}=\boldsymbol{C}
\end{array}
$$</p>
<p>Equation (3) is now a sequence-to-sequence map $u_{k} \mapsto y_{k}$ instead of function-to-function. Moreover the state equation is now a recurrence in $x_{k}$, allowing the discrete SSM to be computed like an RNN. Concretely, $x_{k} \in \mathbb{R}^{N}$ can be viewed as a hidden state with transition matrix $\overline{\boldsymbol{A}}$.
Notationally, throughout this paper we use $\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \ldots$ to denote discretized SSM matrices defined by (3). Note that these matrices are a function of both $\boldsymbol{A}$ as well as a step size $\Delta$; we suppress this dependence for notational convenience when it is clear.</p>
<h3>2.4 Training SSMs: The Convolutional Representation</h3>
<p>The recurrent SSM (3) is not practical for training on modern hardware due to its sequentiality. Instead, there is a well-known connection between linear time-invariant (LTI) SSMs such as (1) and continuous convolutions. Correspondingly, (3) can actually be written as a discrete convolution.
For simplicity let the initial state be $x_{-1}=0$. Then unrolling (3) explicitly yields</p>
<p>$$
\begin{array}{lll}
x_{0}=\overline{\boldsymbol{B}} u_{0} &amp; x_{1}=\overline{\boldsymbol{A} \boldsymbol{B}} u_{0}+\overline{\boldsymbol{B}} u_{1} &amp; x_{2}=\overline{\boldsymbol{A}}^{2} \overline{\boldsymbol{B}} u_{0}+\overline{\boldsymbol{A} \boldsymbol{B}} u_{1}+\overline{\boldsymbol{B}} u_{2} \
y_{0}=\overline{\boldsymbol{C} \boldsymbol{B}} u_{0} &amp; y_{1}=\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_{0}+\overline{\boldsymbol{C} \boldsymbol{B}} u_{1} &amp; y_{2}=\overline{\boldsymbol{C} \boldsymbol{A}}^{2} \overline{\boldsymbol{B}} u_{0}+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_{1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_{2} &amp; \ldots
\end{array}
$$</p>
<p>This can be vectorized into a convolution (4) with an explicit formula for the convolution kernel (5).</p>
<p>$$
\begin{aligned}
y_{k} &amp; =\overline{\boldsymbol{C} \boldsymbol{A}}^{k} \overline{\boldsymbol{B}} u_{0}+\overline{\boldsymbol{C} \boldsymbol{A}}^{k-1} \overline{\boldsymbol{B}} u_{1}+\cdots+\overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}} u_{k-1}+\overline{\boldsymbol{C} \boldsymbol{B}} u_{k} \
y &amp; =\overline{\boldsymbol{K}} * u \
\overline{\boldsymbol{K}} \in \mathbb{R}^{L}:= &amp; \mathcal{K}<em _in_L_="\in[L]" i="i">{L}(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}}):=\left(\overline{\boldsymbol{C} \boldsymbol{A}}^{\prime} \overline{\boldsymbol{B}}\right)</em>\right)
\end{aligned}
$$}=\left(\overline{\boldsymbol{C} \boldsymbol{B}}, \overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}}, \ldots, \overline{\boldsymbol{C} \boldsymbol{A}}^{L-1} \overline{\boldsymbol{B}</p>
<p>In other words, equation (4) is a single (non-circular) convolution and can be computed very efficiently with FFTs, provided that $\overline{\boldsymbol{K}}$ is known. However, computing $\overline{\boldsymbol{K}}$ in (5) is non-trivial and is the focus of our technical contributions in Section 3. We call $\overline{\boldsymbol{K}}$ the SSM convolution kernel or filter.</p>
<h1>3 Method: Structured State Spaces (S4)</h1>
<p>Our technical results focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM (Section 2): the continuous representation $(\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C})(1)$, the recurrent representation $(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}})(3)$, and the convolutional representation $\overline{\boldsymbol{K}}(4)$.
Section 3.1 motivates our approach, which is based on the linear algebraic concepts of conjugation and diagonalization, and discusses why the naive application of this approach does not work. Section 3.2 gives an overview of the key technical components of our approach and formally defines the S4 parameterization. Section 3.3 sketches the main results, showing that S 4 is asymptotically efficient (up to log factors) for sequence models. Proofs are in Appendices B and C.</p>
<h3>3.1 Motivation: Diagonalization</h3>
<p>The fundamental bottleneck in computing the discrete-time SSM (3) is that it involves repeated matrix multiplication by $\overline{\boldsymbol{A}}$. For example, computing (5) naively as in the LSSL involves $L$ successive multiplications by $\overline{\boldsymbol{A}}$, requiring $O\left(N^{2} L\right)$ operations and $O(N L)$ space.
To overcome this bottleneck, we use a structural result that allows us to simplify SSMs.
Lemma 3.1. Conjugation is an equivalence relation on SSMs $(\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}) \sim\left(\boldsymbol{V}^{-1} \boldsymbol{A} \boldsymbol{V}, \boldsymbol{V}^{-1} \boldsymbol{B}, \boldsymbol{C} \boldsymbol{V}\right)$.
Proof. Write out the two SSMs with state denoted by $x$ and $\tilde{x}$ respectively:</p>
<p>$$
\begin{aligned}
x^{\prime} &amp; =\boldsymbol{A} x+\boldsymbol{B} u &amp; \tilde{x}^{\prime} &amp; =\boldsymbol{V}^{-1} \boldsymbol{A} \boldsymbol{V} \tilde{x}+\boldsymbol{V}^{-1} \boldsymbol{B} u \
y &amp; =\boldsymbol{C} x &amp; y &amp; =\boldsymbol{C} \boldsymbol{V} \tilde{x}
\end{aligned}
$$</p>
<p>After multiplying the right side SSM by $\boldsymbol{V}$, the two SSMs become identical with $x=\boldsymbol{V} \tilde{x}$. Therefore these compute the exact same operator $u \mapsto y$, but with a change of basis by $\boldsymbol{V}$ in the state $x$.</p>
<p>Lemma 3.1 motivates putting $\boldsymbol{A}$ into a canonical form by conjugation ${ }^{2}$, which is ideally more structured and allows faster computation. For example, if $\boldsymbol{A}$ were diagonal, the resulting computations become much more tractable. In particular, the desired $\overline{\boldsymbol{K}}$ (equation (4)) would be a Vandermonde product which theoretically only needs $O\left((N+L) \log ^{2}(N+L)\right)$ arithmetic operations [29].
Unfortunately, the naive application of diagonalization does not work due to numerical issues. Werive the explicit diagonalization for the HiPPO matrix (2) and show it has entries exponentially large in the state size $N$, rendering the diagonalization numerically infeasible (e.g. $\boldsymbol{C} \boldsymbol{V}$ in Lemma 3.1 would not be computable). We note that Gu et al. [18] proposed a different (unimplemented) algorithm to compute $\overline{\boldsymbol{K}}$ faster than the naive algorithm. In Appendix B, we prove that it is also numerically unstable for related reasons.
Lemma 3.2. The HiPPO matrix $\boldsymbol{A}$ in equation (2) is diagonalized by the matrix $\boldsymbol{V}<em 3="3" i="i" i_="i,">{i j}=\binom{i+j}{i-j}$. In particular, $\boldsymbol{V}</em>$.}=\binom{4 i}{2 i} \approx 2^{4 i}$. Therefore $\boldsymbol{V}$ has entries of magnitude up to $2^{4 N / 3</p>
<h3>3.2 The S4 Parameterization: Normal Plus Low-Rank</h3>
<p>The previous discussion implies that we should only conjugate by well-conditioned matrices $\boldsymbol{V}$. The ideal scenario is when the matrix $\boldsymbol{A}$ is diagonalizable by a perfectly conditioned (i.e., unitary) matrix. By the Spectral Theorem of linear algebra, this is exactly the class of normal matrices. However, this class of matrices is restrictive; in particular, it does not contain the HiPPO matrix (2).
We make the observation that although the HiPPO matrix is not normal, it can be decomposed as the sum of a normal and low-rank matrix. However, this is still not useful by itself: unlike a diagonal matrix, powering up this sum (in (5)) is still slow and not easily optimized. We overcome this bottleneck by simultaneously applying three new techniques.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Algorithm 1 S4 Convolution Kernel (Sketch)</h1>
<p>Input: S4 parameters $\boldsymbol{\Lambda}, \boldsymbol{P}, \boldsymbol{Q}, \boldsymbol{B}, \boldsymbol{C} \in \mathbb{C}^{N}$ and step size $\Delta$
Output: SSM convolution kernel $\overline{\boldsymbol{K}}=\mathcal{K}<em 00="00">{L}(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}})$ for $\boldsymbol{A}=\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{<em>}$ (equation (5))
1: $\overline{\boldsymbol{C}} \leftarrow\left(\boldsymbol{I}-\overline{\boldsymbol{A}}^{L}\right)^{</em>} \overline{\boldsymbol{C}} \quad \triangleright$ Truncate SSM generating function (SSMGF) to length $L$
2: $\left[\begin{array}{ll}k</em>] \quad \triangleright$ Black-box Cauchy kernel
3: $\overline{\boldsymbol{K}}(\omega) \leftarrow \frac{2}{1+\omega}\left[k_{00}(\omega)-k_{01}(\omega)\left(1+k_{11}(\omega)\right)^{-1} k_{10}(\omega)\right] \quad \triangleright$ Woodbury Identity
4: $\overline{\boldsymbol{K}}=\left{\overline{\boldsymbol{K}}(\omega): \omega=\exp \left(2 \pi i \frac{k}{L}\right)\right} \quad \triangleright$ Evaluate SSMGF at all roots of unity $\omega \in \Omega_{L}$
5: $\overline{\boldsymbol{K}} \leftarrow \mathrm{iFFT}(\overline{\boldsymbol{K}}) \quad \triangleright$ Inverse Fourier Transform}(\omega) &amp; k_{01}(\omega) \ k_{10}(\omega) &amp; k_{11}(\omega)\end{array}\right] \leftarrow\left[\begin{array}{ll}\overline{\boldsymbol{C}} \boldsymbol{Q}\end{array}\right]^{*}\left(\frac{2}{\Delta} \frac{1-\omega}{1+\omega}-\boldsymbol{\Lambda}\right)^{-1}[\boldsymbol{B} \boldsymbol{P</p>
<ul>
<li>Instead of computing $\overline{\boldsymbol{K}}$ directly, we compute its spectrum by evaluating its truncated generating function $\sum_{j=0}^{L-1} \overline{\boldsymbol{K}}_{j} \zeta^{j}$ at the roots of unity $\zeta . \overline{\boldsymbol{K}}$ can then be found by applying an inverse FFT.</li>
<li>This generating function is closely related to the matrix resolvent, and now involves a matrix inverse instead of power. The low-rank term can now be corrected by applying the Woodbury identity which reduces $\left(\boldsymbol{A}+\boldsymbol{P} \boldsymbol{Q}^{*}\right)^{-1}$ in terms of $\boldsymbol{A}^{-1}$, truly reducing to the diagonal case.</li>
<li>Finally, we show that the diagonal matrix case is equivalent to the computation of a Cauchy kernel $\frac{1}{\omega_{j}-\zeta_{k}}$, a well-studied problem with stable near-linear algorithms [30, 31].</li>
</ul>
<p>Our techniques apply to any matrix that can be decomposed as Normal Plus Low-Rank (NPLR).
Theorem 1. All HiPPO matrices from [16] have a NPLR representation</p>
<p>$$
\boldsymbol{A}=\boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^{<em>}-\boldsymbol{P} \boldsymbol{Q}^{\top}=\boldsymbol{V}\left(\boldsymbol{\Lambda}-\left(\boldsymbol{V}^{</em>} \boldsymbol{P}\right)\left(\boldsymbol{V}^{<em>} \boldsymbol{Q}\right)^{</em>}\right) \boldsymbol{V}^{*}
$$</p>
<p>for unitary $\boldsymbol{V} \in \mathbb{C}^{N \times N}$, diagonal $\boldsymbol{\Lambda}$, and low-rank factorization $\boldsymbol{P}, \boldsymbol{Q} \in \mathbb{R}^{N \times r}$. These matrices HiPPO- LegS, LegT, LagT all satisfy $r=1$ or $r=2$. In particular, equation (2) is NPLR with $r=1$.</p>
<h3>3.3 S4 Algorithms and Computational Complexity</h3>
<p>By equation (6), note that NPLR matrices can be conjugated into diagonal plus low-rank (DPLR) form (now over $\mathbb{C}$ instead of $\mathbb{R}$ ). Theorems 2 and 3 describe the complexities of SSMs where $\boldsymbol{A}$ is in DPLR form. S4 is optimal or near-optimal for both recurrent and convolutional representations.</p>
<p>Theorem 2 (S4 Recurrence). Given any step size $\Delta$, computing one step of the recurrence (3) can be done in $O(N)$ operations where $N$ is the state size.</p>
<p>Theorem 2 follows from the fact that the inverse of a DPLR matrix is also DPLR (e.g. also by the Woodbury identity). This implies that the discretized matrix $\overline{\boldsymbol{A}}$ is the product of two DPLR matrices and thus has $O(N)$ matrix-vector multiplication. Appendix C. 2 computes $\overline{\boldsymbol{A}}$ in closed DPLR form.</p>
<p>Theorem 3 (S4 Convolution). Given any step size $\Delta$, computing the SSM convolution filter $\overline{\boldsymbol{K}}$ can be reduced to 4 Cauchy multiplies, requiring only $\widetilde{O}(N+L)$ operations and $O(N+L)$ space.</p>
<p>Appendix C, Definition 3 formally defines Cauchy matrices, which are related to rational interpolation problems. Computing with Cauchy matrices is an extremely well-studied problem in numerical analysis, with both fast arithmetic and numerical algorithms based on the famous Fast Multipole Method (FMM) [29, 30, 31]. The computational complexities of these algorithms under various settings are described in Appendix C, Proposition 5.
We reiterate that Theorem 3 is our core technical contribution, and its algorithm is the very motivation of the NPLR S4 parameterization. This algorithm is formally sketched in Algorithm 1.</p>
<p>Table 1: Complexity of various sequence models in terms of sequence length $(\boldsymbol{L})$, batch size $(\boldsymbol{B})$, and hidden dimension $(\boldsymbol{H})$; tildes denote log factors. Metrics are parameter count, training computation, training space requirement, training parallelizability, and inference computation (for 1 sample and time-step). For simplicity, the state size $N$ of S 4 is tied to $H$. Bold denotes model is theoretically best for that metric. Convolutions are efficient for training while recurrence is efficient for inference, while SSMs combine the strengths of both.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Convolution</th>
<th style="text-align: left;">Recurrence</th>
<th style="text-align: left;">Attention</th>
<th style="text-align: left;">S4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Parameters</td>
<td style="text-align: left;">$L H$</td>
<td style="text-align: left;">$\boldsymbol{H}^{\mathbf{2}}$</td>
<td style="text-align: left;">$\boldsymbol{H}^{\mathbf{2}}$</td>
<td style="text-align: left;">$\boldsymbol{H}^{\mathbf{2}}$</td>
</tr>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: left;">$\tilde{\boldsymbol{L}} \boldsymbol{H}(\boldsymbol{B}+\boldsymbol{H})$</td>
<td style="text-align: left;">$B L H^{2}$</td>
<td style="text-align: left;">$B\left(L^{2} H+L H^{2}\right)$</td>
<td style="text-align: left;">$\boldsymbol{B H}(\tilde{\boldsymbol{H}}+\tilde{\boldsymbol{L}})+\boldsymbol{B} \tilde{\boldsymbol{L}} \boldsymbol{H}$</td>
</tr>
<tr>
<td style="text-align: left;">Space</td>
<td style="text-align: left;">$\boldsymbol{B} \boldsymbol{L} \boldsymbol{H}$</td>
<td style="text-align: left;">$\boldsymbol{B} \boldsymbol{L} \boldsymbol{H}$</td>
<td style="text-align: left;">$B\left(L^{2}+H L\right)$</td>
<td style="text-align: left;">$\boldsymbol{B} \boldsymbol{L} \boldsymbol{H}$</td>
</tr>
<tr>
<td style="text-align: left;">Parallel</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Inference</td>
<td style="text-align: left;">$L H^{2}$</td>
<td style="text-align: left;">$\boldsymbol{H}^{\mathbf{2}}$</td>
<td style="text-align: left;">$L^{2} H+H^{2} L$</td>
<td style="text-align: left;">$\boldsymbol{H}^{\mathbf{2}}$</td>
</tr>
</tbody>
</table>
<h1>3.4 Architecture Details of the Deep S4 Layer</h1>
<p>Concretely, an S4 layer is parameterized as follows. First initialize a SSM with $\boldsymbol{A}$ set to the HiPPO matrix (2). By Lemma 3.1 and Theorem 1, this SSM is unitarily equivalent to some $\left(\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{<em>}, \boldsymbol{B}, \boldsymbol{C}\right)$ for some diagonal $\boldsymbol{\Lambda}$ and vectors $\boldsymbol{P}, \boldsymbol{Q}, \boldsymbol{B}, \boldsymbol{C} \in \mathbb{C}^{N \times 1}$. These comprise S 4 's $5 N$ trainable parameters.
The overall deep neural network (DNN) architecture of S4 is similar to prior work. As defined above, S4 defines a map from $\mathbb{R}^{L} \rightarrow \mathbb{R}^{L}$, i.e. a 1-D sequence map. Typically, DNNs operate on feature maps of size $H$ instead of 1. S4 handles multiple features by simply defining $H$ independent copies of itself, and then mixing the $H$ features with a position-wise linear layer for a total of $O\left(H^{2}\right)+O(H N)$ parameters per layer. Nonlinear activation functions are also inserted between these layers. Overall, S4 defines a sequence-to-sequence map of shape (batch size, sequence length, hidden dimension), exactly the same as related sequence models such as Transformers, RNNs, and CNNs.
Note that the core S4 module is a linear transformation, but the addition of non-linear transformations through the depth of the network makes the overall deep SSM non-linear. This is analogous to a vanilla CNN, since convolutional layers are also linear. The broadcasting across $H$ hidden features described in this section is also analogous to depthwise-separable convolutions. Thus, the overall deep S4 model is closely related to a depthwise-separable CNN but with global convolution kernels.
Finally, we note that follow-up work found that this version of S4 can sometimes suffer from numerical instabilities when the $\boldsymbol{A}$ matrix has eigenvalues on the right half-plane [14]. It introduced a slight change to the NPLR parameterization for S 4 from $\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{</em>}$ to $\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{P}^{*}$ that corrects this potential problem.
Table 1 compares the complexities of the most common deep sequence modeling mechanisms.</p>
<h2>4 Experiments</h2>
<p>Section 4.1 benchmarks S4 against the LSSL and efficient Transformer models. Section 4.2 validates S4 on LRDs: the LRA benchmark and raw speech classification. Section 4.3 investigates whether S4 can be used as a general sequence model to perform effectively and efficiently in a wide variety of settings including image classification, image and text generation, and time series forecasting.</p>
<h3>4.1 S4 Efficiency Benchmarks</h3>
<p>We benchmark that S4 can be trained quickly and efficiently, both compared to the LSSL, as well as efficient Transformer variants designed for long-range sequence modeling. As outlined in Section 3, S4 is theoretically much more efficient than the LSSL, and Table 2 confirms that the S4 is orders of magnitude more speed- and memory-efficient for practical layer sizes. In fact, S4's speed and memory use is competitive with the most</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Deep SSMs: The S4 parameterization with Algorithm 1 is asymptotically more efficient than the LSSL.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Training Step (ms)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Memory Alloc. (MB)</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dim.</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">256</td>
<td style="text-align: left;">512</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">256</td>
</tr>
<tr>
<td style="text-align: left;">LSSL</td>
<td style="text-align: left;">9.32</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;">140.7</td>
<td style="text-align: left;">222.1</td>
<td style="text-align: left;">1685</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">4.77</td>
<td style="text-align: left;">3.07</td>
<td style="text-align: left;">4.75</td>
<td style="text-align: left;">5.3</td>
<td style="text-align: left;">12.6</td>
</tr>
<tr>
<td style="text-align: left;">Ratio</td>
<td style="text-align: left;">$1.9 \times$</td>
<td style="text-align: left;">$6.7 \times$</td>
<td style="text-align: left;">$\mathbf{2 9 . 6} \times$</td>
<td style="text-align: left;">$42.0 \times$</td>
<td style="text-align: left;">$133 \times$</td>
</tr>
</tbody>
</table>
<p>Table 3: Benchmarks vs. efficient Transformers</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Length 1024</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Length 4096</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Speed</td>
<td style="text-align: left;">Mem.</td>
<td style="text-align: left;">Speed</td>
<td style="text-align: left;">Mem.</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">$1 \times$</td>
<td style="text-align: left;">$1 \times$</td>
<td style="text-align: left;">$1 \times$</td>
<td style="text-align: left;">$1 \times$</td>
</tr>
<tr>
<td style="text-align: left;">Performer</td>
<td style="text-align: left;">$1.23 \times$</td>
<td style="text-align: left;">$\underline{0.43 \times}$</td>
<td style="text-align: left;">$3.79 \times$</td>
<td style="text-align: left;">$\underline{0.086 \times}$</td>
</tr>
<tr>
<td style="text-align: left;">Linear Trans.</td>
<td style="text-align: left;">$\mathbf{1 . 5 8 \times}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 7 \times}$</td>
<td style="text-align: left;">$\mathbf{5 . 3 5 \times}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 6 7 \times}$</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">$\mathbf{1 . 5 8 \times}$</td>
<td style="text-align: left;">$\underline{0.43 \times}$</td>
<td style="text-align: left;">$\underline{5.19 \times}$</td>
<td style="text-align: left;">$0.091 \times$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualizations of a trained S4 model on LRA Path-X. SSM convolution kernels $\overline{\boldsymbol{K}} \in \mathbb{R}^{16384}$ are reshaped into a $128 \times 128$ image. (Left) Example from the Path-X task, which involves deducing if the markers are connected by a path (Top) Filters from the first layer (Bottom) Filters from the last layer.</p>
<p>Table 4: (Long Range Arena) (Top) Original Transformer variants in LRA. Full results in Appendix D.2. (Bottom) Other models reported in the literature. Please read Appendix D. 5 before citing this table.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">ListOps</th>
<th style="text-align: left;">Text</th>
<th style="text-align: left;">Retrieval</th>
<th style="text-align: left;">Image</th>
<th style="text-align: left;">Pathfinder</th>
<th style="text-align: left;">Path-X</th>
<th style="text-align: left;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">36.37</td>
<td style="text-align: left;">64.27</td>
<td style="text-align: left;">57.46</td>
<td style="text-align: left;">42.44</td>
<td style="text-align: left;">71.40</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">53.66</td>
</tr>
<tr>
<td style="text-align: left;">Reformer</td>
<td style="text-align: left;">$\underline{37.27}$</td>
<td style="text-align: left;">56.10</td>
<td style="text-align: left;">53.40</td>
<td style="text-align: left;">38.07</td>
<td style="text-align: left;">68.50</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">50.56</td>
</tr>
<tr>
<td style="text-align: left;">BigBird</td>
<td style="text-align: left;">36.05</td>
<td style="text-align: left;">64.02</td>
<td style="text-align: left;">59.29</td>
<td style="text-align: left;">40.83</td>
<td style="text-align: left;">74.87</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">54.17</td>
</tr>
<tr>
<td style="text-align: left;">Linear Trans.</td>
<td style="text-align: left;">16.13</td>
<td style="text-align: left;">$\underline{65.90}$</td>
<td style="text-align: left;">53.09</td>
<td style="text-align: left;">42.34</td>
<td style="text-align: left;">75.30</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">50.46</td>
</tr>
<tr>
<td style="text-align: left;">Performer</td>
<td style="text-align: left;">18.01</td>
<td style="text-align: left;">65.40</td>
<td style="text-align: left;">53.82</td>
<td style="text-align: left;">42.77</td>
<td style="text-align: left;">77.05</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">51.18</td>
</tr>
<tr>
<td style="text-align: left;">FNet</td>
<td style="text-align: left;">35.33</td>
<td style="text-align: left;">65.11</td>
<td style="text-align: left;">59.61</td>
<td style="text-align: left;">38.67</td>
<td style="text-align: left;">$\underline{77.80}$</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">54.42</td>
</tr>
<tr>
<td style="text-align: left;">Nyströmformer</td>
<td style="text-align: left;">37.15</td>
<td style="text-align: left;">65.52</td>
<td style="text-align: left;">$\underline{79.56}$</td>
<td style="text-align: left;">41.58</td>
<td style="text-align: left;">70.94</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">57.46</td>
</tr>
<tr>
<td style="text-align: left;">Luna-256</td>
<td style="text-align: left;">37.25</td>
<td style="text-align: left;">64.57</td>
<td style="text-align: left;">79.29</td>
<td style="text-align: left;">$\underline{47.38}$</td>
<td style="text-align: left;">77.72</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">$\underline{59.37}$</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">$\mathbf{5 9 . 6 0}$</td>
<td style="text-align: left;">$\mathbf{8 6 . 8 2}$</td>
<td style="text-align: left;">$\mathbf{9 0 . 9 0}$</td>
<td style="text-align: left;">$\mathbf{8 8 . 6 5}$</td>
<td style="text-align: left;">$\mathbf{9 4 . 2 0}$</td>
<td style="text-align: left;">$\mathbf{9 6 . 3 5}$</td>
<td style="text-align: left;">$\mathbf{8 6 . 0 9}$</td>
</tr>
</tbody>
</table>
<p>efficient Transformer variants benchmarked by Tay et al. [40]-Linear Transformer [22] and Performer [8]-in a parameter-matched setting (Table 3, following the protocol of Tay et al. [40]).</p>
<h1>4.2 Learning Long Range Dependencies</h1>
<p>As described in Sections 2.2 and 3.1, S4 uses a principled approach to address LRDs based on the HiPPO theory of continuous-time memorization. Our goal in this section is to validate that S4 achieves high performance on difficult tasks that require long-range reasoning. We focus here on two problems: (i) the Long-Range Arena, a well-known benchmark designed to test efficient sequence models on LRDs, and (ii) a speech classification problem as a real-world test of LRDs.
Long Range Arena (LRA). LRA [40] contains 6 tasks with lengths 1K-16K steps, encompassing modalities</p>
<p>and objectives that require similarity, structural, and visuospatial reasoning. Table 4 compares S4 against the 11 Transformer variants from Tay et al. [40] as well as follow-up work. S4 substantially advances the SoTA, outperforming all baselines on all tasks and averaging $80.48 \%$ compared to less than $60 \%$ for every baseline. Notably, S4 solves the Path-X task, an extremely challenging task that involves reasoning about LRDs over sequences of length $128 \times 128=16384$. All previous models have failed (i.e. random guessing) due to memory or computation bottlenecks, or simply being unable to learn such long dependencies.
We analyze S4's performance on Path-X by visualizing its learned representations, in particular 1-D convolution kernels $\overline{\boldsymbol{K}}$ which are the focus of our technical results in Section 3. Fig. 2 shows that S4 learns a variety of filters that display spatially consistent structure and demonstrate awareness of the 2-D nature of the data. In particular, the lower layers learn simple kernels that extract features from just a few rows of local context while ignoring the rest of the image. On the other hand, higher layers aggregate information globally across full columns of the image at varying spatial frequencies. Filters in these higher layers span the entire context (16384 pixels), confirming S4's ability to learn LRDs.
Raw Speech Classification. Speech is a typical real-world time series domain, involving signals sampled from an underlying physical process at high frequency. We perform speech classification using the SC10 subset of the Speech Commands dataset [47] (see Appendix D.5). While most sequence models for speech rely on extensive preprocessing (e.g. to MFCC features), we classify raw speech (length-16000) following Romero et al. [35]. S4 achieves $98.3 \%$ accuracy, higher than all baselines that use the $100 \times$ shorter MFCC features, and validates that a powerful LRD model is able to extract more information from the raw data and outperform hand-crafted pre-processing. Additionally, we include a baseline CNN specifically designed for raw speech, the discriminator from the WaveGAN model [11], which performs worse than S4 while having $90 \times$ more parameters and incorporating many more architectural heuristics (Appendix D.2).</p>
<h1>4.3 S4 as a General Sequence Model</h1>
<p>A key goal of sequence modeling research is to develop a single model that can be applied in many domains (e.g. images, audio, text, time-series) with a broad range of capabilities (e.g. efficient training, fast generation, handling irregularly sampled data). As a fundamental scientific model, SSMs are a promising candidate that come with a range of capabilities, and S4's strong results on LRD benchmarks spanning images, text, and speech are evidence of S4's potential as a general sequence model. In this section, we focus on understanding this question in more depth by highlighting key strengths of S4 in settings that usually require specialized</p>
<p>Table 5: (SC10 classification) Transformer, CTM, RNN, CNN, and SSM models. (MFCC) Standard preprocessed MFCC features (length 161). (Raw) Unprocessed signals (length 16000). $(0.5 \times)$ Frequency change at test time. $\boldsymbol{X}$ denotes not applicable or computationally infeasible on single GPU. Please read Appendix D. 5 before citing this table.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MFCC</th>
<th style="text-align: left;">RAW</th>
<th style="text-align: left;">$0.5 \times$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">90.75</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">Performer</td>
<td style="text-align: left;">80.85</td>
<td style="text-align: left;">30.77</td>
<td style="text-align: left;">30.68</td>
</tr>
<tr>
<td style="text-align: left;">ODE-RNN</td>
<td style="text-align: left;">65.9</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">NRDE</td>
<td style="text-align: left;">89.8</td>
<td style="text-align: left;">16.49</td>
<td style="text-align: left;">15.12</td>
</tr>
<tr>
<td style="text-align: left;">ExpRNN</td>
<td style="text-align: left;">82.13</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">10.8</td>
</tr>
<tr>
<td style="text-align: left;">LipschitzRNN</td>
<td style="text-align: left;">88.38</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">CKConv</td>
<td style="text-align: left;">$\mathbf{9 5 . 3}$</td>
<td style="text-align: left;">71.66</td>
<td style="text-align: left;">$\underline{65.96}$</td>
</tr>
<tr>
<td style="text-align: left;">WaveGAN-D</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\underline{96.25}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">LSSL</td>
<td style="text-align: left;">93.58</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">$\underline{93.96}$</td>
<td style="text-align: left;">$\mathbf{9 8 . 3 2}$</td>
<td style="text-align: left;">$\mathbf{9 6 . 3 0}$</td>
</tr>
</tbody>
</table>
<p>Table 6: (Pixel-level 1-D image classification) Comparison against reported test accuracies from prior works (Transformer, RNN, CNN, and SSM models). Extended results and citations in Appendix D.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">sMNIST</th>
<th style="text-align: left;">pMNIST</th>
<th style="text-align: left;">sCIFAR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">97.9</td>
<td style="text-align: left;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">95.11</td>
<td style="text-align: left;">63.01</td>
</tr>
<tr>
<td style="text-align: left;">r-LSTM</td>
<td style="text-align: left;">98.4</td>
<td style="text-align: left;">95.2</td>
<td style="text-align: left;">72.2</td>
</tr>
<tr>
<td style="text-align: left;">UR-LSTM</td>
<td style="text-align: left;">99.28</td>
<td style="text-align: left;">96.96</td>
<td style="text-align: left;">71.00</td>
</tr>
<tr>
<td style="text-align: left;">UR-GRU</td>
<td style="text-align: left;">99.27</td>
<td style="text-align: left;">96.51</td>
<td style="text-align: left;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">HiPPO-RNN</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">98.3</td>
<td style="text-align: left;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">LMU-FFT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">98.49</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">LipschitzRNN</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">96.3</td>
<td style="text-align: left;">64.2</td>
</tr>
<tr>
<td style="text-align: left;">TCN</td>
<td style="text-align: left;">99.0</td>
<td style="text-align: left;">97.2</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">TrellisNet</td>
<td style="text-align: left;">99.20</td>
<td style="text-align: left;">98.13</td>
<td style="text-align: left;">73.42</td>
</tr>
<tr>
<td style="text-align: left;">CKConv</td>
<td style="text-align: left;">99.32</td>
<td style="text-align: left;">98.54</td>
<td style="text-align: left;">63.74</td>
</tr>
<tr>
<td style="text-align: left;">LSSL</td>
<td style="text-align: left;">$\underline{99.53}$</td>
<td style="text-align: left;">$\mathbf{9 8 . 7 6}$</td>
<td style="text-align: left;">$\underline{84.65}$</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">$\mathbf{9 9 . 6 3}$</td>
<td style="text-align: left;">$\underline{98.70}$</td>
<td style="text-align: left;">$\mathbf{9 1 . 1 3}$</td>
</tr>
</tbody>
</table>
<p>Table 7: (CIFAR-10 density estimation) As a generic Table 8: (WikiText-103 language modeling) S4 apsequence model, S4 is competitive with previous autoregressive proaches the performance of Transformers with much models (in bits per dim.) while incorporating no 2D inductive faster generation. (Top) Transformer baseline which our bias, and has fast generation through its recurrence mode. implementation is based on, with attention replaced by S4. (Bottom) Attention-free models (RNNs and CNNs).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">bpd</th>
<th style="text-align: left;">2D bias</th>
<th style="text-align: left;">Images / sec</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">3.47</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">$0.32(1 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">Linear Transf.</td>
<td style="text-align: left;">3.40</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">$17.85(56 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">PixelCNN</td>
<td style="text-align: left;">3.14</td>
<td style="text-align: left;">2D conv.</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Row PixelRNN</td>
<td style="text-align: left;">3.00</td>
<td style="text-align: left;">2D BiLSTM</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">PixelCNN++</td>
<td style="text-align: left;">2.92</td>
<td style="text-align: left;">2D conv.</td>
<td style="text-align: left;">$\underline{19.19}(59.97 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">Image Transf.</td>
<td style="text-align: left;">2.90</td>
<td style="text-align: left;">2D local attn.</td>
<td style="text-align: left;">$0.54(1.7 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">PixelSNAIL</td>
<td style="text-align: left;">$\underline{2.85}$</td>
<td style="text-align: left;">2D conv. + attn.</td>
<td style="text-align: left;">$0.13(0.4 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">Sparse Transf.</td>
<td style="text-align: left;">$\mathbf{2 . 8 0}$</td>
<td style="text-align: left;">2D sparse attn.</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">S4 (base)</td>
<td style="text-align: left;">2.92</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">$\mathbf{2 0 . 8 4}(\mathbf{6 5 . 1} \times)$</td>
</tr>
<tr>
<td style="text-align: left;">S4 (large)</td>
<td style="text-align: left;">$\underline{2.85}$</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">$3.36(10.5 \times)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Params</th>
<th style="text-align: left;">Test ppl.</th>
<th style="text-align: left;">Tokens / sec</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">247 M</td>
<td style="text-align: left;">$\mathbf{2 0 . 5 1}$</td>
<td style="text-align: left;">$0.8 \mathrm{~K}(1 \times)$</td>
</tr>
<tr>
<td style="text-align: left;">GLU CNN</td>
<td style="text-align: left;">229 M</td>
<td style="text-align: left;">37.2</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AWD-QRNN</td>
<td style="text-align: left;">151 M</td>
<td style="text-align: left;">33.0</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">LSTM + Hebb.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">29.2</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">TrellisNet</td>
<td style="text-align: left;">180 M</td>
<td style="text-align: left;">29.19</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Dynamic Conv.</td>
<td style="text-align: left;">255 M</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">TaLK Conv.</td>
<td style="text-align: left;">240 M</td>
<td style="text-align: left;">23.3</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">S4</td>
<td style="text-align: left;">249 M</td>
<td style="text-align: left;">$\mathbf{2 0 . 9 5}$</td>
<td style="text-align: left;">$\mathbf{4 8 K}(\mathbf{6 0} \times)$</td>
</tr>
</tbody>
</table>
<p>models. The tasks we focus on (generative modeling, image classification, time-series forecasting) are considered as LRD tasks in the literature, and serve as additional validation that S4 handles LRDs efficiently.</p>
<p>Large-scale generative modeling. We investigate two well-studied image and text benchmarks to validate the scalability, flexibility, and efficiency of S4. These tasks require much larger models than our previous tasks - up to 250 M parameters.</p>
<p>First, CIFAR density estimation is a popular benchmark for autoregressive models, where images are flattened into a sequence of 3072 RGB subpixels that are predicted one by one. Table 7 shows that with no 2D inductive bias, S4 is competitive with the best models designed for this task.</p>
<p>Second, WikiText-103 is an established benchmark for language modeling, an important task for large-scale sequence models where tokens are predicted sequentially based on past context. Although RNNs were the model of choice for many years, Transformers are now the dominant model in such applications that contain data that is inherently discrete. We show that alternative models to Transformers can still be competitive in these settings. By simply taking a strong Transformer baseline [2] and replacing the self-attention layers, S4 substantially closes the gap to Transformers (within 0.8 ppl ), setting SoTA for attention-free models by over 2 ppl.</p>
<p>Fast autoregressive inference. A prominent limitation of autoregressive models is inference speed (e.g. generation), since they require a pass over the full context for every new sample. Several methods have been specifically crafted to overcome this limitation such as the Linear Transformer, a hybrid Transformer/RNN that switches to a stateful, recurrent view at inference time for speed.</p>
<p>As a stateful model, SSMs automatically have this ability (Fig. 1). By switching to its recurrent representation (Section 2.3), S4 requires constant memory and computation per time step - in contrast to standard autoregressive models which scale in the context length. On both CIFAR-10 and WikiText-103, we report the throughput of various models at generation time, with S4 around $60 \times$ faster than a vanilla Transformer on both tasks (details in Appendix D.3.3).</p>
<p>Sampling resolution change. As a continuous-time model, S4 automatically adapts to data sampled at different rates, a challenging setting for time series with a dedicated line of work [10, 35, 37]. Without re-training, S4 achieves $96.3 \%$ accuracy at $0.5 \times$ the frequency on Speech Commands 10 (Table 5), simply by changing its internal step size $\Delta$ (Section 2.3).</p>
<p>Learning with weaker inductive bias. Beyond our results on speech (Section 4.2), we further validate that S4 can be applied with minimal modifications on two domains that typically require specialized domainspecific preprocessing and architectures. First, we compare S4 to the Informer [50], a new Transformer architecture that uses a complex encoder-decoder designed for time-series forecasting problems. A simple application of S4 that treats forecasting as a masked sequence-to-sequence transformation (Fig. 5) outperforms the Informer and other baselines on 40/50 settings across 5 forecasting tasks. Notably, S4 is better on the</p>
<p>longest setting in each task, e.g. reducing MSE by $37 \%$ when forecasting 30 days of weather data (Table 9). Finally, we evaluate S4 on pixel-level sequential image classification tasks (Table 6), popular benchmarks which were originally LRD tests for RNNs [1]. Beyond LRDs, these benchmarks point to a recent effort of the ML community to solve vision problems with reduced domain knowledge, in the spirit of models such as Vision Transformers [12] and MLP-Mixer [41] which involve patch-based models that without 2-D inductive bias. Sequential CIFAR is a particularly challenging dataset where outside of SSMs, all sequence models have a gap of over $25 \%$ to a simple 2-D CNN. By contrast, S4 is competitive with a larger ResNet18 ( 7.9 M vs. 11.0 M parameters), both with ( $93.16 \%$ vs. $95.62 \%$ ) or without ( $91.12 \%$ vs. $89.46 \%$ ) data augmentation. Moreover, it is much more robust to other architectural choices (e.g. $90.46 \%$ vs. $79.52 \%$ when swapping BatchNorm for LayerNorm).</p>
<h1>4.4 SSM Ablations: the Importance of HiPPO</h1>
<p>A critical motivation of S4 was the use of the HiPPO matrices to initialize an SSM. We consider several simplifications of S 4 to ablate the importance of each of these components, including: (i) how important is the HiPPO initialization? (ii) how important is training the SSM on top of HiPPO? (iii) are the benefits of S4 captured by the NPLR parameterization without HiPPO?</p>
<p>As a simple testbed, all experiments in this section were performed on the sequential CIFAR-10 task, whicih we found transferred well to other settings. Models were constrained to at most 100 K trainable parameters and trained with a simple plateau learning rate scheduler and no regularization.</p>
<p>Unconstrained SSMs. We first investigate generic SSMs with various initializations. We consider a random Gaussian initialization (with variance scaled down until it did not NaN ), and the HiPPO initialization. We also consider a random diagonal Gaussian matrix as a potential structured method; parameterizing $\boldsymbol{A}$ as a diagonal matrix would allow substantial speedups without going through the complexity of S4's NPLR parameterization. We consider both freezing the $\boldsymbol{A}$ matrix and training it.
Fig. 3 shows both training and validation curves, from which we can make several observations. First, training the SSM improved all methods, particularly the randomly initialized ones. For all methods, training the SSM led to improvements in both training and validation curves.
Second, a large generalization gap exists between the initializations. In particular, note that when $\boldsymbol{A}$ is trained, all initializations are able to reach perfect training accuracy. However, their validation accuracies are separated by over $15 \%$.</p>
<p>NPLR SSMs. The previous experiment validates the importance of HiPPO in SSMs. This was the main motivation of the NPLR algorithm in S4, which utilizes structure of the HiPPO matrix (2) to make SSMs computationally feasible. Fig. 4a shows that random NPLR matrices still do not perform well, which validates that S4's effectiveness primarily comes from the HiPPO initialization, not the NPLR parameterization.
Finally, Fig. 4b considers the main ablations considered in this section (with trainable SSMs) and adds minor regularization. With 0.1 Dropout, the same trends still hold, and the HiPPO initialization - in other words, the full S4 method-achieves $84.27 \%$ test accuracy with just 100 K parameters.</p>
<p>Table 9: Univariate long sequence time-series forecasting results. Full results in Appendix D.3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">S4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LogTrans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LSTMa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DeepAR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ARIMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prophet</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;">ETTh $_{1}$</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">2.112</td>
<td style="text-align: center;">1.436</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">2.735</td>
<td style="text-align: center;">3.253</td>
</tr>
<tr>
<td style="text-align: center;">ETTh $_{2}$</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">2.030</td>
<td style="text-align: center;">1.721</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">2.878</td>
<td style="text-align: center;">1.044</td>
<td style="text-align: center;">3.355</td>
<td style="text-align: center;">4.664</td>
</tr>
<tr>
<td style="text-align: center;">ETTm $_{1}$</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">1.793</td>
<td style="text-align: center;">1.528</td>
<td style="text-align: center;">1.064</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">2.437</td>
<td style="text-align: center;">1.352</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">2.747</td>
<td style="text-align: center;">1.174</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">2.087</td>
<td style="text-align: center;">1.534</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">1.062</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">3.859</td>
<td style="text-align: center;">1.144</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">7.019</td>
<td style="text-align: center;">5.105</td>
<td style="text-align: center;">1.545</td>
<td style="text-align: center;">1.006</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">1.370</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">6.901</td>
<td style="text-align: center;">4.264</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: CIFAR-10 classification with unconstrained, real-valued SSMs with various initializations. (Left) Train accuracy. (Right) Validation accuracy.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: CIFAR-10 validation accuracy of SSMs with different initializations and parameterizations. (Left) NPLR parameterization with random versus HiPPO initialization. (Right) All methods considered in this section, including minor Dropout regularization. S4 achieves SotA accuracy on sequential CIFAR-10 with just 100K parameters.</p>
<h2>5 Conclusion</h2>
<p>We introduce S4, a sequence model that uses a new parameterization for the state space model's continuous-time, recurrent, and convolutional views to efficiently model LRDs in a principled manner. Results across established benchmarks evaluating a diverse range of data modalities and model capabilities suggest that S4 has the potential to be an effective general sequence modeling solution.</p>
<h3>Acknowledgments</h3>
<p>We thank Aditya Grover and Chris Cundy for helpful discussions about earlier versions of the method. We thank Simran Arora, Sabri Eyuboglu, Bibek Paudel, and Nimit Sohoni for valuable feedback on earlier drafts of this work. This work was done with the support of Google Cloud credits under HAI proposals 540994170283 and 578192719349. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of</p>
<p>the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.</p>
<h1>References</h1>
<p>[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016.
[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.
[3] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.
[4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019.
[5] Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
[7] Narsimha Chilkuri and Chris Eliasmith. Parallelizing legendre memory unit training. The International Conference on Machine Learning (ICML), 2021.
[8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In The International Conference on Learning Representations (ICLR), 2020.
[9] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017.
[10] Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous modeling of sporadically-observed time series. In Advances in Neural Information Processing Systems (NeurIPS), 2019 .
[11] Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR, 2019.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[13] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021.
[14] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022.
[15] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2013.
[16] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
[17] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In The International Conference on Machine Learning (ICML), 2020 .</p>
<p>[18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021.
[19] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022.
[20] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. How to train your hippo: State space models with generalized basis projections. arXiv preprint arXiv:2206.12037, 2022.
[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997 .
[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020.
[23] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. arXiv preprint arXiv:2005.08926, 2020.
[24] Mario Lezcano-Casado and David Martínez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In The International Conference on Machine Learning (ICML), 2019.
[25] Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (IndRNN): Building a longer and deeper RNN. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5457-5466, 2018.
[26] Vasileios Lioutas and Yuhong Guo. Time-aware large kernel convolutions. In International Conference on Machine Learning, pages 6172-6183. PMLR, 2020.
[27] Stephen Merity, Nitish Shirish Keskar, James Bradbury, and Richard Socher. Scalable language modeling: Wikitext-103 on a single gpu in 12 hours. SysML, 2018.
[28] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.
[29] Victor Pan. Structured matrices and polynomials: unified superfast algorithms. Springer Science \&amp; Business Media, 2001.
[30] Victor Pan. Fast approximate computations with cauchy matrices and polynomials. Mathematics of Computation, 86(308):2799-2826, 2017.
[31] Victor Y Pan. Transformations of matrix structures work again. Linear Algebra and Its Applications, $465: 107-138,2015$.
[32] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318, 2013.
[33] Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation memorization. The International Conference on Machine Learning (ICML), 2018.
[34] Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017.
[35] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611, 2021.</p>
<p>[36] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In The International Conference on Learning Representations (ICLR), 2022.
[37] Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pages 5321-5331, 2019 .
[38] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies. The International Conference on Machine Learning (ICML), 2021.
[39] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017 .
[40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=qVyeW-grC2k.
[41] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.
[42] Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependencies in RNNs with auxiliary losses. In The International Conference on Machine Learning (ICML), 2018.
[43] Arnold Tustin. A method of analysing the behaviour of linear systems in terms of time series. Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms, 94(1): $130-142,1947$.
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
[45] Aaron Voelker, Ivana Kajić, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 15544-15553, 2019 .
[46] Aaron Russell Voelker. Dynamical systems in spiking neuromorphic hardware. PhD thesis, University of Waterloo, 2019.
[47] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. ArXiv, abs/1804.03209, 2018.
[48] Max A Woodbury. Inverting modified matrices. Memorandum report, 42:106, 1950.
[49] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In The International Conference on Learning Representations (ICLR), 2019.
[50] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, volume 35, pages 1110611115. AAAI Press, 2021.</p>
<h1>A Discussion</h1>
<p>Related Work. Our work is most closely related to a line of work originally motivated by a particular biologically-inspired SSM, which led to mathematical models for addressing LRDs. Voelker et al. [45], Voelker [46] derived a non-trainable SSM motivated from approximating a neuromorphic spiking model, and Chilkuri and Eliasmith [7] showed that it could be sped up at train time with a convolutional view. Gu et al. [16] extended this special case to a general continuous-time function approximation framework with several more special cases of $\boldsymbol{A}$ matrices designed for long-range dependencies. However, instead of using a true SSM, all of these works fixed a choice of $\boldsymbol{A}$ and built RNNs around it. Most recently, Gu et al. [18] used the full (1) explicitly as a deep SSM model, exploring new conceptual views of SSMs, as well as allowing $\boldsymbol{A}$ to be trained. As mentioned in Section 1, their method used a naive instantiation of SSMs that suffered from an additional factor of $N$ in memory and $N^{2}$ in computation.
Beyond this work, our technical contributions (Section 3) on the S4 parameterization and algorithms are applicable to a broader family of SSMs including these investigated in prior works, and our techniques for working with these models may be of independent interest.</p>
<p>Implementation. The computational core of S4's training algorithm is the Cauchy kernel discussed in Sections 3.2 and 3.3 and Appendix C.3. As described in Appendix C. 3 Proposition 5, there are many algorithms for it with differing computational complexities and sophistication. Our current implementation of S4 actually uses the naive $O(N L)$ algorithm which is easily parallelized on GPUs and has more easily accessible libraries allowing it to be implemented; we leverage the pykeops library for memory-efficient kernel operations. However, this library is a much more general library that may not be optimized for the Cauchy kernels used here, and we believe that a dedicated CUDA implementation can be more efficient. Additionally, as discussed in this work, there are asymptotically faster and numerically stable algorithms for the Cauchy kernel (Proposition 5). However, these algorithms are currently not implemented for GPUs due to a lack of previous applications that require them. We believe that more efficient implementations of these self-contained computational kernels are possible, and that S4 (and SSMs at large) may have significant room for further improvements in efficiency.</p>
<p>Limitations and Future Directions. In this work, we show that S4 can address a wide variety of data effectively. However, it may not necessarily be the most suitable model for all types of data. For example, Table 8 still found a gap compared to Transformers for language modeling. An interesting future direction is exploring combinations of S 4 with other sequence models to complement their strengths. We are excited about other directions, including continuing to explore the benefits of S4 on audio data (e.g. pre-training or generation settings), and generalizing HiPPO and S4 to higher-dimensional data for image and video applications.</p>
<h2>B Numerical Instability of LSSL</h2>
<p>This section proves the claims made in Section 3.1 about prior work. We first derive the explicit diagonalization of the HiPPO matrix, confirming its instability because of exponentially large entries. We then discuss the proposed theoretically fast algorithm from [18] (Theorem 2) and show that it also involves exponentially large terms and thus cannot be implemented.</p>
<h1>B. 1 HiPPO Diagonalization</h1>
<p>Proof of Lemma 3.2. The HiPPO matrix (2) is equal, up to sign and conjugation by a diagonal matrix, to</p>
<p>$$
\begin{aligned}
\boldsymbol{A} &amp; =\left[\begin{array}{cccccccc}
1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \
-1 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; \
1 &amp; -3 &amp; 3 &amp; &amp; &amp; &amp; &amp; \
-1 &amp; 3 &amp; -5 &amp; 4 &amp; &amp; &amp; &amp; \
1 &amp; -3 &amp; 5 &amp; -7 &amp; 5 &amp; &amp; &amp; \
-1 &amp; 3 &amp; -5 &amp; 7 &amp; -9 &amp; 6 &amp; &amp; \
1 &amp; -3 &amp; 5 &amp; -7 &amp; 9 &amp; -11 &amp; 7 &amp; \
-1 &amp; 3 &amp; -5 &amp; 7 &amp; -9 &amp; 11 &amp; -13 &amp; 8 \
\vdots &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \ddots
\end{array}\right] \
\boldsymbol{A}_{n k} &amp; =\left{\begin{array}{ll}
(-1)^{n-k}(2 k+1) &amp; n&gt;k \
k+1 &amp; n=k \
0 &amp; n&lt;k
\end{array}\right.
\end{aligned}
$$</p>
<p>Our goal is to show that this $\boldsymbol{A}$ is diagonalized by the matrix</p>
<p>$$
\boldsymbol{V}=\binom{i+j}{i-j}_{i j}=\left[\begin{array}{ccccccc}
1 &amp; &amp; &amp; &amp; &amp; \
1 &amp; 1 &amp; &amp; &amp; &amp; \
1 &amp; 3 &amp; 1 &amp; &amp; &amp; \
1 &amp; 6 &amp; 5 &amp; 1 &amp; &amp; \
1 &amp; 10 &amp; 15 &amp; 7 &amp; 1 &amp; \
1 &amp; 15 &amp; 35 &amp; 28 &amp; 9 &amp; 1 \
\vdots &amp; &amp; &amp; &amp; &amp; &amp; \ddots
\end{array}\right]
$$</p>
<p>or in other words that columns of this matrix are eigenvectors of $\boldsymbol{A}$.
Concretely, we will show that the $j$-th column of this matrix $\boldsymbol{v}^{(j)}$ with elements</p>
<p>$$
\boldsymbol{v}_{i}^{(j)}=\left{\begin{array}{ll}
0 &amp; i&lt;j \
\binom{i+j}{i-j}=\binom{i+j}{2 j} &amp; i \geq j
\end{array}\right.
$$</p>
<p>is an eigenvector with eigenvalue $j+1$. In other words we must show that for all indices $k \in[N]$,</p>
<p>$$
\left(\boldsymbol{A} \boldsymbol{v}^{(j)}\right)<em i="i">{k}=\sum</em>} \boldsymbol{A<em i="i">{k i} \boldsymbol{v}</em>
$$}^{(j)}=(j+1) \boldsymbol{v}_{k}^{(j)</p>
<p>If $k&lt;j$, then for all $i$ inside the sum, either $k&lt;i$ or $i&lt;j$. In the first case $\boldsymbol{A}<em i="i">{k i}=0$ and in the second case $\boldsymbol{v}</em>=0$, so both sides of equation (7) are equal to 0 .
It remains to show the case $k \geq j$, which proceeds by induction on $k$. Expanding equation (7) using the formula for $\boldsymbol{A}$ yields}^{(j)</p>
<p>$$
(\boldsymbol{A} \boldsymbol{v})<em i="i">{k}^{(j)}=\sum</em>} \boldsymbol{A<em i="i">{k i} \boldsymbol{v}</em>
$$}^{(j)}=\sum_{i=j}^{k-1}(-1)^{k-i}(2 i+1)\binom{i+j}{2 j}+(k+1)\binom{k+j}{2 j</p>
<p>In the base case $k=j$, the sum disappears and we are left with $\left(\boldsymbol{A} \boldsymbol{v}^{(j)}\right)<em j="j">{j}=(j+1)\binom{2 j}{2 j}=(j+1) \boldsymbol{v}</em>$, as desired.
Otherwise, the sum for $(\boldsymbol{A} \boldsymbol{v})}^{(j)<em k-1="k-1">{k}^{(j)}$ is the same as the sum for $(\boldsymbol{A} \boldsymbol{v})</em>$ but with sign reversed and a few edge}^{(j)</p>
<p>terms. The result follows from applying the inductive hypothesis and algebraic simplification:</p>
<p>$$
\begin{aligned}
(\boldsymbol{A} \boldsymbol{v})<em k-1="k-1">{k}^{(j)} &amp; =-(\boldsymbol{A} \boldsymbol{v})</em> \
&amp; =-(j+1)\binom{k-1+j}{2 j}-(k-1)\binom{k-1+j}{2 j}+(k+1)\binom{k+j}{2 j} \
&amp; =-(j+k)\binom{k-1+j}{2 j}+(k+1)\binom{k+j}{2 j} \
&amp; =-(j+k) \frac{(k-1+j)!}{(k-1-j)!(2 j)!}+(k+1)\binom{k+j}{2 j} \
&amp; =-\frac{(k+j)!}{(k-1-j)!(2 j)!}+(k+1)\binom{k+j}{2 j} \
&amp; =-(k-j) \frac{(k+j)!}{(k-j)!(2 j)!}+(k+1)\binom{k+j}{2 j} \
&amp; =(j-k)(k+1)\binom{k+j}{2 j}+(k+1)\binom{k+j}{2 j} \
&amp; =(j+1) \boldsymbol{v}_{k}^{(j)}
\end{aligned}
$$}^{(j)}-(2 k-1)\binom{k-1+j}{2 j}+k\binom{k-1+j}{2 j}+(k+1)\binom{k+j}{2 j</p>
<h1>B. 2 Fast but Unstable LSSL Algorithm</h1>
<p>Instead of diagonalization, Gu et al. [18, Theorem 2] proposed a sophisticated fast algorithm to compute</p>
<p>$$
K_{L}(\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}, \overline{\boldsymbol{C}})=(\overline{\boldsymbol{C} \boldsymbol{B}}, \overline{\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}}, \ldots, \overline{\boldsymbol{C}} \boldsymbol{A}^{L-1} \overline{\boldsymbol{B}})
$$</p>
<p>This algorithm runs in $O\left(N \log ^{2} N+L \log L\right)$ operations and $O(N+L)$ space. However, we now show that this algorithm is also numerically unstable.
There are several reasons for the instability of this algorithm, but most directly we can pinpoint a particular intermediate quantity that they use.
Definition 1. The fast LSSL algorithm computes coefficients of $p(x)$, the characteristic polynomial of $A$, as an intermediate computation. Additionally, it computes the coefficients of its inverse, $p(x)^{-1}\left(\bmod x^{L}\right)$.</p>
<p>We now claim that this quantity is numerically unfeasible. We narrow down to the case when $\overline{\boldsymbol{A}}=\boldsymbol{I}$ is the identity matrix. Note that this case is actually in some sense the most typical case: when discretizing the continuous-time SSM to discrete-time by a step-size $\Delta$, the discretized transition matrix $\overline{\boldsymbol{A}}$ is brought closer to the identity. For example, with the Euler discretization $\overline{\boldsymbol{A}}=\boldsymbol{I}+\Delta \boldsymbol{A}$, we have $\overline{\boldsymbol{A}} \rightarrow \boldsymbol{I}$ as the step size $\Delta \rightarrow 0$.
Lemma B.1. When $\overline{\boldsymbol{A}}=\boldsymbol{I}$, the fast LSSL algorithm requires computing terms exponentially large in $N$.
Proof. The characteristic polynomial of $\boldsymbol{I}$ is</p>
<p>$$
p(x)=\operatorname{det}|\boldsymbol{I}-x \boldsymbol{I}|=(1-x)^{N}
$$</p>
<p>These coefficients have size up to $\left(\frac{N}{2}\right) \approx \frac{2^{N}}{\sqrt{\pi N / 2}}$.
The inverse of $p(x)$ has even larger coefficients. It can be calculated in closed form by the generalized binomial formula:</p>
<p>$$
(1-x)^{-N}=\sum_{k=0}^{\infty}\binom{N+k-1}{k} x^{k}
$$</p>
<p>Taking this $\left(\bmod x^{L}\right)$, the largest coefficient is</p>
<p>$$
\binom{N+L-2}{L-1}=\binom{N+L-2}{N-1}=\frac{(L-1)(L-2) \ldots(L-N+1)}{(N-1)!}
$$</p>
<p>When $L=N-1$ this is</p>
<p>$$
\binom{2(N-1)}{N-1} \approx \frac{2^{2 N}}{\sqrt{\pi N}}
$$</p>
<p>already larger than the coefficients of $(1-x)^{N}$, and only increases as $L$ grows.</p>
<h1>C S4 Algorithm Details</h1>
<p>This section proves the results of Section 3.3, providing complete details of our efficient algorithms for S4.
Appendices C. 1 to C. 3 prove Theorems 1 to 3 respectively.</p>
<h2>C. 1 NPLR Representations of HiPPO Matrices</h2>
<p>We first prove Theorem 1, showing that all HiPPO matrices for continuous-time memory fall under the S4 normal plus low-rank (NPLR) representation.</p>
<p>Proof of Theorem 1. We consider each of the three cases HiPPO-LagT, HiPPO-LegT, and HiPPO-LegS separately. Note that the primary HiPPO matrix defined in this work (equation (2)) is the HiPPO-LegT matrix.</p>
<p>HiPPO-LagT. The HiPPO-LagT matrix is simply</p>
<p>$$
\begin{aligned}
\boldsymbol{A}_{n k} &amp; =\left{\begin{array}{lll}
0 &amp; n<k \\
-\frac{1}{2} & n=k \\
-1 & n>k
\end{array}\right. \
\boldsymbol{A} &amp; =-\left[\begin{array}{cccc}
\frac{1}{2} &amp; &amp; &amp; \cdots \
1 &amp; \frac{1}{2} &amp; &amp; \
1 &amp; 1 &amp; \frac{1}{2} &amp; \
1 &amp; 1 &amp; 1 &amp; \frac{1}{2}
\end{array}\right]
\end{aligned}
$$</p>
<p>Adding the matrix of all $\frac{1}{2}$, which is rank 1 , yields</p>
<p>$$
-\left[\begin{array}{cccc}
-\frac{1}{2} &amp; -\frac{1}{2} &amp; -\frac{1}{2} \
&amp; &amp; -\frac{1}{2} &amp; -\frac{1}{2} \
\frac{1}{2} &amp; \frac{1}{2} &amp; &amp; -\frac{1}{2} \
\frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{2} &amp; &amp;
\end{array}\right]
$$</p>
<p>This matrix is now skew-symmetric. Skew-symmetric matrices are a particular case of normal matrices with pure-imaginary eigenvalues.
Gu et al. [16] also consider a case of HiPPO corresponding to the generalized Laguerre polynomials that generalizes the above HiPPO-LagT case. In this case, the matrix $\boldsymbol{A}$ (up to conjugation by a diagonal matrix) ends up being close to the above matrix, but with a different element on the diagonal. After adding the rank-1 correction, it becomes the above skew-symmetric matrix plus a multiple of the identity. Thus after diagonalization by the same matrix as in the LagT case, it is still reduced to diagonal plus low-rank (DPLR) form, where the diagonal is now pure imaginary plus a real constant.</p>
<p>HiPPO-LegS. We restate the formula from equation (2) for convenience.</p>
<p>$$
\boldsymbol{A}_{n k}=-\left{\begin{array}{ll}
(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} &amp; \text { if } n&gt;k \
n+1 &amp; \text { if } n=k \
0 &amp; \text { if } n&lt;k
\end{array} .\right.
$$</p>
<p>Adding $\frac{1}{2}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2}$ to the whole matrix gives</p>
<p>$$
-\left{\begin{array}{ll}
\frac{1}{2}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} &amp; \text { if } n&gt;k \
\frac{1}{2} &amp; \text { if } n=k \
-\frac{1}{2}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} &amp; \text { if } n&lt;k
\end{array}\right.
$$</p>
<p>Note that this matrix is not skew-symmetric, but is $\frac{1}{2} \boldsymbol{I}+\boldsymbol{S}$ where $\boldsymbol{S}$ is a skew-symmetric matrix. This is diagonalizable by the same unitary matrix that diagonalizes $\boldsymbol{S}$.</p>
<h1>HiPPO-LegT.</h1>
<p>Up to the diagonal scaling, the LegT matrix is</p>
<p>$$
\boldsymbol{A}=-\left[\begin{array}{ccccc}
1 &amp; -1 &amp; 1 &amp; -1 &amp; \ldots \
1 &amp; 1 &amp; -1 &amp; 1 &amp; \
1 &amp; 1 &amp; 1 &amp; -1 &amp; \
1 &amp; 1 &amp; 1 &amp; 1 &amp; \
\vdots &amp; &amp; &amp; &amp; \ddots
\end{array}\right]
$$</p>
<p>By adding -1 to this matrix and then the matrix</p>
<p>$$
\left[\begin{array}{lll}
2 &amp; &amp; \
&amp; 2 &amp; \
2 &amp; &amp; 2
\end{array}\right]
$$</p>
<p>the matrix becomes</p>
<p>$$
\left[\begin{array}{ccc}
-2 &amp; &amp; -2 \
2 &amp; &amp; \
&amp; 2 &amp; \
2 &amp; &amp; 2
\end{array}\right]
$$</p>
<p>which is skew-symmetric. In fact, this matrix is the inverse of the Chebyshev Jacobi.
An alternative way to see this is as follows. The LegT matrix is the inverse of the matrix</p>
<p>$$
\left[\begin{array}{rrrr}
-1 &amp; 1 &amp; &amp; 0 \
-1 &amp; &amp; 1 &amp; \
&amp; -1 &amp; &amp; 1 \
&amp; &amp; -1 &amp; -1
\end{array}\right]
$$</p>
<p>This can obviously be converted to a skew-symmetric matrix by adding a rank 2 term. The inverses of these matrices are also rank-2 differences from each other by the Woodbury identity.
A final form is</p>
<p>$$
\left[\begin{array}{cccc}
-1 &amp; 1 &amp; -1 &amp; 1 \
-1 &amp; -1 &amp; 1 &amp; -1 \
-1 &amp; -1 &amp; -1 &amp; 1 \
-1 &amp; -1 &amp; -1 &amp; -1
\end{array}\right]+\left[\begin{array}{llll}
1 &amp; 0 &amp; 1 &amp; 0 \
0 &amp; 1 &amp; 0 &amp; 1 \
1 &amp; 0 &amp; 1 &amp; 0 \
0 &amp; 1 &amp; 0 &amp; 1
\end{array}\right]=\left[\begin{array}{cccc}
0 &amp; 1 &amp; 0 &amp; 1 \
-1 &amp; 0 &amp; 1 &amp; 0 \
0 &amp; -1 &amp; 0 &amp; 1 \
-1 &amp; 0 &amp; -1 &amp; 0
\end{array}\right]
$$</p>
<p>This has the advantage that the rank-2 correction is symmetric (like the others), but the normal skewsymmetric matrix is now 2-quasiseparable instead of 1-quasiseparable.</p>
<h1>C. 2 Computing the S4 Recurrent View</h1>
<p>We prove Theorem 2 showing the efficiency of the S4 parameterization for computing one step of the recurrent representation (Section 2.3).
Recall that without loss of generality, we can assume that the state matrix $\boldsymbol{A}=\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{*}$ is diagonal plus low-rank (DPLR), potentially over $\mathbb{C}$. Our goal in this section is to explicitly write out a closed form for the discretized matrix $\overline{\boldsymbol{A}}$.
Recall from equation (3) that</p>
<p>$$
\begin{aligned}
&amp; \overline{\boldsymbol{A}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1}(\boldsymbol{I}+\Delta / 2 \cdot \boldsymbol{A}) \
&amp; \overline{\boldsymbol{B}}=(\boldsymbol{I}-\Delta / 2 \cdot \boldsymbol{A})^{-1} \Delta \boldsymbol{B}
\end{aligned}
$$</p>
<p>We first simplify both terms in the definition of $\overline{\boldsymbol{A}}$ independently.
Forward discretization. The first term is essentially the Euler discretization motivated in Section 2.3.</p>
<p>$$
\begin{aligned}
\boldsymbol{I}+\frac{\Delta}{2} \boldsymbol{A} &amp; =\boldsymbol{I}+\frac{\Delta}{2}\left(\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{<em>}\right) \
&amp; =\frac{\Delta}{2}\left[\frac{2}{\Delta} \boldsymbol{I}+\left(\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{</em>}\right)\right] \
&amp; =\frac{\Delta}{2} \boldsymbol{A}_{\mathbf{0}}
\end{aligned}
$$</p>
<p>where $\boldsymbol{A}_{\mathbf{0}}$ is defined as the term in the final brackets.
Backward discretization. The second term is known as the Backward Euler's method. Although this inverse term is normally difficult to deal with, in the DPLR case we can simplify it using Woodbury's Identity (Proposition 4).</p>
<p>$$
\begin{aligned}
\left(\boldsymbol{I}-\frac{\Delta}{2} \boldsymbol{A}\right)^{-1} &amp; =\left(\boldsymbol{I}-\frac{\Delta}{2}\left(\boldsymbol{\Lambda}-\boldsymbol{P} \boldsymbol{Q}^{<em>}\right)\right)^{-1} \
&amp; =\frac{2}{\Delta}\left[\frac{2}{\Delta}-\boldsymbol{\Lambda}+\boldsymbol{P} \boldsymbol{Q}^{</em>}\right]^{-1} \
&amp; =\frac{2}{\Delta}\left[\boldsymbol{D}-\boldsymbol{D} \boldsymbol{P}\left(\boldsymbol{I}+\boldsymbol{Q}^{<em>} \boldsymbol{D} \boldsymbol{P}\right)^{-1} \boldsymbol{Q}^{</em>} \boldsymbol{D}\right] \
&amp; =\frac{2}{\Delta} \boldsymbol{A}_{\mathbf{1}}
\end{aligned}
$$</p>
<p>where $\boldsymbol{D}=\left(\frac{2}{\Delta}-\boldsymbol{\Lambda}\right)^{-1}$ and $\boldsymbol{A}_{\mathbf{1}}$ is defined as the term in the final brackets. Note that $\left(1+\boldsymbol{Q}^{*} \boldsymbol{D} \boldsymbol{P}\right)$ is actually a scalar in the case when the low-rank term has rank 1.
S4 Recurrence. Finally, the full bilinear discretization can be rewritten in terms of these matrices as</p>
<p>$$
\begin{aligned}
\overline{\boldsymbol{A}} &amp; =\boldsymbol{A}<em _mathbf_0="\mathbf{0">{\mathbf{1}} \boldsymbol{A}</em> \
\overline{\boldsymbol{B}} &amp; =\frac{2}{\Delta} \boldsymbol{A}}<em _mathbf_1="\mathbf{1">{\mathbf{1}} \Delta \boldsymbol{B}=2 \boldsymbol{A}</em>
\end{aligned}
$$}} \boldsymbol{B</p>
<p>The discrete-time SSM (3) becomes</p>
<p>$$
\begin{aligned}
x_{k} &amp; =\overline{\boldsymbol{A}} x_{k-1}+\overline{\boldsymbol{B}} u_{k} \
&amp; =\boldsymbol{A}<em _mathbf_0="\mathbf{0">{\mathbf{1}} \boldsymbol{A}</em>}} x_{k-1}+2 \boldsymbol{A<em k="k">{\mathbf{1}} \boldsymbol{B} u</em> \
y_{k} &amp; =\boldsymbol{C} x_{k}
\end{aligned}
$$</p>
<p>Note that $\boldsymbol{A}<em _mathbf_1="\mathbf{1">{\mathbf{0}}, \boldsymbol{A}</em>$ are accessed only through matrix-vector multiplications. Since they are both DPLR, they have $O(N)$ matrix-vector multiplication, showing Theorem 2 .}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Refers to global (in the sequence length) and depthwise-separable convolutions, similar to the convolution version of S4.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
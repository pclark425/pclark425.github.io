<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6743 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6743</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6743</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-256ef1f8d0ea2982cc50d3e85e5f1b4920f037fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/256ef1f8d0ea2982cc50d3e85e5f1b4920f037fe" target="_blank">True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</a></p>
                <p><strong>Paper Venue:</strong> STARSEM</p>
                <p><strong>Paper TL;DR:</strong> A benchmark consisting of 191 long-form mystery narratives constructed as detective puzzles, showing that GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art G PT-4 solves only 38% of puzzles.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form (1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the “5 Minute Mystery” platform and include a multiple-choice question for evaluation. Only 47% of humans solve a puzzle successfully on average, while the best human solvers achieve over 80% success rate. We show that GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs’ abilities.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6743.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6743.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (FeedME) - Vanilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (FeedME) using Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruct-tuned GPT-3.5 (FeedME) evaluated with a direct instruction + immediate-answer prompt (no chain-of-thought); reported to perform close to random on the detective benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (FeedME)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>direct</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form abductive detective puzzles (~1200-word narratives) with multiple-choice answers (identify guilty suspect or correct explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>28.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Random guess (0.24) / Human average (0.47)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Vanilla prompting yields performance close to random; authors report no correlation between text length or human solve rate and GPT correctness. Ablation (golden CoT) shows that difficulty is not only generating CoT but making final inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6743.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (PPO) - Vanilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (PPO) using Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RLHF-calibrated GPT-3.5 (PPO) evaluated with direct instruction + immediate-answer prompt; performance similar to random on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>direct</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form abductive detective puzzles (multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>26.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Random guess (0.24) / Human average (0.47)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Vanilla prompting with PPO-tuned model still near-random; authors note overall GPT-3.5 variants struggle on deep abductive inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6743.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Vanilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 using Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-4 evaluated with direct instruction + immediate-answer prompt; performs slightly above random but well below human average on the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not publicly disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Vanilla prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>direct</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form abductive detective puzzles requiring selecting the correct explanation/culprit from choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>27.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Random guess (0.24) / Human average (0.47)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Vanilla GPT-4 performance is still substantially below human average; suggests that immediate-answer prompting is insufficient for deep abductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6743.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (FeedME) - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (FeedME) using Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (FeedME) prompted to generate an explicit step-by-step Chain-of-Thought before answering; yields no improvement (slight decrease) compared to Vanilla on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (FeedME)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Abductive reasoning in long-form mystery narratives (generate chain-of-thought then final answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>26.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Vanilla prompting (same model, 28.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-2.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT prompting did not help GPT-3.5 (FeedME); authors note CoT can hurt or not help smaller/weaker models and that GPT-3.5 may be insufficiently powerful to generate useful CoTs for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6743.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (PPO) - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (PPO) using Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (PPO) with CoT prompting shows small/no gains relative to Vanilla on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate step-by-step reasoning before selecting the correct multiple-choice answer for detective puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>29.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Vanilla prompting (same model, 26.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Small improvement over Vanilla for PPO variant; aligns with prior observation that stronger models benefit more from CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6743.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 using Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 prompted to produce a Chain-of-Thought before answering; shows the best zero-shot CoT performance among tested models but still below average human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not publicly disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form abductive reasoning with explicit stepwise chain-of-thought generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>38.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Vanilla prompting (GPT-4, 27.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>11.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT provides a notable improvement for GPT-4 (≈11 percentage points); authors state CoT helps stronger models while offering little or negative benefit for weaker ones. Also, GPT-4 and human subjects show similar case-level difficulty concordance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6743.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (PPO) - Golden CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (PPO) with Golden (ground-truth) Chains-of-Thought provided in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (PPO) was given the author's ground-truth solution explanations (golden CoTs) as part of the prompt to simplify the task; performance improves but remains below top human solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Golden Chain-of-Thought (ground-truth CoT in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (ground-truth-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inference from long golden CoTs to select final answer; tests final-step inference when full reasoning is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>63.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT prompting (same model, 29.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>34.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Providing ground-truth reasoning greatly improves PPO model performance, indicating that a major failure mode is generating or using correct CoTs; nevertheless, the model still occasionally fails to make trivial final inferences from an explicit solution.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6743.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Golden CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Golden (ground-truth) Chains-of-Thought provided in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 given authors' solution explanations in the prompt achieves performance comparable to top human solvers on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not publicly disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Golden Chain-of-Thought (ground-truth CoT in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (ground-truth-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final inference from provided ground-truth step-by-step solution to select the correct multiple-choice answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>83.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT prompting (GPT-4, 38.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>45.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>With golden CoTs GPT-4 reaches levels comparable to top human solvers (authors report top humans 80-90%); this implies that GPT-4 can perform the final answer extraction when presented with an explicit correct reasoning trace, but struggles to generate or discover that trace unaided.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6743.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies evaluated: Vanilla, Chain-of-Thought (CoT), Golden CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study compares three prompting/reasoning methods (direct answer, model-generated CoT, and ground-truth CoT-in-prompt) across models to analyze their effects on abductive reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (GPT-3.5 variants, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Vanilla; Chain-of-Thought; Golden Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how different prompting styles (no CoT, model-generated CoT, and provided ground-truth CoT) affect solving long-form abductive detective puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors observe that CoT helps stronger models (GPT-4) but not necessarily smaller ones (GPT-3.5); providing golden CoTs yields large gains for all models but particularly closes the gap for GPT-4 to human top-solvers. Overall, the study treats method variety as an ablation to probe where failures occur (CoT generation vs final inference).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6743.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmark - True Detective (5MM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>True Detective: 5 Minute Mystery long-form abductive reasoning benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of 191 long-form (≈1200 words on average) detective puzzles sourced from the 5 Minute Mystery platform, each with multiple-choice answers and an author-provided solution (golden CoT); human average solve rate ~47%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>n/a (dataset/benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice abductive reasoning over long-form mystery narratives (identify guilty suspect or correct explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>47.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Dataset includes golden CoTs (author solutions) and rich human attempt statistics (average solve rate 47%, top human solvers 80-90%, average attempts ~1984 per puzzle). Authors note dataset is significantly harder than prior short-form abductive datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6743.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human baseline - average</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average human solver performance on 5MM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate human baseline computed from first attempts recorded on 5 Minute Mystery platform; authors use this as the principal human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human average</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>human abductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solving the same long-form abductive detective puzzles as the models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>47.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Random guess (0.24) and model performances</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>23.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report average human solve rate 47% (first attempts), while top human solvers achieve ~80-90%; used to contextualize model gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6743.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6743.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random guess baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random guessing baseline (dataset-level)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline representing chance performance on the multiple-choice puzzles (~24% overall, reflecting mix of 3-5 options per puzzle).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Random guess</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>random choice</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>True Detective (5 Minute Mystery)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chance-level performance on the multiple-choice abductive puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors use this baseline to show many model settings perform close to chance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Abductive commonsense reasoning <em>(Rating: 1)</em></li>
                <li>A corpus and cloze evaluation for deeper understanding of commonsense stories <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6743",
    "paper_id": "paper-256ef1f8d0ea2982cc50d3e85e5f1b4920f037fe",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (FeedME) - Vanilla",
            "name_full": "GPT-3.5 (FeedME) using Vanilla prompting",
            "brief_description": "Instruct-tuned GPT-3.5 (FeedME) evaluated with a direct instruction + immediate-answer prompt (no chain-of-thought); reported to perform close to random on the detective benchmark.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (FeedME)",
            "model_size": "≈175B",
            "reasoning_method_name": "Vanilla prompting",
            "reasoning_method_type": "direct",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Long-form abductive detective puzzles (~1200-word narratives) with multiple-choice answers (identify guilty suspect or correct explanation).",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 28.0,
            "comparison_target_method": "Random guess (0.24) / Human average (0.47)",
            "performance_difference": 4.0,
            "statistical_significance": false,
            "analysis_notes": "Vanilla prompting yields performance close to random; authors report no correlation between text length or human solve rate and GPT correctness. Ablation (golden CoT) shows that difficulty is not only generating CoT but making final inferences.",
            "ablation_study_present": true,
            "uuid": "e6743.0",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3.5 (PPO) - Vanilla",
            "name_full": "GPT-3.5 (PPO) using Vanilla prompting",
            "brief_description": "RLHF-calibrated GPT-3.5 (PPO) evaluated with direct instruction + immediate-answer prompt; performance similar to random on the benchmark.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (PPO)",
            "model_size": "≈175B",
            "reasoning_method_name": "Vanilla prompting",
            "reasoning_method_type": "direct",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Long-form abductive detective puzzles (multiple-choice).",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 26.0,
            "comparison_target_method": "Random guess (0.24) / Human average (0.47)",
            "performance_difference": 2.0,
            "statistical_significance": false,
            "analysis_notes": "Vanilla prompting with PPO-tuned model still near-random; authors note overall GPT-3.5 variants struggle on deep abductive inferences.",
            "ablation_study_present": true,
            "uuid": "e6743.1",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-4 - Vanilla",
            "name_full": "GPT-4 using Vanilla prompting",
            "brief_description": "OpenAI GPT-4 evaluated with direct instruction + immediate-answer prompt; performs slightly above random but well below human average on the dataset.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not publicly disclosed",
            "reasoning_method_name": "Vanilla prompting",
            "reasoning_method_type": "direct",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Long-form abductive detective puzzles requiring selecting the correct explanation/culprit from choices.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 27.0,
            "comparison_target_method": "Random guess (0.24) / Human average (0.47)",
            "performance_difference": 3.0,
            "statistical_significance": false,
            "analysis_notes": "Vanilla GPT-4 performance is still substantially below human average; suggests that immediate-answer prompting is insufficient for deep abductive tasks.",
            "ablation_study_present": true,
            "uuid": "e6743.2",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3.5 (FeedME) - CoT",
            "name_full": "GPT-3.5 (FeedME) using Chain-of-Thought prompting",
            "brief_description": "GPT-3.5 (FeedME) prompted to generate an explicit step-by-step Chain-of-Thought before answering; yields no improvement (slight decrease) compared to Vanilla on this benchmark.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (FeedME)",
            "model_size": "≈175B",
            "reasoning_method_name": "Chain-of-Thought (CoT) prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Abductive reasoning in long-form mystery narratives (generate chain-of-thought then final answer).",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 26.0,
            "comparison_target_method": "Vanilla prompting (same model, 28.0)",
            "performance_difference": -2.0,
            "statistical_significance": false,
            "analysis_notes": "CoT prompting did not help GPT-3.5 (FeedME); authors note CoT can hurt or not help smaller/weaker models and that GPT-3.5 may be insufficiently powerful to generate useful CoTs for this task.",
            "ablation_study_present": true,
            "uuid": "e6743.3",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3.5 (PPO) - CoT",
            "name_full": "GPT-3.5 (PPO) using Chain-of-Thought prompting",
            "brief_description": "GPT-3.5 (PPO) with CoT prompting shows small/no gains relative to Vanilla on the benchmark.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (PPO)",
            "model_size": "≈175B",
            "reasoning_method_name": "Chain-of-Thought (CoT) prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Generate step-by-step reasoning before selecting the correct multiple-choice answer for detective puzzles.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 29.0,
            "comparison_target_method": "Vanilla prompting (same model, 26.0)",
            "performance_difference": 3.0,
            "statistical_significance": false,
            "analysis_notes": "Small improvement over Vanilla for PPO variant; aligns with prior observation that stronger models benefit more from CoT prompting.",
            "ablation_study_present": true,
            "uuid": "e6743.4",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-4 - CoT",
            "name_full": "GPT-4 using Chain-of-Thought prompting",
            "brief_description": "GPT-4 prompted to produce a Chain-of-Thought before answering; shows the best zero-shot CoT performance among tested models but still below average human performance.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not publicly disclosed",
            "reasoning_method_name": "Chain-of-Thought (CoT) prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Long-form abductive reasoning with explicit stepwise chain-of-thought generation.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 38.0,
            "comparison_target_method": "Vanilla prompting (GPT-4, 27.0)",
            "performance_difference": 11.0,
            "statistical_significance": false,
            "analysis_notes": "CoT provides a notable improvement for GPT-4 (≈11 percentage points); authors state CoT helps stronger models while offering little or negative benefit for weaker ones. Also, GPT-4 and human subjects show similar case-level difficulty concordance.",
            "ablation_study_present": true,
            "uuid": "e6743.5",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3.5 (PPO) - Golden CoT",
            "name_full": "GPT-3.5 (PPO) with Golden (ground-truth) Chains-of-Thought provided in prompt",
            "brief_description": "GPT-3.5 (PPO) was given the author's ground-truth solution explanations (golden CoTs) as part of the prompt to simplify the task; performance improves but remains below top human solvers.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (PPO)",
            "model_size": "≈175B",
            "reasoning_method_name": "Golden Chain-of-Thought (ground-truth CoT in prompt)",
            "reasoning_method_type": "sequential (ground-truth-augmented)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Inference from long golden CoTs to select final answer; tests final-step inference when full reasoning is provided.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 63.0,
            "comparison_target_method": "CoT prompting (same model, 29.0)",
            "performance_difference": 34.0,
            "statistical_significance": false,
            "analysis_notes": "Providing ground-truth reasoning greatly improves PPO model performance, indicating that a major failure mode is generating or using correct CoTs; nevertheless, the model still occasionally fails to make trivial final inferences from an explicit solution.",
            "ablation_study_present": true,
            "uuid": "e6743.6",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-4 - Golden CoT",
            "name_full": "GPT-4 with Golden (ground-truth) Chains-of-Thought provided in prompt",
            "brief_description": "GPT-4 given authors' solution explanations in the prompt achieves performance comparable to top human solvers on the benchmark.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not publicly disclosed",
            "reasoning_method_name": "Golden Chain-of-Thought (ground-truth CoT in prompt)",
            "reasoning_method_type": "sequential (ground-truth-augmented)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Final inference from provided ground-truth step-by-step solution to select the correct multiple-choice answer.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 83.0,
            "comparison_target_method": "CoT prompting (GPT-4, 38.0)",
            "performance_difference": 45.0,
            "statistical_significance": false,
            "analysis_notes": "With golden CoTs GPT-4 reaches levels comparable to top human solvers (authors report top humans 80-90%); this implies that GPT-4 can perform the final answer extraction when presented with an explicit correct reasoning trace, but struggles to generate or discover that trace unaided.",
            "ablation_study_present": true,
            "uuid": "e6743.7",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Prompting strategies",
            "name_full": "Prompting strategies evaluated: Vanilla, Chain-of-Thought (CoT), Golden CoT",
            "brief_description": "The study compares three prompting/reasoning methods (direct answer, model-generated CoT, and ground-truth CoT-in-prompt) across models to analyze their effects on abductive reasoning performance.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "various (GPT-3.5 variants, GPT-4)",
            "model_size": "various",
            "reasoning_method_name": "Vanilla; Chain-of-Thought; Golden Chain-of-Thought",
            "reasoning_method_type": "mixed",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Evaluate how different prompting styles (no CoT, model-generated CoT, and provided ground-truth CoT) affect solving long-form abductive detective puzzles.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors observe that CoT helps stronger models (GPT-4) but not necessarily smaller ones (GPT-3.5); providing golden CoTs yields large gains for all models but particularly closes the gap for GPT-4 to human top-solvers. Overall, the study treats method variety as an ablation to probe where failures occur (CoT generation vs final inference).",
            "ablation_study_present": true,
            "uuid": "e6743.8",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Benchmark - True Detective (5MM)",
            "name_full": "True Detective: 5 Minute Mystery long-form abductive reasoning benchmark",
            "brief_description": "A dataset of 191 long-form (≈1200 words on average) detective puzzles sourced from the 5 Minute Mystery platform, each with multiple-choice answers and an author-provided solution (golden CoT); human average solve rate ~47%.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "n/a (dataset/benchmark)",
            "reasoning_method_type": "n/a",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Multiple-choice abductive reasoning over long-form mystery narratives (identify guilty suspect or correct explanation).",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 47.0,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Dataset includes golden CoTs (author solutions) and rich human attempt statistics (average solve rate 47%, top human solvers 80-90%, average attempts ~1984 per puzzle). Authors note dataset is significantly harder than prior short-form abductive datasets.",
            "ablation_study_present": true,
            "uuid": "e6743.9",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Human baseline - average",
            "name_full": "Average human solver performance on 5MM",
            "brief_description": "Aggregate human baseline computed from first attempts recorded on 5 Minute Mystery platform; authors use this as the principal human baseline.",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "Human average",
            "model_size": null,
            "reasoning_method_name": "human abductive reasoning",
            "reasoning_method_type": "n/a",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Solving the same long-form abductive detective puzzles as the models.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 47.0,
            "comparison_target_method": "Random guess (0.24) and model performances",
            "performance_difference": 23.0,
            "statistical_significance": null,
            "analysis_notes": "Authors report average human solve rate 47% (first attempts), while top human solvers achieve ~80-90%; used to contextualize model gaps.",
            "ablation_study_present": null,
            "uuid": "e6743.10",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Random guess baseline",
            "name_full": "Random guessing baseline (dataset-level)",
            "brief_description": "Baseline representing chance performance on the multiple-choice puzzles (~24% overall, reflecting mix of 3-5 options per puzzle).",
            "citation_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
            "mention_or_use": "use",
            "model_name": "Random guess",
            "model_size": null,
            "reasoning_method_name": "random choice",
            "reasoning_method_type": "n/a",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "True Detective (5 Minute Mystery)",
            "task_description": "Chance-level performance on the multiple-choice abductive puzzles.",
            "performance_metric": "solve rate (accuracy)",
            "performance_value": 24.0,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors use this baseline to show many model settings perform close to chance.",
            "ablation_study_present": null,
            "uuid": "e6743.11",
            "source_info": {
                "paper_title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Abductive commonsense reasoning",
            "rating": 1
        },
        {
            "paper_title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "rating": 1
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.0173935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4</h1>
<p>Maksym Del and Mark Fishel<br>Institute of Computer Science<br>University of Tartu, Estonia<br>{maksym, mark}@tartunlp.ai</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form ( 1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the "5 Minute Mystery" platform and include a multiple-choice question for evaluation. Only $47 \%$ of humans solve a puzzle successfully on average, while the best human solvers achieve over $80 \%$ success rate. We show that GPT-3 models barely outperform random on this benchmark (with $28 \%$ accuracy) while state-of-the-art GPT-4 solves only $38 \%$ of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs' abilities. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have gained significant attention in recent years due to their impressive performance on a wide range of natural language processing tasks, including reasoning tasks (Srivastava et al., 2022; Wei et al., 2022). This calls for new, genuinely challenging benchmarks requiring LLMs to possess truly advanced reasoning capabilities to be solved.</p>
<p>Abductive reasoning is a type of inference aiming at finding the minimal and most justified explanation for the set of phenomena or observations. Previous benchmarks on this topic, such as Mostafazadeh et al. (2016), consisted of short</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and straightforward common-sense observations and were solved by GPT models (Radford and Narasimhan, 2018). However, the canonical example of abductive reasoning, a demanding process of a detective finding the best solution to a complex crime based on the clues and observations, was not explored as a foundation for the LLM benchmark in the literature.</p>
<p>Motivated by the need for a new reasoning benchmark and inspired by the complexities and particularities of a detective enterprise, we present a novel abductive reasoning benchmark consisting of 191 detective puzzles/mysteries. Mysteries are sourced from the "5 Minute Mystery" platform, where professional and aspiring authors wrote them. A puzzle is structured as a $&gt;1000$ words story with $4-5$ answer options. Over the last 15 years, puzzles were attempted by humans around 2000 times each with an average solve rate of $47 \%$ (only the first try for each human for each puzzle counts). However, top human solvers (top 10) achieve a success rate of over $80 \%$ solving more than 154 of 192 puzzles correctly.</p>
<p>Moreover, additional modifications such as chain-of-thought (CoT) prompting Wei et al. (2022); Kojima et al. (2022) that are meant to invoke emergent reasoning abilities in LLMs do not help for GPT-3.</p>
<p>In this study, we also assess the performance of the current state-of-the-art GPT-3 and GPT-4 models on our newly proposed dataset. We show that these models, even equipped with the Chain of Thought prompts (Wei et al., 2022; Kojima et al., 2022), are getting an accuracy rate of only $28 \%$, barely better than random guessing (GPT-3.5), or scoring $38 \%$ (GPT-4), which is halfway between random guessing and average human baseline, and far behind top human solvers with their $80 \%$ solve rate. These results reveal a significant gap in the reasoning abilities of GPT models and humans.</p>
<p>In our ablation study, we also supply models</p>
<p>with golden CoTs. Golden CoTs are narratives that represent the reasoning behind the correct answer for each story (written by the mystery authors). When we attach golden CoTs to the input prompt, the best-performing GPT-3.5 model only achieves a solve rate of $63 \%$. This indicates LLMs' difficulty making even trivial inferences from the complex long-form story. GPT-4 models, however, get as good as the best human solvers when presented with our chain of thoughts (even though humans do not have access to the golden CoTs).</p>
<p>Our contributions in this paper are twofold: (1) a new challenging benchmark for evaluating LLMs for advanced abductive reasoning; (2) a showcase of GPT-3.5 and GPT-4 models failing to perform reasonably.</p>
<h2>2 Related Work</h2>
<p>Mostafazadeh et al. (2016) introduced the ROCStories benchmark: narrative cloze test, which requires choosing the correct ending of the four-sentence story. Bhagavatula et al. (2020) expand on this dataset, requiring finding plausible explanations for narrative gaps instead of focusing on the sequence of events. Our benchmark contains stories of around 70 sentences that require solving the detective mystery (as opposed to simply figuring out commonsense story continuation), which is a much harder inference.</p>
<p>Natural language inference (NLI) is another related domain, but NLI tasks usually include much simpler and smaller inferences (Bowman et al., 2015; Williams et al., 2018). Zellers et al. (2018) introduced the SWAG dataset that offers a largescale natural language inference challenge where grounded knowledge is required to make an inference. This shares some commonality with our dataset, as some mysteries might require a share of grounded knowledge about the real world. Unlike Zellers et al. (2018), we only offer a test set, but our stories are broader and more involved. On the other hand, Grimm and Cimiano (2021) introduced a question-answering benchmark that requires deeper text understanding based on the football match commentaries. Their questions range from counting the number of goals to identifying the game-winner. While answers to many of these questions are not explicitly provided in the football commentary, our mysteries require solving the whole case specifically designed to be challenging even for humans.</p>
<p>Lastly, Wei et al. (2022) find that while eliciting "Chain of Thought" reasoning helps with stronger models, it can hurt when solving harder tasks with smaller models. We observe this behavior when comparing GPT-3.5 and GPT-4 on our benchmark.</p>
<h2>3 Benchmark</h2>
<h3>3.15 Minute Mystery Platform</h3>
<p>The data for this AI research was obtained from the "5 Minute Mystery" ${ }^{2}$ online platform. This website is an online platform that has functioned for over ten years and allows users to submit and solve mysteries of varying difficulty (see Appendix A for an example mystery).</p>
<p>Based on the website author guidelines, the mysteries on the website collection are intended for readers at the sixth to eighth-grade reading level and have a recommended length of around 1200 words. To facilitate comprehension and challenge the reader, each mystery includes around four suspects and one guilty suspect. Of the 191 mysteries, the overwhelming majority ask the reader to identify the guilty suspect, with only occasional ones asking for the geographic location or the missing person. The aim is for the reader to demonstrate their abductive reasoning abilities by solving the mystery and identifying the correct solution (e.g., the murderer). Typically, one character in the story is faced with the key puzzle, and at the end of the mystery, they exclaim something like: "I figured out who is guilty!" At this point, the reader must choose the correct answer from a list of options.</p>
<p>In addition, mystery writers provided an explanation for the answer: a full solution (golden CoT) that elicits reasoning that leads to the correct answer. The reasoning is presented on behalf of one of the story characters (the one who says, "I know who did it" at the end of the story).</p>
<p>The website also has a unique scoring system that rewards users for correctly solving mysteries, encouraging participation and engagement. In addition to providing entertainment, the website can also be used in an educational setting to help students develop their comprehension and critical thinking skills.</p>
<h3>3.2 Benchmark Dataset</h3>
<p>The mysteries in this study were obtained from the "5 Minute Mystery" (5MM) platform. We have included links to the original mysteries and to the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of the number of attempts for each mystery. The red dot indicates that almost 2000 people attempted mysteries on average. This suggests that our dataset provides a robust estimate of human performance and is representative of human performance on the mysteries.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average human solve rate for each mystery in the dataset. The performance for most puzzles is around $40-60 \%$. The red dot indicates the average solve rate. This figure reveals that the majority of puzzles are challenging for human solvers, providing a good benchmark for evaluating the performance of AI models on these types of tasks.
author pages in the study, and we want to emphasize that all copyrights remain with the original authors and the 5MM team. See the authors list in the Appendix B section.</p>
<p>Dataset size and the number of answers. The dataset used in this study consists of 191 puzzles, including 160 puzzles with four answer options, 30 puzzles with five answer options, and one puzzle with three answer options.</p>
<p>Attempts. The "5 Minute Mystery" platform has been in operation for approximately 14 years and has attracted thousands of users, with over 20,000 registered by 2013. These users have made numerous attempts at each mystery, but only their first attempt is counted towards the platform's statistics. As shown in Figure 1, the average number of attempts per mystery is 1984 , with only a few puzzles being significantly more or less popular.</p>
<p>With such a large sample size, the resulting human performance estimate is highly robust and reliable as a benchmark.</p>
<p>Human Solve Rate. In the 5MM platform, human solvers have achieved moderate success. The average solve rate is $47 \%$, significantly higher than
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Number of words in each mystery in the dataset. Mysteries range from 600 to around 2000 words with most of them being around 1204 words (red dot). This suggests that not only does the task require drawing highly nontrivial conclusions from the text but also doing so over relatively large texts.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Number of words in the solution explanations for each mystery. The red dot indicates the average number of words per explanation. This figure reveals that the average solution length is 265 words, and the longest solutions are around 600 words. Solutions (golden CoTs) are useful for a setup testing the ability of LLMs to do a trivial final answer inference over the given CoT.
random guessing (around $24 \%$ ), indicating that the tasks are challenging even for humans. The top ten human solvers have an average solve rate of $80-90 \%$, per platform statistics. Figure 2 shows that most mysteries are solved between $40 \%$ and $60 \%$ of the time, with some being solved up to $70 \%$ of the time and others close to random guessing. While the mysteries were designed to vary in difficulty, it is possible that the best explanation provided by humans may not always align with the author's intended solution for the hardest ones. However, we continue to include these mysteries in our dataset to investigate whether language models can better infer the author's intent in these cases.</p>
<p>Mystery word count. Figure 3 shows the distribution of the number of words in each mystery in the dataset. On average, mysteries have 1204 words, with some being as long as 2000 words. This suggests that the puzzles used in the study not only require advanced reasoning skills to solve but also require finding relevant clues from a relatively long body of text that can incriminate or exonerate suspects. This further complicates the task.</p>
<p>Golden CoTs. Each mystery in the dataset includes a full-text solution that provides an expla-</p>
<p>nation of how one of the story characters came up with the correct answers. The average length of these solutions is 265 words, as shown in Figure 4. The solution lengths do not vary significantly, with the longest solution being around 600 words.</p>
<p>These solutions can be considered as groundtruth Chains-of-Thought (cite paper here), which provide insight into the author's reasoning for each puzzle. This information is valuable for a few reasons. First, it can be used as part of few-shot learning examples (again, cite). Second, as we demonstrate in Section 4, we can use these Chains-ofThought to simplify the abductive reasoning task and evaluate whether language models can perform inference when the solution is strongly hinted at.</p>
<h2>4 Evaluation</h2>
<h3>4.1 Models</h3>
<p>The models used in this study are the InstructGPT3.5 models GPT-3.5 (FeedME), GPT-3.5 (PPO) (OpenAI, 2022), and GPT-4 (OpenAI, 2023). They are causal language models based on the Transformer architecture (Vaswani et al., 2017) featuring supposedly around 175B parameters for GPT-3.5s.</p>
<p>GPT-3.5 (FeedME): a model was trained using the FeedME method, a supervised fine-tuning method based on human-written instructions and model samples (Ouyang et al., 2022; OpenAI, 2022).</p>
<p>GPT-3.5 (PPO): is a more performant update over GPT-3.5 (FeedME) model. Apart from instruction tuning, it was also calibrated with RLHF, a reinforcement learning method that uses reward models trained from human comparisons (Stiennon et al., 2020; OpenAI, 2022).</p>
<p>GPT-4: state-of-the-art commercial model from OpenAI. Achieves human parity on multiple extremely challenging tasks (OpenAI, 2023).</p>
<h3>4.2 Methods</h3>
<p>In this study, we tested GPTs in a zero-shot manner in three scenarios. This subsection outlines them.</p>
<p>Vanilla: This method involves the task description, mystery body, and an immediate request for the final answer (Brown et al., 2020).</p>
<p>CoT: This method asks LLMs to generate a Chain-of-Thought first (Wei et al., 2022; Kojima et al., 2022) and only then requests the final answer. Chain-of-thought, if reasonable, allows the
model to approach complex problems gradually and unlocks strong reasoning abilities at a particular model scale (Wei et al., 2022).</p>
<p>Golden CoT: This method involves generating answers to instruction-based questions by using a set of ground-truth Chain-of-Though solutions included as part of the prompt. This significantly simplifies the task for the model as it does not need to come up with CoT, so we can test how much of the performance depends on the CoT and how much on the final abductive reasoning step.</p>
<h3>4.3 Prompt Templates</h3>
<p>Figure 5 shows the task instruction that we give to the InstructGPT models at the beginning of the prompt.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Task instruction that we use as a prompt prefix.</p>
<p>Then we always add the mystery name, list of suspects, and mystery content (body) to the prompt.</p>
<p>When we want to invoke Chain-of-Though reasoning, we also append the following:</p>
<div class="codehilite"><pre><span></span><code>Full answer:
Let&#39;s think step by step.
</code></pre></div>

<p>When we want to provide a golden Chain-ofthought, we append the following prompt:</p>
<h2>Solution: <br> {solution}</h2>
<p>Finally, we always ask for the final answer with Final answer:</p>
<h3>4.4 Results and Discussion</h3>
<p>The evaluation results shown in Table 1 indicate that the performance of both davinci models under both Vanilla and CoT prompting scenarios is close to random. In our analysis, we also found that there is no correlation between the length of the mystery or human solve rate and the GPT's correctness.</p>
<p>Our Golden CoT ablation study (Table 1) demonstrates that even with relevant explanatory CoT, GPT-3.5s can only solve $63 \%$ of puzzles correctly, suggesting that difficulty lies not only in generating</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Solve rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random guess</td>
<td style="text-align: center;">0.24</td>
</tr>
<tr>
<td style="text-align: left;">Human average</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: left;">Human top</td>
<td style="text-align: center;">$0.8-0.9$</td>
</tr>
<tr>
<td style="text-align: left;">Vanilla</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (FeedME)</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (PPO)</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (FeedME)</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (PPO)</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$\mathbf{0 . 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Golden CoT*</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (FeedME)</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (PPO)</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.83</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of GPT-3.5 (FeedME), GPT-3.5 (PPO), and GPT-4 under different prompting scenarios against the human baseline. Both vanilla task formation (Instruction and immediate answer request) and "step-by-step" chain-of-thought approaches perform almost equivalent to random guess. Even in unfair comparison, GPTs cannot match/outperform top human solvers when provided with golden chains of thought.
the correct theory for the crime but also in making final inferences when all information is available. On the other hand, GPT-4 does not help such a problem with $83 \%$.</p>
<p>CoT performance of GPT-3.5 models show small to no gains in performance compared to Vanilla. As indicated in Wei et al. (2022), a similar decrement (between GPT-3 and smaller models) was observed in models that weren't sufficiently powerful for the task suggesting that the GPT-3.5 models might also not be strong enough to generate CoT chains that would benefit the task. On the other hand, CoT GPT-4 performs better, although still underachieving compared to the average human solve rate.</p>
<p>The complexity of the long-form multi-character narrative and the level of reasoning required to solve the detective puzzle makes our benchmark especially difficult and sets it apart.</p>
<p>Finally, we explore the complexity of the cases that GPT-4 (CoT) found easier or harder to manage. In our study, we did not observe a direct correlation between the length of a mystery and the level of difficulty it presented. However, when considering the level of concurrence between human decisions and those made by GPT-4, Figure 6 demonstrates a considerable degree of agreement. Specifically, the cases perceived as challenging or straightforward by the GPT-4 were often viewed similarly by human subjects.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Red line indicates case difficulty for humans, green points indicate cases where GPT-4 (CoT) solved the case successfully, and black points are for cases where GPT-4 failed. Black points are crowded on the right and green points are crowded on the left which correlates with hard and easy cases (as per humans) respectively. Therefore, GPT-4 and humans find similar cases easy/difficult.</p>
<h2>5 Conclusion</h2>
<p>We presented a new benchmark in the form of detective puzzles to evaluate the abductive reasoning capabilities of Large Language Models. Results from state-of-the-art GPT-3.5 models across three prompting strategies showed poor performance close to random. GPT-4 managed to show comparably solid performance (when prompted with CoT), but even this model is behind the average human solve rate on our benchmark. When provided with golden CoTs, which significantly simplifies the task, GPT-4 shows good performance, while GPT-3 is still unable to do a final inference well enough. Overall, our benchmark offers insights into LLMs' limitations and provides a difficult challenge for future research on abductive reasoning in large LMs.</p>
<h2>6 Limitations</h2>
<p>Our evaluation focused solely on the performance of leading-edge GPT models details and weights of which are not publicly available. However, there is potential value in extending this study to incorporate other models like PaLM (Chowdhery et al.) or LLaMA (Touvron et al.), which we have earmarked for future research.</p>
<p>Also, as the performance for average humans is only $47 \%$ it is possible that some mysteries are ill-defined or unreasonably complicated. Among the top 10 human solvers, the solve rate is also only around $80-90 \%$, and GPT-4 only solves $83 \%$ of tasks when provided with ground truth CoTs which drastically simplifies the task.</p>
<h2>References</h2>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representations.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Frank Grimm and Philipp Cimiano. 2021. BiQuAD: Towards QA based on deeper text understanding. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 105-115, Online. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguistics.</p>
<p>OpenAI. 2022. Model index for researchers. https://beta.openai.com/docs/ model-index-for-researchers.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Alec Radford and Karthik Narasimhan. 2018. Improving language understanding by generative pretraining.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito,</p>
<p>Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo JaimovitchLópez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütli Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans,</p>
<p>Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramón Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard</p>
<p>Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93-104, Brussels, Belgium. Association for Computational Linguistics.</p>
<h2>A Example: The Easter Egg Mystery</h2>
<p>This appendix provides the most attempted mystery under 700 words as an example. Copyright belongs to the mystery author.</p>
<h2>Metadata</h2>
<ul>
<li>Mystery Name: The Easter Egg Mystery</li>
<li>Author: Tom Fowler ${ }^{3}$</li>
<li>Solve Rate: $60.8 \%$</li>
<li>Attempts: 1871</li>
<li>Answer options: (a) Anna; (b) Cole; (c) Justin; (d) Lizzie; (e) Rachel.</li>
</ul>
<p>Mystery Body Karen Sheldon had loved Easter egg hunts ever since she was a little girl. That is why she eagerly volunteered to assist with this year's Hunt for the children at her church.</p>
<p>This year, the Children's Day Out mothers decided to do something different. Because there were so many children of all ages in the congregation, they split the hunt up into age groups. Karen's job was to oversee several of the 6-10 year olds.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Within her group were five children she knew well. They were Rachel Smithson, whose mother Karla had volunteered to help a very grateful Karen, Justin Bates, a classmate of Rachel's, Karen's daughter Lizzie, Lizzie's best friend Anna Laughlin and Cole Bryant, who was also the Sheldon's next door neighbor.</p>
<p>The Easter egg hunt was on Saturday morning, the day before Easter Sunday. It was held in the large field in back of the church. Karen and Karla were grateful that today was sunny and warm although it was a bit windy. Karen was excited as the children prepared for the hunt, which was to begin at 10:00 am and last for one hour. Just before the start whistle blew, Karen told the children, "I have placed a golden Easter egg in our hunting area. There is an extra bag of candy for the child who finds it." Only Karla and she knew that the golden egg was placed in back of the largest tree in the field, an old oak in the far corner to the left of where she and the children now stood and an area dedicated to the 6-10 year old age group.</p>
<p>During the hunt, Karen and Karla visited while they watched the egg hunt. During the hunt, Karen noticed that Cole stayed focused on the evergreen shrubbery in the middle of the field, finding several eggs there, much to his delight.</p>
<p>Karen was amused when Rachel ran to her mother and told her, "I have found a lot of eggs. I'm heading back to the rock pile. I bet I will find the golden egg there!" The rock pile was to the right of the evergreen shrubbery.</p>
<p>In the middle of the hunt, Karen excused herself to go inside the church to get a drink of water and sit for a few minutes. When she returned, Karla told her, "I had to run over and warn Lizzie to be careful of the dead branches on the big oak tree. One of them fell last week, hitting one of the older kids."</p>
<p>As the hunt began to wind down, Karla walked out to speak with a very agitated Anna. After returning to Karen, she told her, "Anna is upset because she has found only a few eggs. I told her to keep looking; there are still a few minutes to go." Karen noticed that Anna stayed close to Karla for the remainder of the hunt.</p>
<p>As the whistle blew to end the hunt, Karen walked to the center of the field to wave Justin back in. He was in the far right corner of the field, where he had been for the entire hunt. There was a sand pit in that area and Justin found several eggs</p>
<p>there.
As the kids headed back to the start area, Karen once again excused herself to go inside. The wind had blown a speck of dust in her eye when waving Justin down and it was very painful. When she returned from rinsing her eyes, Karla and the five children were smiling at her. She asked, "What's up?"</p>
<p>Karla answered, "One of our kids found the golden egg. We want you to guess which one."</p>
<p>Karen smiled in return, saying, "So that's it!" Thinking for a moment, she said, "I only have one question. When I was inside the first time, did any of the children move from one side of the field to another?"</p>
<p>Karla answered, "No."
Karen tousled Justin's hair and said, "Good. Then I know who has the golden egg!"</p>
<h2>A. 1 Golden CoT and Answer</h2>
<p>Golden CoT. "Good naturedly, Karla exclaimed, "How do you know?" Smiling at Anna, she answered, "It's not too hard to figure out. Let me explain." The eyes of all of the children and Karla were upon her as she continued, "I placed the golden egg behind the big oak tree." Smiling next at Cole, she said, "Cole spent the entire hour in the shrubbery, in the middle of the field, far away from the oak tree." She patted Rachel's shoulder and said; "Rachel did all of her hunting in the rock pile, even farther away from the oak tree." Looking back at Anna, Karen said, "I know you don't have the golden egg, sweetie. You were upset that you had so few eggs with only a few minutes left in the hunt and stayed close to Karla until the whistle blew." Patting her hand, she added, "I'm sure you will do better next year." Turning to Justin, Karen said, "You were farther away from the oak than anyone. You spent the whole hour far out in the sand pit. I even had to come get you because you could not hear the whistle." All eyes turned toward Lizzie. Her mother said, "So, you must have the egg. Karla told me she had to warn you of the dead branches on the oak. You were the only one near it." Pausing, she added, "I hope everyone believes that I did not tell you where I put that egg! Karla jumped in, "Of course we do not think that!" All of the kids echoed their support. Lizzie broke the silence. She said, "I didn't know about the egg until Mother told everyone else before the hunt." Walking over to her side, Lizzie looked at Anna and offered her the golden
egg, saying, "I would like for you to have this." Tearfully, Anna thanked her friend, saying, "This is the best Easter egg hunt ever!" Karen was so proud of Lizzie that she heartily agreed with Anna."</p>
<h2>Answer: (d) Lizzie</h2>
<h2>B Mystery Authors Acknowledgment</h2>
<p>We thank all the 5MM mystery writers: Moe Zilla, Tom Fowler, William Shepard, Laird Long, Robbie Cutler, Barney Parmington, Stefanina Hill, Steve Shrott, Nick Andreychuk, Nicholas LeVack, Ernest Capraro, Andrea Hein, Doug Fellin, Tammy-Lee Miller, Meghan Ford, Brad Marsh, Susanne Shaphren, Randy Godwin, Ryan Hogan, Matthew Lieff, Perry McCarney, Nicholas Lovell, Mike Wever, Meg A. Write, Elsa Darcy, PIP Writer, Julie Hockenberry.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www.5minutemystery.com/ author/tfowler&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
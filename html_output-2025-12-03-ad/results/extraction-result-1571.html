<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1571 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1571</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1571</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-201070811</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/1904.01095v3.pdf" target="_blank">Fast, accurate, and transferable many-body interatomic potentials by symbolic regression</a></p>
                <p><strong>Paper Abstract:</strong> The length and time scales of atomistic simulations are limited by the computational cost of the methods used to predict material properties. In recent years there has been great progress in the use of machine-learning algorithms to develop fast and accurate interatomic potential models, but it remains a challenge to develop models that generalize well and are fast enough to be used at extreme time and length scales. To address this challenge, we have developed a machine-learning algorithm based on symbolic regression in the form of genetic programming that is capable of discovering accurate, computationally efficient many-body potential models. The key to our approach is to explore a hypothesis space of models based on fundamental physical principles and select models within this hypothesis space based on their accuracy, speed, and simplicity. The focus on simplicity reduces the risk of overfitting the training data and increases the chances of discovering a model that generalizes well. Our algorithm was validated by rediscovering an exact Lennard-Jones potential and a Sutton-Chen embedded-atom method potential from training data generated using these models. By using training data generated from density functional theory calculations, we found potential models for elemental copper that are simple, as fast as embedded-atom models, and capable of accurately predicting properties outside of their training set. Our approach requires relatively small sets of training data, making it possible to generate training data using highly accurate methods at a reasonable computational cost. We present our approach, the forms of the discovered models, and assessments of their transferability, accuracy and speed.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1571.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1571.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POET-GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic regression genetic programming used in POET (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic programming / symbolic regression system (implemented in the POET code) that evolves analytic interatomic potential expressions (expression trees) by crossover and mutation, selecting candidates by multi-objective criteria (accuracy, computational cost, simplicity) and refining numeric parameters with CMA-ES and conjugate-gradient optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic programming symbolic regression (POET)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system represents candidate interatomic potentials as expression trees built from constants, distance variables, arithmetic operators (+, -, *, /, power), a neighbor-sum operator, and a smoothing function. It evolves populations of these trees using standard genetic programming operations (crossover and mutation) with a hierarchical/island-style parallelization across 12 processors (each processor has its own population and local subset of training data). Selection is multi-objective: candidate models are evaluated for fitness (a weighted, normalized MSE of energies, forces, and stresses), computational cost (number of neighbor summations), and complexity (number of nodes). A global set is maintained by retaining models on convex hulls (Pareto frontiers) over fitness, cost and complexity. Numeric parameters inside expressions are optimized using CMA-ES periodically (every 10k genetic ops) and conjugate-gradient per-new-individual; local and global tournaments (Pareto tournament size 10) control selection and migration between islands. The codebase is provided as the POET implementation (open-source repository).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>symbolic expressions / mathematical programs (expression trees representing interatomic potential models)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Two crossover variants were used: (1) subtree crossover (90% of crossovers): randomly select a branch (subtree) in one parent's expression tree and replace it with a randomly selected branch from the other parent's tree; (2) linear-combination crossover (10%): create an offspring by taking a linear combination of two randomly selected branches from two different trees. Overall crossover probability was 0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mutation performed three sub-operations with equal probability: (a) crossover of a tree with a randomly generated tree (i.e., replace part with randomly grown tree), (b) swap the arguments of a binary non-commutative operator (argument swap), and (c) locally modify the expression tree by replacing or inserting a randomly selected non-terminal node with a randomly selected operator. Random trees (for insertion/crossover-with-random) were generated with 'grow' or 'full' methods (50/50) with depth drawn from Gaussian(mean=5, sd=1). Overall mutation probability was 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Two aspects were used to evaluate 'executability' and functionality: (1) numeric fitness = 1000 * (0.5 * MSE_energy + 0.4 * MSE_force + 0.1 * MSE_stress), where energy/force/stress errors are normalized (energies: subtract min and divide by std; forces/stresses: subtract mean divide by std); (2) computational cost estimated as number of neighbor summations (proxy for runtime), and validated by wall-clock benchmarks (µs/step/atom) in LAMMPS. Complexity (number of nodes) was used as a proxy for simplicity/generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Quantitative results reported: fitness values for top models are listed in their convex-hull table (e.g. GP1 fitness ≈ 7.8230, GP2 fitness ≈ 4.7294 in the paper's internal fitness units). Runtime benchmarks: GP1 = 2.1 µs/step/atom, GP2 = 3.5 µs/step/atom, GP3 = 3.6 µs/step/atom, EAM1 = 3.0 µs/step/atom on a Haswell core (2.5 GHz) for the 32-atom benchmark. Accuracy/functionality metrics (validation): energy MAE (meV/atom) — GP1: 3.5, GP2: 2.7, GP3: 2.5, neural-network baseline: 2.2; force MAE (meV/Å) — GP1: 75, GP2: 60, GP3: 62, neural-network baseline: 56. CPU effort: ~330 CPU-hours to rediscover Lennard-Jones, ~3600 CPU-hours for Sutton-Chen, ~360 CPU-hours to find GP1/GP2/GP3.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>The paper explicitly enforces and analyzes tradeoffs between accuracy (fitness), computational cost (number of neighbor summations / runtime), and model complexity (number of nodes) by constructing 3D convex hulls (Pareto frontiers) over these three objectives. Models on the convex hull are retained, which produces a Pareto set of models balancing novelty/complexity versus executability/accuracy. The authors report that GP1 and GP2 advance the Pareto frontier (i.e., achieve comparable accuracy to state-of-the-art models at substantially lower complexity and competitive runtime), demonstrating an explicit multi-objective tradeoff selection but they do not present a formal novelty-vs-executability numeric tradeoff beyond the convex-hull selection.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td>Yes — the method builds convex hulls / Pareto frontiers across three objectives: fitness (weighted normalized MSE), computational cost (number of neighbor summations), and complexity (node count). Models on these convex hulls are promoted to the global set. The paper shows Pareto frontier plots (accuracy error vs complexity) and reports that discovered models (GP1/GP2) push the Pareto frontier to lower error at lower complexity; specific numeric hull points are given in their Table 1 (fitness and cost metrics) though a closed-form frontier equation is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Discovering analytic interatomic potentials (materials domain) via symbolic regression; evaluated on rediscovering Lennard-Jones and Sutton–Chen potentials and on DFT data for elemental copper (predicting energies, forces, stresses, elastic constants, surface energies, defect formation energies, phonons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines compared in the paper include: known analytic potentials (Lennard-Jones, Sutton–Chen), multiple published EAM/glue potentials (EAM1, EAM2, ABCHM, CuNi, Cuu3, Cuu6, etc.), and a neural-network potential (high-data neural-network baseline). Also compared against other published interatomic potentials available in the Interatomic Potentials Repository.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Crossover + mutation GP (with the described operators and island/hierarchical setup) can (a) exactly rediscover analytic potentials (Lennard-Jones, Sutton–Chen) from MD data, (b) discover new simple many-body potential forms (GP1, GP2, GP3) that generalize well to properties not in the training set (surfaces, defects), and (c) produce models that lie on improved Pareto frontiers balancing accuracy, simplicity, and speed. Practical design choices that mattered: heavy use of subtree crossover (90%), occasional linear-combination crossover (10%), mutation types to insert local variation, hierarchical island model and local subsets of training data to increase population diversity, convex-hull-based global archive to maintain multi-objective diversity, and periodic numeric optimization (CMA-ES) to refine parameters. There is no explicit novelty metric (e.g., novelty search) — novelty/diversity were encouraged by structural choices (islands, separate data subsets, convex-hull retention) rather than measured quantitatively; executability and functionality were measured directly via runtime benchmarks and the weighted MSE fitness and a suite of validation metrics (energy/force MAE, elastic constants, surface energies, defect energies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast, accurate, and transferable many-body interatomic potentials by symbolic regression', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic programming -on the programming of computers by means of natural selection <em>(Rating: 2)</em></li>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 2)</em></li>
                <li>Searching for globally optimal functional forms for interatomic potentials using genetic programming with parallel tempering <em>(Rating: 2)</em></li>
                <li>Symbolic Regression of Inter-Atomic Potentials via Genetic Programming <em>(Rating: 1)</em></li>
                <li>Fitting potential-energy surfaces: A search in the function space by directed genetic programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1571",
    "paper_id": "paper-201070811",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "POET-GP",
            "name_full": "Symbolic regression genetic programming used in POET (this work)",
            "brief_description": "A genetic programming / symbolic regression system (implemented in the POET code) that evolves analytic interatomic potential expressions (expression trees) by crossover and mutation, selecting candidates by multi-objective criteria (accuracy, computational cost, simplicity) and refining numeric parameters with CMA-ES and conjugate-gradient optimizers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Genetic programming symbolic regression (POET)",
            "system_description": "The system represents candidate interatomic potentials as expression trees built from constants, distance variables, arithmetic operators (+, -, *, /, power), a neighbor-sum operator, and a smoothing function. It evolves populations of these trees using standard genetic programming operations (crossover and mutation) with a hierarchical/island-style parallelization across 12 processors (each processor has its own population and local subset of training data). Selection is multi-objective: candidate models are evaluated for fitness (a weighted, normalized MSE of energies, forces, and stresses), computational cost (number of neighbor summations), and complexity (number of nodes). A global set is maintained by retaining models on convex hulls (Pareto frontiers) over fitness, cost and complexity. Numeric parameters inside expressions are optimized using CMA-ES periodically (every 10k genetic ops) and conjugate-gradient per-new-individual; local and global tournaments (Pareto tournament size 10) control selection and migration between islands. The codebase is provided as the POET implementation (open-source repository).",
            "input_type": "symbolic expressions / mathematical programs (expression trees representing interatomic potential models)",
            "crossover_operation": "Two crossover variants were used: (1) subtree crossover (90% of crossovers): randomly select a branch (subtree) in one parent's expression tree and replace it with a randomly selected branch from the other parent's tree; (2) linear-combination crossover (10%): create an offspring by taking a linear combination of two randomly selected branches from two different trees. Overall crossover probability was 0.9.",
            "mutation_operation": "Mutation performed three sub-operations with equal probability: (a) crossover of a tree with a randomly generated tree (i.e., replace part with randomly grown tree), (b) swap the arguments of a binary non-commutative operator (argument swap), and (c) locally modify the expression tree by replacing or inserting a randomly selected non-terminal node with a randomly selected operator. Random trees (for insertion/crossover-with-random) were generated with 'grow' or 'full' methods (50/50) with depth drawn from Gaussian(mean=5, sd=1). Overall mutation probability was 0.1.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Two aspects were used to evaluate 'executability' and functionality: (1) numeric fitness = 1000 * (0.5 * MSE_energy + 0.4 * MSE_force + 0.1 * MSE_stress), where energy/force/stress errors are normalized (energies: subtract min and divide by std; forces/stresses: subtract mean divide by std); (2) computational cost estimated as number of neighbor summations (proxy for runtime), and validated by wall-clock benchmarks (µs/step/atom) in LAMMPS. Complexity (number of nodes) was used as a proxy for simplicity/generalizability.",
            "executability_results": "Quantitative results reported: fitness values for top models are listed in their convex-hull table (e.g. GP1 fitness ≈ 7.8230, GP2 fitness ≈ 4.7294 in the paper's internal fitness units). Runtime benchmarks: GP1 = 2.1 µs/step/atom, GP2 = 3.5 µs/step/atom, GP3 = 3.6 µs/step/atom, EAM1 = 3.0 µs/step/atom on a Haswell core (2.5 GHz) for the 32-atom benchmark. Accuracy/functionality metrics (validation): energy MAE (meV/atom) — GP1: 3.5, GP2: 2.7, GP3: 2.5, neural-network baseline: 2.2; force MAE (meV/Å) — GP1: 75, GP2: 60, GP3: 62, neural-network baseline: 56. CPU effort: ~330 CPU-hours to rediscover Lennard-Jones, ~3600 CPU-hours for Sutton-Chen, ~360 CPU-hours to find GP1/GP2/GP3.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "The paper explicitly enforces and analyzes tradeoffs between accuracy (fitness), computational cost (number of neighbor summations / runtime), and model complexity (number of nodes) by constructing 3D convex hulls (Pareto frontiers) over these three objectives. Models on the convex hull are retained, which produces a Pareto set of models balancing novelty/complexity versus executability/accuracy. The authors report that GP1 and GP2 advance the Pareto frontier (i.e., achieve comparable accuracy to state-of-the-art models at substantially lower complexity and competitive runtime), demonstrating an explicit multi-objective tradeoff selection but they do not present a formal novelty-vs-executability numeric tradeoff beyond the convex-hull selection.",
            "frontier_characterization": "Yes — the method builds convex hulls / Pareto frontiers across three objectives: fitness (weighted normalized MSE), computational cost (number of neighbor summations), and complexity (node count). Models on these convex hulls are promoted to the global set. The paper shows Pareto frontier plots (accuracy error vs complexity) and reports that discovered models (GP1/GP2) push the Pareto frontier to lower error at lower complexity; specific numeric hull points are given in their Table 1 (fitness and cost metrics) though a closed-form frontier equation is not provided.",
            "benchmark_or_domain": "Discovering analytic interatomic potentials (materials domain) via symbolic regression; evaluated on rediscovering Lennard-Jones and Sutton–Chen potentials and on DFT data for elemental copper (predicting energies, forces, stresses, elastic constants, surface energies, defect formation energies, phonons).",
            "comparison_baseline": "Baselines compared in the paper include: known analytic potentials (Lennard-Jones, Sutton–Chen), multiple published EAM/glue potentials (EAM1, EAM2, ABCHM, CuNi, Cuu3, Cuu6, etc.), and a neural-network potential (high-data neural-network baseline). Also compared against other published interatomic potentials available in the Interatomic Potentials Repository.",
            "key_findings": "Crossover + mutation GP (with the described operators and island/hierarchical setup) can (a) exactly rediscover analytic potentials (Lennard-Jones, Sutton–Chen) from MD data, (b) discover new simple many-body potential forms (GP1, GP2, GP3) that generalize well to properties not in the training set (surfaces, defects), and (c) produce models that lie on improved Pareto frontiers balancing accuracy, simplicity, and speed. Practical design choices that mattered: heavy use of subtree crossover (90%), occasional linear-combination crossover (10%), mutation types to insert local variation, hierarchical island model and local subsets of training data to increase population diversity, convex-hull-based global archive to maintain multi-objective diversity, and periodic numeric optimization (CMA-ES) to refine parameters. There is no explicit novelty metric (e.g., novelty search) — novelty/diversity were encouraged by structural choices (islands, separate data subsets, convex-hull retention) rather than measured quantitatively; executability and functionality were measured directly via runtime benchmarks and the weighted MSE fitness and a suite of validation metrics (energy/force MAE, elastic constants, surface energies, defect energies).",
            "uuid": "e1571.0",
            "source_info": {
                "paper_title": "Fast, accurate, and transferable many-body interatomic potentials by symbolic regression",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic programming -on the programming of computers by means of natural selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 2,
            "sanitized_title": "distilling_freeform_natural_laws_from_experimental_data"
        },
        {
            "paper_title": "Searching for globally optimal functional forms for interatomic potentials using genetic programming with parallel tempering",
            "rating": 2,
            "sanitized_title": "searching_for_globally_optimal_functional_forms_for_interatomic_potentials_using_genetic_programming_with_parallel_tempering"
        },
        {
            "paper_title": "Symbolic Regression of Inter-Atomic Potentials via Genetic Programming",
            "rating": 1,
            "sanitized_title": "symbolic_regression_of_interatomic_potentials_via_genetic_programming"
        },
        {
            "paper_title": "Fitting potential-energy surfaces: A search in the function space by directed genetic programming",
            "rating": 1,
            "sanitized_title": "fitting_potentialenergy_surfaces_a_search_in_the_function_space_by_directed_genetic_programming"
        }
    ],
    "cost": 0.012955499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fast, accurate, and transferable many-body interatomic potentials by symbolic regression</p>
<p>Alberto Hernandez 
Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Adarsh Balasubramanian 
Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Fenglin Yuan 
Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Simon Mason 
Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Tim Mueller tmueller@jhu.edu 
Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Department of Materials Science and Engineering
Johns Hopkins University
21218 MDBaltimoreUSA</p>
<p>Fast, accurate, and transferable many-body interatomic potentials by symbolic regression
787526773206B37CA7BB39DFD4DE8AF2
The length and time scales of atomistic simulations are limited by the computational cost of the methods used to predict material properties.In recent years there has been great progress in the use of machine learning algorithms to develop fast and accurate interatomic potential models, but it remains a challenge to develop models that generalize well and are fast enough to be used at extreme time and length scales.To address this challenge, we have developed a machine learning algorithm based on symbolic regression in the form of genetic programming that is capable of discovering accurate, computationally efficient manybody potential models.The key to our approach is to explore a hypothesis space of models based on fundamental physical principles and select models within this hypothesis space based on their accuracy, speed, and simplicity.The focus on simplicity reduces the risk of overfitting the training data and increases the chances of discovering a model that generalizes well.Our algorithm was validated by rediscovering an exact Lennard-Jones potential and a Sutton Chen embedded atom method potential from training data generated using these models.By using training data generated from density functional theory calculations, we found potential models for elemental copper that are simple, as fast as embedded atom models, and capable of accurately predicting properties outside of their training set.Our approach requires relatively small sets of training data, making it possible to generate training data using highly accurate methods at a reasonable computational cost.We present our approach, the forms of the discovered models, and assessments of their transferability, accuracy and speed.</p>
<p>INTRODUCTION</p>
<p>][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] In this approach, the development of an interatomic potential model is treated as a supervised learning problem, 21 in which an optimization algorithm is used to search a hypothesis space of possible functions to find those that best reproduce the energies, forces, and possibly other properties of a set of training data.Potential models developed in this way are often able to achieve accuracy close to that of the method used to generate the training data, with linear scalability and orders of magnitude increase in performance.Alternatively, potential models may be generated by using fundamental physical relationships to derive a simple parameterized function.The parameters of this function are typically then fit to a smaller set of training data.[24][25][26][27][28] There are advantages and disadvantages to both approaches to potential model development.2][3] On the other hand, models developed from fundamental physical relationships are often simpler and orders of magnitude faster than machine learning potential models, 29 allowing them to be used to model systems at much longer time and length scales.Because they are derived from physics, they can be expected to perform relatively well when they encounter local environments that are unlike the ones they were trained on.The hypothesis space of these potential models is relatively small compared to most machine learning potentials, meaning that less data is required to train them but also that they are typically unable to achieve the same level of accuracy as many potentials generated using machine learning.</p>
<p>Here we present a hybrid approach in which machine learning is used to develop simple, fast potential models.1][32][33][34][35] Our approach adds to these efforts by identifying new functional forms for the models themselves.To accomplish this we use symbolic regression as implemented using genetic programming, in which simple expressions for the potential energy surface are optimized by simulating the process of natural selection. 36,37Genetic programming has been used to rediscover fundamental physical laws 38 and applied in materials science to find descriptors of complex material properties. 39,402][43] Here we go beyond these previous efforts by demonstrating that genetic programming is capable of finding fast, accurate and transferable many-body potentials for a metallic system from ab-initio calculations.</p>
<p>The key to our approach is the construction of a physically meaningful hypothesis space, achieved by analyzing interatomic potentials that were derived from physical principles. 25,27,28We take advantage of natural similarities in the functional forms of simple, physics-derived models 25 to construct a hypothesis space that contains many such functional forms.The hypothesis space that we use consists of all functions that can be constructed from combinations of addition, subtraction, multiplication, division, and power operators; constant values and distances between atoms; and an operator that performs a sum over functions of distances between a given atom and all neighbors within a given cutoff radius.This space contains a wide variety of potential models derived from fundamental physical interactions, including nearly all pair potentials (e.g.Lennard-Jones, 44 Coulomb, 45 Morse 46 ) as well as many-body glue potentials, 25 bond-order potentials (without the bond angle terms), 25,26,47,48 and combinations thereof.Even for relatively simple hypothesis spaces such as this one, it is difficult to enumerate a list of even relatively simple functional forms that can be created due to the large number of ways in which the various operators and values can be combined. 40Here we use a genetic algorithm and multi-objective optimization to search this hypothesis space for interatomic potentials that are simple (and thus more likely to be generalizable 49 ), fast, and accurate.Additional details of our approach are provided in the Methods section.</p>
<p>RESULTS</p>
<p>Validating the machine learning algorithm</p>
<p>To validate our algorithm, we tested its ability to rediscover the exact form of two interatomic potentials: the Lennard-Jones potential and the Sutton-Chen (SC) EAM potential.In each case, the genetic algorithm was able to identify the exact function used to generate the training data.The training data for the Lennard-Jones potential were generated by taking 75 snapshots (1 snapshot every 5000 steps with a time step of 1 fs) of 32-atom molecular dynamics simulations: 15 snapshots at 80 K (NVT), 15 snapshots at 80 K and 100 kPa (NPT), 15 snapshots at 100 K (NVT), 15 snapshots at 100 K and 100 kPa (NPT) and 15 snapshots at 20,000 K (NVT).It consisted of 75 energies and 7200 components of force 50 , generated using the following parameterized model for argon 51 : 12  6   49304.15 34.88
LJ i j V r r   = −     (1)
where V LJ is the potential energy of the system, the index i represents an atom in the structure, j is its neighbor and r is the distance between the two atoms.The genetic programming algorithm found:
j j i V r r − −     = − −              (2)
which simplifies to the form of the Lennard-Jones potential in equation (1).</p>
<p>The training data for the SC EAM potential were obtained from 100 snapshots (1 snapshot every 100 steps with a time step of 1 fs) of 32-atom molecular dynamics simulations: 25 snapshots at 300 K, 25 snapshots at 1600 K, 25 snapshots at 3800 K and 25 snapshots at 20,000 K, all in the NVT ensemble.The training set consisted of 100 energies and 9600 components of force.The potential used to generate the training data was parametrized for copper: 0.5 9  6   644.7 52 .62  52
SC i j j V r r       = −           (3)
The artificial intelligence algorithm found: 0.50
j j i r r V − − −           − − − +                      =         (4)
When it is simplified, it gives the same form as V SC with a constant shift and a slight difference between the constant parameters that could be eliminated by tightening the convergence criterion for parameter optimization.The values of the parameters in the exponents were found to the second decimal place.</p>
<p>Discovering new models for copper</p>
<p>Having established that our genetic programming algorithm can find the exact form of simple pair and many-body potentials, we evaluated its ability to find potential models from data generated using density functional theory 52 (DFT).For this purpose, we generated 150 snapshots (1 snapshot every 100 steps with a time step of 1 fs) of 32-atom DFT molecular dynamics simulations on fcc copper: 50 snapshots at 300K (NVT), 50 snapshots at 1400 K (NVT) and 50 snapshots at 1400 K (NPT at 100 kPa).The copper had melted and lost its fcc structure for the simulations at 1400 K.The data consisted of 150 energies, 14400 components of forces and 900 components of virial stress tensors 53 .One half was randomly selected for training and the other half for validation.Models were evaluated on three metrics: complexity, defined as the number of nodes on the model; computational cost, defined as the number of summations over neighbors, as these typically consume most of the execution time; and fitness, defined as a weighted sum of the mean squared errors of the energies, forces and stresses, which were normalized to unitless values as described in the methods section: 1000*(0.5 0.4 0.1 )
energy force stress fitness MSE MSE MSE = + +(5)
To identify promising models we constructed a three-parameter convex hull based on fitness, computational cost, and complexity.Some of the models on this hull are shown in Table 1.
j i j r r r E r f r f r − −   = + −       GP2 ( )9
)
r r i j j j r f r E f r r f r − − − −     = +      +      EAM2 55 ( ) ( ) ( ) ( ) ( ) 0 0 0 0 6 1/ 2 9 2 ( ) ( ) 2 1 2 ( ) ( ) 1 0 2 ()2 () 2 ( ) 1 2
( / 1)/ ( /9</p>
<dl>
<dt>)</dt>
<dt>) ((1 )  = − + +     = − − = = − Ω − +     ABCHM 56&lt; &lt; +     = + ⋅ −        − &lt; &lt; − − &lt; &lt; = + − &lt; &lt; − − &lt; &lt;     3 3 3 3 0.ψ          + − &lt; &lt;  + − &lt; &lt;   &lt; &lt; = &lt; − − &lt; +   CuNi 57 ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) 2 2 (1) (1) 2 1 1 (1) 0.5 2 6 9 2 ( 1 ) 2 2 (1) 2 0 1 1 2 tanh 20 2 0.1 1 1 2 ( /1) / ( ) ( ) ( ) ( ) ( 1 MM B B M M r R i M M j r R r R r r j r R M M i i j a su i i i sub E f rF f r F E L f e r D e D r r e e e D e D a e a a a E E L E α μ μ β β α σ σ ρ ρ μ ρ * − −     − − − − +     − − − − * − * + = = − = + −   = − −     + −       − −   = − +     1/ 2 / 9 ) b BΩ EAM155 ( ) ( ) ( ) ( ) ( ( ) ( ) ( ) ( ) ( ) ( ) (1)</dt>
<dt>(1)</dt>
<dt>1 1 0 0 (2) (2) 2 2 0 0 2 ( ) ( ) 3 1 ( ) ( ) 4 2 ( ) ( ) 1 2 4 2 2 (0) (2) 1 2 (0)(2) 1 2 ( ) ( ) 2 ( 1)F F q else F E α α α α ρ δ ρ ρ ρ ρ ρ ρ ρ − − − − − − − − = + =     − +     = − − − +     − +         &lt; = + − + − + − + =    ( ) ( ) ( )(3) 2 ( 4) 1 0 2 0 ( ) 3 4 1 ) ( 3 2 1 1 1 ( 1)</dt>
<dd>( )
i i i r r r r j ae Q Q where f e r β β ρ ρ ρ − − − − − =   +   + − + −
 Note: All potentials are in units of eV and Å. ( ) f r is a smoothing function; for GP1 and GP1 it is defined in equation (7).EAM2 and CuNi defined the embedding function to match a universal equation of state 58 .</dd>
</dl>
<p>Many of the models discovered by the genetic programming algorithm have forms that resemble the embedded atom model, or "glue" type potentials.The models consist of a sum of a pairwise term with a repulsive component and a many-body "glue" type attractive term which consists of a nonlinear transformation (an "embedding" function) of a sum over neighbors (the "density").Here we select two of the models, which we label GP1 and GP2, for further analysis based on their favorable tradeoff between simplicity and their prediction errors for the elastic constants (Table S9).In GP1, the simpler of the two models, the embedding function is simply the inverse of the density.In GP2, the embedding function is the same, and it is multiplied by a sum of pairwise interactions to form the glue term.Although GP1 and GP2 resemble known potential models, there are some notable differences.They have much simpler functional forms than most other copper potential models, and they have a different form for the attractive "glue" part of the potential.It is common in EAM-type potential models for the embedding function to be the negative square root of the density; this can be derived from the second moment approximation. 25In GP1 and GP2, the attractive term instead depends on the positive inverse of a sum over pairwise interactions.Unlike the other models, this embedding function is bounded in the limit of high densities and diverges to infinity in the limit of zero density.GP1 and GP2 also include terms with the unusual form of a br r − , which grows by a power law before decaying superexponentially.The resulting models demonstrate high predictive power for condensed phases and defects that were not included in the training data and, even though there were no surfaces in the data used to train them, they largely avoid the severe underprediction of surface energies that are common for embedded-atom type models (Table S6). 55</p>
<p>Validating and evaluating the transferability of the interatomic potentials</p>
<p>As might be expected by their simplicity, neither GP1 nor GP2 overfit their training data.For each model, there is little difference between the training mean absolute error and validation mean absolute error for energies, components of force vectors and components of the virial stress tensors (Figure 1).Both models similarly reproduce the radial distribution function of a liquid state well (Figure 2), which is likely partially due to the inclusion of snapshots of the liquid state in their training data.As an initial comparison of the performance between GP1, GP2, and other similar potential models, we evaluate how well they predict the elastic constants of fcc copper.The elastic constants C11, C22, and C44 are a widely used benchmark of copper potential model performance, allowing us to make a comparison between nine different copper potential models for which elastic constant data is available.We have plotted the maximum percent error in predicted elastic constants against the complexity of the model, as measured by number of nodes, in Figure 3.These errors, and all errors listed in this paper, are measured against each model's own target values, which are provided in the Supplementary Information.The potentials discovered by the machine learning algorithm presented in this work significantly change the Pareto frontier of interatomic potentials, defined as the set of interatomic potentials for which no other potential has less error and is less complex.They have errors comparable to the most accurate potential models and complexity comparable to the simplest.Table 3. Error of the values predicted by interatomic potentials for copper relative to the respective reference.The models displayed in this table are near the Pareto frontiers in Figure 3, values of other potentials are in Tables S2 to S7. C ij are elastic constants, a 0 is the lattice parameter, ΔE (bcc-fcc) is the energy difference between bcc and fcc phases, E v is the fcc bulk vacancy formation energy, E v (unrelaxed, 2×2×2) is the unrelaxed vacancy formation energy computed on a 2×2×2 supercell, E m is the migration energy for fcc bulk vacancy diffusion, E a is the activation energy for fcc bulk vacancy diffusion, E dumbbell is the dumbbell &lt;100&gt; formation energy, ν is the phonon frequency, and γ ISF and γ USF are the intrinsic and unstable stacking fault energies, respectively.There is also good agreement between the newly discovered potential models and other DFT-calculated properties (Table 3).Other models near the Pareto frontiers also show good agreement with their target values, but a notable difference is other than being more complex, these models were also directly trained on many of the properties listed in Table 3 whereas GP1 and GP2 were not.The errors on the elastic constants predicted by GP2 are almost as small as for EAM1, and the simpler model GP1 has errors on elastic constants that are comparable to ABCHM.The GP1 and GP2 models perform well on properties involving hcp and bcc phases, even though no hcp or bcc data were included in the training set.For the bcc lattice constant, the relative energy between the fcc and bcc phases, and the relative energy between fcc and hcp phases, GP1 and GP2 perform comparably to models that were trained on those data points and outperform all models that were not trained on them.</p>
<p>Property</p>
<p>For vacancy formation energies in the dilute limit, GP2 performs very well, with an error of 2 meV relative to the extrapolated DFT energy (see Supplementary Information for details).GP1 performs less well, with an error of 138 meV.Comparisons with other models for vacancy formation energies are difficult, as the models that report their performance on vacancy formation energies were trained with those values, whereas GP1 and GP2 were not.An exception is a neural network potential we discuss later, for which the extrapolated error is 146 meV, comparable to GP1 (Table S6, Supplementary Information).The GP1 error in vacancy formation energy is largely offset by an error in the opposite direction for migration energy, and as a result the errors for both GP1 and GP2 for the activation energy for vacancy-mediated diffusion are comparable to models that were trained on that value.GP1 and GP2 also demonstrate good predictive accuracy on phonon frequencies, which were also not included in their training set (Figure 4 and Table 3).On average, GP2 outperforms all other models on phonon frequencies used for validation, with a mean absolute error of 2.0%, and it also outperforms EAM1 on the phonon frequencies on which EAM1 was trained.GP1 does not do as well as GP2 on phonon frequencies, performing on average slightly better than EAM2 but worse than EAM1 and CuNi.The difference in the performance of GP1 and GP2 on phonons is evident in their calculated phonon dispersion curves (Figure 4).The strong performance of GP2 on phonon frequencies and elastic constants suggests that it does well at capturing the curvature of local minima on the potential energy surface, but it may not do as well in states away from the local minima, such as the vacancy formation energy of an unrelaxed 2×2×2 fcc unit cell (Table 3).</p>
<p>Both GP1 and GP2 perform better than the other models for the formation energy of a dumbbell defect.The absolute errors for GP1 and GP2 are only 49 meV and 56 meV respectively, as compared to an absolute prediction error of 250 meV for the ABCHM model (Table 3).Of the three models that included the dumbbell defect formation energy in their training data, the best has an absolute error that is about twice that of the absolute prediction error of GP2 (Table S4).On the other hand, both GP1 and GP2 underestimate the formation energy of a stable intrinsic stacking fault (see Supplementary Information for details) to a greater extent than the other models that report a comparison to this value.The largest absolute error, 29 mJ / m 2 for GP1, is 10.2 meV / atom along the (111) plane of the fault.GP1 and GP2 similarly underestimate the formation energy of an unstable stacking fault, but it is hard to assess how this compares to other models as none of the other models reports a benchmark value for the unstable stacking fault energy.</p>
<p>EAM-type models are well known to underpredict surface energies.Surface energies predicted by EAMtype models trained on ab-initio calculations for copper are about 40-50% below their target values for the (100), ( 110) and ( 111) surfaces (Table S6).In contrast, GP1 underpredicts these surface energies by only 8%, 1% and 5% respectively, and GP2 underpredicts them by 14%, 10% and 10% respectively (Figure 5).</p>
<p>For potentials that use experimental data for their target values, evaluating performance in calculating surface energies is more difficult as only the average value of experimental surface energies is available. 55,57,61To make this comparison we have calculated weighted average surface energies over 13 different low-index surface facets, where the weights are based on the relative surface areas in Wulff constructions (details are provided in the Supplementary Information) 63 .EAM1 and Cuu3 underpredict the weighted surface energies by about 30%, and CuNi overpredicts the weighted surface energies by about 10% (Table S6). 56GP1 underpredicts the weighted surface energies by 8% and GP2 by 13%.GP1-predicted surface energies are the most accurate of any of the evaluated EAM-type potential models relative to its target values.</p>
<p>The performance of GP1 and GP2 on surface energies is remarkable because there were no surfaces in the training set; this is a case of machine-learning potential models demonstrating extrapolative predictive ability.Similarly, both GP1 and GP2 demonstrated high predictive accuracy for the dumbbell defect compared to the other models, indicating that they are able to accurately predict energies in both lowcoordination and high-coordination environments.There are likely two reasons for the predictive accuracy of these models.The first is that other than SC, GP1 and GP2 are the simplest models considered here, and in general simpler models are less likely to overfit the training data. 49A similar trend of simpler models demonstrating greater extrapolative ability was observed by Zuo et al. in a recent comparison of different types of machine learned potential models. 19The second reason is that these models were discovered in a hypothesis space designed to contain models resembling those for which there is fundamental physical justification.In general, the more physics can be included in the machine learning procedure, the more likely it is that a model will have extrapolative predictive power.</p>
<p>Figure 5. Surface energies of elemental copper as computed using DFT, and the interatomic potentials GP1, GP2, and GP3.</p>
<p>When new data are added to the training set, the genetic programming search for new models can build off of what has been previously learned by using known high-performing models to seed the search.As a demonstration of this approach, we have performed an additional search using an augmented training set in which the 13 low-index surfaces (shown in Figure 5) were added to the training data and always included in the subsets of data used to evaluate candidate models.This search was seeded with GP1 and GP2, and as a result the models it discovered (Table S10) had many features in common with these.One of these models, which we label GP3 (equation ( 6)) resembles GP2 but, as expected, demonstrates better performance on surface energies (Figure 5).The absolute error for the weighted surface energies is 7% for GP3, as compared to 13% for GP2.The equation for GP3, which is slightly simpler than that of GP2, is provided below.</p>
<p>( ) ( )
1r r r f r r f r f r − − − + −   (6)
On average, the improved performance on surface energies for GP3 does not significantly affect its performance on the other properties listed in Table 3 compared to GP2.GP3 performs worse on average on elastic constants and phonon frequencies, but significantly better on the dumbbell formation energy and stacking fault energies.It is difficult to assess the extent to which these changes in performance can be attributed to the addition of surfaces to the training data due to the stochastic nature of the search.</p>
<p>Although GP1, GP2, and GP3 are simpler than many other EAM-type models, they have a similar computational cost when implemented in LAMMPS 29,64 due to the extensive use of tabulated values.Based on our benchmarks (Figure S1) GP1 takes 2.1 µs/step/atom, GP2 3.5 µs/step/atom, and GP3 takes 3.6 µs/step/atom, whereas EAM1 has a cost of 3.0 µs/step/atom.These speeds rank them among the fastest potential models, capable of modeling systems at large time and length scales. 29</p>
<p>DISCUSSION</p>
<p>There are advantages and disadvantages to the different approaches for using machine learning to generate potential models.In many machine learning approaches, including (but not limited to) neural network potentials, Gaussian approximation potentials, moment tensor potentials, SNAP potentials, and AGNI force fields, [1][2][3][4][5] the general idea is to construct a highly flexible hypothesis space that respects local symmetry and, with the help of large amounts of training data, identify the models within that hypothesis space that best reproduce the training data.Such models are capable of achieving very high accuracy for systems in which the local environments of the atoms are similar to those contained in the training set.[67] Here we have demonstrated that machine learning can also be used to develop the types of simple, fast potential models that are needed to model systems at extreme time and length scales.The key to our approach is to use genetic programming to search for computationally simple and efficient models in a hypothesis space that is constructed so that it contains simple models that are also physically meaningful.</p>
<p>The models are then selected based on a combination of simplicity, speed, and accuracy relative to the training data.9][70] For example, GP1 and GP2 were trained with 75 32-atom structures, for a total of 2400 atomic environments.For comparison, Artrith and Behler 71 have constructed a neural network potential for copper with a focus on surfaces.The potential was trained using 554,187 atomic environments, including tens of thousands of slabs and cluster structures.It performs comparably to GP1 and GP2 for many bulk properties, and much better for surface energies (Table S6, Supplementary Information).The neural network approach demonstrates very low errors on the types of systems on which it was trained, but as the genetic programming approach requires less training data it is likely that some accuracy can be recovered by using more accurate (and computationally expensive) methods to generate the training data.</p>
<p>The potential models discovered by the genetic programming approach are as fast as EAM-type models and demonstrate good predictive accuracy on properties they were not trained on.In particular, GP1 and GP2 show surprisingly good performance in predicting surface energies (the GP1 mean absolute error for surface energies is only 35mJ/m 2 ) despite the fact that there were no surfaces in their training data.Trained only on DFT data, the genetic programming algorithm found models that resemble widely-used glue potentials with a unique form for the many-body term that depends on the inverse of a sum over pair interactions.One of the advantages of generating potential models using simple analytical expressions is that it may be possible to analyze the expressions to get an insight into the underlying physical interactions that are responsible for the shape of the potential energy surface.</p>
<p>There are some notable limitations and areas for improvement for the approach presented here.For each system studied, it will be necessary to ensure that the hypothesis space contains simple expressions that capture important contributions to the potential energy; for example, for many systems it will likely be necessary to introduce terms that depend on bond angles, which was not done in this work.We used fixed inner and outer cutoff distances in this study, but it would almost certainly be better to let them vary as do other parameters of the potential.There is also the question of how to determine which of the models discovered by the genetic programming algorithm provide the best balance of speed and predictive accuracy.This could be achieved in a number of ways, 40,72,73 including by evaluating performance against validation data, but it is not clear which approach is best.Finally, the genetic programming approach is likely not suitable for on-the-fly learning.Because it is a stochastic method, it can take an indeterminate amount of time to find a set of promising models, and there is no guarantee that an incremental change to the training data will result in an incremental change to the shapes of the potential energy surfaces on the convex hull.Other potential model approaches are probably better-suited for this purpose.Despite these current limitations, our results demonstrate that machine learning holds great promise to improve the accuracy of atomistic calculations at extreme time and length scales.</p>
<p>METHODS</p>
<p>Description of the hypothesis space</p>
<p>Our machine learning algorithm uses genetic programming to search a hypothesis space of models that can be constructed by combining real numbers, addition, subtraction, multiplication, division, exponentiation, and a sum over neighbors of an atom.As discussed previously in the text, the hypothesis space was based on physical principles.Within this hypothesis space, each function can be represented as a tree graph, as shown in Figure 6.The space was constrained so that the maximum number of summations over neighbors was 6, no nested summations over neighbors were allowed, the maximum allowed depth of a tree was 32 and the maximum allowed number of nodes was 511.To ensure smoothness of the potential, all functions of distances are multiplied by the following smoothing function before the sum over neighbors is taken: 74
( )( ) ( ) 2 3 2 2 2 2 2 2 2 ( ) 2 3 in out out out in r r r r r r r f r − − + − − = (7)
where r in and r out are the inner and outer cutoff radii, for GP1 and GP2, r in = 3 Å and r out = 5 Å, including the 3 rd nearest neighbors. 55</p>
<p>Description of the algorithm</p>
<p>Genetic programming evolves computer programs following Darwin's natural selection by performing crossover and mutation operations on a set of individuals.Crossover was performed by 2 different operations: by randomly selecting a branch from one tree and replacing it with a randomly selected branch of another tree (Figure 7), and by creating a linear combination of 2 randomly selected branches from 2 different tress -the first method was randomly selected 90% of the time and the second one 10% of the time.The mutation operation performed 3 different sub-operations with equal probability: crossover of a tree with a randomly generated tree, swapping the arguments of a binary non-commutative function, and slightly modifying the expression tree by replacing (or inserting) a randomly selected non-terminal node with a randomly selected operator. 75The randomly generated trees were generated with the grow or full method with equal probability, 36 and the depth was drawn from a Gaussian distribution of mean 5 and standard deviation of 1.The overall algorithm performed crossover with a probability of 0.9, and mutation with a probability of 0.1.</p>
<p>Increasing diversity is known to improve the quality of the optimization. 75To increase diversity, we implemented a hierarchical way of creating separate environments in which the individuals (i.e., potential models) evolved.We ran the algorithm on 12 processors, and each processor had its own environment, consisting of a population of models and a subset of the training data.Conceptually this allows potentials within a specific environment to develop characteristics that are unique, increasing the diversity.Candidates for crossover and mutation were selected from 3 different sets of models with equal probability:</p>
<p>(1) The population of the current processor.Every 20,000 crossover and mutation operations, 100 individuals were selected based on their fitness (equation ( 5)) with Pareto tournament selection of size 10 while the rest were discarded. 72,76,77) A global set of models.Each processor tried to add the 100 individuals selected in part (1) to the global subset every 20,000 crossover and mutation operations.The models on the global set were then evaluated on the basis of speed (to model large time and length scales), fitness (for accurate results), and complexity (for generalizability).The speed of each model was estimated by the number of summations over neighbors.The complexity was evaluated by the number of nodes in the tree graph.</p>
<p>To identify the best models, we generated separate convex hulls with respect to fitness and complexity for each number of summations (speed) in a potential.Only the models on these convex hulls were retained in the global set.</p>
<p>(3) Individuals from other processors.Each processor was allowed to communicate with other processors every 5000 crossover and mutation operations, importing the current set of individuals from them.</p>
<p>Selection with equal probability was performed when getting an individual from the global set.Tournament selection of size 10 was used for getting individuals from the population of the current processor and from the populations of other processors.</p>
<p>The training data was also arranged in hierarchical subsets to increase diversity and reduce the speed of evaluating fitness.Globally, a subset of 75 energies, 75 forces, and 75 stresses was randomly sampled from the full set of training data every 20,000 crossover and mutation operations.The fitness of the global set of models was evaluated using this subset of training data.The training data on each processor (15-30 energies, forces and stresses) were randomly selected from the global subset of the training data, and this local subset was used to evaluate fitness locally on each processor.The subset of training data for each processor was selected from the global subset because individuals that migrate from a processor to the global set are more likely to survive if the environment is similar.</p>
<p>Optimization of potential model parameters was performed using the covariance matrix adaptation evolution strategy (CMA-ES) optimizer and a conjugate gradient (CG) optimizer. 78,79The CMA-ES algorithm was selected because it performs well in nonlinear or non-convex problems.The potential models on the global set of best individuals were optimized with the CMA-ES every 10,000 crossover and mutation operations by one processor.In contrast, the CG algorithm performed one optimization step for every individual generated by crossover or mutation.</p>
<p>The genetic programming algorithm took about 330 CPU-hours to find the exact Lennard-Jones potential, 3600 CPU-hours to find the exact Sutton Chen potential, and 360 CPU-hours to find GP1, GP2 and GP3.We note that it is likely that with additional tuning and performance enhancements the efficiency of the algorithm can be improved.</p>
<p>To facilitate this, our code is open source and available at https://gitlab.com/muellergroup/poet.</p>
<p>Details about the target data</p>
<p>The DFT data were computed using the Vienna Ab initio Simulation Package 80 (VASP) with the Perdew-Burke-Ernzerhof 81 (PBE) generalized gradient approximation (GGA) exchange correlation functional.The projector augmented wave method 82 (PAW) Cu_pv pseudopotential was used for copper.Efficient k-point grids were obtained from the k-point grid server with MINDISTANCE = 50Å. 83A cutoff energy of 750 eV and ADDGRID = TRUE in VASP were required to converge the stress tensor to less than 0.05 GPa.The elastic constants were converged to within 3 |% error| using a MINDISTANCE = 100Å.The DFT point defect energies were computed by linear extrapolation (see Supplementary Information for more details).The phonon dispersion curves were computed on a 3×3×3 supercell.The DFT calculation used a 5×5×5 kpoint grid and electronic self-consistency convergence of 10 -8 eV.The radial distribution function molecular dynamics simulations were performed in the NVT ensemble at the experimental 1400K liquid density on a 3×3×3 supercell.The temperature was increased from 300K to 2500K during 1 ps.Then the temperature was maintained at 2500K during 10 ps.Then, the temperature was decreased from 2500K to 1400K over 1ps.Then the temperature was maintained at 1400K during 1ps.Finally, the radial distribution function data was collected at 1400K over 40 ps.The DFT molecular dynamics for the radial distribution function was performed with a cutoff energy of 400 eV for the equilibration steps and 750 eV for the final 40 ps during which data was collected.Electronic self-consistency convergence was 10 -5 eV and only the k-point at Γ was used.For the computation of the fitness of the models, the energies were transformed by subtracting the minimum and dividing by the standard deviation, and the forces and stresses were standardized by subtracting the mean and dividing by the standard deviation.</p>
<p>The data used to rediscover the Lennard-Jones potential and the SC potential, and the data used to validate GP1 and GP2 were computed on LAMMPS.Instructions and files required to use GP1 and GP2 on LAMMPS are provided on the Supplementary Information.Lennard-Jones calculations used a cutoff distance of 7.5 Å, and SC, GP1, GP2 and GP3 calculations used a cutoff distance of 5 Å.</p>
<p>Benchmarking model speed</p>
<p>Benchmarking of model speed was done on a single core of a Haswell node with a clock speed of 2.5 GHz.The benchmarking simulation consisted of 10,000,000 molecular dynamics steps for a 32-atom unit cell.
FIGURE LEGENDS</p>
<p>Description of the potential models</p>
<p>Table S1.Acronyms used for the interatomic potential models Acronym Description Fitting procedure SC 1 Sutton-Chen (SC) EAM interatomic potential.Defined by a Finnis-Sinclair 2 potential with a van der Waals term for long-range interactions.Developed for metallic bonding and mechanical interactions between clusters Experimental data GP1, GP2 and GP3</p>
<p>Interatomic potential developed in this work Ab initio data EAM2 3 Compared against EAM1 in the article. 3Defined by a Morse 4 function, the universal equation of state 5  Developed for crystallization kinetics from deeply undercooled melts. 9 initio and experimental data.Crystalline, liquid and melting point data Cuu6 10 Developed and defined in the same way as Cuu3 but used more accurate vacancy formation energies</p>
<p>Experimental data</p>
<p>Cuu3 11 Defined by the universal equation of state 5 and the spherically averaged free-atom densities calculated from Hartree-Fock theory.The paper developed several EAM potentials to study the effects of stacking fault energy on dislocation nucleation (a) Ab initio and experimental data.Melting properties, crystalline properties, twin boundary energy and stacking fault energy Notes: (a) MCu31 was chosen for comparison here because it calculates stacking fault energies more accurately than the others. 15The equation for MCu31 was not provided, but it was described as being based on Cu2 and fit to additional training data, so we have assigned it the same complexity as Cu2.The acronyms of the models are taken from the original paper or adapted from the Interatomic Potentials Repository. 15ls that trained with ab-initio and experimental data ABCHM, Cu1, Cu2, MCu31 and EAM1 used experimental and ab initio data for training.About 40 % of the training data of ABCHM, Cu1, Cu2 and MCu31 was ab initio data; it included relative energies between phases, lattice parameters, vacancy formation energy and interstitial formation energy.EAM1 used initio data for relative energies of hcp, bcc, and fcc phases, and energies of the fcc phase and a diatomic molecule under strong compressions.</p>
<p>Comparison of GP1, GP2 and GP3 against EAM-type potentials</p>
<p>The errors of each of the interatomic potentials are obtained directly from the original paper in which the potential was first published, with one exception: the weighted surface energies of CuNi, EAM1 and Cuu3 were computed using the interatomic potentials from the Interatomic Potentials Repository using LAMMPS. 15,17e dumbbell was formed by displacing an atom along a &lt;100&gt; direction.The relaxed vacancy formation energy, the migration energy, the activation energy, and the dumbbell formation energy of GP1, GP2, GP3, EAM2, and EAM1 were computed with 6x6x6 supercells, CuNi used a supercell with 1200 atoms, Cuu3 500 atoms, and Cuu6 250 atoms.The DFT target values were obtained by linear extrapolation of the values at 2×2×2 and 3×3×3 supercells with respect to the inverse of the supercell size. 18The dumbbell formation energies of ABCHM, Cu1, Cu2, and MCu31 were computed with a 3×3×3 supercell.</p>
<p>Table S5 Errors on phonon frequencies</p>
<p>Model
Complexity ν L (X) ν T (X) ν L (L) ν T (L) ν L (K) ν T1 (K) ν T2 (K)</p>
<p> </p>
<p>Here, γ is the surface energy, A hkl is the total area of all the planes in the {hkl} family in the Wulff construction. 20The % error of the weighted surface energy was computed by comparing the experimental (except for GP1 and GP2) target value reported in the paper against the weighted surface energy predicted by the potential. 19,21The target value of GP1 and GP2 was the weighted surface energy computed by DFT.The DFT, GP1, GP2 and GP3 intrinsic stacking fault energy and unstable stacking fault energy were computed with a (111) slab, with a gap between periodic slabs of 20 Å and a thickness of 22 (111) atomic layers.The atoms were only allowed to relax in the direction normal to the slab interface for the USF computation.The intrinsic stacking fault and the unstable stacking fault were formed by displacing atoms above a { } 111 plane along a 211 direction by 0 / 6 a , and 0 /12 a , respectively..5 -0.4 Notes: properties in orange were used for training and properties in blue were used for validation.All the target properties were computed by DFT.The DFT dilute vacancy formation energies were obtained by linearly extrapolating the values at 2×2×2 and 3×3×3 with respect to the inverse of the supercell size. 18he dilute values for GP1, GP2, and GP3 were computed with a 6x6x6 supercell.(a) The dilute values for the neural network were computed by extrapolating the values at 2×2×2 and 3×3×3.</p>
<p>Comparison of GP1, GP2, and GP3 against neural network potential</p>
<p>Convex hull of models found by the machine learning algorithm</p>
<p>Table S9.Errors on different properties for models on the 3-dimensional convex hull in Table 1 of the main text, listed in the order they appear in the table.C ij are elastic constants, a 0 is the lattice parameter, ΔE (bcc-fcc) is the energy difference between bcc and fcc phases, E v is the fcc bulk vacancy formation energy, E v (unrelaxed, 2×2×2) is the unrelaxed vacancy formation energy computed on a 2×2×2 supercell, E m is the migration energy for fcc bulk vacancy diffusion, E a is the activation energy for fcc bulk vacancy diffusion, E dumbbell is the dumbbell &lt;100&gt; formation energy, ν is the phonon frequency, γ ISF and γ USF are the intrinsic and unstable stacking fault energies, respectively, γ is the average surface energy weighted according to the Wulff construction, and abs γ is the mean absolute surface energy over 13 surfaces.</p>
<p>Figure 1 .
1
Figure 1.Parity plots of training (orange) and validation (blue) energies, components of force and components of the virial stress tensor for the interatomic potential GP1 (a) and GP2 (b).The black dashed line is the identity.The mean absolute error (MAE) is presented above each sub-figure for validation and training data respectively.</p>
<p>Figure 2 .Figure 3 .
23
Figure 2. Radial distribution functions of liquid copper at 1400K</p>
<p>Figure 4 .
4
Figure 4. Calculated phonon dispersion curves for DFT, GP1, GP2, and GP3.</p>
<p>Figure 6 .Figure 7 .
67
Figure 6.Tree graphs of a) Lennard-Jones potential parametrized for argon, equation (1), b) Sutton-Chen EAM potential parametrized for copper, equation (3), c) GP1 and d) GP2</p>
<p>83</p>
<p>Wisesa, P., McGill, K. A. &amp; Mueller, T. Efficient generation of generalized Monkhorst-Pack grids through the use of informatics.Phys.Rev. B 93, 155109, doi:10.1103/PhysRevB.93.155109 (2016).</p>
<p>Figure 1 .
1
Figure 1.Parity plots of training (orange) and validation (blue) energies, components of force and components of the virial stress tensor for the interatomic potential GP1 (a) and GP2 (b).The black dashed line is the identity.The mean absolute error (MAE) is presented above each sub-figure for validation and training data respectively.</p>
<p>Figure 2 .Figure 3 .
23
Figure 2. Radial distribution functions of liquid copper at 1400K Figure 3. Pareto frontiers of interatomic potentials for copper.No model has less error and is less complex than a model in the Pareto frontier.The orange dashed line was the Pareto frontier before the development of GP1 and GP2, and the blue dashed line is the new Pareto frontier.The percent error for each model was evaluated against the model's own target values, described in the Supplementary Information.Complexity was measured by the number of nodes in the tree representation of the model.Because the smoothing function for some models is unknown, to construct this plot each smoothing function was counted as 2 nodes, representing the smoothing function and a multiplication operation.Sources: SC54, ABCHM56, Cu156, EAM155, EAM255, Cu259, Cuu660, Cuu361 and CuNi57.The interatomic potentials were found in the Interatomic Potentials Repository.62 GP1 and GP2 were developed in this work.</p>
<p>Figure 4 .
4
Figure 4. Calculated phonon dispersion curves for DFT, GP1, GP2, and GP3.</p>
<p>Figure 5 .
5
Figure 5. Surface energies of elemental copper as computed using DFT, and the interatomic potentials GP1, GP2, and GP3.</p>
<p>Figure 6 .Figure 7 .
67
Figure 6.Tree graphs of a) Lennard-Jones potential parametrized for argon, equation (1), b) Sutton-Chen EAM potential parametrized for copper, equation (3), c) GP1 and d) GP2 Figure 7. Example of a crossover operation</p>
<p>in orange were used for training and properties in blue were used for validation.(a) ab initio target data.(b) experimental target data.(c) fitted to the vacancy formation energy</p>
<p>in orange were used for training and properties in blue were used for validation.(a) ab initio target data.(b) experimental target data.</p>
<p>7 a -40.1 a -Notes: properties in orange were used for training and properties in blue were used for validation.(a) ab initio target data.(b) experimental target data.The weighted surface energy is:</p>
<p>Table 1 .
1
The 3-dimensional convex hull of models found by the machine learning algorithm
Fitness 5393157 1 Cost* Complexity Expression 2 ( ) rf r  1800.1 1 3 .2 0 ( ) r f r −  4 105.30 1 9.83 (6 4 9 .1 7 r −  8 54.144 1 10 10.20 5.49 ( 0 . 0 7 ) () 0 .0 9 ) ( ) fr − r r fr − − 26.906213r10.20 5.49 −r( ) 33.77 f r +(( ) f r)1 −8.1584215r10.21 5.48 −r( ) 1.19 f r +(0.33 ( ) r f r)1 −7.8230221(r10.21 5.47 −r−0.21 ) ( ) 0.97 r fr +(0.33 ( ) r fr)1 −7.82292250.999 ( r10.21 5.46 −r−0.21 ) ( ) 0.97 r fr +(0.33 ( ) r fr)1 −+5.767.4131419r10.21 5.48 −r( ( ) 3.07 f r +( ) f r)(0.31 ( ) r f r) ( 1 −( ) r f r)1 −4.72943287.33 r3.98 3.94 −r( ( ) 27.32 f r +−(11.1 3 0.03 r +11.74 2.93 −r) ( ) f r) (( ) f r)1 −4.29324296.76 r4.00 3.88 −r( ) 17.25 f r +(( ) f r)(r11.68 3.07 −r( ) f r)1 −+25.30(( ) f r)1 −Notes: the models with fitness 7.8230 and 4.7294 are named GP1 and GP2 respectively. "Cost" is based onthe number of summations. ( ) f r is the smoothing function defined in equation (7).</p>
<p>Table 2 .
2
Interatomic potentials near the Pareto frontiers in Figure3.
NameExpressionSC 54i E=j 9 4.5 64 2 ( ) fr r −  j 6 5 . 6 2 2 7 r( fr)  0.5GP1(1 10.2 5.470 . 2 1) ()0 . 7 90 . 3 3 ()1</p>
<p>Table S2 .
S2
Errors on elastic constants and lattice parameters in orange were used for training and properties in blue were used for validation.(a) experiment target data.(b) fit to bulk modulus.(c) ab initio calculation target data.
ModelComplexityC11C12C44a0 (FCC) a0 (BCC)Description Number of nodes ErrorErrorErrorErrorErrorUnitsCount%%%%%SC15-3.6 a, b3.8 a, b-29.4 a, b0.0 a-GP1215.8 c7.0 c-2.0 c-0.3 c0.1 cGP3262.9 c2.5 c-0.4 c0.2 c-0.2 cGP228-0.7 c0.5 c-1.2 c0.3 c-0.1 cEAM21131.9 a0.2 a0.0 a0.0 a-ABCHM146-0.6 a-4.1 a-6.6 a-0.7 c2.4 cCuNi1500.1 a0.0 a0.0 a0.0 a0.9 cEAM1158-0.1 a0.1 a0.5 a0.0 a-Cu13482.9 a4.1 a10.5 a0.0 c0.0 cCuu6503-1.2 a1.2 a4.2 a0.0 a-Cuu3503-1.8 a1.2 a0.3 a0.0 a-Cu25842.4 a3.3 a10.5 a0.0 c0.0 cMCu31584-1.2 a6.5 a13.2 a0.0 c-Notes: properties</p>
<p>Table S3 .
S3
Errors on difference between energies of FCC, BCC and HCP phases Notes: properties in orange were used for training and properties in blue were used for validation.All values are from ab initio calculations.(a) EAM2 was fit subject to the requirement that the fcc structure be more stable than bcc or hcp.
ModelComplexityΔE(BCC-FCC) ΔE(HCP-FCC)DescriptionNumber of nodesPred.-Targ.Pred.-Targ.UnitsCountmeV/atommeV/atomGP1218-3GP32612-1GP2284-2EAM21132 a-6 aABCHM146-11-2CuNi150-13-4EAM11582-4Cu134850Cu258461MCu315840-6</p>
<p>Table S4 .
S4
Errors on bulk vacancy formation energy, migration energy, activation energy and dumbbell &lt;100&gt; formation energy
ModelComplexityE v (unrelaxed,2×2×2) E vE mE aE dumbbellDescription Num. of nodes Pred.-Targ.Pred.-Targ.Pred.-Targ.Pred.-Targ. Pred.-Targ.UnitsCountmeVmeVmeVmeVmeVGP12132 a138 a-106 a32 a49 aGP326-106 a12 a-36 a-15 aGP228-123 a2 a-37 a-34 a-56 aEAM2113--17 b-20 b-37 b-ABCHM14680 a---24 b, c</p>
<p>Table S6 .
S6
Prediction errors for surface energies
ModelComplexity%error of% error% error% errorMean absoluteweighted% error.surface energyDescription Number of nodes 13 surfaces(100)(110)(111)13 surfacesUnitsCount%%%%%GP121-7.6 a-8.2 a-1.3 a-4.8 a2.3 aGP326-7.4 a-10.5 a-6.1 a-5.1 a4.4 aGP228-12.6 a-14.2 a-9.7 a-10.5 a7.2 aABCHM146--49.7 a-47.4 a-53.8 a-CuNi1509.8 b----EAM1158-28.4 b----Cu1348--50.0 a-48.4 a-53.8 a-Cuu3503-31.8 b----MCu31584--39.3 a-37.</p>
<p>Table S7
S7
Prediction errors for the intrinsic stacking fault (γ ISF ) energy and the unstable stacking fault (γ USF ) energy Notes: properties in orange were used for training and properties in blue were used for validation.(a) ab initio target data.(b) experimental target data.
ModelComplexity pred. -ref. (mJ/m 2 )pred. -ref. (mJ/m 2 )Description Number ofγ ISFγ USFnodesUnitsCount%%GP121-29 a-44 aGP326-6 a-27 aGP228-20 a-31 aEAM2113-9 b-CuNi1500 b-EAM1158-1 b-MCu315846 b1 a</p>
<p>Table S8 .
S8
Comparison between genetic programming potentials and a neural network potential.22
DetailsGP1GP3GP2Neural networkParametersCount5782521Atomic environments inCount240026512400554,187training setEnergy MAEmeV/atom3.52.52.72.2Force MAEmeV/Å75626056C 11% error5.82.9-0.72.3C 12% error7.02.50.5-3.3C 44% error-2.0-0.4-1.23.8a 0 (fcc)% error-0.30.20.30.0a 0 (bcc)% error0.1-0.2-0.10.1ΔE (bcc-fcc)pred. -ref. (meV/atom) 8124-4ΔE (hcp-fcc)pred. -ref. (meV/atom) -3-1-2-7E vpred. -ref. (meV)136100146 aE v (relaxed, 3×3×3)pred. -ref. (meV)111-15-26106E v (relaxed, 2×2×2) E v (unrelaxed)pred. -ref. (meV) pred. -ref. (meV)53 142-75 2-88 -1510 147 aE v (unrelaxed, 3×3×3)pred. -ref. (meV)109-31-47103E v (unrelaxed, 2×2×2)pred. -ref. (meV)32-106-123-1(100) surface energy% error-8.2-10.5-14.20.5(110) surface energy% error-1.3-6.1-9.71.5(111) surface energy% error-4.8-5.1-10
ACKNOWLEDGEMENTSThis work was done using high-performance computing resources from the Maryland Advanced Research Computing Cluster (MARCC) and from the Homewood High-Performance Cluster (HHPC).The authors thank Qing-Jie Li, Zhao Fan, Dihui Ruan and the members of the Mueller Research Group for useful discussions.DATA AVAILABILITYOur code is open source and available at https://gitlab.com/muellergroup/poet.The instructions and files required to use GP1, GP2 and GP3 on LAMMPS are provided in the Supplementary Information.The data used to train the models is provided in the Supplementary Information.FUNDINGThe authors acknowledge financial support from the Office of Naval Research, grant number N000141512665.COMPETING INTERESTSThe authors declare no competing financial interests.SUPPLEMENTARY INFORMATIONThe Supplementary Information shows tables of the errors of the predictions of the interatomic potentials on: elastic constants, lattice parameters, energy difference between phases, vacancy formation energies, migration and activation energies, and surface energies.FigureS1shows the computational cost of LJ, SC, GP1, GP2, GP3 and EAM1 as implemented in LAMMPS.FigureS2shows that the Pareto frontier of absolute percent error on elastic constants against complexity does not change when using the average instead of the maximum error.The Supplementary Information includes the instructions and the files required for enabling GP1, GP2 and GP3 in LAMMPS.Supplementary informationNotes: the model with fitness 5.98 is GP3."Cost" is based on the number of summations.( ) f r is the smoothing function defined in the main manuscriptComputational cost benchmarks of GP1 and GP2The interatomic potentials GP1 and GP2 were implemented in LAMMPS.The performance was measured in a system with 32 atoms for 10 million relaxation steps with a timestep of 1 fs in a single core.The paper by Sutton and Chen does not report a cutoff radius, but we found that a radius of at least 10 Å was required to reproduce their results.If we use the same 5 Å radius as used for GP1 and GP2, Sutton Chen EAM and GP1 have similar speeds because both are EAM-type potentials with 2 summations.EAM1 is slower than GP1 (FigureS1) because it used a greater cutoff distance of 5.50679 Å. GP2 is slower because it has 3 summations.However the difference in speed between the potentials compared in FigureS1is small compared to the difference in speed between EAM and other potential models, which can be several orders of magnitude.23,24.The computational cost was measured on a single core of a Haswell node with a clock speed of 2.5 GHz.The poet pair_style can be compiled in LAMMPS following these steps:1. Copy the files "pair_poet.cpp" and "pair_poet.h"(available in the Supplementary Information) to <lammps_main_directory>/src/MANYBODY and edit the file <lammps_main_directory>/src/Makefile.list by adding "pair_poet.cpp" and "pair_poet.h" to the end of the respective lines 2. make LAMMPS by including the "yes-manybody" flag
Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons. A P Bartók, M C Payne, R Kondor, G Csányi, 10.1103/PhysRevLett.104.136403Phys. Rev. Lett. 1041364032010</p>
<p>Spectral neighbor analysis method for automated generation of quantum-accurate interatomic potentials. A P Thompson, L P Swiler, C R Trott, S M Foiles, G J Tucker, 10.1016/j.jcp.2014.12.018J. Comput. Phys. 2852015</p>
<p>Generalized neural-network representation of high-dimensional potential-energy surfaces. J Behler, M Parrinello, Physical Review Letters. 981464012007</p>
<p>A universal strategy for the creation of machine learning-based atomistic force fields. T D Huan, 10.1038/s41524-017-0042-yComputational Materials. 3372017</p>
<p>Moment Tensor Potentials: A Class of Systematically Improvable Interatomic Potentials. A Shapeev, 10.1137/15M1054183Multiscale Modeling &amp; Simulation. 142016</p>
<p>Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning. M Rupp, A Tkatchenko, K.-R Müller, O A Von Lilienfeld, 10.1103/PhysRevLett.108.058301Physical Review Letters. 108583012012</p>
<p>Support vector machine regression (LS-SVM)-an alternative to artificial neural networks (ANNs) for the analysis of quantum chemistry data?. R M Balabin, E I Lomakina, 10.1039/C1CP00051APhysical Chemistry Chemical Physics. 132011</p>
<p>First-principles interatomic potentials for ten elemental metals via compressed sensing. A Seko, A Takahashi, I Tanaka, 10.1103/PhysRevB.92.054113Physical Review B. 92541132015</p>
<p>Efficient hybrid evolutionary optimization of interatomic potential models. W M Brown, A P Thompson, P A Schultz, 10.1063/1.3294562The Journal of Chemical Physics. 132241082010</p>
<p>Bayesian approach to cluster expansions. T Mueller, G Ceder, 10.1103/PhysRevB.80.024103Physical Review B. 80241032009</p>
<p>Perspective: Machine learning potentials for atomistic simulations. J Behler, 10.1063/1.4966192The Journal of Chemical Physics. 1451709012016</p>
<p>Machine learning of accurate energy-conserving molecular force fields. S Chmiela, 10.1126/sciadv.1603015Science Advances. 3e16030152017</p>
<p>Molecular Dynamics with On-the-Fly Machine Learning of Quantum-Mechanical Forces. Z Li, J R Kermode, A De Vita, 10.1103/PhysRevLett.114.096405Physical Review Letters. 114964052015</p>
<p>Adaptive machine learning framework to accelerate ab initio molecular dynamics. V Botu, R Ramprasad, 10.1002/qua.24836International Journal of Quantum Chemistry. 1152015</p>
<p>Efficient and accurate machine-learning interpolation of atomic energies in compositions with many species. N Artrith, A Urban, G Ceder, 10.1103/PhysRevB.96.014112Physical Review B. 96141122017</p>
<p>ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. J S Smith, O Isayev, A E Roitberg, 10.1039/C6SC05720AChemical Science. 82017</p>
<p>The Use of Cluster Expansions To Predict the Structures and Properties of Surfaces and Nanostructured Materials. L Cao, C Li, T Mueller, 10.1021/acs.jcim.8b00413Journal of Chemical Information and Modeling. 582018</p>
<p>Deep Potential Molecular Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics. L Zhang, J Han, H Wang, R Car, E , W , 10.1103/PhysRevLett.120.143001Physical Review Letters. 1201430012018</p>
<p>Yunxing Zuo, C C , Xiangguo Li, Zhi Deng, Yiming Chen, Jörg Behler, Gábor Csányi, Alexander V Shapeev, Aidan P Thompson, Mitchell A Wood, arXiv:1906.08888v3Shyue Ping Ong. A Performance and Cost Assessment of Machine Learning Interatomic Potentials. 2019physics.comp-ph</p>
<p>Machine-learned multi-system surrogate models for materials prediction. C Nyshadham, 10.1038/s41524-019-0189-9Computational Materials. 5512019</p>
<p>T Mueller, A G Kusne, R Ramprasad, Reviews in Computational Chemistry. Abby L Parrill, B Kenneth, Lipkowitz, John Wiley &amp; Sons, Inc201629</p>
<p>Embedded-atom method: Derivation and application to impurities, surfaces, and other defects in metals. M S Daw, M I Baskes, 10.1103/PhysRevB.29.6443Physical Review B. 291984</p>
<p>A simple empirical N-body potential for transition metals. M W Finnis, J E Sinclair, 10.1080/01418618408244210Philosophical Magazine A. 501984</p>
<p>Simulation of gold in the glue model. F Ercolessi, M P , E Tosatti, 10.1080/01418618808205184Philosophical Magazine A. 581988</p>
<p>Quantum-Based Analytic Interatomic Forces and Materials Simulation. D W Brenner, O A Shenderova, D A Areshkin, 10.1002/9780470125892.ch4Reviews in Computational Chemistry. 1998</p>
<p>New empirical approach for the structure and energy of covalent systems. J Tersoff, Physical Review B. 371988</p>
<p>Three decades of many-body potentials in materials research. S B Sinnott, D W Brenner, 10.1557/mrs.2012.88MRS Bulletin. 372012</p>
<p>Concepts for simulating and understanding materials at the atomic scale. M W Finnis, 10.1557/mrs.2012.92MRS Bulletin. 372012</p>
<p>Computational aspects of many-body potentials. S J Plimpton, A P Thompson, 10.1557/mrs.2012.96MRS Bulletin. 372012</p>
<p>Machine Learning Classical Interatomic Potentials for Molecular Dynamics from First-Principles Training Data. H Chan, 10.1021/acs.jpcc.8b09917The Journal of Physical Chemistry C. 1232019</p>
<p>Machine Learning Force Field Parameters from Ab Initio Data. Y Li, 10.1021/acs.jctc.7b00521Journal of Chemical Theory and Computation. 132017</p>
<p>Determination of best-fit potential parameters for a reactive force field using a genetic algorithm. P Pahari, S Chaturvedi, 10.1007/s00894-011-1124-2Journal of Molecular Modeling. 182012</p>
<p>Global optimization of parameters in the reactive force field ReaxFF for SiOH. H R Larsson, A C T Van Duin, B Hartke, 10.1002/jcc.23382J. Comput. Chem. 342013</p>
<p>Towards accurate prediction of catalytic activity in IrO2 nanoclusters via first principles-based variable charge force field. F G Sen, 10.1039/C5TA04678EJournal of Materials Chemistry A. 32015</p>
<p>Ab Initio-Based Bond Order Potential to Investigate Low Thermal Conductivity of Stanene Nanostructures. M J Cherukara, 10.1021/acs.jpclett.6b01562The Journal of Physical Chemistry Letters. 72016</p>
<p>Genetic programming -on the programming of computers by means of natural selection. J R Koza, 1992MIT Press</p>
<p>Tim Mueller, A G K , Rampi Ramprasad, Reviews in Computational Chemistry. L Parrill, Kenny B Lipkowitz, John Wiley &amp; Sons, Inc201629188</p>
<p>Distilling Free-Form Natural Laws from Experimental Data. M Schmidt, H Lipson, 10.1126/science.1165893Science. 3242009</p>
<p>Origins of hole traps in hydrogenated nanocrystalline and amorphous silicon revealed through machine learning. T Mueller, E Johlin, J C Grossman, Physical Review B. 891152022014</p>
<p>Identifying models of dielectric breakdown strength from high-throughput data via genetic programming. F Yuan, T Mueller, 10.1038/s41598-017-17535-3Scientific Reports. 7175942017</p>
<p>Searching for globally optimal functional forms for interatomic potentials using genetic programming with parallel tempering. A Slepoy, M D Peters, A P Thompson, 10.1002/jcc.20710J. Comput. Chem. 282007</p>
<p>Symbolic Regression of Inter-Atomic Potentials via Genetic Programming. Abdel Kenoufi, K T K , Biological and Chemical Research. 22015</p>
<p>Fitting potential-energy surfaces: A search in the function space by directed genetic programming. D E Makarov, H Metiu, 10.1063/1.475421The Journal of Chemical Physics. 1081998</p>
<p>. J E Lennard-Jones, 10.1088/0959-5309/43/5/301Cohesion. Proceedings of the Physical Society. 431931</p>
<p>Mémoires sur l'électricité et la magnétisme. C A Coulomb, Chez Bachelier, libraire, 1789</p>
<p>Diatomic Molecules According to the Wave Mechanics. II. Vibrational Levels. P M Morse, Physical Review. 341929</p>
<p>Relationship between the embedded-atom method and Tersoff potentials. D W Brenner, Physical Review Letters. 631989</p>
<p>Empirical chemical pseudopotential theory of molecular and metallic bonding. G C Abell, 10.1103/PhysRevB.31.6184Physical Review B. 311985</p>
<p>The Problem of Overfitting. D M Hawkins, 10.1021/ci0342472Journal of Chemical Information and Computer Sciences. 442004</p>
<p>Interatomic Potentials from First-Principles Calculations: The Force-Matching Method. F Ercolessi, J B Adams, 10.1209/0295-5075/26/8/005Europhysics Letters (EPL). 261994</p>
<p>L D Cloutman, 10.2172/793685A Selected Library of Transport Coefficients for Combustion and Plasma Physics Applications. 2000</p>
<p>. P Hohenberg, W Kohn, 10.1103/PhysRev.136.B864Inhomogeneous Electron Gas. Physical Review. 1361964</p>
<p>Accurate force field for molybdenum by machine learning large materials data. C Chen, 10.1103/PhysRevMaterials.1.043603Phys. Rev. Materials. 1436032017</p>
<p>Long-range Finnis-Sinclair potentials. A P Sutton, J Chen, 10.1080/09500839008206493Philosophical Magazine Letters. 611990</p>
<p>Structural stability and lattice defects in copper: Ab initio, tight-binding, and embedded-atom calculations. Y Mishin, M J Mehl, D A Papaconstantopoulos, A F Voter, J D Kress, 10.1103/PhysRevB.63.224106Physical Review B. 632241062001</p>
<p>Analysis of semi-empirical interatomic potentials appropriate for simulation of crystalline and liquid Al and Cu. M I Mendelev, M J Kramer, C A Becker, M Asta, 10.1080/14786430802206482Philosophical Magazine. 882008</p>
<p>An optimized interatomic potential for Cu-Ni alloys with the embedded-atom method. B Onat, S Durukanoğlu, 10.1088/0953-8984/26/3/035404Journal of Physics: Condensed Matter. 26354042013</p>
<p>Universal features of the equation of state of metals. J H Rose, J R Smith, F Guinea, J Ferrante, 10.1103/PhysRevB.29.2963Physical Review B. 291984</p>
<p>The interactions of self-interstitials with twin boundaries. M I Mendelev, A H King, 10.1080/14786435.2012.747012Philosophical Magazine. 932013</p>
<p>Self-diffusion and impurity diffusion of fee metals using the five-frequency model and the Embedded Atom Method. J B Adams, S M Foiles, W G Wolfer, 10.1557/JMR.1989.0102Journal of Materials Research. 41989</p>
<p>Embedded-atom-method functions for the fcc metals Cu, Ag, Au, Ni, Pd, Pt, and their alloys. S M Foiles, M I Baskes, M S Daw, 10.1103/PhysRevB.33.7983Physical Review B. 331986</p>
<p>Considerations for choosing and using force fields and interatomic potentials in materials science and engineering. Current Opinion in Solid State and Materials Science. C A Becker, F Tavazza, Z T Trautt, R A Buarque De Macedo, 10.1016/j.cossms.2013.10.001201317</p>
<p>Zur Frage der Geschwindigkeit des Wachstums und der Auflösung der Krystallflagen. G Wulff, Z. Kryst. Mineral. 341901</p>
<p>Fast Parallel Algorithms for Short-Range Molecular Dynamics. S Plimpton, 10.1006/JCPH.1995.1039Journal of Computational Physics. 1171995</p>
<p>Extending the accuracy of the SNAP interatomic potential form. M A Wood, A P Thompson, W , B D , 10.1063/1.5017641The Journal of Chemical Physics. 1482417212018</p>
<p>The TensorMol-0.1 model chemistry: a neural network augmented with long-range physics. K Yao, J E Herr, David W Toth, R Mckintyre, J Parkhill, 10.1039/C7SC04934JChemical Science. 92018</p>
<p>J Behler, Chemical Modelling: Applications and Theory. The Royal Society of Chemistry20107</p>
<p>Machine learning based interatomic potential for amorphous carbon. V L Deringer, G Csányi, 10.1103/PhysRevB.95.094203Physical Review B. 95942032017</p>
<p>Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations. J Behler, Phys. Chem. Chem. Phys. 132011</p>
<p>Accuracy and transferability of Gaussian approximation potential models for tungsten. W J Szlachta, A P Bartók, G Csányi, Physical Review B. 901041082014</p>
<p>High-dimensional neural network potentials for metal surfaces: A prototype study for copper. N Artrith, J Behler, 10.1103/PhysRevB.85.045439Physical Review B. 85454392012</p>
<p>G F Smits, M Kotanchek, Genetic Programming Theory and Practice II 283-299. Springer2005</p>
<p>C E Borges, Proceedings of the 12th annual conference on Genetic and evolutionary computation. the 12th annual conference on Genetic and evolutionary computationPortland, Oregon, USAACM2010</p>
<p>Optimal spline cutoffs for Coulomb and van der Waals interactions. H.-Q Ding, N Karasawa, W A Goddard, 10.1016/0009-2614(92)85708-IChemical Physics Letters. 1931992</p>
<p>. R Poli, W B Langdon, N F Mcphee, A Field Guide to Genetic Programming. 2008</p>
<p>A niched Pareto genetic algorithm for multiobjective optimization. Jeffrey Horn, N N , David E Goldberg, Proc. First IEEE Conf. Evolutionary Computation. First IEEE Conf. Evolutionary Computation1994</p>
<p>Selection Based on the Pareto Nondomination Criterion for Controlling Code Growth in Genetic Programming. A Ekárt, S Z Németh, 10.1023/A:1010070616149Genetic Programming and Evolvable Machines. 22001</p>
<p>N Hansen, A Ostermeier, Proceedings of IEEE International Conference on Evolutionary Computation. IEEE International Conference on Evolutionary Computation1996</p>
<p>Note surla convergence des methodes de directions conjuguees. E Polak, G Ribiere, Imform. Rech. Oper. 16351969</p>
<p>Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. G Kresse, J Furthmuller, Phys. Rev. B. 54111691996</p>
<p>Generalized Gradient Approximation Made Simple. J P Perdew, K Burke, M Ernzerhof, 10.1103/PhysRevLett.77.3865Phys. Rev. Lett. 771996</p>
<p>Projector augmented-wave method. P E Blöchl, 10.1103/PhysRevB.50.17953Phys. Rev. B. 501994</p>
<p>Long-range Finnis-Sinclair potentials. A P Sutton, J Chen, 10.1080/09500839008206493Philosophical Magazine Letters. 611990</p>
<p>A simple empirical N-body potential for transition metals. M W Finnis, J E Sinclair, 10.1080/01418618408244210Philosophical Magazine A. 501984</p>
<p>Structural stability and lattice defects in copper: Ab initio, tight-binding, and embedded-atom calculations. Y Mishin, M J Mehl, D A Papaconstantopoulos, A F Voter, J D Kress, 10.1103/PhysRevB.63.224106Physical Review B. 632241062001</p>
<p>Diatomic Molecules According to the Wave Mechanics. II. Vibrational Levels. P M Morse, 10.1103/PhysRev.34.57Physical Review. 341929</p>
<p>Universal features of the equation of state of metals. J H Rose, J R Smith, F Guinea, J Ferrante, 10.1103/PhysRevB.29.2963Physical Review B. 291984</p>
<p>Analysis of semi-empirical interatomic potentials appropriate for simulation of crystalline and liquid Al and Cu. M I Mendelev, M J Kramer, C A Becker, M Asta, 10.1080/14786430802206482Philosophical Magazine. 882008</p>
<p>Computer simulation of point defect properties in dilute Fe-Cu alloy using a many-body interatomic potential. G J Ackland, D J Bacon, A F Calder, T Harry, 10.1080/01418619708207198Philosophical Magazine A. 751997</p>
<p>An optimized interatomic potential for Cu-Ni alloys with the embedded-atom method. B Onat, S Durukanoğlu, 10.1088/0953-8984/26/3/035404Journal of Physics: Condensed Matter. 26354042013</p>
<p>The interactions of self-interstitials with twin boundaries. M I Mendelev, A H King, 10.1080/14786435.2012.747012Philosophical Magazine. 932013</p>
<p>Self-diffusion and impurity diffusion of fee metals using the five-frequency model and the Embedded Atom Method. J B Adams, S M Foiles, W G Wolfer, 10.1557/JMR.1989.0102Journal of Materials Research. 41989</p>
<p>Embedded-atom-method functions for the fcc metals Cu, Ag, Au, Ni, Pd, Pt, and their alloys. S M Foiles, M I Baskes, M S Daw, 10.1103/PhysRevB.33.7983Physical Review B. 331986</p>
<p>Roothaan-Hartree-Fock atomic wave functions Slater basis-set expansions for Z = 55-92. Atomic Data and Nuclear Data Tables. A D Mclean, R S Mclean, org/10.1016/0092-640X(81)90012-7198126</p>
<p>Roothaan-Hartree-Fock atomic wavefunctions: Basis functions and their coefficients for ground and certain excited states of neutral and ionized atoms, Z≤54. Atomic Data and Nuclear Data Tables. E Clementi, C Roetti, org/10.1016/S0092-640X(74)80016-1197414</p>
<p>Effects of stable and unstable stacking fault energy on dislocation nucleation in nano-crystalline metals. V Borovikov, M I Mendelev, A H King, 10.1088/0965-0393/24/8/085017Modelling and Simulation in Materials Science and Engineering. 24850172016</p>
<p>Considerations for choosing and using force fields and interatomic potentials in materials science and engineering. Current Opinion in Solid State and Materials Science. C A Becker, F Tavazza, Z T Trautt, R A Buarque De Macedo, 10.1016/j.cossms.2013.10.001201317</p>
<p>Interatomic potentials for monoatomic metals from experimental data and ab initio calculations. Y Mishin, D Farkas, M J Mehl, D A Papaconstantopoulos, 10.1103/PhysRevB.59.3393Physical Review B. 591999</p>
<p>Fast Parallel Algorithms for Short-Range Molecular Dynamics. S Plimpton, 10.1006/JCPH.1995.1039Journal of Computational Physics. 1171995</p>
<p>Elemental vacancy diffusion database from high-throughput first-principles calculations for fcc and hcp structures. T Angsten, T Mayeshiba, H Wu, D Morgan, 10.1088/1367-2630/16/1/015018New Journal of Physics. 16150182014</p>
<p>Surface energies of elemental crystals. R Tran, 10.1038/sdata.2016.80Scientific Data. 31600802016</p>
<p>Zur Frage der Geschwindigkeit des Wachstums und der Auflösung der Krystallflagen. G Wulff, Z. Kryst. Mineral. 341901</p>
<p>Python Materials Genomics (pymatgen): A robust, open-source python library for materials analysis. S P Ong, 10.1016/j.commatsci.2012.10.028Computational Materials Science. 682013</p>
<p>High-dimensional neural network potentials for metal surfaces: A prototype study for copper. N Artrith, J Behler, 10.1103/PhysRevB.85.045439Physical Review B. 85454392012</p>
<p>Computational aspects of many-body potentials. S J Plimpton, A P Thompson, 10.1557/mrs.2012.96MRS Bulletin. 372012</p>
<p>Ab initio based empirical potential used to study the mechanical properties of molybdenum. H Park, 10.1103/PhysRevB.85.214121Phys. Rev. B. 852141212012</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9329 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9329</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9329</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-273350705</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.11020v5.pdf" target="_blank">Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), primarily built on decoder-only transformer architectures, excel in natural language generation tasks and have shown promise in adapting to diverse downstream tasks using zero-shot and few-shot prompting techniques. However, these prompting methods often fall short on natural language understanding (NLU) tasks, where smaller encoder-only models like BERT-base consistently outperform LLMs on benchmarks such as GLUE and SuperGLUE. In this paper, we explore two approaches—supervised fine-tuning and proximal policy optimization (PPO)—to enhance the NLU capabilities of LLMs. To reduce the computational cost of full-model fine-tuning, we integrate low-rank adaptation (LoRA) layers, restricting updates to these layers during both supervised fine-tuning and PPO stages. In the supervised fine-tuning approach, task-specific prompts are concatenated with input queries and ground-truth labels from the NLU training corpus, optimizing the model using the next-token prediction objective. Despite this, LLMs still underperform compared to encoder-only models like BERT-base on several NLU tasks. To address this gap, we employ PPO, a reinforcement learning technique that treats each token generation as an action and evaluates the sequence of generated tokens using a reward function based on their alignment with ground-truth answers. PPO then updates the model to maximize these rewards, effectively aligning its outputs with the correct labels. Our experiments with the LLAMA2-7B model demonstrate that PPO-based fine-tuning significantly improves performance, delivering an average gain of 6.3 points over supervised fine-tuning on the GLUE benchmark. PPO surpasses zero-shot prompting by 38.7 points and few-shot prompting by 26.1 points on GLUE, while also outperforming these baselines by 28.8 and 28.5 points on SuperGLUE. Additionally, PPO exceeds the performance of BERT-large, a strong baseline, with an average improvement of 2.7 points on GLUE and 9.3 points on SuperGLUE. These improvements are consistent</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9329.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9329.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs Few-shot vs SFT vs PPO (GLUE avg)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of zero-shot prompting, few-shot prompting, supervised fine-tuning (SFT), and PPO fine-tuning on GLUE average</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing how prompt presentation (zero/few-shot) and fine-tuning paradigm (SFT vs PPO) affect LLAMA2-7B-chat-hf performance on GLUE; PPO yields the highest average, with SFT improving over prompting but trailing PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GLUE (aggregate over GLUE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Suite of natural language understanding tasks (classification, regression, NLI, acceptability, semantic textual similarity) aggregated to a single average score.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting using a task-specific prompt (task description + required strict output format between '<Judgement></Judgement>' tags); few-shot prompting with 1-5 labeled demonstrations prepended to the prompt (sweep over 1-5 shots, report best); SFT: supervised next-token prediction on prompt + input + ground-truth answer; PPO: same prompt format with generation constrained to '<Judgement>' tags but model optimized with PPO using per-example reward extracted by regex from the generated tags.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot prompting; Few-shot prompting (1-5 examples); Supervised fine-tuning (SFT) using LoRA; PPO fine-tuning using LoRA with reward from exact label match.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PPO average: 84.6 (average across GLUE datasets reported for LLAMA2-7B-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SFT average: 78.5; Few-shot prompting: 58.7; Zero-shot prompting: 46.1; BERT-base (encoder baseline): 79.6; BERT-large: 82.1</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>PPO vs SFT: +6.1 points on GLUE average (paper also reports +6.3 points in places); Few-shot vs Zero-shot: +12.6 points (58.7 vs 46.1); PPO vs Few-shot: +25.9 points; PPO vs Zero-shot: +38.5 points</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute PPO's improvement to reward-driven optimization aligning generation with task labels while maintaining proximity to the base policy via PPO's clipping/KL penalty; SFT helps but may narrow the model's distribution to training data and underperform on some generalization aspects. Prompting alone (zero/few-shot) is insufficient for LLMs under 14B to reach encoder-model performance on GLUE without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruction-tuned LLAMA2-7B-chat-hf, LoRA rank r=16 for adapters; few-shot sweep 1-5 shots and best reported; generation: multinomial sampling temperature=1, one response per sample, lengths 12-32 tokens ending with '</Judgement>'; evaluation from GLUE servers for test sets; numbers reported are dataset averages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9329.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt format + Model family (STS-B & COPA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of zero-shot / few-shot prompting and PPO fine-tuning across model families on STS-B (Spearman) and COPA (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that the same prompt presentation formats (zero-shot, few-shot) have very different baseline performance across LLM families (LLAMA2, Qwen2.5, MPT), and that PPO fine-tuning substantially raises performance for each.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: LLAMA2-7B-chat-hf, Qwen2.5-7B-Instruct, MPT-7B-chat (evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (for each listed LLM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>STS-B (semantic textual similarity, Spearman correlation) and COPA (choice of cause/effect, accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>STS-B: regression-style semantic similarity scoring (reported as Spearman correlation). COPA: commonsense causal choice (two-way) reported as accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting with task-specific prompt and strict '<Judgement>' tag output; Few-shot prompting with 1-5 examples; PPO fine-tuning applied to same prompt format and evaluated zero-shot after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs Few-shot vs PPO (for each model family)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLAMA2-7B-chat-hf: STS-B zero-shot 27.5 (Spearman), few-shot 45.5, PPO-ST 92.6; COPA zero-shot 57.0 (accuracy), few-shot 73.4, PPO-ST 88.6. Qwen2.5-7B-Instruct: STS-B zero-shot 83.7, few-shot 87.0, PPO-ST 92.2; COPA zero-shot 96.6, few-shot 96.0, PPO-ST 97.0. MPT-7B-chat: STS-B zero-shot 19.7, few-shot 21.7, PPO-ST 89.3; COPA zero-shot 57.4, few-shot 57.2, PPO-ST 84.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Across all three LLMs, PPO fine-tuning yields large gains over zero- and few-shot prompting; magnitude varies by base model (LLAMA2 and MPT gain the most). BERT-large reported STS-B 86.57 and COPA 70.6 for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example: LLAMA2-7B-chat-hf STS-B: zero-shot -> PPO +65.1 points (27.5 -> 92.6); COPA: zero-shot -> PPO +31.6 points (57.0 -> 88.6). Effect sizes vary by model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Base instruction-tuned LLMs can be brittle under simple prompting (especially LLAMA2 and MPT), but PPO can adapt outputs to task objectives via reward signals; pretraining and model family matter for prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Results from official GLUE and SuperGLUE evaluation servers; STS-B reported as Spearman correlation; COPA reported as accuracy; PPO fine-tuning used LoRA, TRL library, batch sizes tuned, generation settings as above.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9329.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT vs PPO vs GRPO (selected GLUE tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of SFT, PPO, and GRPO fine-tuning methods on five GLUE tasks and runtime trade-offs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing PPO and GRPO (an RL variant) outperform SFT on several GLUE tasks, with different runtime/memory trade-offs: PPO faster per-step but requires a critic; GRPO avoids critic but needs multiple sampled responses increasing runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of GLUE tasks: SST-2 (sentiment), MRPC (paraphrase), RTE (NLI), CoLA (acceptability), QNLI (QA/NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classic GLUE classification tasks: SST-2 binary sentiment accuracy, MRPC paraphrase classification (accuracy/F1), RTE entailment accuracy, CoLA acceptability (Matthew's corr or accuracy depending), QNLI QA/NLI accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Fine-tuning regimes differ (SFT: supervised next-token on prompt+input+label; PPO: RL with end-of-generation reward via regex extraction inside '<Judgement>' tags; GRPO: group-relative policy optimization sampling multiple trajectories per prompt and using batch statistics for advantages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>SFT vs PPO vs GRPO (same prompt formatting constraints for generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>From table: SFT SST-2 73.8, MRPC 85.8, RTE 80.4, CoLA 50.7, QNLI 93.6, average ~76.9; PPO SST-2 96.4, MRPC 89.4, RTE 84.3, CoLA 59.9, QNLI 93.2, average ~83.2; GRPO SST-2 96.7, MRPC 91.2, RTE 88.5, CoLA 55.2, QNLI 93.1, average ~85.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PPO and GRPO both substantially outperform SFT across these tasks; GRPO sometimes slightly outperforms PPO on some tasks but at higher runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example: SST-2: SFT -> PPO +22.6 points (73.8 -> 96.4); average improvement SFT->PPO: ~6.3 points (consistent with other reported averages).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GRPO reduces memory (no critic) but needs multiple trajectory samples per prompt, increasing runtime; PPO uses a critic (extra memory) but is more runtime-efficient. Both RL approaches benefit from end-task reward signals compared to SFT which uses next-token loss.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-step runtime: SFT ~4.124s, PPO ~4.299s (about 4% increase), GRPO ~5.155s; experiments run on a single Nvidia A100 with TRL, batch sizes tuned per task (PPO batch sizes grid-searched 4-16), LoRA rank=16, PPO epoch=4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9329.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SQuAD: Prompting vs SFT vs PPO (reading comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of reward-formatting and fine-tuning method on SQuAD reading comprehension (EM and F1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reading comprehension evaluated with zero-shot prompting, SFT, and PPO using reward functions based on Exact Match (EM) and F1; PPO further improves over SFT on both EM and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SQuAD (reading comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question answering on passages; metrics: Exact Match (EM) and token-level F1 comparing predicted span vs gold answer.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting used a task-specific prompt and extraction via tags; SFT trained using ground-truth answers (next-token prediction); PPO optimized for reward functions derived from EM and F1 computed at the end of generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs SFT vs PPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: EM 7.66, F1 32.27; SFT: EM 59.17, F1 76.48; PPO: EM 65.74, F1 81.82.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PPO improves over SFT by +6.57 EM and +5.34 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>PPO vs SFT: EM +6.57 points, F1 +5.34 points; SFT vs zero-shot: EM +51.51, F1 +44.21</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimizing directly for EM and F1 as end-task rewards via PPO leads to better reading-comprehension metrics than SFT's token-loss objective or zero-shot prompting, since the RL objective aligns generation with evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models fine-tuned for one epoch on SQuAD training set; evaluation on development set; PPO used EM and F1 as reward functions; generation/formatting used '<Judgement>' tagging as in other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9329.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-following under format shift (Amazon rating)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of input-output format mismatch on instruction-following: models fine-tuned on SST-2 then zero-shot evaluated on Amazon rating (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assesses how fine-tuning method affects ability to follow a different instruction/format (generate integer 1-5 rating) when fine-tuned on a related but differently formatted dataset; PPO preserves some instruction-following, SFT collapses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Amazon review rating (1-5) — out-of-distribution instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a textual review, produce an integer rating from 1 to 5; test measures adherence to instruction/output format and accuracy relative to dataset labels.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Consistent prompt template requiring an integer rating 1-5; models evaluated zero-shot on this format after being fine-tuned on SST-2 (binary sentiment) using either SFT or PPO, or left non-fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original non-fine-tuned vs SFT-fine-tuned (on SST-2) vs PPO-fine-tuned (on SST-2), all evaluated with the same Amazon prompt template.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PPO-finetuned model accuracy: 39.35% (95% CI: 38.39, 40.29); non-fine-tuned model: 27% (95% CI: 19.00, 36.03); SFT-fine-tuned model: <1% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PPO outperforms non-fine-tuned by ~12.35 percentage points and vastly outperforms SFT (SFT effectively failed under this format shift).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>PPO vs SFT: +~39 percentage points (since SFT <1%); PPO vs non-fine-tuned: +12.35 points.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>PPO maintains proximity to the original model distribution via clipping and KL-divergence penalty, preserving instruction-following and allowing adaptation to a new output format; SFT narrows the model distribution toward task-specific training data, causing poor generalization to a different required output format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuned on SST-2 dataset (PPO or SFT); evaluation used bootstrapped 95% CIs from 100 predictions per example; generation settings consistent with other experiments; qualitative analysis reported that PPO-generated outputs adhered to required format and produced reasoning while SFT often failed to produce the required format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9329.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output formatting (strict '<Judgement>' tags)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of enforcing a strict output format (responses wrapped between '<Judgement></Judgement>') and penalizing incorrect format on training and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper uses a strict tag-based output format in prompts to standardize extraction and reward computation; incorrect formats are explicitly penalized in reward design and this formatting requirement affects both extraction and model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf (and other evaluated LLMs under same protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (for LLAMA2-7B-chat-hf; comparable for others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applicable across GLUE/SuperGLUE tasks and other evaluated datasets (STS-B, SST-2, SQuAD, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various NLU tasks where a single discrete/regression output is expected and must be extracted reliably from generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All task-specific prompts instruct the model to output results within '<Judgement></Judgement>' tags and to produce only the required content (e.g., 'Yes'/'No' or a float 0-5); regex extraction used to find text within these tags; incorrectly formatted responses receive an explicit negative reward (-1 for classification, -2.5 for regression).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not a single numeric performance metric — but the format was used across experiments; incorrect format responses were penalized and the paper reports that PPO-finetuned models exhibited fewer formatting errors and less output-class bias compared to zero-/few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Standardizing output via tags simplifies automatic answer extraction for reward computation; penalizing incorrect formats in the reward function steers the generator to produce parsable outputs. The paper notes SFT often produced outputs that failed to follow required formats in some OOD scenarios, while PPO maintained adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Regex-based extraction of content within '<Judgement>' tags; classification reward R = 1(ŷ==y) for correct format and match, incorrect format penalized -1; STS-B regression reward R = 2.5 - |ŷ - y| with incorrect format -2.5; generation terminated on special identifier '</Judgement>'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9329.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward model integration (PPO-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incorporation of a learned reward model (trained on GPT-4o rankings) into PPO training and its effects on classification accuracy vs analysis quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a learned reward model (BERT-based) trained on GPT-4o rankings to score responses (judgment + analysis) during PPO leads to higher judged analysis quality but reduced classification accuracy compared to simple exact-label reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (sentiment classification with accompanying analysis) — reward model trained on a subset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification with a supporting analysis; evaluation includes classification accuracy and GPT-4o scoring of generated analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>During reward-model training: generate 4 responses per example including judgment and analysis; GPT-4o ranks responses by correctness/consistency/analysis quality; train a BERT-based reward model with pairwise ranking loss; during PPO-RM training, use reward model scores instead of simple correct-answer reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>PPO with simple rule-based correct-answer reward (PPO) vs PPO with reward model scores (PPO-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PPO (rule-based): SST-2 classification accuracy 96.4%, GPT evaluation score of analyses 3.479; PPO-RM (reward model): accuracy 89.7%, GPT evaluation score 4.104 for analysis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Using reward model (PPO-RM) improves quality of generated analyses (GPT eval +0.625) but reduces classification accuracy by 6.7 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>PPO-RM vs PPO: -6.7% classification accuracy, +~0.625 GPT-eval points for analysis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize limited reward-model training data (5k samples) and potential reward-hacking during optimization caused degradation in classification accuracy despite better qualitative analyses; suggests richer/more robust reward models are needed to balance correctness and output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reward model trained on first 5,000 SST-2 samples with 4 responses each, GPT-4o used as ranker; reward model architecture: bert-base-cased trained with pairwise ranking loss; PPO-RM used reward model outputs as training reward during RL fine-tuning; evaluation of analyses done by GPT-4o scoring 100 sampled data points per model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9329.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9329.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity: SFT vs PPO effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of fine-tuning method on general language modeling ability measured by perplexity on WikiText-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assesses whether SFT or PPO fine-tuning degrades general language modeling ability; PPO preserves baseline perplexity while SFT increases perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiText-2 perplexity (general language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perplexity (PPL) measured on natural human-written corpora (WikiText-2 test set) to gauge general generative ability post fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>No special prompt format for this evaluation; fine-tuned models (SFT or PPO on SST-2) evaluated for PPL on WikiText-2 to detect degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Non-fine-tuned baseline vs SFT-fine-tuned vs PPO-fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Non-fine-tuned PPL: 6.939; SFT-fine-tuned PPL: 7.384; PPO-fine-tuned PPL: 6.966.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SFT increases PPL by 0.445 (worse), PPO increases PPL by 0.027 (negligible) relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>SFT vs baseline: +6.4% relative increase in perplexity; PPO vs baseline: +0.4% relative increase.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>PPO's clipping and KL-divergence regularization constrain policy updates, preserving the model's pre-fine-tuning generative distribution; SFT tends to converge to task-specific distributions causing degradation in general language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>PPL evaluated on WikiText-2 test set; models fine-tuned on SST-2 using LoRA; PPO used TRL library with KL term in objective; reported PPL values are exact from paper Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Proximal policy optimization algorithms <em>(Rating: 2)</em></li>
                <li>Fine-tuning language models from human preferences <em>(Rating: 2)</em></li>
                <li>DeepSeek-v3 technical report <em>(Rating: 1)</em></li>
                <li>Lora: Low-rank adaptation of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9329",
    "paper_id": "paper-273350705",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot vs Few-shot vs SFT vs PPO (GLUE avg)",
            "name_full": "Comparison of zero-shot prompting, few-shot prompting, supervised fine-tuning (SFT), and PPO fine-tuning on GLUE average",
            "brief_description": "Empirical comparison showing how prompt presentation (zero/few-shot) and fine-tuning paradigm (SFT vs PPO) affect LLAMA2-7B-chat-hf performance on GLUE; PPO yields the highest average, with SFT improving over prompting but trailing PPO.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "GLUE (aggregate over GLUE datasets)",
            "task_description": "Suite of natural language understanding tasks (classification, regression, NLI, acceptability, semantic textual similarity) aggregated to a single average score.",
            "presentation_format": "Zero-shot prompting using a task-specific prompt (task description + required strict output format between '&lt;Judgement&gt;&lt;/Judgement&gt;' tags); few-shot prompting with 1-5 labeled demonstrations prepended to the prompt (sweep over 1-5 shots, report best); SFT: supervised next-token prediction on prompt + input + ground-truth answer; PPO: same prompt format with generation constrained to '&lt;Judgement&gt;' tags but model optimized with PPO using per-example reward extracted by regex from the generated tags.",
            "comparison_format": "Zero-shot prompting; Few-shot prompting (1-5 examples); Supervised fine-tuning (SFT) using LoRA; PPO fine-tuning using LoRA with reward from exact label match.",
            "performance": "PPO average: 84.6 (average across GLUE datasets reported for LLAMA2-7B-chat-hf)",
            "performance_comparison": "SFT average: 78.5; Few-shot prompting: 58.7; Zero-shot prompting: 46.1; BERT-base (encoder baseline): 79.6; BERT-large: 82.1",
            "format_effect_size": "PPO vs SFT: +6.1 points on GLUE average (paper also reports +6.3 points in places); Few-shot vs Zero-shot: +12.6 points (58.7 vs 46.1); PPO vs Few-shot: +25.9 points; PPO vs Zero-shot: +38.5 points",
            "explanation_or_hypothesis": "Authors attribute PPO's improvement to reward-driven optimization aligning generation with task labels while maintaining proximity to the base policy via PPO's clipping/KL penalty; SFT helps but may narrow the model's distribution to training data and underperform on some generalization aspects. Prompting alone (zero/few-shot) is insufficient for LLMs under 14B to reach encoder-model performance on GLUE without task-specific fine-tuning.",
            "null_or_negative_result": false,
            "experimental_details": "Instruction-tuned LLAMA2-7B-chat-hf, LoRA rank r=16 for adapters; few-shot sweep 1-5 shots and best reported; generation: multinomial sampling temperature=1, one response per sample, lengths 12-32 tokens ending with '&lt;/Judgement&gt;'; evaluation from GLUE servers for test sets; numbers reported are dataset averages.",
            "uuid": "e9329.0",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Prompt format + Model family (STS-B & COPA)",
            "name_full": "Effect of zero-shot / few-shot prompting and PPO fine-tuning across model families on STS-B (Spearman) and COPA (accuracy)",
            "brief_description": "Demonstrates that the same prompt presentation formats (zero-shot, few-shot) have very different baseline performance across LLM families (LLAMA2, Qwen2.5, MPT), and that PPO fine-tuning substantially raises performance for each.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Multiple: LLAMA2-7B-chat-hf, Qwen2.5-7B-Instruct, MPT-7B-chat (evaluated separately)",
            "model_size": "7B (for each listed LLM variant)",
            "task_name": "STS-B (semantic textual similarity, Spearman correlation) and COPA (choice of cause/effect, accuracy)",
            "task_description": "STS-B: regression-style semantic similarity scoring (reported as Spearman correlation). COPA: commonsense causal choice (two-way) reported as accuracy.",
            "presentation_format": "Zero-shot prompting with task-specific prompt and strict '&lt;Judgement&gt;' tag output; Few-shot prompting with 1-5 examples; PPO fine-tuning applied to same prompt format and evaluated zero-shot after fine-tuning.",
            "comparison_format": "Zero-shot vs Few-shot vs PPO (for each model family)",
            "performance": "LLAMA2-7B-chat-hf: STS-B zero-shot 27.5 (Spearman), few-shot 45.5, PPO-ST 92.6; COPA zero-shot 57.0 (accuracy), few-shot 73.4, PPO-ST 88.6. Qwen2.5-7B-Instruct: STS-B zero-shot 83.7, few-shot 87.0, PPO-ST 92.2; COPA zero-shot 96.6, few-shot 96.0, PPO-ST 97.0. MPT-7B-chat: STS-B zero-shot 19.7, few-shot 21.7, PPO-ST 89.3; COPA zero-shot 57.4, few-shot 57.2, PPO-ST 84.0.",
            "performance_comparison": "Across all three LLMs, PPO fine-tuning yields large gains over zero- and few-shot prompting; magnitude varies by base model (LLAMA2 and MPT gain the most). BERT-large reported STS-B 86.57 and COPA 70.6 for reference.",
            "format_effect_size": "Example: LLAMA2-7B-chat-hf STS-B: zero-shot -&gt; PPO +65.1 points (27.5 -&gt; 92.6); COPA: zero-shot -&gt; PPO +31.6 points (57.0 -&gt; 88.6). Effect sizes vary by model and task.",
            "explanation_or_hypothesis": "Base instruction-tuned LLMs can be brittle under simple prompting (especially LLAMA2 and MPT), but PPO can adapt outputs to task objectives via reward signals; pretraining and model family matter for prompt sensitivity.",
            "null_or_negative_result": false,
            "experimental_details": "Results from official GLUE and SuperGLUE evaluation servers; STS-B reported as Spearman correlation; COPA reported as accuracy; PPO fine-tuning used LoRA, TRL library, batch sizes tuned, generation settings as above.",
            "uuid": "e9329.1",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SFT vs PPO vs GRPO (selected GLUE tasks)",
            "name_full": "Comparison of SFT, PPO, and GRPO fine-tuning methods on five GLUE tasks and runtime trade-offs",
            "brief_description": "Empirical comparison showing PPO and GRPO (an RL variant) outperform SFT on several GLUE tasks, with different runtime/memory trade-offs: PPO faster per-step but requires a critic; GRPO avoids critic but needs multiple sampled responses increasing runtime.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "Subset of GLUE tasks: SST-2 (sentiment), MRPC (paraphrase), RTE (NLI), CoLA (acceptability), QNLI (QA/NLI)",
            "task_description": "Classic GLUE classification tasks: SST-2 binary sentiment accuracy, MRPC paraphrase classification (accuracy/F1), RTE entailment accuracy, CoLA acceptability (Matthew's corr or accuracy depending), QNLI QA/NLI accuracy.",
            "presentation_format": "Fine-tuning regimes differ (SFT: supervised next-token on prompt+input+label; PPO: RL with end-of-generation reward via regex extraction inside '&lt;Judgement&gt;' tags; GRPO: group-relative policy optimization sampling multiple trajectories per prompt and using batch statistics for advantages).",
            "comparison_format": "SFT vs PPO vs GRPO (same prompt formatting constraints for generation)",
            "performance": "From table: SFT SST-2 73.8, MRPC 85.8, RTE 80.4, CoLA 50.7, QNLI 93.6, average ~76.9; PPO SST-2 96.4, MRPC 89.4, RTE 84.3, CoLA 59.9, QNLI 93.2, average ~83.2; GRPO SST-2 96.7, MRPC 91.2, RTE 88.5, CoLA 55.2, QNLI 93.1, average ~85.3.",
            "performance_comparison": "PPO and GRPO both substantially outperform SFT across these tasks; GRPO sometimes slightly outperforms PPO on some tasks but at higher runtime.",
            "format_effect_size": "Example: SST-2: SFT -&gt; PPO +22.6 points (73.8 -&gt; 96.4); average improvement SFT-&gt;PPO: ~6.3 points (consistent with other reported averages).",
            "explanation_or_hypothesis": "GRPO reduces memory (no critic) but needs multiple trajectory samples per prompt, increasing runtime; PPO uses a critic (extra memory) but is more runtime-efficient. Both RL approaches benefit from end-task reward signals compared to SFT which uses next-token loss.",
            "null_or_negative_result": false,
            "experimental_details": "Per-step runtime: SFT ~4.124s, PPO ~4.299s (about 4% increase), GRPO ~5.155s; experiments run on a single Nvidia A100 with TRL, batch sizes tuned per task (PPO batch sizes grid-searched 4-16), LoRA rank=16, PPO epoch=4.",
            "uuid": "e9329.2",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SQuAD: Prompting vs SFT vs PPO (reading comprehension)",
            "name_full": "Effect of reward-formatting and fine-tuning method on SQuAD reading comprehension (EM and F1)",
            "brief_description": "Reading comprehension evaluated with zero-shot prompting, SFT, and PPO using reward functions based on Exact Match (EM) and F1; PPO further improves over SFT on both EM and F1.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "SQuAD (reading comprehension)",
            "task_description": "Question answering on passages; metrics: Exact Match (EM) and token-level F1 comparing predicted span vs gold answer.",
            "presentation_format": "Zero-shot prompting used a task-specific prompt and extraction via tags; SFT trained using ground-truth answers (next-token prediction); PPO optimized for reward functions derived from EM and F1 computed at the end of generation.",
            "comparison_format": "Zero-shot vs SFT vs PPO",
            "performance": "Zero-shot: EM 7.66, F1 32.27; SFT: EM 59.17, F1 76.48; PPO: EM 65.74, F1 81.82.",
            "performance_comparison": "PPO improves over SFT by +6.57 EM and +5.34 F1.",
            "format_effect_size": "PPO vs SFT: EM +6.57 points, F1 +5.34 points; SFT vs zero-shot: EM +51.51, F1 +44.21",
            "explanation_or_hypothesis": "Optimizing directly for EM and F1 as end-task rewards via PPO leads to better reading-comprehension metrics than SFT's token-loss objective or zero-shot prompting, since the RL objective aligns generation with evaluation metrics.",
            "null_or_negative_result": false,
            "experimental_details": "Models fine-tuned for one epoch on SQuAD training set; evaluation on development set; PPO used EM and F1 as reward functions; generation/formatting used '&lt;Judgement&gt;' tagging as in other tasks.",
            "uuid": "e9329.3",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Instruction-following under format shift (Amazon rating)",
            "name_full": "Effect of input-output format mismatch on instruction-following: models fine-tuned on SST-2 then zero-shot evaluated on Amazon rating (1-5)",
            "brief_description": "Assesses how fine-tuning method affects ability to follow a different instruction/format (generate integer 1-5 rating) when fine-tuned on a related but differently formatted dataset; PPO preserves some instruction-following, SFT collapses.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "Amazon review rating (1-5) — out-of-distribution instruction-following",
            "task_description": "Given a textual review, produce an integer rating from 1 to 5; test measures adherence to instruction/output format and accuracy relative to dataset labels.",
            "presentation_format": "Consistent prompt template requiring an integer rating 1-5; models evaluated zero-shot on this format after being fine-tuned on SST-2 (binary sentiment) using either SFT or PPO, or left non-fine-tuned.",
            "comparison_format": "Original non-fine-tuned vs SFT-fine-tuned (on SST-2) vs PPO-fine-tuned (on SST-2), all evaluated with the same Amazon prompt template.",
            "performance": "PPO-finetuned model accuracy: 39.35% (95% CI: 38.39, 40.29); non-fine-tuned model: 27% (95% CI: 19.00, 36.03); SFT-fine-tuned model: &lt;1% accuracy.",
            "performance_comparison": "PPO outperforms non-fine-tuned by ~12.35 percentage points and vastly outperforms SFT (SFT effectively failed under this format shift).",
            "format_effect_size": "PPO vs SFT: +~39 percentage points (since SFT &lt;1%); PPO vs non-fine-tuned: +12.35 points.",
            "explanation_or_hypothesis": "PPO maintains proximity to the original model distribution via clipping and KL-divergence penalty, preserving instruction-following and allowing adaptation to a new output format; SFT narrows the model distribution toward task-specific training data, causing poor generalization to a different required output format.",
            "null_or_negative_result": true,
            "experimental_details": "Fine-tuned on SST-2 dataset (PPO or SFT); evaluation used bootstrapped 95% CIs from 100 predictions per example; generation settings consistent with other experiments; qualitative analysis reported that PPO-generated outputs adhered to required format and produced reasoning while SFT often failed to produce the required format.",
            "uuid": "e9329.4",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Output formatting (strict '&lt;Judgement&gt;' tags)",
            "name_full": "Effect of enforcing a strict output format (responses wrapped between '&lt;Judgement&gt;&lt;/Judgement&gt;') and penalizing incorrect format on training and evaluation",
            "brief_description": "Paper uses a strict tag-based output format in prompts to standardize extraction and reward computation; incorrect formats are explicitly penalized in reward design and this formatting requirement affects both extraction and model behavior.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf (and other evaluated LLMs under same protocol)",
            "model_size": "7B (for LLAMA2-7B-chat-hf; comparable for others)",
            "task_name": "Applicable across GLUE/SuperGLUE tasks and other evaluated datasets (STS-B, SST-2, SQuAD, etc.)",
            "task_description": "Various NLU tasks where a single discrete/regression output is expected and must be extracted reliably from generated text.",
            "presentation_format": "All task-specific prompts instruct the model to output results within '&lt;Judgement&gt;&lt;/Judgement&gt;' tags and to produce only the required content (e.g., 'Yes'/'No' or a float 0-5); regex extraction used to find text within these tags; incorrectly formatted responses receive an explicit negative reward (-1 for classification, -2.5 for regression).",
            "comparison_format": null,
            "performance": "Not a single numeric performance metric — but the format was used across experiments; incorrect format responses were penalized and the paper reports that PPO-finetuned models exhibited fewer formatting errors and less output-class bias compared to zero-/few-shot prompting.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Standardizing output via tags simplifies automatic answer extraction for reward computation; penalizing incorrect formats in the reward function steers the generator to produce parsable outputs. The paper notes SFT often produced outputs that failed to follow required formats in some OOD scenarios, while PPO maintained adherence.",
            "null_or_negative_result": false,
            "experimental_details": "Regex-based extraction of content within '&lt;Judgement&gt;' tags; classification reward R = 1(ŷ==y) for correct format and match, incorrect format penalized -1; STS-B regression reward R = 2.5 - |ŷ - y| with incorrect format -2.5; generation terminated on special identifier '&lt;/Judgement&gt;'.",
            "uuid": "e9329.5",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reward model integration (PPO-RM)",
            "name_full": "Incorporation of a learned reward model (trained on GPT-4o rankings) into PPO training and its effects on classification accuracy vs analysis quality",
            "brief_description": "Using a learned reward model (BERT-based) trained on GPT-4o rankings to score responses (judgment + analysis) during PPO leads to higher judged analysis quality but reduced classification accuracy compared to simple exact-label reward.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "SST-2 (sentiment classification with accompanying analysis) — reward model trained on a subset",
            "task_description": "Binary sentiment classification with a supporting analysis; evaluation includes classification accuracy and GPT-4o scoring of generated analyses.",
            "presentation_format": "During reward-model training: generate 4 responses per example including judgment and analysis; GPT-4o ranks responses by correctness/consistency/analysis quality; train a BERT-based reward model with pairwise ranking loss; during PPO-RM training, use reward model scores instead of simple correct-answer reward.",
            "comparison_format": "PPO with simple rule-based correct-answer reward (PPO) vs PPO with reward model scores (PPO-RM)",
            "performance": "PPO (rule-based): SST-2 classification accuracy 96.4%, GPT evaluation score of analyses 3.479; PPO-RM (reward model): accuracy 89.7%, GPT evaluation score 4.104 for analysis quality.",
            "performance_comparison": "Using reward model (PPO-RM) improves quality of generated analyses (GPT eval +0.625) but reduces classification accuracy by 6.7 percentage points.",
            "format_effect_size": "PPO-RM vs PPO: -6.7% classification accuracy, +~0.625 GPT-eval points for analysis quality.",
            "explanation_or_hypothesis": "Authors hypothesize limited reward-model training data (5k samples) and potential reward-hacking during optimization caused degradation in classification accuracy despite better qualitative analyses; suggests richer/more robust reward models are needed to balance correctness and output quality.",
            "null_or_negative_result": true,
            "experimental_details": "Reward model trained on first 5,000 SST-2 samples with 4 responses each, GPT-4o used as ranker; reward model architecture: bert-base-cased trained with pairwise ranking loss; PPO-RM used reward model outputs as training reward during RL fine-tuning; evaluation of analyses done by GPT-4o scoring 100 sampled data points per model.",
            "uuid": "e9329.6",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Perplexity: SFT vs PPO effect",
            "name_full": "Effect of fine-tuning method on general language modeling ability measured by perplexity on WikiText-2",
            "brief_description": "Assesses whether SFT or PPO fine-tuning degrades general language modeling ability; PPO preserves baseline perplexity while SFT increases perplexity.",
            "citation_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "LLAMA2-7B-chat-hf",
            "model_size": "7B",
            "task_name": "WikiText-2 perplexity (general language modeling)",
            "task_description": "Perplexity (PPL) measured on natural human-written corpora (WikiText-2 test set) to gauge general generative ability post fine-tuning.",
            "presentation_format": "No special prompt format for this evaluation; fine-tuned models (SFT or PPO on SST-2) evaluated for PPL on WikiText-2 to detect degradation.",
            "comparison_format": "Non-fine-tuned baseline vs SFT-fine-tuned vs PPO-fine-tuned",
            "performance": "Non-fine-tuned PPL: 6.939; SFT-fine-tuned PPL: 7.384; PPO-fine-tuned PPL: 6.966.",
            "performance_comparison": "SFT increases PPL by 0.445 (worse), PPO increases PPL by 0.027 (negligible) relative to baseline.",
            "format_effect_size": "SFT vs baseline: +6.4% relative increase in perplexity; PPO vs baseline: +0.4% relative increase.",
            "explanation_or_hypothesis": "PPO's clipping and KL-divergence regularization constrain policy updates, preserving the model's pre-fine-tuning generative distribution; SFT tends to converge to task-specific distributions causing degradation in general language modeling.",
            "null_or_negative_result": false,
            "experimental_details": "PPL evaluated on WikiText-2 test set; models fine-tuned on SST-2 using LoRA; PPO used TRL library with KL term in objective; reported PPL values are exact from paper Table 6.",
            "uuid": "e9329.7",
            "source_info": {
                "paper_title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Proximal policy optimization algorithms",
            "rating": 2,
            "sanitized_title": "proximal_policy_optimization_algorithms"
        },
        {
            "paper_title": "Fine-tuning language models from human preferences",
            "rating": 2,
            "sanitized_title": "finetuning_language_models_from_human_preferences"
        },
        {
            "paper_title": "DeepSeek-v3 technical report",
            "rating": 1,
            "sanitized_title": "deepseekv3_technical_report"
        },
        {
            "paper_title": "Lora: Low-rank adaptation of large language models",
            "rating": 1,
            "sanitized_title": "lora_lowrank_adaptation_of_large_language_models"
        }
    ],
    "cost": 0.019674249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning
26 Sep 2025</p>
<p>Ashish Sai 
UC San Diego</p>
<p>Somayajula 
UC San Diego</p>
<p>Bokai Hu 
UC San Diego</p>
<p>Qi Cao q9cao@ucsd.edu 
UC San Diego</p>
<p>Xin Pan 
UC San Diego</p>
<p>Pengtao Xie p1xie@ucsd.edu 
UC San Diego</p>
<p>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning
26 Sep 20256E504998CC8569A5D167DFC7A309A323arXiv:2410.11020v5[cs.CL]
Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE.Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs.We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels.PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and fewshot prompting by 38.7 and 26.1 points, respectively.Notably, PPO-tuned models outperform GPT-4o by over 4% on average across sentiment and natural language inference tasks, including gains of 7.3% on the Mental Health dataset and 10.9% on SIGA-nli.This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation.Our code is available at https://github.com/coder-qicao/RL4GLUE.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Radford et al., 2019;Brown, 2020;Touvron et al., 2023b) have revolutionized natural language processing (NLP) with their powerful text generation capabilities (Radford, 2018).Pretrained on large-scale unlabeled corpora, LLMs can generate coherent and contextually relevant content.Using promptbased strategies like zero-shot and few-shot prompting (Brown, 2020), these models can address a * Equal Contribution wide range of downstream tasks without taskspecific fine-tuning.However, when applied to instruction-fine-tuned LLMs under 14B parameters1 -such as LLAMA2-7B-chat-hf-these methods often underperform on natural language understanding (NLU) tasks compared to encoder-only models like BERT (Devlin, 2018), which consistently excel on benchmarks such as GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020).For instance, our evaluation of LLAMA2-7Bchat-hf shows that zero-shot prompting with taskspecific prompts yields an average performance of 46.1 across all GLUE datasets, while few-shot prompting improves performance to 58.7-both significantly trailing BERT-base's 79.6, as shown in Table 1.</p>
<p>To enhance NLU capabilities of LLMs, we investigate reinforcement learning (RL)-based finetuning approaches.Motivated by recent work such as DeepSeek (Liu et al., 2024), which demonstrates the utility of reward-driven optimization for improving reasoning abilities, we explore the use of Proximal Policy Optimization (PPO) (Schulman et al., 2017a) to align model outputs with taskspecific objectives.</p>
<p>While standard fine-tuning (SFT) is commonly used to adapt LLMs to downstream tasks, we find it insufficient for NLU-often underperforming even smaller encoder-only models like BERT-base.In contrast, we use PPO to enhance LLM performance by framing NLU as a reinforcement learning problem, as shown in Fig 1 .The sequence of input tokens up to timestep t−1 represents the state s t , and the token generated at timestep t is treated as the action a t .After generating the full response, a heuristic extracts the predicted answer, which is compared to the ground-truth label to assign a scalar reward R. PPO then updates the model to maximize To address this performance gap, we recast the task of NLU adaptation as an RL problem, fine-tuning the model using PPO.The input consists of a prompt and a query.In the standard setting (top path), the base model, with zero-shot/few-shot prompting, struggles to generate correct answers.Our approach selectively updates lightweight LoRA layers, using PPO to optimize a reward signal based on task correctness.This optimization encourages performance gains while constraining deviation from the base policy.We find that PPO fine-tuning substantially outperforms standard SFT.</p>
<p>this reward, enabling direct optimization for taskspecific objectives.Empirically, PPO-based finetuning of LLAMA2-7B-chat-hf improves GLUE performance by 6.3 points over SFT, surpasses zero and few-shot prompting by 38.7 and 26.1 points, respectively, and even outperforms GPT-4o by over 4% on sentiment and inference tasks-achieving gains of 7.3% on the Mental Health dataset and 10.8% on SIGA-nli.These results demonstrate the effectiveness of reinforcement learning in aligning LLMs under 14B parameters with NLU objectives.</p>
<p>Pre-trained LLMs possess broad linguistic knowledge, spanning syntactic and semantic structures, acquired from large-scale text corpora.We show that reinforcement learning, specifically PPO, can refine this general understanding to better align with task-specific NLU objectives.Similar patterns are observed in DeepSeek, where chain-of-thought pre-training enhances reasoning capabilities, and subsequent RL-based fine-tuning further improves performance.Building on this insight, our findings suggest a promising direction: adapting LLMs to new tasks by formulating them as reinforcement learning problems.When models are sufficiently pre-trained, task alignment may be achieved without additional labeled data-requiring only a well-defined reward function over the outputs.PPO can then optimize the model toward high-reward behaviors.This approach offers a scalable, label-efficient alternative to conventional supervised fine-tuning through reward-driven adaptation.</p>
<p>Method</p>
<p>To enhance the performance of LLMs on NLU tasks, we adopt two distinct fine-tuning methods.The first approach involves supervised fine-tuning, where the input consists of a concatenation of the task-specific prompt, query and the ground truth answer, with the model optimized using the nexttoken prediction objective.The second approach utilizes PPO, framing response generation as a reinforcement learning problem.In this setup, the sequence of input tokens until timestep t − 1 represents the state s t , and each token generated at timestep t is treated as an action a t .After generating the entire sequence, a heuristic-based process extracts the final answer from this generated sequence, and is compared to the ground truth.PPO is then employed to optimize the model by maximizing the cumulative reward derived from this comparison.To reduce computational complexity, we fine-tune LoRA layers instead of the full model.We refer the readers to Appendix A and B for preliminaries on PPO.</p>
<p>Task-Specific Prompt Design</p>
<p>We detail the construction of task-specific prompts used to query the LLM for NLU tasks.Each prompt begins with a clear task description, outlining the necessary background information to guide the model in solving the task.Following this, we specify strict requirements for the output format, ensuring that the response is encapsulated within a predefined structure, specifically between '<Judge-ment></Judgement>' tags.This structure ensures consistency in the model's responses, facilitating easier extraction and evaluation of the results.</p>
<p>For example, in the CoLA task, which assesses grammatical acceptability, the prompt is structured as follows:</p>
<p>System_prompt :</p>
<p>You are an assistant to analyze the linguistic properties of a sentence .The task is to decide the linguistic acceptability of a sentence .If the sentence is linguistically correct then it is acceptable , else it is not .The result you give should have the following form : &lt; Judgement &gt; { Insert only " Yes " or "</p>
<p>No " here } &lt;/ Judgement &gt; Prompt :</p>
<p>Now judge if the sentence "{ sentence }" is linguistically acceptable .Assistant :</p>
<p>&lt; Judgement &gt;</p>
<p>The prompt starts with background information about CoLA, specifies restrictions on the output (such as labeling a sentence as acceptable or unacceptable), and concludes with a special start token, <Judgement>, to initiate the model's response generation.</p>
<p>Supervised Fine-tuning of LLM on NLU Tasks</p>
<p>Given an NLU training dataset,
D (tr) = {(x i , y i )} N i=1
, where x i represents the input text and y i the ground truth label, we fine-tune the LLM on a sequence consisting of the task-specific prompt p (described in section 2.1) concatenated with the input x i and the ground truth answer y i .The model is trained using the next-token prediction objective, where it predicts the next token in the sequence by conditioning on all preceding tokens.This objective trains the model to learn to predict the correct answer for the NLU task conditioned on the task-specific prompt and input.</p>
<p>Proximal Policy Optimization for LLM</p>
<p>Fine-tuning on NLU Tasks</p>
<p>We utilize PPO to fine-tune the LLM on NLU tasks, following the training protocol outlined in Appendix B. The reward function is specifically designed for each NLU task.In this work, we use a simple reward function, where a reward is assigned at the end of the generation based on alignment with the ground truth labels.We use regular expression matching to extract answers from the LLMs outputs by first locating the text within the '<Judge-ment></Judgement>' tags.Depending on the task, we then search for task-specific keywords (such as "yes", "no", "acceptable", or "not acceptable") to identify the answer.These extracted answers are compared with the ground truth to determine the appropriate rewards.</p>
<p>For instance, CoLA, a classification task, answers are categorized as acceptable, unacceptable, or exceptional (incorrect format).For STS-B, a regression task, the extracted answer is a floating-point number between 0 and 5. Reward per generation for classification tasks is given by R = 1(ŷ == y i ), where ŷ is model's prediction and y is ground truth.For STS-B, a regression task, the reward for each generation is defined as R = 2.5 − |ŷ i − y i |, where ŷi is the predicted score and y i is the ground truth.Since both scores lie in [0, 5], their absolute difference ranges from 0 to 5. Subtracting this from 2.5 centers the reward around zero and bounds it within [−2.5, 2.5], with the maximum reward achieved when ŷi = y i .Incorrectly formatted responses are penalized with a value of -1 for classification tasks and -2.5 for regression tasks, representing the largest penalties applied in each case.</p>
<p>Low-Rank Adaptation</p>
<p>To mitigate the computational cost of full-model fine-tuning, we employ LoRA (Hu et al., 2021) during both the supervised fine-tuning and PPO stages.Instead of updating the entire model, we restrict the updates to LoRA layers, which significantly reduces the number of trainable parameters by decomposing the weight matrices into low-rank matrices.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>We trained and evaluated our models on the GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020) benchmarks.All experiments were conducted using instruction-tuned LLAMA2-7B models (Touvron et al., 2023a) 2 .We perform both single task and multi-task fine-tuning: 1) Singletask fine-tuning: For each subtask within GLUE and SuperGLUE, a separate task-specific LoRA module was trained independently.2) Multi-task fine-tuning: In the multi-task setting, datasets from different subtasks within each benchmark were combined, and a single LoRA module was trained to handle all tasks simultaneously.Please refer to Appendix C for detailed hyperparameter settings.</p>
<p>Baselines</p>
<p>We evaluated the performance of our approach against three baselines:</p>
<p>• Encoder-only models: We compare our results with encoder-only transformer models, specifically BERT-base (110M parameters) and BERT-large (340M parameters) (Devlin et al., 2019).</p>
<p>• Zero-shot prompting: The model is provided with task-specific prompts, as outlined in section 2.1, along with the input query.</p>
<p>The model is required to generate predictions solely based on these prompts and the input query, without any additional task-specific fine-tuning.</p>
<p>• Few-shot prompting: In this setting, the model is provided with both the task-specific prompt and one to five labeled examples (which ever gave the best performance) from the training dataset as demonstrations.These examples are provided as reference to guide the model in generating more accurate responses for the input query.Similarly, no task-specific fine-tuning is performed.</p>
<p>After generating a response, we applied regular expression matching to extract the relevant answer from the model's output.We directly matched taskspecific keywords (like "yes" or "no") in the generated text to identify the answer.This extracted answer was then compared to the ground truth label to evaluate the model's performance.</p>
<p>Results on GLUE Benchmark</p>
<p>In this section, we present our experiments on the GLUE benchmark, comparing the results with encoder-only models such as BERT (Devlin et al., 2019).We use the LLAMA2-7B-chat-hf model as the LLM for our evaluations.The baselines include zero-shot prompting and few-shot prompting.For fine-tuning methods, we compare both supervised fine-tuning and PPO across single-task and multi-task settings.The results are summarized in Table 1.From the results, we make the following observations.</p>
<p>First, we observed that zero-shot prompting of the LLAMA2-7B-chat-hf model with task-specific prompts consistently underperformed compared to the smaller BERT-base model.LLAMA2-7B-chathf struggled notably on simpler tasks like SST-2, which only required classifying sentiment as positive or negative.This underscores the model's weak language understanding capabilities, with zero-shot prompting proving inadequate compared to BERTbase.Second, few-shot prompting showed improvements over the zero-shot baseline, achieving an average score of 58.7 compared to 46.1, but it still lagged significantly behind the BERTbase model's score of 79.6.Third, supervised finetuning (SFT) using LoRA modules for each task further boosted performance, bringing it closer to BERT's level with an average score of 78.5, though still slightly behind BERT-base's 79.6.Fourth, fine-tuning with PPO delivered the best results, achieving an average score of 84.6, surpassing even BERT-large's 82.1.Moreover, zero-shot and fewshot prompting of LLAMA2-7B-chat-hf displayed a noticeable output imbalance, with a tendency to favor certain classes or values.In contrast, models fine-tuned with PPO showed no significant bias.Fifth, the total computational time for PPO is approximately 1.32 times that of SFT, indicating only a marginal increase in computational costs.</p>
<p>Additionally, we compared the results with multi-task training, where a single LoRA module was trained across all datasets using both SFT and PPO to reduce time complexity.We found that SFT on individual tasks outperformed its multi-task fine-tuning counterpart.However, while PPO on multi-task training did not perform as well as PPO on single-task training, it still outperformed BERTlarge in average performance, achieving a score of 82.9 compared to BERT-large's 82.1.These results demonstrate that while single-task fine-tuning yields the best performance, multi-task training with PPO can still achieve competitive results, even surpassing state-of-the-art models like BERT-large.</p>
<p>To assess the consistency of our findings across different models, we evaluated Qwen2.5-7B-Instruct and MPT-7B-chat alongside LLAMA2-7B-chat-hf on the STS-B dataset from the GLUE benchmark and the COPA dataset from the Super-GLUE benchmark.The results are summarized in Table 2.The results confirm that PPO-based fine-tuning consistently outperforms the BERTlarge model, as well as the zero-shot and few-shot prompting baselines for all LLMs, highlighting its effectiveness across different LLMs.</p>
<p>Comparison of RL Algorithms: PPO vs. GRPO</p>
<p>Our objective is to improve the natural language understanding capabilities of the base (policy) model through RL fine-tuning.In this context, we compare two RL approaches: PPO and Group Rela-tive Policy Optimization (GRPO) (Shao et al., 2024).PPO requires updating a separate critic model to compute value functions, which introduces (modest) additional memory constraints.On the other hand, GRPO was designed to bypass the critic model entirely.Instead, GRPO samples multiple trajectories per prompt and computes each trajectory's advantage by comparing its reward to the batch's average (and standard deviation).This method not only simplifies the architecture but also reduces memory usage.</p>
<p>For our experiments, we utilized the TRL library (von Werra et al., 2020) on a single Nvidia A100 GPU, with a batch size of 16 and gradient checkpointing enabled.While SFT involves a simple forward pass, loss computation, and backward pass per step, both PPO and GRPO add extra steps such as LLM sampling, reward calculation, and advantage estimation.</p>
<p>As detailed in GRPO's need to generate multiple responses per sample results in a higher runtime, despite its memory efficiency.Overall, our analysis highlights the trade-offs between these RL algorithms: PPO offers efficient runtime with the cost of additional overhead from the critic model, while GRPO reduces memory usage at the expense of increased sampling time.</p>
<p>Evaluating Zero-Shot Generalization of PPO Fine-Tuned Models and Comparison with GPT-4o</p>
<p>We evaluate the zero-shot generalization capabilities of LLAMA2 7B and 13B models fine-tuned using PPO on a single dataset and subsequently tested across multiple other datasets (Table 4).For sentiment analysis tasks, the models were fine-tuned on SST-2 and evaluated on diverse datasets, including Financial PhraseBank (Malo et al., 2014), Labelled Financial News (Sood, 2024), Mental Health (Gaes, 2023), and Emotion (Saravia et al., 2018).Similarly, for natural language inference (NLI) tasks, the models were fine-tuned on MNLI and evaluated on Babi-nli (Weston et al., 2015) and SIGAnli (Nizamani et al., 2024).</p>
<p>Our results demonstrate that PPO fine-tuning improves the zero-shot performance of LLAMA2chat-hf models compared to GPT-4o, a strong base-line.For sentiment analysis, LLAMA2-13B-chathf achieves 97.7% accuracy on Financial Phrase-Bank, slightly outperforming GPT-4o (97.5%).On Labelled Financial News, LLAMA2-13B-chat-hf records 72.3%, exceeding GPT-4o by 4.5%.Similarly, on the Mental Health dataset, LLAMA2-7B-chat-hf achieves 67.2%, marking a notable gain of 7.3% over GPT-4o.For the Emotion dataset, LLAMA2-7B-chat-hf achieves 78.0%, with a smaller gain of 0.4%.For NLI tasks, LLAMA2-13B-chat-hf achieves 69.4% accuracy on Babi-nli, surpassing GPT-4o by 6.2%.Additionally, LLAMA2-13B-chat-hf achieves 46.3% accuracy on SIGA-nli, outperforming GPT-4o by more than 10%.On average, both 7B and 13B versions of PPO fine-tuned LLAMA2-chat-hf models demonstrate a performance gain of over 4% compared to GPT-4o, which is significantly larger in size and highly optimized.</p>
<p>To ensure robust comparisons, we quantify uncertainty in our evaluations by generating 100 predictions for each example in the dataset.The evaluation metric is then computed over the entire dataset for each set, yielding a distribution of values.The 95% confidence interval is defined by the 2.5th and 97.5th percentiles of this distribution.Results are presented in Table 8.</p>
<p>These results demonstrate the effectiveness of simple PPO fine-tuning on a single task-specific dataset in significantly enhancing model performance on similar tasks.LLAMA2-chat-hf models fine-tuned with PPO consistently outperform GPT-4o across diverse downstream tasks, reinforcing PPO fine-tuning as a robust approach for improving the NLU capabilities of LLMs.</p>
<p>We measured inference time on the Financial PhraseBank dataset with a batch size of 4. The BERT-base model, with 110M parameters, required 0.035s per step, while the LLAMA2-7B model, with 7B parameters and multi-token generation, took 0.997s per step.This difference is expected given the larger model size and the need for multiple forward passes in LLAMA2-7B.While LLM inference is slower, our focus is on improving natural language understanding with PPO, which achieves strong performance gains on both in-distribution and out-of-distribution NLU and NLI tasks.</p>
<p>Evaluation of Instruction-Following in Out-of-Distribution Tasks</p>
<p>To assess the instruction-following capabilities of LLMs in tasks differing from their fine-tuned for- Table 4: Accuracy of different models across downstream tasks.For sentiment analysis tasks, models are fine-tuned on SST-2 and zero-shot evaluated on Financial PhraseBank (Malo et al., 2014), Labelled Financial News (Sood, 2024), Mental Health (Gaes, 2023), and Emotion (Saravia et al., 2018).Similarly, for natural language inference tasks, models are fine-tuned on MNLI and zero-shot evaluated on Babi-nli (Weston et al., 2015) and SIGAnli (Nizamani et al., 2024).PPO-ST represents fine-tuning using Proximal Policy Optimization.Gains over GPT-4o model in the average row is indicated with green arrows.</p>
<p>mat, we conduct evaluations using the LLAMA2-7B-chat-hf model fine-tuned on the SST-2 dataset.Specifically, we evaluate the performance of this model on the Amazon review task, which requires generating an integer rating between 1 and 5 based on the provided textual review.Although SST-2 and Amazon reviews both involve sentiment analysis, the two tasks differ distinctly in their inputoutput formatting, providing a clear measure of instruction-following adaptability.</p>
<p>We compare zero-shot prompting of three versions of the LLAMA2-7B-chat-hf model: the original non-fine-tuned model, a version fine-tuned using SFT, and another fine-tuned with PPO.The 95% confidence intervals (CI) reported here are defined by the 2.5th and 97.5th percentiles of the bootstrap distribution.Using a consistent prompt template across models, we find that the PPO-finetuned model achieves an accuracy of 39.35% (95% CI: 38.39, 40.29), significantly outperforming the original model, which achieves 27% accuracy (95% CI: 19.00, 36.03).Conversely, the SFT-fine-tuned model demonstrates extremely poor performance, achieving less than 1% accuracy.</p>
<p>Qualitative analysis of sampled outputs reveals that the PPO-fine-tuned model reliably adheres to  the instruction format and generates detailed reasoning to support its predictions.In contrast, the SFT-fine-tuned model often fails to adapt its responses to the required format, demonstrating limited generalization capabilities.PPO fine-tuning maintains proximity to the original model distribution through its clipping mechanism and the KLdivergence minimization term in the objective function (part of the TRL library), thereby preserving and enhancing the model's intrinsic instructionfollowing capabilities.In contrast, SFT fine-tuning appears to narrow the model's learned distribution to task-specific training data, negatively impacting its original instruction-following proficiency.</p>
<p>Impact of Fine-Tuning on Language Modeling Ability</p>
<p>We experiment with SFT and PPO to improve NLU capabilities of LLMs and observe improved performance using PPO.However, it is crucial to ensure that fine-tuning methods do not significantly degrade the models' general language generation abilities.To assess this, we directly evaluate the PPL (jel, 1977;Chelba and Jelinek, 2000) of LLAMA2-7B-chat-hf models fine-tuned on the SST-2 dataset using the WikiText-2 test set (Merity et al., 2016), which follows a natural humanwritten text distribution.We compare these finetuned models against the original, non-fine-tuned baseline model, with the expectation that the PPL of the fine-tuned models should closely match the baseline.Our results reveal that the original LLAMA2-7B-chat-hf achieves a perplexity of 6.939.The PPO-fine-tuned model closely maintains this baseline performance with a perplexity of 6.966, indicating minimal impact on its general language modeling capabilities.</p>
<p>In contrast, the SFT-fine-tuned model displays a higher perplexity of 7.384, suggesting a significant reduction in generation capabilities due to convergence toward task-specific training distributions.We conjecture that PPO's clipping mechanism and the KL-divergence minimization term in the objective function (part of the TRL library), effectively constrains policy updates, preventing large deviations from the reference model and thereby preserving the original language modeling capabilities of LLMs.These findings underscore PPO's effectiveness in maintaining the general language abilities of LLMs during fine-tuning.</p>
<p>Method perplexity</p>
<p>Non-fine-tuned 6.939 SFT 7.384 PPO 6.966</p>
<p>Table 6: Perplexity of LLAMA2-7B-chat-hf on the WikiText-2 test set.Lower perplexity indicates better language modeling ability.</p>
<p>Integrating a Reward Model</p>
<p>While our primary reward function is based on matching generated outputs to true labels, we recognize that more sophisticated reward designs may be necessary for complex NLU tasks.To address this, we investigate the effect of integrating a reward model into our PPO training, with the aim of enhancing not only classification performance on SST-2 but also the quality of generated analyses.Reward Modeling Setup.For the first 5,000 training samples of the SST-2 dataset, LLAMA2-7B-chat-hf generates four responses per data point.Each response includes a sentiment judgment (Positive/Negative) and a supporting analysis.To robustly rank these responses, we use GPT-4o as an evaluator.GPT-4o ranks the responses based on: (i) the correctness of the sentiment judgment (i.e., matching the ground truth), (ii) the consistency between the judgment and its accompanying analysis, and (iii) the overall factual correctness and helpfulness of the analysis.To ensure clear differentiation, we include two reference responses-one with only the correct answer and one with only the incorrect answer-and define the ranking order as: correct answer with analysis &gt; only correct answer &gt; incorrect answer with analysis &gt; only incorrect answer.</p>
<p>Training the Reward Model.A reward model is then trained on this ranked dataset using a BERTbased architecture (bert-base-cased).For each input x, we consider pairs of responses (y w , y l ), where y w denotes a response ranked higher by our evaluator (GPT-4o) due to its correct sentiment and coherent analysis, and y l denotes a lower-ranked response.The model learns to assign higher scores to better responses via a pairwise ranking loss:
L(θ) = −E (x,yw,y l )∼D log σ r θ (x, y w ) − r θ (x, y l ) ,(1)
where r θ (x, y) is the score assigned to response y given x, and σ is the sigmoid function converting the score difference into a probability.This loss encourages the reward model to output higher scores for responses with superior judgments and analyses.</p>
<p>Incorporating the Reward Model into PPO Training.During PPO training on SST-2, LLM is tasked with generating both a sentiment judgment and an analysis.The trained reward model provides the reward signal by scoring these outputs.As shown in Table 7a, while the PPO model trained with reward signals from the reward model (PPO-RM) produces analyses of higher quality, it suffers from a significant reduction in classification performance, dropping from 96.4% to 89.7%.We believe this discrepancy might be due to the limited sample size used for reward model training and potential reward hacking (Amodei et al., 2016) during optimization.However, we will explore this further in our future works.Evaluation of Generated Analyses.To further assess the impact of our reward design, we evaluated the quality of generated analyses.We sampled 100 data points from three models: the original LLAMA2-7B-chat-hf, the PPO model trained using only correct-answer rewards (PPO), and the PPO model trained with the reward model (PPO-RM).GPT-4o then scored each analysis on a scale from 1 to 5 based on answer correctness and logical coherence.As indicated in Table 7b, the PPO model using reward model signals achieved the highest average score, suggesting that a more complex reward function can enhance the quality of generated outputs.</p>
<p>In summary, while the integration of a reward model in PPO training significantly reduces classification performance compared to using only correctanswer rewards, it considerably improves the GPT evaluation scores of the analyses produced by the LLM.</p>
<p>Conclusion</p>
<p>Prompting-based approaches, including zero-shot and few-shot prompting, are commonly used to adapt LLMs to downstream tasks.However, our experiments show that when applied to LLAMA2-7B-chat-hf, these methods underperform on NLU benchmarks such as GLUE and SuperGLUE (table 10), often trailing smaller encoder-only models like BERT-base.To address this, we investigate two fine-tuning strategies that update only LoRA layers for computational efficiency: SFT and PPO.While SFT yields modest improvements, PPO provides substantial gains by framing NLU tasks as reinforcement learning problems.PPO-tuned models not only outperform strong baselines like BERTlarge but also generalize well across model families and tasks.Notably, PPO-trained LLAMA2-7Bchat-hf outperforms GPT-4o by 10.8% on SIGA-nli and 7.3% on the Mental Health dataset, demonstrating strong zero-shot generalization from single-task fine-tuning.More broadly, we highlight a promising direction: adapting LLMs to new tasks without labeled data by using reward-driven learning.With a well-defined reward function, PPO can steer models toward high-reward behaviors-offering a scalable, label-efficient alternative to SFT.</p>
<p>Limitations</p>
<p>This work takes an initial step toward framing NLU as a reinforcement learning problem for LLMs under 14B parameters.While our long-term goal is to reduce reliance on curated datasets by leveraging richer, task-specific reward models, we currently adopt a simple binary reward signal based on exact label matching.This design enables a controlled evaluation of PPO as an effective adaptation strategy, showing consistent gains over prompting and supervised fine-tuning.Although we present a preliminary exploration of model-driven reward functions in Section 3.8, further research is needed to develop robust and generalizable reward signals that can support more complex or weakly supervised tasks without requiring extensive manual annotation.Overall, our findings suggest that casting nuanced tasks as reinforcement learning problems, through the design of appropriate environments and reward functions, offers a scalable and flexible alternative to standard fine-tuning, particularly when the model is already well-initialized through pretraining.</p>
<p>A Related Works</p>
<p>Policy-based reinforcement learning (RL) directly optimizes an agent's policy by learning its parameters to maximize long-term rewards.Unlike valuebased methods like Q-learning (Watkins and Dayan, 1992) and DQN (Hester et al., 2018), which indirectly derive policies through value functions, policy-based methods represent the policy as a parameterized function.This function, p θ (a|s), defines the probability of taking action a in state s, where θ represents the policy parameters.The goal is to learn optimal parameters θ * that maximize the expected cumulative reward, typically through policy gradient methods (Sutton et al., 1999).These methods excel in high-dimensional or continuous action spaces, where value-based methods can struggle (Deisenroth et al., 2013).</p>
<p>Policy-based methods in reinforcement learning (RL) have evolved significantly over time, starting with REINFORCE (Williams, 1992), which optimizes policies using the policy gradient theorem but suffers from high variance due to its reliance on Monte Carlo estimates of the reward.Monte Carlo estimates refer to calculating the total reward based on full episodes of interaction, meaning updates are made only after an entire sequence of actions and rewards is observed, which can lead to noisy and slow learning.To address this, actor-critic methods like A2C and A3C (Mnih, 2016) introduced a critic that estimates the value of the current state, allowing for smoother updates by reducing the variability in policy updates and speeding up convergence.However, these methods still faced instability when large updates caused the new policy to diverge too far from the previous one.Trust Region Policy Optimization (TRPO) (Schulman, 2015) tackled this by limiting the size of policy updates using a KL divergence constraint, but its implementation was complex and computationally expensive.Proximal policy optimization (PPO) (Schulman et al., 2017a) simplified this process by introducing a clipped objective function that keeps policy updates within a stable range while being easier to implement.PPO's balance between simplicity and stability has made it a widely adopted method in modern RL research.</p>
<p>In NLP, PPO has been effectively used in reinforcement learning from human feedback (RLHF) to align LLM outputs with human preferences, as seen in works like InstructGPT (Ouyang et al., 2022) and Constitutional AI (Bai et al., 2022).</p>
<p>These approaches treat the LLM as a policy, where model responses are actions, and human feedback serves as rewards.PPO updates the policy based on the reward model trained on human preferences.Additionally, policy-based RL methods have been applied to enhance LLM reasoning capabilities (Ziegler et al., 2019;Havrilla et al., 2024;Hu and Shu, 2023).In this work, we apply PPO to fine-tune LLMs on NLU tasks.</p>
<p>B Preliminaries on Application of PPO for Fine-tuning LLMs</p>
<p>Proximal policy optimization (PPO) (Schulman et al., 2017b) is an online reinforcement learning algorithm.In this section, we describe the process to fine-tune an LLM using PPO.During training, at each timestep t, the LLM (policy) generates a token prediction a t (action) based on the state s t , which consists of the sequence of generated tokens up to timestep t − 1.The final generated output is evaluated in the context of the downstream task, where the environment provides feedback in the form of rewards.The model updates its parameters based on these rewards to improve its ability to generate accurate predictions over time.</p>
<p>Actor Model PPO uses gradient ascent to optimize the following objective, aiming to maximize cumulative rewards:
J(θ) = E (st,at)∼π θ ′ min p θ (a t | s t ) p θ ′ (a t | s t ) Ât , clip p θ (at|st) p θ ′ (at|st) , 1 − ϵ, 1 + ϵ Ât (2)
Here, p θ (a t |s t ) is the probability of taking action a t in state s t under the current policy, while p θ ′ (a t |s t ) represents this probability under the old policy.</p>
<p>In PPO, the training data-specifically, the stateaction pairs (s t , a t )-are sampled using the old policy π θ ′ (the LLM before it is updated), rather than the new policy currently being optimized.Thus, the ratio p θ (at|st) p θ ′ (at|st) accounts for how much the new policy has changed relative to the old policy and adjusts the likelihood of an action accordingly.This ratio is multiplied by Ât , the Generalized Advantage Estimation (GAE) (Schulman et al., 2018), which measures how much better or worse an action a t is compared to the expected outcome under the current policy.
Ât = R t + γV t+1 − V t + γλ Ât+1 ,
Here, R t + γV t+1 − V t represents the temporal difference (TD) error (Sutton, 1988).In this expression, R t is the immediate reward received after taking action a t , V t is the expected reward before the action, and γV t+1 is the discounted estimate of the future reward after the action.This term reflects how the action a t performed when compared to the expected return at state s t .The second term, γλ Ât+1 , is the smoothing factor in GAE, where λ is the trade-off parameter.This recursive estimate allows the model to incorporate future information, making the advantage estimate more stable.</p>
<p>Smaller values of λ emphasize on immediate rewards, while larger values capture longer-term dependencies.The discount factor γ controls how much emphasis is placed on future rewards compared to immediate ones, with higher values of γ giving more weight to future rewards.V t , which represents the expected future reward from state s t , is estimated by a critic model.</p>
<p>The clipping function clip(ratio, 1−ϵ, 1+ϵ) limits the change between the current and old policy, ensuring stable updates by preventing large deviations.This helps avoid too-large policy changes that could destabilize training.In summary, PPO optimizes the policy using gradient ascent to maximize cumulative rewards while ensuring stable updates through clipping, with the GAE providing a more stable and accurate advantage estimate by incorporating future information recursively.</p>
<p>Critic Model</p>
<p>The critic model consists of a value head, which is a multi-layer perceptron attached to the final layer of the LLM.It takes the LLMs representation of the generated token sequence up to timestep t (i.e., the state s t ) and predicts a scalar value representing the value function V t for that state.The critic model is updated using the square of TD error, which is computed as:
δ t = (R t + γV t+1 − V t ) 2 ,(3)
where δ t represents the L-2 loss between the actual reward R t , combined with the discounted estimate of future rewards γV t+1 , and the current predicted value V t for state s t .By minimizing this TD error via gradient descent, the critic model updates its value function predictions, improving alignment with the actual rewards and future outcomes.In summary, LLM uses the PPO objective to update its policy based on feedback from the critic model, while the critic model is updated to better predict the value function for future states.</p>
<p>We implement PPO using TRL library (Face) that includes a KL divergence term in the PPO objective 2.</p>
<p>C Hyperparameter Settings</p>
<p>For PPO-based fine-tuning, grid search is performed to select the batch size in 4, 8, 12, and 16 for each task.A batch size of 24 was used across all tasks during supervised fine-tuning (SFT).The PPO epoch is set to 4, meaning that each sampled batch is used for updating the model four times.The initial learning rate for all tasks was set to 9 × 10 −6 .We utilized the Adafactor optimizer for PPO training and AdamW for SFT.A cosine annealing learning rate scheduler with a warmup phase was employed, where the learning rate was gradually increased during the first 10% of training steps and then reduced to one-tenth of the initial value by the end of training.We use a rank r = 16 for the LoRA layers.We trained both PPO and SFT models until convergence on the validation set.The best hyperparameters were selected based on performance on the validation set.The final reported results for the GLUE and SuperGLUE are from their corresponding evaluation server.For evaluation, multinomial sampling with a temperature of 1 was used to generate a single response per data sample.The model generated responses with lengths between 12 and 32 tokens, with the generation process concluding using a special identifier "</Judgement>".</p>
<p>For few-shot evaluations, we conducted a sweep over 1-5 shots on the validation set and reported results using the best-performing configuration for each experiment.We ensured that the total input length, including few-shot demonstrations, remained within the model's context window (e.g., in MultiRC, where input lengths can be long).For our experiments, we utilized the TRL library (von Werra et al., 2020) on a single Nvidia A100 GPU.</p>
<p>D Reward Curve for PPO Fine-Tuning</p>
<p>We present the reward curve from fine-tuning LLAMA2-7B-chat-hf using PPO in a multitask setting on the GLUE dataset.Figure 2 illustrates the reward values over training iterations, offering insights into the training dynamics of the model.The curve serves as a key performance metric, tracking the model's learning progress across multiple tasks.The consistent upward trend demonstrates that PPO fine-tuning effectively improves LLAMA2-Table 8: To quantify uncertainty in our evaluations, we generate 100 predictions for each example in the dataset.The evaluation metric is then computed for each set over the entire dataset, forming a distribution of values.The 95% confidence interval is defined by the 2.5th and 97.5th percentiles of this distribution.For sentiment analysis, models fine-tuned on SST-2 are evaluated in a zero-shot setting on Financial PhraseBank, Labelled Financial News, Mental Health, and Emotion datasets.For natural language inference, models fine-tuned on MNLI are zero-shot evaluated on Babi-NLI and SIGA-NLI.7B-chat-hf's ability to generate task-relevant outputs.</p>
<p>E Confidence Intervals on Validation Set of GLUE Benchmark</p>
<p>Our GLUE results are obtained from the official online evaluation servers, which impose a submission limit and keep test-set labels hidden, so applying a bootstrapping procedure directly is not feasible.We compute 95% confidence intervals on the validation sets of the GLUE benchmark (available offline) using the following procedure (used in the paper).For each example in a given dataset, we generate 100 independent predictions.We then compute the evaluation metric over the entire dataset for each set of predictions, yielding a distribution of metric values.The 2.5th and 97.5th percentiles of this distribution define the bounds of our 95% confidence interval.The resulting intervals are reported in the table 9.</p>
<p>F Results on SuperGLUE Benchmark</p>
<p>We fine-tuned the LLAMA2-7B-chat-hf model using PPO on the SuperGLUE dataset and compared its performance against several baselines, including BERT-large, BERT-large++, and zero-shot and fewshot prompting of LLAMA2-7B-chat-hf.The term "BERT++" refers to a BERT model fine-tuned using the supplementary training on intermediate labeleddata tasks (STILTs) approach (Phang et al., 2018), where the model is first fine-tuned on related transfer tasks before being fine-tuned on SuperGLUE tasks.For example, MNLI from the GLUE benchmark (Wang et al., 2019) is used as an intermediate task for CB, RTE, and BoolQ (Wang et al., 2020).In contrast, our experiments with LLM did not use this method.Our models were only fine-tuned on the datasets in the SuperGLUE benchmark.</p>
<p>As shown in Table 10, the PPO-tuned LLAMA2-7B-chat-hf achieved the highest average performance, surpassing all baselines.PPO demonstrated particularly strong improvements on reasoningintensive tasks like COPA and MultiRC, where it significantly outperformed both prompting methods and encoder-only models.These results highlight the effectiveness of PPO in improving the model's capabilities, particularly for tasks requiring reasoning and contextual understanding.</p>
<p>It is worth noting that on MultiRC, few-shot prompting performs slightly worse than zero-shot prompting.This may be because MultiRC involves long input contexts, and incorporating examples in a few-shot prompt can cause the total input length to approach or exceed the LLMs maximum context window.</p>
<p>G Evaluation on Reading Comprehension Tasks</p>
<p>We evaluate LLAMA2-7B-chat-hf on the SQuAD reading comprehension task, where the objective is to select a passage from a given context that best answers a question.Two fine-tuning strategies are compared: SFT, which directly uses the ground-truth answer as the training label, and PPO, which leverages reward functions based on Exact Match (EM) and F1 Score.EM metric is computed by comparing a normalized prediction against the normalized ground truth (with normalization involving lowercasing and punctuation removal); a perfect match yields an EM score of 1, otherwise 0. F1 score measures word-level overlap, balancing how many predicted words are correct (precision) and how many ground-truth words are included (recall).Models were fine-tuned for one epoch on the SQuAD training set and evaluated on the development set.In our evaluation, zero-shot prompting yields an EM of 7.66 and an F1 score of 32.27.SFT significantly improves these metrics (EM: 59.17, F1: 76.48), while PPO further enhances performance, achieving an EM of 65.74 and an F1 score of 81.82-corresponding to improvements of 6.57 and 5.34 points over SFT, respectively.</p>
<p>These results indicate that optimizing with reward functions based on EM and F1 via PPO leads to further improvements in reading comprehension performance, thereby validating our approach relative to both zero-shot prompting and standard SFT.</p>
<p>Method EM F1</p>
<p>Non-fine-tuned 7.66 32.27 SFT 59.17 76.48 PPO 65.74 81.82</p>
<p>H Comparison of LLaMA2-7B-base and LLaMA2-7B-chat-hf on GLUE Tasks</p>
<p>We fine-tuned LLaMA2-7B-base with PPO and compared it to PPO-tuned LLaMA2-7B-chat-hf on GLUE tasks.The instruction-tuned chat model, with enhanced prompt-following capabilities, outperforms the base model on most tasks, averaging 85.13 vs. 82.51.This highlights the benefit of starting from an instruction-tuned checkpoint for task-specific RL.</p>
<p>Figure 1 :
1
Figure1: Despite extensive pre-training and instruction tuning, zero-shot and few-shot prompting of models like LLAMA2-7B-chat-hf and LLAMA2-13B-chat-hf continues to underperform on NLU tasks, often falling short of smaller encoder-based models like BERT.To address this performance gap, we recast the task of NLU adaptation as an RL problem, fine-tuning the model using PPO.The input consists of a prompt and a query.In the standard setting (top path), the base model, with zero-shot/few-shot prompting, struggles to generate correct answers.Our approach selectively updates lightweight LoRA layers, using PPO to optimize a reward signal based on task correctness.This optimization encourages performance gains while constraining deviation from the base policy.We find that PPO fine-tuning substantially outperforms standard SFT.</p>
<p>Figure 2 :
2
Figure 2: Reward curve for multitask PPO fine-tuning of LLAMA2-7B-chat-hf on the GLUE dataset.The plot illustrates the relationship between training iterations (x-axis) and reward values (y-axis), demonstrating the effectiveness of the PPO optimization approach in improving model performance over time.</p>
<p>Table 3, both PPO and GRPO deliver notable performance improvements over SFT.Notably, PPO only incurs about a 4% increase in per-step runtime compared to SFT.However,
ModelsSTS-BCOPABERT-large86.570.6LLAMA2-7B-chat-hfZero-shot prompting27.557.0Few-shot prompting45.573.4PPO-ST92.688.6Qwen2.5-7B-InstructZero-shot prompting83.796.6Few-shot prompting87.096.0PPO-ST92.297.0MPT-7B-chatZero-shot prompting19.757.4Few-shot prompting21.757.2PPO-ST89.384.0Table 2: Performance comparison of LLAMA2-7B-chat-hf, Qwen2.5-7B-Instruct (Hui et al., 2024), and MPT-7B-chat (MosaicML, 2023) models on the GLUE STS-B and SuperGLUE COPA tasks under zero-shot prompt-ing, few-shot prompting, and PPO based fine-tuning.Results are sourced from the official GLUE benchmarkand SuperGLUE benchmark evaluation servers. ForSTS-B, we report Spearman correlation, and for COPA,accuracy is used as the evaluation metric.</p>
<p>Table 3 :
3
Comparison of SFT, PPO, and GRPO fine-tuning methods on the LLAMA2-7B-chat-hf model across five GLUE benchmark tasks (SST-2, MRPC, RTE, CoLA, QNLI), along with per-step runtime.
Algorithm SST-2 MRPC RTE CoLA QNLI Avg. Per-Step Runtime (s)SFT73.885.880.450.793.676.94.124PPO96.489.484.359.993.284.64.299GRPO96.791.288.555.293.184.95.155TasksLLAMA2-7B PPO-ST LLAMA2-13B PPO-ST GPT-4oSentiment AnalysisFinancial PhraseBank97.297.797.5Labelled Financial News70.272.367.8Mental Health67.266.659.9Emotion78.076.477.6Natural Language InferenceBabi-nli68.369.463.2SIGA-nli46.246.335.4Average71.2 (4.3↑)71.5 (4.6↑)66.9</p>
<p>Table 5 :
5
Performance of LLAMA2-7B-chat-hf on the Amazon Review dataset.Best results are highlighted in bold.</p>
<p>Table 7 :
7
Comparison of reward function designs for LLAMA2-7B-chat-hf.The model trained with a rule based reward (PPO) achieves a high SST-2 classification accuracy of 96.4%, while incorporating a sophisticated reward model (PPO-RM) significantly reduces accuracy (89.7%) but yields substantially improved analysis quality, with a GPT evaluation score of 4.104 compared to 3.479 for the simple reward.Best results are highlighted in bold.</p>
<p>Table 10 :
10
(Wang et al., 2020)lts are scored by the evaluation server (SuperGLUE benchmark).The experimental data for BERT-large and BERT-large++ are taken from the original SuperGLUE paper(Wang et al., 2020).The metrics used in the experiments are as follows: CB: F1 / Acc; MultiRC: F1 / Exact Match; ReCoRD: F1 / Exact Match; AXb: MCC; AXg: Gender parity score / Acc.For the remaining tasks not mentioned, accuracy (Acc) is reported.Average column corresponds to the averaged performance across all the datasets.For tasks with multiple evaluation metrics, we first compute the average of those metrics to obtain a single task score, which is then used in the overall average calculation.The bolded results indicate the best results, and the underlined results indicate the second-best results.</p>
<p>Table 11 :
11
Performance of LLAMA2-7B-chat-hf on the SQuAD dataset.PPO uses Exact Match and F1 as reward signals.Best results are highlighted in bold.</p>
<p>Table 12 :
12
Performance comparison of PPO fine-tuning on LLaMA2-7B-chat-hf and LLaMA2-7B-base across GLUE tasks.Best results for each dataset are highlighted in bold.
DatasetLLaMA2-7B-LLaMA2-7B-chat-hfbaseCoLA64.5457.29MRPC88.4180.35STS-B88.5786.81QQP83.3484.39QNLI93.1792.95MNLI-matched88.6387.15MNLI-unmatched 88.9588.22RTE84.9083.75SST-296.1096.56WNLI74.6567.61Average85.1382.51
The 14B cutoff reflects our computational constraints; LLAMA-2-13B-chat-hf is the largest model we were able to evaluate.
https://huggingface.co/daryl149/ llama-2-7b-chat-hf</p>
<p>Perplexity-a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America. 62S1</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565Concrete problems in ai safety. 2016arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprintCameron McKinnon, and 1 others</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Structured language modeling. Ciprian Chelba, Frederick Jelinek, Comput. Speech Lang. 142000</p>
<p>A survey on policy search for robotics. Marc Peter Deisenroth, Gerhard Neumann, Foundations and Trends® in Robotics. 21-2Jan Peters, and 1 others. 2013</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, arXiv:1810.048052018arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052019Preprint</p>
<p>Depression Dataset on hugging face. Joan Gaes, 2023</p>
<p>Teaching large language models to reason with reinforcement learning. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu, arXiv:2403.046422024arXiv preprint</p>
<p>Deep q-learning from demonstrations. Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832Ian Osband, and 1 others</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021Preprint</p>
<p>Language models, agent models, and world models: The law for machine reasoning and planning. Zhiting Hu, Tianmin Shu, arXiv:2312.052302023arXiv preprint</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, arXiv:2409.12186Kai Dang, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, arXiv:2412.19437Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint</p>
<p>Good debt or bad debt: Detecting semantic orientations in economic texts. P Malo, A Sinha, P Korhonen, J Wallenius, P Takala, arXiv:1609.07843Journal of the Association for Information Science and Technology. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher652014. 2016PreprintPointer sentinel mixture models</p>
<p>Volodymyr Mnih, arXiv:1602.01783Asynchronous methods for deep reinforcement learning. 2016arXiv preprint</p>
<p>Introducing mpt-7b: A new standard for open-source, commercially usable llms. Accessed. Mosaicml, 2023</p>
<p>SIGA: A naturalistic NLI dataset of English scalar implicatures with gradable adjectives. Rashid Nizamani, Sebastian Schuster, Vera Demberg, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 20242024</p>
<p>Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Advances in neural information processing systems. 35</p>
<p>Jason Phang, Thibault Févry, Samuel R Bowman, arXiv:1811.01088Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. 2018arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 201919</p>
<p>CARER: Contextualized affect representations for emotion recognition. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, Yi-Shin Chen, 10.18653/v1/D18-1404Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>John Schulman, arXiv:1502.05477Trust region policy optimization. 2015arXiv preprint</p>
<p>High-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.024382018Preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017aarXiv preprint</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.063472017bPreprint</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.033002024Preprint</p>
<p>Sentiment analysis -labelled financial news data. Arav Sood, 10.34740/KAGGLE/DSV/74141902024</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, Machine learning. 31988</p>
<p>Policy gradient methods for reinforcement learning with function approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in neural information processing systems. 199912</p>
<p>Wenyin Fu, and 49 others. 2023a. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, and 1 others. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint</p>
<p>Kashif Rasul, and Quentin Gallouédec. Younes Leandro Von Werra, Lewis Belkada, Edward Tunstall, Tristan Beeching, Nathan Thrush, Shengyi Lambert, Huang, Trl: Transformer reinforcement learning. 2020</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1905.005372020Preprint</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019</p>
<p>Qlearning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 81992</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.05698Towards ai-complete question answering: A set of prerequisite toy tasks. 2015arXiv preprint</p>
<p>Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 81992</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
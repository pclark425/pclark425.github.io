<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7141 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7141</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7141</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-16f8bbfb04b1d9ac350e15732bc021e778a2cf68</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/16f8bbfb04b1d9ac350e15732bc021e778a2cf68" target="_blank">DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel reasoning principle: Dynamic Reflection with Divergent Thinking within a retriever-reranker framework that outperforms GPT-Turbo-3.5 on three datasets using 7b models e.g., Vicuna-7b and Openchat-7b on NDCG@10.5.</p>
                <p><strong>Paper Abstract:</strong> The rise of Large Language Models (LLMs) has sparked interest in their application to sequential recommendation tasks as they can provide supportive item information. However, due to the inherent complexities of sequential recommendation, such as sequential patterns across datasets, noise within sequences, and the temporal evolution of user preferences, existing LLM reasoning strategies, such as in-context learning and chain-of-thought are not fully effective. To address these challenges, we introduce a novel reasoning principle: Dynamic Reflection with Divergent Thinking within a retriever-reranker framework. Our approach starts with a collaborative in-context demonstration retriever, which collects sequences exhibiting collaborative behaviors as in-context examples. Following this, we abstract high-level user preferences across multiple aspects, providing a more nuanced understanding of user interests and circumventing the noise within the raw sequences. The cornerstone of our methodology is dynamic reflection, a process that emulates human learning through probing, critiquing, and reflecting, using user feedback to tailor the analysis more effectively to the target user in a temporal manner. We evaluate our approach on three datasets using six pre-trained LLMs. The superior performance observed across these models demonstrates the efficacy of our reasoning strategy, notably achieved without the need to fine-tune the LLMs. With our principle, we managed to outperform GPT-Turbo-3.5 on three datasets using 7b models e.g., Vicuna-7b and Openchat-7b on NDCG@10. This research not only highlights the potential of LLMs in enhancing sequential recommendation systems but also underscores the importance of developing tailored reasoning strategies to fully harness their capabilities.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7141.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7141.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT - Vicuna-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking applied to Vicuna-7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's Dynamic Reflection (DR) combined with Divergent Thinking (DT) (DRDT) is applied as a generate-then-reflect reranking procedure on Vicuna-7b: the LLM probes next-item predictions, is given the ground-truth next item as feedback to critique and update aspect weights, and repeats this probe-critique-reflect cycle for T steps to improve temporal preference modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter open-source instruction-tuned LLM used as a reranker; run via FastChat without fine-tuning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking (DRDT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative probe-critique-reflect: use Divergent Thinking to produce multi-aspect preference analyses from a subsequence, predict the next item, reveal the true next item as feedback, ask the model to critique and reweight aspects, then step forward and repeat for T steps before final reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@10, Recall@20, NDCG@10, NDCG@20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Plain-text baseline — ML-1M: Recall@10 0.3700, NDCG@10 0.1733; Games: Recall@10 0.3700, NDCG@10 0.1783; Luxury: Recall@10 0.3450, NDCG@10 0.1534.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>DRDT — ML-1M: Recall@10 0.5250, NDCG@10 0.2698; Games: Recall@10 0.4300, NDCG@10 0.2361; Luxury: Recall@10 0.4550, NDCG@10 0.2418.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes general risks: DRDT reduces hallucination accumulation but is limited by context length and available subsequence length; DR/DT depend on LLM internal reasoning and may still hallucinate if initial analyses are poor; collaborative in-context examples (CIC) can introduce noise if not regulated by DR. No specific per-case catastrophic failures reported for Vicuna-7b, but improvements vary across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7141.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7141.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT - Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking applied to Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DRDT applied to Vicuna-13b uses the same probe-critique-reflect cycles with multi-aspect analysis and ground-truth feedback to update aspect importance over T steps before final reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 13B-parameter instruction-tuned open LLM used as a reranker via FastChat, not fine-tuned in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking (DRDT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multi-aspect analyses, predict next engagement, present the actual next item as feedback, ask the model to critique and reweight aspects, repeat for T steps to model temporal evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@10, Recall@20, NDCG@10, NDCG@20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Plain-text baseline — ML-1M: Recall@10 0.5550, NDCG@10 0.2855; Games: Recall@10 0.4400, NDCG@10 0.2269; Luxury: Recall@10 0.2800, NDCG@10 0.1525.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>DRDT — ML-1M: Recall@10 0.5450, NDCG@10 0.3153; Games: Recall@10 0.4500, NDCG@10 0.2731; Luxury: Recall@10 0.4450, NDCG@10 0.2608.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mixed results: a slight decrease in ML-1M Recall@10 (0.5550 → 0.5450) but improvements in NDCG@10 and on other datasets; paper emphasizes that DRDT can sometimes reduce certain metrics for specific model/dataset combinations. General limitations: dependence on context length, potential for noisy CIC examples unless DR regulates them, and vulnerability to LLM hallucinations in DT stage if not corrected by reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7141.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7141.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT - Openchat-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking applied to Openchat-7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DRDT was applied to Openchat-7b as the reranker: the model generates multi-aspect preferences, predicts, receives the true next item as critique feedback, and updates aspect weights across multiple steps (up to T), improving ranking performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Openchat-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter open instruction-tuned chat model used as reranker via FastChat, evaluated zero/few-shot without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking (DRDT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative probe (predict next item using DT), provide ground-truth item to prompt critique and reflection, reinforce aspects deemed influential, repeat for T steps before final reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@10, Recall@20, NDCG@10, NDCG@20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Plain-text baseline — ML-1M: Recall@10 0.4200, NDCG@10 0.2075; Games: Recall@10 0.1450, NDCG@10 0.0702; Luxury: Recall@10 0.2950, NDCG@10 0.1341.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>DRDT — ML-1M: Recall@10 0.6350, NDCG@10 0.3335; Games: Recall@10 0.4800, NDCG@10 0.3069; Luxury: Recall@10 0.4550, NDCG@10 0.2589.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes DRDT gives consistent improvements here, but limitations include maximal reflection steps constrained by sequence length/context tokens (they limited to ≤3), and reliance on ground-truth user feedback (sequence) — technique requires that such feedback is available for reflection during the probing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7141.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7141.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT - Mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking applied to Mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DRDT applied to Mistral-7b: the method uses DT to extract multi-aspect analyses, predicts next-item, exposes ground-truth for critique, and repeats probe-critique-reflect cycles (T steps) to update predictions and improve ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter LLM (Mistral family) used as a reranker through FastChat without finetuning in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking (DRDT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Multi-aspect divergent analysis, predict next, present true next item to solicit critique and adjust aspect importance, repeat for T steps (probe-critique-reflect cycles) before reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@10, Recall@20, NDCG@10, NDCG@20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Plain-text baseline — ML-1M: Recall@10 0.2350, NDCG@10 0.1109; Games: Recall@10 0.2000, NDCG@10 0.1094; Luxury: Recall@10 0.1000, NDCG@10 0.0590.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>DRDT — ML-1M: Recall@10 0.3750, NDCG@10 0.1842; Games: Recall@10 0.4450, NDCG@10 0.2462; Luxury: Recall@10 0.4100, NDCG@10 0.2512.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DRDT produced very large relative improvements for weaker baseline models (paper reports up to >100% relative gains), but general caveats remain: dependence on available subsequence length, context window constraints, and possibility that DT stage can hallucinate analyses which must be corrected by reflection; no per-case model-specific failures were singled out beyond these general limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7141.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7141.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT - GPT-Turbo-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking applied to GPT-Turbo-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors applied DRDT (probe-critique-reflect cycles with multi-aspect divergent analyses and ground-truth feedback) to GPT-Turbo-3.5 as a reranker and compared performance to plain-text, ICL, and COT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Turbo-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's ChatGPT Turbo (3.5) API model used here as a strong baseline reranker; used without additional fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Reflection with Divergent Thinking (DRDT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use DT to generate multi-aspect analyses, predict next engagement, reveal the true next item (human feedback from sequence) to prompt critique and adjust aspect weights, repeat across T steps to model temporal evolution before reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@10, Recall@20, NDCG@10, NDCG@20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Plain-text baseline — ML-1M: Recall@10 0.5450, NDCG@10 0.2688; Games: Recall@10 0.4400, NDCG@10 0.2499; Luxury: Recall@10 0.4350, NDCG@10 0.2260.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>DRDT — ML-1M: Recall@10 0.7550, NDCG@10 0.4630; Games: Recall@10 0.5450, NDCG@10 0.3435; Luxury: Recall@10 0.5600, NDCG@10 0.3190.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some metrics show small negative deltas versus certain baselines in Table 2 (e.g., slight decrease in R@20 for GPT-Turbo-3.5 when comparing some rows), indicating DRDT does not uniformly improve every metric/dataset combination; overall limits include context length, maximum reflection steps (paper limited to ≤3), potential dependence on quality of multi-aspect DT analyses and noisy collaborative examples unless DR corrects them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Large language models are reasoners with selfverification <em>(Rating: 2)</em></li>
                <li>Deductive verification of chain-of-thought reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7141",
    "paper_id": "paper-16f8bbfb04b1d9ac350e15732bc021e778a2cf68",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "DRDT - Vicuna-7b",
            "name_full": "Dynamic Reflection with Divergent Thinking applied to Vicuna-7b",
            "brief_description": "The paper's Dynamic Reflection (DR) combined with Divergent Thinking (DT) (DRDT) is applied as a generate-then-reflect reranking procedure on Vicuna-7b: the LLM probes next-item predictions, is given the ground-truth next item as feedback to critique and update aspect weights, and repeats this probe-critique-reflect cycle for T steps to improve temporal preference modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b",
            "model_description": "A 7B-parameter open-source instruction-tuned LLM used as a reranker; run via FastChat without fine-tuning in this work.",
            "model_size": "7B",
            "reflection_method_name": "Dynamic Reflection with Divergent Thinking (DRDT)",
            "reflection_method_description": "Iterative probe-critique-reflect: use Divergent Thinking to produce multi-aspect preference analyses from a subsequence, predict the next item, reveal the true next item as feedback, ask the model to critique and reweight aspects, then step forward and repeat for T steps before final reranking.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)",
            "task_description": "Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).",
            "evaluation_metric": "Recall@10, Recall@20, NDCG@10, NDCG@20",
            "performance_before_reflection": "Plain-text baseline — ML-1M: Recall@10 0.3700, NDCG@10 0.1733; Games: Recall@10 0.3700, NDCG@10 0.1783; Luxury: Recall@10 0.3450, NDCG@10 0.1534.",
            "performance_after_reflection": "DRDT — ML-1M: Recall@10 0.5250, NDCG@10 0.2698; Games: Recall@10 0.4300, NDCG@10 0.2361; Luxury: Recall@10 0.4550, NDCG@10 0.2418.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Paper notes general risks: DRDT reduces hallucination accumulation but is limited by context length and available subsequence length; DR/DT depend on LLM internal reasoning and may still hallucinate if initial analyses are poor; collaborative in-context examples (CIC) can introduce noise if not regulated by DR. No specific per-case catastrophic failures reported for Vicuna-7b, but improvements vary across datasets.",
            "uuid": "e7141.0",
            "source_info": {
                "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DRDT - Vicuna-13b",
            "name_full": "Dynamic Reflection with Divergent Thinking applied to Vicuna-13b",
            "brief_description": "DRDT applied to Vicuna-13b uses the same probe-critique-reflect cycles with multi-aspect analysis and ground-truth feedback to update aspect importance over T steps before final reranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-13b",
            "model_description": "A 13B-parameter instruction-tuned open LLM used as a reranker via FastChat, not fine-tuned in these experiments.",
            "model_size": "13B",
            "reflection_method_name": "Dynamic Reflection with Divergent Thinking (DRDT)",
            "reflection_method_description": "Generate multi-aspect analyses, predict next engagement, present the actual next item as feedback, ask the model to critique and reweight aspects, repeat for T steps to model temporal evolution.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)",
            "task_description": "Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).",
            "evaluation_metric": "Recall@10, Recall@20, NDCG@10, NDCG@20",
            "performance_before_reflection": "Plain-text baseline — ML-1M: Recall@10 0.5550, NDCG@10 0.2855; Games: Recall@10 0.4400, NDCG@10 0.2269; Luxury: Recall@10 0.2800, NDCG@10 0.1525.",
            "performance_after_reflection": "DRDT — ML-1M: Recall@10 0.5450, NDCG@10 0.3153; Games: Recall@10 0.4500, NDCG@10 0.2731; Luxury: Recall@10 0.4450, NDCG@10 0.2608.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Mixed results: a slight decrease in ML-1M Recall@10 (0.5550 → 0.5450) but improvements in NDCG@10 and on other datasets; paper emphasizes that DRDT can sometimes reduce certain metrics for specific model/dataset combinations. General limitations: dependence on context length, potential for noisy CIC examples unless DR regulates them, and vulnerability to LLM hallucinations in DT stage if not corrected by reflection.",
            "uuid": "e7141.1",
            "source_info": {
                "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DRDT - Openchat-7b",
            "name_full": "Dynamic Reflection with Divergent Thinking applied to Openchat-7b",
            "brief_description": "DRDT was applied to Openchat-7b as the reranker: the model generates multi-aspect preferences, predicts, receives the true next item as critique feedback, and updates aspect weights across multiple steps (up to T), improving ranking performance in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Openchat-7b",
            "model_description": "A 7B-parameter open instruction-tuned chat model used as reranker via FastChat, evaluated zero/few-shot without fine-tuning.",
            "model_size": "7B",
            "reflection_method_name": "Dynamic Reflection with Divergent Thinking (DRDT)",
            "reflection_method_description": "Iterative probe (predict next item using DT), provide ground-truth item to prompt critique and reflection, reinforce aspects deemed influential, repeat for T steps before final reranking.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)",
            "task_description": "Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).",
            "evaluation_metric": "Recall@10, Recall@20, NDCG@10, NDCG@20",
            "performance_before_reflection": "Plain-text baseline — ML-1M: Recall@10 0.4200, NDCG@10 0.2075; Games: Recall@10 0.1450, NDCG@10 0.0702; Luxury: Recall@10 0.2950, NDCG@10 0.1341.",
            "performance_after_reflection": "DRDT — ML-1M: Recall@10 0.6350, NDCG@10 0.3335; Games: Recall@10 0.4800, NDCG@10 0.3069; Luxury: Recall@10 0.4550, NDCG@10 0.2589.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Paper notes DRDT gives consistent improvements here, but limitations include maximal reflection steps constrained by sequence length/context tokens (they limited to ≤3), and reliance on ground-truth user feedback (sequence) — technique requires that such feedback is available for reflection during the probing steps.",
            "uuid": "e7141.2",
            "source_info": {
                "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DRDT - Mistral-7b",
            "name_full": "Dynamic Reflection with Divergent Thinking applied to Mistral-7b",
            "brief_description": "DRDT applied to Mistral-7b: the method uses DT to extract multi-aspect analyses, predicts next-item, exposes ground-truth for critique, and repeats probe-critique-reflect cycles (T steps) to update predictions and improve ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7b",
            "model_description": "A 7B-parameter LLM (Mistral family) used as a reranker through FastChat without finetuning in these experiments.",
            "model_size": "7B",
            "reflection_method_name": "Dynamic Reflection with Divergent Thinking (DRDT)",
            "reflection_method_description": "Multi-aspect divergent analysis, predict next, present true next item to solicit critique and adjust aspect importance, repeat for T steps (probe-critique-reflect cycles) before reranking.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)",
            "task_description": "Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).",
            "evaluation_metric": "Recall@10, Recall@20, NDCG@10, NDCG@20",
            "performance_before_reflection": "Plain-text baseline — ML-1M: Recall@10 0.2350, NDCG@10 0.1109; Games: Recall@10 0.2000, NDCG@10 0.1094; Luxury: Recall@10 0.1000, NDCG@10 0.0590.",
            "performance_after_reflection": "DRDT — ML-1M: Recall@10 0.3750, NDCG@10 0.1842; Games: Recall@10 0.4450, NDCG@10 0.2462; Luxury: Recall@10 0.4100, NDCG@10 0.2512.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "DRDT produced very large relative improvements for weaker baseline models (paper reports up to &gt;100% relative gains), but general caveats remain: dependence on available subsequence length, context window constraints, and possibility that DT stage can hallucinate analyses which must be corrected by reflection; no per-case model-specific failures were singled out beyond these general limitations.",
            "uuid": "e7141.3",
            "source_info": {
                "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DRDT - GPT-Turbo-3.5",
            "name_full": "Dynamic Reflection with Divergent Thinking applied to GPT-Turbo-3.5",
            "brief_description": "The authors applied DRDT (probe-critique-reflect cycles with multi-aspect divergent analyses and ground-truth feedback) to GPT-Turbo-3.5 as a reranker and compared performance to plain-text, ICL, and COT baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Turbo-3.5",
            "model_description": "OpenAI's ChatGPT Turbo (3.5) API model used here as a strong baseline reranker; used without additional fine-tuning.",
            "model_size": "not specified (API model)",
            "reflection_method_name": "Dynamic Reflection with Divergent Thinking (DRDT)",
            "reflection_method_description": "Use DT to generate multi-aspect analyses, predict next engagement, reveal the true next item (human feedback from sequence) to prompt critique and adjust aspect weights, repeat across T steps to model temporal evolution before reranking.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "Sequential recommendation (Movielens-1M, Amazon Games, Amazon Luxury Beauty)",
            "task_description": "Next-item ranking / reranking among a candidate set using user interaction sequences (sequential recommendation).",
            "evaluation_metric": "Recall@10, Recall@20, NDCG@10, NDCG@20",
            "performance_before_reflection": "Plain-text baseline — ML-1M: Recall@10 0.5450, NDCG@10 0.2688; Games: Recall@10 0.4400, NDCG@10 0.2499; Luxury: Recall@10 0.4350, NDCG@10 0.2260.",
            "performance_after_reflection": "DRDT — ML-1M: Recall@10 0.7550, NDCG@10 0.4630; Games: Recall@10 0.5450, NDCG@10 0.3435; Luxury: Recall@10 0.5600, NDCG@10 0.3190.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Some metrics show small negative deltas versus certain baselines in Table 2 (e.g., slight decrease in R@20 for GPT-Turbo-3.5 when comparing some rows), indicating DRDT does not uniformly improve every metric/dataset combination; overall limits include context length, maximum reflection steps (paper limited to ≤3), potential dependence on quality of multi-aspect DT analyses and noisy collaborative examples unless DR corrects them.",
            "uuid": "e7141.4",
            "source_info": {
                "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Large language models are reasoners with selfverification",
            "rating": 2
        },
        {
            "paper_title": "Deductive verification of chain-of-thought reasoning",
            "rating": 1
        }
    ],
    "cost": 0.014318249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DRDT: DYNAMIC REFLECTION WITH DIVERGENT THINKING FOR LLM-BASED SEQUENTIAL RECOMMENDATION</h1>
<p>Yu Wang<br>University of Illinois Chicago<br>ywang617@uic.edu</p>
<p>Zhiwei Liu*<br>Salesforce AI Research<br>zhiweiliu@salesforce.com</p>
<p>Jianguo Zhang, Weiran Yao, Shelby Heinecke<br>Salesforce AI Research<br>{jianguozhang, weiran.yao, shelby.heinecke}@salesforce.com</p>
<p>Philip S. Yu<br>University of Illinois Chicago<br>psyu@uic.edu</p>
<h4>Abstract</h4>
<p>The rise of Large Language Models (LLMs) has sparked interest in their application to sequential recommendation tasks as they can provide supportive item information. However, due to the inherent complexities of sequential recommendation, such as sequential patterns across datasets, noise within sequences, and the temporal evolution of user preferences, existing LLM reasoning strategies, such as in-context learning and chain-of-thought are not fully effective. To address these challenges, we introduce a novel reasoning principle: Dynamic Reflection with Divergent Thinking within a retriever-reranker framework. Our approach starts with a collaborative in-context demonstration retriever, which collects sequences exhibiting collaborative behaviors as in-context examples. Following this, we abstract high-level user preferences across multiple aspects, providing a more nuanced understanding of user interests and circumventing the noise within the raw sequences. The cornerstone of our methodology is dynamic reflection, a process that emulates human learning through probing, critiquing, and reflecting, using user feedback to tailor the analysis more effectively to the target user in a temporal manner. We evaluate our approach on three datasets using six pre-trained LLMs. The superior performance observed across these models demonstrates the efficacy of our reasoning strategy, notably achieved without the need to fine-tune the LLMs. With our principle, we managed to outperform GPT-Turbo-3.5 on three datasets using 7 b models e.g., Vicuna- 7 b and Openchat- 7 b on NDCG@ 10 . This research not only highlights the potential of LLMs in enhancing sequential recommendation systems but also underscores the importance of developing tailored reasoning strategies to fully harness their capabilities.</p>
<h2>1 INTRODUCTION</h2>
<p>The remarkable advancements in Large Language Models (LLMs) also inspire the exploration of LLM-based recommender systems. The effectiveness of LLM-based recommender systems stems from vast pre-trained knowledge in LLMs, such as the understanding of item information and explanations of user behaviors Gao et al. (2023); Liu et al. (2023a); Hou et al. (2023b); Wang and Lim (2023). One approach involves fine-tuning existing LLMs to tailor them to recommendation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>scenarios Bao et al. (2023b); Chen (2023); Lin et al. (2023); Cui et al. (2022); Wu et al. (2023), with methods like TALLRec Bao et al. (2023b) using LORA Hu et al. (2021) technology for this adaptation with minimal training examples. Another strategy is fine-tuning LLMs specifically for recommendation tasks Geng et al. (2022); Hou et al. (2023a; 2022); Li et al. (2023a). Additionally, some research regards LLMs as zero-shot rankers Liu et al. (2023b); Gao et al. (2023); Liu et al. (2023a); Wang and Lim (2023), where they directly rank candidate items based on contextual information, e.g. past user interactions. However, the strategies for prompting LLMs are not yet fully explored, particularly in more complex areas like sequential recommendation, which consider the evolving nature of user interactions. Specifically, current methods involve using historical interactions to formulate prompts about possible future engagements in the form of: The user has such historical interactions: <history>, what is the possible next engagement among the candidate set $&lt;$ candidates $&gt;$ ? In the realm of zero-shot LLM ranking, further investigations Hou et al. (2023b) have delved into in-context-learning (ICL) and chain-of-thought (COT) Wei et al. (2022) approaches for prompt construction. An example includes using historical interactions, ending with the penultimate item as a query, and the second-last item as an answer, coupled with a "step-by-step" thought process to facilitate LLM's recommendation process. These varied approaches highlight the ongoing exploration and potential of LLMs in enhancing the effectiveness of recommendation systems.</p>
<p>The significant impact of prompt strategy on the reasoning capabilities of Large Language Models (LLMs) is the primary motivation for this paper. Recognizing the under-explored area of reasoning principles in sequential recommendation, this study aims to introduce and evaluate various prompting strategies to fully harness the reasoning potential of LLMs for sequential recommendation tasks without fine-tuning process, thereby aiming to bridge existing gaps in the field. To establish an effective reasoning framework for sequential recommendation using LLMs, it is essential to address these three challenges:</p>
<ol>
<li>The first challenge lies in capturing the collaborative signal for recommendations. Conventional recommender systems make recommendations by referring other collaborative users with similar behaviors. In sequential recommendation, these signals are learned through similar sequence representations. However, existing prompting strategies, including ICL and COT, are predominantly informed by current sequences, lacking a view across the broader dataset that includes inter-item correlations. Moreover, the context length limitations of LLMs make it impractical to feed entire datasets into the LLM during inference.</li>
<li>The second challenge is handling the personalized aspects and ubiquitous noise. Different users may care about different aspects when consuming the same item. Furthermore, user interactions with items can often include a certain amount of noise, which does not accurately reflect their true preferences.</li>
<li>The third and perhaps most intricate challenge is accurately capturing the temporal evolution of user interests. Unlike general NLP tasks like question-answering, sequential recommendation requires understanding how a user's preferences change over time, a nuance that current prompting strategies like plain-text prompting, ICL, and COT are not inherently designed to handle.</li>
</ol>
<p>Addressing the identified challenges, our paper introduces a novel reasoning principle called Dynamic Reflection with Divergent Thinking (DRDT) within a retriever-reranker framework. This approach is designed to effectively incorporate collaborative signals, manage personalized aspects and noise, enable temporal reasoning and omit error accumulating caused by hallucinations.</p>
<p>Collaborative In-Context Demonstration Retriever: The ICL within LLM-based recommender systems Hou et al. (2023b) primarily uses a portion of the target sequence to create an in-context example. However, this approach mainly aids the LLM in understanding the task and output format without offering additional supportive information. To enhance this process, we introduce a collaborative in-context demonstration retriever. This component is designed to gather sequences that end with the same item as the target user's collaborative in-context demonstrations. The in-context example informs LLM about the choice made by this collaborative user next. By employing this method, we not only assist the LLM in grasping the nature of the task but also provide it with reference information about the choices made by other users in similar situations.</p>
<p>Divergent Thinking for Various Aspects and Noise Management: Existing COT implementations predominantly follow a convergent thinking paradigm. In zero-shot COT, the phrase "Let's think</p>
<p>step-by-step" is often added to prompt the LLM to autonomously develop a reasoning path. Typically, the LLM tends to use the similarity between a candidate and a historical item as one of the reasoning paths. In few-shot COT, specific examples of reasoning paths are provided. Once a path is established, results are generated along it. Variants like COT-SC and TOT construct multiple reasoning paths, determining results through a consensus among these paths. However, in the context of sequential recommender systems, constructing predefined reasoning paths before inference presents challenges. Users consider multiple aspects - such as price, color, reviews, quality, etc. - and different users may prioritize these aspects differently for the same target items. Additionally, noise within sequences is ubiquitous. If this noise is not accurately identified and filtered out, the recommender system might misinterpret it as evidence of diverse interests, rather than discerning clear patterns in a user's true preferences. To address these challenges, we propose a shift from the traditional convergent thinking paradigm to a divergent thinking paradigm. Instead of seeking a universal reasoning path or allowing the LLM to independently construct a path, our approach involves considering user engagement from multiple aspects. Recommendations are then made based on this multi-faceted analysis, rather than solely relying on similarity comparisons between candidate and historical items. This method aims to provide a more nuanced and accurate reflection of individual user preferences and interests.</p>
<p>Dynamic Reflection for Temporal Reasoning: Existing prompting strategies often fall short of capturing the temporal evolutions inherent within a sequence. Specifically, ICL primarily assists LLMs in understanding the task's nature and in formulating the output. While COT encourages the LLM to reason through a certain path step-by-step, it often faces difficulty in identifying the most relevant factors leading to current user engagements. This uncertainty can lead to hallucination Yao et al. (2023); Martino et al. (2023); Zhang et al. (2023), where the LLM makes assumptions that deviate from the actual influencing factors, resulting in unreliable recommendations and potential error accumulation along the reasoning path. Instead of generating answers along an unreliable path, we propose a dynamic reflection scheme inspired by human learning processes, which includes probing, critiquing, and reflecting. Dynamic reflection begins with using a segment of the target sequence to prompt the LLM to probe the next possible item via divergent thinking. This process involves a two-step approach: firstly, the LLM generates a multi-aspect analysis to understand the various dimensions of user preferences. Following this, it utilizes this analysis to predict a specific item, based on the information available in the preceding subsequence. We then use this item to prompt LLM to critique its prediction and the associated analysis. This step allows the LLM to adjust and correct its analysis, thereby preventing the accumulation of errors. Following this, the process moves to the next item in the temporal order, with repeated rounds of probing, critiquing, and reflecting. Through this iterative process, the LLM can dynamically uncover the true preferences of the users.</p>
<p>The effectiveness of the DRDT principle was evaluated using three datasets and six existing LLMs. The numerical results demonstrate the effectiveness of this approach. We hope this paper can inspire further research in designing effective prompts for sequential recommendation systems. Our contributions can be summarized as follows:</p>
<ul>
<li>To the best of our knowledge, this paper is the first to comprehensively explore the impact of various prompt designs on sequential recommendation tasks. Our research extends across six LLMs and three distinct datasets, marking a significant advancement in understanding how prompt strategies influence LLM performance in this field.</li>
<li>We introduce the Dynamic Reflection with Divergent Thinking (DRDT) in a retriever-reranker framework, a novel approach that effectively integrates collaborative signals and the temporal evolution of user preferences into sequential recommendation tasks using LLMs. This framework is specifically designed to be robust against the noise commonly found in datasets, thereby enhancing the reliability of recommendations.</li>
<li>The implementation of our DRDT principle has led to notable improvements in the performance of six different LLMs across three datasets. These significant results underscore the efficacy of our design and highlight the importance of further research in this direction to refine and optimize prompt strategies for sequential recommendation systems.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 LLM FOR FEW-SHOT SEQUENTIAL RECOMMENDATION</h3>
<p>The work of Wang et al. (2023a) utilizes the LLM to generate the reasoning graph along the user attribute and interacted items, and use LLM to guess the next possible item to enrich the original useritem interaction graph. Then, they utilize the conventional GNN to learn the user/item embedding on such an enriched graph for recommendation. For the prompt design, they feed the user history interactions and attributes together to guide the LLM to generate possible reasoning chains among such factors. They also prompt masked user histories, and let the LLM fill in the mask with possible items. They compute the plausible scores measuring the similarity score between the filled-in items and the original items. RecMind Wang et al. (2023b) introduces the autonomous agents for multiple recommendation tasks by integrating SQL and database to incorporate item-related information for decision-making. The prompt they adopt for sequential recommendation is the combination of historical interactions and candidate items. They also introduce the self-inspiring reasoning that enables the LLM to reason through multiple paths. Liu et al. (2023a) conduct preliminary case studies on GPT for multiple recommendation tasks. For sequential recommendation, they conduct the zero-shot prompt that is similar to Wang et al. (2023b), a few-shot prompt that uses the previous interactions with the second last one as the example, and update the records for the final recommendation. Liu et al. (2023b) also conduct inference on LLM resembling the zero-shot prompt construction. Wang and Lim (2023) introduces a 3-step prompting strategy. They first use LLM to analyze the preference according to the user historical interactions (Step-1), then, they let LLM to pick movies according to the preference analysis (Step-2), and finally, they ask the LLM to rank movies given in the candidate sets (Step-3). The candidate sets are collected according to the cosine-smiliaries between item k-hot encoders constructed from the interacted users. Sanner et al. (2023) utilize completion, zero-shot, and few-shot prompt methods for LLM inference. Hou et al. (2023b) also adopt the zero-shot and few-shot prompting methods. The candidate sets are constructed via random selection or embedding-based model similarity prediction.</p>
<p>Another direction is to fine-tune the existing LLM for recommendation task. Gao et al. (2023) also utilize the zero-shot prompt including the user profiles, movie rating, and categories for LLM to select top-k items from candidates. Harte et al. (2023) finetune an LLM by completing the input corpus. The corpus the formalized in a zero-shot prompt manner, that forces the LLM to fill in the last item based on the historical interactions. TALLRec Bao et al. (2023b) constructs the recommendation task as the CTR task, they construct the task corpus using historical interaction as well as the user's ratings, the task is to predict whether the user likes the last item. PALR Chen (2023) also fine-tunes LLM using historical interactions. The input corpus are generated in a two-step manner. They first let the LLM summarize the user preferences according to the historical interactions, then, they let the LLM select the next item from candidates. BIGRec Bao et al. (2023a) first fine-tunes the LLM using the training corpus that are generated in zero-shot prompt format, i.e., input is the historical interactions, and the output is the last item. Then, they compare the L2 distance between the LLM output and item representations for a top-k recommendation. They also incorporate the popularity factor to rescale the L2 distance, such that the more popular item will obtain a smaller distance and, thus, a higher ranking. ReLLa Lin et al. (2023) further improves TALLRec by introducing the semantic-related sequences, which are the top-1 closed sequences measured in the space of LLM. They also adopt a zero-shot prompt similar method to construct the prompt.</p>
<h3>2.2 COT REASONING IN LARGE LANGUAGE MODELS</h3>
<p>Chain-of-Thought is one principle for prompt design, that facilitates the LLM to generate the answer along with the provided reasoning path Wei et al. (2022). Kojima et al. (2022) introduce the Zero-ShotCOT with a naive initialization adding "please think step-by-step" to the prompt. It is a very coarse level design and may lead significant error rate even with the most powerful LLM e.g., ChatGPT. Especially for the recommendation task, where the hidden reasoning varies across different users and different items. Zhang et al. (2022) introduce the Auto-COT inspired by Wei et al. (2022), they first cluster the questions into k-clusters, and sample typical questions in each cluster to generate the reasoning chains using the Zero-Shot-COT. Then, they sample such demonstrations into the prompt template in the Manual-COT manner. COT-SC Wang et al. (2022a) is proposed to sample multiple COT reasoning paths and choose the major voted answers as the final answer. Wang et al. (2022b)</p>
<p>proposes the prompt order ensemble and input-rationale ensemble COT methods to improve the LLM inference performance from the ensemble perspective. Li et al. (2023b) further improves the COT by introducing the diversified prompt for each question and training an additional path voter to assign the probability of the rationality of each generated reasoning path. They also introduce the strategy to assign the pseudo label of each step, which is included during the optimization procedure of path voters. COT-RR He et al. (2022) first sample diverse set of reasoning paths for a given question. Given such reasoning paths and corresponding predictions, they design a post-processing mechanism that checks the faithfulness of reasoning paths according to the retrieved fact from the common knowledge base. IRCOT Trivedi et al. (2022) interleaves the retrieval and COT in an alternative manner. They first extend the reasoning step of COT, and use the intermediate reasoning step as the query to retrieve the intermediate knowledge.</p>
<h1>2.3 Self-Verification in Large Language Models</h1>
<p>COT is vulnerable to intermediate mistakes and results in error accumulations. Instead of fine-tuning an additional LLM as the verifier, recent works tend to design prompts to allow LLMs to self-verify. Ling et al. (2023) introduce the natural program to generate the in-context verification step-by-step. Madaan et al. (2023) utilize the very LLM serving feedback for their output and refine their output iteratively. Shinn et al. (2023) adopt one LLM as the evaluator, and another LLM as the feedback giver, and both of them are stored in the memory module. Weng et al. (2022) use the LLM to generate multiple candidate answers, then they rewrite such answers or mask the conditions, which is fed into another LLM for inference with COT. They rank the second LLM's output with the ground truth or masked conditions to give the corresponding ranking score of the original candidates. However, the above methods rely on feedback generated by LLM, which still exists hallucinations within the feedback. Huang et al. (2023) evaluate the intrinsic self-evaluation methods, and point out that such paradigms are not always beneficial to the task. On the contrary, utilizing external tools or human feedback instead of LLM provides strict supervision of the LLM generation. However, in the recommendation task, the sequence is already the human feedback, we can directly utilize such human feedback to correct the LLM generation process. Valmeekam et al. (2023) also evaluate the effectiveness of the self-critic strategy using LLM as the verifier. They conducted empirical studies on the plan generation task, and the results indicate that the self-criticism from LLM degrades the performance compared to that using external feedback as the critic source.
In summary, both of these two directions do not dig deep into the prompt design for the sequential recommendation. They simply concate the history and the candidates together in a prompt. Current methods have pointed out that the prompt format can vary the performance in a substantial range. In this paper, we focus on the discussion of the effect of prompt design for recommendation performance. And we propose the prompt design principle for the sequential recommendation that dynamically reflects the belief of user preference that deep dive into the user feedback.</p>
<h2>3 Problem Definition</h2>
<p>Given the sequence of item title $s=\left{i_{1}, i_{2}, \ldots, i_{s}\right}$ of user $u$, and a pretrained large language model $L L M$, we design a prompt method $p(s)$ that maximize the ranking performance of $L L M$ over a pre-selected candidate item set $\left{c_{1}, c_{2}, \ldots, c_{n}\right}$.</p>
<h2>4 Methodology</h2>
<p>In this section, we first describe the retriever-reranker framework, specifically designed for recommender systems. Then, we discuss the details of each component, including the collaborative in-context demonstration retriever, the divergent thinking, and the dynamic reflection of user preferences with their feedback.</p>
<h3>4.1 Retriever-Reranker Framework</h3>
<p>One challenge in leveraging LLMs for recommendation tasks is the inability to process the entire dataset during inference. Firstly, excessive noise is a concern. For instance, when recommending movies to a user $i$ who primarily consumes fantasy films, it's crucial to focus on movies relevant</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of DRDT within retriever-reranker framework in a) and the reasoning process of DRDT in b).
to their viewing history. Irrelevant data in the dataset can act as noise, potentially misleading the LLM and impairing its performance. Secondly, the token length limitation of LLMs necessitates the selection of the most informative examples for effective processing. Recently, the use of retrievers in retrieval-augmented generation has gained increasing attention. These retrievers typically leverage semantic distances within the embedding space to identify sentences most similar to a given context as in-context examples. However, in the realm of recommender systems, collaborative information, which focuses on extracting similar purchase behavior patterns from the dataset, is vital. This means that there may be item co-occurrence patterns and noise in two sequences, where the noise results in the dissimilarity between these two sequences, a factor not typically accounted for in standard retrieval models. These considerations highlight the need for a retriever specifically designed for recommendation tasks, one that can effectively identify and utilize relevant collaborative information.</p>
<p>Another significant challenge arises from the mismatch between the typical generation tasks for which LLMs are optimized and the specific needs of recommender systems. In a recommender system, the objective is to have the system identify the top-k items that align with a user's potential interests. This requires the LLM's output to reference existing items rather than generating new pseudo items. Furthermore, users often exhibit a broad spectrum of interests that evolve over time. However, LLMs, being trained primarily for generating the next token based on input queries, lack the explicit capability to analyze and track the temporal evolution of user preferences. This limitation underscores the necessity for a reranker capable of dynamically analyzing user preferences in a temporal manner. Additionally, the vast item set typical in recommender systems makes it impractical to include all items for reranking by an LLM. Many effective methods exist for pre-selecting a subset of candidate items for further reranking. Therefore, our focus shifts to enhancing the reranking capabilities of LLMs over these pre-selected sets of candidate items, ensuring a more targeted and efficient recommendation process.</p>
<p>Formally, our framework comprises a retriever component and a reranker as shown in Figure 1. The retriever's primary function is to identify the most relevant purchase behaviors within the dataset, which serve as in-context references for the subsequent reranking process. The reranker, on the other hand, focuses on two key tasks. Firstly, it extracts collaborative information from the in-context examples provided by the retriever. Secondly, it analyzes user preferences dynamically based on their historical records. The reranker ensures that the recommendations are both relevant to the user's past behavior and reflective of emerging trends or changes in their preferences.</p>
<h1>4.2 Collaborative In-Context Demonstration Retriever</h1>
<p>In-context learning (ICL) has demonstrated effectiveness in improving the inference capabilities of LLMs, as it offers a thinking strategy and a template for answering queries. However, the application of ICL in recommendation tasks presents unique challenges compared to traditional NLP tasks like question answering (QA). In QA, the temporal pattern of the content is not typically critical, whereas, in sequential recommendations, it's essential to consider both the global temporal patterns and the content of the items to make informed decisions. Current implementations of ICL in recommender systems tend to focus solely on extracting purchase behaviors from a given sequence. For example, given a historical sequence $s=\left{i_{1}, i_{2}, i_{3}, i_{4}\right}$, the in-context example might be framed as: the</p>
<p>historical records are $\left{i_{1}, i_{2}\right}$ and user purchased $i_{3}$. This approach highlights the sequential pattern within the last two interactions, but it does not provide the LLM with sufficient context or collaborative information to effectively recommend the next likely engagement.</p>
<p>To effectively utilize information for the LLM reranker's reference procedure, it is crucial to discern and choose the most relevant data while avoiding noise or distractions. Intuitively, incorporating collaborative information into the LLM helps it understand the likely actions of other users in similar circumstances. We achieve this by using the last item in the user's historical sequence, as an anchor to identify other users who have also interacted with this item as shown in Figure 1. Formally, given the user history $s=\left{i_{1}, i_{2}, \ldots, i_{s}\right}$, we random sample historical records that also end at $\left{i_{1}^{\prime}, i_{2}^{\prime} \ldots, i_{s}\right}$, as well as their next purchase $i_{t}$ to construct the in-context example as There is another user that has history: $\left{i_{1}^{\prime}, i_{2}^{\prime} \ldots, i_{s}\right}$, given the candidate set $\mathcal{C}=\left{c_{1}, i_{t}, \ldots, c_{n}\right}$, the possible answer is $\left{i_{t}, c_{i}, \ldots, c_{j}\right}$, where the possible answer is the pseudo answer that gives the next possible engagement $i_{t}$ the higher rank. This approach not only provides the LLM with insights about potential next engagements based on collaborative user behavior but also indicates which candidates should be ranked higher.</p>
<h1>4.3 Divergent Thinking (DT)</h1>
<p>Chain-of-Thought (COT) reasoning has gained prominence in enhancing LLM inference for complex tasks where a direct, one-step answer generation is insufficient. The recommendation task, particularly concerning predicting the next user engagement, presents a similarly intricate scenario that demands a nuanced application of COT. In this context, there are several key challenges to consider in order to effectively guide the LLM in understanding the behavior of users: 1) Multi-aspect Interests: User records often encode a variety of interest aspects. A user's interaction with different items can represent diverse facets of their preferences, ranging from genre preferences in media consumption to specific features in product choices. 2) Ubiquity of Noise Interactions: Not all interactions in a user's history are equally indicative of their true preferences. Some may be outliers or noise, such as accidental clicks or one-time exploratory actions, which can skew the understanding of their interests. Given these challenges, the conventional application of COT, which might involve providing a specific reasoning example or simply appending a "Let's think step by step" prompt, falls short for recommendation tasks.</p>
<p>To tackle the identified challenges in recommendation tasks, we introduce a Divergent Thinking approach as shown in Figure 1 blue arrows. This method involves initially extracting a high-level abstraction of user preferences across multiple aspects, and then utilizing this abstraction as a guidance for predicting the next engagements. The process begins by querying the LLM (e.g., GPT) to identify the critical aspects users might consider when engaging with a specific item. For instance, in the context of movie watching, aspects such as actors, directors, or genres could be pivotal in a user's decision-making process. By identifying these aspects, we can analyze user preferences from a multifaceted perspective, effectively sidestepping the noise presented in raw input sequences. Formally, for each dataset, we prompt the LLM to generate a list of aspects that users may consider: $\left{a_{1}, a_{2}, \ldots, a_{k}\right}$. We then derive a list of user preference analyses based on these aspects respectively: $\left{p_{1}, p_{2}, \ldots, p_{k}\right}$. These high-level preference analyses are subsequently incorporated into the reranking process during LLM inference.</p>
<h3>4.4 DYNAMIC REFLECTION WITH USER FEEDBACK (DR)</h3>
<p>Addressing the temporal evolution of user interests is a crucial yet challenging aspect of sequential recommendation, especially when using LLMs. This task is more complex than standard NLP tasks like QA or sentiment analysis. In sequential recommendation, understanding user preferences goes beyond merely arranging behaviors in temporal order due to several intricacies: 1), Temporal Evolution of Interests: User interests are not static; they evolve over time. The factors driving these changes are often unobserved or hidden within the data. A failure to capture this temporal evolution leads to reducing the recommendation process to simplistic methods like majority counting, which do not truly reflect individual user preferences. 2), Alternating Short- and Long-Term Correlations: User behavior often exhibits a mix of short- and long-term interests. For instance, a user might consistently enjoy fantasy movies (long-term interest) but occasionally watch a comedy for a family</p>
<p>Table 1: Dataset Statistics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># Users</th>
<th style="text-align: center;"># Items</th>
<th style="text-align: center;"># Interactions</th>
<th style="text-align: center;">Avg. Actions of Users</th>
<th style="text-align: center;">Avg. Actions of Items</th>
<th style="text-align: center;">Sparsity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ML-1M</td>
<td style="text-align: center;">6,041</td>
<td style="text-align: center;">3,707</td>
<td style="text-align: center;">1,000,209</td>
<td style="text-align: center;">165.60</td>
<td style="text-align: center;">269.89</td>
<td style="text-align: center;">$95.53 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Games</td>
<td style="text-align: center;">50,567</td>
<td style="text-align: center;">16,860</td>
<td style="text-align: center;">389,718</td>
<td style="text-align: center;">7.71</td>
<td style="text-align: center;">23.12</td>
<td style="text-align: center;">$99.95 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Luxury</td>
<td style="text-align: center;">2,028</td>
<td style="text-align: center;">936</td>
<td style="text-align: center;">18,005</td>
<td style="text-align: center;">8.88</td>
<td style="text-align: center;">19.26</td>
<td style="text-align: center;">$99.05 \%$</td>
</tr>
</tbody>
</table>
<p>event (short-term deviation). This interplay between consistent long-term interests and intermittent short-term preferences adds complexity to understanding user preferences over time.</p>
<p>The Divergent Thinking (DT) approach addresses these complexities to a certain degree in understanding user preferences, yet it faces several limitations: First, Difficulty in Preference Analysis: Generating a comprehensive user preference analysis from raw sequences is challenging, especially with complex, evolving sequences. In such scenarios, the LLM may default to simply counting the majority of occurrences, which can oversimplify and misrepresent the true, nuanced preferences of the user. Second, Vulnerability to Hallucination: DT relies on the internal reasoning capabilities of the LLM. If this reasoning is flawed or invalid, it can lead to hallucination - where the LLM generates outputs based on incorrect or unfounded assumptions. As a result, the reranking based on these outputs may lead to inaccurate or misleading recommendations. Third, Decision-making Complexity with Multiple Aspects: While DT provides insights into user preferences from various aspects, it can be difficult for the LLM to prioritize these aspects effectively. Determining which aspects are most influential in predicting the user's next engagement is a complex task that DT does not inherently resolve.</p>
<p>To overcome these limitations, we propose a novel dynamic reflection (DR) scheme as shown in Figure 1 orange arrows. This method enables the learning of user preferences as they evolve over time, while omitting the accumulations of errors caused by hallucinations. DR mimics the human learning process and involves probing, critiquing, and reflecting. First, Inital Subsequence Analysis with DT: We begin by scrolling back $T$ steps in the user's item sequence, where $T$ is a predefined hyperparameter. For the subsequence $\left{i_{1}, i_{2}, \ldots, i_{s-T}\right}$, we apply the DT approach to derive a multiaspect user preference analysis $\left{p_{1}, p_{2}, \ldots, p_{k}\right}$. Second, Engagement Prediction and Reflection: The LLM is then tasked with predicting the next possible engagement between the actual target item and a randomly chosen candidate item $\left{i_{s-T+1}, r\right}$. We use $i_{s-T+1}$ as the ground truth for reflection, informing the LLM of the user's actual choice and querying whether its prediction aligns with this reality. The LLM is prompted to reflect on its multi-aspect belief and adjust accordingly. Third, Aspect-Specific Analysis Reinforcement: To enhance the focus on specific aspects of user behavior, we ask the LLM to identify which aspect might have influenced the user's choice of $i_{s-T+1}$ and give more weight to this factor in its analysis. Fourth, Stepwise Aspect Importance Update for Reranking: This probe-critique-reflect procedure is repeated for $T$ steps. With each step, the LLM updates its understanding of the user's behavior pattern, correcting any earlier misjudgments and preventing the accumulation of errors that might arise from hallucinations. After each iteration, we move one step forward, until we reach the last item, and the reranker starts the reranking process for the target item.</p>
<h1>5 EXPERIMENTS</h1>
<p>Dataset. In this paper, we adopt the Movielens-1M dataset and the Games and Luxury Beauty categories from the Amazon Review dataset, following common practice in sequential recommendation scenarios. We chose these datasets for several reasons: First, information about movies, games, and beauty products is ubiquitous online, and we hope the existing pre-trained LLMs can provide useful contextual information for such items to help improve recommendation performance. Second, the Amazon dataset is known for its sparsity and cold-start issues, which necessitate research on zero/few-shot recommendations leveraging supportive information from LLMs. We report the statistics of the datasets in Table 1.</p>
<p>Metrics. Following the common practice in sequential recommendation Kang and McAuley (2018); Wang et al. (2022c); Sun et al. (2019); Qiu et al. (2022), we adopt the Recall@K and NDCG@K as the metrics of ranking performance of recommender systems, where $\mathrm{K}=10,20$.</p>
<p>Baseline Methods. The zero-shot/few-shot sequential recommendation using pre-trained LLMs is still under-explored. Here, we compare our methods with common strategies adopted in Liu et al. (2023b); Hou et al. (2023b), including Plain-text, ICL, and COT. Specifically, Plain-text uses historical interactions as sentences to construct the prompt, allowing the LLM to rank candidate items, which include the target items. ICL uses the historical interactions before the second-to-last item as the query sentence, and the second-to-last item as an answer to construct the in-context example. The query is then updated to include historical interactions, prompting the LLM to rank candidate items, including the target items. COT is the zero-shot COT that adds "Let's think step-by-step" at the end of the prompt to guide the LLM in ranking along the intermediate reasoning paths.</p>
<p>Implementation Details. We use FastChat to launch pre-trained language models, including Vicuna-7b-16k, Vicuna-13b-16k, Openchat-7b, Longchat-7b-16k, Longchat-13b-16k, and Yarn-Mistral-7b-128k, which serve as the reranker module in our framework. We also adopt the API-based GPT-Turbo-3.5 as a Reranker for comparison. We randomly sample 200 users from each dataset and use the target items along with 19 randomly sampled items to construct the candidate item set. It is worth noting that we randomly pick the position of the target item in the candidate set to prevent the LLM from recognizing a shortcut to the answer.</p>
<h1>5.1 OVERALL COMPARISON</h1>
<p>In this section, we present a performance comparison between our reasoning principle and existing methods like plain-text, ICL, and COT, using pre-trained large LLMs including Vicuna-7b, Vicuna13b, Openchat-7b, Mistral-7b, Longchat-7b, and GPT-Turbo-3.5. The experimental results are detailed in Table 2. Based on these numerical results, we observe the following:</p>
<p>First, our strategy, which utilizes dynamic reflection and divergent thinking on inference to practice to predict the future, significantly outperforms plain-text, ICL, and COT across all datasets. Our approach successfully enhances the ranking performance of all LLMs on all datasets with respect to almost all metrics. These results underscore the effectiveness of our reasoning strategy. Moreover, our strategy enables a relatively smaller LLM, such as Openchat-7b, to surpass the performance of GPT-Turbo-3.5 with plain-text prompting across the ML-1M, Games, and Luxury datasets in terms of Recall@10, NDCG@10, and NDCG@20 metrics. These findings underscore the significant impact that the choice of reasoning strategy can have on an LLM's performance. With thoughtful design, even a 7-billion parameter LLM can outperform larger models like GPT-Turbo-3.5. This demonstrates the potential for more strategic reasoning design to optimize the performance of LLMs in various tasks, especially in zero/few-shot ranking or when fine-tuning LLMs for recommendation scenarios. We hope our paper will inspire further research in this area, focusing on the development of innovative reasoning strategies tailored to enhance the capabilities of LLMs in recommendation and other complex applications.</p>
<p>Regarding the ICL and COT strategies, we observed that different models show a preference for different prompting strategies across various datasets. For instance, Vicuna-13b using ICL achieved the second-best performance on the ML-1M and Games datasets. However, its performance was worse than the plain-text strategy on the Luxury dataset. A similar pattern is evident with the COT strategy; Vicuna-13b-COT underperformed plain-text on the ML-1M dataset but surpassed it on the Games and Luxury datasets. For Mistral-7b and Longchat-7b, both ICL and COT generally underperformed compared to plain-text, except on the Luxury dataset. These observations suggest that ICL and COT may be more suitable for certain LLMs and specific datasets. Typically, models that already perform reasonably well with plain-text benefit more from ICL. This could be because these LLMs have better-encoded knowledge, and ICL assists in understanding the ranking task and retrieving useful information. For LLMs with better overall performance, such as Openchat and GPT-Turbo-3.5, ICL tends to outperform COT, and COT often performs worse than plain-text. This could be attributed to the highly personalized nature of recommendation tasks, where there may not be a universal reasoning path of COT suitable for all users. Additionally, the input sequences are complex and evolve dynamically, making it challenging for LLMs to analyze user preferences over time without a defined reasoning path. They tend to make recommendations based on similarity comparisons. From another perspective, models with weaker plain-text performance, such as Mistral7 b and Longchat-7b, generally show that both COT and ICL underperform plain-text, except in the case of the Luxury dataset. This trend might stem from these models' challenges in comprehending</p>
<p>Table 2: Overall Comparison.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>ML-1M</th>
<th></th>
<th></th>
<th></th>
<th>Games</th>
<th></th>
<th></th>
<th></th>
<th>Luxury</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R@10</td>
<td>@ R@20</td>
<td>N@10</td>
<td>N@20</td>
<td>R@10</td>
<td>@ R@20</td>
<td>N@10</td>
<td>N@20</td>
<td>R@10</td>
<td>@ R@20</td>
<td>N@10</td>
<td>N@20</td>
</tr>
<tr>
<td>Vicuna-7b</td>
<td>0.3700</td>
<td>0.6400</td>
<td>0.1733</td>
<td>0.2421</td>
<td>0.3700</td>
<td>0.6150</td>
<td>0.1783</td>
<td>0.2405</td>
<td>0.3450</td>
<td>0.4800</td>
<td>0.1534</td>
<td>0.1882</td>
</tr>
<tr>
<td>Vicuna-7b-ICL</td>
<td>0.4450</td>
<td>0.7800</td>
<td>0.2183</td>
<td>0.3024</td>
<td>0.4000</td>
<td>0.5950</td>
<td>0.2102</td>
<td>0.2598</td>
<td>0.3200</td>
<td>0.4500</td>
<td>0.1589</td>
<td>0.1927</td>
</tr>
<tr>
<td>Vicuna-7b-COT</td>
<td>0.3700</td>
<td>0.6850</td>
<td>0.1667</td>
<td>0.2461</td>
<td>0.4100</td>
<td>0.6150</td>
<td>0.1948</td>
<td>0.2467</td>
<td>0.3400</td>
<td>0.3900</td>
<td>0.1562</td>
<td>0.1979</td>
</tr>
<tr>
<td>Vicuna-7b-DRDT</td>
<td>0.5250</td>
<td>0.8450</td>
<td>0.2698</td>
<td>0.3500</td>
<td>0.4300</td>
<td>0.6900</td>
<td>0.2361</td>
<td>0.3023</td>
<td>0.4550</td>
<td>0.6300</td>
<td>0.2418</td>
<td>0.2867</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>17.98</td>
<td>8.33</td>
<td>23.59</td>
<td>15.74</td>
<td>4.88</td>
<td>12.20</td>
<td>12.32</td>
<td>16.36</td>
<td>31.88</td>
<td>26.00</td>
<td>52.17</td>
<td>44.87</td>
</tr>
<tr>
<td>Vicuna-13b</td>
<td>0.5550</td>
<td>0.8000</td>
<td>0.2855</td>
<td>0.3475</td>
<td>0.4400</td>
<td>0.6700</td>
<td>0.2269</td>
<td>0.2855</td>
<td>0.2800</td>
<td>0.4600</td>
<td>0.1525</td>
<td>0.1995</td>
</tr>
<tr>
<td>Vicuna-13b-ICL</td>
<td>0.5450</td>
<td>0.8050</td>
<td>0.2885</td>
<td>0.3541</td>
<td>0.4150</td>
<td>0.6950</td>
<td>0.2293</td>
<td>0.2988</td>
<td>0.2550</td>
<td>0.4100</td>
<td>0.1356</td>
<td>0.1755</td>
</tr>
<tr>
<td>Vicuna-13b-COT</td>
<td>0.4850</td>
<td>0.7600</td>
<td>0.2354</td>
<td>0.3044</td>
<td>0.4400</td>
<td>0.6600</td>
<td>0.2344</td>
<td>0.2895</td>
<td>0.3100</td>
<td>0.3900</td>
<td>0.1563</td>
<td>0.2057</td>
</tr>
<tr>
<td>Vicuna-13b-DRDT</td>
<td>0.5450</td>
<td>0.8150</td>
<td>0.3153</td>
<td>0.3824</td>
<td>0.4500</td>
<td>0.7150</td>
<td>0.2731</td>
<td>0.3399</td>
<td>0.4450</td>
<td>0.6600</td>
<td>0.2608</td>
<td>0.3164</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>-1.80</td>
<td>1.24</td>
<td>9.29</td>
<td>7.99</td>
<td>2.27</td>
<td>2.88</td>
<td>16.51</td>
<td>13.76</td>
<td>43.55</td>
<td>32.00</td>
<td>66.86</td>
<td>53.81</td>
</tr>
<tr>
<td>Openchat-7b</td>
<td>0.4200</td>
<td>0.6650</td>
<td>0.2075</td>
<td>0.2684</td>
<td>0.1450</td>
<td>0.3700</td>
<td>0.0702</td>
<td>0.1264</td>
<td>0.2950</td>
<td>0.5300</td>
<td>0.1341</td>
<td>0.1945</td>
</tr>
<tr>
<td>Openchat-7b-ICL</td>
<td>0.5100</td>
<td>0.8300</td>
<td>0.2734</td>
<td>0.3542</td>
<td>0.4200</td>
<td>0.6900</td>
<td>0.2355</td>
<td>0.3038</td>
<td>0.4300</td>
<td>0.5900</td>
<td>0.2229</td>
<td>0.2642</td>
</tr>
<tr>
<td>Openchat-7b-COT</td>
<td>0.4350</td>
<td>0.6950</td>
<td>0.2209</td>
<td>0.2867</td>
<td>0.2200</td>
<td>0.5050</td>
<td>0.1096</td>
<td>0.1806</td>
<td>0.2300</td>
<td>0.4500</td>
<td>0.1151</td>
<td>0.1705</td>
</tr>
<tr>
<td>Openchat-7b-DRDT</td>
<td>0.6350</td>
<td>0.9000</td>
<td>0.3335</td>
<td>0.4016</td>
<td>0.4800</td>
<td>0.7150</td>
<td>0.3069</td>
<td>0.3651</td>
<td>0.4550</td>
<td>0.6500</td>
<td>0.2589</td>
<td>0.3091</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>24.51</td>
<td>8.43</td>
<td>21.98</td>
<td>13.38</td>
<td>14.28</td>
<td>3.62</td>
<td>30.32</td>
<td>20.18</td>
<td>5.81</td>
<td>10.17</td>
<td>16.15</td>
<td>16.99</td>
</tr>
<tr>
<td>Mistral-7b</td>
<td>0.2350</td>
<td>0.3350</td>
<td>0.1109</td>
<td>0.1362</td>
<td>0.2000</td>
<td>0.2000</td>
<td>0.1094</td>
<td>0.1094</td>
<td>0.1000</td>
<td>0.1100</td>
<td>0.0590</td>
<td>0.0617</td>
</tr>
<tr>
<td>Mistral-7b-ICL</td>
<td>0.1150</td>
<td>0.2000</td>
<td>0.0497</td>
<td>0.0711</td>
<td>0.1700</td>
<td>0.1700</td>
<td>0.1072</td>
<td>0.1072</td>
<td>0.1350</td>
<td>0.1500</td>
<td>0.0995</td>
<td>0.1034</td>
</tr>
<tr>
<td>Mistral-7b-COT</td>
<td>0.1950</td>
<td>0.3050</td>
<td>0.0937</td>
<td>0.1220</td>
<td>0.1750</td>
<td>0.1750</td>
<td>0.0915</td>
<td>0.0915</td>
<td>0.1450</td>
<td>0.1600</td>
<td>0.0991</td>
<td>0.1030</td>
</tr>
<tr>
<td>Mistral-7b-DRDT</td>
<td>0.3750</td>
<td>0.6300</td>
<td>0.1842</td>
<td>0.2489</td>
<td>0.4450</td>
<td>0.5750</td>
<td>0.2462</td>
<td>0.2797</td>
<td>0.4100</td>
<td>0.4950</td>
<td>0.2512</td>
<td>0.2736</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>59.57</td>
<td>88.06</td>
<td>66.10</td>
<td>82.75</td>
<td>122.50</td>
<td>187.50</td>
<td>123.05</td>
<td>155.67</td>
<td>182.76</td>
<td>209.38</td>
<td>152.46</td>
<td>164.60</td>
</tr>
<tr>
<td>Longchat-7b</td>
<td>0.3900</td>
<td>0.6450</td>
<td>0.1808</td>
<td>0.2448</td>
<td>0.3450</td>
<td>0.5700</td>
<td>0.1653</td>
<td>0.2223</td>
<td>0.2350</td>
<td>0.3350</td>
<td>0.0976</td>
<td>0.1230</td>
</tr>
<tr>
<td>Longchat-7b-ICL</td>
<td>0.3250</td>
<td>0.5450</td>
<td>0.1555</td>
<td>0.2114</td>
<td>0.3150</td>
<td>0.5000</td>
<td>0.1523</td>
<td>0.1995</td>
<td>0.2400</td>
<td>0.3000</td>
<td>0.1129</td>
<td>0.1286</td>
</tr>
<tr>
<td>longchat-7b-COT</td>
<td>0.3750</td>
<td>0.5950</td>
<td>0.1739</td>
<td>0.2294</td>
<td>0.2650</td>
<td>0.4950</td>
<td>0.1277</td>
<td>0.1860</td>
<td>0.2600</td>
<td>0.3750</td>
<td>0.1289</td>
<td>0.1592</td>
</tr>
<tr>
<td>Longchat-7b-DRDT</td>
<td>0.4650</td>
<td>0.8400</td>
<td>0.2063</td>
<td>0.3011</td>
<td>0.4150</td>
<td>0.6200</td>
<td>0.2050</td>
<td>0.2566</td>
<td>0.3800</td>
<td>0.5300</td>
<td>0.2047</td>
<td>0.2433</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>19.23</td>
<td>30.23</td>
<td>14.10</td>
<td>23.00</td>
<td>20.29</td>
<td>8.77</td>
<td>24.02</td>
<td>15.43</td>
<td>46.15</td>
<td>41.33</td>
<td>58.81</td>
<td>52.83</td>
</tr>
<tr>
<td>GPT-Turbo-3.5</td>
<td>0.5450</td>
<td>0.9200</td>
<td>0.2688</td>
<td>0.3631</td>
<td>0.4400</td>
<td>0.7450</td>
<td>0.2499</td>
<td>0.3275</td>
<td>0.4350</td>
<td>0.7000</td>
<td>0.2260</td>
<td>0.2924</td>
</tr>
<tr>
<td>GPT-Turbo-3.5-ICL</td>
<td>0.6600</td>
<td>0.9200</td>
<td>0.3829</td>
<td>0.4483</td>
<td>0.5050</td>
<td>0.7100</td>
<td>0.3106</td>
<td>0.3618</td>
<td>0.5150</td>
<td>0.6750</td>
<td>0.3049</td>
<td>0.3464</td>
</tr>
<tr>
<td>GPT-Turbo-3.5-COT</td>
<td>0.5050</td>
<td>0.9000</td>
<td>0.2714</td>
<td>0.3694</td>
<td>0.4400</td>
<td>0.7250</td>
<td>0.2664</td>
<td>0.3371</td>
<td>0.3800</td>
<td>0.6250</td>
<td>0.2030</td>
<td>0.2653</td>
</tr>
<tr>
<td>GPT-Turbo-3.5-DRDT</td>
<td>0.7550</td>
<td>0.9100</td>
<td>0.4630</td>
<td>0.5031</td>
<td>0.5450</td>
<td>0.7300</td>
<td>0.3435</td>
<td>0.3898</td>
<td>0.5600</td>
<td>0.6700</td>
<td>0.3190</td>
<td>0.3482</td>
</tr>
<tr>
<td>Improvement (\%)</td>
<td>14.39</td>
<td>-1.09</td>
<td>20.91</td>
<td>12.22</td>
<td>7.92</td>
<td>-2.01</td>
<td>10.59</td>
<td>7.74</td>
<td>8.74</td>
<td>-4.29</td>
<td>4.62</td>
<td>0.52</td>
</tr>
</tbody>
</table>
<p>the task through in-context examples and their difficulty in autonomously generating a coherent reasoning path. These may inadvertently introduce more noise into the process, rather than providing useful insights. In contrast, our reasoning strategy combines the strengths of both ICL and COT, guiding the LLM's reasoning in a temporal manner to dynamically analyze user preferences. Even with Mistral-7b, DRDT managed to provide reasonable performance with a maximum of 209.38\% improvements on the Luxury dataset. This approach consistently improves performance significantly across different models and datasets.</p>
<h1>5.2 Ablation Study</h1>
<p>In this section, we carry out an ablation study to assess the effectiveness of each module within our framework. Additionally, we report empirical results for Openchat-7b, Vicuna-7b, and GPT-Turbo-3.5 on the ML-1M, Games, and Luxury datasets in Figures 2, 3, and 4 respectively. From these figures, we make several observations:</p>
<p>Importance of CIC, DT, and DR: Collaborative In-Context (CIC), Divergent Thinking (DT), and Dynamic Reflection (DR) are vital for achieving optimal performance. Removing DR leads to a significant drop in performance, and a similar decline is observed when CIC/DT is excluded. This highlights the crucial role these components play in the overall effectiveness of our model.</p>
<p>Impact of CIC combined with DT on Different Datasets: While combining DT with CIC consistently improves performance on the Luxury dataset, it has a lesser or even negative impact on the ML-1M and Games datasets. This suggests that CIC does not always aid LLM reasoning; in some cases, it may introduce additional noise or distract the LLM. This observation underscores the importance of Dynamic Reflection in controlling performance analysis through probing. Without such a regulatory mechanism, the combination of CIC and DT can lead to performance deterioration compared to using either CIC or DT alone.</p>
<p>Effectiveness of DT Combined with DR: The integration of DT with DR consistently enhances performance on the ML-1M and Luxury datasets but has a lesser impact on the Games dataset. This may indicate that collaborative information plays a more significant role compared to the dynamic patterns of user behavior evolution in certain contexts.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of Openchat-7b with different components on three datasets.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of Vicuna-7b with different components on three datasets.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance of GPT-Turbo-3.5 with different components on three datasets.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Recall@10 of three LLMs with different DR steps on three datasets.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: NDCG@10 of three LLMs with different DR steps on three datasets.</p>
<h1>5.3 HyPERPARAMETER SENSITIVITY</h1>
<p>In this section, we delve into the analysis of Recall@10 and NDCG@10 metrics for Openchat-7b, GPT-Turbo-3.5, and Vicuna-7b on three datasets, focusing on the impact of varying the number of dynamic reflection steps. Here, a single step of dynamic reflection is defined as one cycle of prediction probing and analysis reflection using a ground-truth item at a specific position. The number of steps corresponds to the frequency of probing. For instance, a 3-step dynamic reflection involves using the historical sequence $\left{i_{1}, i_{2}, \ldots, i_{s-4}\right}$ for user preference analysis from different aspects, then using item $i_{s-3}$ as the answer for the LLM to reflect on their belief, and so forth.
The numerical results are presented in Figures 5 and 6. Due to constraints such as the context length of the LLMs and the length of user sequences, we have limited our reflection steps to a maximum of 3. Observations from these figures show an increasing trend in performance metrics with the number of reflection steps. This trend underscores the effectiveness of dynamic reflection, where ground-truth items from user behaviors are utilized before making predictions on future engagements.</p>
<h3>5.4 CASE STUDY</h3>
<p>In this section, we compare the ranking lists generated by Openchat-7b using DRDT with other reasoning principles. For reference, we consider the plain-text approach, where the historical sequence is used as the query and the LLM is asked to rerank the candidates directly. In the ICL approach, the sequence before the penultimate item is used as the query, guiding the LLM to recommend the second-to-last item to construct the in-context example. The sequence is then updated to include the penultimate item for reranking the candidates. Additionally, we apply zero-shot COT for comparison, given the lack of a universal reasoning path for personalized preferences, and allow the LLM to autonomously determine the reasoning path. We report the results on Figure7. Our observations from comparing the outputs are as follows:</p>
<p>1), The superiority of DRDT: The DRDT effectively improved the ranking of the ground-truth item Judge Dredd to the first position, surpassing other prompting strategies. This is achieved by posing</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Case Study
specific questions to the LLM, enabling it to retrieve more relevant information about the items for recommendation. Additionally, the LLM incorporates more aspects into the user preference analysis through the probe predictions, aiding the final reranking process. 2), Limitations of Plain-Text and COT: Both plain text and COT strategies struggled to accurately understand the task of reranking candidate items. They tend to regenerate historical interactions, as these are the semantically most similar items. COT, in particular, failed to develop a coherent reasoning path, defaulting to retrieving items similar to those in the history, rather than effectively reranking the candidate items. 3), Effectiveness of ICL: ICL helped the LLM understand the task more effectively. The items generated are mostly from the candidate set, and the target items are ranked higher. This indicates that while ICL assists in task comprehension, it may not fully capture the nuances required for optimal reranking in personalized recommendation scenarios. These insights highlight the strengths and limitations of various prompting strategies and underscore the effectiveness of DRDT in enhancing the LLM's performance in sequential recommendation tasks.</p>
<h1>6 CONCLUSION</h1>
<p>In this paper, we explore the significant effects of different prompting strategies on the reasoning capacity of Large Language Models (LLMs) for sequential recommendation tasks. We propose a Retriever-Reranker framework and instantiate the retriever with a collaborative in-context demonstration retriever that collects collaborative sequences from the dataset to facilitate recommendations. We introduce divergent thinking, which abstracts high-level user preferences from multiple aspects, and dynamic reflection that mimics the human learning procedure, including a probing-critiquingreflecting iteration on user preference analysis to encourage the LLM to think in a temporal manner. This reasoning principle successfully improves the LLM's ranking performance substantially without the need for fine-tuning the LLM. We hope this paper could inspire the investigation of maximizing the reasoning capacity of existing LLM for sequential recommendation scenarios.</p>
<h2>REFERENCES</h2>
<p>K. Bao, J. Zhang, W. Wang, Y. Zhang, Z. Yang, Y. Luo, F. Feng, X. He, and Q. Tian. A bi-step grounding paradigm for large language models in recommendation systems. arXiv preprint arXiv:2308.08434, 2023a.
K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447,</p>
<p>2023b.
Z. Chen. Palr: Personalization aware llms for recommendation. arXiv preprint arXiv:2305.07622, 2023.
Z. Cui, J. Ma, C. Zhou, J. Zhou, and H. Yang. M6-rec: Generative pretrained language models are open-ended recommender systems. arXiv preprint arXiv:2205.08084, 2022.
Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524, 2023.
S. Geng, S. Liu, Z. Fu, Y. Ge, and Y. Zhang. Recommendation as language processing (rlp): A unified pretrain, personalized prompt \&amp; predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems, pages 299-315, 2022.
J. Harte, W. Zorgdrager, P. Louridas, A. Katsifodimos, D. Jannach, and M. Fragkoulis. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1096-1102, 2023.
H. He, H. Zhang, and D. Roth. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303, 2022.
Y. Hou, S. Mu, W. X. Zhao, Y. Li, B. Ding, and J.-R. Wen. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 585-593, 2022.
Y. Hou, Z. He, J. McAuley, and W. X. Zhao. Learning vector-quantized item representation for transferable sequential recommenders. In Proceedings of the ACM Web Conference 2023, pages $1162-1171,2023 a$.
Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. McAuley, and W. X. Zhao. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845, 2023b.
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.
W. Kang and J. J. McAuley. Self-attentive sequential recommendation. In IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018, pages 197-206. IEEE Computer Society, 2018.
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.
J. Li, M. Wang, J. Li, J. Fu, X. Shen, J. Shang, and J. McAuley. Text is all you need: Learning language representations for sequential recommendation. arXiv preprint arXiv:2305.13731, 2023a.
Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333, 2023b.
J. Lin, R. Shan, C. Zhu, K. Du, B. Chen, S. Quan, R. Tang, Y. Yu, and W. Zhang. Rella: Retrievalenhanced large language models for lifelong sequential behavior comprehension in recommendation. arXiv preprint arXiv:2308.11131, 2023.
Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and H. Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.
J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149, 2023a.</p>
<p>J. Liu, C. Liu, P. Zhou, Q. Ye, D. Chong, K. Zhou, Y. Xie, Y. Cao, S. Wang, C. You, et al. Llmrec: Benchmarking large language models on recommendation task. arXiv preprint arXiv:2308.12241, 2023b.
A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
A. Martino, M. Iannelli, and C. Truong. Knowledge injection to counter large language model (llm) hallucination. In European Semantic Web Conference, pages 182-185. Springer, 2023.
R. Qiu, Z. Huang, H. Yin, and Z. Wang. Contrastive learning for representation degeneration problem in sequential recommendation. In WSDM '22: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022, pages 813-823. ACM, 2022.
S. Sanner, K. Balog, F. Radlinski, B. Wedin, and L. Dixon. Large language models are competitive near cold-start recommenders for language-and item-based preferences. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 890-896, 2023.
N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019, pages 1441-1450. ACM, 2019.
H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Interleaving retrieval with chain-ofthought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022.
K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.
L. Wang and E.-P. Lim. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153, 2023.
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a.
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.
Y. Wang, H. Zhang, Z. Liu, L. Yang, and P. S. Yu. Contrastvae: Contrastive variational autoencoder for sequential recommendation. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management, pages 2056-2066, 2022c.
Y. Wang, Z. Chu, X. Ouyang, S. Wang, H. Hao, Y. Shen, J. Gu, S. Xue, J. Y. Zhang, Q. Cui, et al. Enhancing recommender systems with large language model reasoning graphs. arXiv preprint arXiv:2308.10835, 2023a.
Y. Wang, Z. Jiang, Z. Chen, F. Yang, Y. Zhou, E. Cho, X. Fan, X. Huang, Y. Lu, and Y. Yang. Recmind: Large language model powered agent for recommendation. arXiv preprint arXiv:2308.14296, 2023b.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
Y. Weng, M. Zhu, S. He, K. Liu, and J. Zhao. Large language models are reasoners with selfverification. arXiv preprint arXiv:2212.09561, 2022.</p>
<p>L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen. Exploring large language model for graph data understanding in online job recommendations. arXiv preprint arXiv:2307.05722, 2023.
J.-Y. Yao, K.-P. Ning, Z.-H. Liu, M.-N. Ning, and L. Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023.
Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.
Z. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding Author: zhiweiliu@salesforce.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
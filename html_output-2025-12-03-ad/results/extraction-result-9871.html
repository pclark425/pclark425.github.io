<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735" target="_blank">Benchmarking Large Language Models As AI Research Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes MLAgentBench, a suite of ML tasks for benchmarking AI research agents, and designs an LLM-based research agent to automatically perform experimentation loops in such an environment, finding that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and environment introduced in this paper for evaluating LM-driven agents on end-to-end machine learning experimentation tasks; it supplies 13 tasks, starter files, a task-independent action API, and per-task evaluators and records full interaction traces for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini Pro, Mixtral, Claude v1.0, Claude v2.1</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Various proprietary and open LLMs evaluated as the decision-making core of agents; models differ in API speed, prompting behavior and generation quality (as described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / Automation of ML experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated per-task evaluators compute a final performance metric from the agent's submission or produced checkpoint; the benchmark records full interaction traces (actions, rationales, observations, workspace snapshots) and aggregates run-level outcomes (success rate, metric improvement) and efficiency measures (tokens, wall-clock time).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary competence criterion: a run is a success if the final performance metric improves >=10% relative to the baseline in the starter code. Efficiency criteria: total wall-clock time and total LM input+output tokens consumed. Additional trace-based analyses (error taxonomy, intermediate metric trajectories) are used for qualitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>A suite of 13 tasks across modalities and problem types (Table 2): canonical (CIFAR-10, imdb, ogbn-arxiv), classic Kaggle (house-price, spaceship-titanic), Kaggle challenges (four recent competitions), recent research (CLRS, BabyLM), and code-improvement tasks (llama-inference, vectorization); each task has a specified evaluator and metric (e.g., classification accuracy, MAE, SMAPE, MAP@20, MCRMSE, Dice coefficient, MSE, perplexity, wall-clock time).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Across 8 runs per LM-agent, Claude v3 Opus-based agent achieved highest average success rate (37.5%). Success rates vary dramatically by task (100% on house-price across 8 runs; 0% on BabyLM). GPT-4 agents produced larger average metric improvements in some tasks but were more prone to hallucination. LangChain/AutoGPT adaptations underperformed relative to the paper's agent in most settings (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Large variance across tasks and runs; agents struggle on recent or out-of-distribution datasets probably unseen during LM pretraining; failure modes include hallucination (claiming improvements without execution), bad plans, response-format errors, and submission-format errors; long-horizon planning and debugging complex edits are failure points.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>The benchmark emphasizes interpretability (research plans and trace logs) to enable human oversight and augmentation. There is no direct head-to-head comparison against human experimenters in this paper; rather, comparison is across agent designs and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Record full interaction traces and require machine-verifiable evaluators; define explicit baseline runs and a concrete numerical success threshold (>=10% improvement used here); constrain expensive operations (e.g., train epoch limits); include interpretability affordances (Research Plan & Status, Fact Check entries) and strict response formats to reduce parsing/submission errors; measure both competence and efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Success criterion (>=10%)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Success definition: >=10% improvement over baseline performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational success used by MLAgentBench: a final submission is considered successful if the task-specific performance metric improves by at least 10% relative to the baseline implemented in the starter code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>various (agents based on the evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM itself; applied to runs of agents driven by different LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning evaluation / benchmark methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute task metric via the environment's evaluator on the final workspace/submission; compare final metric value to baseline metric value; mark success if relative improvement >=10%. Aggregate over multiple runs to compute success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative improvement percentage over baseline; valid submissions required (parser- and format-correct). Runs without valid submissions are excluded from per-submission average-improvement statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across all MLAgentBench tasks (the 13 tasks listed in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used to produce the success-rate matrices in Table 3. This threshold produces coarse binary labeling of competence across model families; average success rates ranged from 0% to 100% depending on task and model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Arbitrary threshold; percent-improvement is undefined/unstable for tasks without reasonable baseline (paper notes special-casing for tasks without baseline); a single binary cutoff obscures smaller useful improvements and can be sensitive to baseline quality and metric scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike human expert judgment that might weigh qualitative improvements, MLAgentBench enforces a strict numerical threshold to enable automatic evaluation and repeatability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For automated evaluation of agent-driven research, pair a clear numeric success threshold with clear baseline definitions and ensure baseline code runs; for tasks lacking baselines, report absolute metric changes and use supplementary qualitative trace review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9871.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Efficiency metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficiency measurement: LM tokens and wall-clock time</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative efficiency measures tracked per run: total LM input+output tokens consumed and total wall-clock time spent by the agent during a task run; used to assess cost-effectiveness and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4-turbo (noted as most token-efficient), GPT-4, Claude family (slower/more tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Efficiency characteristics differ: GPT-4-turbo consumed ~51% fewer tokens on average versus the mean agent, while Claude v3 Opus used near the most tokens and wall-clock time, attributed to slower API and longer ML experiment runtimes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML systems benchmarking / cost and efficiency analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sum prompt + completion tokens across all LM calls during a run; measure total wall-clock time from run start to finish; convert tokens/time to monetary cost using API prices for approximate cost per successful run.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lower token usage and lower wall-clock time are better; efficiency tradeoffs considered alongside success rate to compute expected cost to achieve a task (paper cites expected cost estimate of ~$231 given mean success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Computed across MLAgentBench tasks and runs; per-task breakdown in appendix figures.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4-turbo agents were most efficient in tokens and also achieved competitive success rates, making them cost-effective. Claude v3 Opus had higher success rate but higher token/time cost. Total benchmark run with GPT-4-turbo used ~6M tokens (~$60 at the time), showing per-run cost is small but expected cost per successful task grows with low success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Token counts dominated by prompt tokens that are reused across steps, complicating marginal-cost reasoning; wall-clock time depends strongly on the ML experiments themselves (not just LM latency).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Traditional human-driven experimentation measures researcher time differently (expert hours); token/time metrics allow direct cost estimation of automated agents but do not capture human cognitive costs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Track both tokens and wall-clock time, report cost-per-success (expected cost given success rate), and optimize both model choice and prompt/context length to balance competence and expense.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9871.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task suite (13 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLAgentBench 13-task suite across modalities and recency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated set of 13 executable ML tasks spanning image, text, graph, tabular, time-series, segmentation and code-speed optimization, chosen to vary in difficulty and dataset recency to probe generalization and contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>agents driven by the evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>N/A (benchmarks are LLM-agnostic tasks used to evaluate agent performance).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning (multiple subdomains including image classification, NLP, graph ML, regression, segmentation, language modeling, and system optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Per-task automated evaluators return a task-specific metric (e.g., accuracy, MAE, SMAPE, MAP@20, MCRMSE, Dice, MSE, perplexity, wall-clock time) from the agent's final submission or saved checkpoint; these metrics are compared to baselines to determine success and improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific metrics as listed in Table 2; success judged by >=10% improvement where baseline exists; for code improvement tasks, wall-clock time is minimized.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Canonical (CIFAR-10 accuracy, imdb accuracy, ogbn-arxiv accuracy), Classic Kaggle (house-price MAE, spaceship-titanic accuracy), Kaggle Challenges (parkinsons-disease SMAPE, fathomnet MAP@20, feedback MCRMSE, identify-contrails Dice), Recent Research (CLRS MSE, BabyLM perplexity), Code Improvement (llama-inference and vectorization wall-clock).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performance varied drastically across tasks and LLMs: high success on older/well-understood tasks (e.g., house-price 100% for Claude v3 Opus) and near-zero success on challenging/recent tasks (e.g., BabyLM 0% across models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Newer datasets (post-LM-pretraining) are especially challenging; tasks with complex training pipelines (e.g., BabyLM language-model pretraining) often timed out or failed; tasks without robust baselines make relative improvement ill-defined.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>The benchmark automates evaluation via standard ML metrics rather than human judgement; this enables repeatable, scalable comparison but can miss nuanced or qualitative research contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include a mix of canonical, classic, and recent tasks to probe generalization and contamination; provide runnable starter code and clear submission formats; include explicit metric definitions and baseline implementations where possible.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9871.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent design: Research Plan & Fact Check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-based agent design with explicit Research Plan/Status and Fact Check entries (plus ReAct/Reflexion-inspired reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent prompts enforce a structured response format: Reflection, Research Plan and Status, Fact Check, Thought, then Action and Action Input; Research Plan/Status provides interpretable stepwise plans and progress tracking, while Fact Check reduces hallucinated claims about unexecuted experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>applied across evaluated LLMs (Claude family, GPT-4 family, Gemini Pro, Mixtral)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>The LM provides the structured responses; specific performance varies by model generation and family.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Agent design / human-interpretable scientific process automation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative trace analysis (e.g., categorizing failure modes from CIFAR-10 traces) and comparing error rates (hallucination, bad plan, response format error, submission format error) across agent variants and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Reduction in hallucinations and bad plans, increased interpretability of stepwise plans, and downstream effect on success rate and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on MLAgentBench tasks; qualitative examples and errors analyzed primarily on CIFAR-10 runs (Figure 5 and Appendix examples).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The Research Plan and Fact Check entries improved interpretability and helped reduce some hallucinations and planning errors compared to simpler ReAct-style baselines; agents with this structure (especially with Claude v3 Opus) achieved higher success rates on several tasks and produced more human-interpretable traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fact Check and Research Plan do not eliminate hallucination; agents still sometimes fabricate results or assert outcomes without execution. Reflection can lead to excessive internal dialogue at the cost of doing actions; complex edits can cause the agent to become stuck in debugging (40% of Claude v1.0 runs got stuck in debugging per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These design elements are intended to make automated research traces closer to human lab notebooks / experiment logs, enabling human oversight and intervention; no direct user-study comparison presented yet.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Require structured LM outputs that separate plan/status/fact-check from actions; log and expose Research Plan and Fact Check to enable human review; enforce execution before confirming experimental outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9871.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical results (tables)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluation results and comparative tables (success rates, average improvements, agent-framework comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative outcomes reported in the paper: per-task success rates (Table 3), average percent improvements when valid submissions are made (Table 4), and comparison across agent frameworks (Table 5) showing relative strengths/weaknesses of LLMs and designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Claude v3 Opus, GPT-4, GPT-4-turbo, Claude v1.0, Claude v2.1, Gemini Pro, Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Different LLMs produce different tradeoffs: Claude v3 Opus highest average success rate (37.5%), GPT-4 produced larger average metric improvements in successful submissions but had different failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate across 8 independent runs per model-agent per task (with action/time limits), compute success rates (>=10% improvement), and compute average percent improvement among valid submissions; compare agent framework adaptations (Ours vs AutoGPT vs LangChain).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Success rate (binary per run) and average percent metric improvement among valid submissions; token/time consumed as efficiency criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the MLAgentBench 13 tasks; detailed per-task numbers in Tables 3-5 and figures illustrating intermediate trajectories and token/time breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key numbers: Claude v3 Opus average success rate 37.5%; GPT-4 average success rate ~19.2% but higher average percent improvement on successful submissions; GPT-4-turbo was most token-efficient and had competitive success rate (26%); LangChain with Claude v3 performed competitively on some tasks. Results emphasize large inter-task variance (0-100% success depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Averaging can obscure task-specific behavior; small number of runs (8) yields noisy estimates; cost constraints limited GPT-4 runs to fewer actions in some experiments; some task categories (e.g., BabyLM) are effectively unsolved by current agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No direct human baselines reported; comparisons are across agent designs and LLMs only.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both success rates and continuous improvement statistics; show per-task breakdowns and intermediate metric trajectories to reveal regressions over longer runs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9871.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9871.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent failure-mode taxonomy (Hallucination, Bad Plan, Response Format Error, Submission Format Error, Small Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Categorization of major failure modes observed in agent runs (especially on CIFAR-10): hallucination of results, inadequate/bad experimental plans, invalid JSON/response parsing errors, incorrect submission formats, and runs producing improvements smaller than the success threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>observed across models, with GPT-4 relatively more prone to hallucination vs Claude v3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Failure mode prevalence depends on LLM prompting and model family; qualitative differences noted between families in error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology / agent analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual and automated trace analysis categorizing final-run traces into the taxonomy; quantitative tallies reported for CIFAR-10 (Figure 5) and some per-model statistics in text (e.g., 40% of Claude v1.0 runs got stuck debugging; ~20% hallucination example rate cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Count and fraction of runs falling into each error category; inspect traces for evidence (e.g., no execution but claimed improvements indicates hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Derived from MLAgentBench runs; primary qualitative analysis on CIFAR-10 traces.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Common failure modes: hallucination (claiming unexecuted improvements), bad plans in initial steps that are hard to recover from, and format errors that produce invalid submissions. GPT-4-based agents were reported to hallucinate and plan poorly more often than the best Claude v3 Opus agent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some failures stem from problem misspecification (e.g., misunderstanding whether lower or higher metric is better), ambiguous instructions, or starter-code issues; Fact Check and Research Plan reduced but did not eliminate hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Humans also produce poor experimental plans or misinterpret metrics, but the benchmark can automatically detect many machine-specific failure modes (like invalid JSON submissions) that are less relevant to human review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include Fact Check entries and require execution-confirmed performance numbers; make metric directionality explicit in task descriptions; enforce strict response and submission formats to catch parsing/format errors early.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>AgentBench: Evaluating LLMs as agents <em>(Rating: 2)</em></li>
                <li>WebArena: A realistic web environment for building autonomous agents <em>(Rating: 1)</em></li>
                <li>Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous <em>(Rating: 1)</em></li>
                <li>AutoML-GPT: Automatic machine learning with gpt <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9871",
    "paper_id": "paper-c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "MLAgentBench",
            "name_full": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
            "brief_description": "A benchmark and environment introduced in this paper for evaluating LM-driven agents on end-to-end machine learning experimentation tasks; it supplies 13 tasks, starter files, a task-independent action API, and per-task evaluators and records full interaction traces for analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini Pro, Mixtral, Claude v1.0, Claude v2.1",
            "llm_description": "Various proprietary and open LLMs evaluated as the decision-making core of agents; models differ in API speed, prompting behavior and generation quality (as described in paper).",
            "scientific_domain": "Machine Learning / Automation of ML experimentation",
            "evaluation_method": "Automated per-task evaluators compute a final performance metric from the agent's submission or produced checkpoint; the benchmark records full interaction traces (actions, rationales, observations, workspace snapshots) and aggregates run-level outcomes (success rate, metric improvement) and efficiency measures (tokens, wall-clock time).",
            "evaluation_criteria": "Primary competence criterion: a run is a success if the final performance metric improves &gt;=10% relative to the baseline in the starter code. Efficiency criteria: total wall-clock time and total LM input+output tokens consumed. Additional trace-based analyses (error taxonomy, intermediate metric trajectories) are used for qualitative evaluation.",
            "benchmark_or_dataset": "A suite of 13 tasks across modalities and problem types (Table 2): canonical (CIFAR-10, imdb, ogbn-arxiv), classic Kaggle (house-price, spaceship-titanic), Kaggle challenges (four recent competitions), recent research (CLRS, BabyLM), and code-improvement tasks (llama-inference, vectorization); each task has a specified evaluator and metric (e.g., classification accuracy, MAE, SMAPE, MAP@20, MCRMSE, Dice coefficient, MSE, perplexity, wall-clock time).",
            "results_summary": "Across 8 runs per LM-agent, Claude v3 Opus-based agent achieved highest average success rate (37.5%). Success rates vary dramatically by task (100% on house-price across 8 runs; 0% on BabyLM). GPT-4 agents produced larger average metric improvements in some tasks but were more prone to hallucination. LangChain/AutoGPT adaptations underperformed relative to the paper's agent in most settings (Table 5).",
            "limitations_or_challenges": "Large variance across tasks and runs; agents struggle on recent or out-of-distribution datasets probably unseen during LM pretraining; failure modes include hallucination (claiming improvements without execution), bad plans, response-format errors, and submission-format errors; long-horizon planning and debugging complex edits are failure points.",
            "comparison_to_human_or_traditional": "The benchmark emphasizes interpretability (research plans and trace logs) to enable human oversight and augmentation. There is no direct head-to-head comparison against human experimenters in this paper; rather, comparison is across agent designs and LLMs.",
            "recommendations_or_best_practices": "Record full interaction traces and require machine-verifiable evaluators; define explicit baseline runs and a concrete numerical success threshold (&gt;=10% improvement used here); constrain expensive operations (e.g., train epoch limits); include interpretability affordances (Research Plan & Status, Fact Check entries) and strict response formats to reduce parsing/submission errors; measure both competence and efficiency.",
            "uuid": "e9871.0"
        },
        {
            "name_short": "Success criterion (&gt;=10%)",
            "name_full": "Success definition: &gt;=10% improvement over baseline performance",
            "brief_description": "Operational success used by MLAgentBench: a final submission is considered successful if the task-specific performance metric improves by at least 10% relative to the baseline implemented in the starter code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "various (agents based on the evaluated LLMs)",
            "llm_description": "Not an LLM itself; applied to runs of agents driven by different LLMs.",
            "scientific_domain": "Machine Learning evaluation / benchmark methodology",
            "evaluation_method": "Compute task metric via the environment's evaluator on the final workspace/submission; compare final metric value to baseline metric value; mark success if relative improvement &gt;=10%. Aggregate over multiple runs to compute success rate.",
            "evaluation_criteria": "Relative improvement percentage over baseline; valid submissions required (parser- and format-correct). Runs without valid submissions are excluded from per-submission average-improvement statistics.",
            "benchmark_or_dataset": "Applied across all MLAgentBench tasks (the 13 tasks listed in Table 2).",
            "results_summary": "Used to produce the success-rate matrices in Table 3. This threshold produces coarse binary labeling of competence across model families; average success rates ranged from 0% to 100% depending on task and model.",
            "limitations_or_challenges": "Arbitrary threshold; percent-improvement is undefined/unstable for tasks without reasonable baseline (paper notes special-casing for tasks without baseline); a single binary cutoff obscures smaller useful improvements and can be sensitive to baseline quality and metric scale.",
            "comparison_to_human_or_traditional": "Unlike human expert judgment that might weigh qualitative improvements, MLAgentBench enforces a strict numerical threshold to enable automatic evaluation and repeatability.",
            "recommendations_or_best_practices": "For automated evaluation of agent-driven research, pair a clear numeric success threshold with clear baseline definitions and ensure baseline code runs; for tasks lacking baselines, report absolute metric changes and use supplementary qualitative trace review.",
            "uuid": "e9871.1"
        },
        {
            "name_short": "Efficiency metrics",
            "name_full": "Efficiency measurement: LM tokens and wall-clock time",
            "brief_description": "Quantitative efficiency measures tracked per run: total LM input+output tokens consumed and total wall-clock time spent by the agent during a task run; used to assess cost-effectiveness and scalability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4-turbo (noted as most token-efficient), GPT-4, Claude family (slower/more tokens)",
            "llm_description": "Efficiency characteristics differ: GPT-4-turbo consumed ~51% fewer tokens on average versus the mean agent, while Claude v3 Opus used near the most tokens and wall-clock time, attributed to slower API and longer ML experiment runtimes.",
            "scientific_domain": "ML systems benchmarking / cost and efficiency analysis",
            "evaluation_method": "Sum prompt + completion tokens across all LM calls during a run; measure total wall-clock time from run start to finish; convert tokens/time to monetary cost using API prices for approximate cost per successful run.",
            "evaluation_criteria": "Lower token usage and lower wall-clock time are better; efficiency tradeoffs considered alongside success rate to compute expected cost to achieve a task (paper cites expected cost estimate of ~$231 given mean success rate).",
            "benchmark_or_dataset": "Computed across MLAgentBench tasks and runs; per-task breakdown in appendix figures.",
            "results_summary": "GPT-4-turbo agents were most efficient in tokens and also achieved competitive success rates, making them cost-effective. Claude v3 Opus had higher success rate but higher token/time cost. Total benchmark run with GPT-4-turbo used ~6M tokens (~$60 at the time), showing per-run cost is small but expected cost per successful task grows with low success rates.",
            "limitations_or_challenges": "Token counts dominated by prompt tokens that are reused across steps, complicating marginal-cost reasoning; wall-clock time depends strongly on the ML experiments themselves (not just LM latency).",
            "comparison_to_human_or_traditional": "Traditional human-driven experimentation measures researcher time differently (expert hours); token/time metrics allow direct cost estimation of automated agents but do not capture human cognitive costs.",
            "recommendations_or_best_practices": "Track both tokens and wall-clock time, report cost-per-success (expected cost given success rate), and optimize both model choice and prompt/context length to balance competence and expense.",
            "uuid": "e9871.2"
        },
        {
            "name_short": "Task suite (13 tasks)",
            "name_full": "MLAgentBench 13-task suite across modalities and recency",
            "brief_description": "A curated set of 13 executable ML tasks spanning image, text, graph, tabular, time-series, segmentation and code-speed optimization, chosen to vary in difficulty and dataset recency to probe generalization and contamination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "agents driven by the evaluated LLMs",
            "llm_description": "N/A (benchmarks are LLM-agnostic tasks used to evaluate agent performance).",
            "scientific_domain": "Machine Learning (multiple subdomains including image classification, NLP, graph ML, regression, segmentation, language modeling, and system optimization)",
            "evaluation_method": "Per-task automated evaluators return a task-specific metric (e.g., accuracy, MAE, SMAPE, MAP@20, MCRMSE, Dice, MSE, perplexity, wall-clock time) from the agent's final submission or saved checkpoint; these metrics are compared to baselines to determine success and improvement.",
            "evaluation_criteria": "Task-specific metrics as listed in Table 2; success judged by &gt;=10% improvement where baseline exists; for code improvement tasks, wall-clock time is minimized.",
            "benchmark_or_dataset": "Canonical (CIFAR-10 accuracy, imdb accuracy, ogbn-arxiv accuracy), Classic Kaggle (house-price MAE, spaceship-titanic accuracy), Kaggle Challenges (parkinsons-disease SMAPE, fathomnet MAP@20, feedback MCRMSE, identify-contrails Dice), Recent Research (CLRS MSE, BabyLM perplexity), Code Improvement (llama-inference and vectorization wall-clock).",
            "results_summary": "Performance varied drastically across tasks and LLMs: high success on older/well-understood tasks (e.g., house-price 100% for Claude v3 Opus) and near-zero success on challenging/recent tasks (e.g., BabyLM 0% across models).",
            "limitations_or_challenges": "Newer datasets (post-LM-pretraining) are especially challenging; tasks with complex training pipelines (e.g., BabyLM language-model pretraining) often timed out or failed; tasks without robust baselines make relative improvement ill-defined.",
            "comparison_to_human_or_traditional": "The benchmark automates evaluation via standard ML metrics rather than human judgement; this enables repeatable, scalable comparison but can miss nuanced or qualitative research contributions.",
            "recommendations_or_best_practices": "Include a mix of canonical, classic, and recent tasks to probe generalization and contamination; provide runnable starter code and clear submission formats; include explicit metric definitions and baseline implementations where possible.",
            "uuid": "e9871.3"
        },
        {
            "name_short": "Agent design: Research Plan & Fact Check",
            "name_full": "LM-based agent design with explicit Research Plan/Status and Fact Check entries (plus ReAct/Reflexion-inspired reasoning)",
            "brief_description": "Agent prompts enforce a structured response format: Reflection, Research Plan and Status, Fact Check, Thought, then Action and Action Input; Research Plan/Status provides interpretable stepwise plans and progress tracking, while Fact Check reduces hallucinated claims about unexecuted experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "applied across evaluated LLMs (Claude family, GPT-4 family, Gemini Pro, Mixtral)",
            "llm_description": "The LM provides the structured responses; specific performance varies by model generation and family.",
            "scientific_domain": "Agent design / human-interpretable scientific process automation",
            "evaluation_method": "Qualitative trace analysis (e.g., categorizing failure modes from CIFAR-10 traces) and comparing error rates (hallucination, bad plan, response format error, submission format error) across agent variants and LLMs.",
            "evaluation_criteria": "Reduction in hallucinations and bad plans, increased interpretability of stepwise plans, and downstream effect on success rate and efficiency.",
            "benchmark_or_dataset": "Evaluated on MLAgentBench tasks; qualitative examples and errors analyzed primarily on CIFAR-10 runs (Figure 5 and Appendix examples).",
            "results_summary": "The Research Plan and Fact Check entries improved interpretability and helped reduce some hallucinations and planning errors compared to simpler ReAct-style baselines; agents with this structure (especially with Claude v3 Opus) achieved higher success rates on several tasks and produced more human-interpretable traces.",
            "limitations_or_challenges": "Fact Check and Research Plan do not eliminate hallucination; agents still sometimes fabricate results or assert outcomes without execution. Reflection can lead to excessive internal dialogue at the cost of doing actions; complex edits can cause the agent to become stuck in debugging (40% of Claude v1.0 runs got stuck in debugging per paper).",
            "comparison_to_human_or_traditional": "These design elements are intended to make automated research traces closer to human lab notebooks / experiment logs, enabling human oversight and intervention; no direct user-study comparison presented yet.",
            "recommendations_or_best_practices": "Require structured LM outputs that separate plan/status/fact-check from actions; log and expose Research Plan and Fact Check to enable human review; enforce execution before confirming experimental outcomes.",
            "uuid": "e9871.4"
        },
        {
            "name_short": "Empirical results (tables)",
            "name_full": "Empirical evaluation results and comparative tables (success rates, average improvements, agent-framework comparison)",
            "brief_description": "Quantitative outcomes reported in the paper: per-task success rates (Table 3), average percent improvements when valid submissions are made (Table 4), and comparison across agent frameworks (Table 5) showing relative strengths/weaknesses of LLMs and designs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Claude v3 Opus, GPT-4, GPT-4-turbo, Claude v1.0, Claude v2.1, Gemini Pro, Mixtral",
            "llm_description": "Different LLMs produce different tradeoffs: Claude v3 Opus highest average success rate (37.5%), GPT-4 produced larger average metric improvements in successful submissions but had different failure modes.",
            "scientific_domain": "Machine Learning benchmarking",
            "evaluation_method": "Aggregate across 8 independent runs per model-agent per task (with action/time limits), compute success rates (&gt;=10% improvement), and compute average percent improvement among valid submissions; compare agent framework adaptations (Ours vs AutoGPT vs LangChain).",
            "evaluation_criteria": "Success rate (binary per run) and average percent metric improvement among valid submissions; token/time consumed as efficiency criteria.",
            "benchmark_or_dataset": "Applied to the MLAgentBench 13 tasks; detailed per-task numbers in Tables 3-5 and figures illustrating intermediate trajectories and token/time breakdowns.",
            "results_summary": "Key numbers: Claude v3 Opus average success rate 37.5%; GPT-4 average success rate ~19.2% but higher average percent improvement on successful submissions; GPT-4-turbo was most token-efficient and had competitive success rate (26%); LangChain with Claude v3 performed competitively on some tasks. Results emphasize large inter-task variance (0-100% success depending on task).",
            "limitations_or_challenges": "Averaging can obscure task-specific behavior; small number of runs (8) yields noisy estimates; cost constraints limited GPT-4 runs to fewer actions in some experiments; some task categories (e.g., BabyLM) are effectively unsolved by current agents.",
            "comparison_to_human_or_traditional": "No direct human baselines reported; comparisons are across agent designs and LLMs only.",
            "recommendations_or_best_practices": "Report both success rates and continuous improvement statistics; show per-task breakdowns and intermediate metric trajectories to reveal regressions over longer runs.",
            "uuid": "e9871.5"
        },
        {
            "name_short": "Error taxonomy",
            "name_full": "Agent failure-mode taxonomy (Hallucination, Bad Plan, Response Format Error, Submission Format Error, Small Improvement)",
            "brief_description": "Categorization of major failure modes observed in agent runs (especially on CIFAR-10): hallucination of results, inadequate/bad experimental plans, invalid JSON/response parsing errors, incorrect submission formats, and runs producing improvements smaller than the success threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "observed across models, with GPT-4 relatively more prone to hallucination vs Claude v3 Opus",
            "llm_description": "Failure mode prevalence depends on LLM prompting and model family; qualitative differences noted between families in error rates.",
            "scientific_domain": "Evaluation methodology / agent analysis",
            "evaluation_method": "Manual and automated trace analysis categorizing final-run traces into the taxonomy; quantitative tallies reported for CIFAR-10 (Figure 5) and some per-model statistics in text (e.g., 40% of Claude v1.0 runs got stuck debugging; ~20% hallucination example rate cited).",
            "evaluation_criteria": "Count and fraction of runs falling into each error category; inspect traces for evidence (e.g., no execution but claimed improvements indicates hallucination).",
            "benchmark_or_dataset": "Derived from MLAgentBench runs; primary qualitative analysis on CIFAR-10 traces.",
            "results_summary": "Common failure modes: hallucination (claiming unexecuted improvements), bad plans in initial steps that are hard to recover from, and format errors that produce invalid submissions. GPT-4-based agents were reported to hallucinate and plan poorly more often than the best Claude v3 Opus agent.",
            "limitations_or_challenges": "Some failures stem from problem misspecification (e.g., misunderstanding whether lower or higher metric is better), ambiguous instructions, or starter-code issues; Fact Check and Research Plan reduced but did not eliminate hallucination.",
            "comparison_to_human_or_traditional": "Humans also produce poor experimental plans or misinterpret metrics, but the benchmark can automatically detect many machine-specific failure modes (like invalid JSON submissions) that are less relevant to human review.",
            "recommendations_or_best_practices": "Include Fact Check entries and require execution-confirmed performance numbers; make metric directionality explicit in task descriptions; enforce strict response and submission formats to catch parsing/format errors early.",
            "uuid": "e9871.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "AgentBench: Evaluating LLMs as agents",
            "rating": 2
        },
        {
            "paper_title": "WebArena: A realistic web environment for building autonomous agents",
            "rating": 1
        },
        {
            "paper_title": "Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous",
            "rating": 1
        },
        {
            "paper_title": "AutoML-GPT: Automatic machine learning with gpt",
            "rating": 1
        }
    ],
    "cost": 0.018785499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</h1>
<p>Qian Huang ${ }^{1}$ Jian Vora ${ }^{1}$ Percy Liang ${ }^{1}$ Jure Leskovec ${ }^{1}$</p>
<h4>Abstract</h4>
<p>A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with $37.5 \%$ average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from $100 \%$ on well-established older datasets to as low as $0 \%$ on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination. ${ }^{2}$</p>
<h2>1. Introduction</h2>
<p>Much of the progress in machine learning is driven by effective experimentation: Given a task (e.g., image classification), a researcher develops a method (e.g., choice of model architecture and learning algorithm), runs an experiment, and then evaluates the results. Based on the outcome of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the experiment (e.g., validation accuracy), they revise their method to improve performance on the task. This iterative process is challenging, as it requires the researcher to possess extensive prior knowledge about potential methods, to produce functional code, and to interpret experimental results for future improvements.</p>
<p>The complexity and expertise required for successful machine learning experimentation pose significant barriers to entry. In light of these challenges, there has been interest in the possibility of automating aspects of the machine learning workflow, such as Neural Architecture Search (Elsken et al., 2019) and AutoML (He et al., 2021). The emergence of advanced language models, with their ability to understand and generate human-like text, presents an promising opportunity to further automate ML experimentation end to end. Can we develop an agent capable of conducting machine learning experimentation autonomously?</p>
<p>In this paper, we propose MLAgentBench, the first benchmark for evaluating agents capable of machine learning experimentation (Figure 1). MLAgentBench is a general framework for specifying experimentation tasks with clear goals and automatically evaluates agents on these tasks. Concretely, each task is specified with a task description, a set of starter files (including starter code and data, e.g., Kaggle data package), and an evaluator that can assign a performance metric score to a final submission (such as test set accuracy of the submitted test set prediction). Given these, an agent can perform actions like reading/writing files and executing Python code in a workspace. During the agent's interaction with the environment, we collect its interaction trace for evaluation, which is the agent actions and intermediate snapshots of the workspace (i.e., the set of files and directories in the working directory). We evaluate the agent along two aspects: 1) competence in accomplishing the task, i.e., the fraction of time that the agent was able to improve the performance metric (e.g., test accuracy) by at least $10 \%$ over the baseline in the starter code; 2) efficiency, the amount of time and number of tokens LM queries spent by the agent. While our benchmark is framed in terms of automation for simplicity, we stress the importance of interpretability for building trust and also providing a hook for human augmentation: Indeed, a researcher could intervene</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of MLAgentBench. Each environment in MLAgentBench includes a task description, a set of starter files, and an evaluator. An agent can read/write files and execute Python code repeatedly, eventually producing a final file (e.g., test predictions in submission.csv). The agent is evaluated based on the quality of this file. At each time step, the language agent should produce a language output $r_{t}$, which contains reflection, research plan and status, etc, and action $a_{t}$, which is then executed by the environment to update state $s_{t}$, i.e. the set of files in the workspace and produce an observation $o_{t}$ as shown in Table 1.
and edit the workspace or plans of the agent.
MLAgentBench includes 13 ML tasks from diverse domains ranging in difficulty and recency (Table 2), where the code execution is relatively inexpensive-on the order of minutes. For example, one task is to increase the test accuracy of a baseline Convolution Neural Networks (CNN) model on the CIFAR-10 dataset (Krizhevsky, 2009) by more than 10\%. Beyond well-established datasets like CIFAR-10, we also include more recent Kaggle challenges launched between August 31, 2022 and May 11, 2023 and other research datasets launched in January 2023 to see whether the agent can extrapolate to newer datasets potentially unseen during (pre-)training.</p>
<p>We then create an agent for ML experimentation inspired by existing works (Yao et al., 2023; Shinn et al., 2023; Wang et al., 2023a; aut, 2023; Schick et al., 2023; Park et al., 2023). At each step, we automatically construct a prompt that summarizes all known information about the task and prior actions, and query the LM to produce a step-wise reflection (Shinn et al., 2023), a high-level plan (aut, 2023), a fact-checking section, a reasoning step before action (Yao et al., 2023), and the next action to take. The actions include basic actions in the environment as well as compound actions that involve several basic actions and modular LM calls, such as understanding a file and editing a file based on instructions. See more details in Section 2.2.1 and 3.</p>
<p>On MLAgentBench, we benchmark agents based on GPT-4 (0613), GPT-4-turbo (0125),(Nakano et al., 2021; OpenAI, 2023), Claude v1.0, Claude v2.1, Claude v3 Opus (opus-</p>
<p>20240229)(Anthropic, 2023), Gemini Pro (Anil et al., 2023), and Mixtral (Instruct-v0.1) (Jiang et al., 2024). We find that our agent performs the best in terms of success rate when based on Claude v3 Opus with $37.5 \%$ average success rate. Our agent is able to successfully solve many tasks and generate highly interpretable research plans along the way, though there are still many limitations. On well-established tasks like training a model over the house-price dataset, it is able to achieve $100 \%$ success rate over 8 runs. However, the agent struggles with Kaggle challenges and BabyLM (Warstadt et al., 2023), with only a $0-25 \%$ success rate. We then compare results against the adaptation of other existing agents such as ReAct and AutoGPT and find improvements upon them. We also identify several key challenges for LM-based agent designs, e.g. how to effectively plan and replan over long horizons and hallucination about the current progress, and show how our design handles them qualitatively. Overall, our agent demonstrates feasibility and success with LM-based agents for ML experimentation, but there is still some ways until they can succeed reliably.</p>
<h2>2. MLAgentBench: Benchmarking ML experimentation</h2>
<p>MLAgentBench introduces a general framework for specifying well-scoped executable tasks and automatically evaluating agents on these tasks. The benchmark provides a modular implementation of the environment and the agent, and captures the entire interaction trace for evaluation. We include 13 concrete and diverse machine learning tasks in</p>
<table>
<thead>
<tr>
<th>Action Name</th>
<th>Input</th>
<th>Observation</th>
<th>Side Effects</th>
</tr>
</thead>
<tbody>
<tr>
<td>List Files</td>
<td>directory (e.g. . )</td>
<td>list of files in the directory</td>
<td>None</td>
</tr>
<tr>
<td>Read File</td>
<td>file name (e.g. train.py)</td>
<td>contents of the file</td>
<td>None</td>
</tr>
<tr>
<td>Write File</td>
<td>file name, content</td>
<td>A success or error message</td>
<td>Content written to given file</td>
</tr>
<tr>
<td>Append File</td>
<td>file name, content</td>
<td>A success or error message</td>
<td>Content appended to given file</td>
</tr>
<tr>
<td>Copy File</td>
<td>Source (e.g. train.py), destination (e.g. train_copy.py)</td>
<td>A success or error message</td>
<td>Source file copied to destination</td>
</tr>
<tr>
<td>Inspect Script Lines</td>
<td>file name, start line number, end line number</td>
<td>the file content between start and end line numbers</td>
<td>None</td>
</tr>
<tr>
<td>Undo Edit Script</td>
<td>file name (e.g. train.py)</td>
<td>The content of the file after undo</td>
<td>The given file is restored to before an edit</td>
</tr>
<tr>
<td>Execute Script</td>
<td>file name (e.g. train.py)</td>
<td>Any output from the execution</td>
<td>Any side effect from code execution</td>
</tr>
<tr>
<td>Final Answer</td>
<td>None</td>
<td>None</td>
<td>The environment shuts down</td>
</tr>
<tr>
<td>Understand File</td>
<td>file name, a query (e.g. the model architecture)</td>
<td>retrieved content from the file relevant to the query</td>
<td>None</td>
</tr>
<tr>
<td>Edit Script</td>
<td>file name, edit instruction (e.g. change epoch to 20), save file name</td>
<td>The diff of the edited file based on the instruction</td>
<td>Edited file is saved to save path</td>
</tr>
<tr>
<td>Edit Script Segment</td>
<td>file name, start line number, end line number, edit instruction, save file name</td>
<td>The diff of the edited file based on the instruction</td>
<td>Edited file is saved to save path</td>
</tr>
</tbody>
</table>
<p>Table 1. Actions in MLAgentBench, where each action has a name, input, output, and side effects. Most of the actions are primitive actions that include file system operations and python script execution. The last three are compound actions that is composed of multiple primitive actions and LM calls.
the benchmark. Each task is specified by task description, starter files, and an evaluator, and instantiated in a general environment with a task-independent set of actions and states. In the subsequent subsections, we describe each of the key components of MLAgentBench: task specification (section 2.1), general environment (section 2.2), and evaluation (section 2.3).</p>
<h3>2.1. Task Specification</h3>
<p>Each task is specified by a textual task description, a set of starter files, and an evaluator.</p>
<p>Task description. In MLAgentBench, the task description describes the desired goal, e.g. "Given a training script on a dataset train.py, improve upon the current model accuracy" (as shown in Figure 1), and how the agent should submit the final answer for evaluation, e.g. "Save per class probabilities for test set examples to submission.csv". The description could also include constraints like limiting the model size and training epochs, or occasionally include specific directions to approach the problem like "by fine-tuning a pretrained BERT model".</p>
<p>Starter Files. The starter files include training and testing data (without test labels), detailed data descriptions, metric descriptions, and the starter code. The starter code is based on diverse ML frameworks, including PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2015), JAX (Bradbury et al., 2018), Keras (Chollet et al., 2015), etc. The starter code mostly implements a simple baseline model that we can compare with during evaluation, but some tasks do not have any baseline implementation, and the agent is responsible for coding up the model from scratch from the task description and dataset files.</p>
<p>Evaluator. Each environment has its own evaluator. The evaluator assigns a raw score to a final submission of the agent. A typical evaluator, for example, gives the test accuracy of the predictions recorded in submission.csv.</p>
<h3>2.2. General Environment</h3>
<p>Each task in MLAgentBench is instantiated in a taskagnostic environment. As shown in Figure 1, the agent operates over a sequence of time steps $t=1, \ldots, T$. Each time step is broken into three parts:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Task Type</th>
<th>Modality</th>
<th>Dataset Name</th>
<th>Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>Canonical</td>
<td>Classification</td>
<td>Image</td>
<td>CIFAR-10 (Krizhevsky, 2009)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td>Tasks</td>
<td>Classification</td>
<td>Text</td>
<td>imdb (Maas et al., 2011)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td></td>
<td>Node Classification</td>
<td>Graph</td>
<td>ogbn-arxiv (Hu et al., 2020)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td>Classic</td>
<td>Regression</td>
<td>Tabular</td>
<td>house-price (Anna Montoya, 2016)</td>
<td>Mean absolute error</td>
</tr>
<tr>
<td>Kaggle</td>
<td>Classification</td>
<td>Tabular</td>
<td>spaceship-titanic (Howard et al., 2022)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td></td>
<td>Regression</td>
<td>Time Series</td>
<td>parkinsons-disease (Kirsch et al., 2023)</td>
<td>SMAPE score</td>
</tr>
<tr>
<td>Kaggle</td>
<td>Classification</td>
<td>Image</td>
<td>fathomnet (Woodward et al., 2023)</td>
<td>MAP@20</td>
</tr>
<tr>
<td>Challenges</td>
<td>Regression</td>
<td>Text</td>
<td>feedback (Franklin et al., 2022)</td>
<td>MCRMSE</td>
</tr>
<tr>
<td></td>
<td>Segmentation</td>
<td>Images</td>
<td>identify-contrails (Sarna et al., 2023)</td>
<td>Dice coefficient</td>
</tr>
<tr>
<td>Recent</td>
<td>Node Regression</td>
<td>Graph</td>
<td>CLRS (Velivckovic et al., 2022)</td>
<td>Mean square error</td>
</tr>
<tr>
<td>Research</td>
<td>Language Modeling</td>
<td>Text</td>
<td>BabyLM (Warstadt et al., 2023)</td>
<td>Perplexity</td>
</tr>
<tr>
<td>Code</td>
<td>Improve speed</td>
<td>Text</td>
<td>llama-inference</td>
<td>Wall Clock Time</td>
</tr>
<tr>
<td>Improvement</td>
<td>Improve speed</td>
<td>Image</td>
<td>vectorization</td>
<td>Wall Clock Time</td>
</tr>
</tbody>
</table>
<p>Table 2. 13 MLAgentBench tasks. For each task, we show its task category, task type, modality and evaluator metric.</p>
<ol>
<li>Act: The agent takes its memory $m_{t}$ (see 3 for an example) and current workspace $s_{t-1}$ and produces a rationale $r_{t}$ (e.g., reflecting on previous actions and observations) and action $a_{t}$ (e.g., read a file).</li>
</ol>
<p>$$
r_{t}, a_{t}=\operatorname{Agent}\left(s_{t-1}, m_{t-1}\right)
$$</p>
<ol>
<li>Execution: The environment then executes the action $a_{t}$ on workspace $s_{t-1}$ to produce updated workspace $s_{t}$ and returns observation $o_{t}$ (See 2.2.1), based on descriptions in section 2.2.1:</li>
</ol>
<p>$$
s_{t}, o_{t}=\operatorname{Env}\left(s_{t-1}, a_{t}\right)
$$</p>
<ol>
<li>Update: Finally, agent updates its memory $m_{t-1}$ based on its action $a_{t}$, its rationale $r_{t}$, and observation $o_{t}$ :</li>
</ol>
<p>$$
m_{t}=\operatorname{Update}\left(m_{t-1}, a_{t}, r_{t}, o_{t}\right)
$$</p>
<p>The agent can take a variable number of actions many times until it decides to submit the final answer, or the environment shuts down itself due to exceeding a maximum number of actions or maximum time.</p>
<h3>2.2.1. Actions</h3>
<p>As listed in table 1, actions that are available in the environment include file system operations (read, write, append, copy, edit, undo edit), execution of any arbitrary Python script, and a final answer declaration action. Beyond these, we also manually designed a few commonly useful compound actions that perform several basic environment actions and separate modular LM calls together:</p>
<p>Understand File. This action takes a file name and a short query as input e.g. what is the model architecture, reads the file, and calls an LM to summarize it based on the short query. It then returns the retrieved and summarized information with detailed references to line numbers.</p>
<p>Edit Script. This action takes a file name, a string of edit instruction, e.g. change learning rate to $1 \mathrm{e}-3$, and a save file name as inputs. It first reads the file, calls an LM to perform an edit of a file given a short edit instruction from the main agent, then writes the modified version to the file with the given file name.</p>
<p>Edit Script Segment. Similar to Edit Script, but also takes start and end line numbers as inputs and only edits the segment in between. This is particularly helpful when the task involves manipulating a large codebase (i.e. CLRS and BabyLM).</p>
<p>Each action is specified with a name, description, usage, return value description, and a Python implementation. See Table 1 for complete descriptions.</p>
<h3>2.3. Evaluation</h3>
<p>After the agent submits the result or the environment shuts down, all actions $a_{1}, \ldots, a_{T}$, responses $r_{1}, \ldots, r_{T}$, observations $o_{1}, \ldots, o_{T}$, and snapshots of the workspace $s_{1}, \ldots, s_{T}$ after each action is executed are recorded as an interaction trace. Given the interaction traces collected, we then evaluate the agent from three aspects:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Overview of our LM-based agent. On the left we show the prompt and context of LM $p_{t}$ at each step, which includes past three steps of observations. On the upper right, we show the agent's response to the prompt $r_{t}$ and action $a_{t}$. On the lower right, the code execution results are returned as observations $o_{t}$ to the agent. Next step, this observation is incorporated into the prompt as the past history, and then the cycle repeats. Note that in this example, the agent starts with a baseline train.py and is now trying to execute this baseline.</p>
<p>Competence in accomplishing the objectives. We run the evaluator to obtain a single performance metric based on the final snapshot of the working directory. Then we define success as whether the performance metric is improved over baseline in the starter code by $10 \%$. We then compute aggregated metrics over the performence metric of multiple runs such as success rate and the average amount of improvement of the performance metric.</p>
<p>Efficiency. We evaluate efficiency in terms of the total amount of wall clock time spent and the total number of input and output tokens consumed by the agent.</p>
<h3>2.4. Tasks</h3>
<p>MLAgentBench includes 13 tasks from diverse domains including text, image, time series, graphs, and tabular data as shown in Table 2. Our tasks include both well-studied datasets like CIFAR-10 and open challenges like Parkinson's disease progression prediction from Kaggle, which is released after the language model (e.g. GPT-4) pre-training that therefore has not been pretrained on . The tasks are chosen such that they range in various difficulties and recency. In this way, we test the generalizability of the agent and mitigate data contamination. They are divided to the
following categories:</p>
<p>Canonical Tasks. We included CIFAR-10 (image classification) (Krizhevsky, 2009), imdb (sentiment classification) (Maas et al., 2011), and ogbn-arxiv (paper category classification over citation network) (Hu et al., 2020) as canonical tasks that are well-studied and easy to iterate on. For CIFAR10 and ogbn-arxiv, the task was to improve a baseline model, but for $i m d b$, the agent was expected to write the model from scratch which involved finetuning a BERT model as mentioned in the task description.</p>
<p>Classic Kaggle. House-price (Anna Montoya, 2016) and spaceship-titanic (Howard et al., 2022) are two introductory Kaggle challenges for tabular regression and classification. These tasks mainly involve feature engineering, writing, and training models from scratch (no baselines provided), and properly following the Kaggle submission format.</p>
<p>Kaggle Challenges. We select four recent open Kaggle Challenges launched between August 31, 2022 and May 11, 2023 to test agents' ability to generalize to more realistic and out-of-distribution tasks.</p>
<p>Recent Research. We include CLRS (Velivckovi'c et al., 2022) and BabyLM (Warstadt et al., 2023) as two datasets that are actively being researched and do not yet have a consensus on the best approaches. CLRS involves predicting the output of classic algorithms over graphs and lists. BabyLM requires training a language model over 10M words.</p>
<p>Code Improvement. We include llama-inference and vectorization as two datasets where the goal is to improve the runtime of code instead of optimizing its prediction performance. llama-inference is about improving the autoregressive generation speed of the LLaMA 7B model (Touvron et al., 2023), and vectorization is about speeding up the inference of a convolutional model with stacks of for loops in the forward pass.</p>
<p>More details on the benchmark tasks can be found in Appendix B.</p>
<h2>3. Our LM-based Agent</h2>
<p>To tackle MLAgentBench, we design an LM-based agent as shown in Figure 2. At a high level, we prompt the LM to provide the next step action and action arguments $a_{t}$ in a JSON format. The prompt $p_{t}$ starts with a description of all the actions available, the task description, a template to instruct the LM to produce text in parsable format, and the last 3 steps taken including $r_{t-3}, a_{t-3}, o_{t-3}, r_{t-2}, a_{t-2}, o_{t-2}, r_{t-1}, a_{t-1}, o_{t-1}$ (see Appendix F for a full example of what prompt the agent sees at each interaction step). Formally, our agent implements equation 1:</p>
<p>$$
r_{t}, a_{t}=\operatorname{Agent}\left(s_{t-1}, m_{t-1}\right)
$$</p>
<p>where $m_{t}=\left(o_{&lt;t}, r_{&lt;t}\right)$. At each time step, the agent constructs prompt $p_{t}$ and queries LM to get $r_{t}, a_{t}=L M\left(p_{t}\right)$, where $a_{t}$ is parsed from part of LLM response $r_{t}$ as detailed bellow.</p>
<h3>3.1. Thinking before Acting</h3>
<p>The most important component of our agent is specifying the response format, i.e., "Please respond in this format exactly:..." (see Figure 2), so that the LM can first generate plan and thought before proposing an action. Specifically, we ask the LM to generate the rationale $r_{t}$ before the action $a_{t}$, where the thought consists of a Reflection, Research Plan and Status, Fact Check, Thought, and then Action and Action Input.</p>
<p>As shown in Figure 2, Reflection is an entry for reflecting about the previous step as inspired by Reflexion (Shinn et al., 2023); Research Plan and Status is an entry for current planning and status designed to produce better planning and keep track of what has been done; Fact</p>
<p>Check double-checks whether a statement in Research Plan and Status has been confirmed or hallucinated; Thought is an entry for thought about what action to take similar to ReAct (Yao et al., 2023).</p>
<p>Specifically, the Research Plan and Status entries produced by our agent at each step are highly detailed and interpretable, so it is both useful for guiding the agent through the exploration process and for human understanding. It essentially enumerates the steps agent will take. The Fact Check entry allows the agent to double-check whether the update to Research Plan and Status is factual. One common failure mode during our preliminary experiments is that the model hallucinates improvement after modifying the file without ever executing it. For example, with the Fact Check entry, it will show the model that the performance of the updated model is still unknown. We discuss these entries more in Appendix D. 1 and D.2.</p>
<h2>4. Experiments</h2>
<p>We evaluate our designed agent with GPT-4 (0613), GPT-4-turbo (0125),(OpenAI, 2023), Claude v1.0, Claude v2.1, Claude v3 Opus (opus-20240229)(Anthropic, 2023), Gemini Pro (Anil et al., 2023), and Mixtral (Instruct-v0.1) (Jiang et al., 2024) on MLAgentBench. We also benchmark the adaptation of several existing generative agents: 1) AutoGPT, a popular open-source project for general-purpose autonomous AI agents (aut, 2023) which has much more complicated tools such as Google search, and 2) LangChain, another popular framework that implements various generative agents. Here we use "zero-shot-react-description" which implements ReAct (Yao et al., 2023) too similar to our agent, but just does not have research status and plan and fact checking entries. We evaluated GPT-4-turbo and Claude v3 Opus for both agents.</p>
<p>We conduct 8 runs for all agents. For most runs, we allow a maximum of 50 actions in the environment and a maximum time of 5 hours, whereas for GPT-4 runs we only allow 30 actions due to the cost associated with GPT-4 API calls.</p>
<h3>4.1. Competence in Accomplishing The Objectives</h3>
<p>As shown in Tables 3 and 4, the Claude v3 Opus agent achieves the best results over most tasks and a far better average success rate of $37.5 \%$, but with varying degrees of success from $100 \%$ over house-price to $0 \%$ over BabyLM. We also see a general positive progression of performance across different generations of models in the same family. However, GPT-4 obtains a much higher average improvement in performance metric, which means it is improving the performance metric more positively overall than Claude v3 Opus. Note that the simple averaging may exaggerate how much better GPT-4 is than Claude v3, since the gain</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-4- <br> turbo</th>
<th style="text-align: left;">Claude <br> v1.0</th>
<th style="text-align: left;">Claude <br> v2.1</th>
<th style="text-align: left;">Claude <br> v3 Opus</th>
<th style="text-align: left;">Gemini <br> Pro</th>
<th style="text-align: left;">Mixtral</th>
<th style="text-align: left;">Baseline</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">42.9</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">19.2</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">16.3</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">18.3</td>
<td style="text-align: left;">3.8</td>
<td style="text-align: left;">10.4</td>
</tr>
</tbody>
</table>
<p>Table 3. For each task and LM, we show the success rate, the percentage over 8 trials where the LM-based agent achieves an 10\% improvement on the performance metric over the baseline in the starter code.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. At each time step on the x-axis, we evaluate performance metric based on the workspace and take average across all runs to obtain the test accuracy shown. We can see that running longer generally degrades the performance except for Claude v3 Opus. is mainly dominated by the high improvement on identifycontrails.</p>
<p>Comparing our proposed agent with existing baseline agents based on GPT-4-turbo and Claude v3 Opus, our agent achieves a higher success rate on average, as shown in table 5. We note that LangChain with Claude v3 is very competitive to our method, partially because it is simpler so that the agent does not attempt to change the submission format incorrectly.</p>
<h3>4.2. Research Process</h3>
<p>We show a full example of agent responses on CIFAR-10 to demonstrate what our agent actually does qualitatively
in Appendix F. Several example actions trace on CIFAR10 are shown in Figure 4. As shown in the example, our agent generally follows the cycle of making/revising plans, editing scripts, performing experiments, interpreting results, etc. We also show a plot of average performance metric across different time steps, i.e. we evaluate not only the last step but all intermediate steps. As shown in figure 3, the agent can sometimes regress in performance as step goes later on and generally running longer steps tends to degrade the performance metric except for Claude v3 Opus. We show more analysis in Appendix C.</p>
<p>To more carefully evaluate the reasoning and research process of the agent, we analyze the traces of all runs for CIFAR-10 and categorize them as shown in Figure 5:</p>
<ol>
<li>Hallucination, where the agent claims to know something or fabricates some results such as claiming performance increase without even executing any edits in the training script.</li>
<li>Bad Plan, where the agent fails to make a correct plan that brings direct progress (such as dropping some features of the data before finding the utility of those in predicting that target). Most of these bad plans occur in the initial steps and recovery is difficult thereafter.</li>
<li>Response Format Error, where the agent produces invalid JSON and cannot be parsed with our best effort.</li>
<li>Submission Format Error, where the agent changes the submission.csv format incorrectly that our evaluator cannot recognize, even if the predictions are good.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-4- <br> turbo</th>
<th style="text-align: left;">Claude <br> v1.0</th>
<th style="text-align: left;">Claude <br> v2.1</th>
<th style="text-align: left;">Claude <br> v3 Opus</th>
<th style="text-align: left;">Gemini <br> Pro</th>
<th style="text-align: left;">Mixtral</th>
<th style="text-align: left;">Baseline</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: left;">9.2</td>
<td style="text-align: left;">5.3</td>
<td style="text-align: left;">-3.1</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">18.5</td>
<td style="text-align: left;">-36.4</td>
<td style="text-align: left;">6.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: left;">86.4</td>
<td style="text-align: left;">86.2</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">82.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: left;">48.9</td>
<td style="text-align: left;">38.6</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">19.8</td>
<td style="text-align: left;">49.5</td>
<td style="text-align: left;">7.3</td>
<td style="text-align: left;">-2.2</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: left;">45.8</td>
<td style="text-align: left;">45.0</td>
<td style="text-align: left;">48.4</td>
<td style="text-align: left;">40.5</td>
<td style="text-align: left;">44.8</td>
<td style="text-align: left;">45.4</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">-13.3</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">-0.2</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: left;">78.0</td>
<td style="text-align: left;">68.1</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">32.8</td>
<td style="text-align: left;">74.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: left;">143.3</td>
<td style="text-align: left;">114.9</td>
<td style="text-align: left;">-48.9</td>
<td style="text-align: left;">24.1</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-98.8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: left;">-1.3</td>
<td style="text-align: left;">-0.3</td>
<td style="text-align: left;">8.1</td>
<td style="text-align: left;">18.5</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;">-23.0</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-6.8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-10.0</td>
<td style="text-align: left;">-18.7</td>
<td style="text-align: left;">-11.9</td>
<td style="text-align: left;">-3.9</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">-24.2</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">-22.1</td>
<td style="text-align: left;">-11.6</td>
<td style="text-align: left;">-28.7</td>
<td style="text-align: left;">-6.6</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">-0.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">41.3</td>
<td style="text-align: left;">32.8</td>
<td style="text-align: left;">8.9</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">26.1</td>
<td style="text-align: left;">-3.6</td>
<td style="text-align: left;">8.0</td>
<td style="text-align: left;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 4. For each task and each agent, we show the average percentage improvement of the performance metric over the baseline in starter code among the runs that made a valid submission at the last step. If the improvement is beyond $10 \%$ we count it as success in Table 3. for the tasks that don't have a baseline, how do you compute improvement? technically any non-zero improvement is infinite percent increase?
5. Small Improvement, where the agent successfully makes minor improvement but it does not reach $10 \%$.</p>
<p>Note that the GPT-4 based agent is more prone to hallucinations and poor planning compared to the Claude v3 Opus based agent. We show a more detailed qualitative analysis in Appendix D, which demonstrates the benefits of Research Plan and Status entries for long-term interpretable planning and Fact Check entries against hallucination.</p>
<h3>4.3. Efficiency</h3>
<p>We compare the average number of tokens and time spent by each agent for all tasks in Figure 6. We also break down the tokens and time spent for each task in Figure 8 and 9 in the Appendix. On average, the GPT-4-turbo based agent is the most efficient, spending $51.0 \%$ fewer tokens than an average agent due to its efficiency in finishing the task and submitting early, while having a high success rate. On the other hand, the best Claude v3 Opus model spends nearly the most tokens and wall clock time, potentially due to the slower API and longer time spent on running ML experiments. Overall, gpt-4 family models have improved performance to tokens ratio trend, while Claude models generally improve performance at the cost of more tokens. Converting with the current API prices, each run on each task only costs a few dollars. In total, running the entire benchmark with GPT-4-turbo once took 6 million tokens, which is around 60 dollars. However, with the low average success rate of $26 \%$, the expected cost to accomplish a task
becomes $\$ 231$, making reliability important for the usability of the agents.</p>
<h2>5. Related Work</h2>
<h3>5.1. Language Agents</h3>
<p>This combination of strong prior knowledge and action/reaction abilities of LMs gives rise to explorations of developing various LM-based agents, such as generative agents for simulating interactions between humans (Park et al., 2023), Voyager for playing Minecraft (Wang et al., 2023a), SayCan for physical robotics (Ahn et al., 2022), as well as open source projects like AutoGPT (aut, 2023) for everything and commercial product like Adept. However, it is hard to evaluate the performance and reliability of these agents, especially over a long horizon of complex interactions. Moreover, such under-studied experimental agents can become increasingly dangerous when allowed to interact directly with personal data, the internet, or even bank accounts and military devices.</p>
<p>There are several concurrent works that also benchmark agent abilities in different aspects: AgentBench (Liu et al., 2024) benchmarks large language models with fixed simple agents in diverse environment; WebArena (Zhou et al., 2023) benchmarks agents in web interactions; ARA (Kinniment et al., 2023) evaluates agents on realistic high stakes scenarios. From this general benchmarking perspective, our MLAgentBench offers a testbed for agents with the desired combination of containability, complexity, evaluability, and practical usefulness.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Example agent traces on CIFAR-10. As shown on the left, agents mostly alternate between editing and executing training script when performing good planing; sometimes, it strays off to random actions when having a bad plan as shown on the right.</p>
<h3>5.2. Language Models for AutoML</h3>
<p>Several concurrent works have explored using LMs for AutoML type of tasks: AutoML-GPT (Zhang et al., 2023c) repeatedly prompts LMs with data and model cards and predicts training logs to perform efficient hyperparameter tuning; MLcopilot (Zhang et al., 2023a) prompts LMs with past experiences and knowledge to predict one final categorized hyperparameter setting (e.g. low or high weight decay). In contrast, our work focuses on benchmarking and developing agents that can perform very open-ended decisions by interacting with the file system and executing code with full flexibility.</p>
<h3>5.3. AI for Automating Scientific Discovery</h3>
<p>Numerous research endeavors seek to enhance the pace of manual observations and experiments through automated ML predictions (Berens et al., 2023; Zhang et al., 2023b; Jumper et al., 2021; Adam-Bourdarios et al., 2016; Schwaller et al., 2017; Wang et al., 2023b). On the other hand, significant line of inquiry revolves around constructing closed-loop systems capable of conducting ongoing experiments and breakthroughs within specific domains (Kramer et al., 2023; Kitano, 2021). For example, Robot Scientist "Adam" is developed to autonomously generate</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Percentage of runs over CIFAR-10 task that falls into different error modes.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Comparing different agents in terms of efficiency, i.e. the number of tokens spent (on x axis and the smaller the better) and success rate (on y axis and the higher the better).
functional genomics hypotheses about the yeast Saccharomyces cerevisiae and experimentally test these hypotheses by using laboratory automation (King et al., 2009; 2004). Nevertheless, these existing systems are highly tailored to process specific types of data for designated tasks and domains. Our work aims to help push toward the ultimate goal of a general and versatile research assistant agent that can perform open-ended decision-making.</p>
<h2>6. Conclusion</h2>
<p>In this paper, we introduce MLAgentBench for benchmarking LM-based agents on performing machine learning experimentation end-to-end. We develop an LM-based agent based on prompting that can accomplish many tasks in MLAgentBench with varying success rates. In the future, we would like to pursue a more robust agent and expand MLAgentBench with more complex and creative tasks accordingly. We would also like to explore the usability of our agents from a human-AI collaboration perspective with real user studies.</p>
<h2>Impact Statement</h2>
<p>Our paper presents the development and evaluation of MLAgentBench for language model-based ML experimentation agents. It carries both significant potential benefits and risks that warrant careful consideration.</p>
<p>On the positive side, the advancement of language models as tools for ML experimentation can democratize access to sophisticated ML research. It can enable a broader range of researchers, including those without extensive coding or ML expertise, to engage in ML research and experimentation. This has the potential to accelerate innovation in various fields, foster interdisciplinary research, and potentially lead to breakthroughs in areas like healthcare, environmental science, and others. The use of autonomous agents in ML experimentation also helps the reproducibility of results.</p>
<p>However, there are notable risks. The ability of these agents to autonomously modify and run ML pipelines arbitrarily can lead to unpredictable outcomes, such as writing dangerous system code. This makes it important to be under close human supervision. Furthermore, by accelerating AI development, it could make it harder for people to adapt to the new technology and defend against the risks.</p>
<p>Finally, there's a societal impact to consider in terms of employment and skill displacement. As these agents become more capable, there is a potential for them to replace or diminish the role of human engineers or researchers in certain aspects of ML experimentation, which could have broader implications for the job market and required skill sets in the field. To mitigate these risks and transform potential challenges into opportunities, it is crucial to involve ML researchers and engineers in the development and implementation of these AI systems. By doing so, AI can be used to augment the work of professionals rather than replace it. This approach not only preserves jobs but also enhances the productivity and creativity of human workers.</p>
<h2>References</h2>
<p>Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous. https://github.com/SignificantGravitas/Auto-GPT, 2023.</p>
<p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vigas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.</p>
<p>Adam-Bourdarios, C., Cowan, G., Germain, C., Guyon, I. M., Kgl, B., and Rousseau, D. How machine learning won the higgs boson challenge. In The European Symposium on Artificial Neural Networks, 2016.</p>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R. C., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D. M., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., and Yan, M. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022.</p>
<p>Anil, G. T. G. R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K. W., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L. N., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Brustemi, A., Clay, N., Crone, P., Kocisk, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D. H., Barnes, M., Thakoor, S., Austin, J., Barth-Maron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R.,</p>
<p>Li, Y., Cogan, S., Chen, J., Jia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., de Castro Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z. C., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J. C., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozi'nska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C. A., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn, D., Sachan, D. S., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Gim'enez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., Luvci'c, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasevic, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins,
R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V., Lan, C. L., Haridasan, K. S., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjosund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V. V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., cCauglar Unlu, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M. L., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T. N., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J. R., Mandhane, A., Joshi, P. M., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Raki'cevi'c, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., hsin Chi, E. H., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I.-R., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Staczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C. F., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Castano, A., Giannoumis, I., Kim, W., Rybi'nski, M., Sreevatsa, A., Prendki, J., Soergel, D. G., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A. Y., Cui, A., Lin, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Piper, J., Ie, N., Lui, M., Pasumarthi, R. K., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng,</p>
<p>D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q. V., Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D. F., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D. S., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, O., Kushman, N., Krakovna, V., Brown, S. M., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J. F. J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J. M., Korchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Krishna, K., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J. M., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., Heliou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., Santamaria-Fernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D. N., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., Poder, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P. D., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Riviere, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A. ., Scellato, S., Latorre-Chimoto, E., Klimczak-Plucinska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio,</p>
<p>P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb, L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S., Wang, L., Tekur, C., Kale, M., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A. W., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M. E., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B. A., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models. ArXiv, abs/2312.11805, 2023. URL https://api.semanticscholar.org/ CorpusID:266361876.</p>
<p>Anna Montoya, D. House prices - advanced regression techniques, 2016. URL https://kaggle.com/ competitions/house-prices-advancedregression-techniques.</p>
<p>Anthropic. Introducing claude, 2023. URL https://www.anthropic.com/index/ introducing-claude.</p>
<p>Berens, P., Cranmer, K., Lawrence, N. D., von Luxburg, U., and Montgomery, J. Ai for science: An emerging agenda. ArXiv, abs/2303.04217, 2023.</p>
<p>Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Chollet, F. et al. Keras, 2015. URL https:// github.com/fchollet/keras.</p>
<p>Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search: a survey. J. Mach. Learn. Res., 20(1):1997-2017, jan 2019. ISSN 1532-4435.</p>
<p>Franklin, A., Maggie, Benner, M., Rambis, N., Baffour, P., Holbrook, R., Crossley, S., and ulrichboser. Feedback prize - english language learning, 2022. URL https: //kaggle.com/competitions/feedback-prize-english-language-learning.</p>
<p>He, X., Zhao, K., and Chu, X. Automl: A survey of the state-of-the-art. Knowledge-Based Systems, 212:106622, 2021. ISSN 0950-7051. doi: https://doi.org/10.1016/j.knosys.2020.106622. URL https://www.sciencedirect.com/science/ article/pii/S0950705120307516.</p>
<p>Howard, A., Chow, A., and Holbrook, R. Spaceship titanic, 2022. URL https://kaggle.com/ competitions/spaceship-titanic.</p>
<p>Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. ArXiv, abs/2005.00687, 2020.</p>
<p>Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de Las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts. ArXiv, abs/2401.04088, 2024. URL https://api.semanticscholar.org/ CorpusID:266844877.</p>
<p>Jumper, J. M., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Zdek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D. A., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. Highly accurate protein structure prediction with alphafold. Nature, 596:583 - 589, 2021.</p>
<p>King, R. D., Whelan, K. E., Jones, F. M., Reiser, P. G. K., Bryant, C. H., Muggleton, S. H., Kell, D. B., and Oliver, S. G. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427:247252, 2004.</p>
<p>King, R. D., Rowland, J. J., Oliver, S. G., Young, M., Aubrey, W., Byrne, E., Liakata, M., Markham, M., Pir, P., Soldatova, L. N., Sparkes, A., Whelan, K. E., and Clare, A. The automation of science. Science, 324:85 - 89, 2009.</p>
<p>Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk,
H., Burget, J., Ho, A., Barnes, E., and Christiano, P. F. Evaluating language-model agents on realistic autonomous tasks. ArXiv, abs/2312.11671, 2023. URL https://api.semanticscholar.org/ CorpusID:260472392.</p>
<p>Kirsch, L., Dane, S., Adam, S., and Dardov, V. Ampparkinson's disease progression prediction, 2023. URL https://kaggle.com/competitions/ amp-parkinsons-disease-progressionprediction.</p>
<p>Kitano, H. Nobel turing challenge: creating the engine for scientific discovery. NPJ Systems Biology and Applications, 7, 2021.</p>
<p>Kramer, S., Cerrato, M., Dzeroski, S., and King, R. D. Automated scientific discovery: From equation discovery to autonomous discovery systems. ArXiv, abs/2305.02251, 2023.</p>
<p>Krizhevsky, A. Learning multiple layers of features from tiny images. 2009.</p>
<p>Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. URL https : //openreview.net/forum?id=zAdUB0aCTQ.</p>
<p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P11-1015.</p>
<p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Long, O., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. URL https://api.semanticscholar.org/ CorpusID:245329531.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.</p>
<p>Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and</p>
<p>Technology, UIST '23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763. URL https:// doi.org/10.1145/3586183.3606763.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.</p>
<p>Sarna, A., Elkin, C., inversion, Ng, J., Maggie, and Reade, W. Google research - identify contrails to reduce global warming, 2023. URL https://kaggle.com/competitions/ google-research-identify-contrails-reduce-global-warming.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=Yacmpz84TH.</p>
<p>Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C., and Laino, T. "found in translation": predicting outcomes of complex organic chemistry reactions using neural sequence-tosequence models? ?electronic supplementary information (esi) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. see do. Chemical Science, 9:6091 6098, 2017.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=vAElhFcKW6.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.</p>
<p>Velivckovi'c, P., Badia, A. P., Budden, D., Pascanu, R., Banino, A., Dashevskiy, M., Hadsell, R., and Blundell, C. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning, 2022.</p>
<p>URL https://api.semanticscholar.org/ CorpusID:249210177.</p>
<p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L. J., and Anandkumar, A. Voyager: An openended embodied agent with large language models. ArXiv, abs/2305.16291, 2023a.</p>
<p>Wang, Q., Downey, D., Ji, H., and Hope, T. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint arXiv:2305.14259, 2023b.</p>
<p>Warstadt, A., Choshen, L., Mueller, A., Williams, A., Wilcox, E. G., and Zhuang, C. Call for papers - the babylm challenge: Sample-efficient pretraining on a developmentally plausible corpus. ArXiv, abs/2301.11796, 2023.</p>
<p>Woodward, B., eor123, GenevievePatterson, and Carlsen, L. Fathomnet 2023, 2023. URL https://kaggle.com/competitions/ fathomnet-out-of-sample-detection.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Zhang, L., Zhang, Y., Ren, K., Li, D., and Yang, Y. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. ArXiv, abs/2304.14979, 2023a. URL https: //api.semanticscholar.org/CorpusID: 258418182.</p>
<p>Zhang, M., Qamar, M., Kang, T., Jung, Y., Zhang, C., Bae, S.-H., and Zhang, C. A survey on graph diffusion models: Generative ai in science for molecule, protein and material. ArXiv, abs/2304.01565, 2023b.</p>
<p>Zhang, S., Gong, C., Wu, L., Liu, X., and Zhou, M. Automl-gpt: Automatic machine learning with gpt. ArXiv, abs/2305.02499, 2023c. URL https: //api.semanticscholar.org/CorpusID: 258480269.</p>
<p>Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https://api.semanticscholar.org/ CorpusID:260164780.</p>
<h1>A. Agent Framework Comparison</h1>
<p>In Table 5, we show the comparison against different agent frameworks such as LangChain and AutoGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: right;">GPT-4-turbo</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Claude v3 Opus</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Ours</td>
<td style="text-align: right;">AutoGPT</td>
<td style="text-align: right;">LangChain</td>
<td style="text-align: right;">Ours</td>
<td style="text-align: right;">AutoGPT</td>
<td style="text-align: right;">LangChain</td>
</tr>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: right;">$\mathbf{1 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: right;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">62.5</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">25.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: right;">$\mathbf{5 0 . 0}$</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: right;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: right;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: right;">$\mathbf{2 6 . 0}$</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: right;">13.5</td>
<td style="text-align: right;">33.7</td>
</tr>
</tbody>
</table>
<p>Table 5. The comparison of success rates of different agent frameworks using GPT-4-turbo and Claude v3 Opus.</p>
<h2>B. Benchmark Details</h2>
<p>For Canonical Tasks, Classic Kaggle, Kaggle Challenges and Recent Research, we require the agent to generate a submission.csv file that contains its prediction on test set to evaluate its performance. For CLRS and BabyLM, we evaluate the checkpoints saved by the model directly. For these tasks, we provide a starter code train.py that can already generate the required submission files properly with a baseline model or dummy predictions. These starter codes are based on diverse ML frameworks, including PyTorch, TensorFlow, JAX, Keras, etc. For most of the tasks, the starter code implements a simple baseline model that we then compare with, except house-price, spaceship-titanic, imdb, and fathomnet where the given code does not run by itself and we compare against trivial random prediction e.g. 0.5 accuracy for imdb. For Code Improvement tasks, we simply time the produced code. For Tools tasks, we perform preliminary human evaluation.</p>
<h2>C. Quantitative Analysis</h2>
<p>In Figure 7, we show the percentage of time agents spent on using each action and the distribution of numbers of steps used by agents.</p>
<h2>D. Qualitative Examples</h2>
<p>Bellow, we show some examples to demonstrate the benefits of each component in our agent as well as the failure modes.</p>
<h2>D.1. Research Plan and Status</h2>
<p>The Research Plan and Status entries produced by our agent at each step are highly detailed and interpretable, so it is both useful for guiding the agent through the exploration process and for human understanding. Here we present one example from the agent with Claude v1.0 for CIFAR-10 training.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Left: the percentage of time agents spent on using each action; Right: the distribution of numbers of steps used by agents.</p>
<p>At step 0 , the agent comes up the following plan:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nl">Status:</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">Understand</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">train</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">dataset</span><span class="p">.</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">familiar</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span>
<span class="w">    </span><span class="n">architecture</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">hyperparameters</span><span class="p">.</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Come</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="nl">performance:</span>
<span class="o">-</span><span class="w"> </span><span class="n">Try</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="n">learning</span><span class="w"> </span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">Add</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">decay</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">Change</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">architecture</span><span class="w"> </span><span class="p">(</span><span class="n">more</span><span class="w"> </span><span class="n">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">nodes</span><span class="p">)</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">Define</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">baseline</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">original</span><span class="w"> </span><span class="n">train</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">script</span><span class="p">.</span><span class="w"> </span><span class="n">Record</span><span class="w"> </span><span class="n">accuracy</span><span class="w"> </span><span class="n">on</span>
<span class="w">    </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="p">.</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">Try</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">planned</span><span class="w"> </span><span class="n">improvements</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">one</span><span class="p">,</span><span class="w"> </span><span class="n">evaluating</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">each</span>
<span class="w">    </span><span class="n">trial</span><span class="p">.</span><span class="w"> </span><span class="n">Compare</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">baseline</span><span class="p">.</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">Once</span><span class="w"> </span><span class="n">improved</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="mh">10</span><span class="o">%</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">achieved</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="mh">10</span><span class="w"> </span><span class="n">epochs</span><span class="p">,</span><span class="w"> </span><span class="n">save</span>
<span class="w">    </span><span class="n">per</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">probabilities</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">submission</span><span class="p">.</span><span class="n">csv</span><span class="p">.</span>
<span class="mf">6.</span><span class="w"> </span><span class="n">Submit</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">configuration</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">performance</span><span class="p">.</span>
</code></pre></div>

<p>At step 10 before the agent submit the final answer, the agent's plan and status is updated to below:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Understand</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span><span class="w"> </span><span class="nx">Got</span><span class="w"> </span><span class="nx">familiar</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span>
<span class="w">    </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">simple</span><span class="w"> </span><span class="nx">CNN</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">lr</span><span class="p">=</span><span class="m m-Double">0.1</span><span class="p">,</span><span class="w"> </span><span class="nx">momentum</span><span class="p">=</span><span class="m m-Double">0.9</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span>
<span class="w">    </span><span class="nx">size</span><span class="p">=</span><span class="mi">128</span><span class="p">).</span><span class="w"> </span><span class="nx">Dataset</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">CIFAR</span><span class="o">-</span><span class="mi">10</span><span class="p">.</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span><span class="w"> </span><span class="nx">Increase</span>
<span class="w">    </span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span><span class="p">,</span><span class="w"> </span><span class="nx">decrease</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span><span class="nx">Tried</span><span class="w"> </span><span class="nx">increasing</span><span class="w"> </span><span class="nx">LR</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span>
<span class="w">    </span><span class="k">in</span><span class="w"> </span><span class="nx">train_lr03</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">resulted</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">lower</span><span class="w"> </span><span class="nx">accuracy</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">regularization</span><span class="w"> </span><span class="p">(</span><span class="nx">dropout</span><span class="p">,</span><span class="w"> </span><span class="nx">weight</span><span class="w"> </span><span class="nx">decay</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">dropout</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">p</span><span class="p">=</span><span class="m m-Double">0.5</span><span class="w"> </span><span class="nx">after</span>
<span class="w">    </span><span class="nx">conv</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">fc</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">train_dropout</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Improved</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">lower</span>
<span class="w">        </span><span class="nx">accuracy</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Change</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">more</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">nodes</span><span class="p">)</span><span class="w"> </span><span class="nx">Increase</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">filters</span><span class="w"> </span><span class="k">in</span>
<span class="w">    </span><span class="nx">conv</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">train_arch</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Test</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="m m-Double">64.31</span><span class="o">%</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="mi">5</span>
<span class="w">    </span><span class="nx">epochs</span><span class="p">.</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Define</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="p">.</span><span class="w"> </span><span class="nx">Record</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">test</span><span class="w"> </span><span class="nx">set</span><span class="p">.</span><span class="w"> </span><span class="nx">Baseline</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">52.53</span><span class="o">%</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">epochs</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">planned</span><span class="w"> </span><span class="nx">improvements</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">each</span>
<span class="w">    </span><span class="nx">trial</span><span class="p">.</span><span class="w"> </span><span class="nx">Compare</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">baseline</span><span class="p">.</span><span class="w"> </span><span class="nx">Tried</span><span class="w"> </span><span class="nx">increasing</span><span class="w"> </span><span class="nx">LR</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span>
<span class="w">    </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">train_lr03</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">worse</span><span class="p">.</span><span class="w"> </span><span class="nx">Added</span>
</code></pre></div>

<p>dropout to train_dropout.py, test accuracy 49.34\% after 5 epochs. Modified architecture in train_arch.py, test accuracy 64.31\% after 5 epochs.
5. Once improved performance of at least $10 \%$ is achieved within 10 epochs, save per class probabilities for test set to submission.csv. Saved test set probabilities to submission.csv after running train_arch_submission.py.
6. Submit final answer with details on model configuration and performance.</p>
<p>Between these two steps, the agent gradually updated the Research Plan and Status entry after editing the file and executing it as recorded. See the full example in the appendix.</p>
<p>However, one common failure mode that this entry fails to prevent is when the agent plans to carry out too complex an edit and becomes stuck with debugging, which occurs in $40 \%$ of the runs for Claude v1.0 as shown in 5. Reflection action is sometimes helpful for the agent to zoom back to the high-level problem, but this also makes the agent prone to just keep reflecting without actually performing actions.</p>
<h1>D.2. Fact Check</h1>
<p>The Fact Check entry allows the agent to double-check whether the update to Research Plan and Status is factual. One common failure mode during our prelimiary experiments is that the model hallucinates improvement after modifying the file without ever executing it. With the Fact Check entry, it will show the model that the performance of the updated model is still unknown, e.g.</p>
<div class="codehilite"><pre><span></span><code>Fact Check: Performance after running train_dropout.py still needs to be
    evaluated. Other facts unchanged.
</code></pre></div>

<p>Of course, this does not guard against hallucination completely. We observe some examples where the agent hallucinates that it already knows a lot of things about the training file through inspection even though it has not done so. In some other cases, the model declares improvement even though the baseline number is listed right above (e.g. $51.80 \%$ ) and clearly is higher: "Achieved test accuracy of $26.35 \%$ which improves over baseline by $10 \%$ ". As shown in 5 , this happens to $20 \%$ of the runs for Claude-1</p>
<div class="codehilite"><pre><span></span><code>Research Plan and Status:
1. Understand the existing train.py script and extract the baseline performance.
    Done. Baseline test accuracy is 51.80%.
2. Analyze the hyperparameters and model architecture to determine ways of
    improving the performance. In progress.
3. Run experiments with new hyperparameters and model architecture. Done. Tried
    increasing learning rate to 0.3 and 10 epochs. Achieved test accuracy of
    26.35% which improves over baseline by 10%.
...
Fact Check: The statement &quot;**Increased learning rate to 0.3 and trained for 10
    epochs. Achieved test accuracy of 26.35% which improves over baseline by 10%,
    meeting the performance goal.**&quot; is confirmed by running the train_lr03.py
    script and observing the output.
</code></pre></div>

<h2>D.3. Problem Misspecification</h2>
<p>One "failure mode" we observe during the development of this benchmark is that the problem specification can be critical to agent performance. The problem description needs to clearly specify what file and what metrics will be evaluated. In one of the extreme case, we actually observed that our agent tried to increase SMAPE score on amp-parkinsons-disease-progression-prediction dataset, since it does not know that SMAPE is the lower the better:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Assistant</span><span class="o">:</span>
<span class="n">Reflection</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">shows</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">executing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">updated</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="na">py</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="k">with</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">tuned</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">forest</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">achieved</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cross</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mf">76.16</span><span class="o">,</span>
</code></pre></div>

<p>which is an improvement over the baseline score of 61.40. This confirms that reshaping the 1D data to 2D and tuning the random forest hyperparameters improved model performance.</p>
<p>We compare the average amount of tokens and time spent by different agents for each task in Figure 8 and 9. Note that the total tokens is the sum prompt and completion tokens. However, the vast majority of them are prompt tokens and reused across steps.</p>
<h1>E. Efficiency</h1>
<h2>F. Full Example</h2>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="w"> </span><span class="n">research</span><span class="w"> </span><span class="n">assistant</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">access</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">tools</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">Files</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">navigate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">system</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">Files</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;dir_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s2">&quot;.&quot;</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="s2">&quot;</span>
<span class="w">            </span><span class="n">folder1</span><span class="o">/</span><span class="n">folder2</span><span class="s2">&quot;]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">folders</span><span class="w"> </span><span class="ow">in</span>
<span class="w">        </span><span class="n">dir_path</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">dir_path</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">empty</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span>
<span class="w">        </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">dir_path</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">invalid</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Copy</span><span class="w"> </span><span class="n">File</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">copy</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">name</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Copy</span><span class="w"> </span><span class="n">File</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;destination&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span>
<span class="w">                </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">A</span><span class="w"> </span><span class="n">success</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">copied</span><span class="w"> </span><span class="n">successfully</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">an</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">copied</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">undo</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">before</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">edit</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    an error message.]
    ***
<span class="k">-</span> Execute Script:
    Use this to execute the python script. The script must already exist.
    Usage:
    ***
    Action: Execute Script
    Action Input: {
        &quot;script_name&quot;: [a valid python script name with relative path to
            current directory if needed]
    }
    Observation: [The observation will be output of the script or errors.]
    ***
<span class="k">-</span> Final Answer:
    Use this to provide the final answer to the current task.
    Usage:
    ***
    Action: Final Answer
    Action Input: {
        &quot;final_answer&quot;: [a detailed description on the final answer]
    }
    Observation: [The observation will be empty.]
    ***
<span class="k">-</span> Understand File:
    Use this to read the whole file and understand certain aspects. You
        should provide detailed description on what to look for and what
        should be returned. To get a better understanding of the file, you
        can use Inspect Script Lines action to inspect specific part of the
        file.
    Usage:
    ***
    Action: Understand File
    Action Input: {
        &quot;file_name&quot;: [a valid file name with relative path to current
            directory if needed],
            &quot;things_to_look_for&quot;: [a detailed description on what to look for and
                what should returned]
    }
    Observation: [The observation will be a description of relevant content
        and lines in the file. If the file does not exist, the observation
        will be an error message.]
    ***
<span class="k">-</span> Inspect Script Lines:
    Use this to inspect specific part of a python script precisely, or the
        full content of a short script. The number of lines to display is
        limited to 100 lines. This is especially helpful when debugging.
    Usage:
    ***
    Action: Inspect Script Lines
    Action Input: {
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">            </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;start_line_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">number</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;end_line_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">number</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">between</span>
<span class="w">        </span><span class="n">start_line_number</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">end_line_number</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">message</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="p">(</span><span class="n">AI</span><span class="p">):</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">relatively</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">cohesive</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">        </span><span class="n">Instead</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">editing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">directly</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">describe</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span>
<span class="w">        </span><span class="n">instruction</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">this</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="p">(</span><span class="n">AI</span><span class="p">)</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="o">.</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">sctipt</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="n">it</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="o">.</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;edit_instruction&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">to</span>
<span class="w">                </span><span class="n">edit</span><span class="w"> </span><span class="n">it</span><span class="o">.</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;save_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span>
<span class="w">                </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edited</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">        </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span>
<span class="w">        </span><span class="n">message</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">always</span><span class="w"> </span><span class="n">double</span><span class="w"> </span><span class="n">check</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="o">.</span>
<span class="w">        </span><span class="n">If</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">far</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">correct</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">undo</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="n">Research</span><span class="w"> </span><span class="n">Problem</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="p">,</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">upon</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">current</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="p">(</span><span class="n">trained</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">hyperparmeters</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">The</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">epochs</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">time</span><span class="o">.</span><span class="w"> </span><span class="n">Save</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="k">class</span>
<span class="w">    </span><span class="n">probabilities</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">submission</span><span class="o">.</span><span class="n">csv</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">shown</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="o">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">know</span><span class="w"> </span><span class="n">anything</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">far</span><span class="o">.</span>
<span class="n">Follow</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">instructions</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">forget</span><span class="w"> </span><span class="n">them</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">First</span><span class="p">,</span><span class="w"> </span><span class="n">come</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">understanding</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">problem</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">tools</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">record</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="o">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">revise</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">later</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">well</span><span class="w"> </span><span class="n">organized</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">succinctly</span><span class="w"> </span><span class="n">keep</span><span class="w"> </span><span class="n">track</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="n">high</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="p">(</span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">revised</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">steps</span>
<span class="n">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">progress</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">conclusions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">has</span>
<span class="n">been</span><span class="w"> </span><span class="n">performed</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="n">progress</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">made</span><span class="w"> </span><span class="n">by</span>
<span class="n">previous</span><span class="w"> </span><span class="n">steps</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="n">confirmed</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span>
<span class="n">previous</span><span class="w"> </span><span class="n">observation</span><span class="o">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">-</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">estimates</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">included</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">status</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">observing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">good</span><span class="w"> </span><span class="nx">experiment</span><span class="w"> </span><span class="nx">design</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">addresses</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="p">,</span>
<span class="w">    </span><span class="k">and</span><span class="w"> </span><span class="nx">whenever</span><span class="w"> </span><span class="nx">applicable</span><span class="p">,</span><span class="w"> </span><span class="nx">define</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">measure</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">relevant</span><span class="w"> </span><span class="nx">system</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">attempting</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">improvements</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Follow</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">try</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">achieve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">straightforwardly</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">possible</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Highlight</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">supporting</span><span class="w"> </span><span class="nx">experiment</span><span class="w"> </span><span class="nx">results</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">reasoning</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">drawing</span><span class="w"> </span><span class="nx">any</span>
<span class="w">    </span><span class="nx">conclusions</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">try</span><span class="w"> </span><span class="nx">installing</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">packages</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">libraries</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">believe</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">solved</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="p">,</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Final</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="nx">action</span>
<span class="w">        </span><span class="nx">to</span><span class="w"> </span><span class="nx">submit</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">answer</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">submit</span><span class="w"> </span><span class="nx">once</span><span class="p">,</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">check</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span>
<span class="w">        </span><span class="nx">have</span><span class="w"> </span><span class="nx">achieved</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">submitting</span><span class="p">.</span>
<span class="nx">Always</span><span class="w"> </span><span class="nx">respond</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="nx">exactly</span><span class="p">:</span>
<span class="nx">Reflection</span><span class="p">:</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">does</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">observation</span><span class="w"> </span><span class="nx">mean</span><span class="p">?</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">error</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">caused</span><span class="w"> </span><span class="nx">the</span>
<span class="w">        </span><span class="nx">error</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">how</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">debug</span><span class="p">?</span>
<span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">full</span><span class="w"> </span><span class="nx">high</span><span class="w"> </span><span class="nx">level</span><span class="w"> </span><span class="nx">research</span><span class="w"> </span><span class="nx">plan</span><span class="p">,</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">status</span>
<span class="w">    </span><span class="k">and</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">results</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">briefly</span><span class="w"> </span><span class="nx">annotated</span><span class="p">.</span><span class="w"> </span><span class="nx">It</span><span class="w"> </span><span class="nx">must</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">include</span>
<span class="w">    </span><span class="nx">progress</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="nx">been</span><span class="w"> </span><span class="nx">made</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">steps</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">update</span><span class="p">,</span>
<span class="w">    </span><span class="nx">enclose</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">update</span><span class="w"> </span><span class="nx">text</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">asterisks</span><span class="w"> </span><span class="o">**</span><span class="k">like</span><span class="w"> </span><span class="nx">this</span><span class="o">**</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">no</span>
<span class="w">        </span><span class="nx">update</span><span class="p">,</span><span class="w"> </span><span class="nx">just</span><span class="w"> </span><span class="nx">copy</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">high</span><span class="w"> </span><span class="nx">level</span>
<span class="w">        </span><span class="nx">plan</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">fully</span><span class="w"> </span><span class="nx">retained</span><span class="p">,</span><span class="w"> </span><span class="nx">unless</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="nx">intentionally</span><span class="w"> </span><span class="nx">revised</span><span class="p">.</span>
<span class="nx">Fact</span><span class="w"> </span><span class="nx">Check</span><span class="p">:</span><span class="w"> </span><span class="nx">List</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">objective</span><span class="w"> </span><span class="nx">statements</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">updates</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span>
<span class="w">        </span><span class="nx">Status</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">point</span><span class="w"> </span><span class="nx">out</span><span class="w"> </span><span class="nx">whether</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">guessed</span><span class="w"> </span><span class="nx">versus</span><span class="w"> </span><span class="nx">directly</span>
<span class="w">        </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">observation</span><span class="w"> </span><span class="nx">directly</span><span class="w"> </span><span class="nx">above</span><span class="p">.</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">can</span>
<span class="w">        </span><span class="nx">only</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">observing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span>
<span class="nx">Thought</span><span class="p">:</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">currently</span><span class="w"> </span><span class="nx">doing</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">actions</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">perform</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">why</span>
<span class="nx">Action</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">take</span><span class="p">,</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">tools</span>
<span class="nx">Action</span><span class="w"> </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">valid</span><span class="w"> </span><span class="nx">JSON</span><span class="w"> </span><span class="kt">string</span>
<span class="nx">Observation</span><span class="p">:</span>
<span class="w">    </span><span class="o">...</span>
<span class="nx">the</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span>
<span class="w">    </span><span class="o">...</span>
<span class="nx">Step</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="nx">Assistant</span><span class="p">:</span>
<span class="nx">Reflection</span><span class="p">:</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">read</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">tools</span><span class="w"> </span><span class="nx">available</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">least</span><span class="w"> </span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">within</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="nx">epochs</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">training</span><span class="p">.</span>
<span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Understand</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span><span class="w"> </span><span class="nx">Get</span><span class="w"> </span><span class="nx">familiar</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span>
<span class="w">        </span><span class="nx">architecture</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="p">.</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">regularization</span><span class="w"> </span><span class="p">(</span><span class="nx">dropout</span><span class="p">,</span><span class="w"> </span><span class="nx">weight</span><span class="w"> </span><span class="nx">decay</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Change</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">more</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">nodes</span><span class="p">)</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Define</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="p">.</span><span class="w"> </span><span class="nx">Record</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">test</span><span class="w"> </span><span class="nx">set</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">planned</span><span class="w"> </span><span class="nx">improvements</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">each</span>
<span class="w">        </span><span class="nx">trial</span><span class="p">.</span><span class="w"> </span><span class="nx">Compare</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">baseline</span><span class="p">.</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Stanford University. Correspondence to: Qian Huang &lt;qhwang@cs.stanford.edu $&gt;$.</p>
<p>2 Our code is released at https://github.com/snapstanford/MLAgentBench/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
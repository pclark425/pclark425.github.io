<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7056 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7056</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7056</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-03e0b834e4077047cc2b3426a2b52b7b368968fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/03e0b834e4077047cc2b3426a2b52b7b368968fe" target="_blank">FLARE: Faithful Logic-Aided Reasoning and Exploration</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work uses the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space, allowing the faithfulness of the reasoning process to be computed.</p>
                <p><strong>Paper Abstract:</strong> Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7056.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7056.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Logic-Aided Reasoning and Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular prompting paradigm that uses an LLM to (i) generate a natural-language plan, (ii) produce a Prolog-style logic program (soft formalisation), and (iii) simulate program execution (multi-path search) with the same LLM to derive answers while enabling a measurable faithfulness check against actual Prolog execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (applied with multiple LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model — FLARE is a prompting/method pipeline applied to various transformer LLMs to produce plans, Prolog-like code, and simulated search traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based LLM + simulated Prolog-style execution (neuro-symbolic prompting pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Plan → program (logic formalisation) → LLM-simulated search (Prolog-like traces); measures faithfulness by comparing simulated traces to real Prolog execution using ROUGE-Lsum</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>FLARE can (optionally) execute the generated Prolog code with an external Prolog engine for validation and to obtain gold traces, but its inference pipeline primarily simulates execution with the LLM (the method does not require external solvers at inference).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>9 QA datasets (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR) and 3 logical inference datasets (PrOntoQA, LogicalDeductions, AR-LSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mix of math word problems, multi-hop QA, relation inference and strict logical-deductive tasks (PrOntoQA: synthetic logical implications; LogicalDeductions: BIG-bench logical deduction subset; AR-LSAT: analytical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-hop logical reasoning, program-aided formalisation, deductive logical inference (first-order/Horn-clause style tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and ROUGE-Lsum for faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: SOTA on 7/9 QA datasets (average +28% over CoT on QA in paper), base FLARE: SOTA on 2/3 logic benchmarks; with 2 iterations of code self-refinement (FLARE_SR=2) achieves SOTA on all 3 logic benchmarks. Example values (logic datasets, GPT-4): PrOntoQA FLARE=98.87% (FLARE_SR=99.24%), LogicalDeductions FLARE=88.00% (SR=90.33%), AR-LSAT FLARE=39.82% (SR=45.02%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>On QA: +28% average vs CoT; For code-tuned models (GPT-3.5): +16% vs F-CoT and +9% vs CoT (average reported). On logic benchmarks: FLARE ~+10% over CoT and ~+7% over Logic-LM in the paper's aggregate reporting; further improved with 2 self-refinement iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulating logic-program execution with LLMs (soft formalisation + simulated search) yields large improvements over CoT and F-CoT on many QA tasks and strong gains on logical inference benchmarks; faithfulness of simulated traces (measured by ROUGE-Lsum against actual Prolog traces) correlates positively with accuracy; combining soft reasoning and formalisation allows handling tasks that are hard to auto-formalise for strict solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on the LLM's ability to produce correct plans and code; generated code is executable by Prolog only ~67% on average (>=50% worst-case); simulated execution may miss parts of very large or open-ended search spaces; prompt sensitivity and errors in formalisation propagate; strict external-solvers can still outperform when a perfect formalisation exists, and FLARE trades strict executability for soft/fuzzy formalisation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7056.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>F-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Chain-of-Thought (F-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that prompts models to generate code (programs) corresponding to reasoning traces and executes that code in an external symbolic engine to obtain faithful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (applied with various LLMs in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic pipeline: LLM produces executable code; external symbolic executor runs code to produce answers (original F-CoT used Codex for code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external symbolic executor (program execution)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis (autoformalisation) followed by external execution to ensure faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Delegates reasoning to an external symbolic solver (e.g., Python/Prolog) that executes synthesized programs; in prior work Codex + executor used, in this paper replaced Codex with GPT-3.5 for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same benchmarks used for comparison (MWP, multi-hop QA, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Programmatic formalisation evaluated on math, multi-hop, and logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Program-executable formal reasoning (strict logical execution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Often strong on problems that are straightforward to formalise and execute; in this paper F-CoT underperforms on many non-strict or fuzzy tasks and can fail catastrophically for LLMs not tuned for coding (numerical examples: F-CoT results often much lower than FLARE and CoT on StrategyQA, Sports, CLUTRR in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>F-CoT can yield higher faithfulness in prior work, but in this paper F-CoT underperforms FLARE on many benchmarks — LLMs not tuned for code produce many unexecutable programs leading to failures (paper reports FLARE is better than F-CoT across most evaluated LLMs; for code-tuned models FLARE still outperforms F-CoT by ~16% average).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an external executor enforces faithful execution but requires consistent generation of executable code; models not explicitly trained for code struggle with F-CoT, causing execution errors and performance collapse on tasks needing fuzzy commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires executable, syntactically correct code; brittle when LLM cannot reliably generate code matching the expected formalisation; cannot handle fuzzy/soft reasoning that doesn't map cleanly to deterministic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7056.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLMs to formalise natural language queries into code and delegates reasoning to symbolic solvers, aimed at improving faithfulness in logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (compared in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: LLM formalisation into programmatic logic + external symbolic solver for execution to produce faithful logical reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + symbolic solver integration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis into formal logic and external solver execution</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates symbolic solvers to execute the formalised programs and obtain faithful traces/answers (paper compares FLARE to Logic-LM on logic inference benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PrOntoQA, LogicalDeductions, AR-LSAT (logic inference benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks with strictly formal logical deduction and inference tasks (deductive/analytical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Strict logical deduction via solver-executed formal programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in comparisons: FLARE outperforms Logic-LM by ~7% on aggregate logic benchmarks in this paper; specific Logic-LM numbers in Table 2 (e.g., GPT-4 Logic-LM on PrOntoQA=83.20% vs FLARE=98.87%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>In this paper FLARE > Logic-LM on the evaluated logic benchmarks; Logic-LM improves faithfulness relative to plain CoT but is limited by strict executability requirements on fuzzy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Delegating to symbolic solvers improves faithfulness for strictly formal tasks but loses flexibility for fuzzy/commonsense reasoning; FLARE's simulated-search + soft formalisation approach can surpass Logic-LM on several logic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on the ability to autoformalise queries into strictly executable code; suffers when tasks require soft/fuzzy reasoning or when LLMs generate non-executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7056.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate natural-language reasoning steps from LLMs (CoT) to improve complex multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>technique (applied to multiple LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Natural-language intermediate step prompting applied to transformer LLMs; not tied to a single model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + natural-language intermediate reasoning (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought few-shot prompting (natural-language decomposition of reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as baseline across the paper's QA and logic benchmarks (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>General few-shot reasoning benchmarks (math, multi-hop, relational, logical-deductive)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning via natural-language chains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performance varies per dataset; in paper FLARE shows average +28% improvement over CoT on QA tasks and ~+10% on logic tasks (aggregate figures reported). Example: on some datasets CoT lags behind FLARE by large margins (see Table 1, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT is the baseline for many experiments; FLARE consistently outperforms CoT across most datasets in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT helps elicit reasoning steps but suffers from unfaithfulness, cascading errors, and limited ability to decompose/search/backtrack compared to program-aided or simulated-search approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Natural-language CoT traces are often unfaithful to the model's internal reasoning and struggle with rigorous multi-hop planning, backtracking, and arithmetic precision; prone to hallucinations and inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7056.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo / GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large transformer-based conversational model family used here as both a generalist reasoning model and the recommended successor to Codex for code generation in experimental baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based conversational model with documented coding capabilities (used here both for FLARE and as replacement for Codex in F-CoT comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>=100B (paper cites >=100B for GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (decoder or encoder-decoder family as per OpenAI), applied with prompting pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with CoT, F-CoT (replacing Codex), Logic-LM, and FLARE prompting pipelines; used for both code generation and simulated search.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same set of QA and logic benchmarks (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>General reasoning and strict logical inference datasets used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic/MWP, multi-hop QA, relational inference, logical deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from paper: On GSM8K with FLARE GPT-3.5 reported 68.1% (Table 3 shows plan-only vs FLARE improvements); on logic datasets ChatGPT/GPT-3.5 entries in Table 2: PrOntoQA FLARE=73.40% (FLARE_SR=79.40%), LogicalDeductions FLARE=58.60% (SR=64.43%), AR-LSAT FLARE=27.39% (SR=30.73%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>When used as the code-generation model in F-CoT (replacing Codex), GPT-3.5 still underperforms FLARE on many fuzzy/non-strict tasks; FLARE improves GPT-3.5 performance versus CoT and F-CoT in the paper's experiments (~+9% over CoT, ~+16% over F-CoT for code-tuned models aggregate reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 has stronger coding abilities than some generalist models, but FLARE still benefits GPT-3.5 by combining soft formalisation and simulated search rather than relying only on external solver execution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even when replacing Codex, code-generation for strict executability is imperfect; produced Prolog code is executable in only a portion of cases, and strict external execution can still fail if code is not exact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7056.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4o as used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's more capable LLM (gpt-4 family) evaluated in the paper; used to compare Standard, CoT, Logic-LM, FLARE, and FLARE with self-refinement across logic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity transformer family from OpenAI used as an evaluation point; applied with multiple prompting pipelines in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (large >100B typical), reported as GPT-4 family</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with Standard prompting, CoT, Logic-LM, FLARE, and FLARE with SR=2 (2 iterations of code self-refinement); used for both code generation and simulated search.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Logic benchmarks: PrOntoQA, LogicalDeductions, AR-LSAT (Table 2); also used across other QA datasets in aggregate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Formal logical reasoning benchmarks and general QA tested under different prompting paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical inference and multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 2 examples (GPT-4): PrOntoQA: Standard=77.40, CoT=98.79, Logic-LM=83.20, FLARE=98.87, FLARE_SR=99.24; LogicalDeductions: Standard=71.33, CoT=75.25, Logic-LM=87.63, FLARE=88.00, FLARE_SR=90.33; AR-LSAT: Standard=33.33, CoT=35.06, Logic-LM=43.04, FLARE=39.82, FLARE_SR=45.02.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>FLARE (and FLARE with code self-refinement) performs competitively and, in the paper, reaches or exceeds Logic-LM and CoT depending on dataset; FLARE_SR often yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-capacity models like GPT-4 show strong performance across methods; FLARE yields state-of-the-art or near-SOTA outcomes on logic benchmarks when combined with self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even strong models can exhibit gaps between producing executable code and achieving correct answers; improvement often requires iterative refinement and careful prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7056.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter family member of the Llama-3 series evaluated in this paper under FLARE, CoT and F-CoT prompting to study scale effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8-billion parameter transformer family model (Llama-3 series) evaluated as a generalist LLM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with CoT, FLARE, and F-CoT prompting; used to generate plan, code, and simulated search traces.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Various QA and logic benchmarks from the paper (see FLARE entry)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math word problems, multi-hop QA, and relation/logic tasks used to measure effect of scale.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-hop and deductive reasoning under prompting pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and internal search statistics (hops, failures, #paths)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example from Table 1 and others: Llama-3.1-8B FLARE results shown across datasets (e.g. GSM8K 72.7% in one row of Table 1), and search statistics Table 4: avg hops per path ~9.12 for correct answers, hallucination and unutilised knowledge rates in Table 6 (Hal.=63.3%, UK.=62.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>FLARE improves performance over CoT and F-CoT for this 8B model family in the paper's experiments; however absolute performance trails larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller models benefit from FLARE but show higher hallucination and unutilised-knowledge percentages; scale helps reduce hallucinations and increase faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher rates of hallucinated facts and unutilised code knowledge compared to larger models; fewer hops and search efficiency limited by capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7056.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7056.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CmDR / CmDR+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CmDR (30B) and CmDR+ (100B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cohere model family members evaluated in the paper: CmDR (30B) and a larger CmDR+ (100B); used to study how models tuned at different scales behave under FLARE and baseline prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CmDR (30B), CmDR+ (100B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs from Cohere (30B and 100B variants) evaluated as general-purpose models for reasoning with FLARE and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B (CmDR), 100B (CmDR+)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with plan-only, FLARE, CoT and F-CoT prompting; measured search statistics and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same broad benchmark suite (MWP, multi-hop QA, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks for arithmetic, multi-hop reasoning and strict logical inference to measure effect of scale and coding-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic/MWP, multi-hop QA, relational and logical inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and simulated-search statistics (avg hops, fails, hallucination %, unutilised knowledge %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: Table 3 shows CmDR_FLARE GSM8K 52.4% vs CmDR+ plan-only and FLARE values; Table 4/6 shows search stats and hallucination/UK rates: CmDR Hal.=54.7%, UK.=56.9%, CmDR+ Hal.=54.3%, UK.=56.3%; reported aggregate gains for code-tuned models relative to F-CoT and CoT in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CmDR+ (100B) shows improvements over CmDR (30B) in faithfulness and accuracy; FLARE improves both models relative to CoT and F-CoT in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling within the same family improves faithfulness and performance; FLARE yields larger relative gains for models not explicitly tuned for code, and remains competitive for code-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even the larger CmDR+ exhibits non-trivial hallucination and unutilised-knowledge rates; absolute performance remains behind strongest models like GPT-4 on some logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7056",
    "paper_id": "paper-03e0b834e4077047cc2b3426a2b52b7b368968fe",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "FLARE",
            "name_full": "Faithful Logic-Aided Reasoning and Exploration",
            "brief_description": "A modular prompting paradigm that uses an LLM to (i) generate a natural-language plan, (ii) produce a Prolog-style logic program (soft formalisation), and (iii) simulate program execution (multi-path search) with the same LLM to derive answers while enabling a measurable faithfulness check against actual Prolog execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method (applied with multiple LLMs)",
            "model_description": "Not a single model — FLARE is a prompting/method pipeline applied to various transformer LLMs to produce plans, Prolog-like code, and simulated search traces.",
            "model_size": "N/A",
            "architecture_type": "transformer-based LLM + simulated Prolog-style execution (neuro-symbolic prompting pipeline)",
            "training_data": null,
            "reasoning_method": "Plan → program (logic formalisation) → LLM-simulated search (Prolog-like traces); measures faithfulness by comparing simulated traces to real Prolog execution using ROUGE-Lsum",
            "external_tool_used": null,
            "external_tool_description": "FLARE can (optionally) execute the generated Prolog code with an external Prolog engine for validation and to obtain gold traces, but its inference pipeline primarily simulates execution with the LLM (the method does not require external solvers at inference).",
            "benchmark_name": "9 QA datasets (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR) and 3 logical inference datasets (PrOntoQA, LogicalDeductions, AR-LSAT)",
            "benchmark_description": "Mix of math word problems, multi-hop QA, relation inference and strict logical-deductive tasks (PrOntoQA: synthetic logical implications; LogicalDeductions: BIG-bench logical deduction subset; AR-LSAT: analytical reasoning).",
            "task_type": "Multi-hop logical reasoning, program-aided formalisation, deductive logical inference (first-order/Horn-clause style tasks)",
            "performance_metric": "Accuracy (%) and ROUGE-Lsum for faithfulness",
            "performance_value": "Reported: SOTA on 7/9 QA datasets (average +28% over CoT on QA in paper), base FLARE: SOTA on 2/3 logic benchmarks; with 2 iterations of code self-refinement (FLARE_SR=2) achieves SOTA on all 3 logic benchmarks. Example values (logic datasets, GPT-4): PrOntoQA FLARE=98.87% (FLARE_SR=99.24%), LogicalDeductions FLARE=88.00% (SR=90.33%), AR-LSAT FLARE=39.82% (SR=45.02%).",
            "comparison_with_baseline": "On QA: +28% average vs CoT; For code-tuned models (GPT-3.5): +16% vs F-CoT and +9% vs CoT (average reported). On logic benchmarks: FLARE ~+10% over CoT and ~+7% over Logic-LM in the paper's aggregate reporting; further improved with 2 self-refinement iterations.",
            "key_findings": "Simulating logic-program execution with LLMs (soft formalisation + simulated search) yields large improvements over CoT and F-CoT on many QA tasks and strong gains on logical inference benchmarks; faithfulness of simulated traces (measured by ROUGE-Lsum against actual Prolog traces) correlates positively with accuracy; combining soft reasoning and formalisation allows handling tasks that are hard to auto-formalise for strict solvers.",
            "limitations": "Quality depends on the LLM's ability to produce correct plans and code; generated code is executable by Prolog only ~67% on average (&gt;=50% worst-case); simulated execution may miss parts of very large or open-ended search spaces; prompt sensitivity and errors in formalisation propagate; strict external-solvers can still outperform when a perfect formalisation exists, and FLARE trades strict executability for soft/fuzzy formalisation capacity.",
            "uuid": "e7056.0",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "F-CoT",
            "name_full": "Faithful Chain-of-Thought (F-CoT)",
            "brief_description": "A neuro-symbolic approach that prompts models to generate code (programs) corresponding to reasoning traces and executes that code in an external symbolic engine to obtain faithful outputs.",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "use",
            "model_name": "method (applied with various LLMs in comparisons)",
            "model_description": "Neuro-symbolic pipeline: LLM produces executable code; external symbolic executor runs code to produce answers (original F-CoT used Codex for code generation).",
            "model_size": "N/A",
            "architecture_type": "transformer + external symbolic executor (program execution)",
            "training_data": null,
            "reasoning_method": "Program synthesis (autoformalisation) followed by external execution to ensure faithfulness",
            "external_tool_used": true,
            "external_tool_description": "Delegates reasoning to an external symbolic solver (e.g., Python/Prolog) that executes synthesized programs; in prior work Codex + executor used, in this paper replaced Codex with GPT-3.5 for experiments.",
            "benchmark_name": "Same benchmarks used for comparison (MWP, multi-hop QA, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)",
            "benchmark_description": "Programmatic formalisation evaluated on math, multi-hop, and logical reasoning tasks.",
            "task_type": "Program-executable formal reasoning (strict logical execution)",
            "performance_metric": "Accuracy (%)",
            "performance_value": "Often strong on problems that are straightforward to formalise and execute; in this paper F-CoT underperforms on many non-strict or fuzzy tasks and can fail catastrophically for LLMs not tuned for coding (numerical examples: F-CoT results often much lower than FLARE and CoT on StrategyQA, Sports, CLUTRR in Table 1).",
            "comparison_with_baseline": "F-CoT can yield higher faithfulness in prior work, but in this paper F-CoT underperforms FLARE on many benchmarks — LLMs not tuned for code produce many unexecutable programs leading to failures (paper reports FLARE is better than F-CoT across most evaluated LLMs; for code-tuned models FLARE still outperforms F-CoT by ~16% average).",
            "key_findings": "Using an external executor enforces faithful execution but requires consistent generation of executable code; models not explicitly trained for code struggle with F-CoT, causing execution errors and performance collapse on tasks needing fuzzy commonsense reasoning.",
            "limitations": "Requires executable, syntactically correct code; brittle when LLM cannot reliably generate code matching the expected formalisation; cannot handle fuzzy/soft reasoning that doesn't map cleanly to deterministic solvers.",
            "uuid": "e7056.1",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM",
            "brief_description": "A method that uses LLMs to formalise natural language queries into code and delegates reasoning to symbolic solvers, aimed at improving faithfulness in logical reasoning.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "use",
            "model_name": "method (compared in experiments)",
            "model_description": "Pipeline: LLM formalisation into programmatic logic + external symbolic solver for execution to produce faithful logical reasoning outputs.",
            "model_size": "N/A",
            "architecture_type": "transformer + symbolic solver integration",
            "training_data": null,
            "reasoning_method": "Program synthesis into formal logic and external solver execution",
            "external_tool_used": true,
            "external_tool_description": "Integrates symbolic solvers to execute the formalised programs and obtain faithful traces/answers (paper compares FLARE to Logic-LM on logic inference benchmarks).",
            "benchmark_name": "PrOntoQA, LogicalDeductions, AR-LSAT (logic inference benchmarks)",
            "benchmark_description": "Benchmarks with strictly formal logical deduction and inference tasks (deductive/analytical reasoning).",
            "task_type": "Strict logical deduction via solver-executed formal programs",
            "performance_metric": "Accuracy (%)",
            "performance_value": "Reported in comparisons: FLARE outperforms Logic-LM by ~7% on aggregate logic benchmarks in this paper; specific Logic-LM numbers in Table 2 (e.g., GPT-4 Logic-LM on PrOntoQA=83.20% vs FLARE=98.87%).",
            "comparison_with_baseline": "In this paper FLARE &gt; Logic-LM on the evaluated logic benchmarks; Logic-LM improves faithfulness relative to plain CoT but is limited by strict executability requirements on fuzzy tasks.",
            "key_findings": "Delegating to symbolic solvers improves faithfulness for strictly formal tasks but loses flexibility for fuzzy/commonsense reasoning; FLARE's simulated-search + soft formalisation approach can surpass Logic-LM on several logic benchmarks.",
            "limitations": "Relies on the ability to autoformalise queries into strictly executable code; suffers when tasks require soft/fuzzy reasoning or when LLMs generate non-executable code.",
            "uuid": "e7056.2",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate natural-language reasoning steps from LLMs (CoT) to improve complex multi-step problem solving.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "technique (applied to multiple LLMs)",
            "model_description": "Natural-language intermediate step prompting applied to transformer LLMs; not tied to a single model architecture.",
            "model_size": "N/A",
            "architecture_type": "transformer + natural-language intermediate reasoning (CoT)",
            "training_data": null,
            "reasoning_method": "Chain-of-Thought few-shot prompting (natural-language decomposition of reasoning)",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "Used as baseline across the paper's QA and logic benchmarks (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)",
            "benchmark_description": "General few-shot reasoning benchmarks (math, multi-hop, relational, logical-deductive)",
            "task_type": "Multi-step reasoning via natural-language chains",
            "performance_metric": "Accuracy (%)",
            "performance_value": "Performance varies per dataset; in paper FLARE shows average +28% improvement over CoT on QA tasks and ~+10% on logic tasks (aggregate figures reported). Example: on some datasets CoT lags behind FLARE by large margins (see Table 1, Table 2).",
            "comparison_with_baseline": "CoT is the baseline for many experiments; FLARE consistently outperforms CoT across most datasets in this study.",
            "key_findings": "CoT helps elicit reasoning steps but suffers from unfaithfulness, cascading errors, and limited ability to decompose/search/backtrack compared to program-aided or simulated-search approaches.",
            "limitations": "Natural-language CoT traces are often unfaithful to the model's internal reasoning and struggle with rigorous multi-hop planning, backtracking, and arithmetic precision; prone to hallucinations and inconsistency.",
            "uuid": "e7056.3",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (gpt-3.5-turbo / GPT-3.5 family)",
            "brief_description": "OpenAI's large transformer-based conversational model family used here as both a generalist reasoning model and the recommended successor to Codex for code generation in experimental baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Large transformer-based conversational model with documented coding capabilities (used here both for FLARE and as replacement for Codex in F-CoT comparisons).",
            "model_size": "&gt;=100B (paper cites &gt;=100B for GPT-3.5)",
            "architecture_type": "transformer (decoder or encoder-decoder family as per OpenAI), applied with prompting pipelines",
            "training_data": null,
            "reasoning_method": "Evaluated with CoT, F-CoT (replacing Codex), Logic-LM, and FLARE prompting pipelines; used for both code generation and simulated search.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Same set of QA and logic benchmarks (GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sport, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)",
            "benchmark_description": "General reasoning and strict logical inference datasets used for comparison.",
            "task_type": "Arithmetic/MWP, multi-hop QA, relational inference, logical deduction",
            "performance_metric": "Accuracy (%)",
            "performance_value": "Examples from paper: On GSM8K with FLARE GPT-3.5 reported 68.1% (Table 3 shows plan-only vs FLARE improvements); on logic datasets ChatGPT/GPT-3.5 entries in Table 2: PrOntoQA FLARE=73.40% (FLARE_SR=79.40%), LogicalDeductions FLARE=58.60% (SR=64.43%), AR-LSAT FLARE=27.39% (SR=30.73%).",
            "comparison_with_baseline": "When used as the code-generation model in F-CoT (replacing Codex), GPT-3.5 still underperforms FLARE on many fuzzy/non-strict tasks; FLARE improves GPT-3.5 performance versus CoT and F-CoT in the paper's experiments (~+9% over CoT, ~+16% over F-CoT for code-tuned models aggregate reported).",
            "key_findings": "GPT-3.5 has stronger coding abilities than some generalist models, but FLARE still benefits GPT-3.5 by combining soft formalisation and simulated search rather than relying only on external solver execution.",
            "limitations": "Even when replacing Codex, code-generation for strict executability is imperfect; produced Prolog code is executable in only a portion of cases, and strict external execution can still fail if code is not exact.",
            "uuid": "e7056.4",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt-4o as used in paper)",
            "brief_description": "OpenAI's more capable LLM (gpt-4 family) evaluated in the paper; used to compare Standard, CoT, Logic-LM, FLARE, and FLARE with self-refinement across logic benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4o)",
            "model_description": "High-capacity transformer family from OpenAI used as an evaluation point; applied with multiple prompting pipelines in the study.",
            "model_size": "Not specified in paper (large &gt;100B typical), reported as GPT-4 family",
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Evaluated with Standard prompting, CoT, Logic-LM, FLARE, and FLARE with SR=2 (2 iterations of code self-refinement); used for both code generation and simulated search.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Logic benchmarks: PrOntoQA, LogicalDeductions, AR-LSAT (Table 2); also used across other QA datasets in aggregate experiments.",
            "benchmark_description": "Formal logical reasoning benchmarks and general QA tested under different prompting paradigms.",
            "task_type": "Deductive logical inference and multi-step reasoning",
            "performance_metric": "Accuracy (%)",
            "performance_value": "Table 2 examples (GPT-4): PrOntoQA: Standard=77.40, CoT=98.79, Logic-LM=83.20, FLARE=98.87, FLARE_SR=99.24; LogicalDeductions: Standard=71.33, CoT=75.25, Logic-LM=87.63, FLARE=88.00, FLARE_SR=90.33; AR-LSAT: Standard=33.33, CoT=35.06, Logic-LM=43.04, FLARE=39.82, FLARE_SR=45.02.",
            "comparison_with_baseline": "FLARE (and FLARE with code self-refinement) performs competitively and, in the paper, reaches or exceeds Logic-LM and CoT depending on dataset; FLARE_SR often yields further gains.",
            "key_findings": "High-capacity models like GPT-4 show strong performance across methods; FLARE yields state-of-the-art or near-SOTA outcomes on logic benchmarks when combined with self-refinement.",
            "limitations": "Even strong models can exhibit gaps between producing executable code and achieving correct answers; improvement often requires iterative refinement and careful prompting.",
            "uuid": "e7056.5",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3.1-8B",
            "name_full": "Llama 3.1 (8B)",
            "brief_description": "An 8B-parameter family member of the Llama-3 series evaluated in this paper under FLARE, CoT and F-CoT prompting to study scale effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_description": "8-billion parameter transformer family model (Llama-3 series) evaluated as a generalist LLM in experiments.",
            "model_size": "8B",
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Evaluated with CoT, FLARE, and F-CoT prompting; used to generate plan, code, and simulated search traces.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Various QA and logic benchmarks from the paper (see FLARE entry)",
            "benchmark_description": "Math word problems, multi-hop QA, and relation/logic tasks used to measure effect of scale.",
            "task_type": "Multi-hop and deductive reasoning under prompting pipelines",
            "performance_metric": "Accuracy (%) and internal search statistics (hops, failures, #paths)",
            "performance_value": "Example from Table 1 and others: Llama-3.1-8B FLARE results shown across datasets (e.g. GSM8K 72.7% in one row of Table 1), and search statistics Table 4: avg hops per path ~9.12 for correct answers, hallucination and unutilised knowledge rates in Table 6 (Hal.=63.3%, UK.=62.9%).",
            "comparison_with_baseline": "FLARE improves performance over CoT and F-CoT for this 8B model family in the paper's experiments; however absolute performance trails larger models.",
            "key_findings": "Smaller models benefit from FLARE but show higher hallucination and unutilised-knowledge percentages; scale helps reduce hallucinations and increase faithfulness.",
            "limitations": "Higher rates of hallucinated facts and unutilised code knowledge compared to larger models; fewer hops and search efficiency limited by capacity.",
            "uuid": "e7056.6",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CmDR / CmDR+",
            "name_full": "CmDR (30B) and CmDR+ (100B)",
            "brief_description": "Cohere model family members evaluated in the paper: CmDR (30B) and a larger CmDR+ (100B); used to study how models tuned at different scales behave under FLARE and baseline prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CmDR (30B), CmDR+ (100B)",
            "model_description": "Transformer-based LLMs from Cohere (30B and 100B variants) evaluated as general-purpose models for reasoning with FLARE and baselines.",
            "model_size": "30B (CmDR), 100B (CmDR+)",
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Evaluated with plan-only, FLARE, CoT and F-CoT prompting; measured search statistics and faithfulness.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Same broad benchmark suite (MWP, multi-hop QA, CLUTRR, PrOntoQA, LogicalDeductions, AR-LSAT)",
            "benchmark_description": "Benchmarks for arithmetic, multi-hop reasoning and strict logical inference to measure effect of scale and coding-tuning.",
            "task_type": "Arithmetic/MWP, multi-hop QA, relational and logical inference",
            "performance_metric": "Accuracy (%) and simulated-search statistics (avg hops, fails, hallucination %, unutilised knowledge %)",
            "performance_value": "Examples: Table 3 shows CmDR_FLARE GSM8K 52.4% vs CmDR+ plan-only and FLARE values; Table 4/6 shows search stats and hallucination/UK rates: CmDR Hal.=54.7%, UK.=56.9%, CmDR+ Hal.=54.3%, UK.=56.3%; reported aggregate gains for code-tuned models relative to F-CoT and CoT in the paper.",
            "comparison_with_baseline": "CmDR+ (100B) shows improvements over CmDR (30B) in faithfulness and accuracy; FLARE improves both models relative to CoT and F-CoT in many tasks.",
            "key_findings": "Scaling within the same family improves faithfulness and performance; FLARE yields larger relative gains for models not explicitly tuned for code, and remains competitive for code-tuned models.",
            "limitations": "Even the larger CmDR+ exhibits non-trivial hallucination and unutilised-knowledge rates; absolute performance remains behind strongest models like GPT-4 on some logic tasks.",
            "uuid": "e7056.7",
            "source_info": {
                "paper_title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 1
        }
    ],
    "cost": 0.020554499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FLARE: Faithful Logic-Aided Reasoning and Exploration</h1>
<p>Erik Arakelyan ${ }^{115}$ Pasquale Minervini ${ }^{23}$ Pat Verga ${ }^{4}$ Patrick Lewis ${ }^{4}$ Isabelle Augenstein ${ }^{1}$<br>${ }^{1}$ University of Copenhagen ${ }^{2}$ University of Edinburgh<br>${ }^{3}$ Miniml.AI ${ }^{4}$ Cohere ${ }^{5}$ NVIDIA<br>earakelyan@nvidia.com augenstein@di.ku.dk p.minervini@ed.ac.uk<br>{pat, patrick}@cohere.com</p>
<h4>Abstract</h4>
<p>Modern Question Answering (QA) and Reasoning approaches with Large Language Models (LLMs) commonly use Chain-of-Thought (CoT) prompting but struggle with generating outputs faithful to their intermediate reasoning chains. While neuro-symbolic methods like Faithful CoT (F-CoT) offer higher faithfulness through external solvers, they require codespecialized models and struggle with ambiguous tasks. We introduce Faithful Logic-Aided Reasoning and Exploration (FLARE), which uses LLMs to plan solutions, formalize queries into logic programs, and simulate code execution through multi-hop search without external solvers. Our method achieves SOTA results on 7 out of 9 diverse reasoning benchmarks and 3 out of 3 logic inference benchmarks while enabling measurement of reasoning faithfulness. We demonstrate that model faithfulness correlates with performance and that successful reasoning traces show an $18.1 \%$ increase in unique emergent facts, $8.6 \%$ higher overlap between code-defined and execution-trace relations, and $3.6 \%$ reduction in unused relations.</p>
<h2>1 Introduction</h2>
<p>Complex Reasoning in natural Question Answering (QA) tasks requires exploring a problem space with formalized facts, relations, commonsense knowledge and logical implications. In line with this, LLMs have been enhanced with CoT (Wei et al., 2022) prompting, which supplements the QA process by generating intermediate reasoning chains given a set of in-context examples (Brown et al., 2020a), as shown in fig. 1. This allowed for advancement in commonsense (Madaan et al., 2022), symbolic (Wang et al., 2022; Sprague et al., 2024) and mathematical (Jie et al., 2023) reasoning. Although CoT allows for a problem exploration in natural language steps, such an approach</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>has been shown to cause performance degradation for reasoning tasks involving multi-step planning (Valmeekam et al., 2022; Suzgun et al., 2023), problem exploration (Yao et al., 2022), and arithmetic tasks (Hendrycks et al., 2021a; Madaan and Yazdanbakhsh, 2022a). These discrepancies arise as CoT suffers from a limited ability to decompose, search, verify and backtrack using intermediate rationale chains (Yao et al., 2022), cascading hallucinations and errors (Ling et al., 2023) and that natural language might not be an optimal representation for describing the reasoning process ( Li et al., 2024). Simultaneously, LLM output has been shown to be unfaithful and inconsistent w.r.t. the intermediate CoT rationale (Jacovi et al., 2024; Lanham et al., 2023b; Turpin et al., 2023).</p>
<p>To mitigate the problem of CoT faithfulness and allow for more robust reasoning during QA, Lyu et al. (2023, Faithful CoT) and Logic-LM (Pan et al., 2023) suggested generating code which is further executed using an external symbolic solver. Producing and executing code enables the generation of outputs guided by external solvers, leveraging search with backtracking to explore the problem space effectively. However, strict translations of natural language queries into code, such as autoformalisation (Szegedy, 2020; Wang et al., 2018), is a non-trivial task involving direct inference of implicit commonsense and domain-specific knowledge and the ability to align abstract and informal concepts directly to constrained formal definitions for further execution (Wu et al., 2022). An example query, "Do all parts of the aloe vera plant taste good?", is challenging to formalize or address with a strict algorithmic solution, as it requires interpretative, deductive and context-dependent reasoning, referred to as soft or fuzzy reasoning. Using external solvers makes such fuzzy reasoning impossible and requires consistently generating syntactically correct executable code. While some LLMs have coding capabilities stemming from their pretraining</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A depiction of the <em>plan</em>, <em>code</em> and simulated <em>search</em> in FLARE. Each module is generated separately and iteratively, allowing us to obtain the final answer. The green and yellow highlighted text shows the overlap between the facts and the relations between the code and the simulated search.</p>
<p><em>Jiang et al. (2024); Aryabumi et al. (2024)</em>, relative code consistency is more probable with models explicitly trained for coding <em>Chen et al. (2021)</em>.</p>
<p>To overcome these problems, we propose Faithful Logic-Aided Reasoning and Exploration (FLARE), an interpretable method that allows for planning, fuzzy reasoning, and traversing the problem space with backtracking, exact task decomposition, and measuring faithfulness. In FLARE, given a natural language query, we prompt an LLM to sequentially generate a <em>plan</em> that includes an analysis and the logical steps necessary for formalising and answering the question, a logic programming <em>Wielemaker et al. (2012)</em> <em>code</em> that allows formalising the query into a set of facts, relations and their composition forming the space for exploring that query and the <em>search</em>, which is an LLM-generated code execution simulation. An illustration of FLARE can be seen in fig. 1. This work focuses on models that <strong>have not</strong> been explicitly trained on CoT on other reasoning traces, as these models have been shown to struggle with generalisation towards differing reasoning paradigms <em>Chen et al. (2024)</em>, consistency in intermediate reasoning steps <em>Wang et al. (2025)</em> and instruction following <em>Zhang et al. (2025)</em>. In our framework, the generated code must not be consistently executable by an external solver, allowing for the soft-formalisation of natural language. Although we see that even generalist LLMs are able to produce executable code in ≥ 50% of cases. FLARE allows us to measure the faithfulness of the outcome w.r.t. the simulated code execution by directly comparing the search paths produced by the external solver to that LLM generation. This comparison also allows for pinpointing model hallucinations and inconsistencies. We systematically study the effectiveness of our method using 4 general-purpose LLMs of varying scales across 9 diverse QA and 3 logical inference benchmarks, covering Math World Problems, Multi-hop QA, Relation inference, deductive and analytical reasoning and show that our method achieves state-of-the-art results in 7 out of 9 QA datasets and 2 out of 3 logic datasets in comparison to CoT, F-CoT and Logic-LM. We also show that the method is competitive for models tuned for coding, with an average overall increase of 16% over F-Cot and 10% over CoT. Our key contributions are the following: <strong>(i)</strong> We introduce FLARE, a novel paradigm for logic-aided and interpretable formalisation and search over the problem space in QA and logic reasoning tasks. <strong>(ii)</strong> We perform a systematic evaluation across 9 QA and 3 logical inference benchmarks and 4 models of varying scales, showing the advantages of using FLARE for QA in a few-shot setup over prior approaches. <strong>(iii)</strong> The modularity of FLARE allows defining a simple ingrained method for measuring model faithfulness, which is further shown to be strongly correlated with performance. <strong>(iv)</strong> We further show that using FLARE allows us to interpretably and rigorously detect hallucinations along with sub-optimal and inconsistent reasoning patterns.</p>
<h2>2 Related Work</h2>
<h3>Reasoning in Natural Language</h3>
<p>Reasoning in Natural Language Few-shot prompting <em>Brown et al. (2020b)</em> improves LLM reasoning, and extensions like Chain-of-Thought (CoT)<em>Wei et al. (2022)</em>, "think step by step" <em>Kojima et al. (2022)</em>, and Least-to-</p>
<p>Most (Zhou et al., 2023) explicitly decompose problems into intermediate steps. Despite their promise, these methods exhibit arithmetic errors (Lewkowycz et al., 2022; Hendrycks et al., 2021b) and logical inconsistencies (Madaan and Yazdanbakhsh, 2022b). Planning-based variants introduce a separate plan-execute loop (Yao et al., 2023b; Wang et al., 2023a). The plan stage in FLARE draws on these ideas but focuses on generating a natural-language strategy for later formalisation into code.</p>
<p>Reasoning with Search Recent work augments LLM reasoning by explicitly searching the problem space. Self-consistency decoding (Wang et al., 2023b) samples multiple chains of thought and selects the majority answer, while Tree-of-Thoughts (ToT; Yao et al., 2023a) performs tree-structured exploration with LLM-evaluated states. Later methods adapt classical search-DFS, BFS (Besta et al., 2024), A"(Lehnert et al., 2024), and hybrids(Gandhi et al., 2024)—via direct tuning, imitation learning (Yang et al., 2022), or few-shot prompting (Zhang et al., 2024). So far, evaluations focus on toy puzzle and algorithmic domains such as the 24 Game, Countdown, Sorting, mazes, and Sokoban (Yang et al., 2022; Wikipedia, 2024; Besta et al., 2024; Lehnert et al., 2024). Although the search module of FLARE shares this multi-path exploration spirit, it targets more general tasks and yields interpretable multi-hop reasoning via simulated code execution.</p>
<p>Reasoning with Formalisation Another research direction explores formalising natural language queries into code (Gao et al., 2023; Li et al., 2024) or pseudo-code (Chae et al., 2024; Gandhi et al., 2024). This enables translating queries into strict structures, delegating reasoning and search to deterministic solvers such as Python (Chen et al., 2023), PDDL (Lyu et al., 2023; Liu et al., 2023), or DataLog (Lyu et al., 2023). Models can synthesize programs (Austin et al., 2021; Nijkamp et al., 2023) and benefit from code in numerical and algorithmic reasoning (Chen et al., 2023; Gao et al., 2023), yet their use for general QA remains underexplored. This is due to the challenge of translating natural language into strictly executable code (Wu et al., 2022), the syntactic rigidity of underrepresented programming languages during pre-training (Liu et al., 2024), and the need for models explicitly tuned for coding (Chen et al., 2021). Additionally, relying on external solvers restricts soft reasoning over commonsense knowledge and implications. In FLARE, we formalise queries as logic programs in Prolog during the code generation step but do not require executability or external solvers at inference. This allows LLMs to simulate code execution via soft reasoning over logic-based traversals-similar to Prolog-while avoiding the need for code-specific tuning.</p>
<p>Reasoning Faithfulness An explanation is considered faithful if it explicitly and accurately describes the reasoning process of the model during inference (Gilpin et al., 2018; Jacovi and Goldberg, 2020). In the context of prompting techniques such as CoT, we are interested in the faithfulness of the intermediate reasoning chains towards the final output. Faithful intermediate reasoning chains should not just look plausible (Herman, 2017) but have exact reflections of the problem exploration and reasoning used to arrive at the final answer. Natural language reasoning chains prevalent in CoT and similar methods are shown to be unfaithful, either masking the reasoning biases (Turpin et al., 2023) of the model or outright ignoring the intermediate reasoning (Lanham et al., 2023a). In FLARE, we introduce a method to seamlessly measure the faithfulness of the final outcome w.r.t. completed search.</p>
<h2>3 Methodology</h2>
<h3>3.1 LLM-Simulated Search</h3>
<p>FLARE comprises three modules for generating a plan, code and simulated search for answering a natural language query $\mathcal{Q}=\left{T_{1}^{\mathcal{Q}} \ldots T_{|\mathcal{Q}|}^{\mathcal{Q}}\right}$, where each $T_{i}^{\mathcal{Q}}$ is a token in the query $\mathcal{Q}$.</p>
<p>Generating A Plan For each query $\mathcal{Q}$, given an LLM $\mathcal{M}$, we initially use instructions $\mathcal{I}^{\mathcal{P}}$ to prompt it to generate a plan $\mathcal{P}$, which should be comprised of task explanation, analysis and a plan for further formalising the query. An example of this can be seen in the plan section in fig. 1. We use in-context few shot examples $\mathcal{E}_{\mathcal{P}}$ of such plan generations for obtaining the final plan:</p>
<p>$$
\mathcal{P}<em _mathcal_M="\mathcal{M">{i} \sim p</em>\right)
$$}}\left(T_{i}^{\mathcal{P}} \mid T_{: i-1}^{\mathcal{P}}, \mathcal{E}_{\mathcal{P}}, \mathcal{Q}, \mathcal{I}^{\mathcal{P}</p>
<p>where $\mathcal{P}<em i="i">{i}$ and $T</em>$.}^{\mathcal{P}}$ is the $i$-th token in the generated plan $\mathcal{P}$ and $p_{\mathcal{M}}$ is the probability of the next token over the vocabulary obtained from model $\mathcal{M</p>
<p>Generating Code After generating the plan, we use instructions $\mathcal{I}^{\mathcal{C}}$ to prompt the LLM $\mathcal{M}$ to generate a Prolog code $\mathcal{C}$, an example of which can</p>
<table>
<thead>
<tr>
<th></th>
<th>Math Word Problems</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Multi-hop QA</th>
<th></th>
<th></th>
<th>Relation</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Method</td>
<td>GSM8K</td>
<td>SVAMP</td>
<td>MultiArith</td>
<td>ASDiv</td>
<td>AQuA</td>
<td>StrategyQA</td>
<td>Date</td>
<td>Sport</td>
<td>CLUTRR</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>72.7</td>
<td>86.0</td>
<td>96.3</td>
<td>83.1</td>
<td>62.9</td>
<td>70.2</td>
<td>59.3</td>
<td>76.6</td>
<td>36.8</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>12.2</td>
<td>53.2</td>
<td>0</td>
<td>0</td>
<td>32</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>85.2</td>
<td>82.4</td>
<td>91.6</td>
<td>79.1</td>
<td>51.6</td>
<td>43.5</td>
<td>74.1</td>
<td>89.4</td>
<td>45.7</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{\text {FLARE }}$</td>
<td>52.4</td>
<td>74.0</td>
<td>84.5</td>
<td>72.2</td>
<td>43.7</td>
<td>67.0</td>
<td>52.3</td>
<td>78.9</td>
<td>29.1</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{\text {F- }}$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>59.7</td>
<td>0</td>
<td>0</td>
<td>8.6</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{\mathrm{CoT}}$</td>
<td>46.5</td>
<td>57.3</td>
<td>83.1</td>
<td>37.2</td>
<td>28.3</td>
<td>21.3</td>
<td>47.4</td>
<td>55.2</td>
<td>29.5</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{+}$FLARE</td>
<td>71.4</td>
<td>83.5</td>
<td>90.4</td>
<td>81.3</td>
<td>55.9</td>
<td>70.8</td>
<td>61.8</td>
<td>77.7</td>
<td>41.0</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{+}$FLO</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>15.4</td>
<td>57.6</td>
<td>0</td>
<td>0</td>
<td>35.3</td>
</tr>
<tr>
<td>$\mathrm{CmDR}_{+}$</td>
<td>48.7</td>
<td>81.1</td>
<td>86.6</td>
<td>44.6</td>
<td>44.1</td>
<td>48.4</td>
<td>79.1</td>
<td>62.6</td>
<td>42.5</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>82.1</td>
<td>82.7</td>
<td>98.3</td>
<td>85.4</td>
<td>55.1</td>
<td>65.5</td>
<td>82.4</td>
<td>85.6</td>
<td>49.8</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>75.8</td>
<td>83.0</td>
<td>95.3</td>
<td>81.7</td>
<td>53.5</td>
<td>51.5</td>
<td>73.5</td>
<td>52.3</td>
<td>12.1</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>79.8</td>
<td>82.4</td>
<td>98.2</td>
<td>75.8</td>
<td>59.4</td>
<td>51.7</td>
<td>69.9</td>
<td>95.8</td>
<td>4.3</td>
</tr>
</tbody>
</table>
<p>Table 1: The following table shows the performance of each of the tested models given a technique for reasoning. Each bold, underlined, and italicised element highlights the best, second best and worst technique per specific model. The overall best method per dataset is highlighted in green.
be seen in fig. 1. We append executable code generation samples $\mathcal{C}<em _mathcal_P="\mathcal{P">{\text {sample }}$ to the previous in-context examples $\mathcal{E}</em>}}$ and obtain few-shot code generation demonstrations $\mathcal{E<em _mathcal_P="\mathcal{P">{\mathcal{C}}=\left[\mathcal{E}</em>\right]$}} ; \mathcal{C}_{\text {sample }</p>
<p>$$
\begin{aligned}
&amp; \mathcal{C}<em _mathcal_M="\mathcal{M">{i} \sim p</em>}}\left(T_{i}^{\mathcal{S}} \mid T_{; i-1}^{\mathcal{S}} \mathcal{E<em _code="{code" _text="\text">{\mathcal{C}}, \mathcal{Q}, \mathcal{I}^{\mathcal{P}}, \mathcal{P}, \mathcal{I}^{\mathcal{C}}\right) \
&amp; \mathcal{F}</em>}}, \mathcal{R<em _code="{code" _text="\text">{\text {code }}, \mathcal{G}</em>\right)
\end{aligned}
$$}}=\operatorname{EXTRACT}\left(\mathcal{C}_{i</p>
<p>where $\mathcal{C}<em i="i">{i}$ and $T</em>$. We detail the benefits of Prolog and the reasoning behind our choice in section A.5.}^{\mathcal{C}}$ is the $i$-th token in the generated code $\mathcal{C</p>
<p>Simulating Search After generating the logicprogramming code, we want to simulate program execution by generating a problem space traversal trace with our LLM $\mathcal{M}$. We use instructions $\mathcal{I}^{\mathcal{S}}$ and update our in-context samples by appending search traces $\mathcal{S}<em _sample="{sample" _text="\text">{\text {sample }}$ constructed from Prolog execution of sample codes $\mathcal{C}</em>}}$, i.e. $\mathcal{E<em _mathcal_C="\mathcal{C">{\mathcal{S}}=$ $\left[\mathcal{E}</em>\right]$ :}} ; \mathcal{S}_{\text {sample }</p>
<p>$$
\begin{gathered}
\mathcal{S}<em _mathcal_M="\mathcal{M">{i} \sim p</em>}}\left(T_{i}^{\mathcal{S}} \mid T_{; i-1}^{\mathcal{S}} \mathcal{E<em _search="{search" _text="\text">{\mathcal{C}}, \mathcal{Q}, \mathcal{I}^{\mathcal{P}}, \mathcal{P}, \mathcal{I}^{\mathcal{C}}, \mathcal{C}, \mathcal{I}^{\mathcal{S}}\right) \
\mathcal{A}</em>}}, \mathcal{F<em _search="{search" _text="\text">{\text {search }}, \mathcal{R}</em>\right)
\end{gathered}
$$}}=\operatorname{EXTRACT}\left(\mathcal{S}_{i</p>
<p>where $T_{i}^{\mathcal{S}}$ is the $i$-th token in the generated search trace $\mathcal{S}$. During iterative problem space traversal, we can segment the facts $\mathcal{F}<em _search="{search" _text="\text">{\text {search }}$, relations $\mathcal{R}</em>}}$, completed and backtracked paths with their answers $\mathcal{A<em _sample="{sample" _text="\text">{\text {search }}$ used during the search simulation. To get the final answer we update in-context samples with their correct final answers $\mathcal{A}</em>}}$ from the executed search $\mathcal{S<em _mathcal_A="\mathcal{A">{\text {sample }}, \mathcal{E}</em>}}=\left[\mathcal{E<em _sample="{sample" _text="\text">{\mathcal{S}} ; \mathcal{A}</em>\right]$
and use instructions $\mathcal{I}^{\mathcal{A}}$ to obtain the final answer from the model.}</p>
<p>$$
\begin{gathered}
\mathcal{A}<em _mathcal_M="\mathcal{M">{\text {Final }} \sim p</em>\right. \
\left.\mathcal{I}^{\mathcal{P}}, \mathcal{P}, \mathcal{I}^{\mathcal{C}}, \mathcal{C}, \mathcal{I}^{\mathcal{S}}, \mathcal{S}, \mathcal{I}^{\mathcal{A}}\right)
\end{gathered}
$$}}\left(T_{i}^{\mathcal{A}} \mid T_{; i-1}^{\mathcal{A}} \mathcal{E}_{\mathcal{C}}, \mathcal{Q</p>
<p>The prompts used for generating each part in FLARE can be seen in section A. 1 along with a complete example in table 9 and a pseudo-code in section A.5.</p>
<h3>3.2 Detecting Reasoning Inconsistencies</h3>
<p>For each query $\mathcal{Q}$ given the code $\mathcal{C}$ and the simulated search $\mathcal{S}$ along with the extracted facts $\mathcal{F}<em _search="{search" _text="\text">{\text {code }}, \mathcal{F}</em>}}$ and relations $\mathcal{R<em _search="{search" _text="\text">{\text {code }}, \mathcal{R}</em>$ from each designated module, we aim to detect the inconsistencies during the reasoning process of the LLM. We use exact string matching between all these facts and relations in code and simulated search.}</p>
<p>$$
\begin{array}{rll}
\forall i, \exists j &amp; \text { such that } &amp; \mathcal{F}<em _search="{search" _text="\text">{\text {code }}^{i}=\mathcal{F}</em> \
\text { and } &amp; \forall v, \exists q &amp; \mathcal{R}}}^{j<em _search="{search" _text="\text">{\text {code }}^{v}=\mathcal{R}</em> \
\forall j, \exists i &amp; \text { such that } &amp; \mathcal{F}}}^{q<em _search="{search" _text="\text">{\text {code }}^{i}=\mathcal{F}</em> \
\text { and } &amp; \forall q, \exists v &amp; \mathcal{R}}}^{j<em _search="{search" _text="\text">{\text {code }}^{v}=\mathcal{R}</em>
\end{array}
$$}}^{q</p>
<p>With this framework in mind, we define two reasoning failure modes. In the first failure mode, given that some fact or relation was used in the simulated search but did not exist in the generated code, i.e. $\exists j$ such that $\mathcal{F}<em _code="{code" _text="\text">{\text {search }}^{j} \notin \mathcal{F}</em>$, we claim that the LLM has hallucinated. We postulate that the model either produced incomplete knowledge}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The trend of mean model accuracy w.r.t mean faithfulness for all the models.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ChatGPT (gpt-3.5-turbo)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>GPT-4 (gpt-4o)</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Standard</td>
<td>CoT</td>
<td>Logic-LM</td>
<td>FLARE</td>
<td>FLARE_{SR=2}</td>
<td>Standard</td>
<td>CoT</td>
<td>Logic-LM</td>
<td>FLARE</td>
<td>FLARE_{SR=2}</td>
</tr>
<tr>
<td>PrOntoQA</td>
<td>47.40</td>
<td>67.80</td>
<td>61.00</td>
<td>73.40</td>
<td>79.40</td>
<td>77.40</td>
<td>98.79</td>
<td>83.20</td>
<td>98.87</td>
<td>99.24</td>
</tr>
<tr>
<td>LogicalDeduction</td>
<td>40.00</td>
<td>42.33</td>
<td>65.67</td>
<td>58.60</td>
<td>64.43</td>
<td>71.33</td>
<td>75.25</td>
<td>87.63</td>
<td>88.00</td>
<td>90.33</td>
</tr>
<tr>
<td>AR-LSAT</td>
<td>20.34</td>
<td>17.31</td>
<td>26.41</td>
<td>27.39</td>
<td>30.73</td>
<td>33.33</td>
<td>35.06</td>
<td>43.04</td>
<td>39.82</td>
<td>45.02</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of results across datasets for ChatGPT (gpt-3.5-turbo) and GPT-4 (gpt-4o) using Standard, CoT, Logic-LM, FLARE, and FLARE_{SR=2} approaches. <em>SR</em>=2 refers to a maximum of 2 iterations of code self-refinement.</p>
<p>during formalisation to <em>code</em> or created a piece of non-existing information during the <em>search</em>. We do not consider facts that emerged during a direct inference step within the simulated search during our calculation. For example, if we are dealing with a mathematical query 4 · (5 + 6) =?, the search would involve separately evaluating the expression 5 + 6 = 11. In this case, 11 will not be treated as a hallucinated fact within the search but rather as an emergent fact obtained from direct inference. The <em>second</em> failure mode is the reciprocal case, where a fact or relation present in the <em>code</em> is not used during the <em>search</em>. We refer to this phenomenon as <em>sub-optimal reasoning</em> as it shows that the LLM could not explore the problem space completely or injected unsuitable knowledge during formalisation into <em>code</em>.</p>
<h3>3.3 Measuring Faithfulness</h3>
<p>We propose a method to measure the faithfulness of the LLM reasoning process when using FLARE. As mentioned in section 3.1, for each query in a dataset D = [Q_{1}, . . . , Q_{|D|}], we generate a set of codes Φ = [C_{1}, . . . , C_{|Φ|}] and simulated problem space searches Ψ = [S_{1}, . . . , S_{|Ψ|}]. We use the Prolog engine to execute all of the codes Φ and obtain a set of correctly written programs Φ' and exact search paths Ψ'. As we do not require explicit programmatic correctness during inference in FLARE for any code C_{i}, some Prolog executions resulting in an error are filtered out in Ψ'. To assess model reasoning faithfulness towards code formalisations, we compare the search paths Φ' obtained from Prolog execution with their designated counterparts Φ'gen generated by the LLM from the same code. We use ROUGE (Lin, 2004) to compute the matching score for each executed and simulated search path. In particular, we use ROUGE-Lsum, which uses the longest common subsequence (LCS) over each line to obtain the final score. This method fits our cause as a line in a Prolog search execution represents a single logic step within the traversal. This allows us to measure the similarity of the reasoning contents and structure in exact and simulated</p>
<p>searches. We have also used other string-matching techniques, all of which show the same trends; thus, we report our results with ROUGE-Lsum.</p>
<h2>4 Experimental Setup</h2>
<p>Datasets To evaluate FLARE, we use a benchmark of 9 tasks spanning Math Word Problems (MWP), multi-hop QA, relation inference, and 3 logical reasoning datasets. For numerical and mathematical reasoning, we follow CoT (Wei et al., 2022) and include GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), ASDiv (Miao et al., 2020), and AQuA (Ling et al., 2017). GSM8K, SVAMP, MultiArith, and ASDiv focus on elementary and middle school arithmetic with integer or decimal answers. AQuA involves multiple-choice symbolic reasoning with expressions not explicitly defined in the query. We also test FLARE on three multihop QA tasks. StrategyQA (Geva et al., 2021) requires boolean reasoning with sub-goal decomposition (e.g., "Do all parts of the aloe vera plant taste good?" in fig. 1). We further use the Date and Sports Understanding subsets from BIGBench (bench authors, 2023), which involve temporal and feasibility-based reasoning. For relation inference, we use CLUTRR (Sinha et al., 2019), which requires deducing familial relations from partial graph descriptions in natural language. We evaluate logical reasoning using ProntoQA (Saparov and He, 2023), AR-LSAT (Zhong et al., 2021), and LogicalDeductions from BIG-Bench (et al., 2023), focusing on the challenging subsets of (Pan et al., 2023). These cover deductive, analytical, and logical tasks. Dataset details and examples are in table 7 of section A.1. We also study how model size affects performance and faithfulness (section A.1).</p>
<p>Benchmarks We compare FLARE with CoT (Wei et al., 2022), which uses natural language reasoning chains, and with F-CoT (Lyu et al., 2023) and Logic-LM (Pan et al., 2023), which formalise queries into code and delegate reasoning to external solvers. Evaluated models include Llama3.1 (8B)(Dubey et al., 2024), CmDR (30B) and CmDR+ (100B)(Cohere, 2024), and GPT-3.5 (Brown et al., 2020b) ( $\geq 100 \mathrm{~B}$ (Ye et al., 2023)). As OpenAI Codex (code-DaVinci002) (Chen et al., 2021) used in F-CoT has been deprecated, we replace it with the new GPT3.5 as suggested by OpenAI and recalculate the results.</p>
<h2>5 Results</h2>
<h3>5.1 Few-shot prompting</h3>
<p>To evaluate FLARE, we use a set of models of varying sizes on diverse benchmarks, as defined in section 4. We compare the performance of each model while using FLARE, CoT and F-CoT prompting. The results for F-CoT and CoT on all the models are computed using the codebase of the original study (Lyu et al., 2023). We additionally compare Logic-LM and FLARE using the logic reasoning benchmarks proposed in (Pan et al., 2023).</p>
<p>LLMs for general reasoning Our results, presented in table 1, show that using FLARE allows the LLMs to achieve state-of-the-art results on 7 out of 9 datasets, with an average $28 \%$ increase over CoT. We can see a clear trend that FLARE increases the performance compared to CoT and F-CoT for all the models of varying scales. We also see that LLMs not explicitly tuned for coding suffer massive degeneracies when using F-CoT. We postulate that they cannot consistently produce executable programs that satisfy a predefined scheme in F-CoT, thus resulting in an error during execution. This further highlights the value of simulating program execution using an LLM instead of external solvers. The results show that using FLARE yields more benefit on datasets that require longer chains of multi-hop and symbolic reasoning, like AQuA and StrategyQA. Our findings in table 2 show that FLAREachieves state-of-the-art results on 2 out of 3 logic inference benchmarks with $10 \%$ increase over CoT and $7 \%$ increase over LogicLM. Following the practice in (Pan et al., 2023, Logic-LM), we also add 2 iterations of code selfrefinement to FLARE and show that the model model is able to achieve SOTA results on all 3 benchmarks.</p>
<p>LLMs for code generation To understand the effect of FLARE on models tuned for coding, we use GPT3.5 (Brown et al., 2020a) as it was the OpenAI suggested succession model for Codex (Chen et al., 2021) which is used in F-CoT and possesses strong coding capabilities (Ye et al., 2023). The results in table 1 show that using FLARE is beneficial for models tuned for coding and boosts accuracy with a $16 \%$ increase over F-CoT and $9 \%$ over CoT. The reason is that many natural language queries with non-trivial formalisations are more suited to be tackled with more commonsense soft reasoning than direct code execution. This is evident in table 1</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\mathrm{CmDR}_{\text {plan-only }}$</th>
<th style="text-align: center;">$\mathrm{CmDR}_{\text {FLARE }}$</th>
<th style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</th>
<th style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</th>
<th style="text-align: center;">$\mathrm{GPT}-3.5_{\text {plan-only }}$</th>
<th style="text-align: center;">$\mathrm{GPT}-3.5_{\text {FLARE }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">$\mathbf{5 2 . 4}$</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">$\mathbf{7 1 . 4}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">$\mathbf{6 8 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">AQuA</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$\mathbf{4 3 . 7}$</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">$\mathbf{5 5 . 9}$</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">$\mathbf{5 5 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">$\mathbf{6 7 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 7}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">$\mathbf{6 5 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: The table shows the accuracy of an LLM with FLARE compared to prompting for a final answer directly after generating (plan-only) a plan $\mathcal{P}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">#Paths</th>
<th style="text-align: center;">#Hops/p</th>
<th style="text-align: center;">#Fails/p</th>
<th style="text-align: center;">TotHops</th>
<th style="text-align: center;">TotFails</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Incorrect Answers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">2.26</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {FLARE }}$</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">6.55</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">10.56</td>
<td style="text-align: center;">1.39</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">7.52</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">8.57</td>
<td style="text-align: center;">1.32</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">5.22</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">5.32</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">Correct Answers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">9.12</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">12.36</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {FLARE }}$</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">7.10</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">11.29</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">7.19</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">5.65</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">5.69</td>
<td style="text-align: center;">0.27</td>
</tr>
</tbody>
</table>
<p>Table 4: #Paths: Avg. number of reasoning paths tried by the model. #Hops/p: Avg. number of hops per path. #Fails/p: Avg. number of fails (unsuccessful hops) per path. TotHops: Avg. total hops (summed across all paths). TotFails: Avg. total fails (summed across all paths). The purple cells show that incorrect reasoning paths often have fewer failed search paths.
where FLARE and CoT are consistently better than F-CoT in StrategyQA, Sports and CLUTRR. The opposite case of numeric and algorithmic heavy reasoning tasks is also covered by FLARE as it maintains strong performance similar to F-CoT on MWP problems table 1. Consequently, FLARE allows combining algorithmic formalisation with simulated soft-reasoning, circumventing the pitfalls of using a deterministic external solver while still producing a query formalisation and problem space traversal.</p>
<h3>5.2 Is simulating search useful?</h3>
<p>To understand if simulating a search over the problem space is useful, we compare the performance of FLARE where we only generate the plan without the subsequent code or search components. We refer to this framework setup as plan-only, which can be seen in fig. 1 if we were to use only the plan for answer generation. We completed this ablation using $\mathrm{CmDR}, \mathrm{CmDR}+$, and GPT-3.5, and we used GSM8K, AQuA, and StrategyQA as our baselines. The results in table 3 confirm that all of the models suffer massive performance degradation from $61.1 \rightarrow 49.9$ when omitting the code and the search components of FLARE. We hypothesise that this is caused by insufficient problem</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">UEF (\%) in Search</th>
<th style="text-align: center;">OR (\%)</th>
<th style="text-align: center;">UR (\%) in code</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Correct Answers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B</td>
<td style="text-align: center;">74.14</td>
<td style="text-align: center;">43.65</td>
<td style="text-align: center;">5.73</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {FLARE }}$</td>
<td style="text-align: center;">59.06</td>
<td style="text-align: center;">35.96</td>
<td style="text-align: center;">4.02</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">34.47</td>
<td style="text-align: center;">4.54</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">64.46</td>
<td style="text-align: center;">37.55</td>
<td style="text-align: center;">1.90</td>
</tr>
<tr>
<td style="text-align: center;">Avg. (Correct)</td>
<td style="text-align: center;">65.49</td>
<td style="text-align: center;">37.91</td>
<td style="text-align: center;">4.05</td>
</tr>
<tr>
<td style="text-align: center;">Incorrect Answers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B</td>
<td style="text-align: center;">54.69</td>
<td style="text-align: center;">35.04</td>
<td style="text-align: center;">9.28</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {FLARE }}$</td>
<td style="text-align: center;">54.50</td>
<td style="text-align: center;">32.76</td>
<td style="text-align: center;">6.23</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{CmDR}_{\text {+ }}$</td>
<td style="text-align: center;">44.12</td>
<td style="text-align: center;">24.98</td>
<td style="text-align: center;">8.22</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">36.02</td>
<td style="text-align: center;">24.44</td>
<td style="text-align: center;">6.94</td>
</tr>
<tr>
<td style="text-align: center;">Avg. (Incorrect)</td>
<td style="text-align: center;">47.33</td>
<td style="text-align: center;">29.31</td>
<td style="text-align: center;">7.67</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">18.16</td>
<td style="text-align: center;">8.60</td>
<td style="text-align: center;">-3.62</td>
</tr>
</tbody>
</table>
<p>Table 5: The table shows how the percentage of unique emergent facts (UEF) in search, overlapping relations (OR) between code and search, and unused relations (UR) in code impact answer correctness.
space exploration when using the plan-only setting. Furthermore, we have already seen in table 1 that in methods, like F-CoT, that do not use simulated problem space exploration for soft-reasoning and only rely on plan and code, the performance also deteriorates even resulting in a complete breakdown of reasoning over the designated datasets. This can be viewed as a constrained version of FLARE with code-only execution. Consequently, our results show that simulating problem space traversal is highly beneficial as it avoids the pitfalls posed by plan-only and code-only modes by exploring the problem space more rigorously and soft-reasoning during that traversal instead of using external solvers.</p>
<h3>5.3 Faithful Reasoning Improves Performance</h3>
<p>As described in section 3, using FLARE allows us to measure the faithfulness of the LLM reasoning process by comparing the simulated problem space traversals $\Phi_{\text {gen }}^{\prime}$ with actual traces $\Phi^{\prime}$ produced from a symbolic Prolog solver. To do this, we initially compute the percentage of syntactically correct executable code each LLM produces. We have observed that all of the models are capable of producing correct executable Prolog code in $67 \%$ of cases</p>
<p>on average and $\geq 50 \%$ of cases at the very least. The complete details can be seen in the top part of fig. 3 in section A.5. This shows that the simulated searches $\Phi_{\text {gen }}^{\prime}$ can be considered a representative sample that will be further used to accurately measure the faithfulness of the simulated search w.r.t. the generated code. After measuring the reasoning faithfulness for each model, we want to understand what impact it has on the performance of the LLM. In fig. 2, we segment the models w.r.t. their ROUGE-Lsum scores. The results show that model performance is strongly positively correlated with reasoning faithfulness. However, we also observe that executing semantically precise code results in an accurate answer only in $47 \%$ of cases on average. Refer to the bottom part of fig. 3 in section A.5 for more details. Indeed, having a simulated search trace with a ROUGE-Lsum faithfulness score of 1, would be equivalent to simply executing the program as proposed in F-CoT. Yet we have priorly shown that F-CoT struggles with reasoning tasks that are hard to formalise and require multi-hop commonsense and soft reasoning. These two discoveries show that optimal LLM reasoning, conditioned on a search in the problem space, should be increasingly faithful toward the facts, relations and the search strategy defined within the code while simultaneously maintaining the capability for soft-reasoning along more abstractly defined concepts. Our results show that FLARE allows LLMs to maintain a similar reasoning capacity.</p>
<h3>5.4 What is important during the search?</h3>
<p>We also analyze the reasons which can lead to optimal reasoning within an LLM. We calculate several statistics, like the average number of explored paths and the average and total hops and failures per path, for each model during the simulated traversal. The failure in a path is an invalidation of a solution for a sub-goal explored during the search, which is used for backtracking, as explained in section 3. Calculating these statistics is simple as the search component of FLARE, seen in fig. 1, is a structured simulation of a Prolog trace, where each line contains a hop of reasoning inference. We split these statistics for the reasoning paths that lead to correct or incorrect outcomes. Our results in table 4 show that LLM performance and reasoning optimality are not directly connected to the amount of explored paths or multi-hop inferences per path. We also see that traces that lead to incorrect answers have a higher number of failures per
path and in total. We hypothesise that LLMs with traces that were optimal for reasoning and led to correct answers could skip exploring degenerate solutions due to strong commonsense reasoning capabilities. Further analyses focus on identifying inconsistencies and failure modes (section 3.2). By comparing relations in code with those in search traces, we measure emergent hallucinations and unused relations, highlighting areas of sub-optimal reasoning. We also assess the uniqueness of emergent facts per inference hop, indicating the extent of problem-space exploration (table 5). The results in table 5 show consistently over each model that, on average, traces that lead to correct answers had a higher percentage of unique emergent facts (UEF) and overlap in the relations (OR) used between the code and search, while the portion of underutilized relations (UR) was lower. This means that optimal reasoning with an LLM requires a great degree of problem-space exploration with fewer relation hallucinations during the search and more relation utilization from the defined code. This aligns with our prior discoveries, which show a strong correlation between simulated search faithfulness towards the formalised code and model performance.</p>
<h2>6 Conclusion</h2>
<p>This work introduces FLARE, a novel approach for logic-aided interpretable formalisation and reasoning with simulated search over the problem space. We show that models of varying scales obtain state-of-the-art results compared to prompting paradigms like CoT, F-CoT and Logic-LM. We further pinpoint that using FLARE allows us to perform soft reasoning with simulated search, making it flexible for diverse reasoning benchmarks. We introduce a method to measure model reasoning faithfulness w.r.t. the problem formalization ingrained within FLARE. Our results show that model performance is positively correlated with the faithfulness of the reasoning process. The systematic studies of the method show the benefits of using simulated search compared to natural language reasoning and external symbolic solvers. We further show that using FLARE allows us to interpretably and rigorously detect hallucinations and sub-optimal and inconsistent reasoning patterns.</p>
<h2>Limitations</h2>
<p>While FLARE offers significant improvements in faithfulness and interoperability, it depends on the</p>
<p>quality of LLM-generated plans and code; errors or omissions in formalisation can propagate through the simulated search, potentially leading to incorrect or incomplete answers. The generation of formal code and simulated search traces depends heavily on the LLM's prompt-following ability. The simulation of code execution may not fully explore extremely large or open-ended problem spaces, and prompt sensitivity can affect search thoroughness.</p>
<h2>Risks and Impact Statement</h2>
<p>FLARE advances the capabilities of large language models in logical reasoning and problem-solving, with potential positive impacts on applications requiring transparent and verifiable decision-making processes. The ability of the method to formalise reasoning steps and detect inconsistencies could improve reliability in high-stakes domains like healthcare decision support, educational assessment, and automated planning systems. However, this advancement also raises important considerations - the improved reasoning capabilities could be misused to automate deceptive or manipulative argumentation, and the increased persuasiveness through logical formalisation could mask underlying biases or false premises. Additionally, while FLARE improves transparency in reasoning, it may create a false sense of rigour in cases where the underlying logic is flawed but presented in a formally convincing manner.</p>
<h2>Acknowledgments</h2>
<p>Erik is partially funded by a DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, as well as by an NEC PhD fellowship, and is supported by the Pioneer Centre for AI, DNRF grant number P1. Pasquale was partially funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and a donation from Accenture LLP. Isabelle's research is partially funded by the European Union (ERC, ExplainYourself, 101077481), and is supported by the Pioneer Centre for AI, DNRF grant number P1. This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.</p>
<h2>References</h2>
<p>Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. To code, or not to code? exploring impact of code in pre-training. CoRR, abs/2408.10914.</p>
<p>Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732.</p>
<p>BIG bench authors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 17682-17690. AAAI Press.</p>
<p>Kenneth A. Bowen. 1979. Prolog. In Proceedings of the 1979 Annual Conference, Detroit, Michigan, USA, October 29-31, 1979, pages 14-23. ACM.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020b. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, and Jinyoung Yeo. 2024. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. CoRR, abs/2404.02575.</p>
<p>Ashok K. Chandra and David Harel. 1985. Horn clauses queries and generalizations. J. Log. Program., 2(1):115 .</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023.</p>
<p>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others. 2024. Do not think that much for $2+3=?$ on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Cohere. 2024. Command r: Retrieval-augmented generation at production scale. https://txt. cohere. com/command-r.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 82 others. 2024. The llama 3 herd of models. CoRR, abs/2407.21783.</p>
<p>Aarohi Srivastava et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Trans. Mach. Learn. Res., 2023.</p>
<p>Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. 2024. Stream of search (sos): Learning to search in language. CoRR, abs/2404.03683.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: program-aided language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with
implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346-361.</p>
<p>Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In 5th IEEE International Conference on Data Science and Advanced Analytics, DSAA 2018, Turin, Italy, October 1-3, 2018, pages 80-89. IEEE.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021a. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.</p>
<p>Bernease Herman. 2017. The promise and peril of human evaluation for model interpretability. arXiv preprint arXiv:1711.07414.</p>
<p>Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, and Mor Geva. 2024. A chain-of-thought is as strong as its weakest link: A benchmark for verifiers of reasoning chains. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pages 4615-4634. Association for Computational Linguistics.</p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4198-4205. Association for Computational Linguistics.</p>
<p>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. A survey on large language models for code generation. CoRR, abs/2406.00515.</p>
<p>Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. 2023. Design of chain-ofthought in math problem solving. arXiv preprint arXiv:2309.11054.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, and 11 others. 2023a. Measuring faithfulness in chain-of-thought reasoning. CoRR, abs/2307.13702.</p>
<p>Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, and 1 others. 2023b. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702.</p>
<p>Lucas Lehnert, Sainbayar Sukhbaatar, Paul McVay, Michael Rabbat, and Yuandong Tian. 2024. Beyond a*: Better planning with transformers via search dynamics bootstrapping. CoRR, abs/2402.14083.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li FeiFei, Fei Xia, and Brian Ichter. 2024. Chain of code: Reasoning with a language model-augmented code emulator. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 158-167. Association for Computational Linguistics.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023. Deductive verification of chain-of-thought reasoning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023.</p>
<p>LLM+P: empowering large language models with optimal planning proficiency. CoRR, abs/2304.11477.</p>
<p>Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. 2024. Exploring and evaluating hallucinations in llm-powered code generation. CoRR, abs/2404.00971.</p>
<p>John W. Lloyd. 1994. Practical advtanages of declarative programming. In 1994 Joint Conference on Declarative Programming, GULP-PRODE'94 Peñiscola, Spain, September 19-22, 1994, Volume 1, pages $18-30$.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1 - 4, 2023, pages 305329. Association for Computational Linguistics.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. 2022a. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. 2022b. Text and patterns: For effective chain of thought, it takes two to tango. CoRR, abs/2209.07686.</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 975-984. Association for Computational Linguistics.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3806-3824. Association for Computational Linguistics.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the</p>
<p>Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2080-2094. Association for Computational Linguistics.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1743-1752. The Association for Computational Linguistics.</p>
<p>Abulhair Saparov and He He. 2023. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4505-4514. Association for Computational Linguistics.</p>
<p>Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Christian Szegedy. 2020. A promising path towards autoformalization and general artificial intelligence. In Intelligent Computer Mathematics - 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings, volume 12236 of Lecture Notes in Computer Science, pages 3-20. Springer.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 2609-2634. Association for Computational Linguistics.</p>
<p>Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. 2018. First experiments with neural translation of informal to formal mathematics. In Intelligent Computer Mathematics - 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018, Proceedings, volume 11006 of Lecture Notes in Computer Science, pages 255-270. Springer.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, and 1 others. 2025. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837.</p>
<p>Jan Wielemaker, Tom Schrijvers, Markus Triska, and Torbjörn Lager. 2012. Swi-prolog. Theory and Practice of Logic Programming, 12(1-2):67-96.</p>
<p>Wikipedia. 2024. Countdown (game show) - Wikipedia, the free encyclopedia. http://en.wikipedia.org/w/index.php? title=Countdown\%20(game\%20show)\&amp;oldid= 1248084922. [Online; accessed 09-September2024].</p>
<p>Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022. Autoformalization with large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. 2022. Chain of thought imitation with procedure cloning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. CoRR, abs/2303.10420.</p>
<p>Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, and Tingwen Liu. 2025. S1-bench: A simple benchmark for evaluating system 1 thinking capability of large reasoning models. arXiv preprint arXiv:2504.10368.</p>
<p>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. 2024. LLM as a mastermind: A survey of strategic reasoning with large language models. CoRR, abs/2404.01230.</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021. AR-LSAT: investigating analytical reasoning of text. CoRR, abs/2104.06598.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The figure shows the percentage of executable code per model (top) and the accuracy of the executable code when answering the queries (bottom).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Avg. hops per path</th>
<th>Hal. (%)</th>
<th>UK. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-3.1-8B</td>
<td>9.4</td>
<td>63.3</td>
<td>62.9</td>
</tr>
<tr>
<td>CmDR</td>
<td>6.7</td>
<td>54.7</td>
<td>56.9</td>
</tr>
<tr>
<td>CmDR+</td>
<td>7.2</td>
<td>54.3</td>
<td>56.3</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>5.5</td>
<td>49.3</td>
<td>52.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Changes in simulated search statistics when using FLARE across model scales (8B to 100B+). Hallucinations (Hal.) are facts/predicates used only in the trace, while <em>unutilised knowledge</em> (UK.) denotes facts/relations appearing only in the code.</p>
<h2>A Appendix</h2>
<h3>A.1 The effect of scale</h3>
<p>We want to assess the impact of the number of parameters in the model on the overall performance and faithfulness. The results in fig. 4 show no precise relation between model scale, performance and faithfulness. However, scaled models from the same family, i.e. CmDR (30B) and CmDR+ (100B), show improvements in reasoning faithfulness and model performance. We can also see in table 6 that as the model size increases, the average number of hops and the portion of hallucinations and unutilised knowledge decreases. This further confirms our prior assumptions that models with strong commonsense soft-reasoning capabilities can skip steps during the search while maintaining the knowledge and structure of the traversal strategy outlined in the code.</p>
<h3>A.2 LLM Prompts</h3>
<p>We define straight-forward prompts for generating <em>plan</em>, <em>code</em> and <em>search</em> simulation in FLARE, which can be observed in section A.5.</p>
<h3>A.3 Dataset Statistics</h3>
<p>The datasets used in this study encompass a variety of domains, specifically targeting the performance of the models in interpreting Math Word Problems, multi-hop question answering, and relational inference. Table 7 provides a detailed breakdown of each dataset, including the number of few-shot in-context samples (shots), the number of test samples, and representative examples from each dataset. The datasets provide a comprehensive basis for evaluating the models' abilities to handle complex tasks across different domains, facilitating an in-depth analysis of model performance under few-shot conditions.</p>
<h3>A.4 FLARE Pseudo-code</h3>
<p>Below, we present the pseudo-code for the execution of the <em>plan</em>, <em>code</em>, and <em>search</em> procedures in FLARE. The pseudo-code describes the modular pipeline in FLARE for tackling natural language queries with faithful simulated search. (i) <strong>Plan Generation:</strong> This stage creates a structured natural language outline of the reasoning process, breaking down the query into logical steps and analysis. The plan serves as the foundation for formalization into a logic-based representation. (ii) <strong>Code Generation:</strong> Based on the generated plan, a logic programming code (e.g., in Prolog) is synthesized. This code formalizes the query into a set of facts, relations, and goals, which collectively define the problem space for reasoning. (iii) <strong>Search Simulation:</strong> The generated code is utilized to simulate a search trace over the problem space. This includes iterative reasoning, backtracking when goals are unmet, and extracting emergent facts or relations during the process. Each of these stages is implemented as a modular component. The generation from each of the stages feeds into the next, allowing seamless integration and incremental improvement in reasoning accuracy. A detailed pseudo-code is provided below in section A.5.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The effect of the model parameter scale from 8B to 100B+ on model accuracy (left) and faithfulness (right).</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Dataset</th>
<th>Shots</th>
<th>Test Samples</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Math</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Word</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Problems</td>
<td>GSM8K</td>
<td>8</td>
<td>1,319</td>
<td>Q: A robe takes 2 bolts of blue fiber and half that much white fiber.</td>
</tr>
<tr>
<td>How many bolts in total does it take?</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: 3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>SVAMP</td>
<td>8</td>
<td>1,000</td>
<td>Q: Dan had $3 left with him after he bought a candy bar. If he had $4 at the start, how much did the candy bar cost?A: 1</td>
</tr>
<tr>
<td></td>
<td>MultiArith</td>
<td>8</td>
<td>600</td>
<td>Q: A pet store had 13 siamese cats and 5 house cats. During a sale they sold 10 cats.</td>
</tr>
<tr>
<td>How many cats do they have left?</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: 8</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ASDiv</td>
<td>8</td>
<td>2,096</td>
<td>Q: Adam has five more apples than Jackie. Jackie has nine apples. How many apples does Adam have?</td>
</tr>
<tr>
<td>A: 14</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>AQoA</td>
<td>8</td>
<td>254</td>
<td>Q: A man walks at 5 kmph for 6 hrs and at 4 kmph for 12 hrs. His average speed is</td>
</tr>
<tr>
<td>Answer option: A)4 1/3 km/h, B)7 2/3 km/h, C)9 ½ km/h, D)8 km/h, E)81 km/h</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: A</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Multi-hop QA</td>
<td>StrategyQA</td>
<td>6</td>
<td>2,290</td>
<td>Q: Did Aristotle use a laptop?</td>
</tr>
<tr>
<td>A: False</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Date</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Understanding</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sports</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Understanding</td>
<td>10</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>359</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>977</td>
<td>Q: Yesterday was April 30, 2021. What is the date tomorrow in MM/DD/YYYY?</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: "05/02/2021"</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Q: Is the following sentence plausible? Lionel Messi was called for icing?</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: False</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Relational</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Inference</td>
<td>CLUTRR</td>
<td>8</td>
<td>1,042</td>
<td>Q: [Carlos] is [Clarence]'s brother. [Carlos] and his sister, [Annie], went shopping.</td>
</tr>
<tr>
<td>asked her mom [Valerie] if she wanted anything, but [Valerie] said no.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>How is [Valerie] related to [Clarence]?</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A: "mother"</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 7: The statistics and examples of the datasets used in benchmarking. Shots refers to the number of few-shot in-context samples used during benchmarking.</p>
<h3>A.5 Benefits of Prolog</h3>
<p>Prolog is a symbolic logic-programming engine (Bowen, 1979) used for heuristic search over Horn Clauses (Chandra and Harel, 1985). It is a declarative programming paradigm (Lloyd, 1994), meaning that the code is expressed as the logic of computation. In particular, this logic is formalised as a set of facts $\mathcal{F}$ and relations $\mathcal{R}$ forming our problem space, while the final goal $\mathcal{G}$ is a first-order logic combination of them. As a default, Prolog uses a depth-first search (DFS) strategy (Bowen, 1979) for sub-goal decomposition and feasible traversal of the problem space that satisfies the goal $\mathcal{G}$. Such a traversal is referred to as the <em>trace</em>. At each trace step, the program can either confirm or invalidate the sub-goal using the feasibility of fact and relation combinations, expand the search tree or retry satisfying a failed sub-goal with new combinations. An example of such a search can be observed in fig. 1. It is possible to complete an exhaustive search, exploring all possible paths that do or do not satisfy the goal. These characteristics are beneficial as we can explicitly access and segment the facts and relations that form the problem space and the search strategy used for query formalisation. As Prolog is declarative, it is sufficient to use a regexp heuristic for the segmentation, which is referred to as EXTRACT in eq. (2) and eq. (3). Furthermore,</p>
<p>including exhaustive traversal traces in-context allows the LLM to simulate sub-goal decomposition, backtracking, intermediate goal invalidation, etc. We discuss this in more depth in the next paragraph.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 FLARE Methodology: Faithful Logic-Aided Reasoning and Exploration
Require: Query \(\mathcal{Q}\), Language Model \(\mathcal{M}\)
Ensure: Answer \(\mathcal{A}\)
    Initialization: Load few-shot examples for plans \(\left(\mathcal{E}_{P}\right)\), code \(\left(\mathcal{E}_{C}\right)\), and search traces \(\left(\mathcal{E}_{S}\right)\)
    Input: Natural language query \(\mathcal{Q}\)
    procedure Generate Plan
        Prompt \(\mathcal{M}\) with instructions \(\mathcal{I}_{P}\) and examples \(\mathcal{E}_{P}\) to generate a plan \(\mathcal{P}\)
        \(\mathcal{P} \leftarrow \arg \max p_{\mathcal{M}}\left(T_{P} \mid T_{P:&lt;i}, \mathcal{E}_{P}, \mathcal{Q}, \mathcal{I}_{P}\right)\)
    end procedure
    procedure Generate Code
        Append examples \(\mathcal{E}_{C}\) to \(\mathcal{E}_{P}\)
        Prompt \(\mathcal{M}\) with instructions \(\mathcal{I}_{C}\) to generate logic programming code \(\mathcal{C}\)
        \(\mathcal{C} \leftarrow \arg \max p_{\mathcal{M}}\left(T_{C} \mid T_{C:&lt;i}, \mathcal{E}_{C}, \mathcal{Q}, \mathcal{P}, \mathcal{I}_{C}\right)\)
        \(\left(F_{\text {code }}, R_{\text {code }}, G_{\text {code }}\right) \leftarrow \operatorname{ExtracT}(\mathcal{C})\)
    end procedure
    procedure Simulate SEARCH
        Append search trace examples \(\mathcal{E}_{S}\) to \(\mathcal{E}_{C}\)
        Prompt \(\mathcal{M}\) with instructions \(\mathcal{I}_{S}\) to simulate a search trace \(\mathcal{S}\)
        \(\mathcal{S} \leftarrow \arg \max p_{\mathcal{M}}\left(T_{S} \mid T_{S:&lt;i}, \mathcal{E}_{S}, \mathcal{Q}, \mathcal{P}, \mathcal{C}, \mathcal{I}_{S}\right)\)
        \(\left(F_{\text {search }}, R_{\text {search }}, \mathcal{A}_{\text {search }}\right) \leftarrow \operatorname{ExtracT}(\mathcal{S})\)
        while Goal \(\mathcal{G}_{\text {code }}\) is not satisfied do
            Explore next sub-goal in \(\mathcal{S}\)
            if Sub-goal fails then
                Backtrack to the previous state (Learned through in-context sampels)
            end if
        end while
    end procedure
    procedure Final Answer Generation
        Append correct answers from \(\mathcal{A}_{\text {search }}\) to examples
        Prompt \(\mathcal{M}\) with instructions \(\mathcal{I}_{A}\) to finalize answer \(\mathcal{A}\)
        \(\mathcal{A} \leftarrow \arg \max p_{\mathcal{M}}\left(T_{A} \mid T_{A:&lt;i}, \mathcal{E}_{A}, \mathcal{Q}, \mathcal{P}, \mathcal{C}, \mathcal{S}, \mathcal{I}_{A}\right)\)
    end procedure
    return \(\mathcal{A}\)
</code></pre></div>

<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Plan Generation</td>
<td style="text-align: center;">Generate an explanation and analysis, and plan to generate a prompt for writing a swi-prolog code for the last task. The 3 sections should be exactly outlined. Your plan should show enough intermediate reasoning steps towards the answer. Construct the plan as much as you can and describe the logic specifically. When constructing the plan for the code prompt, actively use swi prolog search capabilities.</td>
<td style="text-align: center;">Detailed instructions for generating an outline and plan, with an emphasis on reasoning steps and using Prolog's search capabilities.</td>
</tr>
<tr>
<td style="text-align: center;">Code Generation</td>
<td style="text-align: center;">Write a Prolog code to solve using the plan. If there are unknown or stochastic atoms or predicates, fill in the values for them as a logical assumption and add a comment in the same line Assumed atom/predicate". Do not use write and read commands within the code. The code should be very detailed and utilize swi prolog capabilities to the fullest. To run the program, at the end create a predicate named "query" that returns the correct numerical answer. The last line of the program should be the commented-out driver predicate "query". Write only the code.</td>
<td style="text-align: center;">Instructions for generating a Prolog code based on the plan with assumptions for unknown atoms. Emphasizes code details and a final "query" predicate.</td>
</tr>
<tr>
<td style="text-align: center;">Simulated Search</td>
<td style="text-align: center;">Ignoring the read commands, explicitly write out the search paths that are explored by the code: #### Here are the paths [Starting Search Simulation]: #### [Path 1]:</td>
<td style="text-align: center;">A task to simulate and display the search paths that the Prolog code would follow during execution.</td>
</tr>
<tr>
<td style="text-align: center;">Final Answer</td>
<td style="text-align: center;">Given the plan, the code and the explored search paths answer the question above. Answer with the correct numerical answer. ##### Here is the answer:</td>
<td style="text-align: center;">Final prompt asking for the correct numerical answer based on the previous steps.</td>
</tr>
</tbody>
</table>
<p>Table 8: Table of Prompts for Plan, Code, Simulated Search, and Final Answer generation for GSM8K (Cobbe et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Section</th>
<th style="text-align: center;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">"Can Reiki be stored in a bottle?"</td>
</tr>
<tr>
<td style="text-align: center;">Explanation</td>
<td style="text-align: center;">Reiki is a form of alternative therapy that involves the channeling of energy to promote healing and relaxation. It is often administered through the practitioner's hands, with the energy flowing directly to the recipient. The question raises the possibility of storing Reiki energy in a bottle, which may be a common query due to the abstract nature of energy healing.</td>
</tr>
<tr>
<td style="text-align: center;">Analysis</td>
<td style="text-align: center;">To address the question of whether Reiki can be stored in a bottle, it is important to consider the nature of Reiki energy, the principles of energy healing, and the practical implications of containing and preserving such energy.</td>
</tr>
<tr>
<td style="text-align: center;">Plan</td>
<td style="text-align: center;">- Understand Reiki Energy: ... <br> - Explore Energy Storage: ... <br> - Consider Symbolism and Intention: ... <br> - Evaluate Practicality: ... <br> - Reflect on Tradition: ...</td>
</tr>
<tr>
<td style="text-align: center;">Code</td>
<td style="text-align: center;"><code>% Define the nature of Reiki energy reiki_energy(spiritual_energy). reiki_energy(channeling_through_touch). % Define the feasibility of storing energy in a bottle store_energy_in_bottle(feasible) :- reiki_energy(spiritual_energy), reiki_energy(channeling_through_touch). % Storing Reiki energy is not a common practice not_common_practice(store_reiki_in_bottle). % Query to determine if Reiki can be stored in a bottle query :- store_energy_in_bottle(feasible), not_common_practice(store_reiki_in_bottle). % :- query.</code></td>
</tr>
<tr>
<td style="text-align: center;">Search Paths</td>
<td style="text-align: center;">- 0: Start of execution: Beginning Search <br> - 1: Call: query <br> - 2: Call: store_energy_in_bottle(feasible) <br> - 3: Call: reiki_energy(spiritual_energy) <br> - 4: Call: reiki_energy(channeling_through_touch) <br> - 5: Call: not_common_practice(store_reiki_in_bottle) <br> - 6: Call: feasible=no <br> - 7: Fail: feasible=no</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">"No, Reiki energy cannot be stored in a bottle based on the logical evaluation of its abstract, non-physical nature and traditional practices of Reiki."</td>
</tr>
</tbody>
</table>
<p>Table 9: Complete example of FLARE</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
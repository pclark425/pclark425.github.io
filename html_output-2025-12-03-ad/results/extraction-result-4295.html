<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4295 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4295</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4295</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-276960163</p>
                <p><strong>Paper Title:</strong> Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</p>
                <p><strong>Paper Abstract:</strong> The extraction of molecular annotations from scientific literature is critical for advancing data-driven research. However, traditional methods, which primarily rely on human curation, are labor-intensive and error-prone. Here, we present an LLM-based agentic workflow that enables automatic and efficient data extraction from literature with high accuracy. As a demonstration, our workflow successfully delivers a dataset containing over 91,000 enzyme kinetics entries from around 3,500 papers. It achieves an average F1 score above 0.9 on expert-annotated subsets of protein enzymes and can be extended to the ribozyme domain in fewer than 3 days at less than $90. This method opens up new avenues for accelerating the pace of scientific research.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4295.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4295.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Enzyme Co-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agentic workflow for enzyme kinetic data extraction (Enzyme Co-Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic pipeline that uses OCR, prompt-engineered queries to multiple LLMs, and an aggregation agent to extract, normalize, and standardize enzyme kinetic parameters (Km, kcat, kcat/Km, kobs, kcleave) from full-text scientific papers into structured datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based agentic workflow (Enzyme Co-Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline steps: (1) collect relevant literature PDFs; (2) apply OCR (Mathpix chosen as default after comparison with PyMuPDF) to produce machine-readable text preserving tables and math; (3) use an engineered prompt (task description, detailed instructions, and output format reference) to query multiple LLMs on the paper text to extract target fields; (4) aggregate outputs from multiple LLMs via an aggregation agent (prompt Claude3.5 with the paper text plus outputs from Claude3.5, gpt-4o, Llama3, and Qwen) to consolidate results; (5) post-process and clean outputs with rules: space cleaning, numeric/regex-based normalization (including scientific notation and error representations), unit conversion to standardized units (Km → mM, kcat → s^-1, kcat/Km → s^-1 mM^-1), and alignment into pandas DataFrame for downstream use; (6) evaluate against expert-annotated gold standard and compare to existing database (BRENDA). Prompt engineering was iterative with manual analysis of failure cases and updates to instructions and format templates (Markdown table used as intermediate output).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claude3.5 (Claude-3.5-sonnet-20240620), gpt-4o, Llama3 (locally deployed; used up to 32k input tokens), Qwen (Qwen-plus-0806).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biochemistry (protein enzymes) and RNA biochemistry (ribozymes).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>3,435 protein enzyme papers; 164 ribozyme papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Quantitative kinetic parameters and relationships (enzyme kinetic constants): Km, kcat, kcat/Km, kobs, kcleave (numerical constants and units rather than abstract 'laws').</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tabular data (Markdown-delimited tables from LLM outputs, converted to pandas DataFrame), with numeric values plus units standardized (e.g., mM, s^-1, s^-1 mM^-1).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to expert-annotated gold-standard datasets (156 protein-enzyme papers with 3,563 Km / 3,531 kcat / 3,751 kcat/Km entries; 164 ribozyme papers annotated) and comparison to BRENDA (human-curated database) when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Protein enzymes (annotated subset, 156 papers): Claude3.5 mean F1 = 0.90, median F1 = 0.99 (25%/75% = 0.90/1.00). Aggregation agent (Claude3.5 consolidating outputs from Claude3.5, gpt-4o, Llama3, Qwen) achieved improved overall performance and reduced bad cases (paper-wise F1 distributions improved; exact aggregate F1 not quoted beyond qualitative improvement). BRENDA baseline on same 156 papers: mean F1 = 0.76, median = 0.78 (25%/75% = 0.64/0.95). Ribozyme set (164 papers): Claude3.5 mean F1 = 0.75. Cost/time example: processing 164 ribozyme papers cost < $90 and completed within 3 days.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to human-curated BRENDA: LLMs (Claude3.5 and the aggregation agent) outperformed BRENDA on the 156-paper annotated benchmark (Claude3.5 mean F1 0.90 vs BRENDA mean F1 0.76). The paper explicitly treats BRENDA as the baseline for protein enzyme extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires long-context capabilities (average token count ~25k; many papers >32k tokens); OCR quality impacts extraction (Mathpix slightly outperformed PyMuPDF); some LLMs have complementary strengths/weaknesses—simple ensembling can be dragged down by poorly performing models (aggregation agent must filter/vote out weak models); limits on LLM input/output token capacities vary across models; rounding and representation differences in curated databases (e.g., BRENDA) complicate automatic matching; some challenging extraction cases require specialized prompts or model-specific strengths; aggregation agent performance can be harmed when low-performing models are included; evaluation edge-cases (division-by-zero) required special handling policies.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4295.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4295.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI deep research</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI 'deep research' tool</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step research tool by OpenAI reported to conduct internet-based, multi-step research to assist scientists in literature reviews and identifying knowledge gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI's 'deep research' tool: is it useful for scientists?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>OpenAI 'deep research' (reported capability)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described in the paper as a multi-step research-capable tool that can search the internet and assist in writing literature reviews and identifying knowledge gaps; the current paper only cites this capability as an example of AGI/LLM advances in scientific literature mining.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature (not specific in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4295.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4295.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google AI co-scientist (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google 'AI co-scientist' (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced initiative by Google described as a virtual scientific collaborator to help generate hypotheses, research proposals, and accelerate scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards an AI co-scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Google 'AI co-scientist' (referenced initiative)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as an example of LLM-based systems aiming to assist scientists in hypothesis generation and accelerating discovery; the present paper cites it in context but does not use or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific research (cited as an initiative).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4295.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4295.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structured information extraction (Dagdelen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that uses large language models for structured information extraction from scientific text (cited in context of related work on LLM-based literature mining).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Structured information extraction with LLMs (as per Dagdelen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as related work demonstrating LLMs applied to structured information extraction in scientific texts; specific method details are not provided in this paper and are left to the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature (general; the cited work focuses on materials/chemistry contexts but the present paper only cites it as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4295.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4295.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Materials data extraction (Polak & Morgan)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applies conversational LLMs and prompt engineering to extract materials data from research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Conversational LLM + prompt engineering for materials data extraction (Polak & Morgan)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a relevant example where conversational LLMs and prompt engineering are used to extract structured data from papers in materials science; the current paper references it as related work but does not provide implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (extraction of materials data).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured numerical materials data (as implied by title)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured data (implied)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4295.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4295.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical research (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced Nature paper demonstrating use of LLMs toward autonomous chemical research workflows (cited as an example of LLMs in research automation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Autonomous chemical research pipeline with LLMs (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an example where LLMs are applied to autonomous or semi-autonomous research tasks in chemistry; the present paper cites it as related work but does not provide methodological specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / autonomous experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Potentially experimental design and quantitative optimization relationships (implied by title), but not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>OpenAI's 'deep research' tool: is it useful for scientists? <em>(Rating: 1)</em></li>
                <li>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4295",
    "paper_id": "paper-276960163",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Enzyme Co-Scientist",
            "name_full": "LLM-based agentic workflow for enzyme kinetic data extraction (Enzyme Co-Scientist)",
            "brief_description": "An agentic pipeline that uses OCR, prompt-engineered queries to multiple LLMs, and an aggregation agent to extract, normalize, and standardize enzyme kinetic parameters (Km, kcat, kcat/Km, kobs, kcleave) from full-text scientific papers into structured datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-based agentic workflow (Enzyme Co-Scientist)",
            "method_description": "Pipeline steps: (1) collect relevant literature PDFs; (2) apply OCR (Mathpix chosen as default after comparison with PyMuPDF) to produce machine-readable text preserving tables and math; (3) use an engineered prompt (task description, detailed instructions, and output format reference) to query multiple LLMs on the paper text to extract target fields; (4) aggregate outputs from multiple LLMs via an aggregation agent (prompt Claude3.5 with the paper text plus outputs from Claude3.5, gpt-4o, Llama3, and Qwen) to consolidate results; (5) post-process and clean outputs with rules: space cleaning, numeric/regex-based normalization (including scientific notation and error representations), unit conversion to standardized units (Km → mM, kcat → s^-1, kcat/Km → s^-1 mM^-1), and alignment into pandas DataFrame for downstream use; (6) evaluate against expert-annotated gold standard and compare to existing database (BRENDA). Prompt engineering was iterative with manual analysis of failure cases and updates to instructions and format templates (Markdown table used as intermediate output).",
            "llm_model_used": "Claude3.5 (Claude-3.5-sonnet-20240620), gpt-4o, Llama3 (locally deployed; used up to 32k input tokens), Qwen (Qwen-plus-0806).",
            "scientific_domain": "Biochemistry (protein enzymes) and RNA biochemistry (ribozymes).",
            "number_of_papers": "3,435 protein enzyme papers; 164 ribozyme papers",
            "type_of_quantitative_law": "Quantitative kinetic parameters and relationships (enzyme kinetic constants): Km, kcat, kcat/Km, kobs, kcleave (numerical constants and units rather than abstract 'laws').",
            "extraction_output_format": "Structured tabular data (Markdown-delimited tables from LLM outputs, converted to pandas DataFrame), with numeric values plus units standardized (e.g., mM, s^-1, s^-1 mM^-1).",
            "validation_method": "Comparison to expert-annotated gold-standard datasets (156 protein-enzyme papers with 3,563 Km / 3,531 kcat / 3,751 kcat/Km entries; 164 ribozyme papers annotated) and comparison to BRENDA (human-curated database) when applicable.",
            "performance_metrics": "Protein enzymes (annotated subset, 156 papers): Claude3.5 mean F1 = 0.90, median F1 = 0.99 (25%/75% = 0.90/1.00). Aggregation agent (Claude3.5 consolidating outputs from Claude3.5, gpt-4o, Llama3, Qwen) achieved improved overall performance and reduced bad cases (paper-wise F1 distributions improved; exact aggregate F1 not quoted beyond qualitative improvement). BRENDA baseline on same 156 papers: mean F1 = 0.76, median = 0.78 (25%/75% = 0.64/0.95). Ribozyme set (164 papers): Claude3.5 mean F1 = 0.75. Cost/time example: processing 164 ribozyme papers cost &lt; $90 and completed within 3 days.",
            "baseline_comparison": "Compared to human-curated BRENDA: LLMs (Claude3.5 and the aggregation agent) outperformed BRENDA on the 156-paper annotated benchmark (Claude3.5 mean F1 0.90 vs BRENDA mean F1 0.76). The paper explicitly treats BRENDA as the baseline for protein enzyme extraction.",
            "challenges_limitations": "Requires long-context capabilities (average token count ~25k; many papers &gt;32k tokens); OCR quality impacts extraction (Mathpix slightly outperformed PyMuPDF); some LLMs have complementary strengths/weaknesses—simple ensembling can be dragged down by poorly performing models (aggregation agent must filter/vote out weak models); limits on LLM input/output token capacities vary across models; rounding and representation differences in curated databases (e.g., BRENDA) complicate automatic matching; some challenging extraction cases require specialized prompts or model-specific strengths; aggregation agent performance can be harmed when low-performing models are included; evaluation edge-cases (division-by-zero) required special handling policies.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4295.0",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "OpenAI deep research",
            "name_full": "OpenAI 'deep research' tool",
            "brief_description": "A multi-step research tool by OpenAI reported to conduct internet-based, multi-step research to assist scientists in literature reviews and identifying knowledge gaps.",
            "citation_title": "OpenAI's 'deep research' tool: is it useful for scientists?",
            "mention_or_use": "mention",
            "method_name": "OpenAI 'deep research' (reported capability)",
            "method_description": "Described in the paper as a multi-step research-capable tool that can search the internet and assist in writing literature reviews and identifying knowledge gaps; the current paper only cites this capability as an example of AGI/LLM advances in scientific literature mining.",
            "llm_model_used": null,
            "scientific_domain": "General scientific literature (not specific in this paper).",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4295.1",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Google AI co-scientist (mention)",
            "name_full": "Google 'AI co-scientist' (as referenced)",
            "brief_description": "A referenced initiative by Google described as a virtual scientific collaborator to help generate hypotheses, research proposals, and accelerate scientific discovery.",
            "citation_title": "Towards an AI co-scientist",
            "mention_or_use": "mention",
            "method_name": "Google 'AI co-scientist' (referenced initiative)",
            "method_description": "Mentioned as an example of LLM-based systems aiming to assist scientists in hypothesis generation and accelerating discovery; the present paper cites it in context but does not use or evaluate it.",
            "llm_model_used": null,
            "scientific_domain": "General scientific research (cited as an initiative).",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4295.2",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Structured information extraction (Dagdelen et al.)",
            "name_full": "Structured information extraction from scientific text with large language models",
            "brief_description": "A referenced study that uses large language models for structured information extraction from scientific text (cited in context of related work on LLM-based literature mining).",
            "citation_title": "Structured information extraction from scientific text with large language models",
            "mention_or_use": "mention",
            "method_name": "Structured information extraction with LLMs (as per Dagdelen et al.)",
            "method_description": "Cited as related work demonstrating LLMs applied to structured information extraction in scientific texts; specific method details are not provided in this paper and are left to the cited work.",
            "llm_model_used": null,
            "scientific_domain": "Scientific literature (general; the cited work focuses on materials/chemistry contexts but the present paper only cites it as related work).",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4295.3",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Materials data extraction (Polak & Morgan)",
            "name_full": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "brief_description": "A referenced work that applies conversational LLMs and prompt engineering to extract materials data from research papers.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "method_name": "Conversational LLM + prompt engineering for materials data extraction (Polak & Morgan)",
            "method_description": "Cited as a relevant example where conversational LLMs and prompt engineering are used to extract structured data from papers in materials science; the current paper references it as related work but does not provide implementation details.",
            "llm_model_used": null,
            "scientific_domain": "Materials science (extraction of materials data).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structured numerical materials data (as implied by title)",
            "extraction_output_format": "Structured data (implied)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4295.4",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Autonomous chemical research (Boiko et al.)",
            "name_full": "Autonomous chemical research with large language models",
            "brief_description": "A referenced Nature paper demonstrating use of LLMs toward autonomous chemical research workflows (cited as an example of LLMs in research automation).",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "method_name": "Autonomous chemical research pipeline with LLMs (Boiko et al.)",
            "method_description": "Referenced as an example where LLMs are applied to autonomous or semi-autonomous research tasks in chemistry; the present paper cites it as related work but does not provide methodological specifics.",
            "llm_model_used": null,
            "scientific_domain": "Chemistry / autonomous experimentation.",
            "number_of_papers": null,
            "type_of_quantitative_law": "Potentially experimental design and quantitative optimization relationships (implied by title), but not specified here.",
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4295.5",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_scientific_text_with_large_language_models"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "OpenAI's 'deep research' tool: is it useful for scientists?",
            "rating": 1,
            "sanitized_title": "openais_deep_research_tool_is_it_useful_for_scientists"
        },
        {
            "paper_title": "PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge",
            "rating": 1,
            "sanitized_title": "pubtator_30_an_aipowered_literature_resource_for_unlocking_biomedical_knowledge"
        }
    ],
    "cost": 0.01159525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</p>
<p>Jinling Jiang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Jie Hu 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Siwei Xie 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Menghao Guo 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Yuhang Dong 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Shuai Fu 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Xianyue Jiang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Zhenlei Yue 
Junchao Shi 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Xiaoyu Zhang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Minghui Song 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Guangyong Chen 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Hua Lu 
Department of Computer Science
Aalborg University
Denmark</p>
<p>Xindong Wu 
Key Laboratory of Knowledge Engineering with Big Data (The Ministry of Education of China)
Hefei University of Technology
HefeiAnhuiChina</p>
<p>Pei Guo guopei@ibmc.ac.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Da Han dahan@sjtu.edu.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Zeyi Sun sunzey6@gmail.com 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Jiezhong Qiu jiezhongqiu@zju.edu.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature
51E00E249868E257D972F5B9E81396F9
The extraction of molecular annotations from scientific literature is critical for advancing datadriven research.However, traditional methods, which primarily rely on human curation, are labor-intensive and error-prone.Here, we present an LLM-based agentic workflow that enables automatic and efficient data extraction from literature with high accuracy.As a demonstration, our workflow successfully delivers a dataset containing over 91,000 enzyme kinetics entries from around 3,500 papers.It achieves an average F1 score above 0.9 on expert-annotated subsets of protein enzymes and can be extended to the ribozyme domain in fewer than 3 days at less than $90.This method opens up new avenues for accelerating the pace of scientific research.</p>
<p>Main Text</p>
<p>In scientific research, acquiring a vast amount of annotations or "labels" for functional molecules enables the identification of subtle patterns and the development of predictive models.Increasing availability of human-curated scientific datasets facilitates the integration of interdisciplinary insights, driving innovation and accelerating discovery in multiple fields including chemistry [1], biology [2][3][4][5], and material science [6].By the year 2025, the Nucleic Acids Research (NAR) Database issue has collected up to 2236 databases [7], among which many are constructed through human-curated literature mining such as ChEMBL [8] and BRENDA [9].Robust data-driven research will foster more efficient scientific progress, but traditional methods by human annotation and classical natural language processing tools are labor-intensive, time-consuming, and error-prone [10].Therefore, there is an urgent need for paradigm-shifting methods capable of rapidly and accurately extracting annotations from literature.</p>
<p>Recent advancements in Artificial General Intelligence (AGI), particularly Large Language Models (LLMs), have demonstrated superior performance in scientific literature mining such as named entity recognition, relation extraction, summarization, integration and even planning [5,[11][12][13][14][15][16].Most recently, OpenAI has unveiled 'deep research' that can conduct multi-step research on the internet for complex tasks to assist scientists to write literature reviews and even identify knowledge gaps [17], and Google has launched 'AI co-scientist' which is "a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals and to accelerate the clock speed of scientific and biomedical discoveries" [18].</p>
<p>Nevertheless, extracting data-especially quantitative annotations-from literature that contains both tables and text remains challenging.This task requires comprehensive longcontext capabilities, including long-context retrieval and reasoning, tabular data understanding, and basic math (such as scientific notation understanding).Therefore, mining quantitative data from literature naturally serves as a real-world, large-scale benchmark for evaluating the longcontext capabilities of LLMs.This is particularly significant given that most existing longcontext LLM benchmarks [19,20] rely on synthetic data or costly human annotations.</p>
<p>To address the aforementioned challenges, we have developed an agentic workflow for automated extraction, normalization, and standardization of scientific data from the literature (Fig. 1).The agentic workflow starts with collecting a full dataset of relevant literature in PDF format, which are then transformed to machine-readable text by optical character recognition (OCR).Then the text, together with an engineered prompt, is used to query LLMs to extract data of interest.The outputs of multiple LLMs are then considerably aggregated to provide comprehensive and high-confidence results.To evaluate and further optimize our workflow, we manually annotate a small portion of the full dataset (termed as the annotated dataset) to thoroughly evaluate each individual component of the workflow, including the prompt, choice of LLMs, and aggregation strategy.</p>
<p>First, the prompt is carefully engineered to include task background, field description, output format instruction, etc, which are essential for LLMs to process desired information with high accuracy.Second, numerous LLMs have been developed, each with its own strengths and weaknesses.Benchmarking different LLMs allows for a comprehensive evaluation of their capabilities across various dimensions, including long-context reasoning, information retrieval, coreference resolution, unit conversion, and table understanding.Finally, an LLM aggregator is designed to take the strengths of high-performance LLMs towards higher confidence results.</p>
<p>We first apply our workflow to extract kinetic data of protein enzymes, which is an important domain in the field of biochemistry with well-established databases such as BRENDA [9].This allows us to thoroughly assess the performance of our workflow compared to existing humancurated databases.Following the workflow in Fig. 1, we extract three types of kinetic parameters, including Km, kcat and kcat/Km, from 3,435 papers curated in BRENDA.</p>
<p>To rigorously evaluate extraction performance, we randomly select 156 papers and manually annotate them with a team of experts, resulting in an annotated dataset that contains 3,563 Km, 3,531 kcat, and 3,751 kcat/Km entries.The extracted results are compared to the expert annotation to calculate metrics including paper-wise precision, recall, and F1-score (Methods, Supplementary Table 1).We evaluate four popular LLMs, including two closed-source ones (Claude3.5 and gpt-4o) and two open-source ones (Llama3 and Qwen), with our engineered prompt (Fig. 2a, d).Claude3.5 is identified as the top performer in terms of F1-score distribution (mean=0.90,median=0.99,25%/75% percentile=0.90/1.00)and exhibits more consistent performance across different temperature settings (Supplementary Fig. 1).However, we observe that, despite their lower mean and median F1 scores, the other three LLMs demonstrate distinct strengths in resolving certain challenging cases encountered by Claude3.5 (Examples of these challenging cases are discussed in Supplementary Note 1).Inspired by ensemble learning, we develop an aggregation agent to leverage the strengths of individual LLMs.In particular, we prompt Claude3.5 with the paper text, along with the outputs from Claude3.5, gpt-4o, Llama3, and Qwen, and then instruct it to consolidate the insights from these LLMs (Fig. 2e).Encouragingly, the aggregation agent is indeed capable of integrating the advantages of multiple LLMs and achieves a comprehensively enhanced performance, especially in reducing bad cases (Fig. 2b).Finally, we investigate whether LLM could surpass human intelligence in literature mining by comparing results produced by LLMs with data curated in BRENDA (Methods).For the 156 papers in our annotated dataset, BRENDA achieves suboptimal F1 scores (Fig. 2c, mean=0.76,median=0.78,25%/75% percentile=0.64/0.95)compared to any single LLM, or the advanced aggregation agent.We also validate the performance of our pipeline on the full dataset, comprising 3,435 papers, which shows that our pipeline maintains a quality comparable to that of the annotated dataset (Supplementary Note 2).As an important complement to enzymes, certain RNA molecules, known as ribozymes, possess catalytic activities and pivotal functions in gene regulation.However, to the best of our knowledge, there is no database service available for ribozyme kinetic data.Therefore, we next apply our workflow to the ribozyme research field where a brand-new database needs to be established from scratch.We collect a total of 164 papers from PubMed and bioRxiv by searching the keywords 'ribozymes and (kcat or Km or kcat/Km or kobs or kcleave)', which can be approximately considered as the full dataset of ribozyme kinetic literature.We apply our pipeline to extract five kinetic parameters, including kcat, Km, kcat/Km, kobs, and kcleave, and compare the extracted results with expert annotation (Supplementary Table 2).Again, Claude3.5 shows an exceptional performance with the highest mean F1 score of 0.75 (Fig. 3a).</p>
<p>An aggregation agent that integrates these four LLMs does not perform better than Claude3.5, which is probably lagged by the poorer performances of the other three LLMs, especially gpt-4o and Qwen with mean F1 scores below 0.6.However, integrating Claude3.5 and Llama3 can achieve superior performance (Fig. 3b), which suggests the need for a smarter aggregation agent that should automatically vote out poorly performed LLMs during ensemble.Remarkably, our pipeline is economical and time-saving, i.e., it takes less than 90$ (Fig. 3c) to deliver the final extraction results within 3 days (2 days for literature collection and 1 day for running OCR software and LLMs).This demonstrates the efficiency of our workflow in acquiring knowledge for a specific research field with significantly improved data quality and reduced labor cost.To the end, we release a new archive 1 that offers a structured and accessible collection of 1 https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Archive-LLENKA enzyme kinetic data, including 88,770 entries from 3,435 protein enzyme papers and 2,420 entries from 164 ribozyme papers (Supplementary Fig. 2).The archive contains kinetic parameters with their corresponding enzyme names, mutants, organisms, substrates, and experimental conditions, offering a valuable resource to the field of biochemistry.In addition, this archive serves as a real-world large-scale long-context LLM benchmark, with an average token count of 25k (385 papers containing over 32k tokens), allowing us to assess LLMs' capabilities in understanding long scientific literature.</p>
<p>In sum, we have developed an LLM-based agentic workflow for automatic data extraction from scientific literature, providing a streamlined pipeline that operates without strong assumptions or prior knowledge for the domain of interest.By demonstrating the high accuracy and efficiency of our workflow in the domains of protein enzyme and ribozyme, we anticipate it can also be robustly adapted to other research fields.This work validates and benchmarks the superiority of LLMs in literature mining compared to human efforts.In the future, we aim to refine the enzyme archive to facilitate new discoveries and further extend our workflow to other fields towards a versatile paradigm and a real LLM benchmark in scientific research.</p>
<p>Methods</p>
<p>Literature Collection</p>
<p>The papers about protein enzyme are collected from BRENDA whose core information is manually extracted from original scientific literature from PubMed and Scopus.We get a collection of PubMed Identifier (PMID) from BRENDA database through the BRENDA SOAP (Simple Object Access Protocol) services.The number of PMIDs we collect from BRENDA is more than 90,000 among which 4,925 papers have recorded the experimental results of enzyme kinetics parameters.We finally download 3,435 papers in PDF format by PMIDs as we do not have access to the remaining 1,490 ones.For these papers, we implement a crawler program to download their corresponding enzyme kinetic data from BRENDA.</p>
<p>As for the ribozyme, the 164 papers are collected from PubMed and bioRxiv by searching the keywords 'ribozymes and (kcat or Km or kcat/Km or kobs or kcleave)'.</p>
<p>Construction Expert-Annotated Datasets</p>
<p>We engage a team of 10 highly qualified experts to meticulously annotate, with 6 of them holding PhD degrees in relevant fields and the remaining 4 pursuing their PhDs in related disciplines.Through the annotation process, we ensure precision at every step, employing rigorous validation and repeated confirmations.We designate this as our gold standard for benchmarking, which we believe is exceptionally close to the true values.The final annotated dataset is structured as a wide table with aligned fields, including enzyme, mutant, organism, substrate, experimental conditions (such as temperatures and pHs), and kinetic data fields.</p>
<p>OCR Pre-Processing</p>
<p>OCR is performed to convert PDF format to text format while preserving mathematical content and complex tables.We compare two OCR approaches: Mathpix and PyMuPDF, where we use Claude3.5 as the default LLM to extract protein enzyme data from the annotated datasets.As shown in Supplementary Fig. 3, Mathpix achieves a similar median F1 score compared to PyMuPDF (0.99 vs 1.00) but demonstrated a slightly better mean F1 score (0.90 vs 0.87).</p>
<p>Therefore, we select Mathpix as the default OCR software in our pipeline.</p>
<p>Prompt Engineering</p>
<p>The procedure of prompt engineering contains several key steps:</p>
<ol>
<li>Evaluate the prompt using all samples in the annotated dataset.</li>
</ol>
<p>2.</p>
<p>Manually analyze problematic cases from samples with low precision or recall, identifying common errors.</p>
<ol>
<li>For each identified error, we manually update the instruction part of the prompt.</li>
</ol>
<p>The optimized prompt comprises three key components: task description, instructions, and format reference.The task description specifies the fields to be extracted.The instructions integrate essential reminders from the prompt engineering process.The format reference provides a template for the expected output.We selected Markdown as the output format because it avoids issues with separators (e.g., commas in compound names) and enables direct table visualization during prompt engineering, a feature lacking in other formats like JSON.</p>
<p>The final prompt is listed in Supplementary Note 3. The prompt for the aggregation agent is listed in Supplementary Note 4.</p>
<p>Post-Processing</p>
<p>The post-processing program organizes and cleans the outputs of LLMs, converting them into a standardized data frame format.This transformation enables downstream numerical comparison and evaluation.The program utilizes the Pandas library to structure the extracted data, primarily focusing on the outputs that present a table with 13 columns delineated by pipe delimiters.It removes any non-tabular descriptive content, aligns column headers, and implements necessary data-cleaning procedures while maintaining the integrity of the dataset.</p>
<p>The cleaning procedures consists of several critical tasks:</p>
<ol>
<li>
<p>Space cleaning.It removes undesired non-breaking spaces, fills empty entries with NA (Not Applicable) to avoid leaving blank spaces, and eliminates any rows where all entries are marked as NA.</p>
</li>
<li>
<p>Invalid field cleaning.Values under certain columns carrying units corresponding to other parameters will be marked as NA.</p>
</li>
</ol>
<p>Evaluation Metrics</p>
<p>The performance metrics (precision, recall, and F1) are computed based on the kinetic constants (i.e., kcat, Km, kcat/Km, kobs, and kcleave), including both their numerical values and corresponding units.We observe that it is highly unlikely for a paper to contain multiple data entries with identical kinetic constants, even when papers have dozens or hundreds of entries.</p>
<p>In other words, these constants can function as "unique identifiers," reducing the need for</p>
<p>Fig. 1
1
Fig. 1 Schematic of our LLM-based agentic workflow for enzyme kinetic data extraction.</p>
<p>Fig. 2
2
Fig. 2 Protein Enzyme Kinetic Data Extraction.a-c, Violin plots show the distribution of paper-wise F1 scores of each LLM, sorted by mean F1 (a), the aggregation agent that integrates Claude3.5, gpt-4o, Llama3, and Qwen (b), and BRENDA (c) on the expert-annotated dataset (156 papers).Black bar: median.White bar: mean.25% and 75% percentile and whiskers are also plotted.d-e, The engineered prompts for running single LLM (d) and aggregation agent (e).</p>
<p>Fig. 3
3
Fig. 3 Ribozyme kinetic data extraction.a, Violin plots show the distribution of paper-wise F1 scores of each LLM against our expert-annotated dataset sorted by mean F1 (164 ribozyme papers).b, The performance of aggregation agent that integrates four LLMs including Claude3.5, gpt-4o, Llama3, and Qwen (left), and two LLMs including Claude3.5 and Llama3 (right).Black bar: median.White bar: mean.25% and 75% percentile and whiskers are also plotted.c, The per-step cost of our pipeline when processing the 164 ribozyme papers.</p>
<p>3 .
3
Numerical cleaning.It employs regular expressions to identify various representations of entries with scientific notations and/or error metrics.4. Unit conversion.It involves recognizing the extracted units and standardizing the values of Km, kcat, and kcat/Km to mM, s^-1, and s^-1mM^-1, respectively, by applying specified conversion factors.This program transforms raw data extracted from LLM outputs into a clean and standardized format, ensuring data consistency and facilitating comparability for subsequent evaluations and applications.</p>
<p>AcknowledgmentsThis research is supported by the National Natural Science Foundation of China (62120106008, 62306290), the "Pioneer" and "Leading Goose" R&amp;D Program of Zhejiang (2024SSYS0007).We would like to express our gratitude to the Qwen Team at Alibaba for providing access to the free Qwen API.Data AvailabilityOur annotated dataset (156 protein enzyme papers and 164 ribozyme papers) is available at https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Golden-Benchmark.Our released enzyme archive (88,770 entries from 3,435 protein enzyme papers and 2,420 entries from 164 ribozyme papers) is available at https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Archive-LLENKA.Code AvailabilityAll codes used in this study have been deposited on GitHub:https://github.com/JackKuo666/LLM-BioDataExtractoradditional processing after extraction and thereby simplifying the automatic evaluation process.More formally, the paper-wise precision of each paper in the annotated dataset is calculated using the formula: In some rare cases, the calculation of the above metrics can cause an error of division by zero.Take the computation of precision for example, this can happen if the LLM fails to extract any results, which makes the true positive (TP) as well as the false positive (FP) zero.For these special cases, we follow the common strategy:• If the true positive (TP), false positive (FP), and false negative (FN) are all zeros, the precision, recall, and F1 are 1.This means that the LLM correctly knows "it doesn't know" when there is indeed nothing to be extracted.• If true positives (TP) is zero and at least one of the two others (i.e., FP and FN) is not, the precision, recall and F1 are all zeros.Supplementary Tables1 and 2show the average metrics across all protein enzyme papers and ribozyme ones, respectively.We consider two ways to average ---macro and micro.The macro-averaged metrics (or macro metrics) are computed by taking the arithmetic mean (a.k.a.unweighted mean) of all the paper-wise metrics.For micro metrics, the counters (i.e., true positive, false positive, false negative) of all papers are summed up, which are then used to calculate a single micro precision/recall/F1 value.However, the above metrics may not be directly applicable when comparing to data entries recorded in BRENDA, as BRENDA involves manual extraction where different levels of rounding are applied.For example, a Km value of 0.835 might be rounded to 0.8 or 0.84 in BRENDA.Therefore, when comparing with BRENDA, we consider an extraction to be correct if rounding to any position between the 1st and 6th decimal place results in an exactly matched value.Competing interestsThe authors declare no competing interests.
Version of LLMs: Claude-3.5-sonnet-20240620, Qwen-plus-0806, gpt-4o. September 23, 2024Llama-3</p>
<p>Llama3 is locally deployed, while the other LLMs were used through online APIs. </p>
<p>The maximum outputs of different LLMs vary, which is discussed in our paper: gpt-4o's output capacity is 4096 tokens; Claude3.5's output capacity is 8192 tokens; Qwen-Plus's output capacity is 8000 tokens. and Llama3's output capacity is 4096 tokens</p>
<p>Due to local GPU resource limitations, Llama3 used a maximum input of 32k tokens. References. </p>
<p>Sequential closed-loop Bayesian optimization as a guide for organic molecular metallophotocatalyst formulation discovery. X Li, Che Y Chen, L Liu, T Wang, K Liu, L Yang, H Pyzer-Knapp, E O Cooper, A I , 10.1038/s41557-024-01546-5Nat Chem. 1682024 Aug</p>
<p>Accurately predicting enzyme functions through geometric graph learning on ESMFold-predicted structures. Y Song, Q Yuan, S Chen, Y Zeng, H Zhao, Y Yang, 10.1038/s41467-024-52533-wNat Commun. 15181802024 Sep 18</p>
<p>UniKP: a unified framework for the prediction of enzyme kinetic parameters. H Yu, H Deng, J He, J D Keasling, X Luo, 10.1038/s41467-023-44113-1Nat Commun. 14182112023 Dec 11</p>
<p>Deep learning-based kcat prediction enables improved enzyme-constrained model reconstruction. F Li, L Yuan, H Lu, Nat Catal. 52022</p>
<p>Evaluation of large language models for discovery of gene set function. M Hu, S Alkhairy, I Lee, R T Pillich, D Fong, K Smith, R Bachelder, T Ideker, D Pratt, Nat Methods. 2025</p>
<p>. 10.1038/s41592-024-02525-xJan22</p>
<p>Nature of metal-support interaction for metal catalysts on oxide supports. T Wang, J Hu, R Ouyang, Y Wang, Y Huang, S Hu, W X Li, 10.1126/science.adp6034Science. 38667242024 Nov 22</p>
<p>The 2025 Nucleic Acids Research database issue and the online molecular biology database collection. D J Rigden, X M Fernández, 10.1093/nar/gkae1220Nucleic Acids Res. 53D12025 Jan 6</p>
<p>Activity, assay and target data curation and quality in the ChEMBL database. G Papadatos, A Gaulton, A Hersey, J P Overington, 10.1007/s10822-015-9860-5J Comput Aided Mol Des. 2992015 Sep</p>
<p>A Chang, L Jeske, S Ulbrich, J Hofmann, J Koblitz, I Schomburg, M Neumann-Schaal, D Jahn, D Schomburg, Brenda, 10.1093/nar/gkaa1025ELIXIR core data resource in 2021: new developments and updates. 2021 Jan 849</p>
<p>Enzyme Databases in the Era of Omics and Artificial Intelligence. U Prešern, M Goličnik, 10.3390/ijms242316918Int J Mol Sci. 2423169182023 Nov 29</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024 Feb</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, R Leaman, S Tian, L Luo, Jin Q Wang, Z Chen, Q Lu, Z , 10.1093/nar/gkae235Nucleic Acids Res. 52W12024 Jul 5</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K A Persson, A Jain, 10.1038/s41467-024-45563-xNat Commun. 15114182024 Feb 15</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nat Commun. 15115692024 Feb 21</p>
<p>Closing the gap between open-source and commercial large language models for medical evidence summarization. G Zhang, Jin Q Zhou, Y Wang, S Idnay, B R Luo, Y Park, E Nestor, J G Spotnitz, M E Soroush, A Campion, T Lu, Z Weng, C Peng, Y , Preprint</p>
<p>Update in: NPJ Digit Med. 10.1038/s41746-024-01239-warXiv:2408.00588v12024 Jul 25. 2024 Sep 97239</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 62479922023 Dec</p>
<p>OpenAI's 'deep research' tool: is it useful for scientists? Nature. N Jones, 10.1038/d41586-025-00377-92025 Feb 6</p>
<p>. J Gottweis, W Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomašev, Y Guan, V Dhillon, E D Vaishnav, B Lee, T R Costa, J R Penad'es, G Peltz, Y Xu, A Pawlosky, A Karthikesalingam, V Natarajan, 2025Towards an AI co-scientist</p>
<p>RULER: What's the Real Context Size of Your Long-Context Language Models. C P Hsieh, S Sun, S Kriman, S Acharya, D Rekesh, F Jia, B Ginsburg, First Conference on Language Modeling. </p>
<p>ınftyBench: Extending Long Context Evaluation Beyond 100K Tokens. X Zhang, Y Chen, S Hu, Z Xu, J Chen, M K Hao, X Han, Z L Thai, S Wang, Z Liu, M Sun, ACL. 2024. January.</p>            </div>
        </div>

    </div>
</body>
</html>
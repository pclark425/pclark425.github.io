<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7130 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7130</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7130</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-5eb3f80f30399ff82c322166738cc15e3b066432</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5eb3f80f30399ff82c322166738cc15e3b066432" target="_blank">Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> The goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user, and this approach pairs an LM with a corrector model, trained to translate general feedback into specific edits to repair the model output.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for script generation, that takes a goal (e.g.,"bake a cake") and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, FBNet, learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at https://github.com/allenai/interscript/.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7130.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7130.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBNET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feedback-based Repair Network with Memory (FBNET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed repair pipeline that pairs a fixed base generator with a learned corrector (T5-XXL) that translates natural-language user feedback into concrete graph-edit operations, plus a growing memory of past (erroneous-output, feedback) pairs used to retrieve reusable feedback for new inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XXL (corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A T5-XXL text-to-text transformer (initialized from T5 pretraining) fine-tuned to generate graph-edit operations y^e conditioned on an erroneous graph x and retrieved natural-language feedback fb; retrieval uses a Sentence-BERT encoder and cosine similarity (threshold 0.9) to find matching past keys in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>User-feedback repair with memory (FBNET)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given a generated (possibly erroneous) structured output x, retrieve similar prior feedback from a memory M, concatenate retrieved feedback fb with x, generate a specific edit y^e with the corrector model G, and apply that edit deterministically to x to obtain the repaired output y. New user feedback is also written to memory for future retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-step generate-then-apply with continual-memory accumulation across a query stream</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Script generation (partially-ordered event graphs, ProScript setting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a partially-ordered sequence of essential events (a script) for a given goal; outputs are graphs (nodes = events, edges = temporal ordering constraints) represented in DOT.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact Match (EM) on generated edit, EM_loc / EM_type (location/type components), BLEU, ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>NO-FB baseline: EM 6.94% (on reuse dataset, Table 4); (also reported NO-FB EM 3.5% in Table 2 for interpreting oracle feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>FBNET (retrieved feedback): EM 16.72% (on reuse dataset, Table 4); FBNET (end-to-end with retrieval+memory growth) shows improving accuracy as memory grows (Figure 4). (When given oracle feedback, FBNET_o EM reaches 38.6% on the interpret-feedback test in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations reported in the paper: (1) method depends on the base model producing 'repairable' outputs (Assumption A1); (2) feedback must be reusable/generalizable (Assumption A2) — costly to collect and domain-dependent; (3) retrieval is simple (Sentence-BERT + cosine, threshold 0.9) and may miss useful feedback or require more advanced aggregation from multiple memories; (4) system is sensitive to feedback quality — irrelevant/misleading feedback causes performance to drop dramatically (to ~3% EM); (5) the corrector often struggles to localize the exact erroneous node/edge (localization errors ~20% of failures) and to handle abstract or challenging feedback (~24% of failures); (6) EM undercounts semantically-correct variations (lexical variation in ~36% of errors); (7) poor performance on certain edit types (e.g., missing-step EM very low 2.8%, add-partial-order EM 10.5%); (8) memory pollution/adversarial feedback could degrade performance; (9) no multi-feedback aggregation or complex retrieval trained end-to-end was demonstrated (requires more data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7130.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7130.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBNET_o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FBNET with Oracle Feedback (FBNET_o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An oracle-control variant of FBNET in which ground-truth (oracle) natural-language feedback is provided to the corrector G at inference to measure the upper-bound ability of G to translate NL feedback into edits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XXL (corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same T5-XXL corrector fine-tuned to map (x, fb) -> edit y^e, evaluated with oracle (gold) fb provided at test time to measure how well the model can operationalize correct feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Oracle-provided feedback (upper-bound evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Provide gold/guided NL feedback to the corrector for each erroneous example and generate the corresponding edit; used to measure corrector capacity absent retrieval/noise in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-step generate-then-apply (oracle fb supplied per query)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Script generation (interpret NL feedback to produce repair edits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate oracle natural-language feedback about a generated script into a concrete edit (insert/remove/reorder node/edge) that repairs the script.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact Match (EM) on edit, EM_loc, EM_type, BLEU, ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>NO-FB baseline (no feedback): EM 3.5% (Table 2, interpret-feedback test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>FBNET_o (oracle feedback): EM 38.6% (Table 2) on the interpret-feedback test; also reported FBNET_o EM 22.2% on the reuse dataset (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with oracle feedback, the corrector shows weaknesses: localization is imperfect (lower EM_loc than EM_type), partial-order edits and missing-step exact matches remain challenging; oracle evaluation is not a realistic deployment scenario — retrieval/noisy feedback reduces performance; also EM metric undercounts semantically-correct but lexically-different edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7130.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7130.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NO-FB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-Feedback Baseline (NO-FB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline model that predicts repair edits from the erroneous script x alone, i.e., the corrector trained without receiving NL feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XXL (baseline learner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XXL fine-tuned to generate graph edits conditioned only on the erroneous graph x (no feedback input), used as a point-of-comparison to measure value of feedback and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>none (no self-reflection / no user feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Predict repair edit directly from x without any external feedback or memory retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Script generation / edit prediction (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an erroneous script x, predict the repair edit y^e without any feedback signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact Match (EM), BLEU, ROUGE (same edit-level metrics used throughout)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>This is the 'before' condition: EM 3.5% (Table 2, interpret-feedback test) and EM 6.94% (Table 4, reuse dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>N/A (no reflection applied)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Predicting edits from x alone performs poorly compared to methods that leverage feedback (very low EM). This baseline cannot leverage reusable corrective knowledge and so cannot improve via memory; it underperforms especially on tasks that require inferring the user's intended correction from NL feedback or prior cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>NL-EDIT: Correcting semantic parse errors through natural language interaction <em>(Rating: 2)</em></li>
                <li>BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief <em>(Rating: 2)</em></li>
                <li>Graph-based, self-supervised program repair from diagnostic feedback <em>(Rating: 2)</em></li>
                <li>Teaching pre-trained models to systematically reason over implicit knowledge <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7130",
    "paper_id": "paper-5eb3f80f30399ff82c322166738cc15e3b066432",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "FBNET",
            "name_full": "Feedback-based Repair Network with Memory (FBNET)",
            "brief_description": "A deployed repair pipeline that pairs a fixed base generator with a learned corrector (T5-XXL) that translates natural-language user feedback into concrete graph-edit operations, plus a growing memory of past (erroneous-output, feedback) pairs used to retrieve reusable feedback for new inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XXL (corrector)",
            "model_description": "A T5-XXL text-to-text transformer (initialized from T5 pretraining) fine-tuned to generate graph-edit operations y^e conditioned on an erroneous graph x and retrieved natural-language feedback fb; retrieval uses a Sentence-BERT encoder and cosine similarity (threshold 0.9) to find matching past keys in memory.",
            "model_size": "11B",
            "reflection_method_name": "User-feedback repair with memory (FBNET)",
            "reflection_method_description": "Given a generated (possibly erroneous) structured output x, retrieve similar prior feedback from a memory M, concatenate retrieved feedback fb with x, generate a specific edit y^e with the corrector model G, and apply that edit deterministically to x to obtain the repaired output y. New user feedback is also written to memory for future retrieval.",
            "iteration_type": "single-step generate-then-apply with continual-memory accumulation across a query stream",
            "num_iterations": null,
            "task_name": "Script generation (partially-ordered event graphs, ProScript setting)",
            "task_description": "Generate a partially-ordered sequence of essential events (a script) for a given goal; outputs are graphs (nodes = events, edges = temporal ordering constraints) represented in DOT.",
            "evaluation_metric": "Exact Match (EM) on generated edit, EM_loc / EM_type (location/type components), BLEU, ROUGE",
            "performance_before_reflection": "NO-FB baseline: EM 6.94% (on reuse dataset, Table 4); (also reported NO-FB EM 3.5% in Table 2 for interpreting oracle feedback)",
            "performance_after_reflection": "FBNET (retrieved feedback): EM 16.72% (on reuse dataset, Table 4); FBNET (end-to-end with retrieval+memory growth) shows improving accuracy as memory grows (Figure 4). (When given oracle feedback, FBNET_o EM reaches 38.6% on the interpret-feedback test in Table 2.)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Limitations reported in the paper: (1) method depends on the base model producing 'repairable' outputs (Assumption A1); (2) feedback must be reusable/generalizable (Assumption A2) — costly to collect and domain-dependent; (3) retrieval is simple (Sentence-BERT + cosine, threshold 0.9) and may miss useful feedback or require more advanced aggregation from multiple memories; (4) system is sensitive to feedback quality — irrelevant/misleading feedback causes performance to drop dramatically (to ~3% EM); (5) the corrector often struggles to localize the exact erroneous node/edge (localization errors ~20% of failures) and to handle abstract or challenging feedback (~24% of failures); (6) EM undercounts semantically-correct variations (lexical variation in ~36% of errors); (7) poor performance on certain edit types (e.g., missing-step EM very low 2.8%, add-partial-order EM 10.5%); (8) memory pollution/adversarial feedback could degrade performance; (9) no multi-feedback aggregation or complex retrieval trained end-to-end was demonstrated (requires more data).",
            "uuid": "e7130.0",
            "source_info": {
                "paper_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "FBNET_o",
            "name_full": "FBNET with Oracle Feedback (FBNET_o)",
            "brief_description": "An oracle-control variant of FBNET in which ground-truth (oracle) natural-language feedback is provided to the corrector G at inference to measure the upper-bound ability of G to translate NL feedback into edits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XXL (corrector)",
            "model_description": "Same T5-XXL corrector fine-tuned to map (x, fb) -&gt; edit y^e, evaluated with oracle (gold) fb provided at test time to measure how well the model can operationalize correct feedback.",
            "model_size": "11B",
            "reflection_method_name": "Oracle-provided feedback (upper-bound evaluation)",
            "reflection_method_description": "Provide gold/guided NL feedback to the corrector for each erroneous example and generate the corresponding edit; used to measure corrector capacity absent retrieval/noise in memory.",
            "iteration_type": "single-step generate-then-apply (oracle fb supplied per query)",
            "num_iterations": null,
            "task_name": "Script generation (interpret NL feedback to produce repair edits)",
            "task_description": "Translate oracle natural-language feedback about a generated script into a concrete edit (insert/remove/reorder node/edge) that repairs the script.",
            "evaluation_metric": "Exact Match (EM) on edit, EM_loc, EM_type, BLEU, ROUGE",
            "performance_before_reflection": "NO-FB baseline (no feedback): EM 3.5% (Table 2, interpret-feedback test)",
            "performance_after_reflection": "FBNET_o (oracle feedback): EM 38.6% (Table 2) on the interpret-feedback test; also reported FBNET_o EM 22.2% on the reuse dataset (Table 4)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Even with oracle feedback, the corrector shows weaknesses: localization is imperfect (lower EM_loc than EM_type), partial-order edits and missing-step exact matches remain challenging; oracle evaluation is not a realistic deployment scenario — retrieval/noisy feedback reduces performance; also EM metric undercounts semantically-correct but lexically-different edits.",
            "uuid": "e7130.1",
            "source_info": {
                "paper_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "NO-FB",
            "name_full": "No-Feedback Baseline (NO-FB)",
            "brief_description": "A baseline model that predicts repair edits from the erroneous script x alone, i.e., the corrector trained without receiving NL feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XXL (baseline learner)",
            "model_description": "T5-XXL fine-tuned to generate graph edits conditioned only on the erroneous graph x (no feedback input), used as a point-of-comparison to measure value of feedback and memory.",
            "model_size": "11B",
            "reflection_method_name": "none (no self-reflection / no user feedback)",
            "reflection_method_description": "Predict repair edit directly from x without any external feedback or memory retrieval.",
            "iteration_type": "none",
            "num_iterations": null,
            "task_name": "Script generation / edit prediction (baseline)",
            "task_description": "Given an erroneous script x, predict the repair edit y^e without any feedback signal.",
            "evaluation_metric": "Exact Match (EM), BLEU, ROUGE (same edit-level metrics used throughout)",
            "performance_before_reflection": "This is the 'before' condition: EM 3.5% (Table 2, interpret-feedback test) and EM 6.94% (Table 4, reuse dataset)",
            "performance_after_reflection": "N/A (no reflection applied)",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Predicting edits from x alone performs poorly compared to methods that leverage feedback (very low EM). This baseline cannot leverage reusable corrective knowledge and so cannot improve via memory; it underperforms especially on tasks that require inferring the user's intended correction from NL feedback or prior cases.",
            "uuid": "e7130.2",
            "source_info": {
                "paper_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "NL-EDIT: Correcting semantic parse errors through natural language interaction",
            "rating": 2
        },
        {
            "paper_title": "BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief",
            "rating": 2
        },
        {
            "paper_title": "Graph-based, self-supervised program repair from diagnostic feedback",
            "rating": 2
        },
        {
            "paper_title": "Teaching pre-trained models to systematically reason over implicit knowledge",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1
        }
    ],
    "cost": 0.011620499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback</h1>
<p>Niket Tandon ${ }^{+}$, Aman Madaan ${ }^{* \dagger}$, Peter Clark, Yiming Yang ${ }^{\dagger}$, Allen Institute for Artificial Intelligence, Seattle, WA, USA<br>${ }^{\dagger}$ Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA<br>{nikett, peterc}@allenai.org<br>{amadaan,yiming}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for script generation, that takes a goal (e.g., "bake a cake") and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, FBNET, learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) have achieved remarkable success on many tasks (Wang et al., 2019; Talmor et al., 2019), but they are still prone to mistakes (Bender and Koller, 2020). Correcting mistakes by retraining is not always easy due to the cost and/or unpredictability of how additional training data will change the model. Instead, our goal is to allow users to correct such errors directly through interaction, without retraining - by giving corrective feedback on the model's output. Our approach is to maintain a growing, dynamic memory of such</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a frozen model B, we train a corrector model $\mathbf{G}$ to apply feedback from a user about errors made by the original model. In the example, $\mathbf{B}$ has generated a script with an error in, stating that "driving" and "getting in a car" can occur in any order (red box). The user provides general feedback ("Get in a car before driving"), and $\mathbf{G}$ operationalizes this to generate a corrected graph (by predicting and applying a graph edit operation) in which "get in car" happens first (green box). The feedback is stored in a memory $\mathcal{M}$ so it can also be retrieved to repair similar, future errors.
feedback, and use a trained corrector model to apply such feedback to repair the model output. By doing so, the system can also potentially fix output errors for new unseen inputs using feedback from similar, past cases. The ability to leverage a fixed trained model without re-training could save costs and have a positive environmental impact.</p>
<p>We consider the class of problems where the model's output is repairable, namely a structured output that is (typically) nearly correct, and fixable through a small number of edit operations. Our system is general and admits a general graph based input, so in principle it applies to a large number of tasks. In this paper, we apply our approach to the task of script generation that provides a natural setting for users to critique, and has applications in smart assistants (Zhang et al., 2021). We use an existing, fixed model: proScript (Sakaguchi et al.,</p>
<p>2021) that satisfies the constraint of the model's output to be repairable. proScript takes as input a goal to achieve (expressed in natural language), and outputs a partially ordered sequence of steps a script - required to achieve that goal. Our interest here is not in proScript itself, but in what to do when proScript's output contains an error.</p>
<p>This instantiation of our approach is illustrated in Figure 1. Here, proScript has generated a script $\mathbf{x}$ to achieve the goal "see an alligator", but the script contains an error: it states that the steps of "driving to the zoo" and "get in car" can be applied in any order. To repair this, the user provides the general feedback "Get in a car before driving". The corrector model $\mathbf{G}$ then takes that feedback and the erroneous script, translates it into appropriate edit operations on the script, and applies those edits to generate a corrected script ( $\mathbf{y}$ in Figure 1). The feedback is stored in memory $\mathcal{M}$ so it can also be retrieved in the future. Our system, FBNET, comprises the corrector module $\mathbf{G}$, the memory $\mathcal{M}$, and searching and writing operations. To train our system, we collect examples of bad outputs, general feedback, and specific edits that the feedback should translate to (Section 4.2). This allows $\mathbf{G}$ to learn how to translate general feedback into specific edits to apply. Pairing $\mathbf{G}$ with the memory $\mathcal{M}$ allows FBNET to repair new, unseen scripts containing similar errors to the one the user corrected.</p>
<p>Our approach loosely follows some early AI systems that maintained a memory of the output problems and how to fix them (Sussman, 1973; Hammond, 1986; Riesbeck, 1981), but here, we use neural methods and interact with a user to provide corrective feedback. It also builds on the idea of allowing users to specify edits in natural language, e.g., NLEdit (Elgohary et al., 2021), except we use general user feedback (then translated to examplespecific edits by $\mathbf{G}$ ) and add a memory so that feedback can also be automatically reused.</p>
<p>We evaluate FBNET along two dimensions: (a) How well does FBNET interpret NL feedback? (b) How well can FBNET learn from prior mistakes? We find that (a) it uses NL feedback effectively to repair script errors, with $+30 \%$ (absolute) improvement over a baseline that does not use feedback, and that (b) it makes a start at avoiding past mistakes ( $+7 \%$ (absolute) improvement in a controlled setting). Although these results are only for a single deployment of our general approach, they suggest that memory-based architectures can help deployed
models continue to improve with time, without retraining, potentially broadening their utility.</p>
<h2>2 Related work</h2>
<p>There have been numerous approaches to using user feedback to improve model performance, including:
(1) Providing additional training examples: Dasgupta et al. (2019) show how a user can correct bad model behavior by carefully selecting new training examples for the system to learn from, a style of interactive active learning (Settles, 2012).
(2) Marking/scoring the system's answer(s): In SHRDLURN, the user provides feedback by identifying which of the system's alternative interpretations of a user command is correct (Wang et al., 2016).
(3) Providing hints: (Mehta and Goldwasser, 2019) show how a system can learns to understand regional (e.g., "top left") and directional (e.g., "move down") hints from the user for a (simulated) robot.
(4) Provide additional information: In TeachYourAI (Talmor et al., 2020), given a wrong answer to a question, users can enter NL facts and rules to use as context when reasking the question, to (ideally) produce the correct answer.
(5) Correcting bad answers: In the semantic parsing task of NL-to-SQL, NLEdit learns to interpret and apply syntactic edit operations from the user expressed in NL, e.g., "replace course id with program id." (Elgohary et al., 2021).</p>
<p>These methods all augment/replace the standard use of automated answer feedback (if available), e.g., testing whether a semantic parse correctly executes to the correct answer, e.g., (Zettlemoyer and Collins, 2005), sometimes using unsupervised techniques to generate additional training data, e.g., BIFI (Yasunaga and Liang, 2021).</p>
<p>Our work expands on the above approaches in two important ways. First, users provide general feedback in NL, that can potentially be applied to multiple cases (rather than just correcting a specific instance). The corrector model $\mathbf{G}$ is trained to operationalize that advice in different ways for different examples appropriately, in contrast to (say) NLEdit where the user-provided specific corrective edits for a single example only.</p>
<p>Second, we use a feedback memory, allowing feedback to be reused. While adding external mem-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Proposed architecture: (left) $\mathbf{B}$ does not account for user feedback. (right) FBNET maintains a memory $\mathcal{M}$ of corrective feedback, and searches for feedback from prior queries with similar error intent as $\mathbf{x}$ using a retrieval function $\Omega$. $\mathbf{x}$ is then concatenated to the retrieved feedback to form the input to the corrector model $\mathbf{G}$. Users can also give new feedback which is added to $\mathcal{M}$. In this work, we focus on script generation models that might generate an erroneous script which are correctable using an edit (feedback).
ory to neural systems is not new, e.g., RAG (Lewis et al., 2020), REALM (Guu et al., 2020), ours is the first to utilize a memory of prior user feedback to improve future neural model performance. This can be viewed as a modern approach to failuredriven reminding, an essential theme in earlier AI and Cognitive Science research (Riesbeck, 1981; Schank and Leake, 1989; Ross, 1984).</p>
<h2>3 FBNET</h2>
<h3>3.1 Overview of the Architecture</h3>
<p>Fig. 2 gives an overview of FBNET. The input is a potentially noisy graph $\mathbf{x}$ generated by a base model $\mathbf{B}$ and the output $\mathbf{y}$ is a corrected graph. At inference time, i.e., after deployment, a user can critique $\mathbf{y}$ by providing natural language feedback fb on an error e. As output, the model generates the corrected graph $\mathbf{y}$ that accounts for fb.</p>
<p>The corrector model $\mathbf{G}$ is responsible for improving the potentially noisy output from $\mathbf{B}$. $\mathbf{G}$ achieves it using user feedback stored in a continuously updated memory $\mathcal{M}$.</p>
<p>The Memory $\mathcal{M}$ is a growing lookup table of key-value pairs: key $\left(\mathbf{x}<em i="i">{i}\right)$ - value $\left(\mathbf{f b}</em>}\right)$, where $\mathbf{x<em i="i">{i}$ is a particular incorrect graph, and $\mathbf{f b}</em>)$.}$ is the corresponding feedback. This memory supports lookup (read) and write operations. Given a new query x, FBNET uses feedback fb from similar, prior queries in the memory to enrich $\mathbf{x}$. This feedback fb is retrieved using the lookup function $\Omega(\mathbf{x}, \mathcal{M</p>
<p>The corrector then combines $\mathbf{f b}$ with $\mathbf{x}$, and generates $\mathbf{y}$. The write operation is used whenever a user gives new feedback.</p>
<h3>3.2 Assumptions</h3>
<p>We make two assumptions on the characteristics of the feedback and the input.
A1. Base model B's output is repairable: B typically produces syntactically correct output graph but can have semantic errors that the user can recognize and describe using natural feedback. For example, the script in Figure 1 is repairable.
A2. Feedback is reusable: If two examples $i, j$ have similar errors $e_{i}$ and $e_{j}$ then the feedback $f b$ for one should apply to the other, i.e., $\left(e_{i} \sim\right.$ $\left.e_{j} \Leftrightarrow f b_{i} \sim f b_{j}\right)$</p>
<h3>3.3 Memory $\mathcal{M}$ and $\Omega$</h3>
<p>As mentioned, the feedback is stored in a memory of key ( $\mathbf{x}$ ), value ( fb ) pairs. $\Omega$ is a retrieval function that matches a query key $\left(\mathbf{x}<em i="i">{j}\right)$ to a similar $\mathbf{x}</em>$.}$ in memory implicitly on the similarity of the errors $e_{i}$ and $e_{j</p>
<h3>3.4 Corrector model G</h3>
<p>The graph corrector model $\mathbf{G}$ generates an improved output $\mathbf{y}$ given a noisy graph $\mathbf{x}$ and $\mathbf{f b}$. This is done in a two-step process, (i) learning to predict a graph edit operation $\mathbf{y}^{e}$ given $\mathbf{x}$ and $\mathbf{f b}$ (ii) using simple graph operations to apply $\mathbf{y}^{e}$ to $\mathbf{x}$ to produce $\mathbf{y}$. Our approach of generating an edit instead of directly generating the corrected graph is beneficial for two reasons. First, generating edits is simpler for the model than generating entire graphs. Second, it simplifies evaluation metrics as it is much simpler to compare two smaller generated edits. Note that we can deterministically fix a script given an edit. Thus, the two-step process helps us achieve the same end goal (corrected scripts from noisy scripts and feedback).</p>
<h3>3.5 Training and Inference</h3>
<p>As mentioned, the graph corrector $\mathbf{G}$ first generates an edit $\mathbf{y}^{e}$, which is applied to the incorrect graph $\mathbf{x}$ to generate the correct graph $\mathbf{y}$. We need a corpus of $(\mathbf{x}, \mathbf{f b}, \mathbf{y})$ to train this system. Specifically, we extract an edit from each such tuple, where edit $y^{e}$ is the difference between the output $\mathbf{y}$ and the input $\mathbf{x} . \mathbf{x}$ and $\mathbf{y}$ can be expressed in a string representation using a graph description language such as</p>
<p>DOT. We then train a language model to estimate $P_{\theta}\left(y^{e} \mid \mathbf{x}, \mathbf{f b}\right)$, which allows us to generate an edit for a given $(\mathbf{x}, \mathbf{f b})$ using greedy sampling, where $\theta$ denotes the parameters of the language model.</p>
<h2>4 Application: Script Generation</h2>
<h3>4.1 Task</h3>
<p>We instantiate our framework for the task of script generation. Formally, the script generation task (Sakaguchi et al., 2021) takes as input a scenario and generates a script $G(V, E)$, where $V$ is a set of essential events $\left{v_{1}, \ldots v_{i}, \ldots v_{|V|}\right}$ and $E$ is a set of temporal ordering constraints between events $\left{e_{i j}\right}$ which means that the events $v_{i}$ must precede the event $v_{j}\left(v_{i} \prec v_{j}\right)$. Partial ordering of events is possible, e.g., you can wear a left sock and a right sock in any temporal order. To solve this task, script generation models are required to generate events $(V)$ and predict the edges $(E)$ jointly. See Figure 3 for an example.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example of a script in Sakaguchi et al. (2021). In a script generation task, models take the goal as the input and generate a (possibly) partial-order graph, which consists of essential steps and their ordering.
$\operatorname{PROSCRIPT}_{g e n}$ (Sakaguchi et al., 2021) is a recently released model that, given a goal, generates $V$ and predicts the edge structure $E$ jointly. It is based on the T5-XXL model (11B parameters) and generates the script as a graph in DOT format. The authors report that the DOT format is always valid at inference time and that $V$ and the graph structure are generally of high quality. They characterize the graph edits required to correct a generated script (such as removing a node, adding a node, changing edge order, etc.). Mechanical Turk workers could repair most of the generated scripts within a few
edits (typically an edit distance of 5) - we further validate this in Appendix $\S 8.1$. This makes for an attractive use-case for interactive learning because the generated content from the model is repairable through user feedback.</p>
<h3>4.2 Feedback Data Collection</h3>
<p>To train the corrector $G$, as well as evaluate our approach, we collected a set of $(x, f b, y)$ tuples using crowdworkers, where $x$ is a possibly erroneous script generated by $\operatorname{PROSCRIPT}_{g e n}, f b$ is general feedback about the error (if any), and $y$ is the corrected script. In practice, crowdworkers specified the edits to $x$ to create $y$ (using simple graph operations we can generate $\mathbf{y}$ from $\mathbf{y}^{e}$ - see Table 7 for an example). We collected 1542 tuples of data, randomly splitting it into 843 train, 154 validation, and 545 test points. Examples of the resulting dataset are shown in Table 1.</p>
<h3>4.3 Training the Corrector Model</h3>
<p>We initialize $\theta$ with a checkpoint from the text-to-text pre-trained T5 transformer (Raffel et al., 2020) and fine-tune on our dataset. We use the default hyperparameters (including the Adafactor optimizer) in the T5 library. ${ }^{2}$ We fine-tune a T5-XXL model for the main results, fine-tuned for 5,000 steps (batch size 8), selecting the checkpoint with the highest validation score (usually the final step). To implement the memory $\mathcal{M}$, we use a BERTbased Sentence Transformer to encode $x$ (Reimers and Gurevych, 2019), and use cosine distance with a threshold of 0.9 to find a matching key $\mathbf{x}_{m}$. We leave the investigation of more complex retrieval functions (e.g., using attention mechanism to future work.)</p>
<h2>5 Experiments</h2>
<p>We empirically evaluate two questions:
RQ1. How well does FBNET interpret NL feedback? Specifically, we measure how well FBNET can translate general feedback fb from a user into the correct repair edit on an imperfect script $x$. The main focus of RQ1 is to test the performance of $\mathbf{G}$ in the pipeline (Fig. 2)
RQ2. How well can FBNET learn from prior mistakes? We make the same measurement, but using feedback fb recalled from similar,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Error type</th>
<th style="text-align: left;">Input script x</th>
<th style="text-align: left;">Feedback <br> fb</th>
<th style="text-align: left;">Expected <br> edit $\mathrm{y}^{e^{*}}$</th>
<th style="text-align: left;">Generated <br> edit $\mathrm{y}^{e}$</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">score</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">missing step</td>
<td style="text-align: left;">1. get out of car <br> 2. stop in front of car <br> 3. turn body toward back of car <br> 4. walk to back of car <br> 5. take blanket out of car <br> 6. walk to desired location <br> 7. throw blanket down</td>
<td style="text-align: left;">a person <br> needs <br> to open <br> the door <br> before <br> they take <br> an object <br> out</td>
<td style="text-align: left;">insert <br> node <br> 'open the <br> back door <br> of the car' <br> before <br> 'take <br> blanket <br> out of car'</td>
<td style="text-align: left;">insert <br> node <br> 'open car <br> door' be- <br> fore 'take <br> blanket <br> out of car'</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">missing step</td>
<td style="text-align: left;">1. buy a video game <br> 2. talk to the cashier <br> 3. make the transaction <br> 4. get the receipt <br> 5. load video game into the car <br> 6. get into the car <br> 7. take xbox home</td>
<td style="text-align: left;">after a <br> person <br> makes a <br> transaction, they <br> then head <br> to their <br> car</td>
<td style="text-align: left;">insert <br> node <br> 'walk <br> to the <br> car' after <br> 'get the <br> receipt'</td>
<td style="text-align: left;">insert <br> node 'get <br> into the <br> car' after <br> 'make the <br> transac- <br> tion'</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">wrong step</td>
<td style="text-align: left;">1. make a bunch of cards <br> 2. grab a pen <br> 3. grab some paper <br> 4. pick up a pen <br> 5. place the paper on the table <br> 6. pick up the pen <br> 7. write names on the cards</td>
<td style="text-align: left;">good <br> plans <br> shouldn't <br> include <br> redundant <br> steps</td>
<td style="text-align: left;">remove <br> node <br> 'pick up <br> the pen'</td>
<td style="text-align: left;">remove <br> node <br> 'pick up <br> the pen'</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">wrong order</td>
<td style="text-align: left;">1. leave home and get in car <br> 2. remem. destination address <br> 3. look around for the car <br> 4. walk towards the car <br> 5. open the car door <br> 6. sit down in the car <br> 7. put on the seatbelt</td>
<td style="text-align: left;">you <br> wouldn't <br> look for <br> some- <br> thing <br> you're <br> already <br> with</td>
<td style="text-align: left;">reorder <br> edge <br> between <br> ' ${$ leave <br> home and <br> get in <br> car, look <br> around <br> for the car <br> $j$ '</td>
<td style="text-align: left;">remove <br> node <br> 'look <br> around <br> for the <br> car'</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
<p>Table 1: Some examples of the data points and model predictions. $\mathrm{y}^{e}$ takes the form: <EDIT TYPE> over [<ARG>] at <LOCATION> The dataset contains partial order points as well, but they are omitted here for simplicity.
previous examples. The main focus of RQ2 is to test the performance of $\mathcal{M}$ and $\Omega$.</p>
<p>Metrics To compare the gold edit $\mathrm{y}^{e^{*}}$ and the generated edit $\mathrm{y}^{e}$, we use standard metrics used to evaluate generated text. We report the following metrics:</p>
<ul>
<li>Exact match: EM gives a score of 1 if $\mathrm{y}^{e^{*}}$ is equal to $\mathrm{y}^{e}$ and 0 otherwise.</li>
<li>Generation metrics: We report standard generation metrics BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to account for similar but not exact matches. We use the implementation released in the metrics package of the GEM-benchmark (Gehrmann et al., 2021). ${ }^{3}$</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We report these metrics over the entire edit: EM, BLEU, ROUGE. The components of $\mathrm{y}^{e}$ broadly follow a template: <EDIT TYPE> over [ARG] at <LOCATION> (see Table 1). This allows comparison of the location or edit type in $\mathrm{y}^{e^{*}}$ and $\mathrm{y}^{e}$ : EM $t_{o c}$, BLEU $t_{o c}$, ROUGE $t_{o c}$ and EM $t_{y p e}$, BLEU type, ROUGE type</p>
<p>Baseline As baseline, we train a model that does not use any feedback (we call this, NO-FB) and is trained only with input $=$ erroneous script and output $=$ edit. The language model used in this baseline and FBNET is the same (T5-XXL), allowing a meaningful comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">$E M$</th>
<th style="text-align: right;">$E M_{\text {loc }}$</th>
<th style="text-align: right;">$E M_{\text {type }}$</th>
<th style="text-align: right;">BLEU</th>
<th style="text-align: right;">ROUGE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NO-FB</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">9.7</td>
<td style="text-align: right;">30.4</td>
<td style="text-align: right;">21.7</td>
<td style="text-align: right;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">FBNET $_{o}$</td>
<td style="text-align: right;">$\mathbf{3 8 . 6}$</td>
<td style="text-align: right;">$\mathbf{4 5 . 8}$</td>
<td style="text-align: right;">$\mathbf{6 9 . 3}$</td>
<td style="text-align: right;">$\mathbf{5 4 . 2}$</td>
<td style="text-align: right;">$\mathbf{7 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Interpreting NL Feedback: Correctness of Predicted Edits .... Given an erroneous script $x$, and general feedback $f b$ from the user, FBNet perfectly predicts the specific repair edits $38 \%$ of the time ( $E M$, exact match) - an order of magnitude better than a baseline NO-FB predicting the repair from $x$ alone. $E M_{l o c}$ and $E M_{\text {type }}$ compare just parts of the edit sequences (locations/types of the required edits, respectively), while BLEU and ROUGE are softer matching metrics.</p>
<h3>5.1 RQ1: How well does FBNET interpret NL feedback?</h3>
<p>To measure how well the graph corrector $\mathbf{G}$ learns to interpret NL feedback, we provide oracle feedback to FBNET, and we call this FBNET ${ }<em o="o">{o}$. Table 2 shows that $\mathrm{FBNET}</em>$ learns to react to the feedback, as indicated by a sharp increase in both the exact match scores and automated metrics. Further, we note that the model is good at identifying the error type that the feedback indicates. Still, it is difficult for the model to localize the error in the graph, probably because the location is not explicitly mentioned in the feedback, and the model must infer it.</p>
<p>How consistently does FBNET interpret similar feedback? In $\sim 15 \%$ of the data points, multiple fb can lead to the same $(\mathbf{x}, \mathbf{y})$ pair. FBNET is expected to behave consistently for such re-phrasings of fb. The model consistently produces exactly the same $\mathbf{y}$ for fb re-phrasings $\sim 60 \%$ of the time. Furthermore, we observe majority agreement as the number of fb re-phrasings for a $(\mathbf{x}, \mathbf{y})$ pair increases. In our analysis, a large proportion of the inconsistent edits occur because different fb phrasings prompt the model to generate slightly different, but semantically similar edits: see Table 3 for an example.</p>
<h2>How well can FBNET handle wrong feedback?</h2>
<p>While the ability to react to feedback is a desired trait for FBNET, we also want to ensure that the performance of FBNET is proportional to the quality of feedback. This will ensure that FBNET can act faithfully in settings where the feedback might be potentially misleading. We investigate this question by identifying lexically similar scripts but irrelevant feedback from the training set for each test</p>
<table>
<thead>
<tr>
<th style="text-align: left;">feedback</th>
<th style="text-align: left;">predicted edit</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The feedback is if a <br> person is going to open <br> a book, they need to <br> choose one first</td>
<td style="text-align: left;">insert node 'choose a <br> book to read' before <br> 'open the book'</td>
</tr>
<tr>
<td style="text-align: left;">The feedback is you <br> can't open something <br> you're not holding</td>
<td style="text-align: left;">insert node 'get the <br> book out of the bag' be- <br> fore 'open the book'</td>
</tr>
</tbody>
</table>
<p>Table 3: Multiple feedbacks for the same ( $\mathbf{x}, \mathbf{y}$ ). Here, $\mathbf{x}$ is: You are given a plan to read to child. decide which books to read, open the book, read the book to the child, turn the pages $\ldots . \mathbf{y}^{e}$ is insert node 'pick a book off the shelf' before 'open the book'
example. Note that our setup easily allows us to test this hypothesis since the train/test/val splits were carefully designed to ensure no overlap between the examples. Thus the feedback from one example will typically not apply to another example. We find that with irrelevant feedback, the performance of FBNET drops to 3\%. This shows that FBNET is sensitive to the quality of feedback, and no feedback is better than misleading and irrelevant feedback.</p>
<p>How well does FBNET perform across error types? FBNET $_{o}$ gets the highest performance ( $E M 63.0 \%$ ) on wrong-step error type where fb typically contains negative words that signal the error type, and the model learns to localize the error node. One of the most challenging error types is partial order removal or addition ( $E M 10.5 \%$ ). This can be attributed to the challenging localization involving multiple nodes that participate in a partial order. The lowest-performing is the missing step error type ( $E M 2.73$ ). The reason for this low $E M$ score is that the edit must generate the missing node, and $E M$ undercounts the correctness of the generated text. Other metrics such as ROUGE are much higher validating that the model performs well on this error type. Section $\S 9$ Table 8 breaks down the performance of FBNET by error type.</p>
<h3>5.1.1 Error analysis</h3>
<p>We randomly sampled 50 instances from the test set where the model generates an incorrect edit (i.e., $E M=0$ ). Our goal is to understand the typical errors made by the model and use the analysis to calibrate the findings in Table 2.</p>
<ul>
<li>Lexical variation (36\%) Exact match underestimates the performance of our model (as the</li>
</ul>
<p>task involves generation). We find that more than $35 \%$ of the predicted edits are semantically similar (typically lexical variation) to the reference gold edit. Some examples include: insert node picking a book... vs, insert node choosing a book to read. Another kind of example is the model suggesting swapping the order of edges $A$ and $B$ while the reference edit swaps edges $B$ and $A$ - but both of these are equivalent.</p>
<ul>
<li>Challenging feedback (24\%) This type of error occurs when the model fails to interpret a feedback because it is difficult to interpret e.g., the feedback is expressed abstractly. For example, for the goal "go to locker room," the generated script repeats the step "walk to the locker room.". However, the feedback is 'you can't go where you already are', and FBNET generates the edit "reorder edge between '〈 walk towards the locker room, walk to the locker room }'", failing to interpret the feedback.</li>
<li>Error not localized (20\%) In about 20\% of the failures, FBNET fails to localize the error given the feedback. For example, consider the erroneous input script about the goal buy an xbox: 1. go to the store 2. talk to the cashier 3. make the transaction 4. get the receipt 5. load the video game into the car 6. get into the car 7. take xbox home The feedback is after a person makes a transaction, they then head to their car. The expected edit is: insert node 'walk to the car' after 'get the receipt', but the predicted edit insert node 'get into the car' after 'make the transaction' does not correctly identify the erroneous node. The feedback points to making a transaction, but it also involves getting the receipt.</li>
<li>Alternative answers (16\%) We also encounter cases where there are multiple ways to correct a script. For example, an edit can be expressed as insert node ' $X$ ' before 'step 4' or insert node ' $X$ ' after 'step 3'. This comprises $\sim 16 \%$ of the errors.</li>
</ul>
<p>In $\sim 32 \%$ cases, the model generates a correct edit that differs from the gold. Extrapolating this performance under-counting to the entire test set, the accuracy of FBNET in Table 2 would increase to $\sim 70 \%(+32 \%)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$E M$</th>
<th style="text-align: center;">$E M_{\text {loc }}$</th>
<th style="text-align: center;">$E M_{\text {type }}$</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NO-FB</td>
<td style="text-align: center;">6.94</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: center;">FBNET</td>
<td style="text-align: center;">16.72</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: center;">FBNET $_{\text {o }}$</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">65.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Learning from prior mistakes: On the reuse dataset, given an erroneous script $x$, and feedback $f b$ recalled from similar, prior examples, FBNet perfectly predicts the specific repair edits $16.7 \%$ of the time (or $20.9 \%$ the edit location and $56.9 \%$ the edit type), a promising start to learning from prior mistakes.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance on unseen examples (number of correct data points) improves as memory size grows. NO-FB baseline performance remains static. Note that accuracy is evaluated using exact match, and thus is a lower bound on the actual equivalence as exact match might miss rephrasings.</p>
<h3>5.2 RQ2: How well can FBNET learn from prior mistakes?</h3>
<p>Section $\S 5.1$ shows that the corrector $\mathbf{G}$ can utilize user-supplied feedback to fix an incorrect structure. FBNET combines $\mathbf{G}$ with a memory $\mathcal{M}$ of feedback, allowing us to leverage past feedback on new examples. This section presents a setup where feedback on previously seen inputs is used to fix new, unseen examples.</p>
<p>To investigate this setting, we create a new test set, called the interaction-reuse set or ISET. To create it, we randomly sample 72 test points (referred as interaction-reuse set-genesis or ISETSOURCE) and perturb them linguistically to generate interaction-reuse set(also referred as ISET). The perturbations are performed on the salient entities in the script, including (i) linguistic perturbation on $\sim 20 \%$ samples (e.g., box $\rightarrow$ carton, package) and</p>
<p>Algorithm 1: FBNET inference on a stream of inputs with growing memory Given: FBNET, $\mathcal{M}, \Omega$
Given: Set {ISET $\cup$ ISET-SOURCE $}$ of $N$ queries.
for $i \leftarrow 1,2, \ldots, N$ do
/<em> Check memory for feedback </em>/
$\mathbf{f b}<em i="i">{i}=\Omega\left(\mathbf{x}</em>\right)$;
/}, \mathcal{M<em> Get corrected structure from FBNET. $\mathbf{f b}_{i}$ can be empty. </em>/
$\mathbf{y}<em i="i">{i}=$ FBNET $\left(\mathbf{x}</em>}, \mathbf{f b<em i="i">{i}\right)$;
/<em> Get user feedback </em>/
$\mathbf{f b}</em>}=$ User feedback on $\mathbf{y<em i="i">{i}$;
/<em> Grow memory with new feedback </em>/
Write $\mathbf{f b}</em>$
end
(ii) the relatively harder analogical perturbation on the remaining $\sim 80 \%$ samples (e.g., bus $\rightarrow$ train, and how to lift blinds $\rightarrow$ how to open oven door because the event structure is analogical). The $\mathbf{y}^{e}$ to the original script also applies to the substituted script. We ensured that the perturbations did not introduce additional errors in the substituted script. This ensures that the interaction-reuse setnow contains similar examples to the original test set, a condition that our original splits do not satisfy. There are a total of 72 data points in interaction-reuse set.}$ to $\mathcal{M</p>
<h2>Continually learning using a memory of errors</h2>
<p>Examples in interaction-reuse set are randomly mixed with the original test set. This combined test set of queries $Q$ is then evaluated using our setup as shown in Algorithm 1 Intuitively, interactionreuse set allows us to simulate a setting where the system has been deployed in the wild, and endusers can query. Algorithm 1 runs the memorybased inference described in Section $\S 3$ (Figure 2). As the system is run through the stream of queries, we expect that i) the overall performance of the system will be better than no feedback, as some of the examples in the interaction set will provide meaningful feedback, and ii) the running performance of the system will improve with growing memory: the probability of relevant feedback being present for an unseen example increases with time, thus boosting the performance.</p>
<p>Our experiments show that FBNET meets both these expectations. First, Table 4 shows that retrieved feedback improves over no feedback by</p>
<p>10 points (exact match) and similarly in terms of BLEU and ROUGE scores, respectively. Further, Figure 4 shows a graph confirming that FBNET can improve continuously as memory grows.</p>
<h2>6 Scope</h2>
<p>In principle, we could apply FBNET to any task that satisfies the assumptions (§3.2). However, our approach has some limitations in practice, several of which merit further detailed follow-up work.</p>
<ul>
<li>On Assumption A1: We assume that the output of $\mathbf{B}$ is repairable. Such an assumption is only possible for models that generate mostly correct outputs and have errors that are easy to highlight for humans. In practice, this implies that our approach will most efficiently work in conjunction with modern language models (Bommasani et al., 2021) that are shown to be syntactically correct in form, but can produce output that lacks commonsense (Bender and Koller, 2020), making their output repairable.</li>
<li>On Assumption A2: Having reusable, general feedback is costly and requires careful instructions to collect from general users and crowdworkers (e.g., we asked the crowdworkers how they would explain the model error to a five-year-old). As the domain of the task becomes more specialized, such as database query generation (Elgohary et al., 2021) or code correction (Yasunaga and Liang, 2020), collecting data to train $\mathbf{G}$ becomes difficult. Systems that produce structured explanations are better suited to our model (see Wiegreffe and Marasović (2021) for an overview), rather than specialized domains that require expert users to provide feedback (e.g., in database query generation).</li>
<li>Consistent memory: We show in Section §5.1 that FBNET is sensitive to the appropriateness of the feedback. However, adversarial or incorrect feedback could pollute the memory and possibly make it inconsistent. There has been some recent work to ensure consistency of beliefs of a model (Kassner et al., 2021), and more effort is required in this direction to apply to more complex settings like ours.</li>
<li>Using multiple feedbacks: $\Omega$ can be enhanced with more complex attention mechanisms that aggregate from multiple relevant memory entries and possibly generalize them. We conducted an initial experiment using attention and</li>
</ul>
<p>found that we would need a larger dataset to train $\Omega$ effectively.</p>
<p>Advancements in these directions would further increase the applicability of FBNET. Still, there are several applications (Wiegreffe and Marasović, 2021) where our approach would currently apply in principle, or is easy to set up.</p>
<h2>7 Summary</h2>
<p>Our goal is to create a system that can continuously improve the structured output of a model. Our approach is to train an error correction model that uses natural language (NL) feedback to correct errors in that output. We have presented the first step towards this goal, showing that an error correction module can learn to interpret NL feedback successfully, resulting in $40 \%$ fewer errors in script generation. We have also described ongoing work on the next step, namely adding a memory layer where human feedback is stored and later retrieved efficiently. Together, these offer a possible path to systems that can continuously improve their output over time, with progressively less feedback and without retraining.</p>
<h2>Acknowledgments</h2>
<p>This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. We would like to thank Google for providing the TPU machines for conducting experiments.</p>
<h2>References</h2>
<p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online. Association for Computational Linguistics.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportuni-
ties and risks of foundation models. ArXiv preprint, abs/2108.07258.</p>
<p>Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 15471555. PMLR.</p>
<p>Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599-5610, Online. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Z. Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. ArXiv, abs/2002.08909.
K. Hammond. 1986. Chef: A model of case-based planning. In AAAI.</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. 2021. BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8849-8861, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Nikhil Mehta and Dan Goldwasser. 2019. Improving natural language interaction with robots using advice. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1962-1967, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, M. Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. $J$. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.
C. Riesbeck. 1981. Failure-driven reminding for incremental learning. In IJCAI.
B. Ross. 1984. Remindings and their effects in learning a cognitive skill. Cognitive Psychology, 16:371-416.</p>
<p>Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. 2021. proscript: Partially ordered scripts generation via pre-trained language models.
R. Schank and David B. Leake. 1989. Creativity and learning in a case-based explainer. Artif. Intell., 40:353-385.</p>
<p>Burr Settles. 2012. Active learning. In Active Learning.
G. Sussman. 1973. A computational model of skill acquisition. Technical Report AITR-297, MIT.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Alon Talmor, Oyvind Tafjord, P. Clark, Y. Goldberg, and Jonathan Berant. 2020. Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Sida I. Wang, Percy Liang, and Christopher D. Manning. 2016. Learning language games through interaction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2368-2378, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Sarah Wiegreffe and Ana Marasović. 2021. Teach me to explain: A review of datasets for explainable nlp. ArXiv, abs/2102.12060.</p>
<p>Michihiro Yasunaga and Percy Liang. 2020. Graphbased, self-supervised program repair from diagnostic feedback. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10799-10808. PMLR.</p>
<p>Michihiro Yasunaga and Percy Liang. 2021. Break-it-fix-it: Unsupervised learning for program repair. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 11941-11952. PMLR.</p>
<p>Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.</p>
<p>Yi Zhang, Sujay Kumar Jauhar, Julia Kiseleva, Ryen White, and Dan Roth. 2021. Learning to decompose and organize complex tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2726-2735, Online. Association for Computational Linguistics.</p>
<h2>8 Appendix</h2>
<h3>8.1 Initial study on the errors of B (PROSCRIPT)</h3>
<p>On PROSCRIPT's test set, we performed inference using the released checkpoint (both GPT-2 and T5XXL based model). We then randomly sampled 30 generated graphs and manually wrote feedback for them. Similar to Sakaguchi et al. (2021), we found that the model makes repairable mistakes (leading to assumption A1 being satisfied). Further, we found there instances where a general principle feedback applies across more than one instances (e.g., you have to be near something to use it). (see Table 5).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">What was the error</th>
<th style="text-align: left;">General principle feed- <br> back</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Script was missing the step <br> of not turning off the alarm <br> after waking up</td>
<td style="text-align: left;">People don't leave their <br> alarms ringing all day.</td>
</tr>
<tr>
<td style="text-align: left;">Script mentioned coming to <br> the doorway and passing <br> through it</td>
<td style="text-align: left;">One cannot walk through <br> the doorway without open- <br> ing the door first.</td>
</tr>
<tr>
<td style="text-align: left;">Script tells that getting in <br> car and drive in zoo can be <br> done in any order</td>
<td style="text-align: left;">People must get into a ve- <br> hicle, before driving to any <br> place.</td>
</tr>
<tr>
<td style="text-align: left;">Script is looking for a but- <br> terfly after placing it</td>
<td style="text-align: left;">You don't need to look for a <br> butterfly if it's already in a <br> container.</td>
</tr>
</tbody>
</table>
<p>Table 5: Sample error and the corresponding general principle feedback that could, in principle, repair the model output.</p>
<p>On an average, there were about two mistakes present in the graphs. Often, the error was that the script was using an entity before having it (e.g., write on the paper comes before the node find the paper or reach for the paper). Thus, there seems to be a possibility of applying similar feedback to more than one example. We also found some cases where the script might have to be changed to adapt to special cases. For example, for a script visit Disneyland, an event obtain a visa might be required for some users. We believe the original ProScript dataset aims to generate widely applicable scripts and grounded in commonsense; rather than cover all possible outcomes.</p>
<p>On the surface, the generated scripts were of good quality. However, a closer look at the mistakes revealed that most of them could be attributed to the model lacking basic commonsense. For example, Figure 1 shows a typical mistake the model makes. This underscores the gap between the syntax and semantic correctness of machine-generated
output in the context of automatic script generation. This observation is in-line with other NLP tasks (Bender and Koller, 2020) that distinguish the success of recent models on the correctness of form rather than the far-from-over goal of understanding of meaning.</p>
<h3>8.2 Data collection</h3>
<p>An average user could point out mistakes in the generated scripts, as a majority of the errors in generated scripts are caused by a lack of basic commonsense (§8.1). Consequently, we designed a Mechanical Turk task to provide feedback on mistakes. A broad overview of the annotation process is shown in Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A broad overview of the annotation process. For actual annotation task (including the M-turk task template), see our code repository.</p>
<p>Annotation Now we discuss our crowdsourcing setup to collect the data. To maximize the opportunity to get more feedbacks for a predicted script, we filtered a subset of the test set in ProScript where the human evaluated graph edit distance was likely to be high (i.e., there were likely to be more errors). The ProScript authors released the graph edit value for the set of test set samples they evaluated. We performed inference using their released PROSCRIPT ${ }_{\text {gen }}$ model on those data points with high graph edit distance value ( $\geq 8$ ). With this we collected about 400 (predicted graph, expected gold graph) tuples. The ProScript paper describes that their expected gold graph is also imperfect and might contain about $20 \%$ noise. Nevertheless, having the gold reference graph guides and constrains an annotator about the common script for a scenario rather than the wide open space of solving the task using multiple potentially correct scripts. (e.g., one could go to a zoo without driving the car by hiring a taxi and then they won't need to drive or park the car). As mentioned in $\S 8.1$ our annotation process must focus on scripts that are widely applicable and grounded in commonsense.</p>
<p>The annotators are shown the model-generated</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The mechanical turk page for annotation. We show the generated and the expected ProScript gold reference. The annotator must answer which script is worse and why. They must point out an egregious mistake (and not any trivial errors that have minor grammatical errors), and annotate: the error type (missing step, wrong step, wrong order, wrong partial order), localize the error by providing the node or edge id, and give feedback why it is wrong, and finally to gather the general principle behind the feedback they are asked to explain the feedback to a five-year-old.</p>
<p>and expected gold (reference) scripts, and are required to answer which script is worse and why. It is possible that the gold script is marked as worse. However, we later post-process and remove such cases, as our focus is to get errors on the generated scripts and not the manually created scripts. The annotators must point out an obvious mistake (e.g., an event or an edge that does not follow commonsense). They were asked to ignore grammatical and fluency errors, and focus on critical errors of four types:</p>
<ul>
<li>Wrong ordering: the order in the sequence of steps is not correct (e.g., wearing shoes is described before wearing socks).</li>
<li>Flexible ordering: some steps can be done in a flexible order (e.g., you can wear left sock or right sock first). A good script captures such flexibility.</li>
<li>Missing critical steps: a bad script might have missed critical steps (e.g., the script can say: "wait for a plane" followed by "get off the plane" - here an obvious step "get on the plane" is missing). There is no strict definition for a critical step, so the annotators were instructed to use their commonsense judgment.</li>
<li>Wrong step: a bad script might have irrelevant and wrong steps (e.g., the script describing "go to a party" might describe an irrelevant step such as read a book, open a book, etc.).</li>
</ul>
<p>For every data point, the annotators were asked to answer the following:</p>
<ul>
<li>Explicit feedback type-1: the error type (missing step, wrong step, wrong order, wrong partial order)</li>
<li>Explicit feedback type-2: localize the error by providing the erroneous node or edge id</li>
<li>Implicit feedback type-1: give feedback in a few words, explaining the error</li>
<li>Implicit feedback type-2: An explanation of the error that would potentially make sense to a five-year-old. Such an explanation of the feedback helped gather the general principle that is violated, and is targeted in the feedback.
Fig. 6 shows a sample of our Mechanical Turk task. Annotators were required to list only one critical error that they believe was most important. Each data point is annotated by three annotators, adding some diversity in the errors. The annotators were paid $\$ 15$ an hour. The annotators were English
speaking crowdworkers on Mechanical Turk from USA. The average time for completion of one script was 2 minutes.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">fb type</th>
<th style="text-align: center;">count</th>
<th style="text-align: center;">example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">explicit fb <br> type-1</td>
<td style="text-align: center;">1,553</td>
<td style="text-align: center;">Remove node 'put the shirt on'</td>
</tr>
<tr>
<td style="text-align: center;">explicit fb <br> type-2</td>
<td style="text-align: center;">1,553</td>
<td style="text-align: center;">The following step is not right: put the shirt on</td>
</tr>
<tr>
<td style="text-align: center;">implicit fb <br> type-1</td>
<td style="text-align: center;">1,553</td>
<td style="text-align: center;">It tells you to iron your shirt while it's still on your body.</td>
</tr>
<tr>
<td style="text-align: center;">implicit fb <br> type-2</td>
<td style="text-align: center;">1,553</td>
<td style="text-align: center;">If you hold a hot iron against the clothes you're currently wearing, you'll get terrible burns.</td>
</tr>
<tr>
<td style="text-align: center;">total</td>
<td style="text-align: center;">6,212</td>
<td style="text-align: center;">https:// <br> anonymous.4open. <br> science/r/ <br> interscript/ <br> data.json</td>
</tr>
</tbody>
</table>
<p>Table 6: Dataset statistics. In this paper, we use the hardest feedback (implicit-feedback-type-2). This example is from the input script: input script for the following table (goal: press the wrinkles out) $=1$. put the shirt on, 2. find place to press, 3. grab iron from drawer, 4. place iron on shirt, 5. wait for iron to heat up, 6. use iron to smooth out wrinkles, 7. press the wrinkles out</p>
<p>We measured the agreement on labels (which graph is worse), and on explicit feedback type-1 and type-2. It was difficult to measure agreement on implicit feedback because it is not easy to perform binary comparison on the generated text without accounting for linguistic variations. On the labels, the Fleiss Kappa agreement was 0.90 (almost perfect agreement) and on explicit feedback the agreement was 0.75 Fleiss Kappa (substantial agreement). This also shows that there is some diversity in what the users perceive as a serious mistake in $y_{b a d}$.</p>
<p>Eventually, we compiled these annotations into a dataset of 1,553 4-tuples where each entry consists of (explicit feedback type-1, explicit feedback type2, implicit feedback type-1, implicit feedback type2).</p>
<h3>8.3 Model Output Examples</h3>
<p>Table 7 provides some sample model outputs.</p>
<h2>9 Performance of FBNET by error type</h2>
<p>Table 8 breaks down the performance of FBNET by error type.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">x</th>
<th style="text-align: center;">fb</th>
<th style="text-align: center;">$\mathbf{y}^{\mathrm{e}^{*}}$</th>
<th style="text-align: center;">y</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. ... <br> 2. set alarm for early morning <br> 3. get out of bed <br> 4. prepare for yoga <br> 5. go to the bathroom <br> 6. do yoga <br> 7. do yoga in the morning</td>
<td style="text-align: center;">People don't leave their alarms ringing all day.</td>
<td style="text-align: center;">insert node 'wake up and turn off alarm' before 'get out of bed'</td>
<td style="text-align: center;">1. ... <br> 2. set alarm for early morning <br> 3. wake up and turn off alarm <br> 4. get out of bed <br> 5. ... <br> 6. ... <br> 7. ...</td>
</tr>
<tr>
<td style="text-align: center;">1. ... <br> 3. put on shoes ... <br> 5. open the door <br> 6. drive to the train station <br> 7. get into the car <br> 8. reach the train station</td>
<td style="text-align: center;">You must get into a vehicle, before driving to any place.</td>
<td style="text-align: center;">reorder edge between ' ${$ drive to the train station, get into the car $}$ '</td>
<td style="text-align: center;">1. ... <br> 3. ... <br> 5. open the door <br> 6. get into the car <br> 7. drive to the train station <br> 8. ...</td>
</tr>
<tr>
<td style="text-align: center;">1. ... <br> 3. pick up the butterfly <br> 4. put the butterfly in container <br> 5. look for a butterfly ... <br> 6. Take the butterfly home ...</td>
<td style="text-align: center;">You don't need to look for a butterfly if it's already in a container.</td>
<td style="text-align: center;">remove node 'look for a butterfly'</td>
<td style="text-align: center;">1. ... <br> 3. pick up the butterfly <br> 4. put the butterfly in container <br> 5. Take the butterfly home <br> 6. ...</td>
</tr>
</tbody>
</table>
<p>Table 7: Task: Applying the graph edit to the bad script.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Edit type</th>
<th style="text-align: center;">EM\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: left;">Add partial order exactmatch</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: left;">Add partial order type</td>
<td style="text-align: center;">44.7</td>
</tr>
<tr>
<td style="text-align: left;">Missing step exactmatch</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: left;">Missing step type</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: left;">Remove partial order exactmatch</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Remove partial order type</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Wrong ordering exactmatch</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: left;">Wrong ordering type</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: left;">Wrong step exactmatch</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: left;">Wrong step type</td>
<td style="text-align: center;">78.6</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance of FBNET by error type</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/GEM-benchmark/ GEM-metrics/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
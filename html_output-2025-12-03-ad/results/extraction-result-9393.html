<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9393 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9393</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9393</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-270045144</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15077v4.pdf" target="_blank">Eliciting Informative Text Evaluations with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media. We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9393",
    "paper_id": "paper-270045144",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00955975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Eliciting Informative Text Evaluations with Large Language Models
2 Sep 2024</p>
<p>Yuxuan Lu 
Peking University 
China Shengwei Xu 
Grant Schoenebeck schoeneb@umich.edu </p>
<p>University of Michigan
USA</p>
<p>YICHI ZHANG â€¡
University of Michigan
USA</p>
<p>YUQING KONG â€ 
Peking University
China</p>
<p>University of Michigan
USA</p>
<p>Yuxuan Lu
Peking University
BeijingChina</p>
<p>Shengwei Xu
University of Michigan
Ann ArborUSA</p>
<p>Yichi Zhang
University of Michigan
Ann ArborUSA</p>
<p>Peking University
Yuqing Kong, BeijingChina</p>
<p>Grant Schoenebeck
University of Michigan
Ann ArborUSA</p>
<p>The International Conference on Learning</p>
<p>Eliciting Informative Text Evaluations with Large Language Models
2 Sep 2024206DFF5F4C3BD33BAFA16183D4DE9BEAarXiv:2405.15077v4[cs.CL]
Peer prediction mechanisms motivate high-quality feedback with provable guarantees.However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers.We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models.This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media.We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM).These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report.Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium.Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset.We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -human-written reviews, GPT-4-generated reviews, and GPT-3.5-generatedreviews in terms of expected scores.Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.* Both authors contributed equally to the paper and are listed in alphabetical order.</p>
<p>INTRODUCTION</p>
<p>Consider the following review for an academic paper: "I didn't get much out of reading this paper.Their methods do not seem very rigorous.I don't think the conclusions are supported very well." The above review is not very informative: its initial critique is too general, and the issues with the methods and conclusions should have been explained more thoroughly.If the peer review process only gathers reviews of this quality, it will struggle to make useful and fair publication decisions.The problem has been exacerbated by large language models (LLMs), which greatly reduce the cost of generating reviews that closely mimic human-written reviews but often lack substantial insight [Liang et al., 2023].</p>
<p>Here are two reviews of a randomly selected paper submitted to the ICLR 1 2020.An interesting exercise is to determine which was generated by an LLM and which was written by a human.</p>
<p>. . .I lean towards rejecting this paper however, because I am not convinced of the results' significance.We already know how to learn symmetric functions (see Exercise 3.26 in Mohri et al., 2018).The authors' results show that we can inject this knowledge into a neural network at initialization, and then run SGD without making things too much worse.I do not see how these ideas might apply to more substantial learning problems where our prior knowledge is less precise. . . . . . .The paper makes a valuable theoretical contribution to the understanding of neural network initialization, particularly in the context of symmetric functions.The empirical validation is a strong point, although the experiments could be expanded to provide a more comprehensive evaluation.The paper's focus on a single hidden layer network is both a strength, in terms of theoretical tractability, and a weakness, in terms of practical relevance.To move forward, the authors should consider extending their analysis to more complex architectures and providing a broader empirical evaluation. . . .Fig. 1.An example from our study: Two reviews of a submission at ICLR2020, the left one by a human reviewer, and the right one by  Due to LLMs, it is no longer possible to filter out low-quality reviews by their length, lack of any relation to the paper at hand, or poor grammatical constructions.In the above example, we can observe that the AI-generated review looks informative and effectively summarizes the paper.However, it lacks the depth and insight of the expert human review.</p>
<p>The need to obtain high-quality subjective human evaluation extends beyond academia to other domains, including business, the arts, and more.For example, if customer feedback on Amazon and Yelp is inundated with shallow reviews or those generated by LLMs, consumers will struggle with making well-informed decisions [Resnick et al., 2000, Tadelis, 2016].The problem of incentivizing high-quality reviews is more important and daunting than ever.</p>
<p>One approach is to provide well-designed incentives for the reviewer that, in particular, reward high-quality reviews more than low-quality reviews [Srinivasan and Morgenstern, 2021].However, because reviews are subjective, their correctness cannot be directly verified.This inherent challenge complicates the evaluation of review quality.</p>
<p>One straightforward idea is to ask other people to judge the quality of the reviews.But then we face a new challenge: how to motivate these new judges?Moreover, an automated approach that does not introduce additional participants and procedures is preferable.</p>
<p>Prior work has proposed the peer prediction mechanism, a powerful tool to elicit subjective information [Miller et al., 2005].The high-level idea is to determine the reward of a person according to the "correlation" between her report and a peer's report.The underlying intuition is that better, more insightful reports will naturally align more closely with one another.In their setting, when a person puts in the effort to understand a task, she gains a private signal such as "good" or "bad".She can then choose whether to report this signal honestly.Miller et al. [2005] prove that in their setting, truth-telling is an equilibrium-if a participant believes other people will invest effort and tell the truth, she should also do this.</p>
<p>However, implementing the above mechanism requires knowledge of the prior: the joint distribution of the private signals.In the original peer prediction mechanism [Miller et al., 2005], agents are asked to report their private signals such as "good" or "bad".The mechanism then predicts a peer's report, such as 70% "good", by getting a posterior based on an agent's report and the prior.The agent is then rewarded for the accuracy of this posterior prediction.Assuming the mechanism has perfect knowledge of the prior, this incentivizes truth-telling because only an honest report can lead to the optimal posterior.The mechanism's required knowledge of the prior has been seen as a major impediment to realworld implementation of peer-prediction mechanisms.Significant advances, that follow two main approaches, have enabled overcoming this limitation in several settings.Both these approaches circumvented the requirement of knowing the prior by learning, not the prior itself, but a proxy, usually the relationship between the agent reports, from agent reports themselves.</p>
<p>The first approach, often called the multitask setting, involves assigning agents to multiple a priori similar tasks.This allows learning the structure of agent reports and enables measuring the amount of information in common between agent responses [Dasgupta and Ghosh, 2013, Kong and Schoenebeck, 2019, Liu et al., 2023, Shnayder et al., 2016, Zhang and Schoenebeck, 2023a].In certain settings such as multiple choice questions, even a small number of tasks may suffice [Burrell and Schoenebeck, 2021, Kong, 2024, Schoenebeck and Yu, 2020].The second approach called the signalprediction framework, pioneered by Prelec [2004], involves eliciting second-order predictions, that is asking how they believe other agents will respond, for example, "I think 70% of my peers will answer 'good' " [Chen et al., 2021, Radanovic and Faltings, 2014, Schoenebeck and Yu, 2023b, Witkowski and Parkes, 2012b].</p>
<p>However, because both these approaches rely on learning from agent responses or predictions, they work better when the space is simple-either categorical (such as a multi-choice question) or numerical (such as a rating between 0 and 10).Otherwise, the structure is too involved to learn in the multitask setting, and the forecasts can not be communicated efficiently for forecast elicitation.</p>
<p>However, reducing to such a simple space often loses the rich information within the textual judgments.For example, in peer review, the decision of the editor/area chair often relies more on the arguments and justifications in the textual reviews rather than merely on numerical ratings.Furthermore, on online platforms, the inflation of ratings makes them less reliable and distinguishable, while textual reviews tend to be more stable [Filippas et al., 2018].</p>
<p>Given these limitations and the recent success of large language models (LLMs), our research question is: can we develop automated mechanisms that effectively incentivize high-quality, informative textual feedback by rewarding it more than generic or low-quality content?</p>
<p>Intuitively, eliciting textual feedback is inherently more difficult than eliciting numerical or categorical responses.However, the recent rise of powerful Large Language Models (LLMs) has surprisingly flipped this script.LLMs, more or less, estimate the probability distribution of the entirety of human language.Thus, our goal is to instead run the original peer-prediction mechanism by using LLMs to gain access to the prior.We use the LLM's ability to analyze the structure of textual responses and predict the probability of one text given another text (LLM-prediction).This directly addresses the "knowing prior" problem and eliminates the need for multiple tasks or second-order predictions (common for categorical/numerical responses).It is somewhat paradoxical that moving to this much more complex domain actually may make the entire task easier!In essence, LLMs make eliciting textual responses easier than simpler formats, as textual responses offer more complexity that LLMs can leverage, while simpler formats lack this richness.</p>
<p>Directly employing LLMs may reward superficial similarities, such as matching speaking styles, or reviews that offer no more than a reiteration of the paper's abstract, which may benefit LLMgenerated reviews.Ultimately, the goal is to encourage reviewers to delve deeper, providing unique perspectives.A related question is can we distinguish the valuable, unique human expert reviews from the coherent yet potentially superficial reviews generated by AI?</p>
<p>To answer the above research question and not reward superficial similarities, when using LLMs to compute the correlation, it is important to effectively condition out "shortcut" information such as language styles and information contained in any synopsis of the reviewed item.We borrow the term from "shortcut learning", where a machine learning algorithm learns the undesired information from data that is strongly correlated with labels on the training data but lacks generality [Geirhos et al., 2020].In our setting, for example, a human-written review can have a high correlation with an LLM-generated review because they mentioned several particular terms in the paper.However, such "shortcut" information may lead to unintended rewards for shallow reviews (e.g., LLM-generated reviews) and noise caused by different language styles.By conditioning out "shortcut" information, we aim to filter out these superficial aspects and focus on rewarding reviews that demonstrate a deeper level of engagement.</p>
<p>Our Contribution</p>
<p>We apply the LLM-prediction to peer prediction and propose two mechanisms-the Generative Peer Prediction mechanism (GPPM) and the Generative Synopsis Peer Prediction mechanism (GSPPM).At a high level, the former rewards a review based on how much it helps predict the contents of another review.The latter, however, rewards a review based on how much more it helps predict the contents of another review than a mere synopsis of the item to be reviewed, such as the abstract of a paper, thereby conditioning out the "shortcut" information derived from superficial information contained in the synopsis.</p>
<p>We first use theory to present the main idea of our method, where we consider a model with three layers of effort: high, low, and no effort.We show that when the LLM prediction is sufficiently accurate, both mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium.By conditioning on a synopsis of the item to be reviewed, GSPPM can further shrink the gap of expected scores between low-effort and no-effort reporting while preserving the gap between high-effort and low-effort reporting.</p>
<p>We then propose two heuristic implementations for getting the LLM-prediction-Token and Judgment.We highlight that for a robust implementation, it is necessary to preprocess the responses.We propose a straightforward yet effective heuristic preprocessing method.This involves using an LLM to rephrase and summarize the initial responses, aiming to standardize the language style and remove superficial content, and consequently, mitigate the impact of the "shortcut".</p>
<p>Moreover, we conduct experiments on the mechanisms using an ICLR dataset and a Yelp review dataset and observed the following results:</p>
<p>Result 1: GPPM can effectively penalize report degradations.We apply three report degradation methods, which degrade the information of an agent's report.In both ICLR and Yelp datasets, we observe that the expected score computed by the GPPM significantly decreases after all degradations.Result 2: Both GPPM and GSPPM can differentiate three quality levels-human, GPT-4, and GPT-3.5.In the ICLR dataset, we replace an agent's review with a GPT-4-generated review and a GPT-3.5-generatedreview respectively, representing a decreasing level of effort.</p>
<p>We observe that the expected scores computed by both GPPM and GSPPM significantly decrease.Furthermore, the decrease of the GPT-3.5-generatedreview is larger than that of the GPT-4-generated review.Result 3: GSPPM penalizes LLM-generated peer review more than GPPM.We find that the GSPPM applies a more significant expected score penalty on the LLM-generated peer reviews, including both GPT-4 and GPT3.5, compared to GPPM, which indicates its improved capacity to distinguish high-quality reports from low-quality reports.</p>
<p>We further note that our mechanisms can serve not only to assess the quality of reviews which can inform decision-making, but also to incentivize effort from agents.This can be interpreted by considering GPT-3.5, GPT-4, and human-written reviews as representing three levels of effort.Thus, by rescaling the scores of the mechanisms into payments, we can reward high-effort reviews much more than low-effort reviews.</p>
<p>RELATED WORK</p>
<p>Peer prediction mechanisms.The idea of peer prediction, introduced by Miller et al. [2005], proposes a mechanism that induces truth-telling as an equilibrium, assuming the mechanism knows the joint distribution of agents' signals and this knowledge is common knowledge among agents.This fundamental assumption, however, might be challenged in many settings.There have been two primary research directions that can mitigate it.</p>
<p>One primary direction is to generalize peer prediction to a multi-task setting, making it possible to estimate (implicitly or explicitly) the joint distribution between the reports with multiple tasks [Agarwal et al., 2017, Dasgupta and Ghosh, 2013, Kong, 2020, Kong and Schoenebeck, 2019, Schoenebeck and Yu, 2020, Schoenebeck et al., 2021, Shnayder et al., 2016, Zhang and Schoenebeck, 2023a].The other direction follows another mechanism, the Bayesian Truth Serum (BTS) [Prelec, 2004], which also seeks to incentivize truthful reporting as an equilibrium.It directly solicits predictions from agents, i.e., each agent predicts the reports of their peers, bypassing the need for prior knowledge of distributions.Subsequent research has extended the BTS, enhancing its efficacy in diverse scenarios [Radanovic and Faltings, 2013, Schoenebeck and Yu, 2023a, Witkowski and Parkes, 2012a, Zhang and Chen, 2014].</p>
<p>Our contribution markedly diverges from these established approaches in several ways.First, we focus on incentivizing text-based human responses, which entails a highly complex response space, where both estimating the joint distribution and directly eliciting predictions from agents are no longer practical.Second, we introduce the first peer prediction mechanism that employs a Large Language Model (LLM) as an oracle, which is presumed to effectively approximate the joint distribution of responses.As LLM capabilities enhance, our mechanism naturally improves, benefiting from more accurate oracle estimations of joint response distributions.Overall, our approach enables the deployment of peer prediction mechanisms in single-task settings without the necessity of directly soliciting predictions or possessing priori distribution knowledge.</p>
<p>Shortcuts in information elicitation.Geirhos et al. [2020] use "shortcut learning" to describe a machine learning algorithm's reliance on unintended, simpler features rather than learning the intended, complex representations.For example, a deep neural network might only identify cows accurately within grassy scenes, mistaking "grass" for "cows" [Beery et al., 2018].Niven and Kao [2019] show that a language model, BERT [Devlin et al., 2018], might seem to understand reasoning but actually relies on spurious statistical cues in the dataset, particularly the presence of specific words, failing when these cues are absent.</p>
<p>This phenomenon not only exists in artificial intelligent agents but also in human agents.As accessing and processing the information may cost agents a considerable amount of effort, human agents also rely on shortcuts or heuristics.A case in point is peer review processes, where reviewers might focus primarily on assessing the story-telling in the introduction as a shortcut, instead of providing a deeper, more insightful evaluation of the work's merits.</p>
<p>To address this issue and elicit high-quality information rather than superficial shortcuts, researchers have proposed several approaches.Li et al. [2022] propose optimal scoring rules that maximize the marginal expected payoff increase of an agent who exerts a higher level of effort.Zhang and Schoenebeck [2023b] suggest that paying the agents according to a tournament based on their scores could also help the efficiency to elicit high effort.When applying peer prediction mechanisms in a peer grading setting, Gao et al. [2016] finds a Pareto-optimal Nash equilibrium where all agents exploit a low-effort shortcut.To address this problem, Kong and Schoenebeck [2018] propose a peer prediction mechanism robust against such shortcuts, by explicitly eliciting both the intended response and the shortcut, and then conditioning the shortcut out.Our GSPPM is inspired by Kong and Schoenebeck [2018] but focuses on text-based responses and does not require eliciting additional information from the agents.</p>
<p>Additionally, how to robustly aggregate information when there exists (implicitly or explicitly) a shortcut is also widely studied [Arieli et al., 2018, Chen et al., 2021, Guo et al., 2024, Kong et al., 2022, Lu and Kong, 2024, Palley and Soll, 2019, Pan et al., 2023, Prelec et al., 2017].A well-known task is "Is Philadelphia the capital of Pennsylvania?"where the majority will take the shortcut that Philadelphia is a famous big city and big cities are usually the capital, thus making mistakes in this question.Prelec et al. [2017] provide a solution to this by eliciting the agents' prediction and aggregating the "surprisingly popular" choice.</p>
<p>LLM applications in peer review.The core of our experiments relies on using LLMs to analyze peer review data on ICLR and customer review data on Yelp.Recent works claim that LLMs have great potential to support publication [Gao et al., 2022, Salvagno et al., 2023] and review practices [Liang et al., 2023] if properly used.In a survey, it is shown that GPT-generated reviews are appreciated by over 50% of participants, and over 80% of participants found them more beneficial than feedback from at least some human reviewers [Liang et al., 2023].More evidence exists showing that LLMs can be used to handle easy evaluation tasks in peer review, such as summarizing methodologies and assessing writing styles [Donker, 2023], detecting gender bias [Verharen, 2023], and verifying author checklists [Liu and Shah, 2023].However, other work expresses concerns about the ability of LLMs to handle more complicated reviewing tasks.For example, hallucinating occurs when LLMs are asked to provide suggestions for specific improvements [Donker, 2023], and LLMs tend to perform poorly on distinguishing clearly "better" papers [Liu and Shah, 2023].Hosseini and Horbach [2023] provide some recommendations for the proper use of LLMs to support review or editorial processes.</p>
<p>Our work is largely orthogonal to these concerns.Rather than using LLMs to generate reviews directly, our paper focuses on using LLMs as a tool to design automated mechanisms that incentivize high-quality reviews.If the quality of LLM reviews dominated the quality of high-effort human reviews, our work would largely be superfluous, but LLMs cannot yet do this.While LLM-generated reviews may provide additional feedback, for the moment, subjective high-quality human reviews are still essential.</p>
<p>PRELIMINARIES</p>
<p>This section introduces the classic information elicitation model and the preliminaries that guide the design of our method.</p>
<p>Model</p>
<p>In our setting, a set of items (e.g., papers or restaurants) are reviewed by a set of agents, where each item is assigned to multiple agents for review.We reduce the problem to the setting where there is only one item to be reviewed by two agents and emphasize that our method can be applied to any item and any pair of agents.Let  = {1, 2} be the set of agents reviewing the same item.</p>
<p>Item and Signal.Agents' judgments of the item are influenced by the inherent characteristics of the item and other related background knowledge used to generate the judgment.Let  âˆˆ Z denote an item such that observing  =  is sufficient for an agent to form her judgments about the item.Suppose  is sampled from an unknown common prior .</p>
<p>Fig. 2. An Overview of Our Information Elicitation Model</p>
<p>Given an item  = , each agent receives a subjective signal   âˆˆ X when evaluating the item.We use   to denote a potential value for   .Similar to prior literature [Dasgupta andGhosh, 2013, Miller et al., 2005], we adopt the common assumption that signals are i.i.d.conditioned on the item, i.e., Pr[  |  = ] is identical for any agent .Furthermore, when agents evaluate the same item, their signals are expected to be related in some meaningful way.We thus adopt the following assumption, which is required to guarantee that truthful information is elicitable [CrÃ©mer and McLean, 1985].
Assumption 3.1 (Stochastic Relevance). For any ğ‘¥ ğ‘– , ğ‘¥ â€² ğ‘– âˆˆ X such that ğ‘¥ ğ‘– â‰  ğ‘¥ â€² ğ‘– , there exists ğ‘¥ ğ‘— âˆˆ X such that Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] â‰  Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ â€² ğ‘– ].
Reporting Strategy.We assume the report of agent , denoted as X âˆˆ X, shares the same domain as the signal.Agents can truthfully report their signals or manipulate their signals as reports.However, only reports (not signals) are observable to the mechanism.Let   : X â†’ Î” X denote the reporting strategy of agent  so that X =   (  ).Let  denote the truthful reporting strategy such that  (  ) =   for any signal.Effort Model.We assume that agents can obtain signals with different qualities if they exert different levels of effort.We focus on a three-level effort model: exerting no effort with a cost of 0, low effort with a cost of   â‰¥ 0, and high effort with a cost of  â„ &gt;   .We use   to denote the agent 's cost.</p>
<p>For each agent , if she exerts high effort, she observes a high-quality signal   =   .If she exerts low effort, she observes a low-quality signal    =  (  ), which is part of the high-quality signal.Here,  is a deterministic function, so Pr  [Kong and Schoenebeck, 2018], suggests that the high-effort signals are strictly more informative than the low-effort signals.For example, in the setting of peer review, a low-effort reviewer might focus solely on surface-level aspects such as the writing quality of the paper.In contrast, a high-effort reviewer can assess not only the writing quality but also other aspects, such as the novelty of the idea, the soundness of the method, and the validity of the experiments.</p>
<p>Note that our model can also capture a binary effort setting which is assumed in some previous work [Gao et al., 2016, Miller et al., 2005], by assuming    =  (  ) = Null for any   âˆˆ X.In this setting, the agent either receives an informative signal or a Null signal.</p>
<p>Synopsis.In some applications, we are able to find a commonly known synopsis Î˜ =  of the item .We use Î£ to denote the space of the synopsis.The synopsis may determine the low-effort signals.For example, in peer review, when a low-effort reviewer writes a review based solely on the introduction of the paper, the paper introduction can be regarded as a commonly known synopsis that fully determines the low-effort signal.Definition 3.2 (Synopsis-determined Low-effort Signals).We say the low-effort signal is synopsisdetermined if    = ( ) where  is a deterministic function that maps the synopsis to the low-effort signal (Figure 4).</p>
<p>In some cases, the low-effort signal    may contain more information about   than the synopsis  , and the synopsis  does not reveal more information about   than    .In the peer review example, a low-effort reviewer may write a review based on the whole introduction and a glance at the rest of the paper.Peer Prediction Mechanism and Agents' Incentive.A peer prediction mechanism  :
X |ğ¼ | â†’ R |ğ¼ |
takes all agents' reports as input and outputs a performance score   to each agent , i.e.,   =  ( X , X ).Then, agent  is paid according to a linear function   =  â€¢   + , where  &gt; 0 and  are constant parameters.We assume that the utility of agent  is the difference between her payment and her cost of effort, i.e.,   =  â€¢   +  âˆ’   .</p>
<p>Given an item and a mechanism, each agent aims to maximize her expected utility by choosing a reporting strategy   and an effort   .Note that we focus on pure effort strategy, but the reporting strategy   can be randomized.Let (  ,   ) be agent 's strategy and let {(  ,   )}  âˆˆ be the strategy profile of all agents.Under a peer prediction mechanism, an agent's performance score and utility depend on the other agents' strategy.Therefore, we sometimes write agent 's expected utility as a function of the strategy profile of both agents, i.e.,   ((  ,   ), (  ,   )).</p>
<p>Mechanism Design Goal</p>
<p>We first define the solution concept.At a high level, our goal is to design a mechanism that (approximately) maximizes an agent's expected performance score if she exerts high effort and reports truthfully.We call such a mechanism (-)potent.</p>
<p>Definition 3.5 (Potent Mechanism).A peer prediction mechanism  is -potent if there exists a linear payment scheme with parameters , , such that exerting high effort (  =  â„ ) and reporting truthfully (  = ) is an -Bayesian Nash equilibrium.Miller et al. [2005] propose the first peer prediction mechanism.We refer to this as the original peer prediction mechanism.The original mechanism scores an agent based on how well her report predicts a randomly selected peer's report.They use the log scoring rule (LSR) [Cooke, 1991] to quantify the quality of the prediction.Definition 3.6 (Log Scoring Rule (LSR)).Given a set of outcomes, Y, and a prediction over the outcomes  âˆˆ Î”Y, the log scoring rule maps the prediction and an outcome  âˆˆ Y to a score LSR(, ) = log( ()).Furthermore, for  âˆˆ Î”Y, let LSR(, ) =  âˆˆ Y () LSR(, ) denote the expected log score when the outcome is sampled from the distribution .</p>
<p>Miller et al. [2005]'s Peer Prediction Mechanism</p>
<p>The Log Scoring Rule is strictly proper [Gneiting andRaftery, 2007, Selten, 1998], i.e., LSR(, ) &lt; LSR(, ), âˆ€,  âˆˆ Î”Y and  â‰  , meaning that reporting the true belief of the outcome maximized the score.The idea of Miller et al. [2005]'s peer prediction mechanism is thus very straightforward: scoring agent  based on how well her report can predict her peer's report according to a proper scoring rule (such as the Log Scoring Rule).</p>
<p>Definition 3.7 (Original Peer Prediction Mechanism).Given agent 's report x , and the peer agent 's report x  .The performance score of agent  is
LSR(Pr[ğ‘‹ ğ‘— | ğ‘‹ ğ‘– = xğ‘– ], x ğ‘— ) = log Pr[ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– ].
Proposition 3.8 (Proposition 1 of [Miller et al., 2005]).In the binary effort model, if the common prior  and Pr[  |  = ] are known, the above mechanism is potent.</p>
<p>Intuitively, this mechanism incentivizes effort and truth-telling because only the prediction induced by the informative true signal can maximize the expected score under a strictly proper scoring rule.</p>
<p>PEER PREDICTION MECHANISMS FOR TEXTUAL SIGNALS</p>
<p>As discussed, the original peer prediction mechanism requires knowledge about the prior distribution over the items and signals.However, the conditional distribution over signals Pr[  =   |   =   ] can often be complex and difficult to learn from historical data, especially when eliciting textual signals.To address the textual settings, we propose leveraging Large Language Models (LLMs) to create an estimator of this distribution.We design two new mechanisms -the Generative Peer Prediction Mechanism (GPPM) and Generative Synopsis Peer Prediction Mechanism (GSPPM).Assuming a sufficiently accurate LLM estimator, we theoretically prove that both mechanisms are (approximately) potent and show that GSPPM can better differentiate exerting high effort from exerting low effort by reducing the difference between exerting low effort and exerting no effort.</p>
<p>Main Assumption: the LLM-Prediction</p>
<p>We first integrate LLMs into the information elicitation model and discuss the key assumption that builds the theoretical foundations of our mechanisms.Given a prompt  and a response x, a pre-trained LLM can produce a prediction indicating the likelihood of the response being x.We denote the distribution of responses generated by an LLM with a prompt  as Pr LLM( ) and refer to it as the LLM-prediction.Thus, Pr LLM( ) [ x] denotes the probability that response x is predicted by LLM via prompt  .</p>
<p>Sometimes, the prompt itself depends on some input  (e.g., the review from a different agent), in which case we write the prompt as  ().If the input to the prompt itself is a random variable  , (which it often is as in Figure 5), to be consistent with the classic information elicitation model, we use Pr LLM( ) [  = â€¢ |  = ] to denote the LLM-prediction Pr LLM( () ) [â€¢].We are particularly interested in two predictions.First, Pr LLM(
) [ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– ] is the LLM-prediction of agent ğ‘—'s report while integrating agent ğ‘–'s report into the prompt. Second, Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– , Î˜ = ğœƒ ]
is the LLM-prediction of agent 's report given agent 's report x and a commonly known synopsis  .</p>
<p>Our theoretical results require the following (strong) assumption about the fidelity of the LLMprediction to that of the model: Assumption 4.1 (LLM-Prediction).We assume that for an information elicitation task of interest, there exist sets of prompts { ()}  âˆˆ X , { â€² (,  )}  âˆˆ X, âˆˆÎ£ , and ,  â€² â‰¥ 0 such that for any pair of signal   âˆˆ X and synopsis  âˆˆ Î£:
ğ· ğ¾ğ¿ Pr[ğ‘‹ ğ‘— = â€¢ | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = â€¢ | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] â‰¤ ğœ–, ğ· ğ¾ğ¿ Pr[ğ‘‹ ğ‘— = â€¢ | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , Î˜ = ğœƒ ] Pr LLM(ğœ“ â€² ) [ğ‘‹ ğ‘— = â€¢ | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , Î˜ = ğœƒ ] â‰¤ ğœ– â€² ,
where   [ âˆ¥ ] denotes the KL-divergence3 between two distributions.Furthermore, this fact is common knowledge for all agents.</p>
<p>Assumption 4.1 implies that the prediction of an (idealized) LLM can accurately estimate the underlying information structure of the high-effort signals, allowing us to leverage the vast knowledge embedded within LLMs to predict the probability of a new review   given an existing review   .Such an LLM provides a data-driven way of computing this distribution.In Section 5, we will detail various implementations for computing an approximation to Pr LLM( (  ) ) .</p>
<p>We now turn toward defining our two mechanisms by assuming that we have access to some way of computing the LLM-prediction.</p>
<p>The Generative Peer Prediction Mechanism (GPPM)</p>
<p>To define our Generative Peer Prediction Mechanism (GPPM), we combine the idea of an LLM prediction with Miller et al. 's mechanism in the textual setting.Definition 4.2 (Generative Peer Prediction Mechanism (GPPM)).Given the peer's report x  , the performance score of agent  with report x is
LSR( Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— | ğ‘‹ ğ‘– = xğ‘– ], x ğ‘— ) = log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– ]
If Assumption 4.1 holds and the peer's report x  is of high effort and truthful, GPPM will be able to successfully differentiate between different effort levels.We defer the formal theory to Section 4.4 while providing the high-level intuitions of why GPPM is potent here.</p>
<p>First, GPPM should be able to reward an effortful signal more than a no-effort signal.This is because even a low-effort signal    helps predict some of the terms in   .For example, if    merely makes it clear that the reviewed paper proposes a new machine learning algorithm, then the words "loss function" and "training" will (correctly) be much more likely.</p>
<p>In addition, we anticipate that the GPPM should be able to reward a high-effort signal more than a low-effort signal.Intuitively, this is because some insights/critiques can only be predicted by high-effort signals.Consider the peer review example again, a low-effort signal may report common features of a machine learning paper but could overlook specific details of the particular paper such as an elegant proof and a potential broader impact of the method.</p>
<p>The Generative Synopsis Peer Prediction Mechanism (GSPPM)</p>
<p>Intuitively, the above GPPM pays both high and low-effort signals.However, in some applications where low-effort signals may be easily generated by LLMs, such as academic peer review shown in fig. 1, we only want the high-effort signals.This raises the question: Is it possible to further penalize the reporting of low-effort cheap signals?</p>
<p>We propose the Generative Synopsis Peer Prediction Mechanism (GSPPM).The idea is to condition the LLM-prediction on the commonly known synopsis, such as a summary of the reviewed item.As the low-effort signal can be fully characterized by the synopsis, reporting the low-effort signal brings no extra information on predicting agent 's report when the synopsis is conditioned out.In the above peer review example, if the abstract of the reviewed paper has already been inputted into the LLM as prompts, then a low-effort signal simply reiterating the abstract's contents would be redundant.</p>
<p>Definition 4.3 (Generative Synopsis Peer Prediction Mechanism (GSPPM)).Suppose we have a synopsis  of the item being reviewed.Given the peer's report being x  , the performance score for agent  with report x is
LSR( Pr LLM [ğ‘‹ ğ‘— | ğ‘‹ ğ‘– = xğ‘– , Î˜ = ğœƒ ], x ğ‘— ) = log Pr LLM [ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– , Î˜ = ğœƒ ].
In practice, we hypothesize that GSPPM can generally decrease the scores of low-effort signals to that of no-effort signals.This is because the low-effort signal is unlikely to offer additional insights beyond those already present in the synopsis.</p>
<p>For the same reason, we hypothesize that GSPPM can outperform GPPM in distinguishing between low-effort and high-effort signals.Because the scores of low-effort signals are pushed closer to the baseline of no-effort, the reduction in entropy by providing more specific insights that appear in both   and   should be more salient.In other words, we hypothesize that it will improve the signal-to-noise ratio by making insights more prominent and vocabulary alignment less important.</p>
<p>This intuition is very similar to that in Kong and Schoenebeck [2018], which also used conditioning to motivate high-effort signals above low-effort signals.</p>
<p>Theoretical Results: GPPM and GSPPM are ğœ€-Potent</p>
<p>Here, we provide formal theoretical guarantees of our mechanisms under Assumption 4.1.We first present several important notations before introducing our propositions.</p>
<p>We use  (  ;   ) to denote the Shannon mutual information [Shannon, 1948] between two signals,   and   .It provides a quantitative measure of the information shared between them.</p>
<p>ğ¼ (ğ‘‹
ğ‘– ; ğ‘‹ ğ‘— ) = âˆ‘ï¸ ğ‘¥ ğ‘– ,ğ‘¥ ğ‘— âˆˆ X Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , ğ‘‹ ğ‘— = ğ‘¥ ğ‘— ] log Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , ğ‘‹ ğ‘— = ğ‘¥ ğ‘— ] Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— ]
Furthermore, we use  (  ;   |    ) to denote the conditional mutual information.
ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– ) = âˆ‘ï¸ ğ‘‹ ğ‘™ ğ‘– âˆˆ X Pr[ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] âˆ‘ï¸ ğ‘¥ ğ‘– ,ğ‘¥ ğ‘— âˆˆ X Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— , ğ‘‹ ğ‘– = ğ‘¥ ğ‘– | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ]
Then, we have the following propositions for GPPM and GSPPM respectively.
E[GSPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GSPPM( Ïƒ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] &gt; âˆ’ğœ– â€² .
(High effort, truthful v.s.High effort, untruthful)
E[GSPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GSPPM(ğœ (ğ‘‹ ğ‘™ ğ‘– ), ğ‘‹ ğ‘— )] â‰¥ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜, ğ‘‹ ğ‘™ ğ‘– ) âˆ’ ğœ– â€² . (High effort, truthful v.s. Low effort, either truthful or untruthful) E[GSPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GSPPM(ğœ (Null), ğ‘‹ ğ‘— )] â‰¥ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜) âˆ’ ğœ– â€² .
(High effort, truthful v.s.No effort, either truthful or untruthful)</p>
<p>Note that the mutual information  (  ;   | Î˜,    ) and  (  ;   | Î˜) are non-negative.To prove the above results, we analyze agent 's expected scores obtained by each strategy, given that agent  exerts high effort and reports truthfully.When agent  exerts a high effort and reports truthfully, we provide a lower bound of her expected score based on the LLM-Prediction Assumption (Assumption 4.1), which is the negative entropy of agent 's high-effort signal, conditioning on agent 's high-effort signal.For other strategies, we provide an upper bound of agent 's expected score based on the fact that the log scoring rule is proper.The expected score will be at most the the negative entropy of agent 's high-effort signal, conditioning on agent 's signal.Finally, because the high-effort signal is more informative than the low-effort signal, we prove that exerting high effort and reporting truthfully provides approximately the highest expected score.The gap between the negative conditional entropy can be interpreted as mutual information, as stated in the propositions.We defer the proof to Appendix A.</p>
<p>We now discuss the potent properties of the GPPM and GSPPM.Intuitively, if a mechanism can lead to a positive gap between the performance scores, by linearly rescaling the performance scores as payments with parameters  and , it can motivate effort and truth-telling as long as the gap between exerting high effort and exerting low (or zero) effort overcomes the gap between the cost of effort, and consequently, it is potent.</p>
<p>Formally, for the approximately potent properties, we have the following propositions.Again, we defer the proof to Appendix A.</p>
<p>Proposition 4.6.For the Generative Peer Prediction Mechanism (GPPM), when Assumption 4.1 holds with parameter  â‰¥ 0. When  (  ;
ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– ) &gt; 0, GPPM is ğ›¼ğœ–-potent, where ğ›¼ = max ğ‘ â„ âˆ’ ğ‘ ğ‘™ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– ) , ğ‘ â„ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— ) . When ğœ– = 0, GPPM is potent.
Proposition 4.7.For the Generative Synopsis Peer Prediction Mechanism (GSPPM), when Assumption 4.1 holds with parameter  â€² â‰¥ 0. When  (  ;
ğ‘‹ ğ‘— | Î˜, ğ‘‹ ğ‘™ ğ‘– ) &gt; 0, GSPPM is ğ›¼ğœ– â€² -potent, where ğ›¼ = max ğ‘ â„ âˆ’ ğ‘ ğ‘™ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜, ğ‘‹ ğ‘™ ğ‘– ) , ğ‘ â„ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜)
.</p>
<p>When  â€² = 0, GSPPM is potent.</p>
<p>These two propositions show that better LLM-prediction approximations will lead to better incentive properties.On the other hand, to achieve any desired incentive property, there's a corresponding threshold for the error of the LLM-prediction approximation.</p>
<p>Additionally, we compare GPPM and GSPPM.Let Gap(â„, ) denote the difference in the expected score of agent  between exerting high effort and exerting low effort while both agents report truthfully.Let Gap(â„, Null) be the analogous notation for the gap between exerting high effort and exerting no effort.Combining proposition 4.4 and proposition 4.5 together, we have the following corollary.
; ğ‘‹ ğ‘— | Î˜) âˆ’ ğœ– &lt; ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— ) âˆ’ ğœ– in GSPPM than in GPPM.
Corollary 4.8 suggests that compared with GPPM, GSPPM shrinks the gap between no-effort and low-effort, while preserving the gap between low-effort and high-effort.This property of GSPPM offers two advantages over GPPM: 1) in practice, it is harder for agents to "cheat" the mechanism by submitting a low-effort signal and getting a partial payoff, which consequently further incentivizes high-effort signals; 2) it reduces the noise caused by low-effort signals and produces more reliable scores, better differentiating between low-effort and high-effort.</p>
<p>Although this comparison concerns the lower bounds of the gaps, we will confirm this theoretical insight by empirically showing that GSPPM can distinguish low-effort and high-effort reports better.We demonstrate this by using GSPPM and GPPM to score human-written and LLM-generated reviews in Section 7.4.</p>
<p>ESTIMATING THE POSTERIOR PREDICTION VIA LLMS</p>
<p>Although our mechanisms, at this point, may appear to be straightforward generalizations from prior work, implementing them in practice with textual reports presents distinct challenges.In this section, we present the implementations of our mechanisms, which crucially involve estimating the underlying distribution Pr[  = x  |   = x ] via LLMs.We introduce two heuristic implementation methods, each leveraging the capabilities of the LLM in different ways and degrees.</p>
<p>â€¢ The first implementation, denoted as Token, leverages the LLM by directly accessing its output layer to obtain the log probability feedback of the next token, which requires access to the neural network (NN) of the LLM.It has two variants:</p>
<p>-Token-Raw: We directly use the log probability to predict agents' raw reports.</p>
<p>-Token-Preprocess: We first use the LLM to preprocess agents' reports and use the log probability to predict the pre-processed reports.The goal is to standardize the language styles and extract essential information.â€¢ The second implementation, denoted as Judgment, uses the the LLM to first distill each report into a set of "judgments" and further apply the LLM chatbot to estimate the likelihood of each judgment with textual response.This implementation is particularly useful when it is hard to access the output layer of the LLM, since logprob feedback usually cannot be obtained from commercial LLM APIs, such as GPT-4 Chat Completion.Note that both implementations can be used for zero-shot estimation, meaning that estimating Pr[  = x  |   = x ] only requires x and x  without any historical data.We alternatively offer a non-zero-shot implementation that estimates judgment distributions by clustering historical data.We primarily focus on the zero-shot implementations in the main body, while we defer the discussions of the clustering implementation to Appendix C. The full implementations and results are available at https://github.com/yx-lu/Eliciting-Informative-Text-Evaluations-with-Large-Language-Models.</p>
<p>Implementation</p>
<p>Require</p>
<p>Token: Implementation by LLM Token-Prediction</p>
<p>As discussed, the idea of Token is to use the log-probability (logprob) feedback of an LLM to predict a given report x  .This is possible because LLMs are fundamentally designed to estimate the likelihood of a subsequent token in a sequence based on a distribution induced by the prompt.This inherent capability is deeply embedded in their pre-training datasets.Consequently, with access to an open-source LLM, such as Llama-2 [Touvron et al., 2023] or ChatGLM [Du et al., 2022, Zeng et al., 2023], it becomes feasible to compel the LLM to generate a specific output and report the logprob for each output token.</p>
<p>Prompt  ( x ): You are the second reviewer for a scientific paper.You are given a peer review from the other reviewer: [Review x ] Your task is to provide your own judgments of the paper based on the given materials.</p>
<p>Response: 5.1.2Token-Preprocess.In practice, agents' raw reports can vary significantly in language style, such as vocabulary usage, sentence structure, and grammatical errors.Additionally, reports may contain superficial information, such as a summary of the paper, in cases of peer review.Note that even low-quality reviews can be well correlated with superficial information or language style.However, we aim to reward agents based on the quality of the reports' semantics, rather than the correlation on superficial information or language style.Such information may provide "shortcuts" that confound the LLM predictions and consequently lead to unintended rewards.Therefore, we filter out the shortcut information, including language style and superficial information, before applying LLMs to predict the responses.
[Predicted Review x ğ‘— ] Logprob = log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– ]
To address these issues, we propose a simple yet effective preprocessing technique that involves employing a uniform LLM (It may be the same as or different from the LLM generating logprob) to rephrase the text signal into a pre-set format.Note that the preprocessing step should be tailored for different tasks, considering the trade-off between retaining details and distilling the essential information.The details of our preprocessing prompt are presented in Appendix D.1 for reference.</p>
<p>Formally, the performance score of agent  using the implementation Token-Preprocess can be defined as:</p>
<p>Token-Preprocess( x , x  ) = Token-Raw(Preprocess( x ), Preprocess( x  )).</p>
<p>We will provide evidence comparing Token-Raw and Token-Preprocess and discuss this further in Appendix B.1.Generally, we find that Token-Preprocess is likely to effectively filter out such shortcut information and provide scores according to the semantic quality.Therefore, our main paper will mainly discuss the variant Token-Preprocess.</p>
<p>Judgment: Implementation by LLM Judgment-Prediction</p>
<p>Accessing the output layer of an LLM can sometimes be impossible, especially when the LLM is not open-source.For example, the popular OpenAI GPT-4 chat completion API can only output sampled textual responses rather than the probability of tokens.We propose an alternative method that uses an LLM as a black box.Our idea is to first summarize each report as a set of judgments and then predict the peer's report by predicting the probability of each of its judgments and taking the product.</p>
<p>Formally, suppose the set of all possible judgments is  = { 1 ,  2 , . . .,   }.Suppose each signal  (and report x) is a subset of judgments, i.e.,  âŠ‚  .We assume that the event of whether each judgment belongs to a report is independent, thus, we have
Pr[ Xğ‘— = x ğ‘— | Xğ‘– = xğ‘– ] = ğ‘¤ âˆˆ x ğ‘— Pr ğ‘¤ âˆˆ x ğ‘— | Xğ‘– = xğ‘– â€¢ ğ‘¤ âˆˆ ğ½ \ x ğ‘— Pr ğ‘¤ âˆ‰ x ğ‘— | Xğ‘– = xğ‘– .
In the zero-shot setting, it is hard to access the full universe of judgments  , making it infeasible to estimate the probability Pr  âˆ‰ Furthermore, we discretize the prediction space for LLMs' responses.In particular, given a review x and a target judgment , we ask the LLM to score how much  contradicts or supports x with the score ranging from âˆ’3 (strong contradiction) to 3 (strong support).We provide exact prompts used in our experiment in Appendix D. We view this score as the gain in log probability, i.e., log Pr  âˆˆ X | X = x âˆ’ log Pr  âˆˆ X .</p>
<p>By taking the sum over the judgments in x  , we obtain an estimate of the conditional log probability, which we use as the performance score, i.e.</p>
<p>Judgment( xğ‘–
, x ğ‘— ) = log Pr[ Xğ‘— = x ğ‘— | Xğ‘– = xğ‘– ] âˆ’ log Pr[ Xğ‘— = x ğ‘— ].
Note that the performance score is not exactly the log of the conditional probability (the first term) as defined in definition 4.2.However, subtracting the second term which is independent of agent 's strategy will not disturb the incentive of agent .</p>
<p>We emphasize that this method is not an accurate estimate of the goal probability Pr[  = x  |   = x ] in general.However, it is a plausible and feasible solution given a black-box LLM.As we will see in the experiments, this heuristic simplification can still capture key information within agents' reports.We note that the performance of our method may be improved with future versions of LLMs that can output more calibrated and accurate predictions.</p>
<p>EVALUATION</p>
<p>This section presents the methods we use to empirically evaluate the efficacy of our mechanisms.We are primarily interested in testing whether replacing the original high-quality reports with less informative low-quality reports leads to an expected performance score decrease with our mechanisms.We hypothesize that these low-quality reports can be viewed as reports by low-effort agents, and thus, a decrease in performance scores would indicate the empirical effectiveness of our mechanisms in eliciting high effort.</p>
<p>Toward this goal, we first introduce a general workflow and then propose several methods to create low-quality reports, including degrading the original reports and replacing the original reports with LLM-generated fictitious reports, which are regarded as lower quality than the original human expert reports.Note that these methods generating low-quality reports are not necessarily to be restricted in the effort model in Section 3.1 as we focus on testing the efficacy in a realistic scenario without relying on the theoretical assumptions.We finally introduce the statistical metrics used to measure the significance.</p>
<p>Specifically, let  represent an item randomly selected from the dataset.We randomly draw two reports related to , denoted as x and x  .Given the mechanism , the computed score of agent  is  + :=  ( x , x  ).We then replace x with a low-quality report x  and recalculate the score
ğ‘  âˆ’ := ğ‘€ ( xğ‘™ ğ‘– , x ğ‘— ).
Our hypothesis is that the score will decrease in expectation.To test this, we apply a -test to evaluate whether the decrease in score from  + to  âˆ’ is statistically significant, thereby confirming the sensitivity of our methods to manipulations.</p>
<p>Formally, we describe the implementation of the above workflow in Algorithm 1.</p>
<p>Reports Degradations</p>
<p>We regard the original responses from human agents as truthful and high-effort reports and create three degradation methods, which obviously degrade the information within agents' reports and, consequently, should lead to lower scores.We use these degradations as a "sanity check", implying that any mechanisms that fail to penalize these degradations are unlikely to be useful in practice.</p>
<p>Random Report Replacement: We replace a report x with a new report x â€² , which is randomly selected from a different randomly selected item  â€² .This process is denoted as x  = x â€² .Note that x â€² can also be viewed as a zero-effort signal as the information is likely to be irrelevant.For example, this method corresponds to the behaviors of malicious customers who upload irrelevant reviews in exchange for a restaurant's discount rewards.</p>
<p>Sentence-Level Degradation: We degrade the original reports by deleting every other sentence.</p>
<p>Judgment-Level Degradation:</p>
<p>We degrade the list of judgments by deleting every other judgment.This can only be performed with the Token-Preprocess and Judgment implementations, as the preprocessing step has already provided a well-structured list of the judgments.</p>
<p>The sentence-level and judgment-level degradation methods only depend on the agent's report.Thus, they can be viewed as not only creating a low-effort signal but also untruthfully reporting a high-effort signal.The same experiments here (Section 7.2) can also be used to test whether they can incentivize truth-telling.</p>
<p>LLM-generated Reviews.</p>
<p>Furthermore, we employ the LLMs to create synthetic text reports based on a given item, simulating the scenario of the creation of fictitious academic peer reviews.We conduct the experiment on the ICLR2020 OpenReview dataset4 .Specifically, we provide the paper  as input to both GPT-3.5 and GPT-4 [Achiam et al., 2023, Brown et al., 2020], requesting them to generate comprehensive reviews following Liang et al. [2023]'s method.The prompt for this task is presented in Appendix D.4.</p>
<p>We compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generatedreview.Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality.</p>
<p>It is worth noting that, unlike the degradations discussed in section 6.1, the LLM-generated review does not only depend on the agent's signal   since the reviewed paper is input into the LLMs to create reviews.It can not be regarded as untruthfully reporting the original signal.Thus, our focus remains on assessing the effectiveness of differentiating various quality levels of reports across different mechanisms.</p>
<p>Statistics Metrics</p>
<p>The principal criterion for our method's effectiveness is a statistically significant decrease in scores following the degradations.To empirically validate this criterion, we conduct statistical tests on the samples of scores computed with the original reports ( + ) and the degraded/LLM-generated reports ( âˆ’ ).Specifically, We employ the t-test, which is able to identify whether there is a significant decrease in scores from  + to  âˆ’ .Our empirical distributions (Section 7.5) show that the measured differences ( + âˆ’  âˆ’ ) are approximately normally distributed, confirming our data is suitable for the t-test.Below, we introduce the detailed implementation of the -test.</p>
<p>Paired Difference t-test.For each pair of scores ( + ,  âˆ’ ), we use the paired difference t-test to verify if the mean of their difference is greater than 0. Let   :=  ( ) + âˆ’  ( ) âˆ’ , and d denote the mean of the differences   .Formally, the hypotheses for the Paired Difference t-test are:</p>
<p>â€¢ Null Hypothesis ( 0 ): The mean difference d is equal to 0.</p>
<p>â€¢ Alternative Hypothesis ( 1 ): The mean difference d is greater than 0.</p>
<p>The test statistic for the Paired Difference t-test is calculated as follows:
ğ‘¡ = d ğœ ğ‘‘ /
âˆš  where   is the standard deviation of the differences {  }, and  is the number of pairs.Furthermore, we denote
ğ‘†ğ¸ ( d) = ğœ ğ‘‘ âˆš ğ¾
, which is the standard error of the mean difference.The p-value is obtained by  = Pr[ &gt; ], where  is a random variable following the tdistribution with the calculated degrees of freedom  âˆ’ 1, and  is the calculated t-statistic above.Typically, a p-value threshold of 0.05 is used, where a p-value lower than 0.05 leads to the rejection of the null hypothesis, indicating that there's a statistically significant difference.</p>
<p>EXPERIMENTS</p>
<p>This section presents the setup of our experiments and the empirical results.</p>
<p>Experiment Setup</p>
<p>7.1.1Datasets.We use two datasets for our experiment.</p>
<p>Yelp Online Review Data (Yelp) is publicly available online review data from Yelp.The restaurants featured on Yelp are interpreted as the "items" in our model, and customer reviews for each restaurant are interpreted as the "text reports" for our model.We construct our dataset by randomly sampling 1000 items (restaurants) from the entire dataset.For these 1000 items, we have 198,444 text reports (customer reviews) in total, i.e., averaged about 200 reviews per restaurant.ICLR Peer Review Data (ICLR) includes peer review data from the International Conference on Learning Representations (ICLR) 2020, accessed via the OpenReview API. 5 In this dataset, the "items" are the submitted papers, and the "text reports" are the peer reviews they received.Given the typically longer and more informative nature of ICLR reviews compared to Yelp reviews, we choose a smaller sample size for the ICLR dataset to manage computational demands efficiently.Thus, we construct our dataset by randomly sampling 300 items (papers) from the entire dataset.For these 300 items, we have 911 text reports (peer reviews) in total, i.e., averaged about 3 reviews per paper.The Yelp dataset represents a crowdsourcing setting where reviews are completed by the public.On the other hand, the ICLR dataset can be viewed as an example of expert sourcing, involving reviews provided by experts in a highly specialized field.</p>
<p>LLMs.</p>
<p>GPT-4 / GPT-3.5We employ the gpt-4-1106-preview [Achiam et al., 2023] model for preprocessing the reports on the ICLR dataset and the gpt-3.5-turbo-1106[Brown et al., 2020] model for preprocessing the reports on the Yelp dataset. 6For the Judgment implementation, we use gpt-4-1106-preview to predict judgments.Llama-2 For the Token implementations, including both Token-Preprocess and Token-Raw (As discussed in Section 5.1, we defer the results for Token-Raw to Appendix B.1), we use the llama-2-70b-chat [Touvron et al., 2023]  We provide all prompts used, along with example inputs and outputs, in Appendix D.</p>
<p>Mechanisms.</p>
<p>GPPM.We test the GPPM on both the Yelp and the ICLR datasets with Token-Preprocess and Judgment implementations.We perform the report degradation evaluation (Section 6.1) and LLM-generated-review evaluation (Section 6.2).GSPPM.We test the GSPPM on the ICLR dataset.This is because the Yelp dataset we use lacks detailed features of the items (restaurants) and thus we do not have access to a suitable synopsis of each item.For the ICLR dataset, we consider the abstract of a paper as the commonly-known synopsis.Conditional on this, we apply the LSR to calculate the score in the GSPPM.</p>
<p>We only apply the Token-Preprocess implementation for two reasons: First, when we have access to a neural network of an LLM, Token-Preprocess implementation is more timeand cost-efficient than Judgment, given there is no need for API calls.Second, the results</p>
<p>in GPPM indicate that Token-Preprocess generally works better than Judgment on the ICLR dataset.We are primarily interested in comparing its performance differentiating high-quality and low-quality reports.Thus, we only perform the LLM-generated-review evaluation (Section 6.2) to test the GSPPM.Baseline.Additionally, we present a baseline mechanism (Definition 3.7) that uses only the numerical ratings from reports.This approach rebuilds the joint distribution of two ratings7 based on historical data and assigns a log scoring rule to agent  based on her rating compared to agent 's rating.Since it is not clear how to degrade numerical scores to the same degree as sentence/judgment-level degradations, this baseline is only applicable to the experiment of Random Report Replacement.</p>
<p>Result 1: GPPM Effectively Penalizes Report Degradations.</p>
<p>We now delve into our main results.We first describe the results of evaluating the GPPM with three report degradations defined in Section 6.1.We apply sample sizes of  = 500 and  = 1000 for the experiments on the ICLR and the Yelp datasets respectively.We use both Token-Preprocess and Judgment to compute the estimated conditional probability.We visualize the p-values in Figure 6 and defer the comprehensive statistics metrics to Appendix B (Table 5, 6 and 7).GPPM significantly outperforms the baseline.We observe a positive d in all experiments: for three degradation methods, two datasets, and two implementations.Moreover, in the case of "random report replacement", although all tested mechanisms, including the baseline, exhibit a significance score (âˆ’ log 10 (p-value)) well above the threshold of 1.30 (equivalent to p-values &lt; 0.05), the significance score associated with the GPPM are significantly higher compared with the baseline.This observation matches our intuition that there exists a substantially larger amount of information within agents' textual responses, and our GPPM -with either implementationcan successfully extract it.</p>
<p>Token outperforms Judgment on the ICLR dataset.Compared with the Judgment implementation, we observe a higher significance score (equivalent to lower p-values) of Token-Preprocess implementations on the ICLR dataset.However, Token-Preprocess does not perform well on the "judgment-level degradation" test conducted on the Yelp dataset.</p>
<p>Additionally, one may be interested in the performance of Judgment with Llama-2.As GPT-4 has better inference capacity than Llama-2, the performance of Judgment with Llama-2 is worse than Judgment with GPT-4.We provide detailed results in Appendix B. Hence, the Token-Preprocess implementation can be considered superior to Judgment, as it consistently outperforms Judgment when both are applied with the same LLM model.However, Judgment is still valuable when there is no access to the LLM's log probability feedback.</p>
<p>Result 2:</p>
<p>Both GPPM and GSPPM Differentiate Three Quality Levels -human, GPT-4, and GPT-3.5We now show the results of testing the GPPM and GSPPM with LLM-generated reviews introduced in Section 6.2.We focus on the ICLR dataset and use the Token-Preprocess implementation, given that the previous section shows that it is more efficient and performs better than Judgment on the ICLR dataset.We apply a sample size of  = 500.We visualize the p-values in Figure 7 and provide the statistics metrics in Table 3.</p>
<p>In Table 3, we observe that, with both mechanisms and the Token-Preprocess implementation, replacing human-written reviews with either GPT-3.5 or GPT-4-generated reviews leads to a statistically significant decrease in scores, as the significance score are all more than the threshold of 1.30 indicating p-values are all less than the threshold of 0.05.</p>
<p>Comparing the first row and the second row of Table 3, the scores are on the same scale as they are computed by the same mechanism.Thus, we observe that while the standard deviation  () remains similar, replacing the human-written reviews with GPT-3.5-generatedreviews leads to a greater reduction in the expected score, d, than replacing with GPT-4-generated signals.Similarly, comparing the third row and the fourth row, we have the same observation.GPT-4-Generated Review Therefore, with both GPPM and GSPPM, we can observe three score levels, from high to low: human-written, GPT-4-generated, and GPT-3.5-generated.This observation suggests that both our mechanisms can effectively differentiate among these three quality levels.Furthermore, as hypothesized in Section 6.2, GPT-generated reviews can be viewed as low-effort responses.As GPT-4 generates higher-quality reviews than GPT-3.5, we can thus infer three effort levels: high (human-written), medium (GPT-4-generated), and low (GPT-3.5-generated).Therefore, our results also show the effectiveness of our mechanisms in differentiating various effort levels.</p>
<p>7.4 Result 3: GSPPM Penalizes LLM-Generated Peer Peview More than GPPM.We compare the efficacy of GSPPM and GPPM in differentiating between human-written and LLM-generated reviews.As shown in Corollary 4.8, we expect that the GSPPM can do a better job than the GPPM.</p>
<p>Note that the performance scores are no longer on the same scale.Therefore, we focus on the significance scores in Figure 7.We observe that the GSPPM has higher significance scores (lower p-values) in differentiating both GPT-3.5 and GPT-4-generated reviews, indicating its better performance at penalizing the LLM-generated reviews.Furthermore, note that the GPPM merely obtains a p-value of 8.2e-04 for the GPT-4-generated reviews, which is much larger than the GSPPM's p-value under the same condition, indicating a much lower significance.</p>
<p>Therefore, GSPPM has more significant score gaps among these three quality levels-human, GPT-4, and GPT-3.5.This is because much of the superficial information in the LLM-generated reviews is already contained in the synopsis (abstract).Therefore, GSPPM mitigates the ability of the LLMs to obtain a high score.Consequently, the more informative signals that require high effort to access but are necessary for a high-quality report tend to have a higher impact on the score computed by the GSPPM.As mentioned above, we take this as evidence that GSPPM can likely better differentiate high versus low-effort signals and thus better elicit high-effort reports than GPPM in the peer review scenario.</p>
<p>Empirical Distribution of the Performance Score Change</p>
<p>So far, we have been focused on the expected performance scores.However, in practice, the distribution of the performance matters.For example, risk-averse agents may be concerned with the frequency of receiving negative scores, and the variance in performance scores often reflects the fairness of the mechanism.</p>
<p>Here, we use kernel density estimation (KDE)8 to visualize the empirical distribution of  =  + âˆ’  âˆ’ , the change of the performance score after applying the degradation.We observe that the score change follows a bell-shaped distribution.Furthermore, although the score changes are predominantly positive in all the cases, the probability sometimes approaches half, especially in the Yelp dataset.We hypothesize that this is because Yelp reviews tend to be shorter, more diverse, and less standardized, which greatly decreases the quality of the LLM predictions.</p>
<p>We emphasize that a positive score change in expectation is sufficient to incentivize high-quality reports.However, future research may seek to use the performance score as a metric for assessing data quality, in which case it is crucial to minimize the probability of getting a negative score change.We note that fine-tuning an existing LLM (which is not zero-shot) and the development of more advanced language models may contribute to potential improvements.3.</p>
<p>LIMITATION, FUTURE WORK, AND CONCLUSION</p>
<p>Limitations.As the first paper, to our best knowledge, exploring eliciting subjective textual data with LLMs, our work has several limitations, and each of them may lead to a future direction.</p>
<p>First, the theoretical effectiveness of the GPPM and GSPPM highly depends on how well the LLM prediction estimates the real underlying distribution (Assumption 4.1).The quality of this prediction can be influenced by many factors, including prompt engineering, the capacity of the LLMs, etc.The LLM predictions generated by state-of-the-art models like GPT-4 and Llama-2 may not offer perfect estimates.Nonetheless, we anticipate improvements in the efficacy of our mechanisms in the future, considering the rapid advancements in LLM technology.</p>
<p>Second, our empirical findings confirm that the GPPM and GSPPM can effectively penalize several degradations of the quality of agents' reports.However, the performance of our mechanisms in addressing more sophisticated manipulation strategies or even malicious strategies remains unstudied.Therefore, how to model and understand human agents' strategies in the textual world is an open question.</p>
<p>Third, we primarily focused on integrating LLMs with Miller et al. [2005]'s mechanism.Future directions may involve applying our method to a broader set of classic peer prediction mechanisms, especially in the multi-task setting [Agarwal et al., 2017, Dasgupta and Ghosh, 2013, Kong, 2020, Kong and Schoenebeck, 2019, Schoenebeck and Yu, 2020, Schoenebeck et al., 2021, Shnayder et al., 2016, Zhang and Schoenebeck, 2023a].It might be interesting to test whether the properties of these mechanisms in the classic setting can be generalized to the textual setting.Additionally, fine-tuning LLMs to learn the structure of agent responses in multi-task scenarios might be another promising approach.</p>
<p>Yet another approach is to further explore ways to use LLMs to map text into smaller dimensions.Independent work [Wu and Hartline, 2024], explores using known high-quality texts to determine such a mapping.Their approach shares a similar intuition to our Judgment implementation.</p>
<p>Furthermore, in addition to the potent property, prior work has investigated other desiderata, including fairness [Burrell and Schoenebeck, 2021] and efficiency [Xu et al., 2024, Zhang andSchoenebeck, 2023b].We view the comparisons of different text-elicitation mechanisms in terms of these properties as an interesting future work.</p>
<p>Future Work in Benchmarking LLMs with Peer Review Tasks.Building on our current research, there is significant potential for benchmarking LLMs in peer review tasks to mitigate data contamination and data leakage.</p>
<p>Evaluating LLMs on various benchmarks has become crucial in natural language processing (NLP) research.However, several studies have highlighted potential issues of data contamination and data leakage, where the testing data may have been inadvertently included in the training data for LLMs [Deng et al., 2023, Sainz et al., 2023].These issues question the validity of current benchmarks and necessitate the exploration of alternative approaches.</p>
<p>One potential benchmark that mitigates data contamination is to evaluate LLMs on creating peer reviews for the latest research papers.With thousands of new research papers published annually across various fields, these papers are unlikely to appear in the pre-training data, providing a large number of novel test cases.While previous studies have evaluated LLMs' ability to generate new content, such as writing stories [Chakrabarty et al., 2023], generating crowdsourcing responses [Boussioux et al., 2023], and summarizing news articles [Zhang et al., 2024], a common limitation has been the reliance on human evaluators to assess the LLM-generated content.This reliance on manual evaluation can be time-consuming, expensive, and potentially subjective, hindering the scalability and generalizability of these benchmarks.</p>
<p>In the context of academic peer review, existing approaches have focused on analyzing the fraction of common opinions among human and LLM-generated reviews [Liang et al., 2023] or studying the correlation between LLM review scores and human scores [Thelwall, 2024].However, a more sophisticated benchmark is needed to comprehensively evaluate LLM performance in this domain.</p>
<p>To address this need, our proposed mechanism provides an automated scoring method for the quality of LLM-generated academic peer reviews.By demonstrating the ability to differentiate between reviews from humans, GPT-4, and GPT-3.5, this mechanism shows the potential to serve as an automated benchmark for peer review tasks.This approach offers several advantages, including efficient and scalable evaluation, mitigation of subjectivity associated with human evaluation, and a more standardized and reproducible framework for comparing LLM performance.</p>
<p>Conclusion.</p>
<p>In summary, our research introduces a pioneering framework for eliciting highquality textual judgment.To the best of our knowledge, our work is the first to design peer prediction mechanisms for eliciting high-quality textual reports.We propose two mechanisms, GPPM and GSPPM, which utilize the LLM-derived prediction, two implementations for estimating the LLM-derived prediction, and an evaluation workflow.The use of LLM prediction could extend to other peer prediction mechanisms, given that prediction is the foundation of most peer prediction mechanisms.Our empirical results demonstrate the potential of the GPPM and GSPPM to motivate quality human-written reviews over LLM-generated reviews.</p>
<p>A OMITTED PROOFS</p>
<p>Proposition 4.4.For the Generative Peer Prediction Mechanism (GPPM), when Assumption 4.1 holds with parameter  â‰¥ 0, for any agent , given the peer agent  exerting high effort and reporting truthfully, any untruthful reporting strategy Ïƒ â‰   or effort   âˆˆ {0,   } (no-effort or low-effort) implying signal Null or    won't bring more than  score increase.Specifically, we have
E[GPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GPPM( Ïƒ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] &gt; âˆ’ğœ–.
(High effort, truthful v.s.High effort, untruthful)
E[GPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GPPM(ğœ (ğ‘‹ ğ‘™ ğ‘– ), ğ‘‹ ğ‘— )] â‰¥ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– ) âˆ’ ğœ–. (High effort, truthful v.s. Low effort, either truthful or untruthful) E[GPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ E[GPPM(ğœ (Null), ğ‘‹ ğ‘— )] â‰¥ ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— ) âˆ’ ğœ–.
(High effort, truthful v.s.No effort, either truthful or untruthful)</p>
<p>Note that the mutual information  (  ;   |    ) and  (  ;   ) are non-negative.Proof of Proposition 4.4.For agent , we analyze her expected score under each strategy given that agent  exerts high effort and reports truthfully.</p>
<p>If agent  also exerts high effort and reports truthfully, her expected score is
âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] (Assumption 4.1) â‰¥ âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] âˆ’ ğœ– = âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ) âˆ’ ğœ–,
Taking the expectation of   , her expected score is
âˆ‘ï¸ ğ‘¥ ğ‘– Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] âˆ‘ï¸ ğ‘¥ ğ‘– Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] â‰¥ âˆ‘ï¸ ğ‘¥ ğ‘– Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] âˆ’ ğœ– (Assumption 4.1) = âˆ‘ï¸ ğ‘¥ ğ‘– ,ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘– = ğ‘¥ ğ‘– , ğ‘‹ ğ‘— = ğ‘¥ ğ‘— ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] âˆ’ ğœ– = âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– ) âˆ’ ğœ–,
where  (  |   ) is the conditional entropy.</p>
<p>If agent  exerts high effort but reports non-truthfully  â‰  , her expected score is
âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğœ (ğ‘¥ ğ‘– )] â‰¤ âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ] (Log scoring rule is proper.) = âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– = ğ‘¥ ğ‘– ).
With the stochastic relevance assumption and the fact that LSR is strictly proper, the inequality is strict when   â‰   (  ).Notice that there must exist   such that   â‰   (  ) with positive probability as  â‰  .Therefore, taking the expectation of   and  (  ), her expected score is striclty less than âˆ’ (  |   ).Thus, if agent  also exerts high effort, reporting truthfully will be at least âˆ’ better than reporting non-truthfully.</p>
<p>If  If agent  exerts low effort and observes    , and she reports truthfully or non-truthfully with  (   ) her expected score is
âˆ‘ï¸ ğ‘¥ ğ‘™ ğ‘– Pr[ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] âˆ‘ï¸ ğœƒ Pr[Î˜ = ğœƒ |ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğœ (ğ‘¥ ğ‘™ ğ‘– ), Î˜ = ğœƒ ] = âˆ‘ï¸ ğ‘¥ ğ‘™ ğ‘– ,ğœƒ Pr[ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] log Pr LLM(ğœ“ ) [ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘– = ğœ (ğ‘¥ ğ‘™ ğ‘– ), Î˜ = ğœƒ ] â‰¤ âˆ‘ï¸ ğ‘¥ ğ‘™ ğ‘– ,ğœƒ Pr[ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] âˆ‘ï¸ ğ‘¥ ğ‘— Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] log Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– , Î˜ = ğœƒ ] = âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– , Î˜)
The inequality is based on the fact that LSR is proper.</p>
<p>Thus, if agent  also exerts low effort, her expected score is at most âˆ’ (  |    , Î˜).Therefore, the difference in the expected scores between exerting high-effort compared to low-effort is at least
ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– , Î˜) âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– , Î˜) âˆ’ ğœ– â€² = ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜, ğ‘‹ ğ‘™ ğ‘– ) âˆ’ ğœ– â€² .
With analogous analyses, the difference in the expected scores from investing in no-effort compared to high-effort is at least .
ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜) âˆ’ ğœ– â€² = ğ» (ğ‘‹ ğ‘— | Î˜) âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– , Î˜) âˆ’ ğœ– â€² â‰¥ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– , Î˜) âˆ’ ğ» (ğ‘‹ ğ‘— | ğ‘‹ ğ‘– , Î˜) âˆ’ ğœ– â€² =
When  = 0, GPPM is potent.</p>
<p>Proof of Proposition 4.6.Proposition 4.4 lower bounds the gap of the expected performance score between (truth-telling, high effort) and any strategy of an agent in three cases.To prove that the mechanism is -potent, we have to show that there exist constants  &gt; 0,  such that the gap of agent's expected utility between (truth-telling, high effort) and any strategy in the above three cases is always lower-bounded by âˆ’.</p>
<p>â€¢ Suppose agent  plays (  ,  â„ ) where   is an arbitrary reporting strategy.In this case,
ğ‘ˆ ğ‘– ((ğœ, ğ‘ â„ ), (ğœ, ğ‘ â„ )) âˆ’ ğ‘ˆ ğ‘– ((ğœ ğ‘– , ğ‘ â„ ), (ğœ, ğ‘ â„ )) =ğ›¼ E[GPPM(ğ‘‹ ğ‘– , ğ‘‹ ğ‘— )] âˆ’ E[GPPM(ğœ (ğ‘‹ ğ‘– ), ğ‘‹ ğ‘— )] â‰¥ âˆ’ ğ›¼ğœ–.
(Proposition 4.4)</p>
<p>â€¢ Suppose agent  plays (  ,   ).In this case,   ((,  â„ ), (,  â„ )) âˆ’   ((  ,   ), (,  â„ ))
=ğ›¼ E[GPPM(ğ‘‹ ğ‘– , ğ‘‹ ğ‘— )] âˆ’ E[GPPM(ğœ (ğ‘‹ ğ‘™ ğ‘– ), ğ‘‹ ğ‘— )] âˆ’ ğ‘ â„ + ğ‘ ğ‘™ â‰¥ğ›¼ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | ğ‘‹ ğ‘™ ğ‘– ) âˆ’ ğ›¼ğœ– âˆ’ (ğ‘ â„ âˆ’ ğ‘ ğ‘™ ).
(Proposition 4.4)</p>
<p>â€¢ Suppose agent  plays (  , 0).In this case,
ğ‘ˆ ğ‘– ((ğœ, ğ‘ â„ ), (ğœ, ğ‘ â„ )) âˆ’ ğ‘ˆ ğ‘– ((ğœ ğ‘– , 0), (ğœ, ğ‘ â„ )) =ğ›¼ E[GPPM(ğ‘‹ ğ‘– , ğ‘‹ ğ‘— )] âˆ’ E[GPPM(ğœ (Null), ğ‘‹ ğ‘— )] âˆ’ ğ‘ â„ â‰¥ğ›¼ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— ) âˆ’ ğ›¼ğœ– âˆ’ ğ‘ â„ .
(Proposition 4.4)</p>
<p>B ADDITIONAL RESULTS</p>
<p>In this section, we demonstrate the additional results of our experiments, which are omitted in the main text.</p>
<p>B.1 Token-Raw v.s.Token-Preprocess In this subsection, we delve into the effectiveness of preprocessing by contrasting the performance of Token-Raw and Token-Preprocess.Intuitively, Token-Preprocess provides scores reflecting the semantic quality better since it removes 'shortcut" information confounding the LLM prediction, such as the paper summary or the reviewer's language style.We present and explain our observations as follows.</p>
<p>First, we observe that Token-Raw outperforms Token-Preprocess on two degradation tasks (Figure 12).We conjecture that the success of Token-Raw relies on the superficial information commonly found in different reviews of the same item.One example is the paper summary, which most reviewers write before considering the pros and cons.When we use one review to predict tokens in another review of the same item, such common superficial information can significantly increase the performance score.Therefore, as the random replacement degradation and sentence-level degradation remove or reduce the superficial information, Token-Raw can capture such a change more effectively than Token-Preprocess, since the latter removes the superficial information in both reviews.We defer the comprehensive statistics metrics to Table 5 and Table 6.</p>
<p>Our conjecture is further confirmed by the fact that the performance of GPPM implemented with Token-Raw significantly drops when facing more complex tasks like distinguishing GPT reviews from human reviews, especially for GPT-4 generated reviews (Figure 13(a)).As GPT can successfully generate reviews with plausible superficial information, we conjecture this is because the superficial information can successfully fool Token-Raw.Furthermore, when we condition on the synopsis (papers' abstract) using GSPPM, Token-Preprocess can again successfully distinguish GPT-generated reviews from human-written reviews.This is because the superficial information is primarily included in the synopsis, conditioned out by the mechanism.However, in Table 4, we observe that replacing human reviews with GPT-3.5 or GPT-4 reviews leads to a similar expected score decrease, indicating that GSPPM with Token-Raw cannot distinguish GPT-3.5 reviews from GPT-4 reviews as effectively as GSPPM with Token-Preprocess (as shown in Sections 7.3 and 7.4).We conjecture this is because the language style greatly influences LLMs' predictions on raw reviews.9As GPT-3.5 and GPT-4 have similar language styles but are very different from human reviewers, Token-Raw fails to separate the two types of GPT-written reviews well though it succeeds in separating GPT reviews from human reviews.</p>
<p>B.2 Statistics Metrics of Evaluations</p>
<p>In this subsection, we provide all statistics metrics omitted in the main text (Tables 5 to 7).</p>
<p>B.3 Evaluation Results of Implementation Judgment with Llama-2</p>
<p>As we discussed in Section 7.2, here we present the evaluation results for the implementation Judgment with Llama-2 (referred to as Judgement-Llama-2), detailed in Tables 8 and 9. Since Llama-2 is a weaker model compared to GPT-4 and exhibits issues with adherence to prompt instructions, the results are as expected that Judgement-Llama-2 yields worse results in both evaluations.Implementation.We present the pseudocode of the Cluster implementation in Algorithm 2. Similar to the Judgment implementation (Section 5.2), we reload each report x as a set of judgments.In the preparation, we structure the dataset as a list of paired sets of judgments 
= {( xğ‘– ğ‘¡ , x ğ‘— ğ‘¡ )} ğ‘¡ âˆˆ [ğ‘ ğ· ] ,
where each pair ( x  , x   ) indicates two reviews of the same item.Then, we leverage all the judgments in  to fine-tune a short-text embedder, applying the technique introduced by Zhang et al. [2023].This embedder takes a judgment as input and generates a high-dimensional vector as its embedding, which allows us to employ the Minibatch K-means algorithm to build a clustering structure to classify judgments into a fixed number   of clusters10 .Consequently, We introduce x [] to denote a binary cluster indicator for the existence of a judgment of cluster  âˆˆ [  ] in x, and  [] as the random variable indicating the existence of a judgment of cluster  in  .</p>
<p>To calculate the score of Generative Peer Prediction Mechanism, it is necessary to estimate the conditional probability Pr[  = x  |   = x ].We make two assumptions to simplify the estimation.</p>
<p>(1) First, we assume the clusters capture all the information a review has.This means the joint distribution between two reviews can be represented as the joint distribution between all   [] and   [],
Pr[ğ‘‹ ğ‘– = xğ‘– , ğ‘‹ ğ‘— = x ğ‘— ] = Pr ğ‘‹ ğ‘– [1] = xğ‘– [1], ğ‘‹ ğ‘— [1] = x ğ‘— [1], . . . , ğ‘‹ ğ‘– [ğ‘ ğ‘ ] = xğ‘– [ğ‘ ğ‘ ], ğ‘‹ ğ‘— [ğ‘ ğ‘ ] = x ğ‘— [ğ‘ ğ‘ ] .
(2) Second, we assume indicators of different clusters are independent.That is, for any subset K âŠ† [  ], such that,
Pr ğ‘˜ âˆˆ K ğ‘‹ ğ‘– [ğ‘˜] = xğ‘– [ğ‘˜], ğ‘‹ ğ‘— [ğ‘˜] = x ğ‘— [ğ‘˜] = ğ‘˜ âˆˆ K Pr ğ‘‹ ğ‘– [ğ‘˜] = xğ‘– [ğ‘˜], ğ‘‹ ğ‘— [ğ‘˜] = x ğ‘— [ğ‘˜]
With these two assumptions, we can compute the conditional probability as
Pr[ğ‘‹ ğ‘— = x ğ‘— | ğ‘‹ ğ‘– = xğ‘– ] = Pr[ğ‘‹ ğ‘– = xğ‘– , ğ‘‹ ğ‘— = x ğ‘— ] Pr[ğ‘‹ ğ‘– = xğ‘– ] = ğ‘˜ âˆˆ [ğ‘ ğ‘ ] Pr[ğ‘‹ ğ‘– [ğ‘˜] = xğ‘– [ğ‘˜], ğ‘‹ ğ‘— [ğ‘˜] = x ğ‘— [ğ‘˜]] ğ‘˜ âˆˆ [ğ‘ ğ‘ ] Pr[ğ‘‹ ğ‘– [ğ‘˜] = xğ‘– [ğ‘˜]]
.</p>
<p>Furthermore, the numerator can be estimated by the empirical frequency, Results.Tables 10 and 11 present the performance results of the Cluster implementation.Our results suggest that the performance of Cluster is dominated by Token-Preprocess and Judgment, suggesting a large space of improvement.We hypothesize that this is because the clustering step incurs a significant information loss.The development of better context-specific clustering algorithms can potentially improve the performance of Cluster.Discussions and Limitations.We acknowledge several limitations with the Cluster implementation.First, it relies on historical data and thus is not zero-shot.Second, without non-trivial adaptations, Cluster is not compatible with the Generative Synopsis Peer Prediction Mechanism.Lastly, it relies on certain (strong) assumptions.In particular, Assumption (1) drops a lot of information within the textual data and only captures the information about what categories of judgments the review contains.Moreover, Assumption (2) overlooks the correlations between clusters, thereby weakening the predictive power of the judgment cluster information.These limitations suggest large spaces for improvements.
Pr[ğ‘‹ ğ‘– [ğ‘˜] = xğ‘– [ğ‘˜], ğ‘‹ ğ‘— [ğ‘˜] = x ğ‘— [ğ‘˜]] â‰ˆ 1 |ğ· | âˆ‘ï¸ ( xğ‘– ğ‘¡ , x ğ‘— ğ‘¡ ) âˆˆğ· 1[ xğ‘– ğ‘¡ [ğ‘˜] = xğ‘– [ğ‘˜], x ğ‘— ğ‘¡ [ğ‘˜] = x ğ‘— [ğ‘˜]].</p>
<p>D PROMPTS FOR LLMS</p>
<p>This section details the prompts employed in interactions with Large Language Models (LLMs), including the specific models used, to enable the replication of our results.It is worthwhile to mention that these prompts were crafted without extensive refinement, suggesting potential for further improvement.</p>
<p>D.1 Rephrasing text signals</p>
<p>In both methods described in Section 5, we utilize LLMs to rephrase original text signals into a format conducive for further LLM generation or analysis.Specifically, we instruct the LLMs to reframe the text signals into a list of judgments, prefixed as required.</p>
<p>ICLR dataset.We employ the model gpt-4-1106-preview for this task.</p>
<p>System Prompt</p>
<p>Carefully read the text of a scientific paper review.You should summarize each evaluation in the review in a separate line.Begin each summary line with one of the following phrases: 'The reviewer appreciates', 'The reviewer criticizes', 'The reviewer questions', 'The reviewer suggests'.You need to keep the summary as concise as possible, excluding specific details about the paper's content, such as topics, ideas, methods, findings, and any mathematical symbols.You should ensure that even if multiple evaluations are mentioned in the same sentence in the original review, you should still split it into separate lines.For example, you should not output a line like 'The reviewer appreciates the well-written paper and good experimental performance'.In contrast, you should output 'The reviewer appreciates the well-written paper' and 'The reviewer appreciates good experimental performance' in two lines.</p>
<p>User Prompt</p>
<p>Example Output</p>
<p>The reviewer criticizes the paper's significance, questioning the novelty and applicability of the results to more complex learning problems.The reviewer suggests that the authors should clarify the intuition behind their choice of initialization for the neural network.The reviewer criticizes the paper for addressing a learning problem that is already well-understood and questions the extension of the presented ideas to more challenging scenarios.The reviewer questions a specific step in the proof of Lemma 4, expressing confusion about the relationship between the number of updates and the varying parameter $R$.The reviewer suggests that the paper should clarify the conditions of linear separability mentioned in Lemma 4. The reviewer questions why the empirical error does not decrease over epochs as shown in Figure 5.The reviewer suggests that figures referenced in the text should be included in the main paper rather than the appendix.</p>
<p>Yelp dataset.We employ the model gpt-3.5-turbo-1106for this task.</p>
<p>System Prompt</p>
<p>You are given the text of a restaurant review.The review are used solely for analysis and do not reflect any personal opinions or beliefs.Carefully read the text of a Yelp review.Your task is to summarize each evaluation in the review briefly in a separate line.Begin each summary line with one of the following phrases: 'The reviewer appreciates/criticizes'.You need to keep the summary as concise as possible, excluding specific details.You should ensure that even if multiple evaluations are mentioned in the same sentence in the original review, you should still split it into separate lines.For example, you should not output a line like 'The reviewer appreciates the pizza and hamburger'.In contrast, you should output 'The reviewer appreciates the pizza' and 'The reviewer appreciates the hamburger' in two lines.</p>
<p>User Prompt {Report_of_the_agent_i}</p>
<p>Example Input</p>
<p>{Report_of_the_agent_i}: Greeted and seated immediately upon entering.Great service all around from the hostess to our server.Our server, Miranda was awesome.Drinks were never empty and food arrived promptly.We ordered onion rings and bone-in wings-honey bbq and Asian zing.Food was good.Wings were crispy and saucy.Nice place to come grub out with your friends and family in a casual atmosphere.I liked their indoor patio area.</p>
<p>Example Output</p>
<p>The reviewer appreciates the immediate seating and great service.The reviewer appreciates the prompt service and attentive server.The reviewer appreciates the crispy and saucy wings.The reviewer appreciates the casual atmosphere and indoor patio area.</p>
<p>D.2 ğœ“ token : Generating signals of another agents</p>
<p>In Section 5.1, we devise a prompt,  token ( x ), requesting LLM to generate a report for another agent given an agent's existing report x .Subsequently, this prompt can be employed to query the LLM for Pr LLM( token ( x ) ) [ = x  ], serving as a reliable approximation for Pr[  = x  |  = x ] through logprob feedback.</p>
<p>ICLR dataset.We employ the model Llama-2-70b-chat for this task.</p>
<p>System Prompt</p>
<p>You will receive the abstract of a scientific paper and a set of initial reviewer comments on this paper, with each comment presented on a separate line.Following these, you will be provided with a list of additional, specific reviewer comments.Your task is to evaluate how each of the additional comments aligns with the initial set of reviewer comments.For each additional comment, assign a score from -3 to 3, indicating the degree of support or contradiction by the initial comments.For each additional comment, you should output a single line, including the reasons first (starts with 'the initial comments appreciate/criticizes/do not mention'), and then your score (in the format of <SCORE=...>).You should use the following scale for scoring: -3: Strong contradiction -The initial comments directly oppose the additional comment, significantly reducing its credibility (likelihood less than 1/8).Example: Initial comments appreciate the writing, but the additional comment criticizes it.</p>
<p>-2: Moderate contradiction -The initial comments indirectly oppose the additional comment, moderately reducing its credibility (likelihood about 1/4).Example: Initial comments commend the theoretical analysis, but the additional comment faults a theorem proof.</p>
<p>-1: Slight contradiction -The initial comments slightly reduce the credibility of the additional comment (likelihood about 1/2).0: Neutral -The initial comments neither support nor contradict the additional comment, leaving its credibility unchanged.Please try to avoid giving a score of 0 unless you cannot find any potential support or contradiction.1: Slight support -The initial comments slightly increase the credibility of the additional comment (likelihood about 2).2: Moderate support -The initial comments indirectly support the additional comment, moderately increasing its credibility (likelihood about 4).Example: Initial comments commend the theoretical analysis, and the additional comment finds the proofs are enlightening.3: Strong support -The initial comments directly support the additional comment, significantly increasing its credibility (likelihood more than 8 times).Example: Both the initial comments and the additional comment appreciate the writing quality.</p>
<p>User Prompt</p>
<p>Fig. 3 .
3
Fig. 3.An Example of the Three-Level Effort Model</p>
<p>[   =  (  ) |   =   ] = 1.However, given    , the high-quality signal   is uncertain.If an agent exerts no effort, they observe an uninformative signal Null all the time.In this case, Pr[ =  | Null] = Pr[ = ] for all random variables  .The above model implies that the conditional entropy 2  (   |   ) = 0, and  (  |    ) â‰¥ 0. This effort model, called the hierarchical effort model</p>
<p>Fig. 4 .
4
Fig. 4.An Example of the Synopsis-determined Low-effort Signals</p>
<p>Corollary 4.8.If the low-effort signals are synopsis-determined (definition 3.2) and synopsis-covering (3.3), and  â€² = , Gap(â„, ) has the same lower bound,  (  ;   | Î˜) âˆ’ , in both GPPM and GSPPM.In contrast, Gap(â„, Null) has a smaller lower bound  ( </p>
<p>Fig. 5 .
5
Fig.5.A simplified example of a prompt for Token in academic peer review scenario.In Appendix D, we present all exact prompts used in our experiment.</p>
<p>x  | X = x .Therefore, we use  âˆˆ X Pr  âˆˆ X | X = x as a heuristic predictor of Pr[ X = x  | X = x ].</p>
<p>ALGORITHM 1 :
1
Evaluation Workflow Input: A dataset with items and associated text reports.A mechanism  computing the peer prediction scores.A low-quality report generation process .Output: Lists s + and s âˆ’ and the statistics metrics.for  = 1 to  do Draw an item  u.a.r.from all the items; Draw two text reports x and x  u.a.r.from all reports related to  ; Compute  ( ) + :=  ( x , x  ); Replace the text report x with x  according to process ; Compute the statistics metrics based on lists s + and s âˆ’ ;</p>
<p>Fig. 7 .
7
Fig.7.LLM-generated Review Evaluation Result for GPPM and GSPPM with Token-preprocess Implementation: Significance (âˆ’ log 10 (p-value)) of the expected score difference d &gt; 0, higher is better.Typically, a significance score âˆ’ log 10 (p-value) &gt; 1.30 (equivalent to p-value &lt; 0.05) is regarded as significant difference.</p>
<p>Fig. 8 .Fig. 9 .Fig. 10 .
8910
Fig. 8. Empirical Distribution of the Performance Score Change of Random Signal Replacement Evaluation.</p>
<p>Fig. 11 .
11
Fig. 11.Empirical Distribution of the Performance Score Change of LLM-Generated Review Evaluation.This result corresponds to Table3.</p>
<p>)] âˆ’ E[GSPPM( Ïƒ (  ),   )] &gt; âˆ’ â€² .(High effort, truthful v.s.High effort, untruthful) E[GSPPM( (  ),   )] âˆ’ E[GSPPM( (   ),   )] â‰¥  (  ;   | Î˜,    ) âˆ’  â€² .(High effort, truthful v.s.Low effort, either truthful or untruthful) E[GSPPM( (  ),   )] âˆ’ E[GSPPM( (Null),   )] â‰¥  (  ;   | Î˜) âˆ’  â€² .(High effort, truthful v.s.No effort, either truthful or untruthful) Note that the mutual information  (  ;   | Î˜,    ) and  (  ;   | Î˜) are non-negative.Proof of Proposition 4.5.In GSPPM, with analogous analyses, if agent  also exerts high effort and reports truthfully, her expected score is at least âˆ’ (  |   =   , Î˜ =  ) âˆ’  â€² .Taking the expectation over   and Î˜, the expected score is âˆ’ (  |   , Î˜) âˆ’  â€² .If agent  also exerts high effort and reports non-truthfully, her expected score is strictly less than âˆ’ (  |   =   , Î˜ =  ).Taking the expectation over   and Î˜, the expected score is strictly less than âˆ’ (  |   , Î˜).</p>
<p>Fig. 12 .
12
Fig. 12. Report Degradation Evaluation Result for GPPM with Token-raw and Token-preprocess Implementations: Significance (âˆ’ log 10 (p-value)) of the expected score difference d &gt; 0, higher is better.Typically, a significance score âˆ’ log 10 (p-value) &gt; 1.30 (equivalent to p-value &lt; 0.05) is regarded as significant difference.We defer the comprehensive statistics metrics to Table5 and Table 6.</p>
<p>Fig. 13 .
13
Fig.13.Evaluation Result for GPPM and GSPPM with Token-raw and Token-preprocess Implementations in LLM-generated Review: Significance (âˆ’ log 10 (p-value)) of the expected score difference d &gt; 0, higher is better.Typically, a significance score âˆ’ log 10 (p-value) &gt; 1.30 (equivalent to p-value &lt; 0.05) is regarded as significant difference.The comprehensive statistics metrics are in Table3 and Table 4.</p>
<p>Content of the official blind review #3 of ICLR 2020 conference paper2150, see https://openreview.net/forum?id=Skeh-xBYDH&amp; noteId=Bkg_Pp2rcB.</p>
<p>Definition 3.4.A strategy profile {(  ,   )}  âˆˆ is an -BNE if, for any agent  and for any alternative strategies ( â€²  ,  â€²  ), we have:   ((  ,   ), (  ,   )) â‰¥   (( â€²  ,  â€²  ), (  ,   )) âˆ’ .In other words, no agent can gain more than  in expected utility by unilaterally deviating from her strategy in an -BNE.</p>
<p>or effort   âˆˆ {0,   } (no-effort or low-effort) implying signal Null or    won't bring more than  score increase.Specifically, we haveE[GPPM( (  ),   )] âˆ’ E[GPPM( Ïƒ (  ),   )] &gt; âˆ’.(High effort, truthful v.s.High effort, untruthful)E[GPPM( (  ),   )] âˆ’ E[GPPM( (   ),   )] â‰¥  (  ;   |    ) âˆ’ .(Higheffort,truthfulv.s.Low effort, either truthful or untruthful)E[GPPM( (  ),   )] âˆ’ E[GPPM( (Null),   )] â‰¥  (  ;   ) âˆ’ .(Higheffort,truthful v.s.No effort, either truthful or untruthful)Note that the mutual information  (  ;   |    ) and  (  ;   ) are non-negative.
Proposition 4.5. For the Generative Synopsis Peer Prediction Mechanism (GSPPM), when As-sumption 4.1 holds with parameter ğœ–
Proposition 4.4.For the Generative Peer Prediction Mechanism (GPPM), when Assumption 4.1 holds with parameter  â‰¥ 0, for any agent , given the peer agent  exerting high effort and reporting truthfully, any untruthful reporting strategy Ïƒ â‰  â€² â‰¥ 0, for any agent , given the peer agent  exerting high effort and reporting truthfully, any untruthful reporting strategy Ïƒ â‰   or effort   âˆˆ {0,   } (no-effort or low-effort) implying signal Null or    won't bring more than  score increase.Specifically, we have</p>
<p>Table 2 .
2
model with 4-bit quantization to calculate log probabilities for token prediction.The open-source nature of Llama-2 allows for local execution and access to the log probability for each token in a text report.Table illustrating the use of different LLM models in various tasks
TaskLLM UsageNotesPreprocessingICLR: gpt-4-1106-preview Yelp: gpt-3.5-1106-previewDiscussed in footnote 6Token implementationsllama-2-70b-chatOpen source LLMJudgment implementationgpt-4-1106-preview/Generating LLM reviewsgpt-3.5-1106-preview gpt-4-1106-previewDiscussed in Section 6.2</p>
<p>Table 3 .
3
Statistics Metrics of LLM-Generated Review Evaluation for GPPM/GSPPM implemented with Token-Preprocess.d represents the mean of the score differences,  () represents the standard deviation of the score differences, and  ( d) =  ()/ âˆš  represents the standard error of the mean difference.
Significance: -log(p-value)0 20 40 60 80Token-Preprocess Judgment 26.73 78.30 Random Report Replacement Baseline 12.93Token-Preprocess 59.93 Judgment-Level Degradation Judgment 18.86Token-Preprocess 12.80 Sentence-Level Degradation Judgment 11.05(a) Results on ICLR DatasetSignificance: -log(p-value)0 10 20 30 40Token-Preprocess Judgment 9.20 42.95 Random Report Replacement Baseline 2.83Token-Preprocess 1.91 Judgment-Level Degradation Judgment 5.49Token-Preprocess 1.79 Sentence-Level Degradation Judgment 1.17Mechanism Review-Generating LLMdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)GPPMGPT-3.5 GPT-410.020 19.290 0.863 2.904 20.489 0.91611.603 3.1665.0e-28 8.2e-0427.297 3.086GSPPMGPT-3.5 GPT-49.197 14.173 0.634 5.357 14.716 0.65814.495 8.1312.7e-40 1.7e-1539.562 14.770
(b) Results on Yelp DatasetFig.6. Report Degradation Evaluation Result for GPPM with Token-preprocess and Judgment Implementations, as well as the Baseline Mechanism: Significance (âˆ’ log 10 (p-value)) of the expected score difference d &gt; 0, higher is better.Typically, a significance score âˆ’ log 10 (p-value) &gt; 1.3 (equivalent to p-value &lt; 0.05) is regarded as significant difference.</p>
<p>agent  exerts low effort and observes    , and she reports truthfully or non-truthfully with  (   ), then with an analogous derivation, her expected score isâˆ‘ï¸   |   =  (   )] â‰¤ âˆ’ (  |    ).This inequality is based on the fact that Log scoring rule is proper.Notice that according to our hierarchical effort model, (  |    ) &lt;  (  |   ) as   is determined by   .Therefore, the difference in the expected scores between exerting high effort and reporting truthfully compared to exerting low effort is at least (  |    ) âˆ’  (  |   ) âˆ’  =  (  ;   |    ) âˆ’ .With analogous analyses, the difference in the expected scores from investing in no-effort compared to high-effort is at least (  ;   ) âˆ’  =  (  ) âˆ’  (  |   ) âˆ’  â‰¥  (  |    ) âˆ’  (  |   ) âˆ’  =  (  ;   |    ) âˆ’. â–¡ Proposition 4.5.For the Generative Synopsis Peer Prediction Mechanism (GSPPM), when Assumption 4.1 holds with parameter  â€² â‰¥ 0, for any agent , given the peer agent  exerting high effort and reporting truthfully, any untruthful reporting strategy Ïƒ â‰   or effort   âˆˆ {0,   } (no-effort or low-effort) implying signal Null or    won't bring more than  score increase.Specifically, we have E[GSPPM( (  ),<br />
ğ‘– ğ‘¥ ğ‘™Pr[ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ]âˆ‘ï¸ ğ‘¥ ğ‘—Pr[ğ‘‹ ğ‘— = ğ‘¥ ğ‘— | ğ‘‹ ğ‘™ ğ‘– = ğ‘¥ ğ‘™ ğ‘– ] log Pr LLM(ğœ“ )[ğ‘‹ ğ‘— =</p>
<p>(  ;   | Î˜,    ) âˆ’  â€² .Therefore, whenever  (  ;   | Î˜) &gt;  â€² , exerting high effort and reporting truthfully is an  â€² -Bayesian Nash equilibrium in GSPPM.â–¡ Proposition 4.6.For the Generative Peer Prediction Mechanism (GPPM), when Assumption 4.1 holds with parameter  â‰¥ 0. When  (  ;   |    ) &gt; 0, GPPM is -potent, where  = max  â„ âˆ’    (  ;   |    )
ğ‘ â„,ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— )</p>
<p>Take these together, we have that when  = max  â„ âˆ’   (  ;  |   ) ,  â„  (  ;  ) we can obtain a unified lower bound  = .This completes the proof.â–¡ Proposition 4.7.For the Generative Synopsis Peer Prediction Mechanism (GSPPM), when Assumption 4.1 holds with parameter  â€² â‰¥ 0. When  (  ;   | Î˜,    ) &gt; 0, GSPPM is  â€² -potent, where  = max  â„ âˆ’    (  ;   | Î˜,    ) Proof of Proposition 4.7.With analogous analyses of proof of Proposition 4.6, we have Proposition 4.7.â–¡ Corollary 4.8.If the low-effort signals are synopsis-determined (definition 3.2) and synopsis-covering (3.3), and  â€² = , Gap(â„, ) has the same lower bound,  (  ;   | Î˜) âˆ’ , in both GPPM and GSPPM.In contrast, Gap(â„, Null) has a smaller lower bound  (  ;   | Î˜) âˆ’  &lt;  (  ;   ) âˆ’  in GSPPM than in GPPM.Proof of Corollary 4.8.When    =    = (Î˜),  =  â€² , given the fact that    and    contain partial information of   and   respectively, and the assumption that the synopsis does not reveal more information about   than    and    , the statements of the formulas for Gaps in Proposition 4.4 and Proposition 4.5 directly imply the results.â–¡
ğ‘ â„,ğ¼ (ğ‘‹ ğ‘– ; ğ‘‹ ğ‘— | Î˜).When ğœ– â€² = 0, GSPPM is potent.</p>
<p>Table 4 .
4
Table3 and Table 4. Statistics Metrics of LLM-Generated Review Evaluation for GPPM/GSPPM implemented with Token-Raw.Note that GPPM with Token-raw has a p-value 0.64 &gt; 0.05, indicating a failure in differentiating GPT-4-generated review from human-written review.
Mechanism Review-Generating LLMdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)GPPMGPT-3.5 GPT-48.074 31.891 1.426 -0.511 31.689 1.4175.656 -0.3601.3e-08 6.4e-017.884 0.193GSPPMGPT-3.5 GPT-413.776 23.687 1.059 12.912 23.557 1.05412.992 12.2449.5e-34 1.3e-3033.024 29.898</p>
<p>Table 5 .
5
Statistics Metrics of Random Report Replacement Evaluation for GPPM.d represents the mean of the score differences,  () represents the standard deviation of the score differences, and  ( d) represents the standard error of the mean difference.
DatasetImplementationdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)Token-Raw55.444 31.736 1.41939.0265.0e-154153.305ICLRToken-Preprocess 20.784 20.483 0.91622.6665.0e-7978.298Judgment4.5228.8130.39411.4621.8e-2726.735baseline0.2070.6130.0277.5321.2e-1312.928Token-Raw9.975 15.323 0.48520.5765.2e-7978.284YelpToken-Preprocess 5.007 10.915 0.34514.5001.1e-4342.955Judgment1.0555.4380.1726.1316.3e-109.202baseline0.0180.1950.0062.9821.5e-032.834DatasetImplementationdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)Token-Raw11.192 17.337 0.77514.4215.8e-4039.233ICLRToken-Preprocess 5.117 15.261 0.6837.4901.6e-1312.803Judgment1.6625.3960.2416.8809.0e-1211.046Token-Raw1.7787.8730.2497.1379.2e-1312.038YelpToken-Preprocess 0.4977.3160.2312.1461.6e-021.794Judgment0.1483.1310.0991.4946.8e-021.169</p>
<p>Table 6 .
6
Statistics Metrics of Sentence-Level Degradation Evaluation for GPPM.
DatasetImplementationdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)ICLRToken-Preprocess 8.860 10.487 0.469 Judgment 2.150 5.133 0.23018.872 9.3571.2e-60 1.4e-1959.928 18.857YelpToken-Preprocess 0.399 5.606 Judgment 0.362 2.5240.177 0.0802.252 4.5341.2e-02 3.2e-061.911 5.489</p>
<p>Table 7 .
7
Statistics Metrics of Judgment-Level Degradation Evaluation for GPPM.</p>
<p>Table 8 .
8
Statistics Metrics of Random Report Replacement Evaluation (Judgement-Llama-2)
DatasetImplementationdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)ICLRJudgement-Llama-2 0.815 10.634 0.3362.4227.9e-032.103YelpJudgement-Llama-2 0.513 5.4740.1732.9621.6e-032.806</p>
<p>Table 9 .
9
Statistics Metrics of Sentence-Level Degradation Evaluation (Judgement-Llama-2) C ALTERNATIVE IMPLEMENTATION BASED ON CLUSTERING In this section, we explore an alternative implementation named Cluster.This implementation employs clustering of judgments to estimate Pr[  = x  |   = x ].The rationale behind the Cluster implementation is to address situations where LLMs lack specific task knowledge, resulting in inaccurate log probabilities or judgment predictions.</p>
<p>Table 10 .
10
Review dataset  = {( x  , x   )}  âˆˆ [  ] begin // Each ( x  , x   ) contains two reviews of the same item.Train a short text embedder based on the set of judgments ;   := 30; Map judgments to 1, 2, ...,   using Minibatch K-means clustering with the short text embedder.;Denotex[] as a binary indicator for the existence of a judgment with label  in report x;for  = 1 to   do for  0 = 0 to 1 do for  1 = 0 to 1 do   0 , 1 () := 1 | | ( x  , x   ) âˆˆ 1[ x  [] =  0 , x   [] =  1 ]; Query( x ,x  ) ; Input: textual reports x and x  Output: Score for agent : Cluster( x , x  ) begin //  â‰œ log Pr[  = x  |   = x ]  x [ ],0 ( )+ x [ ],1 ( ) ; end Cluster( x , x  ) := ; Statistics Metrics of Random Report Replacement Evaluation (Cluster)
ALGORITHM 2: Cluster: Peer Prediction Score by Clustering judgmentsInitialization: Initialize(ğ·);Input: endendendendQuery of GPPM score: endDataset Implementationdğœ (ğ‘‘) ğ‘†ğ¸ ( d) t-statistic p-value âˆ’ log 10 (p-value)ICLRCluster0.084 0.397 0.0136.6593.6e-1110.438YelpCluster0.482 1.032 0.03314.7744.0e-4544.396
 := 0; for  = 1 to   do  :=  + log  x [ ], x  [ ] ( )</p>
<p>Table 11 .
11
 ( d) t-statistic p-value âˆ’ log 10 (p-value) Statistics Metrics of Judgment-Level Degradation Evaluation (Cluster)
Dataset Implementation ğœ (ğ‘‘) ICLR d Cluster 0.015 0.219 0.0072.2031.4e-021.853YelpCluster0.044 0.474 0.0152.9551.6e-032.796</p>
<p>[Abstract of the paper] {Abstract_of_the_paper} if mechanism == GSPPM else "Not Provided" [Initial reviewer comments] {Rephrased_report_of_the_agent_i} [Additional reviewer comments] {Rephrased_report_of_the_agent_j}</p>
<p>The conditional entropy ğ» (ğ‘‹ | ğ‘Œ ) measures the average amount of uncertainty in ğ‘‹ âˆˆ X given the value of another random variable ğ‘Œ âˆˆ Y, i.e., ğ» (ğ‘‹ | ğ‘Œ ) = âˆ’ ğ‘¥ âˆˆX ğ‘¦ âˆˆY ğ‘ƒ (ğ‘¥, ğ‘¦) log ğ‘ƒ (ğ‘¥,ğ‘¦) ğ‘ƒ (ğ‘¦) .
The KL-divergence between two distributions over the same probability space is ğ· KL (ğ‘ƒ âˆ¥ğ‘„ ) = ğ‘¥ ğ‘ƒ (ğ‘¥ ) log (ğ‘ƒ (ğ‘¥ )/ğ‘„ (ğ‘¥ ) ) .
As in 2020, the generative AI is not as widely used as now, we assume all the academic peer reviews in the dataset are written by humans.
The reason for using the ICLR 2020 dataset is to exclude the chance that reviewers use LLMs to generate their reports, as we discussed in Section
, in 2020, AI-generated reviews were rare.
This selection is based on the nature of the texts in each dataset: Yelp reviews tend to be shorter and less complex, thus not requiring the advanced capabilities of a more powerful language model. In contrast, ICLR reviews are more intricate, justifying the use of the higher-capacity gpt-4-1106-preview model for effective rephrasing.
There are 4 possible ratings in ICLR dataset and 5 possible ratings in Yelp dataset.
We use seaborn.kdeplot (seaborn.pydata.org/generated/seaborn.kdeplot.html) with default parameters to plot the KDEs.
Previous prompt engineering studies have provided evidence that the language style in the prompt can impact LLMs' output[Arora et al., 2022] 
We set ğ‘ ğ‘ = 30 in the implementation.
ACKNOWLEDGMENTSWe would like to thank Prof. Kerby Shedden from the Consulting for Statistics, Computing, and Analytics Research (CSCAR) at the University of Michigan, as well as Huizi Yu, also from the University of Michigan, for their expert consulting on the statistical methods used in our experiments.We would also like to thank our anonymous reviewers at ACM EC'24 for their insightful feedback and suggestions.â€  Supported by National Natural Science Foundation of China award number 62372007.â€¡ Supported by United States National Science Foundation award number 2313137.System PromptYou are the second reviewer for a scientific paper.You are given the abstract of the paper and a list of review judgments from the first reviewer, starting with 'The reviewer appreciates/criticizes/questions/suggests'.Your task is to provide your own judgments of the paper based on the given materials.You should create a separate line for each judgment you have, starting with 'The reviewer appreciates/criticizes/questions/suggests'.Ensure your judgments are concise, excluding specific details about the paper's content.User Prompt[Abstract of the paper] {Abstract_of_the_paper} if mechanism == GSPPM else "Not Provided" [Review judgments from the first reviewer] {Rephrased_report_of_the_agent_i} Forced LLM Output {Rephrased_report_of_the_agent_j} Yelp dataset.We employ the model Llama-2-70b-chat for this task.System PromptYou are the second food critic for a restaurant.You are given the categories of the restaurant and a list of review judgments from the first food critic, starting with 'The reviewer appreciates/criticizes/questions/suggests'.Your task is to provide your own judgments of the restaurant based on the given materials.You should create a separate line for each judgment you have, starting with 'The reviewer appreciates/criticizes/questions/suggests'.Ensure your judgments are concise.User Prompt[Categories of the restaurant] {Categories_of_the_restaurant} if mechanism == GSPPM else "Not Provided" [Review judgments from the first food critic] {Rephrased_report_of_the_agent_i} Forced LLM Output {Rephrased_report_of_the_agent_j} D.3  judg : predicting judgments of another agents In Section 5.2, we devise a prompt,  judg (  , ), requesting LLM to predict how the probability of another agent making a judgment  shifts in response to an agent's signal   .In practice, the prompt is structured to obtain the LLM's predictions for all judgments  within   in a single response.ICLR dataset.We employ the model gpt-4-1106-preview for this task.Example Input{Abstract_of_the_paper}:This work provides an additional step in the theoretical understanding of neural networks.We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees.We empirically verify this and show that this does not hold when the initial conditions are chosen at random.The proof of convergence investigates the interaction between the two layers of the network.Our results highlight the importance of using symmetry in the design of neural networks.{Rephrased_report_of_the_agent_i}:The reviewer criticizes the paper's significance, questioning the novelty and applicability of the results to more complex learning problems.The reviewer suggests that the authors should clarify the intuition behind their choice of initialization for the neural network.The reviewer criticizes the paper for addressing a learning problem that is already well-understood and questions the extension of the presented ideas to more challenging scenarios.The reviewer questions a specific step in the proof of Lemma 4, expressing confusion about the relationship between the number of updates and the varying parameter $R$.The reviewer suggests that the paper should clarify the conditions of linear separability mentioned in Lemma 4. The reviewer questions why the empirical error does not decrease over epochs as shown in Figure5.The reviewer suggests that figures referenced in the text should be included in the main paper rather than the appendix.{Rephrased_report_of_the_agent_j}:The reviewer criticizes the lack of motivation for the underlying problem, suggesting that the class of symmetric boolean functions is not challenging enough to warrant the use of neural networks.The reviewer questions the necessity of using a neural network for the problem when simpler methods are available.The reviewer suggests that the problem would be more interesting if it focused on symmetric functions in the real domain.The reviewer criticizes the writing, specifically the need for more detailed proofs and better motivation in the introduction.The reviewer questions the choice of representation for indicators using ReLUs and suggests a simpler alternative from prior work.The reviewer criticizes the experimental plots for being hard to parse and inconsistent.The reviewer suggests using line plots instead of scatter plots to better highlight trends.The reviewer criticizes the inconsistent sampling on the x-axis and varying number of epochs in experiments.The reviewer suggests that important plots should be included in the main paper and questions whether the experiments were repeated multiple times or based on a single run.Example Output[1] The initial comments criticize the paper for addressing a learning problem that is already well-understood and question the extension of the presented ideas to more challenging scenarios.This aligns with the additional comment criticizing the lack of motivation for the underlying problem, suggesting that the class of symmetric boolean functions is not challenging enough.<SCORE=3>[2] The initial comments do not mention the availability of simpler methods for the problem, but the criticism of the paper's significance and the questioning of the novelty and applicability of the results indirectly support the additional comment questioning the necessity of using a neural network.<SCORE=2> [3] The initial comments do not mention the domain of the symmetric functions, so there is no direct support or contradiction for the additional comment suggesting that the problem would be more interesting if it focused on symmetric functions in the real domain.<SCORE=0> [4] The initial comments suggest that the authors should clarify the intuition behind their choice of initialization and clarify the conditions of linear separability mentioned in Lemma 4. This indirectly supports the additional comment criticizing the writing and the need for more detailed proofs and better motivation.<SCORE=2>[5] The initial comments do not mention the representation for indicators using ReLUs, so there is no direct support or contradiction for the additional comment suggesting a simpler alternative from prior work.<SCORE=0> [6] The initial comments criticize the empirical error presentation in Figure5and suggest that figures should be included in the main paper.This supports the additional comment criticizing the experimental plots for being hard to parse and inconsistent.<SCORE=3>[7] The initial comments do not directly address the format of the plots, but the suggestion that figures should be included in the main paper and the criticism of the empirical error presentation indirectly support the additional comment suggesting the use of line plots instead of scatter plots.<SCORE=2>[8] The initial comments do not mention the sampling on the x-axis or the varying number of epochs in experiments, so there is no direct support or contradiction for the additional comment criticizing the inconsistent sampling and varying number of epochs.<SCORE=0>[9] The initial comments suggest that figures should be included in the main paper, which directly supports the additional comment that important plots should be included in the main paper.The initial comments do not address whether the experiments were repeated multiple times, so there is no direct support or contradiction for that part of the additional comment.<SCORE=2>Yelp dataset.We employ the model gpt-4-1106-preview for this task.System PromptYou will receive the categories of a restaurant and a set of initial reviewer comments on this restaurant, with each comment presented on a separate line.Following these, you will be provided with a list of additional, specific reviewer comments.Your task is to evaluate how each of the additional comments aligns with the initial set of reviewer comments.For each additional comment, assign a score from -3 to 3, indicating the degree of support or contradiction by the initial comments.For each additional comment, you should output a single line, including the reasons first (starts with 'the initial comments appreciate/criticizes/do not mention'), and then your score (in the format of <SCORE=...>).You should use the following scale for scoring: -3: Strong contradiction -The initial comments directly oppose the additional comment, significantly reducing its credibility (likelihood less than 1/8).Example: Initial comments appreciate the pizza, but the additional comment criticizes it.-2: Moderate contradiction -The initial comments indirectly oppose the additional comment, moderately reducing its credibility (likelihood about 1/4).Example: Initial comments appreciate the overall taste, but the additional comment criticizes the chicken.-1: Slight contradiction -The initial comments slightly reduce the credibility of the additional comment (likelihood about 1/2).0: Neutral -The initial comments neither support nor contradict the additional comment, leaving its credibility unchanged.Please try to avoid giving a score of 0 unless you cannot find any potential support or contradiction.1: Slight support -The initial comments slightly increase the credibility of the additional comment (likelihood about 2).2: Moderate support -The initial comments indirectly support the additional comment, moderately increasing its credibility (likelihood about 4).Example: Initial comments appreciate the overall taste, and the additional comment appreciates the chicken.3: Strong support -The initial comments directly support the additional comment, significantly increasing its credibility (likelihood more than 8 times).Example: Both the initial comments and the additional comment appreciate the pizza.The reviewer appreciates the immediate seating and great service.The reviewer appreciates the prompt service and attentive server.The reviewer appreciates the crispy and saucy wings.The reviewer appreciates the casual atmosphere and indoor patio area.User Prompt{Rephrased_report_of_the_agent_j}:The reviewer appreciates the cleanliness of the restaurant.The reviewer appreciates the awesome staff and prompt service.The reviewer criticizes the flavor of the mango habanero wings.Example Output[1] The initial comments do not mention the cleanliness of the restaurant.<SCORE=0>[2] The initial comments appreciate the great service and attentive server, which aligns with appreciating the awesome staff and prompt service.<SCORE=3> [3] The initial comments appreciate the crispy and saucy wings, but do not specifically mention the mango habanero flavor.This is a slight contradiction since the initial comments were positive about the wings in general.<SCORE=-1> D.4 Generating review from paper In Section 7.3, we employ LLMs to generate signals in the peer review scenario, demonstrating that the Generative Synopsis Peer Prediction Mechanism (GSPPM) is more effective at eliciting high-effort signals than the Generative Peer Prediction Mechanism (GPPM).We design a prompt directing the LLM to act as a reviewer assessing a particular paper.ICLR dataset.We employ the model gpt-4-1106-preview and gpt-3.5-turbo-1106for this task.System PromptYou are a reviewer for a machine learning conference.Your task is to review a submitted paper.In your review, you should thoroughly analyze the paper and offer detailed feedback.Highlight the paper's strengths and weaknesses.Your review should be objective, critical, and supportive, aimed at helping the authors improve their work and contributing to the advancement of machine learning research.The paper makes a valuable theoretical contribution to the understanding of neural network initialization, particularly in the context of symmetric functions.The empirical validation is a strong point, although the experiments could be expanded to provide a more comprehensive evaluation.The paper's focus on a single hidden layer network is both a strength, in terms of theoretical tractability, and a weakness, in terms of practical relevance.To move forward, the authors should consider extending their analysis to more complex architectures and providing a broader empirical evaluation.<strong>Recommendation:</strong> Accept with minor revisions.The paper presents interesting theoretical insights and empirical results that contribute to the understanding of neural network initialization.However, the authors should address the limitations regarding the scope and experimental details to strengthen the paper's impact.User Prompt
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Peer prediction with heterogeneous users. Arpit Agarwal, Debmalya Mandal, David C Parkes, Nisarg Shah, Proceedings of the 2017 ACM Conference on Economics and Computation. the 2017 ACM Conference on Economics and ComputationACMJune 2017</p>
<p>Robust forecast aggregation. Itai Arieli, Yakov Babichenko, Rann Smorodinsky, Proceedings of the National Academy of Sciences. 115522018</p>
<p>Ask me anything: A simple strategy for prompting language models. Simran Arora, Avanika Narayan, Laurel Mayee F Chen, Neel Orr, Kush Guha, Ines Bhatia, Christopher Chami, Re, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Recognition in terra incognita. Sara Beery, Grant Van Horn, Pietro Perona, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>The crowdless future? how generative ai is shaping the future of human crowdsourcing. The Crowdless Future. Leonard Boussioux, Jacqueline N Lane, Miaomiao Zhang, Vladimir Jacimovic, Karim R Lakhani, 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Noah Burrell, Grant Schoenebeck, arXiv:2108.05521Measurement integrity in peer prediction: A peer assessment case study. 2021arXiv preprint</p>
<p>Art or artifice? large language models and the false promise of creativity. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu, arXiv:2309.145562023arXiv preprint</p>
<p>Yi-Chun Chen, Manuel Mueller-Frank, Mallesh M Pai, arXiv:2102.02666The wisdom of the crowd and higher-order beliefs. 2021arXiv preprint</p>
<p>Optimal selling strategies under uncertainty for a discriminating monopolist when demands are interdependent. Roger Cooke, Jacques CrÃ©mer, Richard P Mclean, Econometrica. 00129682, 146802625321991. 1985Oxford University PressExperts in uncertainty: opinion and subjective probability in science</p>
<p>Crowdsourced judgement elicitation with endogenous proficiency. Anirban Dasgupta, Arpita Ghosh, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide Web2013</p>
<p>Benchmark probing: Investigating data leakage in large language models. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly. 2023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>The dangers of using large language models for peer review. The Lancet Infectious Diseases. Tjibbe Donker, ; Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2023. 20221Glm: General language model pretraining with autoregressive blank infilling</p>
<p>Reputation inflation. Apostolos Filippas, John Joseph Horton, Joseph Golden, Proceedings of the 2018 ACM Conference on Economics and Computation. the 2018 ACM Conference on Economics and Computation2018</p>
<p>Incentivizing evaluation via limited access to ground truth: Peerprediction makes things worse. Alice Gao, James R Wright, Kevin Leyton-Brown, arXiv:1606.070422016arXiv preprint</p>
<p>Comparing scientific abstracts generated by chatgpt to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. Catherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan Luo, Alexander T Pearson, BioRxiv. 2022</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, JÃ¶rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 2112020</p>
<p>Strictly proper scoring rules, prediction, and estimation. Tilmann Gneiting, Adrian E Raftery, Journal of the American statistical Association. 1024772007</p>
<p>Yongkang Guo, Jason D Hartline, Zhihuan Huang, Yuqing Kong, Anant Shah, Fang-Yi Yu, arXiv:2401.17743Algorithmic robust forecast aggregation. 2024arXiv preprint</p>
<p>Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Mohammad Hosseini, Serge Horbach, 10.1186/s41073-023-00133-5Research Integrity and Peer Review. 82023</p>
<p>Dominantly truthful multi-task peer prediction with a constant number of tasks. Yuqing Kong, Proceedings of the fourteenth annual acm-siam symposium on discrete algorithms. the fourteenth annual acm-siam symposium on discrete algorithmsSIAM2020</p>
<p>Dominantly truthful peer prediction mechanisms with a finite number of tasks. Yuqing Kong, 10.1145/3638239J. ACM. 0004-5411712apr 2024</p>
<p>Eliciting expertise without verification. Yuqing Kong, Grant Schoenebeck, Proceedings of the 2018 ACM Conference on Economics and Computation. the 2018 ACM Conference on Economics and Computation2018</p>
<p>An information theoretic framework for designing information elicitation mechanisms that reward truth-telling. Yuqing Kong, Grant Schoenebeck, ACM Transactions on Economics and Computation (TEAC). 712019</p>
<p>Eliciting thinking hierarchy without a prior. Yuqing Kong, Yunqi Li, Yubo Zhang, Zhihuan Huang, Jinzhao Wu, Advances in Neural Information Processing Systems. 202235</p>
<p>Optimization of scoring rules. Yingkai Li, Jason D Hartline, Liren Shan, Yifan Wu, Proceedings of the 23rd ACM Conference on Economics and Computation. the 23rd ACM Conference on Economics and Computation2022</p>
<p>Can large language models provide useful feedback on research papers?. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel Mcfarland, James Zou, 2023a large-scale empirical analysis</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, 2023</p>
<p>Surrogate scoring rules. Yang Liu, Juntao Wang, Yiling Chen, 10.1145/3565559ACM Trans. Econ. Comput. 2167-8375103feb 2023</p>
<p>Calibrating "cheap signals" in peer review without a prior. Yuxuan Lu, Yuqing Kong, Advances in Neural Information Processing Systems. 362024</p>
<p>Eliciting informative feedback: The peer-prediction method. Nolan Miller, Paul Resnick, Richard Zeckhauser, Management Science. 5192005</p>
<p>Timothy Niven, Hung-Yu Kao, arXiv:1907.07355Probing neural network comprehension of natural language arguments. 2019arXiv preprint</p>
<p>Extracting the wisdom of crowds when information is shared. B Asa, Jack B Palley, Soll, Management Science. 6552019</p>
<p>Robust decision aggregation with second-order information. Yuqi Pan, Zhaohua Chen, Yuqing Kong, arXiv:2311.140942023arXiv preprint</p>
<p>A solution to the single-question crowd wisdom problem. Drazen Prelec ; DraÅ¾en Prelec, Sebastian Seung, John Mccoy, science. 30656952004. 2017Nature</p>
<p>A robust bayesian truth serum for non-binary signals. Goran Radanovic, Boi Faltings, Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI'13. the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI'13AAAI Press2013</p>
<p>Incentives for truthful information elicitation of continuous signals. Goran Radanovic, Boi Faltings, Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI'14. the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI'14AAAI Press2014</p>
<p>Reputation systems. Paul Resnick, Ko Kuwabara, Richard Zeckhauser, Eric Friedman, Communications of the ACM. 43122000</p>
<p>Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. Oscar Sainz, Jon Ander Campos, Iker GarcÃ­a-Ferrero, Julen Etxaniz, arXiv:2310.180182023arXiv preprintOier Lopez de Lacalle, and Eneko Agirre</p>
<p>Can artificial intelligence help for scientific writing?. Michele Salvagno, Fabio Silvio Taccone, Alberto Giovanni Gerli, Critical care. 2712023</p>
<p>Learning and strongly truthful multi-task peer prediction: A variational approach. Grant Schoenebeck, Fang-Yi Yu, arXiv:2009.147302020arXiv preprint</p>
<p>Two strongly truthful mechanisms for three heterogeneous agents answering one question. Grant Schoenebeck, Fang-Yi Yu, 10.1145/3565560ACM Trans. Econ. Comput. 2167-8375104feb 2023a</p>
<p>Two strongly truthful mechanisms for three heterogeneous agents answering one question. Grant Schoenebeck, Fang-Yi Yu, ACM Transactions on Economics and Computation. 1042023b</p>
<p>Information elicitation from rowdy crowds. Grant Schoenebeck, Fang-Yi Yu, Yichi Zhang, 10.1145/3442381.3449840Proceedings of the Web Conference 2021, WWW '21. the Web Conference 2021, WWW '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Axiomatic characterization of the quadratic scoring rule. Reinhard Selten, Experimental Economics. 11998</p>
<p>A mathematical theory of communication. The Bell system technical journal. Claude Elwood, Shannon ; Victor Shnayder, Arpit Agarwal, Rafael Frongillo, David C Parkes, Proceedings of the 2016 ACM Conference on Economics and Computation. the 2016 ACM Conference on Economics and Computation1948. 201627Informed truthfulness in multi-task peer prediction</p>
<p>Auctions and peer prediction for scientific peer review. ArXivorg, 2021. Steven Tadelis. Reputation and feedback systems in online platform markets. Siddarth Srinivasan, Jamie Morgenstern, arXiv:2402.05519Mike Thelwall. Can chatgpt evaluate research quality?. 2016. 20248arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>ChatGPT identifies gender disparities in scientific peer review. eLife. P H Jeroen, Verharen, 10.7554/eLife.90230nov 202312P90230</p>
<p>A robust bayesian truth serum for small populations. Jens Witkowski, David C Parkes, Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI'12. the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI'12AAAI Press2012a</p>
<p>A robust bayesian truth serum for small populations. Jens Witkowski, David C Parkes, Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI'12. the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI'12AAAI Press2012b</p>
<p>Spot check equivalence: an interpretable metric for information elicitation mechanisms. Yifan Wu, Jason Hartline, arXiv:2406.09363arXiv:2402.13567Shengwei Xu, Yichi Zhang, Paul Resnick, and Grant Schoenebeck2024. 2024arXiv preprintElicitationgpt: Text elicitation mechanisms via language models</p>
<p>GLM-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, The Eleventh International Conference on Learning Representations (ICLR). 2023</p>
<p>Elicitability and knowledge-free elicitation with peer prediction. Peter Zhang, Yiling Chen, Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. the 2014 international conference on Autonomous agents and multi-agent systems2014</p>
<p>Yichi Zhang and Grant Schoenebeck. Multitask peer prediction with task-dependent strategies. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, 10.1145/3543507.3583292Proceedings of the ACM Web Conference 2023, WWW '23. the ACM Web Conference 2023, WWW '23New York, NY, USAAssociation for Computing Machinery2024. 2023a12Benchmarking large language models for news summarization</p>
<p>High-effort crowds: Limited liability via tournaments. Yichi Zhang, Grant Schoenebeck, 10.1145/3543507.3583334Proceedings of the ACM Web Conference 2023, WWW '23. the ACM Web Conference 2023, WWW '23New York, NY, USAAssociation for Computing Machinery2023b</p>
<p>ClusterLLM: Large language models as a guide for text clustering. Yuwei Zhang, Zihan Wang, Jingbo Shang, 10.18653/v1/2023.emnlp-main.858Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>            </div>
        </div>

    </div>
</body>
</html>
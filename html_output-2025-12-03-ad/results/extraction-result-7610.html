<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-adb272fbdea3631059cf88ab764bb6c2ce29f965</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/adb272fbdea3631059cf88ab764bb6c2ce29f965" target="_blank">Visual Prompt Tuning</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision and shows that VPT achieves significant performance gains compared to other parameter efficient tuning protocols.</p>
                <p><strong>Paper Abstract:</strong> The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models are Few-Shot Learners (GPT-3) style prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manually-chosen natural language prompts prepended to text input that enable large autoregressive LMs to perform downstream tasks in few-/zero-shot settings; cited as motivating work for prompt-based adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Few-Shot Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer pretrained on large web-scale corpora; used in few-shot/zero-shot prompting where natural-language prompts or examples are prepended to inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot / zero-shot downstream classification and generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various downstream tasks where the model is given natural-language instructions and a small number of exemplars (few-shot) or just instructions (zero-shot) and required to produce correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt prepended to input; often includes exemplar input–output pairs for few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Manual prompt engineering / prepending of instruction text; can include 0–k exemplars (zero- or few-shot); natural language; in the paper cited as an example of prompting success in NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative generalization / task accuracy (as reported in GPT-3 literature)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Described as showing "strong generalization" in few-shot or zero-shot settings (qualitative statement in this paper; no numeric value reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Mentioned as background motivation; no experimental settings provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Tuning (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous prompt tuning for language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Optimization of continuous prompt vectors (task-specific, small parameter set prepended to LM input) instead of full fine-tuning; mentioned as an inspiration and contrasted with visual prompting results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompt-tuned LMs (various transformer LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer LMs adapted by learning a small set of continuous prompt vectors while keeping backbone weights frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (papers cited include large-scale LMs; sizes vary by study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Downstream NLP tasks (classification, generation) under few-shot / transfer settings</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transfer learning on diverse NLP tasks by learning continuous prompt vectors rather than updating backbone parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Continuous prompt vectors prepended to input; few-shot/zero-shot natural language prompting as a style.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Parameter-efficient: small learned vector(s) inserted into input space; compared against full fine-tuning; prior NLP findings reported that prompt tuning often matches but typically does not surpass full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task accuracy / performance relative to full fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Described in this paper as: prompt tuning in NLP can match, but generally does not exceed, full fine-tuning (qualitative summary; no numeric values given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Full fine-tuning (as referenced in the literature)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Generally: matches but does not exceed full fine-tuning (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Referenced as prior work; this paper draws analogy but focuses experiments on visual models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prepend vs Add (visual prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prepending prompt tokens vs element-wise adding prompt vectors to patch embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two ways to incorporate continuous prompts into Transformer inputs: (1) prepend learnable prompt tokens to the input token sequence, (2) add learnable vectors element-wise to patch embeddings; the paper finds prepending generally superior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer (ViT) pretrained on ImageNet-21k, 12 layers, feature dim d=768 (ViT-B/16 used in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M (backbone feature extractor parameters reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB (VTAB-Natural subgroup) and other downstream classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream visual classification tasks spanning Natural, Specialized and Structured datasets (VTAB-1k collection); experiments averaged across tasks/groups.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two prompt insertion formats: (A) Prepend: add p learnable d-dimensional tokens to input token sequence (sequence length increases). (B) Add: keep sequence length fixed and add prompt vectors element-wise to patch embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Evaluated in both shallow and deep variants (prompts only at first layer vs prompts at every layer); default is Prepend in embedding space. Add variant keeps sequence length unchanged; Prepend variant increases sequence length by p tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (task-averaged across datasets / VTAB subgroups)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prepend (default) generally outperforms Add. Add can be competitive with Full on VTAB-Natural in some cases but generally falls behind the default Prepend in both deep and shallow settings (no single-number summary provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prepend (default) and Full fine-tuning (used as contrasts); exact Full numbers provided elsewhere in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Add falls behind Prepend (qualitative); in particular Prepend outperforms Add across deep and shallow settings (no exact aggregate percent given for Add vs Prepend in paper aside from subgroup comments).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16 pretrained on ImageNet-21k; prompt lengths swept (p ∈ {1,5,10,50,100,200}); deep vs shallow insertion examined.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7610.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent vs Pixel prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt insertion in Transformer latent embedding space vs pixel-level / channel concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of placing prompts in the patch-embedding (latent) space vs inserting prompts in pixel space before embedding (Prepend-pixel) or concatenating extra image channel (Concat-channel); latent-space prepending performs best.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer pretrained on ImageNet-21k used for VTAB experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB-Natural (and other VTAB groups)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Visual classification tasks with 1k training examples per task (VTAB-1k suite); Natural subgroup specifically cited for numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt placement alternatives: (1) Prepend in embedding (latent) space (default), (2) Prepend-pixel: prepend learned pixels before the Embed layer, (3) Concat-channel: concatenate extra channel(s) of learned pixels to image.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt placement / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Measured effect on downstream accuracy when prompts are placed at different input stages (latent vs pixel-level).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prepend in latent space (default) best. Prepend-pixel (shallow) drops 6.9 percentage points on VTAB-Natural relative to default latent Prepend; Concat-channel can degrade performance by as much as ~30 percentage points on VTAB-Natural.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Default Prepend (latent space) accuracy on VTAB-Natural (baseline for the quoted drops).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Prepend-pixel: −6.9 percentage points vs default Prepend on VTAB-Natural; Concat-channel: up to −30 percentage points on VTAB-Natural.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16; prompt lengths swept; shallow prompts for Prepend-pixel example compared to default Prepend in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7610.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Final output pooling choices</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice of final pooling: CLS embedding vs image-patch average vs including prompt outputs in pooling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different classification head inputs evaluated: final [CLS] embedding (default), average pooling over image patch outputs (Image-pool), pooling that includes prompt outputs (Prompt-pool or Global-pool); including prompt outputs in pooling can harm accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer with a learnable [CLS] token; default classification uses final [CLS] embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB subgroups (Specialized example given)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>VTAB-1k visual classification benchmark groups (Natural, Specialized, Structured).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Final representation choices for classification head: (a) use final [CLS] embedding (default), (b) average pool over image patch embeddings (Image-pool), (c) include prompt output vectors in pooling (Prompt-pool/Global-pool).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>output representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compared classification accuracy when different pooling strategies are used; Image-pool vs CLS similar, but pooling that includes prompt outputs degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Image-pool yields essentially same results as default CLS (e.g., 82.4 vs 82.3 for VTAB-Specialized). Pooling that includes final prompt outputs (Prompt-pool and Global-pool) can reduce accuracy by up to ~8 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Default CLS pooling accuracy (used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Prompt-pool / Global-pool: up to −8 percentage points vs default CLS; Image-pool: negligible change.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16; VPT-DEEP default configuration used for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7610.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt depth (VPT-Deep vs VPT-Shallow)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth of prompt insertion across Transformer layers (VPT-Deep: prompts at every layer vs VPT-Shallow: prompts only at first layer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inserting learnable prompts at every Transformer layer (deep) vs only at the first layer (shallow) strongly affects performance: VPT-Deep consistently outperforms VPT-Shallow and often outperforms full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16 (and other ViT scales in related experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer backbone pretrained on ImageNet-21k; experiments also repeated on ViT-L and ViT-H to study scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ViT-B: ≈85.8M; ViT-L/H reported elsewhere (307M, 630M respectively)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>24 downstream recognition tasks (FGVC and VTAB-1k groups)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Set of 24 visual classification tasks including Fine-Grained Visual Classification (FGVC) and VTAB-1k subgroups (Natural, Specialized, Structured).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt depth: VPT-Shallow inserts prompts only into first layer input; VPT-Deep inserts prompts into every Transformer layer input.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt insertion depth</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>VPT-Deep evaluated with prompt length swept per task; compared against full fine-tuning and other parameter-efficient methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average test accuracy across task groups; number of tasks where method outperforms Full.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VPT-Deep outperforms Full in 20 of 24 tasks. Example averages (ViT-B/16, Table 1): FGVC: Full 88.54% vs VPT-Deep 89.11% (+0.57% absolute); VTAB-Specialized: Full 75.88% vs VPT-Deep 78.48% (+2.60%); VTAB-Structured: Full 47.64% vs VPT-Deep 54.98% (+7.34%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Full fine-tuning average accuracies (as quoted above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>VPT-Deep yields absolute gains over Full in the examples above (e.g., +0.57, +2.60, +7.34 percentage points for FGVC, Specialized, Structured respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16 pretrained on ImageNet-21k; prompt lengths p swept; VPT-Deep uses dropout 0.1; results averaged across multiple runs; total params substantially smaller than per-task full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Per-task mean and standard deviation reported in figures/tables, but no formal p-values reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7610.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt length sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of prompt tokens (prompt length) influences accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The number of inserted prompt tokens (p) is a key hyperparameter; optimal p varies by task and even a single prompt (p=1) can yield strong gains for VPT-Deep compared to other parameter-efficient baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer pretrained on ImageNet-21k; VPT-Deep experiments sweep prompt-length hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB subgroups (and FGVC averaged experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream visual classification tasks; analysis averaged over each VTAB subgroup.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt length p ∈ {1,5,10,50,100,200} for ViT experiments; different p values correspond to adding that many learnable tokens per prompt insertion location.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt size / hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Ablation shows optimal p varies across tasks; even p=1 often yields meaningful improvements for VPT-Deep; curves shown in Fig. 6 indicate differing sensitivities across VTAB groups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (task/group averages)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative summary: optimal p differs by task; VPT-Deep with p as low as 1 still significantly outperforms Adapter and MLP baselines in several VTAB subgroups (no single-number provided for every p).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Adapter / MLP / Full baselines as compared in plots; baseline depends on specific comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Varies by p and task; general statement: increasing prompt length can improve performance up to a point and the optimal p is task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt lengths swept per task on validation set; training settings per Appendix A (optimizer, learning rates, epochs as described).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7610.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-Learned vs Prompt-Fixed and [CLS]-learned</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of learning prompt embeddings vs keeping them fixed and learning only [CLS]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learning prompt embeddings (Prompt-Learned) yields significant gains; keeping prompts fixed (Prompt-Fixed) yields performance comparable to linear probing; learning only [CLS] performs similar to learning one prompt but worse than tuning optimal p prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer backbone used to evaluate whether improvement stems from sequence-length expansion or from learned prompt vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB benchmark (averaged comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream classification tasks used to test whether learned prompts are crucial vs merely increasing sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Variants tested: Prompt-Learned (prompts initialized randomly and learned), Prompt-Fixed (prompts randomly initialized but frozen), [CLS]-Learned (only [CLS] token updated).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt learning vs fixed</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Experiment isolates contribution of learning prompts vs enlarging input sequence. Prompt-Learned vs Prompt-Fixed vs [CLS]-Learned compared.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (task/group averages)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt-Learned offers significant gains over Prompt-Fixed; Prompt-Fixed yields results comparable to Linear probing; [CLS]-Learned comparable to Learned with p=1 but lags behind default best p selected on validation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Linear probing and default VPT-Learned (best-p) used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Prompt-Fixed ≈ Linear; Prompt-Learned significantly better than Prompt-Fixed (qualitative; shown in Fig. 11), [CLS]-Learned ≈ Learned with p=1 but worse than best-p Learned.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16; experiments as described in Section B (Extended Analysis); results shown in Fig. 11.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7610.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7610.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sharing prompts (Shared-inter, Shared-intra)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of sharing prompt embeddings across prompts and/or layers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different sharing strategies—sharing prompts within a layer (Shared-intra), sharing the same prompts across all layers (Shared-inter), or sharing across both (Shared-all)—affect performance and parameter efficiency; Shared-inter can slightly outperform default VPT-Deep while using similar total trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViT-B/16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision Transformer backbone used to study prompt parameter sharing strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈85.8M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VTAB tasks (averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>VTAB benchmark groups used to evaluate sharing strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Parameter sharing variants: Shared-intra (same prompt embedding repeated within a layer), Shared-inter (same prompt embeddings shared among layers), Shared-all (one prompt embedding shared everywhere).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt parameterization</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compared performance and parameter counts for sharing strategies; reported total parameter multipliers across all VTAB tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (task/group averages) and total parameter multiplier across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Shared-inter slightly outperforms default VPT-Deep in some settings while having similar total trainable parameters (total params multiplier: Shared-inter 1.14× vs Default 1.13× across VTAB tasks); Shared-all degrades performance but still surpasses linear probing in VTAB subgroups.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Default VPT-Deep (per-task best-p) and Linear probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Shared-inter: small positive gain vs default in reported experiments; Shared-all: performance deterioration vs default but still > Linear (qualitative values reported in Fig. 12 and text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ViT-B/16; per-task validation used to choose prompt length; comparisons summarized in Section B (Extended Analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Prompt Tuning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>The Power of Scale for Parameter-Efficient Prompt Tuning <em>(Rating: 2)</em></li>
                <li>Prefix-Tuning: Optimizing Continuous Prompts for Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7610",
    "paper_id": "paper-adb272fbdea3631059cf88ab764bb6c2ce29f965",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "GPT-3 prompting",
            "name_full": "Language Models are Few-Shot Learners (GPT-3) style prompting",
            "brief_description": "Manually-chosen natural language prompts prepended to text input that enable large autoregressive LMs to perform downstream tasks in few-/zero-shot settings; cited as motivating work for prompt-based adaptation.",
            "citation_title": "Language Models are Few-Shot Learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer pretrained on large web-scale corpora; used in few-shot/zero-shot prompting where natural-language prompts or examples are prepended to inputs.",
            "model_size": "175B",
            "task_name": "Few-shot / zero-shot downstream classification and generation",
            "task_description": "Various downstream tasks where the model is given natural-language instructions and a small number of exemplars (few-shot) or just instructions (zero-shot) and required to produce correct outputs.",
            "problem_format": "Natural-language prompt prepended to input; often includes exemplar input–output pairs for few-shot.",
            "format_category": "prompt style",
            "format_details": "Manual prompt engineering / prepending of instruction text; can include 0–k exemplars (zero- or few-shot); natural language; in the paper cited as an example of prompting success in NLP.",
            "performance_metric": "Qualitative generalization / task accuracy (as reported in GPT-3 literature)",
            "performance_value": "Described as showing \"strong generalization\" in few-shot or zero-shot settings (qualitative statement in this paper; no numeric value reported here).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Mentioned as background motivation; no experimental settings provided in this paper.",
            "statistical_significance": null,
            "uuid": "e7610.0",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Prompt Tuning (NLP)",
            "name_full": "Continuous prompt tuning for language models",
            "brief_description": "Optimization of continuous prompt vectors (task-specific, small parameter set prepended to LM input) instead of full fine-tuning; mentioned as an inspiration and contrasted with visual prompting results.",
            "citation_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "mention_or_use": "mention",
            "model_name": "Prompt-tuned LMs (various transformer LMs)",
            "model_description": "Pretrained transformer LMs adapted by learning a small set of continuous prompt vectors while keeping backbone weights frozen.",
            "model_size": "varies (papers cited include large-scale LMs; sizes vary by study)",
            "task_name": "Downstream NLP tasks (classification, generation) under few-shot / transfer settings",
            "task_description": "Transfer learning on diverse NLP tasks by learning continuous prompt vectors rather than updating backbone parameters.",
            "problem_format": "Continuous prompt vectors prepended to input; few-shot/zero-shot natural language prompting as a style.",
            "format_category": "prompt style",
            "format_details": "Parameter-efficient: small learned vector(s) inserted into input space; compared against full fine-tuning; prior NLP findings reported that prompt tuning often matches but typically does not surpass full fine-tuning.",
            "performance_metric": "Task accuracy / performance relative to full fine-tuning",
            "performance_value": "Described in this paper as: prompt tuning in NLP can match, but generally does not exceed, full fine-tuning (qualitative summary; no numeric values given in this paper).",
            "baseline_performance": "Full fine-tuning (as referenced in the literature)",
            "performance_change": "Generally: matches but does not exceed full fine-tuning (qualitative).",
            "experimental_setting": "Referenced as prior work; this paper draws analogy but focuses experiments on visual models.",
            "statistical_significance": null,
            "uuid": "e7610.1",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Prepend vs Add (visual prompts)",
            "name_full": "Prepending prompt tokens vs element-wise adding prompt vectors to patch embeddings",
            "brief_description": "Two ways to incorporate continuous prompts into Transformer inputs: (1) prepend learnable prompt tokens to the input token sequence, (2) add learnable vectors element-wise to patch embeddings; the paper finds prepending generally superior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer (ViT) pretrained on ImageNet-21k, 12 layers, feature dim d=768 (ViT-B/16 used in main experiments).",
            "model_size": "≈85.8M (backbone feature extractor parameters reported)",
            "task_name": "VTAB (VTAB-Natural subgroup) and other downstream classification tasks",
            "task_description": "Downstream visual classification tasks spanning Natural, Specialized and Structured datasets (VTAB-1k collection); experiments averaged across tasks/groups.",
            "problem_format": "Two prompt insertion formats: (A) Prepend: add p learnable d-dimensional tokens to input token sequence (sequence length increases). (B) Add: keep sequence length fixed and add prompt vectors element-wise to patch embeddings.",
            "format_category": "prompt style / input representation",
            "format_details": "Evaluated in both shallow and deep variants (prompts only at first layer vs prompts at every layer); default is Prepend in embedding space. Add variant keeps sequence length unchanged; Prepend variant increases sequence length by p tokens.",
            "performance_metric": "Classification accuracy (task-averaged across datasets / VTAB subgroups)",
            "performance_value": "Prepend (default) generally outperforms Add. Add can be competitive with Full on VTAB-Natural in some cases but generally falls behind the default Prepend in both deep and shallow settings (no single-number summary provided).",
            "baseline_performance": "Prepend (default) and Full fine-tuning (used as contrasts); exact Full numbers provided elsewhere in paper.",
            "performance_change": "Add falls behind Prepend (qualitative); in particular Prepend outperforms Add across deep and shallow settings (no exact aggregate percent given for Add vs Prepend in paper aside from subgroup comments).",
            "experimental_setting": "ViT-B/16 pretrained on ImageNet-21k; prompt lengths swept (p ∈ {1,5,10,50,100,200}); deep vs shallow insertion examined.",
            "statistical_significance": null,
            "uuid": "e7610.2",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Latent vs Pixel prompts",
            "name_full": "Prompt insertion in Transformer latent embedding space vs pixel-level / channel concatenation",
            "brief_description": "Comparison of placing prompts in the patch-embedding (latent) space vs inserting prompts in pixel space before embedding (Prepend-pixel) or concatenating extra image channel (Concat-channel); latent-space prepending performs best.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer pretrained on ImageNet-21k used for VTAB experiments.",
            "model_size": "≈85.8M",
            "task_name": "VTAB-Natural (and other VTAB groups)",
            "task_description": "Visual classification tasks with 1k training examples per task (VTAB-1k suite); Natural subgroup specifically cited for numeric comparisons.",
            "problem_format": "Prompt placement alternatives: (1) Prepend in embedding (latent) space (default), (2) Prepend-pixel: prepend learned pixels before the Embed layer, (3) Concat-channel: concatenate extra channel(s) of learned pixels to image.",
            "format_category": "prompt placement / input modality",
            "format_details": "Measured effect on downstream accuracy when prompts are placed at different input stages (latent vs pixel-level).",
            "performance_metric": "Classification accuracy (percentage points)",
            "performance_value": "Prepend in latent space (default) best. Prepend-pixel (shallow) drops 6.9 percentage points on VTAB-Natural relative to default latent Prepend; Concat-channel can degrade performance by as much as ~30 percentage points on VTAB-Natural.",
            "baseline_performance": "Default Prepend (latent space) accuracy on VTAB-Natural (baseline for the quoted drops).",
            "performance_change": "Prepend-pixel: −6.9 percentage points vs default Prepend on VTAB-Natural; Concat-channel: up to −30 percentage points on VTAB-Natural.",
            "experimental_setting": "ViT-B/16; prompt lengths swept; shallow prompts for Prepend-pixel example compared to default Prepend in embedding space.",
            "statistical_significance": null,
            "uuid": "e7610.3",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Final output pooling choices",
            "name_full": "Choice of final pooling: CLS embedding vs image-patch average vs including prompt outputs in pooling",
            "brief_description": "Different classification head inputs evaluated: final [CLS] embedding (default), average pooling over image patch outputs (Image-pool), pooling that includes prompt outputs (Prompt-pool or Global-pool); including prompt outputs in pooling can harm accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer with a learnable [CLS] token; default classification uses final [CLS] embedding.",
            "model_size": "≈85.8M",
            "task_name": "VTAB subgroups (Specialized example given)",
            "task_description": "VTAB-1k visual classification benchmark groups (Natural, Specialized, Structured).",
            "problem_format": "Final representation choices for classification head: (a) use final [CLS] embedding (default), (b) average pool over image patch embeddings (Image-pool), (c) include prompt output vectors in pooling (Prompt-pool/Global-pool).",
            "format_category": "output representation",
            "format_details": "Compared classification accuracy when different pooling strategies are used; Image-pool vs CLS similar, but pooling that includes prompt outputs degrades performance.",
            "performance_metric": "Classification accuracy (percentage points)",
            "performance_value": "Image-pool yields essentially same results as default CLS (e.g., 82.4 vs 82.3 for VTAB-Specialized). Pooling that includes final prompt outputs (Prompt-pool and Global-pool) can reduce accuracy by up to ~8 percentage points.",
            "baseline_performance": "Default CLS pooling accuracy (used as baseline).",
            "performance_change": "Prompt-pool / Global-pool: up to −8 percentage points vs default CLS; Image-pool: negligible change.",
            "experimental_setting": "ViT-B/16; VPT-DEEP default configuration used for baseline comparisons.",
            "statistical_significance": null,
            "uuid": "e7610.4",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Prompt depth (VPT-Deep vs VPT-Shallow)",
            "name_full": "Depth of prompt insertion across Transformer layers (VPT-Deep: prompts at every layer vs VPT-Shallow: prompts only at first layer)",
            "brief_description": "Inserting learnable prompts at every Transformer layer (deep) vs only at the first layer (shallow) strongly affects performance: VPT-Deep consistently outperforms VPT-Shallow and often outperforms full fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16 (and other ViT scales in related experiments)",
            "model_description": "Vision Transformer backbone pretrained on ImageNet-21k; experiments also repeated on ViT-L and ViT-H to study scale.",
            "model_size": "ViT-B: ≈85.8M; ViT-L/H reported elsewhere (307M, 630M respectively)",
            "task_name": "24 downstream recognition tasks (FGVC and VTAB-1k groups)",
            "task_description": "Set of 24 visual classification tasks including Fine-Grained Visual Classification (FGVC) and VTAB-1k subgroups (Natural, Specialized, Structured).",
            "problem_format": "Prompt depth: VPT-Shallow inserts prompts only into first layer input; VPT-Deep inserts prompts into every Transformer layer input.",
            "format_category": "prompt insertion depth",
            "format_details": "VPT-Deep evaluated with prompt length swept per task; compared against full fine-tuning and other parameter-efficient methods.",
            "performance_metric": "Average test accuracy across task groups; number of tasks where method outperforms Full.",
            "performance_value": "VPT-Deep outperforms Full in 20 of 24 tasks. Example averages (ViT-B/16, Table 1): FGVC: Full 88.54% vs VPT-Deep 89.11% (+0.57% absolute); VTAB-Specialized: Full 75.88% vs VPT-Deep 78.48% (+2.60%); VTAB-Structured: Full 47.64% vs VPT-Deep 54.98% (+7.34%).",
            "baseline_performance": "Full fine-tuning average accuracies (as quoted above).",
            "performance_change": "VPT-Deep yields absolute gains over Full in the examples above (e.g., +0.57, +2.60, +7.34 percentage points for FGVC, Specialized, Structured respectively).",
            "experimental_setting": "ViT-B/16 pretrained on ImageNet-21k; prompt lengths p swept; VPT-Deep uses dropout 0.1; results averaged across multiple runs; total params substantially smaller than per-task full fine-tuning.",
            "statistical_significance": "Per-task mean and standard deviation reported in figures/tables, but no formal p-values reported in text.",
            "uuid": "e7610.5",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Prompt length sensitivity",
            "name_full": "Number of prompt tokens (prompt length) influences accuracy",
            "brief_description": "The number of inserted prompt tokens (p) is a key hyperparameter; optimal p varies by task and even a single prompt (p=1) can yield strong gains for VPT-Deep compared to other parameter-efficient baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer pretrained on ImageNet-21k; VPT-Deep experiments sweep prompt-length hyperparameter.",
            "model_size": "≈85.8M",
            "task_name": "VTAB subgroups (and FGVC averaged experiments)",
            "task_description": "Downstream visual classification tasks; analysis averaged over each VTAB subgroup.",
            "problem_format": "Prompt length p ∈ {1,5,10,50,100,200} for ViT experiments; different p values correspond to adding that many learnable tokens per prompt insertion location.",
            "format_category": "prompt size / hyperparameter",
            "format_details": "Ablation shows optimal p varies across tasks; even p=1 often yields meaningful improvements for VPT-Deep; curves shown in Fig. 6 indicate differing sensitivities across VTAB groups.",
            "performance_metric": "Classification accuracy (task/group averages)",
            "performance_value": "Qualitative summary: optimal p differs by task; VPT-Deep with p as low as 1 still significantly outperforms Adapter and MLP baselines in several VTAB subgroups (no single-number provided for every p).",
            "baseline_performance": "Adapter / MLP / Full baselines as compared in plots; baseline depends on specific comparison.",
            "performance_change": "Varies by p and task; general statement: increasing prompt length can improve performance up to a point and the optimal p is task-dependent.",
            "experimental_setting": "Prompt lengths swept per task on validation set; training settings per Appendix A (optimizer, learning rates, epochs as described).",
            "statistical_significance": null,
            "uuid": "e7610.6",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Prompt-Learned vs Prompt-Fixed and [CLS]-learned",
            "name_full": "Effect of learning prompt embeddings vs keeping them fixed and learning only [CLS]",
            "brief_description": "Learning prompt embeddings (Prompt-Learned) yields significant gains; keeping prompts fixed (Prompt-Fixed) yields performance comparable to linear probing; learning only [CLS] performs similar to learning one prompt but worse than tuning optimal p prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer backbone used to evaluate whether improvement stems from sequence-length expansion or from learned prompt vectors.",
            "model_size": "≈85.8M",
            "task_name": "VTAB benchmark (averaged comparisons)",
            "task_description": "Downstream classification tasks used to test whether learned prompts are crucial vs merely increasing sequence length.",
            "problem_format": "Variants tested: Prompt-Learned (prompts initialized randomly and learned), Prompt-Fixed (prompts randomly initialized but frozen), [CLS]-Learned (only [CLS] token updated).",
            "format_category": "prompt learning vs fixed",
            "format_details": "Experiment isolates contribution of learning prompts vs enlarging input sequence. Prompt-Learned vs Prompt-Fixed vs [CLS]-Learned compared.",
            "performance_metric": "Classification accuracy (task/group averages)",
            "performance_value": "Prompt-Learned offers significant gains over Prompt-Fixed; Prompt-Fixed yields results comparable to Linear probing; [CLS]-Learned comparable to Learned with p=1 but lags behind default best p selected on validation.",
            "baseline_performance": "Linear probing and default VPT-Learned (best-p) used as baselines.",
            "performance_change": "Prompt-Fixed ≈ Linear; Prompt-Learned significantly better than Prompt-Fixed (qualitative; shown in Fig. 11), [CLS]-Learned ≈ Learned with p=1 but worse than best-p Learned.",
            "experimental_setting": "ViT-B/16; experiments as described in Section B (Extended Analysis); results shown in Fig. 11.",
            "statistical_significance": null,
            "uuid": "e7610.7",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Sharing prompts (Shared-inter, Shared-intra)",
            "name_full": "Effects of sharing prompt embeddings across prompts and/or layers",
            "brief_description": "Different sharing strategies—sharing prompts within a layer (Shared-intra), sharing the same prompts across all layers (Shared-inter), or sharing across both (Shared-all)—affect performance and parameter efficiency; Shared-inter can slightly outperform default VPT-Deep while using similar total trainable parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViT-B/16",
            "model_description": "Vision Transformer backbone used to study prompt parameter sharing strategies.",
            "model_size": "≈85.8M",
            "task_name": "VTAB tasks (averaged)",
            "task_description": "VTAB benchmark groups used to evaluate sharing strategies.",
            "problem_format": "Parameter sharing variants: Shared-intra (same prompt embedding repeated within a layer), Shared-inter (same prompt embeddings shared among layers), Shared-all (one prompt embedding shared everywhere).",
            "format_category": "prompt parameterization",
            "format_details": "Compared performance and parameter counts for sharing strategies; reported total parameter multipliers across all VTAB tasks.",
            "performance_metric": "Classification accuracy (task/group averages) and total parameter multiplier across tasks.",
            "performance_value": "Shared-inter slightly outperforms default VPT-Deep in some settings while having similar total trainable parameters (total params multiplier: Shared-inter 1.14× vs Default 1.13× across VTAB tasks); Shared-all degrades performance but still surpasses linear probing in VTAB subgroups.",
            "baseline_performance": "Default VPT-Deep (per-task best-p) and Linear probing.",
            "performance_change": "Shared-inter: small positive gain vs default in reported experiments; Shared-all: performance deterioration vs default but still &gt; Linear (qualitative values reported in Fig. 12 and text).",
            "experimental_setting": "ViT-B/16; per-task validation used to choose prompt length; comparisons summarized in Section B (Extended Analysis).",
            "statistical_significance": null,
            "uuid": "e7610.8",
            "source_info": {
                "paper_title": "Visual Prompt Tuning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "rating": 2
        },
        {
            "paper_title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
            "rating": 1
        }
    ],
    "cost": 0.019123,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Visual Prompt Tuning</h1>
<p>Menglin Jia<em>1,2, Luming Tang</em>1<br>Bor-Chun Chen ${ }^{2}$, Claire Cardie ${ }^{1}$, Serge Belongie ${ }^{3}$ Bharath Hariharan ${ }^{1}$, and Ser-Nam Lim ${ }^{2}$<br>${ }^{1}$ Cornell University ${ }^{2}$ Meta AI ${ }^{3}$ University of Copenhagen</p>
<h4>Abstract</h4>
<p>The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than $1 \%$ of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.</p>
<h2>1 Introduction</h2>
<p>For a variety of recognition applications, the most accurate results are now obtained by adapting large foundation models pre-trained on massive curated or raw data, a finding that mirrors developments in natural language processing (NLP) [6]. ${ }^{1}$ At first glance,this is a success story: one can make rapid progress on multiple recognition problems simply by leveraging the latest and greatest foundation model. In practice, however, adapting these large models to downstream tasks presents its own challenges. The most obvious (and often the most effective) adaptation strategy is full fine-tuning of the pre-trained model on the task at hand, end-to-end. However, this strategy requires one to store and deploy a separate copy of the backbone parameters for every single task. This is an expensive and often infeasible proposition, especially for modern Transformerbased architectures, which are significantly larger than their convolutional neural networks (ConvNet) counterparts, e.g., ViT-Huge [19] (632M parameters) $v s$. ResNet-50 [31] (25M parameters). We therefore ask, what is the best way to adapt large pre-trained Transformers to downstream tasks in terms of effectiveness and efficiency?</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Visual-Prompt Tuning (VPT) vs. other transfer learning methods. (a) Current transfer learning protocols are grouped based on the tuning scope: Full fine-tuning, Head-oriented, and Backbone-oriented approaches. (b) VPT instead adds extra parameters in the input space. (c) Performance of different methods on a wide range of downstream classification tasks adapting a pre-trained ViT-B backbone, with mean and standard deviation annotated. VPT outperforms Full fine-tuning 20 out of 24 cases while using less than $1 \%$ of all model parameters</p>
<p>One straightforward approach is to turn to other strategies that we have perfected for adapting ConvNets to new tasks, as in Fig. 1(a). A popular approach is to fine-tune only a subset of the parameters, such as the classifier head $[56,36,11]$ or the bias terms [8]. Prior research has also looked at adding additional residual blocks (or adapters) to the backbone $[68,87]$. One could implement similar strategies for Transformers. However, in general these strategies under-perform full fine-tuning in accuracy.</p>
<p>We explore a different route in this paper. Instead of altering or fine-tuning the pre-trained Transformer itself, we modify the input to the Transformer. Drawing inspiration from the recent advances on Prompting in NLP [50,48,45,51], we propose a new simple and efficient method to adapt transformer models for downstream vision tasks (Fig. 1(b)), namely Visual-Prompt Tuning (VPT). Our method only introduces a small amount of task-specific learnable parameters into the input space while freezing the entire pre-trained Transformer backbone during downstream training. In practice, these additional parameters are simply prepended into the input sequence of each Transformer layer and learned together with a linear head during fine-tuning.</p>
<p>On 24 downstream recognition tasks spanning different domains using a pretrained ViT backbone, VPT beats all other transfer learning baselines, even surpassing full fine-tuning in 20 cases, while maintaining the advantage of storing remarkably fewer parameters (less than $1 \%$ of backbone parameters) for each individual task (Fig. 1(c)). This result demonstrates the distinctive strength of visual prompting: whereas in NLP, prompt tuning is only able to match full fine-tuning performance under certain circumstances [45]. VPT is especially effective in the low-data regime, and maintains its advantage across data scales. Finally, VPT is competitive for a range of Transformer scales and designs (ViTBase/Large/Huge, Swin). Put together, our results suggest that VPT is one of the most effective ways of adapting ever-growing vision backbones.</p>
<h1>2 Related Work</h1>
<p>Transformer models [73] have gained huge success in NLP [17,66,7]. The triumph of the Transformer architecture also extends to various computer vision tasks, including image classification [19,52], object detection [9,49], semantic and panoptic segmentation [71,89,78], video understanding [25,79,21] and few-shot learning [18], surpassing previous state-of-the-art approaches. Transformers are also being widely used in recent self-supervised pre-training methods [11,30,3]. Given their superior performance and much larger scale compared to ConvNets, how to efficiently adapt Transformers to different vision tasks remains an important open problem. Our proposed VPT provides a promising path forward.
Transfer learning has been extensively studied for vision tasks in the context of ConvNets [92] and many techniques have been introduced including side tuning [87], residual adapter [67], bias tuning [8], etc. Relatively little attention has been paid to vision Transformers adaptation and how well these aforementioned methods perform on this brand new type of architecture remains unknown. On the other hand, given the dominance of large-scale pre-trained Transformerbased Language Models (LM) [17,66,7], many approaches [29,28,35] have been proposed to efficiently fine-tune LM for different downstream NLP tasks [77,76]. Among them, we focus on the following two representative methods in our experiments for benchmarking purposes: Adapters [64] and BitFit [5].</p>
<p>Adapters [34] insert extra lightweight modules inside each Transformer layer. One adapter module generally consists of a linear down-projection, followed by a nonlinear activation function, and a linear up-projection, together with a residual connection $[63,64]$. Instead of inserting new modules, [8] proposed to update the bias term and freeze the rest of backbone parameters when fine-tuning ConvNets. BitFit [3] applied this technique to Transformers and verified its effectiveness on LM tuning. Our study demonstrates that VPT, in general, provides improved performance in adapting Transformer models for vision tasks, relative to the aforementioned two well-established methods in NLP.
Prompting [50] originally refers to prepending language instruction to the input text so that a pre-trained LM can "understand" the task. With manually chosen prompts, GPT-3 shows strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings [7]. In addition to the follow-up works on how to construct better prompting texts [70,37], recent works propose to treat the prompts as task-specific continuous vectors and directly optimize them via gradients during fine-tuning, namely Prompt Tuning [48,45,51]. Compared to full fine-tuning, it achieves comparable performance but with $1000 \times$ less parameter storage. Although prompting has also been applied to visionlanguage models recently $[65,91,39,84,22]$, prompting is still limited to the input of text encoders. Due to the disparity between vision and language modalities, in this paper we ask: can the same method can be applied successfully to image encoders? We are the first work (see related concurrent works [69,80,14,2]) to tackle this question and investigate the generality and feasibility of visual prompting via extensive experiments spanning multiple kinds of recognition tasks across multiple domains and backbone architectures.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Overview of our proposed Visual-Prompt Tuning. We explore two variants: (a) prepend a set of learnable parameters to each Transformer encoder layer's input (VPT-DEEP); (b) only insert the prompt parameters to the first layer's input (VPTshallow). During training on downstream tasks, only the parameters of prompts and linear head are updated while the whole Transformer encoder is frozen.</p>
<h1>3 Approach</h1>
<p>We propose Visual-Prompt Tuning (VPT) for adapting large pre-trained vision Transformer models. VPT injects a small number of learnable parameters into Transformer's input space and keeps the backbone frozen during the downstream training stage. The overall framework is presented in Fig. 2. We first define the notations in Sec. 3.1, then describe VPT formally in Sec. 3.2.</p>
<h3>3.1 Preliminaries</h3>
<p>For a plain Vision Transformer (ViT) [19] with $N$ layers, an input image is divided into $m$ fixed-sized patches $\left{I_{j} \in \mathbb{R}^{3 \times h \times w} \mid j \in \mathbb{N}, 1 \leq j \leq m\right} . h, w$ are the height and width of the image patches. Each patch is then first embedded into $d$-dimensional latent space with positional encoding:</p>
<p>$$
\mathbf{e}<em j="j">{0}^{j}=\operatorname{Embed}\left(I</em>, j=1,2, \ldots m
$$}\right) \quad \mathbf{e}_{0}^{j} \in \mathbb{R}^{d</p>
<p>We denote the collection of image patch embeddings, $\mathbf{E}<em i="i">{i}=\left{\mathbf{e}</em>\right)$. Together with an extra learnable classification token ([CLS]), the whole ViT is formulated as:}^{j} \in \mathbb{R}^{d} \mid j \in\right.$ $\mathbb{N}, 1 \leq j \leq m}$, as inputs to the $(i+1)$-th Transformer layer $\left(L_{i+1</p>
<p>$$
\begin{aligned}
{\left[\mathbf{x}<em i="i">{i}, \mathbf{E}</em>}\right] } &amp; =L_{i}\left(\left[\mathbf{x<em i-1="i-1">{i-1}, \mathbf{E}</em>\right]\right) &amp; i=1,2, \ldots, N \
\mathbf{y} &amp; =\operatorname{Head}\left(\mathbf{x}_{N}\right)
\end{aligned}
$$</p>
<p>where $\mathbf{x}<em i_1="i+1">{i} \in \mathbb{R}^{d}$ denote [CLS]'s embedding at $L</em>}$ 's input space. $[\cdot, \cdot]$ indicates stacking and concatenation on the sequence length dimension, i.e., $\left[\mathbf{x<em i="i">{i}, \mathbf{E}</em>$ consists of Multiheaded Self-Attention (MSA) and Feed-Forward Networks (FFN) together with LayerNorm [1] and residual con-}\right] \in$ $\mathbb{R}^{(1+m) \times d}$. Each layer $L_{i</p>
<p>nections [31]. A neural classification head is used to map the final layer's [CLS] embedding, $\mathbf{x}_{N}$, into a predicted class probability distribution $\mathbf{y} .^{2}$</p>
<h1>3.2 Visual-Prompt Tuning (VPT)</h1>
<p>Given a pre-trained Transformer model, we introduce a set of $p$ continuous embeddings of dimension $d$, i.e., prompts, in the input space after the Embed layer. Only the task-specific prompts are being updated during fine-tuning, while the Transformer backbone is kept frozen. Depending on the number of Transformer layers involved, our approach has two variants, VPT-SHallow and VPT-DEEP, as shown in Fig. 2.</p>
<p>VPT-Shallow. Prompts are inserted into the first Transformer layer $L_{1}$ only. Each prompt token is a learnable $d$-dimensional vector. A collection of $p$ prompts is denoted as $\mathbf{P}=\left{\mathbf{p}^{k} \in \mathbb{R}^{d} \mid k \in \mathbb{N}, 1 \leq k \leq p\right}$, the shallow-prompted ViT is:</p>
<p>$$
\begin{aligned}
{\left[\mathbf{x}<em 1="1">{1}, \mathbf{Z}</em>}, \mathbf{E<em 1="1">{1}\right] } &amp; =L</em>}\left(\left[\mathbf{x<em 0="0">{0}, \mathbf{P}, \mathbf{E}</em>\right]\right) \
{\left[\mathbf{x}<em i="i">{i}, \mathbf{Z}</em>}, \mathbf{E<em i="i">{i}\right] } &amp; =L</em>}\left(\left[\mathbf{x<em i-1="i-1">{i-1}, \mathbf{Z}</em>}, \mathbf{E<em N="N">{i-1}\right]\right) \
\mathbf{y} &amp; =\operatorname{Head}\left(\mathbf{x}</em>\right)
\end{aligned} \quad i=2,3, \ldots, N
$$</p>
<p>where $\mathbf{Z}<em i="i">{i} \in \mathbb{R}^{p \times d}$ represents the features computed by the $i$-th Transformer layer, and $\left[\mathbf{x}</em>}, \mathbf{Z<em i="i">{i}, \mathbf{E}</em>}\right] \in \mathbb{R}^{(1+p+m) \times d}$. The colors $\bullet$ and $\bullet$ indicate learnable and frozen parameters, respectively. Notably for ViT, $\mathbf{x<em 0="0">{N}$ is invariant to the location of prompts since they are inserted after positional encoding, e.g., $\left[\mathbf{x}</em>}, \mathbf{P}, \mathbf{E<em 0="0">{0}\right]$ and $\left[\mathbf{x}</em>\right]$ are mathematically equivalent. This also applies to VPT-Deep.}, \mathbf{E}_{0}, \mathbf{P</p>
<p>VPT-Deep. Prompts are introduced at every Transformer layer's input space. For $(i+1)$-th Layer $L_{i+1}$, we denote the collection of input learnable prompts as $\mathbf{P}<em i="i">{i}=\left{\mathbf{p}</em>, 1 \leq k \leq m\right}$. The deep-prompted ViT is formulated as:}^{k} \in \mathbb{R}^{d} \mid k \in \mathbb{N</p>
<p>$$
\begin{aligned}
{\left[\mathbf{x}<em i="i">{i}, \ldots, \mathbf{E}</em>}\right] } &amp; =L_{i}\left(\left[\mathbf{x<em i-1="i-1">{i-1}, \mathbf{P}</em>}, \mathbf{E<em N="N">{i-1}\right]\right) \
\mathbf{y} &amp; =\operatorname{Head}\left(\mathbf{x}</em>\right)
\end{aligned} \quad i=1,2, \ldots, N
$$</p>
<p>Storing Visual Prompts. VPT is beneficial in presence of multiple downstream tasks. We only need to store the learned prompts and classification head for each task and re-use the original copy of the pre-trained Transformer model, significantly reducing the storage cost. For instance, given a ViT-Base with 86 million (M) parameters and $d=768,50$ shallow prompts and deep prompts yield additional $p \times d=50 \times 768=0.038 \mathrm{M}$, and $N \times p \times d=0.46 \mathrm{M}$ parameters, amounting to only $0.04 \%$ and $0.53 \%$ of all ViT-Base parameters, respectively.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Experiments</h1>
<p>We evaluate VPT for a wide range of downstream recognition tasks with pretrained Transformer backbones across scales. We first describe our experimental setup in Sec. 4.1, including the pre-trained backbone and downstream tasks, and a brief introduction of alternative transfer learning methods. Then we demonstrate the effectiveness and practical utility of our method in Sec. 4.2. We also systematically study how different design choices would affect performance (Sec. 4.3), which leads to an improved understanding of our approach.</p>
<h3>4.1 Experiment Setup</h3>
<p>Pre-trained Backbones. We experiment with two Transformer architectures in vision, Vision Transformers (ViT) [19] and Swin Transformers (Swin [52]). All backbones in this section are pre-trained on ImageNet-21k [16]. We follow the original configurations, e.g., number of image patches divided, existence of [CLS], etc. More details are included in Appendix A.
Baselines. We compare both variants of VPT with other commonly used finetuning protocols:
(a) Full: fully update all backbone and classification head parameters.
(b) Methods that focus on the classification head. They treat the pre-trained backbone as a feature extractor, whose weights are fixed during tuning:</p>
<ul>
<li>Linear: only use a linear layer as the classification head.</li>
<li>Partial- $k$ : fine-tune the last $k$ layers of backbone while freezing the others, as adopted in $[85,88,60,30]$. It redefines the boundary of backbone and classification head.</li>
<li>MLP- $k$ : utilize a multilayer perceptron (MLP) with $k$ layers, instead of a linear layer, as classification head.
(c) Methods that update a subset backbone parameters or add new trainable parameters to backbone during fine-tuning:</li>
<li>Sidetune [87]: train a "side" network and linear interpolate between pretrained features and side-tuned features before being fed into the head.</li>
<li>Bias [8,5]: fine-tune only the bias terms of a pre-trained backbone.</li>
<li>Adapter [34,63,64]: insert new MLP modules with residual connection inside Transformer layers.
Downstream Tasks. We experiment on the following two collections of datasets:
$F G V C$ consists of 5 benchmarked Fine-Grained Visual Classification tasks including CUB-200-2011 [75], NABirds [72], Oxford Flowers [59], Stanford Dogs [41] and Stanford Cars [23]. If a certain dataset only has train and test sets publicly available, we randomly split the training set into train $(90 \%)$ and val $(10 \%)$, and rely on val to select hyperparameters.
$V T A B-1 k[86]$ is a collection of 19 diverse visual classification tasks, which are organized into three groups: Natural - tasks that contain natural images captured using standard cameras; Specialized - tasks that contain images captured via specialized equipment, such as medical and satellite imagery; and Structured - tasks that require geometric comprehension like object counting. Each task</li>
</ul>
<p>Table 1. ViT-B/16 pre-trained on supervised ImageNet-21k. For each method and each downstream task group, we report the average test accuracy score and number of wins in (.) compared to Full. "Total params" denotes total parameters needed for all 24 downstream tasks. "Scope" denotes the tuning scope of each method. "Extra params" denotes the presence of additional parameters besides the pre-trained backbone and linear head. Best results among all methods except Full are bolded. VPT outshines the full fine-tuning 20 out of 24 cases with significantly less trainable parameters</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ViT-B/16 <br> (85.8M)</th>
<th style="text-align: center;">Total <br> params</th>
<th style="text-align: center;">Scope <br> Input</th>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">Extra <br> params</th>
<th style="text-align: center;">FGVC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VTAB-1k <br> Specialized</th>
<th style="text-align: center;">Structured</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total # of tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">FULL</td>
<td style="text-align: center;">$24.02 \times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">88.54</td>
<td style="text-align: center;">75.88</td>
<td style="text-align: center;">83.36</td>
<td style="text-align: center;">47.64</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$1.02 \times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.32 (0)</td>
<td style="text-align: center;">68.93 (1)</td>
<td style="text-align: center;">77.16 (1)</td>
<td style="text-align: center;">26.84 (0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Partial-1</td>
<td style="text-align: center;">$3.00 \times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.63 (0)</td>
<td style="text-align: center;">69.44 (2)</td>
<td style="text-align: center;">78.53 (0)</td>
<td style="text-align: center;">34.17 (0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLP-3</td>
<td style="text-align: center;">$1.35 \times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">79.80 (0)</td>
<td style="text-align: center;">67.80 (2)</td>
<td style="text-align: center;">72.83 (0)</td>
<td style="text-align: center;">30.62 (0)</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">Sidetune</td>
<td style="text-align: center;">$3.69 \times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">78.35 (0)</td>
<td style="text-align: center;">58.21 (0)</td>
<td style="text-align: center;">68.12 (0)</td>
<td style="text-align: center;">23.41 (0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">$1.05 \times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">88.41 (3)</td>
<td style="text-align: center;">73.30 (3)</td>
<td style="text-align: center;">78.25 (0)</td>
<td style="text-align: center;">44.09 (2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adapter</td>
<td style="text-align: center;">$1.23 \times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">85.66 (2)</td>
<td style="text-align: center;">70.39 (4)</td>
<td style="text-align: center;">77.11 (0)</td>
<td style="text-align: center;">33.43 (0)</td>
</tr>
<tr>
<td style="text-align: center;">(ours)</td>
<td style="text-align: center;">VPT-shallow</td>
<td style="text-align: center;">$1.04 \times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">84.62 (1)</td>
<td style="text-align: center;">76.81 (4)</td>
<td style="text-align: center;">79.66 (0)</td>
<td style="text-align: center;">46.98 (4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VPT-deEP</td>
<td style="text-align: center;">$1.18 \times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.11 (4)</td>
<td style="text-align: center;">78.48 (6)</td>
<td style="text-align: center;">82.43 (2)</td>
<td style="text-align: center;">54.98 (8)</td>
</tr>
</tbody>
</table>
<p>of VTAB contains 1000 training examples. Following [86], we use the provided 800-200 split of the train set to determine hyperparameters and run the final evaluation using the full training data. We report the average accuracy score on test set within three runs.</p>
<p>We report the average accuracy on the FGVC datasets, and the average accuracy on each of the three groups in VTAB. The individual results on each task are in Appendix D, as are image examples of these aforementioned tasks.</p>
<h1>4.2 Main Results</h1>
<p>Tab. 1 presents the results of fine-tuning a pre-trained ViT-B/16 on averaged across 4 diverse downstream task groups, comparing VPT to the other 7 tuning protocols. We can see that:</p>
<ol>
<li>VPT-Deep outperforms Full (Tab. 1(a)) on 3 out of the 4 problem classes ( 20 out of 24 tasks), while using significantly fewer total model parameters $(1.18 \times$ vs. $24.02 \times)$. Thus, even if storage is not a concern, VPT is a promising approach for adapting larger Transformers in vision. Note that this result is in contrast to comparable studies in NLP, where prompt tuning matches, but does not exceed full fine-tuning [45].</li>
<li>VPT-Deep outperforms all the other parameter-efficient tuning protocols (Tab. 1(b,c)) across all task groups, indicating that VPTDEEP is the best fine-tuning strategy in storage-constrained environments.</li>
<li>Although sub-optimal than VPT-DEEP, VPT-SHALLOW still offers non-trivial performance gain than head-oriented tuning methods in Tab. 1(b), indicating that VPT-SHALLOW is a worthwhile choice in deploying multi-task fine-tuned models if the storage constraint is severe.</li>
</ol>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Performance comparison on different downstream data scales, averaged across 5 FGVC tasks. VPT-DEEP is compared with Linear (left), Adapter (middle) and Bias (right). Highlighted region shows the accuracy difference between VPT-DEEP and the compared method. Results of VPT-shallow are Full presented in all plots for easy reference. The size of markers are proportional to the percentage of tunable parameters in log scale
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. VPT vs. Full across model scales (ViT-B, ViT-L and ViT-H), for 3 VTAB task groups. Highlighted region shows the accuracy difference between VPT-DEEP and the full fine-tuning (Full). The size of markers are proportional to the percentage of trainable parameters in log scale</p>
<p>VPT on different downstream data size. We look at the impact of training data size on accuracy in the FGVC tasks (VTAB has only 1 k training examples). We vary the training data between $10 \%$ and $80 \%$ and compare all methods. The same pre-trained ViT-B is used for downstream training. Task-averaged results for each method on different training data scales are presented in Fig. 3.</p>
<p>Fig. 3 shows that VPT-DEEP outperforms all the other baselines across data scales. Digging deeper, methods that use less trainable parameters, i.e., VPT, Linear, Adapter, Bias, dominate over Full in the low-data regimes. This trend, however, is reversed when more training data is available for Linear and Adapter. In contrast, VPT-DEEP still consistently outperforms Fullacross training data sizes. Although Bias offers similar advantages, it still marginally under-performs VPT-DEEP across the board (Fig. 3 right).</p>
<p>VPT on different backbone scales. Fig. 4 shows VTAB-1k performance under 3 different backbone scales: ViT-Base/Large/Huge. VPT-DEEP is signif-</p>
<p>Table 2. Different Transformer architecture: Swin-B pre-trained on supervised ImageNet-21k as backbone. For each method and each downstream task group, we report the average test accuracy score and number of wins in (.) compared to Full. The column "Total params" denotes total parameters needed for all 19 downstream tasks. Best results among all methods except Full are bolded</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Swin-B <br> (86.7M)</th>
<th style="text-align: center;">Total <br> params</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VTAB-1k <br> Specialized</th>
<th style="text-align: center;">Structured</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total $#$ of tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">FULL</td>
<td style="text-align: center;">$19.01 \times$</td>
<td style="text-align: center;">79.10</td>
<td style="text-align: center;">86.21</td>
<td style="text-align: center;">59.65</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">$73.52(5)$</td>
<td style="text-align: center;">$80.77(0)$</td>
<td style="text-align: center;">$33.52(0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLP-3</td>
<td style="text-align: center;">$1.47 \times$</td>
<td style="text-align: center;">$73.56(5)$</td>
<td style="text-align: center;">$75.21(0)$</td>
<td style="text-align: center;">$35.69(0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">$3.77 \times$</td>
<td style="text-align: center;">$73.11(4)$</td>
<td style="text-align: center;">$81.70(0)$</td>
<td style="text-align: center;">$34.96(0)$</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">$1.06 \times$</td>
<td style="text-align: center;">$74.19(2)$</td>
<td style="text-align: center;">$80.14(0)$</td>
<td style="text-align: center;">$42.42(0)$</td>
</tr>
<tr>
<td style="text-align: center;">(ours)</td>
<td style="text-align: center;">VPT-SHAllow</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">79.85 (6)</td>
<td style="text-align: center;">$82.45(0)$</td>
<td style="text-align: center;">$37.75(0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VPT-DEEP</td>
<td style="text-align: center;">$1.05 \times$</td>
<td style="text-align: center;">$76.78(6)$</td>
<td style="text-align: center;">84.53 (0)</td>
<td style="text-align: center;">53.35 (0)</td>
</tr>
</tbody>
</table>
<p>icantly better than Linear and VPT-Shallow across all 3 backbone choices and 3 subgroups of VTAB-1k. More importantly, the advantages of VPT-DEEP over Full indeed still hold as the model scale increases, i.e., VPT-DEEP significantly outperforms Full on Natural and Structured groups, while offering nearly equivalent performance on Specialized.</p>
<p>VPT on hierarchical Transformers. We extend VPT to Swin [52], which employs MSA within local shifted windows and merges patch embeddings at deeper layers. For simplicity and without loss of generality, we implement VPT in the most straightforward manner: the prompts are attended within the local windows, but are ignored during patch merging stages. The experiments are conducted on the ImageNet-21k supervised pre-trained Swin-Base. VPT continues to outperform other parameter-efficient fine-tuning methods (b, c) for all three subgroups of VTAB Tab. 2, though in this case Full yields the highest accuracy scores overall (at a heavy cost in total parameters).</p>
<p>It is surprising that the advantage of VPT-DEEP over VPT-SHallow diminishes for Natural: VPT-SHallow yields slightly better accuracy scores than full fine-tuning.</p>
<h1>4.3 Ablation on Model Design Variants</h1>
<p>We ablate different model design choices on the supervised ImageNet-21k pretrained ViT-Base and evaluate them on VTAB, with same setup in Tab. 1. See more in Appendix B.</p>
<p>Prompt Location. An important distinction between VPT and other methods is the extra learnable parameters introduced as inputs for the Transformer layers. Fig. 5 ablates different choices on how and where to insert prompts in the input space, and how they would affect the final performance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Ablation on prompt location. We illustrate different location choices at top, and present the results at bottom. For easy comparison, two blue dashed lines represent the performance of the default VPT-DEEP and VPT-SHALLOW respectively</p>
<p>Prepend or Add? Instead of prepending prompts to the sequence of the image patches embeddings $\mathbf{E}_{i}$ as described in Sec. 3.2, another option is to directly add prompts element-wise to those embeddings, keeping the Transformer's input sequence length the same as before. Though this variant is competitive to Full in some cases (e.g., VTAB-Natural), its performance generally falls behind with the default Prepend in both deep and shallow settings. More discussion on this phenomenon is in Appendix B.</p>
<p>Latent or pixel space? Instead of inserting the prompts as latent vectors for the first Transformer layer, one could introduce prompts in the pixel level before the Embed layer in Eq. (1), i.e., Prepend-pixel and Concat-channel. Fig. 5 shows that the adaption performance decreases for these two variants. For example, the accuracy score of prepending shallow prompts before the projection layer (Prepend-pixel) drops $6.9 \%$, compared to the default prepending in the embedding space (Prepend) on VTAB-Natural. The performance further deteriorates (even as large as 30 accuracy scores drop on VTAB-Natural) if we instead concatenate a new channel to the input image (Concat-channel). These observations suggest that it's easier for prompts to learn condensed task-dependent signals in the latent input space of Transformers.</p>
<p>Prompt Length. This is the only additional hyper-parameter needed to tune for VPT compared to full fine-tuning. For easy reference, we also ablate two other baselines on their individual additional hyper-parameters, i.e., number of layers for MLP and reduction rate for Adapter. As shown in Fig. 6, the optimal prompt length varies across tasks. Notably, even with as few as only one prompt, VPT-DEEP still significantly outperforms the other 2 baselines, and remains competitive or even better compared to full fine-tuning on VTABStructured and Natural.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Ablation on prompt length. We vary the number of prompts for VPT-DEEP and show the averaged results for each VTAB subgroup. The averaged best VPT-DEEP results for each task is also shown for easy reference
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Ablation on prompt depth. We select the best prompt length for each variant with val sets. $i \rightarrow j$ indicates the Transformer layer indices that prompts are inserted into. The 1-st layer refers to the one closest to input. ViT-B has 12 layers in total</p>
<p>Prompt Depth. Fig. 7 ablates which and how many layers to insert prompts. Each variant reports the best prompt length selected with val set. VPT's performance is positively correlated with the prompt depth in general. Yet the accuracy drops if we insert prompts from top to bottom, suggesting that prompts at earlier Transformer layers matter more than those at later layers.</p>
<p>Final Output. Following the original configuration of ViT, we use the final embedding of [CLS], i.e., $\mathbf{x}<em N="N">{N}$, as the classification head input, which is also the default setting in our ViT experiments. As shown in Fig. 8, if we use the average pooling on image patch output embeddings $\mathbf{E}</em>$ (Prompt-pool and Global-pool), the accuracy could drop as large as 8 points.}$ as final output (Image-pool), the results essentially remain the same (e.g., 82.4 vs. 82.3 for VTAB-Specialized). However, if the pooling involves final prompt outputs $\mathbf{Z}_{N</p>
<h1>5 Analysis and Discussion</h1>
<p>Visualization. Fig. 9 shows t-SNE [55] visualizations of $\mathbf{x}_{\mathbf{N}}$, i.e., embeddings of [CLS] after the last Transformer layer and before the classification head, for 3 tasks in VTAB (SVNH [58], EuroSAT [32], Clevr/count [38]), one for each</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Ablation on final output. Illustration of different strategies is included at top, and results of those are presented at the bottom section. For easy comparison, the blue dashed line represents the performance of default VPT-DEEP
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. t-SNE visualizations of the final [CLS] embedding $\mathbf{x}_{N}$ of 3 VTAB tasks from the test set, from Tab. 1. VPT could produce linearly separable features without updating backbone parameters
subgroup. All plots show that VPT-DEEP enables linearly separable representations while using less parameters than Full. We also observe that extra tunable parameters for every Transformer layer (VPT-DEEP) improve the performance, compared to VPT-SHALlow, which only inserts prompts for the first layer's input. Interestingly on Clevr/count (Fig. 9(c)), VPT-DEEP and Full recover the underlying manifold structure of the task (counting objects in images $v s$. street number or landscape recognition), unlike VPT-SHALlow and LINEAR.</p>
<p>Apply VPT to more vision tasks. We explore the feasibility of VPT beyond visual classification, by evaluating ADE20K [90] semantic segmentation task with a Transformer model, SETR-PUP [89]. It adds a standard ConvNet head to the ViT backbone to perform segmentation. The de-facto approach is still fully fine-tuning the pre-trained backbone together with the ConvNet head (Full). We include two more protocols for comparison: only update the head layers</p>
<p>Table 3. Semantic Segmentation: ADE20k [90] validation results with SETR [89] on ViT-L. The best mIoU scores among all methods but Full are bolded. Results of fully fine-tuning a ResNet-101 [10] are included. SS/MS: single/multi-scale inference</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">ViT-L/16</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ResNet-101</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Full [89]</td>
<td style="text-align: center;">Head Only</td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">VPT-deep</td>
<td style="text-align: center;">VPT+Bias</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full [10]</td>
</tr>
<tr>
<td style="text-align: center;">mIoU-SS</td>
<td style="text-align: center;">48.31</td>
<td style="text-align: center;">35.12</td>
<td style="text-align: center;">43.40</td>
<td style="text-align: center;">42.11</td>
<td style="text-align: center;">44.04</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.47</td>
</tr>
<tr>
<td style="text-align: center;">mIoU-MS</td>
<td style="text-align: center;">50.07</td>
<td style="text-align: center;">37.46</td>
<td style="text-align: center;">45.33</td>
<td style="text-align: center;">44.06</td>
<td style="text-align: center;">45.63</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.27</td>
</tr>
<tr>
<td style="text-align: center;">Tunable params (M)</td>
<td style="text-align: center;">318.31</td>
<td style="text-align: center;">13.18</td>
<td style="text-align: center;">13.46</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">15.79</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.0</td>
</tr>
</tbody>
</table>
<p>Table 4. Different pre-trained objectives: MAE [30] and MoCo v3 [11] with a ViT-B backbone. For each method and each downstream task group, we report the average test accuracy score and number of wins in (.) compared to Full. "Total params" denotes total parameters needed for all 24 downstream tasks. Best results among all methods except Full are bolded</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MAE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MoCo v3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViT-B/16 <br> (85.8M)</td>
<td style="text-align: center;">Total <br> params</td>
<td style="text-align: center;">VTAB-1k <br> Specialised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">Total <br> params</td>
<td style="text-align: center;">VTAB-1k <br>  <br> Specialised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Structured</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total $#$ of tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">$19.01 \times$</td>
<td style="text-align: center;">59.29</td>
<td style="text-align: center;">79.68</td>
<td style="text-align: center;">53.82</td>
<td style="text-align: center;">$19.01 \times$</td>
<td style="text-align: center;">71.95</td>
<td style="text-align: center;">84.72</td>
<td style="text-align: center;">51.98</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">18.87 (0)</td>
<td style="text-align: center;">53.72 (0)</td>
<td style="text-align: center;">23.70 (0)</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">67.46 (4)</td>
<td style="text-align: center;">81.08 (0)</td>
<td style="text-align: center;">30.33 (0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Partial-1</td>
<td style="text-align: center;">$2.58 \times$</td>
<td style="text-align: center;">58.44 (5)</td>
<td style="text-align: center;">78.28 (1)</td>
<td style="text-align: center;">47.64 (1)</td>
<td style="text-align: center;">$2.58 \times$</td>
<td style="text-align: center;">72.31 (5)</td>
<td style="text-align: center;">84.58 (2)</td>
<td style="text-align: center;">47.89 (1)</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">$1.03 \times$</td>
<td style="text-align: center;">54.55 (1)</td>
<td style="text-align: center;">75.68 (1)</td>
<td style="text-align: center;">47.70 (0)</td>
<td style="text-align: center;">$1.03 \times$</td>
<td style="text-align: center;">72.89 (3)</td>
<td style="text-align: center;">81.14 (0)</td>
<td style="text-align: center;">53.43 (4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adapter</td>
<td style="text-align: center;">$1.17 \times$</td>
<td style="text-align: center;">54.90 (3)</td>
<td style="text-align: center;">75.19 (1)</td>
<td style="text-align: center;">38.98 (0)</td>
<td style="text-align: center;">$1.22 \times$</td>
<td style="text-align: center;">74.19 (4)</td>
<td style="text-align: center;">82.66 (1)</td>
<td style="text-align: center;">47.69 (2)</td>
</tr>
<tr>
<td style="text-align: center;">(ours)</td>
<td style="text-align: center;">VPT-shallow</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">39.96 (1)</td>
<td style="text-align: center;">69.65 (0)</td>
<td style="text-align: center;">27.50 (0)</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">67.34 (3)</td>
<td style="text-align: center;">82.26 (0)</td>
<td style="text-align: center;">37.55 (0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VPT-deep</td>
<td style="text-align: center;">$1.04 \times$</td>
<td style="text-align: center;">36.02 (0)</td>
<td style="text-align: center;">60.61 (1)</td>
<td style="text-align: center;">26.57 (0)</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">70.27 (4)</td>
<td style="text-align: center;">83.04 (0)</td>
<td style="text-align: center;">42.38 (0)</td>
</tr>
</tbody>
</table>
<p>(Head Only), update head layers and bias vectors in the backbone (Bias). In Tab. 3, we report val mIoU results with and without multi-scale inference. Though parameter-efficient protocols could not compete with Full, VPT is still comparable with Bias. Notably, VPT offers competitive results to a fully fine-tuned state-of-the-art ConvNet model (DeepLab v3+ [10]), while tuning significantly less parameters ( 15 M vs. 64 M , respectively).</p>
<p>Apply VPT to more pre-training methods. In addition to the backbones pre-trained with labeled data, we experiment with two self-supervised objectives: MAE [30] and MoCo v3 [11]. Tab. 4 reports the results on VTAB-1k with ViTB. We observe that both variants of VPT surpass Linear, yet the comparisons among other techniques are less conclusive. For MAE, other parameter-efficient methods, e.g., Partial-1, outperform both VPT and Linear. In the case of MoCo v3, VPT no longer holds the best performance, though it is still competitive with the others. This suggests that these two self-supervised ViTs are fundamentally different from the supervised ones in previous sections. Exactly why and how these differences arise remain open questions.</p>
<p>Apply VPT to ConvNets. We examine the idea of adding trainable parameters in the input space of ConvNets: padding both height and width by $p$</p>
<p>Table 5. Apply VPT to ConvNets: ResNet-50 and ConvNeXt-Base. For each method and each downstream task group, we report the average test accuracy score and number of wins in (.) compared to Full. "Total params" denotes total parameters needed for all 19 downstream tasks. Best results among all methods except Full are bolded</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ConvNeXt-Base (87.6M)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ResNet-50 (23.5M)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total <br> params</td>
<td style="text-align: center;">VTAB-1k <br> N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total <br> params</td>
<td style="text-align: center;">VTAB-1k <br> N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total $#$ of tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">$19.01 \times$</td>
<td style="text-align: center;">77.97</td>
<td style="text-align: center;">83.71</td>
<td style="text-align: center;">60.41</td>
<td style="text-align: center;">$19.08 \times$</td>
<td style="text-align: center;">59.72</td>
<td style="text-align: center;">76.66</td>
<td style="text-align: center;">54.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$1.01 \times$</td>
<td style="text-align: center;">74.48 (5)</td>
<td style="text-align: center;">81.50 (0)</td>
<td style="text-align: center;">34.76 (1)</td>
<td style="text-align: center;">$1.08 \times$</td>
<td style="text-align: center;">63.75 (6)</td>
<td style="text-align: center;">77.60 (3)</td>
<td style="text-align: center;">30.96 (0)</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">Partial-1</td>
<td style="text-align: center;">$2.84 \times$</td>
<td style="text-align: center;">73.76 (4)</td>
<td style="text-align: center;">81.64 (0)</td>
<td style="text-align: center;">39.55 (0)</td>
<td style="text-align: center;">$4.69 \times$</td>
<td style="text-align: center;">64.34 (6)</td>
<td style="text-align: center;">78.64 (2)</td>
<td style="text-align: center;">45.78 (1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLP-3</td>
<td style="text-align: center;">$1.47 \times$</td>
<td style="text-align: center;">73.78 (5)</td>
<td style="text-align: center;">81.36 (1)</td>
<td style="text-align: center;">35.68 (1)</td>
<td style="text-align: center;">$7.87 \times$</td>
<td style="text-align: center;">61.79 (6)</td>
<td style="text-align: center;">70.77 (1)</td>
<td style="text-align: center;">33.97 (0)</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">$1.04 \times$</td>
<td style="text-align: center;">69.07 (2)</td>
<td style="text-align: center;">72.81 (0)</td>
<td style="text-align: center;">25.29 (0)</td>
<td style="text-align: center;">$1.10 \times$</td>
<td style="text-align: center;">63.51 (6)</td>
<td style="text-align: center;">77.22 (2)</td>
<td style="text-align: center;">33.39 (0)</td>
</tr>
<tr>
<td style="text-align: center;">(ours)</td>
<td style="text-align: center;">Visual-Prompt Tuning</td>
<td style="text-align: center;">$1.02 \times$</td>
<td style="text-align: center;">78.48 (6)</td>
<td style="text-align: center;">83.00 (1)</td>
<td style="text-align: center;">44.64 (1)</td>
<td style="text-align: center;">$1.09 \times$</td>
<td style="text-align: center;">66.25 (6)</td>
<td style="text-align: center;">77.32 (2)</td>
<td style="text-align: center;">37.52 (0)</td>
</tr>
</tbody>
</table>
<p>learnable prompt pixels for the input image. Though this operation seems unconventional, we implement VPT this way given there is no obvious solution to add location-invariant prompts similar to the Transformer counterparts. In fact this approach has been explored before in the adversarial attack literature [20]. The value of $p$ in our experiment is 2 orders of magnitude smaller than previous work: e.g., 5 vs. 263 . Most importantly, we cast this idea in the lens of transfer learning. See Appendix C for more discussion.</p>
<p>Tab. 5 presents the results for ConvNeXt-B [53] (pre-trained on ImageNet21k) and ResNet-50 [31] (pre-trained on ImageNet-1k), respectively. VPT works well in a larger ConvNet backbone, ConvNeXt-B, offering accuracy gains over other sparse tuning protocols (b, c), and outperforming Full on 8 out of 19 cases. The advantages of VPT, however, diminish with smaller ConvNet (ResNet50 ), as there is no clear winner for all 19 VTAB-1k tasks.</p>
<h1>6 Conclusion</h1>
<p>We present Visual Prompt Tuning, a new parameter-efficient approach to leverage large vision Transformer models for a wide range of downstream tasks. VPT introduces task-specific learnable prompts in the input space, keeping the pretrained backbone fixed. We show that VPT can surpass other fine-tuning protocols (often including full fine-tuning) while dramatically reducing the storage cost. Our experiments also raise intriguing questions on fine-tuning dynamics of vision Transformers with different pre-training objectives, and how to transfer to broader vision recognition tasks in an efficient manner. We therefore hope our work will inspire future research on how best to tap the potential of large foundation models in vision.
Acknowledgement. Menglin is supported by a Meta AI research grant awarded to Cornell University, Luming and Bharath is supported by NSF IIS-2144117, Serge is supported in part by the Pioneer Centre for AI, DNRF grant number P1. We would like to thank Alexander Rush, Yin Cui for valuable suggestions and discussion.</p>
<h1>A Implementation Details</h1>
<p>We use PyTorch [62] to implement all experiments on NVIDIA A100-40GB GPUs.</p>
<h2>A. 1 Classification Experiments</h2>
<p>VPT. We use val set of each dataset to find best prompt length $p$, see Sec. 3.2. The prompt length is the only VPT-specific hyper-parameter that we tune. For Transformer backbones, the range of $p$ is ${1,5,10,50,100,200}$ and ${1,5,10,50}$ for ViT and Swin, respectively. The maximum choice of $p$ is approximately close to the number of image patch tokens within each MSA for both architectures (ViT: 196, Swin: 49). We also apply a dropout of 0.1 for VPT-DEEP. For ConvNets, the range of $p$ is ${1,3,5,7,9,11}$. Each prompt is randomly initialized with xavier uniform initialization scheme [26]. We follow the original backbone' design choices, such as the existence of the classification tokens [CLS], or whether or not to use the final [CLS] embeddings for the classification head input.</p>
<p>Adapter. Adapters [34] insert extra lightweight modules inside each Transformer layer. One adapter module generally consists of a linear down-projection (with a reduction rate $r$ ), followed by a nonlinear activation function, and a linear up-projection, together with a residual connection. [63,64] exhaustively searched all possible configurations and found that only inserting adapters after the FFN "Add \&amp; LayerNorm" sub-layer works the best. Therefore we also use this setup in our own implementation. We sweep the reduction rate $r$ in ${8,64,256}$.</p>
<p>Augmentation and other hyper-parameters. We adopt standard image augmentation strategy during training: normalize with ImageNet means and standard deviation, randomly resize crop to $224 \times 224$ and random horizontal flip for five FGVC datasets, and resize to $224 \times 224$ for the VTAB-1k suite. ${ }^{3}$ Tab. 6 summarizes the optimization configurations we used. Following [56], we conduct grid search to find the tuning-specific hyper-parameters, learning rate, and weight decay values using val set of each task. Following the linear scaling rule [42,27,11,30], the learning rate is set as</p>
<p>Table 6. Implementation details for each fine-tuning method evaluated. $\star$ : we observe that for VPT-SHallow sometimes benefit from a larger base LR for 6 out of 24 tasks evaluated, where we search from ${1000.0,500.0,250.0,100.0}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Full,Partial, Bias,Adapter</th>
<th style="text-align: center;">Linear,Sidetune, MLP, VPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW [54]</td>
<td style="text-align: center;">SGD</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer momentum</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">base_lr range</td>
<td style="text-align: center;">${0.001,0.0001,0.0005,0.005}$</td>
<td style="text-align: center;">${50 ., 25 ., 10 ., 5 ., 2.5,1 ., 0.5,0.25,0.1,0.05}^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay range</td>
<td style="text-align: center;">${0.01,0.001,0.0001,0.0}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning rate schedule</td>
<td style="text-align: center;">cosine decay</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Warm up epochs</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Total epochs</td>
<td style="text-align: center;">$100(\mathrm{ViT}-\mathrm{B}, \mathrm{Swin}-\mathrm{B}), 50(\mathrm{ViT}-\mathrm{L} / \mathrm{H})$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 7. Specifications of the various datasets evaluated. *: we randomly sampled the train and val sets since there are no public splits available</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;"># Classes</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Val</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-grained visual recognition tasks (FGVC)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CUB-200-2011 [75]</td>
<td style="text-align: center;">Fine-grained bird species recognition</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">$5,394^{*}$</td>
<td style="text-align: center;">$600^{*}$</td>
<td style="text-align: center;">5,794</td>
</tr>
<tr>
<td style="text-align: center;">NABirds [72]</td>
<td style="text-align: center;">Fine-grained bird species recognition</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">$21,536^{*}$</td>
<td style="text-align: center;">$2,393^{*}$</td>
<td style="text-align: center;">24,633</td>
</tr>
<tr>
<td style="text-align: center;">Oxford Flowers [59]</td>
<td style="text-align: center;">Fine-grained flower species recognition</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">1,020</td>
<td style="text-align: center;">1,020</td>
<td style="text-align: center;">6,149</td>
</tr>
<tr>
<td style="text-align: center;">Stanford Dogs [41]</td>
<td style="text-align: center;">Fine-grained dog species recognition</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">$10,800^{*}$</td>
<td style="text-align: center;">$1,200^{*}$</td>
<td style="text-align: center;">8,580</td>
</tr>
<tr>
<td style="text-align: center;">Stanford Cars [23]</td>
<td style="text-align: center;">Fine-grained car classification</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">$7,329^{*}$</td>
<td style="text-align: center;">$815^{*}$</td>
<td style="text-align: center;">8,041</td>
</tr>
<tr>
<td style="text-align: center;">Visual Task Adaptation Benchmark (VTAB-1k) [86]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CIFAR-100 [43]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10,000</td>
</tr>
<tr>
<td style="text-align: center;">Caltech101 [47]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">102</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6,084</td>
</tr>
<tr>
<td style="text-align: center;">DTD [13]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,880</td>
</tr>
<tr>
<td style="text-align: center;">Flowers102 [59]</td>
<td style="text-align: center;">Natural</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">800/1000</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">6,149</td>
</tr>
<tr>
<td style="text-align: center;">Pets [61]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3,669</td>
</tr>
<tr>
<td style="text-align: center;">SVHN [58]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26,032</td>
</tr>
<tr>
<td style="text-align: center;">Sun397 [83]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">397</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21,750</td>
</tr>
<tr>
<td style="text-align: center;">Patch Camelyon [74]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32,768</td>
</tr>
<tr>
<td style="text-align: center;">EuroSAT [33]</td>
<td style="text-align: center;">Specialized</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">800/1000</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">5,400</td>
</tr>
<tr>
<td style="text-align: center;">Resisc45 [12]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6,300</td>
</tr>
<tr>
<td style="text-align: center;">Retinopathy [40]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">42,670</td>
</tr>
<tr>
<td style="text-align: center;">Clevr/count [38]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15,000</td>
</tr>
<tr>
<td style="text-align: center;">Clevr/distance [38]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15,000</td>
</tr>
<tr>
<td style="text-align: center;">DMLab [4]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22,735</td>
</tr>
<tr>
<td style="text-align: center;">KITTI/distance [24]</td>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">711</td>
</tr>
<tr>
<td style="text-align: center;">dSprites/location [57]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73,728</td>
</tr>
<tr>
<td style="text-align: center;">dSprites/orientation [57]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73,728</td>
</tr>
<tr>
<td style="text-align: center;">SmallNORB/azimuth [44]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12,150</td>
</tr>
<tr>
<td style="text-align: center;">SmallNORB/elevation [44]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12,150</td>
</tr>
</tbody>
</table>
<p>Table 8. Specifications of different pre-trained backbones used in the paper. Parameters (M) are of the feature extractor. "Batch size" column reports the batch size for Linear / Partial / {Full, Bias, Adapter} / VPT ( $p&lt;100$ ) / VPT ( $p \geq 100$ ). All backbones are pre-trained on ImageNet [16] with resolution $224 \times 224$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">Pre-trained <br> Objective</th>
<th style="text-align: center;">Pre-trained <br> Dataset</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { # params } \ &amp; \text { (M) } \end{aligned}$</th>
<th style="text-align: center;">Feature dim $d$</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Pre-trained <br> Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ViT-B/16 [19]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">2048 / 1280 / 128 / 128 / 64</td>
<td style="text-align: center;">checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ViT-L/16 [19]</td>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">ImageNet-21k</td>
<td style="text-align: center;">307</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">2048 / 640 / 64 / 64 / 32</td>
<td style="text-align: center;">checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ViT-H/14 [19]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">630</td>
<td style="text-align: center;">1280</td>
<td style="text-align: center;">1024 / 240 / 28 / 28 / 14</td>
<td style="text-align: center;">checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ViT-B/16 [19]</td>
<td style="text-align: center;">MoCo v3 [11]</td>
<td style="text-align: center;">ImageNet-1k</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">2048 / 1280 / 128 / 128 / 64</td>
<td style="text-align: center;">checkpoint <br> checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ViT-B/16 [19]</td>
<td style="text-align: center;">MAE [30]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Swin-B [52]</td>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">ImageNet-21k</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">1024 / 1024 / 128 / 80 / -</td>
<td style="text-align: center;">checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ConvNeXt-Base [53]</td>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">ImageNet-21k</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">1024 / 1024 / 128 / 128 / -</td>
<td style="text-align: center;">checkpoint</td>
</tr>
<tr>
<td style="text-align: center;">ResNet-50 [31]</td>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">ImageNet-1k</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">2048 / 2048 / 384 / 256 / -</td>
<td style="text-align: center;">checkpoint</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Dataset examples for all classification tasks evaluated</p>
<p>base_lr×b/256, where b is the batch size used for the particular model, and base_lr is chosen from the range specified in Tab. 6. The optimal hyper-parameter values for each experiment can be found in Appendix D.</p>
<p>Datasets and pre-trained backbones specifications. Tabs. 7 and 8 summarize the statistics and details of the evaluated classification datasets and all the pre-trained backbones used in the paper. Fig. 10 includes image examples of all 24 classification tasks evaluated.</p>
<h1>A. 2 Semantic Segmentation Experiments</h1>
<p>ADE20K [90] is a challenging scene parsing benchmark with 150 fine-grained labels. The training and validation sets contain 20,210 and 2,000 images respectively. We utilize the public codebase MMSegmentation [15] in our implementation. ${ }^{4}$ The ViT-L backbone is supervisely pre-trained on ImageNet-21k. ${ }^{5}$</p>
<p>SETR [89] is a competitive segmentation framework using ViT as the encoder. PUP is a progressive upsampling strategy consisting of consecutive convolution layers and bilinear upsampling operations. Among multiple decoder choices, PUP works the best according to MMSegmentation's reproduction therefore we also use it as in our implementation. ${ }^{6}$</p>
<p>When applying VPT to SETR-PUP, we only insert prompts into SETR's ViT encoder backbone. For the decoder, only image patch embeddings are used as inputs and prompt embeddings are discarded. Same as recognition tasks, only the PUP decoder head and prompts are learned during training and the ViT backbone is frozen.</p>
<p>For full fine-tuning, we use the same hyper-parameters as in MMSegmentation. For HeadOnly, Bias, and VPT, we use the hyper-parameter sweep on learning rate ${0.05$, $0.005,0.0005,0.001}$. The optimal learning rate is 0.005 for all methods. We sweep prompt length $p \in{1,5,10,50,100,200}$. For VPT, we also change the learning rate multiplier to 1.0 instead of the default 10.0 , so the decoder head and prompts share the same learning rate. Other hyper-parameters remain the same as full fine-tuning.</p>
<h2>B Extended Analysis</h2>
<p>Effect of expanding input sequence length. As shown in Tab. 1, by expanding the input sequence with learnable prompts, VPT achieves better performance than Full on the 20 out of 24 tasks evaluated. To investigate whether the advantage of VPT is due to its enlarged input sequence length, we experiment on two more variants: (1) the prompts are kept frozen during fine-tuning stage (Prompt-Fixed). (2) only tuning the [CLS] token ([CLS]-Learned). From Fig. 11 we can see that, updating prompt embeddings (Prompt-Learned) offers significant gains, while Prompt-Fixed yields comparable results w.r.t. Linear. This suggests that the final performance of VPT is mainly contributed by the learned prompt embeddings instead of the enlarged sequence length. Updating the [CLS] token performs similarly as updating 1 prompt ([CLS] vs. Learned $_{p=1}$ ), but still lags behind the default setting where we manually select the best number of prompt tokens based on the val set.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. Effect of expanding input sequence. Illustration of different strategies is included at top, and results of those are presented at the bottom section. For easy comparison, two dark and light blue lines represent the performance of default VPT-DEEP and VPT-SHALlow, respectively</p>
<p>Sharing prompts. We examine the effect of sharing parameters of prompts in Fig. 12 by setting the same prompt embedding within Transformer layers (Shared-intra), among all layers (Shared-inter), and for all prompts inserted in the Transformer (Shared-all). We can observe that: (1) Sharing prompts within layer (Shared-intra) performs competitively or slightly outperforms the performance of using one prompt (Default ${ }_{p=1}$ ), further demonstrating the value of expanding input sequence. (2) Although Shared-intra under-performs Default in general, surprisingly, Shared-inter slightly outperforms our default VPT-DEEP while using similar number of trainable parameters (total number of parameters for all VTAB tasks: $1.14 \times$ vs. $1.13 \times$ for Shared-inter vs. Default, respectively). Closer examination reveals that the optimal prompt length $p$ for Shared-inter is in general larger than Default, i.e., average prompt length on all VTAB tasks: 64.58 vs. 60.94 , for Shared-inter vs. Default, respectively. (3) Sharing the same prompt embedding both among and within layers (Shared-all) deteriorates performance, but still surpass the linear probing results across three VTAB subgroups.</p>
<p>Prompt initialization. In NLP, prompt tuning could benefit from more sophisticated prompt initialization, as shown in [45]. We investigate if this is the case for visual prompting as well. We utilize prototype representations for downstream target classes so that the prompts are initialized with embeddings that enumerate the output space. Since we want the model to produce an output embedding that is close to one of these prototype representations given a test example, initializing prompts in this manner might give the model some hints about the target categories thus help improve the optimization process.</p>
<p>Concretely, we use the averaged final [CLS] embeddings whithin each target class of the down-stream dataset train split. Given the pre-trained ViT with $N$ layers, and the down-stream train set with $c$ target classes, for each training example, we compute the final [CLS] embeddings, $\mathbf{x}_{N} \in \mathbb{R}^{d}$. Then we average these embeddings within each</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12. Effect of sharing prompts. Illustration of different strategies is included at top, and results of those are presented at the bottom section. For easy comparison, the blue dashed line represents the performance of default VPT-DEEP
target class to get $\left{\hat{\mathbf{x}}<em N="N">{N}^{k} \in \mathbb{R}^{d} \mid k \in \mathbb{N}, 1 \leq k \leq c\right}{ }^{7}$ Setting prompt length $p=c,{ }^{8}$ we initialize $\mathbf{P}$ with $\left{\hat{\mathbf{x}}</em>\right}}^{k<em i="i">{k=1}^{k=c}$ for VPT-SHallow, and initialize each $\mathbf{P}</em>}$ with $\left{\hat{\mathbf{x}<em k="1">{N}^{k}\right}</em>$, where $i=0,1, \ldots, N-1$, for VPT-DEEP.}^{k=c</p>
<p>We compare the fine-tuning performance using the above initialization strategy (CLS) against the default random initialization (Random) in Fig. 13. We also report results when we fix the prompts during the fine-tuning stage ( $\cdot$-fixed). As shown in Fig. 13, it's quite surprising that our default random initialization (Random) works the best in general, consistently across different subgroups of VTAB without extra pre-processing steps described above (CLS). CLS works comparably in Natural and Specialized subgroups. ${ }^{9}$</p>
<p>Prompt depth vs. prompt length. In Fig. 7, we ablate the number of layers we insert prompts in. For each prompt depth variant, Fig. 7 reports the results using the best prompt length for each task (" $\rightarrow \cdot$ (best)" in Fig. 14). Here we adopt another setting where the best prompt length from $1 \rightarrow 12$ are used for all other prompt depth variants. Comparing both " $\rightarrow \cdot$ (best)" and " $\rightarrow$ ", we observe that there are varied sensitivities to prompt length for different depths, especially if we insert prompts in nine layers only $(3 \rightarrow 12,12 \rightarrow 3)$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13. Effect of prompt initialization. For easy comparison, the two blue dashed line represents the performance of default VPT-DEEP and VPT-SHALlow, respectively
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14. Sensitivity to prompt length for the prompt depth experiments. We select the best prompt length for each variant with val sets. We also include the same prompt length for all depth choices. $i \rightarrow j$ indicates the Transformer layer indices that prompts are inserted into. The 1-st layer refers to the one closest to input. ViT-B has a total of 12 layers</p>
<p>Combine VPT with Bias Tuning. Our experiments in the main paper reveal that Bias is a competitive parameter-efficient tuning baseline (e.g., Tab. 1 (c)). Based on this observation, we explore another protocol where we update both prompts and the bias terms of the pre-trained backbone, keeping everything else in the backbone frozen (VPT+BIAS). As shown in Tab. 9, to our surprise, incorporating BIAS with VPT does not yield superior results in general, even undermines VPT-DEEP for all 3 task subgroups. This suggests that these two methods are not necessarily complementary to each other.</p>
<p>Prompt ensembling. [45] demonstrated prompt's efficiency in the context of model ensembling. For an ensemble of $k$ models, we only need to store the learnt prompt vectors instead of $k$ copies of the whole fine-tuned model parameters (e.g., $k \times 2.5 \mathrm{~GB}$ for ViT-H). Furthermore, given one test example during inference time, only one forward pass is executed with a specially-designed batch with replicated original data but varied prompts.</p>
<p>Given such advantages, we also investigate VPT's effectiveness on prompt ensembling. We train 5 different prompts for each VTAB task with different random seeds,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ if $c&gt;200$, we further apply $k$-means $(k=200)$ to class-averaged embeddings and use the corresponding 200 centroid embeddings as $\left{\hat{\mathbf{x}}<em k="1">{N}^{k} \in \mathbb{R}^{d}\right}</em>$.
${ }^{8}$ if $c&gt;200$, we set $p=200$ so that prompt length won't be too large. In fact, for VTAB, only the Sun397 task in the Natural subgroup has over 200 classes. See Tab. 7.
${ }^{9}$ Utilizing the per-class averaged [CLS] features, we also tried several other different implementation variants, including using per-layer [CLS] embeddings for VPT-DEEP instead of only the final output [CLS] vector. They perform either the same as or even much worse than the CLS strategy above, and none of them is able to out-perform the default Random.&#160;}^{k=200<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
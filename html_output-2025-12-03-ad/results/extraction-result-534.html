<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-534 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-534</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-534</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424" target="_blank">TEACh: Task-driven Embodied Agents that Chat</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation, is introduced and initial models' abilities in dialogue understanding, language grounding, and task execution are evaluated.</p>
                <p><strong>Paper Abstract:</strong> Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from "Make Coffee" to "Prepare Breakfast", asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e534.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e534.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E.T. (EDH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic Transformer (adapted for TEACh) - EDH evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted Episodic Transformer model that fuses language and visual embeddings to predict action sequences for Execution from Dialogue History (EDH) instances in the TEACh benchmark; uses ResNet-50 visual backbone, Mask R-CNN for segmentation, and multimodal transformer layers with a learned action prediction head.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic Transformer for Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Episodic Transformer (E.T.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer language encoder + ResNet-50 visual backbone; two multimodal transformer layers fuse language, image, and action embeddings; Mask R-CNN pretrained on ALFRED to produce segmentation masks used to select interaction targets; adapted action-head to match TEACh action space; trained with cross-entropy on ground-truth action sequences, with options to initialize from ALFRED weights or ALFRED+synthetic language.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Execution from Dialogue History (EDH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an EDH instance (initial simulator state S^E, an action history A_H including dialogue and past environment actions, and reference interaction actions A_R^I), the agent must predict a sequence of actions that transforms the environment to the target state F^E in AI2-THOR. Actions include discrete navigation (Forward, Turn, Strafe, Look) and object interactions (Pickup, Place, Open, Slice, Pour, ToggleOn/Off), and interaction targets are specified as relative screen coordinates resolved via segmentation masks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>household tasks; navigation + object manipulation; multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on TEACh EDH instances (supervised learning); optional initialization from ALFRED pretrained weights and ALFRED synthetic-language augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning on EDH instances (cross-entropy over action sequences); ablations include removing history loss or initializing from ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>multimodal implicit representations in transformer weights: concatenated dialogue as text tokens, CNN image embeddings (ResNet-50), Mask R-CNN segmentation centroids mapped to coordinates for interaction actions, and autoregressively decoded action sequences; no explicit symbolic scene graph learned by the model (implicit encoding only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Goal-Condition Success (GC), and Trajectory-Length-Weighted variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EDH (validation): E.T. SR ~9.05% (seen val, TLW 1.2) and ~13.49% (unseen val, TLW 3.69). E.T. performance is much lower than E.T. on ALFRED (ALFRED reported ~38.24% seen, 8.57% unseen), indicating TEACh is harder.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Succeeded more often on short, local, single-step manipulations (e.g., placing a large object on a countertop) and situations where visual context strongly constrains the action; vision-only signal provided substantial benefit in these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggled with long-horizon, compositional tasks and hierarchical procedures; failed to reliably resolve dialogue phenomena (anaphora, coreference, overlapping speakers, irrelevant phatic chat); language signal from human-human dialogues was noisy and not strongly exploited for grounding; poor temporal alignment between dialogue and environmental actions limited use of procedural cues.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: Random ~1% SR; Lang-only ~3–4% SR; Vision-only ~5–9% SR. E.T. improves over random and unimodal baselines (E.T. ~9–13% SR across splits), but gains are modest and inconsistent across seen/unseen splits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding a history loss (H) that includes cross-entropy over the past action sequence improved SR (e.g., seen val E.T. 9.05% -> +H 12.5%). Initializing from ALFRED weights (A) and ALFRED synthetic language (S) showed small, non-significant gains. Unimodal ablations (Lang-only, Vision-only) perform worse than multimodal E.T.; Vision-only was sometimes competitive indicating heavy reliance on visual cues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Spatial, procedural, and object-relational knowledge in this multimodal transformer is encoded implicitly in fused visual-language embeddings and learned action-decoding weights; strong reliance on visual input means the model exploits spatial/object cues better when direct sensory observations are available, but it fails to robustly extract procedural structure or ground complex dialogue-only instructions—especially given human dialogue complexity and lack of tight alignment between utterances and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TEACh: Task-driven Embodied Agents that Chat', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e534.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e534.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E.T. (TfD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic Transformer (adapted for TEACh) - TfD evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same adapted Episodic Transformer evaluated on the Trajectory from Dialogue (TfD) benchmark, which requires predicting the entire sequence of environment actions from dialogue history alone (long-horizon trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic Transformer for Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Episodic Transformer (E.T.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above: multimodal transformer with ResNet-50 visual encoder and transformer language encoder; for TfD the model is given concatenated dialogue utterances as text input and (in the TfD task configuration used) has no past-action visual observations to align with utterances (|A_H^I| = 0), so it must infer a long action trajectory primarily from language plus any initial state signal provided during instance construction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Trajectory from Dialogue (TfD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given the initial simulator state S_i and the entire dialogue history A_H^D (all utterances between Commander and Follower for a session), predict the whole sequence of Follower environment actions A_R^I that were executed during the session to reach final state S_f; trajectories are long (~130 actions on average) and tasks are compositional.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>household tasks; long-horizon multi-step planning and instruction following from dialogue only</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+spatial (but primarily tested from language)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised fine-tuning on TfD instances (dialogue-to-trajectory); model initialization from ALFRED optional but TfD-specific training data used</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning using concatenated dialogue as input and ground-truth action sequences as targets; no additional prompting; no in-context methods applied in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural sequences are represented as target autoregressive action tokens to be decoded; dialogue is provided as plain text concatenation (implicit procedural cues encoded in transformer weights), i.e., knowledge is represented implicitly in weights without explicit symbolic plan structures or spatial maps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Goal-Condition Success (GC), Trajectory-Length-Weighted metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>TfD performance is poor but non-zero: E.T. validation SR ~1.02% (TfD val), TfD test SR ~0.51% (values reported in paper, trajectory lengths average ~130 actions). Random baseline is 0%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasional success on extremely short or simple trajectories where the dialogue tightly constrains a small sequence of actions; very rare non-zero successes indicate some capacity to map simple language sequences to short action fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failed to infer long-horizon (>100 steps) action sequences from dialogue alone; unable to reliably reconstruct procedural orderings, spatial navigation subtleties, or object interactions purely from unconstrained human dialogue that contains anaphora, omissions, and irrelevant utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random baseline 0% SR; E.T. small non-zero (~0.5–1%); no further ablations performed due to low absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No detailed TfD-specific ablations reported beyond those applied to the general E.T. adaptation; authors note they did not perform additional TfD ablations given low initial performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When deprived of direct sensory input (past visual observations), a transformer language model that concatenates dialogue struggles to represent sufficient spatial/procedural/object-relational information to plan or reproduce long action trajectories; dialogue-only input in TEACh (human-human, noisy, anaphoric) is insufficient for reliable long-horizon embodied planning in current architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TEACh: Task-driven Embodied Agents that Chat', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e534.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e534.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based TATC agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hand-crafted rule-based Commander and Follower agents for Two-Agent Task Completion (TATC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Engineered Commander and Follower agents that use Progress Check outputs and hand-crafted subgoal templates to generate language and action sequences; no learning — policies map task templates to sequences of navigation and interaction primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>rule-based Commander/Follower</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of hand-engineered policies/templates that (1) run a Progress Check to retrieve task/subgoal descriptions, (2) map those templates to sequences of primitive navigation and interaction actions (e.g., 'navigate to X, pick up X, navigate to Y, put down X'), and (3) emit simplified language utterances to the Follower; policies rely on object search, a map for navigation steps, and hard-coded handling of subgoal templates. No learning involved.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Two-Agent Task Completion (TATC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Model both Commander and Follower agents end-to-end: Commander must query the environment (Progress Check/SearchObject) to gather task/subgoal information and communicate that to the Follower via language; the Follower must execute navigation and object interactions to complete the full TEACh task. Environment observations are available to both agents (disembodied camera for Commander, egocentric view for Follower).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>household tasks; multi-agent coordination; navigation + object manipulation; hierarchical planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit hand-coded rules, templates and subgoal policies built from the TEACh task definition language and Progress Check outputs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>rule engineering: mapping Progress Check / object search outputs to language templates and action sequences; no training or prompting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit symbolic templates and procedural policies (if-then sequences), mapping task templates (e.g., 'X needs to be on some Y') to concrete navigation+interaction action programs; actions target resolved object instances discovered via search and segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) measured by TEACh task success criteria (state-change checks defined by TDL)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall TATC rule-based agent SR 24.40% (varying by task). Examples: MAKE COFFEE SR 54.55%, PLANT 26.70%, many compositional tasks (BOIL, TOAST, SANDWICH, BREAKFAST) had 0% because subgoal policies were not developed or failed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Capable of solving simple, single-subgoal tasks where the required subgoal template was covered by hand-coded policy (e.g., straightforward 'pick X -> place X on Y' tasks like making coffee in some scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failed on hierarchical/compositional tasks requiring many subgoals, complex object-state changes (boiling, toasting, cooking, composing multi-item meals), and handling simulation corner cases; required extensive hand engineering (~150 hours) and still had incomplete coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to humans: human average actions/session ~164.65 vs rule-based ~161.54 actions/session (rule-based often more verbose or failed); many tasks the rule-based agent had far lower success than human followers.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No learned-component ablations; paper reports that increasing subgoal policy coverage could raise SR but creating policies for complex tasks proved infeasible in reasonable engineering time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit symbolic/planner-style rule systems can solve a subset of simple TEACh tasks but do not scale to the compositional, dialogue-rich, object-state-change heavy tasks in TEACh; this underscores the need for learned models that can combine procedural decomposition with grounding of object relations and spatial plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TEACh: Task-driven Embodied Agents that Chat', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e534.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e534.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEACh dataset / TDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-driven Embodied Agents that Chat (TEACh) dataset and Task Definition Language (TDL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of 3,047 human-human interactive dialogues interleaved with environment actions in AI2-THOR for household tasks, plus a Task Definition Language (TDL) that defines parameterized, hierarchical tasks and success conditions in terms of object properties and state changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; a dataset and framework: TDL specifies tasks via object property-based goal conditions (e.g., 'mug is clean and filled with coffee'), enables parameterization (object classes, numbers, determiners), hierarchical tasks/subtasks, and provides tools (Progress Check, SearchObject) and simulator-state checkers for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TEACh dataset / EDH, TfD, TATC benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A realistic embodied-dialogue dataset: Commander (oracle) and Follower (actor) communicate in free-form text to complete household tasks in AI2-THOR; dataset includes initial and final simulator states, full action sequences, and per-session dialogue; three benchmarks derived: EDH (predict actions from dialogue+history), TfD (predict full trajectory from dialogue), and TATC (learn both agents from observations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dataset for household tasks; navigation, object manipulation, multi-step planning, dialog grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+spatial (designed to test all three)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>human-human dialogues and human demonstrations collected in simulator; TDL encodes task goals and success checks</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>dataset collection via paired crowdworkers using AI2-THOR interface where Commander can query oracle info (object locations, top-down map) and Follower acts with egocentric observations; dataset supports supervised learning tasks and benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>TDL: explicit template-based task descriptions and hierarchical subtask decomposition; dataset provides segmentation masks, top-down maps (for Commander), egocentric images and full action traces — enabling training of models that learn to map language to spatial, procedural, and object-relational representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dataset enabling SR and GC metrics for downstream models (EDH/TfD/TATC evaluation metrics defined in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>N/A (dataset/benchmark provider). The paper reports baseline model results (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Dataset captures successful human strategies: use of clarification questions, corrections, coreference and grounding in environment, hierarchical task decomposition; these allow models to potentially learn how dialogue conveys procedural and object-relational knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>N/A as dataset, but authors note many human dialogues contain irrelevant utterances, anaphora, and non turn-taking structure that make extracting precise procedural/spatial cues challenging for models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A (dataset), but TEACh benchmarks demonstrate that existing baseline models (E.T., rule-based) perform poorly on many tasks, indicating the dataset's difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>N/A for dataset itself.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TEACh operationalizes spatial, procedural, and object-relational knowledge through (1) a task definition language with explicit state-goal checks, (2) interactive oracle tools (SearchObject, Progress Check, top-down map) that expose spatial info to the Commander, and (3) dialogues interleaved with actions — providing a resource to study how language-only or multimodal models encode and use embodied knowledge, and showing that current architectures struggle when deprived of sensory alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TEACh: Task-driven Embodied Agents that Chat', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks <em>(Rating: 2)</em></li>
                <li>Episodic Transformer for Vision-and-Language Navigation <em>(Rating: 2)</em></li>
                <li>A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution <em>(Rating: 2)</em></li>
                <li>Vision-and-Dialog Navigation <em>(Rating: 2)</em></li>
                <li>Learning Interpretable Spatial Operations in a Rich 3D Blocks World <em>(Rating: 2)</em></li>
                <li>Just Ask: An Interactive Learning Framework for Vision and Language Navigation <em>(Rating: 1)</em></li>
                <li>Speaker-Follower Models for Vision-and-Language Navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-534",
    "paper_id": "paper-6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "E.T. (EDH)",
            "name_full": "Episodic Transformer (adapted for TEACh) - EDH evaluation",
            "brief_description": "An adapted Episodic Transformer model that fuses language and visual embeddings to predict action sequences for Execution from Dialogue History (EDH) instances in the TEACh benchmark; uses ResNet-50 visual backbone, Mask R-CNN for segmentation, and multimodal transformer layers with a learned action prediction head.",
            "citation_title": "Episodic Transformer for Vision-and-Language Navigation",
            "mention_or_use": "use",
            "model_name": "Episodic Transformer (E.T.)",
            "model_size": null,
            "model_description": "Transformer language encoder + ResNet-50 visual backbone; two multimodal transformer layers fuse language, image, and action embeddings; Mask R-CNN pretrained on ALFRED to produce segmentation masks used to select interaction targets; adapted action-head to match TEACh action space; trained with cross-entropy on ground-truth action sequences, with options to initialize from ALFRED weights or ALFRED+synthetic language.",
            "task_name": "Execution from Dialogue History (EDH)",
            "task_description": "Given an EDH instance (initial simulator state S^E, an action history A_H including dialogue and past environment actions, and reference interaction actions A_R^I), the agent must predict a sequence of actions that transforms the environment to the target state F^E in AI2-THOR. Actions include discrete navigation (Forward, Turn, Strafe, Look) and object interactions (Pickup, Place, Open, Slice, Pour, ToggleOn/Off), and interaction targets are specified as relative screen coordinates resolved via segmentation masks.",
            "task_type": "household tasks; navigation + object manipulation; multi-step instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "fine-tuning on TEACh EDH instances (supervised learning); optional initialization from ALFRED pretrained weights and ALFRED synthetic-language augmentation",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised fine-tuning on EDH instances (cross-entropy over action sequences); ablations include removing history loss or initializing from ALFRED",
            "knowledge_representation": "multimodal implicit representations in transformer weights: concatenated dialogue as text tokens, CNN image embeddings (ResNet-50), Mask R-CNN segmentation centroids mapped to coordinates for interaction actions, and autoregressively decoded action sequences; no explicit symbolic scene graph learned by the model (implicit encoding only).",
            "performance_metric": "Success Rate (SR), Goal-Condition Success (GC), and Trajectory-Length-Weighted variants",
            "performance_result": "EDH (validation): E.T. SR ~9.05% (seen val, TLW 1.2) and ~13.49% (unseen val, TLW 3.69). E.T. performance is much lower than E.T. on ALFRED (ALFRED reported ~38.24% seen, 8.57% unseen), indicating TEACh is harder.",
            "success_patterns": "Succeeded more often on short, local, single-step manipulations (e.g., placing a large object on a countertop) and situations where visual context strongly constrains the action; vision-only signal provided substantial benefit in these cases.",
            "failure_patterns": "Struggled with long-horizon, compositional tasks and hierarchical procedures; failed to reliably resolve dialogue phenomena (anaphora, coreference, overlapping speakers, irrelevant phatic chat); language signal from human-human dialogues was noisy and not strongly exploited for grounding; poor temporal alignment between dialogue and environmental actions limited use of procedural cues.",
            "baseline_comparison": "Baselines: Random ~1% SR; Lang-only ~3–4% SR; Vision-only ~5–9% SR. E.T. improves over random and unimodal baselines (E.T. ~9–13% SR across splits), but gains are modest and inconsistent across seen/unseen splits.",
            "ablation_results": "Adding a history loss (H) that includes cross-entropy over the past action sequence improved SR (e.g., seen val E.T. 9.05% -&gt; +H 12.5%). Initializing from ALFRED weights (A) and ALFRED synthetic language (S) showed small, non-significant gains. Unimodal ablations (Lang-only, Vision-only) perform worse than multimodal E.T.; Vision-only was sometimes competitive indicating heavy reliance on visual cues.",
            "key_findings": "Spatial, procedural, and object-relational knowledge in this multimodal transformer is encoded implicitly in fused visual-language embeddings and learned action-decoding weights; strong reliance on visual input means the model exploits spatial/object cues better when direct sensory observations are available, but it fails to robustly extract procedural structure or ground complex dialogue-only instructions—especially given human dialogue complexity and lack of tight alignment between utterances and actions.",
            "uuid": "e534.0",
            "source_info": {
                "paper_title": "TEACh: Task-driven Embodied Agents that Chat",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "E.T. (TfD)",
            "name_full": "Episodic Transformer (adapted for TEACh) - TfD evaluation",
            "brief_description": "The same adapted Episodic Transformer evaluated on the Trajectory from Dialogue (TfD) benchmark, which requires predicting the entire sequence of environment actions from dialogue history alone (long-horizon trajectories).",
            "citation_title": "Episodic Transformer for Vision-and-Language Navigation",
            "mention_or_use": "use",
            "model_name": "Episodic Transformer (E.T.)",
            "model_size": null,
            "model_description": "As above: multimodal transformer with ResNet-50 visual encoder and transformer language encoder; for TfD the model is given concatenated dialogue utterances as text input and (in the TfD task configuration used) has no past-action visual observations to align with utterances (|A_H^I| = 0), so it must infer a long action trajectory primarily from language plus any initial state signal provided during instance construction.",
            "task_name": "Trajectory from Dialogue (TfD)",
            "task_description": "Given the initial simulator state S_i and the entire dialogue history A_H^D (all utterances between Commander and Follower for a session), predict the whole sequence of Follower environment actions A_R^I that were executed during the session to reach final state S_f; trajectories are long (~130 actions on average) and tasks are compositional.",
            "task_type": "household tasks; long-horizon multi-step planning and instruction following from dialogue only",
            "knowledge_type": "procedural+object-relational+spatial (but primarily tested from language)",
            "knowledge_source": "supervised fine-tuning on TfD instances (dialogue-to-trajectory); model initialization from ALFRED optional but TfD-specific training data used",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning using concatenated dialogue as input and ground-truth action sequences as targets; no additional prompting; no in-context methods applied in the paper",
            "knowledge_representation": "Procedural sequences are represented as target autoregressive action tokens to be decoded; dialogue is provided as plain text concatenation (implicit procedural cues encoded in transformer weights), i.e., knowledge is represented implicitly in weights without explicit symbolic plan structures or spatial maps.",
            "performance_metric": "Success Rate (SR), Goal-Condition Success (GC), Trajectory-Length-Weighted metrics",
            "performance_result": "TfD performance is poor but non-zero: E.T. validation SR ~1.02% (TfD val), TfD test SR ~0.51% (values reported in paper, trajectory lengths average ~130 actions). Random baseline is 0%.",
            "success_patterns": "Occasional success on extremely short or simple trajectories where the dialogue tightly constrains a small sequence of actions; very rare non-zero successes indicate some capacity to map simple language sequences to short action fragments.",
            "failure_patterns": "Failed to infer long-horizon (&gt;100 steps) action sequences from dialogue alone; unable to reliably reconstruct procedural orderings, spatial navigation subtleties, or object interactions purely from unconstrained human dialogue that contains anaphora, omissions, and irrelevant utterances.",
            "baseline_comparison": "Random baseline 0% SR; E.T. small non-zero (~0.5–1%); no further ablations performed due to low absolute performance.",
            "ablation_results": "No detailed TfD-specific ablations reported beyond those applied to the general E.T. adaptation; authors note they did not perform additional TfD ablations given low initial performance.",
            "key_findings": "When deprived of direct sensory input (past visual observations), a transformer language model that concatenates dialogue struggles to represent sufficient spatial/procedural/object-relational information to plan or reproduce long action trajectories; dialogue-only input in TEACh (human-human, noisy, anaphoric) is insufficient for reliable long-horizon embodied planning in current architectures.",
            "uuid": "e534.1",
            "source_info": {
                "paper_title": "TEACh: Task-driven Embodied Agents that Chat",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Rule-based TATC agents",
            "name_full": "Hand-crafted rule-based Commander and Follower agents for Two-Agent Task Completion (TATC)",
            "brief_description": "Engineered Commander and Follower agents that use Progress Check outputs and hand-crafted subgoal templates to generate language and action sequences; no learning — policies map task templates to sequences of navigation and interaction primitives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "rule-based Commander/Follower",
            "model_size": null,
            "model_description": "A set of hand-engineered policies/templates that (1) run a Progress Check to retrieve task/subgoal descriptions, (2) map those templates to sequences of primitive navigation and interaction actions (e.g., 'navigate to X, pick up X, navigate to Y, put down X'), and (3) emit simplified language utterances to the Follower; policies rely on object search, a map for navigation steps, and hard-coded handling of subgoal templates. No learning involved.",
            "task_name": "Two-Agent Task Completion (TATC)",
            "task_description": "Model both Commander and Follower agents end-to-end: Commander must query the environment (Progress Check/SearchObject) to gather task/subgoal information and communicate that to the Follower via language; the Follower must execute navigation and object interactions to complete the full TEACh task. Environment observations are available to both agents (disembodied camera for Commander, egocentric view for Follower).",
            "task_type": "household tasks; multi-agent coordination; navigation + object manipulation; hierarchical planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "explicit hand-coded rules, templates and subgoal policies built from the TEACh task definition language and Progress Check outputs",
            "has_direct_sensory_input": true,
            "elicitation_method": "rule engineering: mapping Progress Check / object search outputs to language templates and action sequences; no training or prompting",
            "knowledge_representation": "Explicit symbolic templates and procedural policies (if-then sequences), mapping task templates (e.g., 'X needs to be on some Y') to concrete navigation+interaction action programs; actions target resolved object instances discovered via search and segmentation.",
            "performance_metric": "Success Rate (SR) measured by TEACh task success criteria (state-change checks defined by TDL)",
            "performance_result": "Overall TATC rule-based agent SR 24.40% (varying by task). Examples: MAKE COFFEE SR 54.55%, PLANT 26.70%, many compositional tasks (BOIL, TOAST, SANDWICH, BREAKFAST) had 0% because subgoal policies were not developed or failed.",
            "success_patterns": "Capable of solving simple, single-subgoal tasks where the required subgoal template was covered by hand-coded policy (e.g., straightforward 'pick X -&gt; place X on Y' tasks like making coffee in some scenes).",
            "failure_patterns": "Failed on hierarchical/compositional tasks requiring many subgoals, complex object-state changes (boiling, toasting, cooking, composing multi-item meals), and handling simulation corner cases; required extensive hand engineering (~150 hours) and still had incomplete coverage.",
            "baseline_comparison": "Compared to humans: human average actions/session ~164.65 vs rule-based ~161.54 actions/session (rule-based often more verbose or failed); many tasks the rule-based agent had far lower success than human followers.",
            "ablation_results": "No learned-component ablations; paper reports that increasing subgoal policy coverage could raise SR but creating policies for complex tasks proved infeasible in reasonable engineering time.",
            "key_findings": "Explicit symbolic/planner-style rule systems can solve a subset of simple TEACh tasks but do not scale to the compositional, dialogue-rich, object-state-change heavy tasks in TEACh; this underscores the need for learned models that can combine procedural decomposition with grounding of object relations and spatial plans.",
            "uuid": "e534.2",
            "source_info": {
                "paper_title": "TEACh: Task-driven Embodied Agents that Chat",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "TEACh dataset / TDL",
            "name_full": "Task-driven Embodied Agents that Chat (TEACh) dataset and Task Definition Language (TDL)",
            "brief_description": "A dataset of 3,047 human-human interactive dialogues interleaved with environment actions in AI2-THOR for household tasks, plus a Task Definition Language (TDL) that defines parameterized, hierarchical tasks and success conditions in terms of object properties and state changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "Not a model; a dataset and framework: TDL specifies tasks via object property-based goal conditions (e.g., 'mug is clean and filled with coffee'), enables parameterization (object classes, numbers, determiners), hierarchical tasks/subtasks, and provides tools (Progress Check, SearchObject) and simulator-state checkers for evaluation.",
            "task_name": "TEACh dataset / EDH, TfD, TATC benchmarks",
            "task_description": "A realistic embodied-dialogue dataset: Commander (oracle) and Follower (actor) communicate in free-form text to complete household tasks in AI2-THOR; dataset includes initial and final simulator states, full action sequences, and per-session dialogue; three benchmarks derived: EDH (predict actions from dialogue+history), TfD (predict full trajectory from dialogue), and TATC (learn both agents from observations).",
            "task_type": "dataset for household tasks; navigation, object manipulation, multi-step planning, dialog grounding",
            "knowledge_type": "procedural+object-relational+spatial (designed to test all three)",
            "knowledge_source": "human-human dialogues and human demonstrations collected in simulator; TDL encodes task goals and success checks",
            "has_direct_sensory_input": null,
            "elicitation_method": "dataset collection via paired crowdworkers using AI2-THOR interface where Commander can query oracle info (object locations, top-down map) and Follower acts with egocentric observations; dataset supports supervised learning tasks and benchmarks",
            "knowledge_representation": "TDL: explicit template-based task descriptions and hierarchical subtask decomposition; dataset provides segmentation masks, top-down maps (for Commander), egocentric images and full action traces — enabling training of models that learn to map language to spatial, procedural, and object-relational representations.",
            "performance_metric": "Dataset enabling SR and GC metrics for downstream models (EDH/TfD/TATC evaluation metrics defined in paper)",
            "performance_result": "N/A (dataset/benchmark provider). The paper reports baseline model results (see other entries).",
            "success_patterns": "Dataset captures successful human strategies: use of clarification questions, corrections, coreference and grounding in environment, hierarchical task decomposition; these allow models to potentially learn how dialogue conveys procedural and object-relational knowledge.",
            "failure_patterns": "N/A as dataset, but authors note many human dialogues contain irrelevant utterances, anaphora, and non turn-taking structure that make extracting precise procedural/spatial cues challenging for models.",
            "baseline_comparison": "N/A (dataset), but TEACh benchmarks demonstrate that existing baseline models (E.T., rule-based) perform poorly on many tasks, indicating the dataset's difficulty.",
            "ablation_results": "N/A for dataset itself.",
            "key_findings": "TEACh operationalizes spatial, procedural, and object-relational knowledge through (1) a task definition language with explicit state-goal checks, (2) interactive oracle tools (SearchObject, Progress Check, top-down map) that expose spatial info to the Commander, and (3) dialogues interleaved with actions — providing a resource to study how language-only or multimodal models encode and use embodied knowledge, and showing that current architectures struggle when deprived of sensory alignment.",
            "uuid": "e534.3",
            "source_info": {
                "paper_title": "TEACh: Task-driven Embodied Agents that Chat",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
            "rating": 2
        },
        {
            "paper_title": "Episodic Transformer for Vision-and-Language Navigation",
            "rating": 2
        },
        {
            "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-Dialog Navigation",
            "rating": 2
        },
        {
            "paper_title": "Learning Interpretable Spatial Operations in a Rich 3D Blocks World",
            "rating": 2
        },
        {
            "paper_title": "Just Ask: An Interactive Learning Framework for Vision and Language Navigation",
            "rating": 1
        },
        {
            "paper_title": "Speaker-Follower Models for Vision-and-Language Navigation",
            "rating": 1
        }
    ],
    "cost": 0.019152,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TEACh: Task-Driven Embodied Agents That Chat</h1>
<p>Aishwarya Padmakumar ${ }^{<em> 1}$, Jesse Thomason ${ }^{</em> 2}$, Ayush Shrivastava ${ }^{3}$, Patrick Lange ${ }^{1}$, Anjali<br>Narayan-Chen ${ }^{1}$, Spandana Gella ${ }^{1}$, Robinson Piramuthu ${ }^{1}$, Gokhan Tur ${ }^{1}$, Dilek-Hakkani Tur ${ }^{1}$<br>${ }^{1}$ Amazon Alexa AI<br>${ }^{2}$ USC Viterbi Department of Computer Science, University of Southern California<br>${ }^{3}$ Department of Electrical Engineering And Computer Science, University of Michigan<br>padmakua@amazon.com, jessedt@amazon.com, ayshrv@umich.edu, patlange@amazon.com, naraanja@amazon.com, sgella@amazon.com, robinpir@amazon.com, gokhatur@amazon.com, hakkanit@amazon.com</p>
<h4>Abstract</h4>
<p>Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from MaKe Coffee to Prepare BREAKFAST, asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution.</p>
<h2>1 Introduction</h2>
<p>Many benchmarks for translating visual observations and an initial language instruction to actions assume no further language communication (Anderson et al. 2018; Shridhar et al. 2020). However, obtaining clarification via simulated interactions (Chi et al. 2020; Nguyen and Daumé III 2019) or learning from human-human dialogue (Thomason et al. 2019; Suhr et al. 2019) can improve embodied navigation. We hypothesize that dialogue has even more to offer for object-centric, hierarchical tasks.</p>
<p>We introduce Task-driven Embodied Agents that Chat (TEACh) to study how agents can learn to ground natural language (Harnad 1990; Bisk et al. 2020) to the visual world and actions, while considering long-term and intermediate goals, and using dialogue to communicate. TEACh contains over 3,000 human-human sessions interleaving utterances and environment actions where a Commander with oracle task and world knowledge and a Follower with the ability to interact with the world communicate in written English to complete household chores (Figure 1).</p>
<p>TEACh dialogues are unconstrained, not turn-based, yielding variation in instruction granularity, completeness, relevance, and overlap. Utterances include coreference with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The Commander has oracle task details (a), object locations (b), a map (c), and egocentric views from both agents. The Follower carries out the task and asks questions (d). The agents can only communicate via language.
previously mentioned entities, past actions, and locations. Because TEACh sessions are human, rather than plannerbased (Ghallab et al. 1998), Follower trajectories include mistakes and corresponding, language-guided correction.</p>
<p>We propose three benchmarks based on TEACh sessions to study the ability of learned models to achieve aspects of embodied intelligence: Execution from Dialog History (EDH), Trajectory from Dialog (TfD) and Two-Agent Task Completion (TATC) ${ }^{1}$. We evaluate a baseline Follower agent for the EDH and TfD benchmarks based on the Episodic</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">- Object -</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">- Language -</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Demonstrations</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Interaction State Changes Conversational # Sessions Freeform</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">R2R (Anderson et al. 2018)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Planner</td>
</tr>
<tr>
<td style="text-align: left;">CHAI (Misra et al. 2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">CVDN (Thomason et al. 2019)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2050</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">CerealBar (Suhr et al. 2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1202</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">MDC (Narayan-Chen et al. 2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">509</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">ALFRED (Shridhar et al. 2020)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Planner</td>
</tr>
<tr>
<td style="text-align: left;">III (Abramson et al. 2020)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">TEACh</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3215</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human</td>
</tr>
</tbody>
</table>
<p>Table 1: TEACh is the first dataset where human-human, conversational dialogues were used to perform tasks involving object interaction, such as picking up a knife, and state changes, such as slicing bread, in a visual simulation environment. TEACh task demonstrations are created by the human Follower, who engages in a free-form, rather than turn-taking, dialogue with the human Commander. Compared to past dialogue datasets for visual tasks, TEACh contains many more individual dialogues.</p>
<p>Transformer (E.T.) model (Pashevich, Schmid, and Sun 2021) and demonstrate the difficulty of engineering rulebased solvers for end-to-end task completion.</p>
<p>The main contributions of this work are:</p>
<ul>
<li>TEACh, a dataset of over 3000 human-human dialogs simulating the experience of a user interacting with their robot to complete tasks in their home, that interleaves dialogue messages with actions taken in the environment.</li>
<li>An extensible task definition framework (§3) that can be used to define and check completion status for a wide range of tasks in a simulated environment.</li>
<li>Three benchmarks based on TEACh sessions and experiments demonstrating initial models for each.</li>
</ul>
<h2>2 Related Work</h2>
<p>Table 1 situates TEACh with respect to other datasets involving natural language instructions for visual task completion.
Vision \&amp; Language Navigation (VLN) tasks agents with taking in language instructions and a visual observation to produce an action, such as turning or moving forward, to receive a new visual observation. VLN benchmarks have evolved from the use of symbolic environment representations (MacMahon, Stankiewicz, and Kuipers 2006; Chen and Mooney 2011; Mei, Bansal, and Walter 2016) to photorealistic indoor (Anderson et al. 2018) and outdoor environments (Chen et al. 2019), as well as the prediction of continuous control (Blukis et al. 2018). TEACh goes beyond navigation to object interactions for task completion, and beyond single instructions to dialogue.
Vision \&amp; Language Task Completion involves actions beyond navigation. Models have evolved from individual rule-based or learned components for language understanding, perception and action execution (Matuszek et al. 2013; Kollar et al. 2013), to end-to-end models in fully observable blocks worlds (Bisk et al. 2018; Misra et al. 2018). More complex tasks involve partially observable worlds (Kim et al. 2020) and object state changes (Misra et al. 2018; Puig et al. 2018; Shridhar et al. 2020). Some works use a planner to generate ideal demonstrations that are then labeled, while
others first gather instructions and gather human demonstrations (Misra et al. 2018; Shah et al. 2021; Abramson et al. 2020). In TEACh, human instructions and demonstrations are gathered simultaneously.
Vision \&amp; Dialogue Navigation and Task Completion Agents that additionally engage in dialogue can be learned by combining individual rule-based or learned components (Tellex et al. 2016; Arumugam et al. 2018; Thomason et al. 2020). End-to-end VLN models can be improved by simulated clarification (Chi et al. 2020; Nguyen and Daumé III 2019) and incorporating human-human conversation history (Thomason et al. 2019; Zhu et al. 2020). Other works learn agent-agent policies for navigating and speaking (Roman et al. 2020; Shrivastava et al. 2021), and deploy individual agent policies for human-in-the-loop evaluation (Suhr et al. 2019). However, such models and underlying datasets are limited to navigation actions and turntaking conversation. In contrast, TEACh involves Follower navigation and object interaction, as well as freeform dialogue acts with the Commander. The Minecraft Dialogue Corpus (MDC) (Narayan-Chen, Jayannavar, and Hockenmaier 2019) gives full dialogues between two humans for assembly tasks. MDC is similar in spirit to TEACh; we introduce a larger action space and resulting object state changes, such as slicing and toasting bread, as well as collecting many more human-human dialogues.</p>
<h2>3 The TEACh Dataset</h2>
<p>We collect 3,047 human-human gameplay sessions for completing household tasks in the AI2-THOR simulator (Kolve et al. 2017). Each session includes an initial environment state, Commander actions to access oracle information, utterances between the Commander and Follower, movement actions, and object interactions taken by the Follower. Figure 2 gives an overview of the annotation interface.</p>
<h3>3.1 Household Tasks</h3>
<p>We design a task definition language (TDL) to define household tasks in terms of object properties to satisfy, and implement a framework over AI2-THOR that evaluates these</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: To collect TEACh, the Commander knows the task to be completed and can query the simulator for object locations. Searched items are highlighted in green for the Commander; highlights blink to enable seeing the underlying true scene colors. The Commander has a topdown map of the scene, with the current camera position shown in red, the Follower position shown in blue, and the object search camera position shown in yellow. The Follower moves around in the environment and interacts with objects, such as placing a fork (middle). Target objects for each interaction action are highlighted.
criteria. For example, for a task to make coffee, we consider the environment to be in a successful state if there is a mug in the environment that is clean and filled with coffee.</p>
<p>Parameterized tasks such as Put All X On Y enable task variation. Parameters can be object classes, such as putting all forks on a countertop, or predefined abstract hypernyms, for example putting all silverwareforks, spoons, and knives-on the counter. TEACh task definitions are also hierarchical. For example, PrePare Breakfast contains the subtasks Make Coffee and Make Plate of Toast. We incorporate determiners such as "a", "all" and numbers such as 2 to enable easy definition of a wide range of tasks, such as N Slices of X in Y. The TEACh TDL includes template-based language prompts to describe tasks and subtasks to Commanders (Figure 3).</p>
<h3>3.2 Gameplay Session Collection</h3>
<p>Annotators first completed a tutorial task demonstrating the interface to vet their understanding. For each session, two vetted crowdworkers were paired using a web interface and assigned to the Commander and Follower roles (Figure 2). The Commander is shown the task to be completed and the steps needed to achieve this given the current state of the environment, using template-based language prompts, none of which are accessible to the Follower. The Commander can additionally search for the location of objects, either by string name, such as "sink", or by clicking a task-relevant object in the display (Figure 3). The Commander and Follower must use text chat to communicate the parameters of the task and clarify object locations. Only the Follower can interact with objects in the environment.</p>
<p>We obtained initial states for each parameterized task by randomizing AI2-THOR environments and retaining those that satisfied preconditions such as task-relevant objects being present and reachable. For each session, we store the initial simulator state $S_{i}$, the sequence of actions $A=\left(a_{1}, a_{2}, \ldots\right)$ taken, and the final simulator state $S_{f}$. TEACh Follower actions are Forward, Backward, Turn Left, Turn Right, Look Up, Look Down,</p>
<p>Strafe Left, Strafe Right, Pickup, Place, Open, Close, ToggleOn, ToggleOff, Slice, and Pour. Navigation actions move the agent in discrete steps. Object manipulation expects the agent to specify an object via a relative coordinate $(x, y)$ on Follower egocentric frame. The TEACh wrapper on the AI2-THOR simulator examines the ground truth segmentation mask of the agent's egocentric image, selects an object in a 10x10 pixel patch around the coordinate if the desired action can be performed on it, and executes the action in AI2-THOR. The Commander can execute a Progress Check and SearchObject actions, demonstrated in Figure 3. TEACh Commander actions also allow navigation, but the Commander is a disembodied camera.</p>
<h3>3.3 TEACh Statistics</h3>
<p>TEACh is comprised of 3,047 successful gameplay sessions, each of which can be replayed using the AI2-THOR simulator for model training, feature extraction, or model evaluation. In total, 4,365 crowdsourced sessions were collected with a human-level success rate of $74.17 \%$ ( 3320 sessions) and total cost of $\$ 105 \mathrm{k}$; more details in appendix. Some successful sessions were not included in the final split used in benchmarks due to replay issues. TEACh sessions span all 30 AI2-THOR kitchens, and include most of the 30 each AI2-THOR living rooms, bedrooms, and bathrooms.</p>
<p>Successful TEACh sessions consist of over 45 k utterances, with an average of 8.40 Commander and 5.25 Follower utterances per session. The average Commander utterance length is 5.70 tokens and the average Follower utterance length is 3.80 tokens. The TEACh data has a vocabulary size of 3,429 unique tokens. ${ }^{2}$ Table 2 summarizes such metrics across the 12 task types in TEACh. Simple tasks like MAKE COFFEE require fewer dialogue acts and Follower actions on average than complex, composite tasks like PREPARE BREAKFAST which subsume those simpler tasks.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example task definition from the TEACh task definition language (left) and how it informs the initial simulator state and the Commander Progress Check action. The Commander can SearchObject with a string query (right) or object instance (center) returned by the Progress Check task status, yielding a camera view, segmentation mask, and location.</p>
<h2>4 TEACh Benchmarks</h2>
<p>We introduce three benchmarks based on TEACh sessions to train and evaluate the ability of embodied AI models to complete household tasks using natural language dialogue. Execution from Dialogue History and Trajectory from Dialogue require modeling the Follower. Two-Agent Task Completion, by contrast, requires modeling both the Commander and Follower agents to complete TEACh tasks end-to-end. For each benchmark, we define how we derive benchmark instances from TEACh gameplay sessions, and by what metrics we evaluate model performance.</p>
<p>Each session has an initial state $S_{i}$, the sequence of actions $A=\left(a_{1}, a_{2}, \ldots\right)$ taken by the Commander and Follower including dialogue and environment actions, and the final state $S_{f}$. We denote the subsequence of all dialogue actions as $A^{D}$, and of all navigation and interaction as $A^{I}$. Following ALFRED, we create validation and test splits in both seen and unseen environments (Table 3). Seen splits contain sessions based in AI2-THOR rooms that were seen the training, whereas unseen splits contain only sessions in rooms absent from the training set.</p>
<h3>4.1 Execution from Dialogue History (EDH)</h3>
<p>We segment TEACh sessions into EDH instances. We construct EDH instances $\left(S^{E}, A_{H}, A_{R}^{I}, F^{E}\right)$ where $S^{E}$ is the initial state of the EDH instance, $A_{H}$ is an action history, and the agent is tasked with predicting a sequence of actions that changes the environment state to $F^{E}$, using $A_{R}^{I}$ reference interaction actions taken in the session as supervision. We constrain instances to have $\left|A_{H}^{D}\right|&gt;0$ and at least one object interaction in $A_{R}^{I}$. Each EDH instance is punctuated by a dialogue act starting a new instance or the session end. We append a Stop action to each $A_{R}^{I}$. An example is included in Figure 4.</p>
<p>To evaluate inferred EDH action sequences, we compare the simulator state changes $\hat{E}$ at the end of inference with $F^{E}$ using similar evaluation criteria generalized from the ALFRED benchmark.</p>
<ul>
<li>Success ${0,1}: 1$ if all expected state changes $F^{E}$ are present in $\hat{E}$, else 0 . We average over all trajectories.</li>
<li>Goal-Condition Success (GC) $(0,1)$ : The fraction of expected state changes in $F^{E}$ present in $\hat{E}$. We average over all trajectories. ${ }^{3}$</li>
<li>Trajectory Weighted Metrics: For a reference trajectory $A_{R}^{I}$ and inferred action sequence $\hat{A}^{I}$, we calculate trajectory length weighted metric for metric value $m$ as</li>
</ul>
<p>$$
T L W-m=\frac{m *\left|A_{R}^{I}\right|}{\max \left(\left|A_{R}^{I}\right|,\left|\hat{A}^{I}\right|\right)}
$$</p>
<p>During inference, the learned Follower agent predicts actions until either it predicts the Stop action, hits a limit of 1000 steps, or hits a limit of 30 failed actions.</p>
<h3>4.2 Trajectory from Dialogue (TfD)</h3>
<p>A Follower agent model is tasked with inferring the whole sequence of Follower environmental actions taken during the session conditioned on the dialogue history. A TfD instance is $\left(S_{i}, A_{H}^{D}, A_{R}^{I}, S_{f}\right)$, where $A_{H}^{D}$ is all dialogue actions taken by both agents, and $A_{R}^{I}$ is all non-dialogue actions taken by the Follower. We append a Stop action to $A_{R}^{I}$. The agent does not observe dialogue actions in context, however, we use this task to test long horizon action</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Parameter <br> Variants</th>
<th style="text-align: right;">Unique <br> Scenes</th>
<th style="text-align: right;">Total <br> Sessions</th>
<th style="text-align: right;">Utterances <br> per Session</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Follower <br> Actions/Session</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">All <br> Actions/Session</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WATER PLANT</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$6.37 \pm 4.36$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$51.86 \pm 30.71$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$67.93 \pm 40.70$</td>
</tr>
<tr>
<td style="text-align: left;">MAKE COFFEE</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">308</td>
<td style="text-align: right;">$7.75 \pm 5.08$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$55.25 \pm 33.61$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$72.29 \pm 50.85$</td>
</tr>
<tr>
<td style="text-align: left;">CLEAN ALL X</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">$9.65 \pm 7.03$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$74.06 \pm 59.66$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$96.92 \pm 71.31$</td>
</tr>
<tr>
<td style="text-align: left;">PUT ALL X ON Y</td>
<td style="text-align: right;">209</td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">344</td>
<td style="text-align: right;">$8.66 \pm 5.82$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$82.13 \pm 66.39$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$103.53 \pm 80.97$</td>
</tr>
<tr>
<td style="text-align: left;">BOIL POTATO</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">202</td>
<td style="text-align: right;">$10.65 \pm 7.61$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$104.66 \pm 79.50$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$130.13 \pm 94.80$</td>
</tr>
<tr>
<td style="text-align: left;">MAKE Plate of Toast</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">225</td>
<td style="text-align: right;">$12.26 \pm 8.51$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$108.30 \pm 55.81$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$136.11 \pm 70.73$</td>
</tr>
<tr>
<td style="text-align: left;">N SLICES OF X In Y</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">29</td>
<td style="text-align: right;">304</td>
<td style="text-align: right;">$13.50 \pm 10.86$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$113.62 \pm 94.25$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$146.23 \pm 113.96$</td>
</tr>
<tr>
<td style="text-align: left;">PUT ALL X In ONE Y</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">302</td>
<td style="text-align: right;">$11.32 \pm 7.03$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$115.74 \pm 90.13$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$147.80 \pm 104.45$</td>
</tr>
<tr>
<td style="text-align: left;">N COOKED X SLICES In Y</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">240</td>
<td style="text-align: right;">$14.94 \pm 9.43$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$155.18 \pm 75.17$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$189.26 \pm 87.90$</td>
</tr>
<tr>
<td style="text-align: left;">PREPARE SANDWICH</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">241</td>
<td style="text-align: right;">$18.03 \pm 9.96$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$195.93 \pm 83.96$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$241.61 \pm 100.86$</td>
</tr>
<tr>
<td style="text-align: left;">PREPARE SALAD</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">323</td>
<td style="text-align: right;">$20.47 \pm 10.80$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$206.29 \pm 111.47$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$253.94 \pm 130.09$</td>
</tr>
<tr>
<td style="text-align: left;">PREPARE BREAKFAST</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">308</td>
<td style="text-align: right;">$27.67 \pm 14.73$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$295.06 \pm 138.76$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$359.90 \pm 162.33$</td>
</tr>
<tr>
<td style="text-align: left;">TEACh Overall</td>
<td style="text-align: right;">438</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">3320</td>
<td style="text-align: right;">$13.67 \pm 10.81$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$131.80 \pm 109.68$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$164.65 \pm 130.89$</td>
</tr>
</tbody>
</table>
<p>Table 2: The 12 tasks represented in TEACh sessions vary in complexity. Tasks like Put All X On Y take object class parameters and can require more actions per session to finish. Composite tasks like PrePARE SALAD contain sub-tasks like N SLICES OF X In Y. Per session data are averages with standard deviation across task types.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Fold</th>
<th style="text-align: left;">Split</th>
<th style="text-align: left;"># Sessions</th>
<th style="text-align: center;"># EDH Instances</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$1482(49 \%)$</td>
<td style="text-align: center;">$5758(49 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Val</td>
<td style="text-align: left;">Seen</td>
<td style="text-align: left;">$181(6 \%)$</td>
<td style="text-align: center;">$654(5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Unseen</td>
<td style="text-align: left;">$614(20 \%)$</td>
<td style="text-align: center;">$2188(19 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">Seen</td>
<td style="text-align: left;">$181(6 \%)$</td>
<td style="text-align: center;">$696(6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Unseen</td>
<td style="text-align: left;">$589(19 \%)$</td>
<td style="text-align: center;">$2370(20 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Session and EDH instances in TEACh data splits.
prediction with a block of instructions, analogous to ALFRED or TouchDown (Chen et al. 2019). We calculate success and goal-conditioned success by comparing $\hat{E}$ against state changes between $S_{i}$ and $S_{f}$.</p>
<h3>4.3 Two-Agent Task Completion (TATC)</h3>
<p>To explore modeling both a Commander and Follower agent, the TATC benchmark gives as input only environment observations to both agents. The Commander model must use the Progress Check action to receive task information, then synthesize that information piece by piece to the Follower agent via language generation. The Follower model can communicate back via language generation. The TATC benchmark represents studying the "whole" set of challenges the TEACh dataset provides. We calculate success and goal-conditioned success by comparing $\hat{E}$ against state changes between $S_{I}$ and $S_{f}$.</p>
<h2>5 Experiments and Results</h2>
<p>We implement initial baseline models and establish the richness of TEACh data and difficulty of resulting benchmarks.</p>
<h3>5.1 Follower Models for EDH and TfD</h3>
<p>We use a single model architecture to train and evaluate on the EDH and TfD benchmark tasks.</p>
<p>Model. We establish baseline performance for the EDH and TfD tasks using the Episodic Transformer (E.T.) model (Pashevich, Schmid, and Sun 2021), designed for the ALFRED benchmark. The original E.T. model trains a transformer language encoder and uses a ResNet-50 backbone to encode visual observations. Two multimodal transformer layers are used to fuse information from the language, image, and action embeddings, followed by a fully connected layer to predict the next action and target object category for interaction actions. E.T. uses a MaskRCNN (He et al. 2017) model pretrained on ALFRED images to predict a segmentation of the egocentric image for interactive actions, matching the predicted mask to the predicted object category. We convert the centroid of this mask to a relative coordinate specified to the TEACh API wrapper for AI2-THOR.</p>
<p>We modify E.T. by learning a new action prediction head to match TEACh Follower actions. Given an EDH or TfD instance, we extract all dialogue utterances from the action history $A_{H}^{D}$ and concatenate these with a separator between utterances to form the language input. The remaining actions $A_{H}^{I}$ are fed in order as the past action input with associated image observations. Consequently, our adapted E.T. does not have temporal alignment between dialogue actions and environment actions.</p>
<p>Following the mechanism used in the original E.T. paper, we provide image observations from both actions in the history $A_{H}^{I}$, and the reference actions $A_{R}^{I}$, and task the model to predict the entire sequence of actions. The model parameters are optimized using cross entropy loss between the predicted action sequence and the ground truth action sequence. For EDH, we ablate a history loss (H) as cross entropy over the entire action sequence-actions in both $A_{H}^{I}$ and $A_{R}^{I}$, to compare against loss only against actions in $A_{R}^{I}$. Note that in TfD, $\left|A_{H}^{I}\right|=0$.</p>
<p>We additionally experiment with initializing the model using weights trained on the ALFRED dataset. Note that since the language vocabulary and action space change,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Two EDH instances are constructed from this real example from the <em>TEACh</em> data. The first instance input contains only dialogue actions. After inference on the first instance, the agent is evaluated based on whether it moved the potato, pot, and the items cleared out of the sink to their target destinations. In this example, the pot cannot fit into the sink. The second instance input has both dialogue and environment actions, and is evaluated at inference by whether the pot lands on the stove filled with water, and whether the potato is inside the pot and boiled.</p>
<p>Some layers need to be retrained. For EDH, we experiment with initializing the model both with weights from the E.T. model trained only on base ALFRED annotations (A) and the model trained on ALFRED augmented with synthetic instructions (S) (from Pashevich, Schmid, and Sun (2021)). We also perform unimodal ablations of the E.T. model to determine whether the model is simply memorizing sequences from the training data (Thomason, Gordon, and Bisk 2018).</p>
<p>At inference time, the agent uses dialogue history as language input, and the environment actions in $A_{H}^{I}$ as past action input along with their associated visual observations. At each time step we execute the predicted action, with predicted object coordinate when applicable, in the simulator. The predicted action and resulting image observation are added to agent's input for the next timestep. The appendix details model hyperparameters.</p>
<p><strong>Results.</strong> Table 4 summarizes our adapted E.T. model performance on the EDH and TfD benchmarks.</p>
<p>We observe that all E.T. model conditions in EDH are significantly better than Random and Lang-Only condition on all splits on SR and GC, according to a paired two-sided Welch <em>t</em>-test with Bonferroni corrections. Compared to the Vision-Only baseline, the improvements of the E.T. models are statistically significant on unseen splits, but not on seen splits. Qualitatively, we observe that the Random baseline only succeeds on very short EDH instances that only include one object manipulation involving a large target object, for example placing an object on a countertop. The same is true of most of the successful trajectories of the Lang-Only baseline. The success rate of the Vision-Only baseline suggests that the E.T.-based models are not getting much purchase with language signal. Notably, E.T. performs well below its success rates on ALFRED, where it achieves 38.24% on the ALFRED test-seen split and 8.57% on the ALFRED test-unseen split. Additionally, although there appears to be a small benefit from initializing the E.T. model with pretrained weights from ALFRED, these differences are not statistically significant. <em>TEACh</em> language is more complex, involving multiple speakers, irrelevant phatic utterances, and dialogue anaphora.</p>
<p>E.T. model performance on TfD is poor but non-zero, unlike a Random baseline. We do not perform additional ablations for TfD given the low initial performance. Notably, in addition to the complexity of language, TfD instances have substantially longer average trajectory length (~130) than those in ALFRED (~50).</p>
<h3>5.2 Rule-based Agents for TATC</h3>
<p>In benchmarks like ALFRED, a PDDL (Ghallab et al. 1998) planner can be used to determine what actions are necessary to solve relatively simple tasks. In VLN, simple search algorithms yield the shortest paths to goals. Consequently, some language-guided visual task models build a semantic representation of the environment, then learn a hierarchical policy to execute such planner-style goals (Blukis et al. 2021).</p>
<p>Inspired by such planning-based solutions, we attempted to write a pair of rule-based <em>Commander</em> and <em>Follower</em> agents to tackle the TATC benchmark. In a loop, the rule-based <em>Commander</em> executes a Progress Check action, then forms a language utterance to the <em>Follower</em> consisting of navigation and object interaction actions needed to accomplish the next sub-goal in the response. Each sub-goal needs to be identified by the language template used to describe it, then a hand-crafted policy must be created for the rule-based <em>Commander</em> to reference. For example, for the Pot All X On Y task, all sub-goals are of the form "X needs to be on some Y" for a particular instance of object X, and so a rule-based policy can be expressed as "navigate to the X instance, pick up the X instance, navigate to Y, put X down on Y." <em>Commander</em> utterances are simplified to se-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">EDH Validation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">SR [TLW]</td>
<td style="text-align: center;">GC [TLW]</td>
<td style="text-align: center;">SR [TLW]</td>
<td style="text-align: center;">GC [TLW]</td>
<td style="text-align: center;">SR [TLW]</td>
<td style="text-align: center;">GC [TLW]</td>
<td style="text-align: center;">SR [TLW]</td>
<td style="text-align: center;">GC [TLW]</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$0.82[0.62]$</td>
<td style="text-align: center;">$0.17[0.2]$</td>
<td style="text-align: center;">$1.54[0.55]$</td>
<td style="text-align: center;">$0.04[-0.16]$</td>
<td style="text-align: center;">$0.6[0.09]$</td>
<td style="text-align: center;">$0.25[0.24]$</td>
<td style="text-align: center;">$1.9[0.94]$</td>
<td style="text-align: center;">$0.17[0.06]$</td>
</tr>
<tr>
<td style="text-align: center;">Lang</td>
<td style="text-align: center;">$3.12[0.27]$</td>
<td style="text-align: center;">$1.84[1.25]$</td>
<td style="text-align: center;">$4.0[1.19]$</td>
<td style="text-align: center;">$3.93[4.34]$</td>
<td style="text-align: center;">$4.2[1.0]$</td>
<td style="text-align: center;">$2.79[2.71]$</td>
<td style="text-align: center;">$4.01[0.63]$</td>
<td style="text-align: center;">$4.66[4.06]$</td>
</tr>
<tr>
<td style="text-align: center;">Vision</td>
<td style="text-align: center;">$8.88[0.89]$</td>
<td style="text-align: center;">$8.79[2.24]$</td>
<td style="text-align: center;">$5.68[1.07]$</td>
<td style="text-align: center;">$4.99[3.91]$</td>
<td style="text-align: center;">$3.45[0.79]$</td>
<td style="text-align: center;">$2.45[1.82]$</td>
<td style="text-align: center;">$6.44[0.87]$</td>
<td style="text-align: center;">$6.95[4.2]$</td>
</tr>
<tr>
<td style="text-align: center;">E.T.</td>
<td style="text-align: center;">$\mathbf{9 . 0 5}[\mathbf{1 . 2}]$</td>
<td style="text-align: center;">$\mathbf{9 . 0 5}[\mathbf{4 . 1 7}]$</td>
<td style="text-align: center;">$\mathbf{1 3 . 4 9 [ 3 . 6 9 ]}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 9 7 [ 1 2 . 1 5 ]}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 1 6 [ 2 . 4 8 ]}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 9 6 [ 6 . 4 1 ]}$</td>
<td style="text-align: center;">$\mathbf{9 . 6 2 [ 2 . 5 2 ]}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 4 9 [ 7 . 6 4 ]}$</td>
</tr>
<tr>
<td style="text-align: center;">+H</td>
<td style="text-align: center;">$12.5[1.78]$</td>
<td style="text-align: center;">$16.96[5.61]$</td>
<td style="text-align: center;">$12.19[2.9]$</td>
<td style="text-align: center;">$12.36[10.57]$</td>
<td style="text-align: center;">$15.62[1.56]$</td>
<td style="text-align: center;">$17.57[5.66]$</td>
<td style="text-align: center;">$6.66[0.46]$</td>
<td style="text-align: center;">$8.19[3.9]$</td>
</tr>
<tr>
<td style="text-align: center;">+A</td>
<td style="text-align: center;">$8.88[1.14]$</td>
<td style="text-align: center;">$9.1[3.49]$</td>
<td style="text-align: center;">$14.01[3.97]$</td>
<td style="text-align: center;">$13.35[12.28]$</td>
<td style="text-align: center;">$10.06[1.3]$</td>
<td style="text-align: center;">$9.21[4.28]$</td>
<td style="text-align: center;">$8.82[1.14]$</td>
<td style="text-align: center;">$9.68[5.53]$</td>
</tr>
<tr>
<td style="text-align: center;">+S</td>
<td style="text-align: center;">$7.73[0.93]$</td>
<td style="text-align: center;">$7.77[3.41]$</td>
<td style="text-align: center;">$13.22[3.67]$</td>
<td style="text-align: center;">$13.01[11.91]$</td>
<td style="text-align: center;">$9.76[0.95]$</td>
<td style="text-align: center;">$8.62[3.73]$</td>
<td style="text-align: center;">$8.82[1.06]$</td>
<td style="text-align: center;">$9.62[5.52]$</td>
</tr>
<tr>
<td style="text-align: center;">$+\mathrm{H}+\mathrm{A}$</td>
<td style="text-align: center;">$9.38[1.27]$</td>
<td style="text-align: center;">$9.93[4.38]$</td>
<td style="text-align: center;">$13.45[3.14]$</td>
<td style="text-align: center;">$13.42[11.17]$</td>
<td style="text-align: center;">$10.36[1.3]$</td>
<td style="text-align: center;">$8.45[3.54]$</td>
<td style="text-align: center;">$8.16[0.89]$</td>
<td style="text-align: center;">$7.7[4.58]$</td>
</tr>
<tr>
<td style="text-align: center;">$+\mathrm{H}+\mathrm{S}$</td>
<td style="text-align: center;">$11.18[0.97]$</td>
<td style="text-align: center;">$10.55[4.48]$</td>
<td style="text-align: center;">$13.26[2.97]$</td>
<td style="text-align: center;">$12.93[10.59]$</td>
<td style="text-align: center;">$10.96[1.78]$</td>
<td style="text-align: center;">$11.02[4.98]$</td>
<td style="text-align: center;">$6.66[1.02]$</td>
<td style="text-align: center;">$7.8[4.2]$</td>
</tr>
<tr>
<td style="text-align: center;">TfD Validation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TfD Test</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rand</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
<td style="text-align: center;">$0.00[0.00]$</td>
</tr>
<tr>
<td style="text-align: center;">E.T.</td>
<td style="text-align: center;">$\mathbf{1 . 0 2 [ 0 . 1 7 ]}$</td>
<td style="text-align: center;">$\mathbf{1 . 4 2 [ 4 . 8 2 ]}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 [ 0 . 1 2 ]}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 [ 0 . 5 9 ]}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 [ 0 . 2 3 ]}$</td>
<td style="text-align: center;">$\mathbf{1 . 6 0 [ 6 . 4 6 ]}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 [ 0 . 0 4 ]}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 [ 2 . 5 0 ]}$</td>
</tr>
</tbody>
</table>
<p>Table 4: E.T. outperforms random and unimodal baselines (bold). We ablate history loss (H), initializing with ALFRED (A), and initializing with ALFRED synthetic language (S). Metrics are success rate (SR) and goal condition success rate (GC). Trajectory length weighted metrics are included in [ brackets ]. All values are percentages. For all metrics, higher is better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task <br> (Shrtnd)</th>
<th style="text-align: center;">Success <br> Rate</th>
<th style="text-align: center;">Rule Agent <br> Actions/Session</th>
<th style="text-align: center;">Human <br> Actions/Session</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PLANT</td>
<td style="text-align: center;">26.70</td>
<td style="text-align: center;">$230.26 \pm 54.65$</td>
<td style="text-align: center;">$67.93 \pm 40.70$</td>
</tr>
<tr>
<td style="text-align: left;">COFFEE</td>
<td style="text-align: center;">54.55</td>
<td style="text-align: center;">$120.24 \pm 66.55$</td>
<td style="text-align: center;">$72.29 \pm 50.85$</td>
</tr>
<tr>
<td style="text-align: left;">CLEAN</td>
<td style="text-align: center;">52.98</td>
<td style="text-align: center;">$182.38 \pm 79.84$</td>
<td style="text-align: center;">$96.92 \pm 71.31$</td>
</tr>
<tr>
<td style="text-align: left;">ALL X Y</td>
<td style="text-align: center;">52.91</td>
<td style="text-align: center;">$126.82 \pm 64.75$</td>
<td style="text-align: center;">$103.53 \pm 80.97$</td>
</tr>
<tr>
<td style="text-align: left;">BOIL</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$130.13 \pm 94.80$</td>
</tr>
<tr>
<td style="text-align: left;">TOAST</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$136.11 \pm 70.73$</td>
</tr>
<tr>
<td style="text-align: left;">N SLICES</td>
<td style="text-align: center;">22.51</td>
<td style="text-align: center;">$248.77 \pm 98.57$</td>
<td style="text-align: center;">$146.23 \pm 113.96$</td>
</tr>
<tr>
<td style="text-align: left;">X ONE Y</td>
<td style="text-align: center;">50.98</td>
<td style="text-align: center;">$150.09 \pm 97.12$</td>
<td style="text-align: center;">$147.80 \pm 104.45$</td>
</tr>
<tr>
<td style="text-align: left;">COOKED</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">$424.25 \pm 135.57$</td>
<td style="text-align: center;">$189.26 \pm 87.90$</td>
</tr>
<tr>
<td style="text-align: left;">SNDWCH</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$241.61 \pm 100.86$</td>
</tr>
<tr>
<td style="text-align: left;">SALAD</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">$351.20 \pm 82.09$</td>
<td style="text-align: center;">$253.94 \pm 130.09$</td>
</tr>
<tr>
<td style="text-align: left;">BFAST</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$359.90 \pm 162.33$</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">$161.54 \pm 92.00$</td>
<td style="text-align: center;">$164.65 \pm 130.89$</td>
</tr>
</tbody>
</table>
<p>Table 5: Rule-based agent policies were expansive enough to solve some simple tasks about half the time, while being unable to solve most compositional tasks at all. Note that TATC performance is not directly comparable to EDH or TfD due to two-agent modeling in TATC.
quences of action names with a one-to-one mapping to Follower actions to execute, with interaction actions including $(x, y)$ screen click positions to select objects. The rule-based agents perform no learning.</p>
<p>Table 5 summarizes the success rate of these rule-based agents across task types. Note that for the tasks Boil Potato, Make Plate of Toast, Make Sandwich, and BREAKFAST, sub-goal policies were not successfully developed. The rule-based agents represent about 150 hours of engineering work to hand-craft subgoal policies. While success rates could certainly be increased by increasing subgoal policy coverage and handling simulation corner cases, it is clear that, unlike ALFRED and navigation-only tasks,
a planner-based solution is not reasonable for TEACh data and the TATC benchmark. The appendix contains detailed implementation information about the rule-based agents.</p>
<h2>6 Conclusions and Future Work</h2>
<p>We introduce Task-driven Embodied Agents that Chat (TEACh), a dataset of over 3000 situated dialogues in which a human Commander and human Follower collaborate in natural language to complete household tasks in the AI2THOR simulation environment. TEACh contains dialogue phenomena related to grounding dialogue in objects and actions in the environment, varying levels of instruction granularity, and interleaving of utterances between speakers in the absence of enforced turn taking. We also introduce a task definition language that is extensible to new tasks and even other simulators. We propose three benchmarks based on TEACh. To study Follower models, we define the Execution from Dialogue History (EDH) and Trajectory from Dialogue (TfD) benchmarks, and evaluate an adapted Episodic Transformer (Pashevich, Schmid, and Sun 2021) as an initial baseline. To study the potential of Commander and Follower models, we define the Two-Agent Task Completion benchmark, and explore the difficulty of defining rule-based agents from TEACh data.</p>
<p>In future, we will apply other ALFRED modeling approaches (Blukis et al. 2021; Kim et al. 2021; Zhang and Chai 2021; Suglia et al. 2021) to the EDH and TfD Follower model benchmarks. However, TEACh requires learning several different tasks, all of which are more complex than the simple tasks in ALFRED. Models enabling few shot generalization to new tasks will be critical for TEACh Follower agents. For Commander models, a starting point would be to train a Speaker model (Fried et al. 2018) on TEACh sessions. We are excited to explore human-in-the-loop evaluation of Commander and Follower models developed for TATC.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Ron Rezac, Shui Hu, Lucy Hu, Hangjie Shi for their assistance with the data and code release, and Sijia Liu for assistance with data cleaning. We would also like to thank Nicole Chartier, Savanna Stiff, Ana Sanchez, Ben Kelk, Joel Sachar, Govind Thattai, Gaurav Sukhatme, Joel Chengottusseriyil, Tony Bissell, Qiaozi Gao, Kaixiang Lin, Karthik Gopalakrishnan, Alexandros Papangelis, Yang Liu, Mahdi Namazifar, Behnam Hedayatnia, Di Jin, Seokhwan Kim and Nikko Strom for feedback and suggestions over the course of the project.</p>
<h2>References</h2>
<p>Abramson, J.; Ahuja, A.; Brussee, A.; Carnevale, F.; Cassin, M.; Clark, S.; Dudzik, A.; Georgiev, P.; Guy, A.; Harley, T.; Hill, F.; Hung, A.; Kenton, Z.; Landon, J.; Lillicrap, T.; Mathewson, K.; Muldal, A.; Santoro, A.; Savinov, N.; Varma, V.; Wayne, G.; Wong, N.; Yan, C.; and Zhu, R. 2020. Imitating Interactive Intelligence. arXiv.
Anderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.; Sünderhauf, N.; Reid, I.; Gould, S.; and van den Hengel, A. 2018. Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Arumugam, D.; Karamcheti, S.; Gopalan, N.; Williams, E. C.; Rhee, M.; Wong, L. L.; and Tellex, S. 2018. Grounding Natural Language Instructions to Semantic Goal Representations for Abstraction and Generalization. Autonomous Robots.
Bisk, Y.; Holtzman, A.; Thomason, J.; Andreas, J.; Bengio, Y.; Chai, J.; Lapata, M.; Lazaridou, A.; May, J.; Nisnevich, A.; Pinto, N.; and Turian, J. 2020. Experience Grounds Language. In Empirical Methods in Natural Language Processing (EMNLP).
Bisk, Y.; Shih, K.; Choi, Y.; and Marcu, D. 2018. Learning Interpretable Spatial Operations in a Rich 3D Blocks World. In Proceedings of the Thirty Second AAAI Conference on Artificial Intelligence (AAAI), volume 32.
Blukis, V.; Misra, D.; Knepper, R. A.; and Artzi, Y. 2018. Mapping Navigation Instructions to Continuous Control Actions with Position Visitation Prediction. In Proceedings of the Conference on Robot Learning (CoRL).
Blukis, V.; Paxton, C.; Fox, D.; Garg, A.; and Artzi, Y. 2021. A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution. arXiv.
Chen, D.; and Mooney, R. 2011. Learning to Interpret Natural Language Navigation Instructions from Observations. In Proceedings of the Twenty Fifth AAAI Conference on Artificial Intelligence (AAAI), volume 25.
Chen, H.; Suhr, A.; Misra, D.; Snavely, N.; and Artzi, Y. 2019. Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 12538-12547.</p>
<p>Chi, T.-C.; Shen, M.; Eric, M.; Kim, S.; and Hakkani-Tür, D. 2020. Just Ask: An Interactive Learning Framework for Vision and Language Navigation. In Proceedings of the AAAI Conference on Artificial Intelligence.
Fried, D.; Hu, R.; Cirik, V.; Rohrbach, A.; Andreas, J.; Morency, L.-P.; Berg-Kirkpatrick, T.; Saenko, K.; Klein, D.; and Darrell, T. 2018. Speaker-Follower Models for Vision-and-Language Navigation. In Neural Information Processing Systems (NeurIPS).
Ghallab, M.; Howe, A.; Knoblock, C.; McDermott, D.; Ram, A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL The Planning Domain Definition Language. Yale Center for Computational Vision and Control.
Harnad, S. 1990. The Symbol Grounding Problem. Physica D: Nonlinear Phenomena, 42(1-3): 335-346.
He, K.; Gkioxari, G.; Dollár, P.; and Girshick, R. B. 2017. Mask R-CNN. International Conference on Computer Vision (ICCV).
Kim, B.; Bhambri, S.; Singh, K. P.; Mottaghi, R.; and Choi, J. 2021. Agent with the Big Picture: Perceiving Surroundings for Interactive Instruction Following. In Embodied AI Workshop CVPR.
Kim, H.; Zala, A.; Burri, G.; Tan, H.; and Bansal, M. 2020. ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments. In Findings of the Association for Computational Linguistics: EMNLP 2020.
Kollar, T.; Tellex, S.; Walter, M. R.; Huang, A.; Bachrach, A.; Hemachandra, S.; Brunskill, E.; Banerjee, A.; Roy, D.; Teller, S.; et al. 2013. Generalized Grounding Graphs: A Probabilistic Framework for Understanding Grounded Language. Journal of Artificial Intelligence Research.
Kolve, E.; Mottaghi, R.; Han, W.; VanderBilt, E.; Weihs, L.; Herrasti, A.; Gordon, D.; Zhu, Y.; Gupta, A.; and Farhadi, A. 2017. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv preprint arXiv:1712.05474.
MacMahon, M.; Stankiewicz, B.; and Kuipers, B. 2006. Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions. In Proceedings of the Twentieth AAAI Conference on Artifial Intelligence (AAAI), volume 20.
Matuszek, C.; Herbst, E.; Zettlemoyer, L.; and Fox, D. 2013. Learning to Parse Natural Language Commands to a Robot Control System. In Experimental Robotics, 403-415. Springer.
Mei, H.; Bansal, M.; and Walter, M. 2016. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), volume 30.
Misra, D. K.; Bennett, A.; Blukis, V.; Niklasson, E.; Shatkhin, M.; and Artzi, Y. 2018. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J., eds., Empirical Methods in Natural (EMNLP).
Narayan-Chen, A.; Jayannavar, P.; and Hockenmaier, J. 2019. Collaborative Dialogue in Minecraft. In Association for Computational Linguistics (ACL).</p>
<p>Nguyen, K.; and Daumé III, H. 2019. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning. In Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Pashevich, A.; Schmid, C.; and Sun, C. 2021. Episodic Transformer for Vision-and-Language Navigation. arXiv preprint arXiv:2105.06453.
Puig, X.; Ra, K.; Boben, M.; Li, J.; Wang, T.; Fidler, S.; and Torralba, A. 2018. VirtualHome: Simulating Household Activities via Programs. In Computer Vision and Pattern Recognition (CVPR).
Roman, H. R.; Bisk, Y.; Thomason, J.; Celikyilmaz, A.; and Gao, J. 2020. RMM: A Recursive Mental Model for Dialog Navigation. In Findings of Empirical Methods in Natural Language Processing (EMNLP Findings).
Shah, R.; Wild, C.; Wang, S. H.; Alex, N.; Houghton, B.; Guss, W.; Mohanty, S.; Kanervisto, A.; Milani, S.; Topin, N.; et al. 2021. The MineRL BASALT Competition on Learning from Human Feedback. arXiv preprint arXiv:2107.01969.
Shridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y.; Han, W.; Mottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In Computer Vision and Pattern Recognition (CVPR).
Shrivastava, A.; Gopalakrishnan, K.; Liu, Y.; Piramuthu, R.; Tür, G.; Parikh, D.; and Hakkani-Tür, D. 2021. VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. arXiv preprint arXiv:2105.11589.
Suglia, A.; Gao, Q.; Thomason, J.; Thattai, G.; and Sukhatme, G. 2021. Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. arXiv preprint arXiv:2108.04927.
Suhr, A.; Yan, C.; Schluger, J.; Yu, S.; Khader, H.; Mouallem, M.; Zhang, I.; and Artzi, Y. 2019. Executing Instructions in Situated Collaborative Interactions. In Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Tellex, S.; Knepper, R. A.; Li, A.; Roy, N.; and Rus, D. 2016. Asking for Help Using Inverse Semantics. In Robotics: Science and Systems Conference (RSS).
Thomason, J.; Gordon, D.; and Bisk, Y. 2018. Shifting the Baseline: Single Modality Performance on Visual Navigation \&amp; QA. arXiv preprint arXiv:1811.00613.
Thomason, J.; Murray, M.; Cakmak, M.; and Zettlemoyer, L. 2019. Vision-and-Dialog Navigation. In Conference on Robot Learning (CoRL).
Thomason, J.; Padmakumar, A.; Sinapov, J.; Walker, N.; Jiang, Y.; Yedidsion, H.; Hart, J.; Stone, P.; and Mooney, R. 2020. Jointly improving parsing and perception for natural language commands through human-robot dialog. Journal of Artificial Intelligence Research, 67: 327-374.
Zhang, Y.; and Chai, J. 2021. Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring. arXiv preprint arXiv:2106.03427.</p>
<p>Zhu, W.; Hu, H.; Chen, J.; Deng, Z.; Jain, V.; Ie, E.; and Sha, F. 2020. BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps. In Association for Computational Linguistics (ACL).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We follow ALFRED in using a macro-, rather than microaverage for Goal-Conditioned Success Rate.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://github.com/alexa/teach&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
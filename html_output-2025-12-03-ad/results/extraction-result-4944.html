<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4944 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4944</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4944</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab" target="_blank">Binding Language Models in Symbolic Languages</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Binder is a training-free neural-symbolic framework that maps the task input to a program, which allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions.</p>
                <p><strong>Paper Abstract:</strong> Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4944.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4944.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex BINDER (diverse ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex used with the BINDER framework using diverse program generation and majority-vote ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary method: OpenAI Codex (code-davinci-002) is prompted to generate multiple candidate BINDER programs (sampling_n generations, varied temperature) which are executed and aggregated via majority-vote (with variants) to produce final answers; Codex is also used to realize API calls during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A code‑pretrained variant of GPT-3 used both as a semantic parser (program generation) and as the underlying model for API calls; used with in‑context few-shot prompting (14 shots for main tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diverse program generation + majority voting (BINDER)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Parsing stage: sample multiple candidate BINDER programs (sampling_n = 20 for WikiTQ/MMQA, 50 for TabFact) with nonzero temperature (e.g., 0.4 or 0.6) to induce diverse program variants; Execution: run each program, collect answers, and apply majority voting (optionally reweighted: answer-biased or program-biased) to choose final answer. API-call outputs use deterministic decoding (temperature=0.0) during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WikiTQ), TabFACT, MMQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Structured knowledge grounding (table QA and table fact verification) and multimodal QA across text, tables and images; tasks require compositional operations and sometimes external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>WikiTQ execution accuracy: dev 65.0%, test 64.6%; TabFACT small test accuracy: 85.1% (86.0% with few-shot retriever); MMQA dev F1 57.1 / EM 51.0 (64.5 / 58.1 with oracle retriever).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Codex SQL (semantic parsing): WikiTQ dev 60.2% / test 61.1%; TabFACT 80.7%. Codex end-to-end QA: WikiTQ dev 50.5% / test 48.7%; TabFACT 72.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating diverse candidate programs and ensembling their executed answers substantially improves performance over single-run end-to-end decoding and over standard semantic parsing with the same LM. BINDER's diverse program generation + voting yields the best reported few-shot (no fine-tuning) results on WikiTQ and TabFACT in this paper, and gives the largest gains on questions not solvable by pure SQL (program-unsolvable subset).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit case reported where the diverse ensemble underperforms the similar/deterministic approach overall; however, the paper notes only modest additional gain from retrieving similar in-context exemplars on TabFACT (0.9% improvement when using a few-shot retriever), indicating diminishing returns from certain sample-selection strategies. Also execution-phase LM calls are run deterministically (temperature=0.0) to avoid variability in API-call outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Binding Language Models in Symbolic Languages', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4944.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4944.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex end-to-end QA (single output)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex used to directly generate answers in an end-to-end few-shot setting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline in which Codex is prompted (few-shot) to output final answers directly from the inputs (question + full table/context) without generating intermediate executable programs or API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-trained GPT-3 variant used with few-shot prompting; in the end-to-end QA baseline the model receives full table content and produces answers directly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Single-shot end-to-end decoding (deterministic/standard few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Provide few-shot examples and the full input to Codex and ask it to output the final answer directly (no program generation or multiple sampled programs aggregated); typically uses full table input in examples to enable direct QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTQ, TabFACT, MMQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same structured and multimodal QA/fact verification tasks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>WikiTQ execution accuracy: dev 50.5%, test 48.7%; TabFACT small test accuracy: 72.6%; MMQA dev F1 55.4 / EM 48.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to Codex BINDER (diverse ensemble): WikiTQ +14.5 pp (dev), TabFACT +12.5 pp; Codex SQL lies between end-to-end and BINDER.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end Codex is substantially weaker than the BINDER diverse-program + voting approach and weaker than semantic parsing with Codex for structured table grounding; shows brittleness on large or noisy tables and lower interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>End-to-end QA performed better than SQL on some program-unsolvable questions subset (end-to-end QA had 40.3% on program-unsolvable WikiTQ dev vs SQL 31.2%), indicating that direct LM knowledge can sometimes help on questions outside symbolic grammar coverage, but BINDER still outperforms end-to-end on that subset (41.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Binding Language Models in Symbolic Languages', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4944.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4944.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex SQL (semantic parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex used to generate SQL programs (semantic parsing) and execute them deterministically</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline semantic-parsing approach where Codex generates standard SQL programs (no BINDER API calls) which are executed to obtain answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-pretrained GPT-3 variant employed as a semantic parser to map natural language questions to SQL; generation likely uses few-shot prompting and sampling for candidates (parsing hyperparameters provided in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Semantic parsing to SQL (single or sampled program generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate SQL (standard grammar) from question + table and execute; in the paper Codex SQL is evaluated as a baseline (parsing may use sampling but without neural API augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTQ, TabFACT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table question answering and fact verification where exact symbolic programs can be executed on tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>WikiTQ execution accuracy: dev 60.2%, test 61.1%; TabFACT small test accuracy: 80.7%. On program-unsolvable WikiTQ dev subset Codex SQL: 31.2% (vs BINDER 41.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to Codex BINDER: BINDER dev +4.8 pp overall on WikiTQ (65.0% vs 60.2%) and +10.1 pp on program-unsolvable subset (41.3% vs 31.2%). Compared to end-to-end QA: SQL substantially better (+9.7 pp on WikiTQ dev).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic parsing with Codex is a strong baseline and substantially better than end-to-end QA for structured table grounding, but is limited by the grammar coverage of the target language (SQL) and thus underperforms BINDER on questions requiring external knowledge or unsupported operations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SQL achieves reasonable performance on program-solvable questions (e.g., 68.4% on program-solvable subset) and outperforms end-to-end in that subset; however, SQL shows higher spurious program rates on program-unsolvable cases (33% spurious vs BINDER 12% in a sampled analysis), indicating susceptibility to accidental correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Binding Language Models in Symbolic Languages', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4944.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4944.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority voting / sampling strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling multiple candidate parses and aggregating executed answers via majority voting (including answer-biased and program-biased weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensembling method used in BINDER: generate n candidate programs (via stochastic decoding), execute them, and aggregate answers by majority vote with optional reweighting heuristics to bias toward certain answers or program types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002) used to produce candidate programs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex is sampled multiple times during parsing (sampling_n set per dataset) to produce a diverse set of candidate BINDER or SQL programs; execution uses deterministic decoding for API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Sampling + majority voting (self-consistency-like ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Increase diversity by sampling multiple program candidates (sampling_n = 20 for WikiTQ/MMQA, 50 for TabFACT) with nonzero temperature; collect executed outputs and select final answer via majority voting. Variants: answer-biased weighting (e.g., weight 'entailment' answers more on TabFACT) and program-biased weighting (favoring BINDER program outputs). The paper links this to MBR-EXEC style voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTQ, TabFACT, MMQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring robust reasoning over tables/multimodal inputs, benefitting from ensembling across different programmatic decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When applied in BINDER: contributed to final WikiTQ dev 65.0% (sampling_n=20), TabFACT 85.1% (sampling_n=50 for TabFACT parsing). The paper reports that increasing sampling_n and temperature for TabFACT helps determine final answer better in binary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Implicit comparison: single-run/deterministic parsing or end-to-end decoding yields lower performance (see Codex SQL and end-to-end QA numbers); paper reports TabFACT particularly benefits from larger sampling_n and higher temperature (empirically chosen).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling diverse candidate program decompositions and aggregating their executed answers via majority voting is an effective way to boost accuracy and robustness; for binary tasks (TabFACT) increased sampling (higher sampling_n and temperature) improved reliability of final answer. Weighted voting (answer-biased or program-biased) is used to correct for known biases of generation distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper does not present direct ablation numbers contrasting sampling_n extremes in all cases; it notes modest gains from retrieval of similar exemplars on TabFACT (0.9%) and indicates that effective sample selection remains an open challenge. No explicit report where sampling+voting hurts performance relative to single-deterministic parse in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Binding Language Models in Symbolic Languages', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4944.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4944.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (generating intermediate natural-language reasoning steps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as an alternative large-LM reasoning paradigm where the model generates intermediate reasoning steps; acknowledged as improving reasoning but suffering from unreliability/uncontrollability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General LLMs that can be prompted to produce chain-of-thought style intermediate natural-language reasoning traces (cited works: Wei et al., 2022; Kojima et al., 2022; Chowdhery et al., 2022a).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Encourage model to produce explicit step-by-step natural language reasoning traces before giving an answer; diversity comes from varied generated chains and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General reasoning benchmarks (cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where chain-of-thought has been applied to elicit multi-step reasoning in LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No performance numbers reported in this paper (only cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a model-generated intermediate representation approach that improves complex reasoning in some settings but suffers from unreliability and uncontrollability relative to structured/executable program representations; BINDER positions itself as an interpretable executable alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper does not experimentally compare chain-of-thought to BINDER; it only notes general downsides (unreliability/uncontrollability) from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Binding Language Models in Symbolic Languages', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
                <li>MBR-EXEC: Minimum Bayes Risk Execution for Programmatic Reasoning (Shi et al., 2022) <em>(Rating: 2)</em></li>
                <li>Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4944",
    "paper_id": "paper-f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Codex BINDER (diverse ensemble)",
            "name_full": "Codex used with the BINDER framework using diverse program generation and majority-vote ensembling",
            "brief_description": "The paper's primary method: OpenAI Codex (code-davinci-002) is prompted to generate multiple candidate BINDER programs (sampling_n generations, varied temperature) which are executed and aggregated via majority-vote (with variants) to produce final answers; Codex is also used to realize API calls during execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002)",
            "model_description": "A code‑pretrained variant of GPT-3 used both as a semantic parser (program generation) and as the underlying model for API calls; used with in‑context few-shot prompting (14 shots for main tasks).",
            "reasoning_method_name": "Diverse program generation + majority voting (BINDER)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Parsing stage: sample multiple candidate BINDER programs (sampling_n = 20 for WikiTQ/MMQA, 50 for TabFact) with nonzero temperature (e.g., 0.4 or 0.6) to induce diverse program variants; Execution: run each program, collect answers, and apply majority voting (optionally reweighted: answer-biased or program-biased) to choose final answer. API-call outputs use deterministic decoding (temperature=0.0) during execution.",
            "task_name": "WikiTableQuestions (WikiTQ), TabFACT, MMQA",
            "task_description": "Structured knowledge grounding (table QA and table fact verification) and multimodal QA across text, tables and images; tasks require compositional operations and sometimes external knowledge.",
            "performance": "WikiTQ execution accuracy: dev 65.0%, test 64.6%; TabFACT small test accuracy: 85.1% (86.0% with few-shot retriever); MMQA dev F1 57.1 / EM 51.0 (64.5 / 58.1 with oracle retriever).",
            "comparison_with_other_method": true,
            "performance_other_method": "Codex SQL (semantic parsing): WikiTQ dev 60.2% / test 61.1%; TabFACT 80.7%. Codex end-to-end QA: WikiTQ dev 50.5% / test 48.7%; TabFACT 72.6%.",
            "key_findings": "Generating diverse candidate programs and ensembling their executed answers substantially improves performance over single-run end-to-end decoding and over standard semantic parsing with the same LM. BINDER's diverse program generation + voting yields the best reported few-shot (no fine-tuning) results on WikiTQ and TabFACT in this paper, and gives the largest gains on questions not solvable by pure SQL (program-unsolvable subset).",
            "counter_examples_or_negative_results": "No explicit case reported where the diverse ensemble underperforms the similar/deterministic approach overall; however, the paper notes only modest additional gain from retrieving similar in-context exemplars on TabFACT (0.9% improvement when using a few-shot retriever), indicating diminishing returns from certain sample-selection strategies. Also execution-phase LM calls are run deterministically (temperature=0.0) to avoid variability in API-call outputs.",
            "uuid": "e4944.0",
            "source_info": {
                "paper_title": "Binding Language Models in Symbolic Languages",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Codex end-to-end QA (single output)",
            "name_full": "OpenAI Codex used to directly generate answers in an end-to-end few-shot setting",
            "brief_description": "A baseline in which Codex is prompted (few-shot) to output final answers directly from the inputs (question + full table/context) without generating intermediate executable programs or API calls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002)",
            "model_description": "Code-trained GPT-3 variant used with few-shot prompting; in the end-to-end QA baseline the model receives full table content and produces answers directly.",
            "reasoning_method_name": "Single-shot end-to-end decoding (deterministic/standard few-shot)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Provide few-shot examples and the full input to Codex and ask it to output the final answer directly (no program generation or multiple sampled programs aggregated); typically uses full table input in examples to enable direct QA.",
            "task_name": "WikiTQ, TabFACT, MMQA",
            "task_description": "Same structured and multimodal QA/fact verification tasks as above.",
            "performance": "WikiTQ execution accuracy: dev 50.5%, test 48.7%; TabFACT small test accuracy: 72.6%; MMQA dev F1 55.4 / EM 48.0.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to Codex BINDER (diverse ensemble): WikiTQ +14.5 pp (dev), TabFACT +12.5 pp; Codex SQL lies between end-to-end and BINDER.",
            "key_findings": "End-to-end Codex is substantially weaker than the BINDER diverse-program + voting approach and weaker than semantic parsing with Codex for structured table grounding; shows brittleness on large or noisy tables and lower interpretability.",
            "counter_examples_or_negative_results": "End-to-end QA performed better than SQL on some program-unsolvable questions subset (end-to-end QA had 40.3% on program-unsolvable WikiTQ dev vs SQL 31.2%), indicating that direct LM knowledge can sometimes help on questions outside symbolic grammar coverage, but BINDER still outperforms end-to-end on that subset (41.3%).",
            "uuid": "e4944.1",
            "source_info": {
                "paper_title": "Binding Language Models in Symbolic Languages",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Codex SQL (semantic parsing)",
            "name_full": "OpenAI Codex used to generate SQL programs (semantic parsing) and execute them deterministically",
            "brief_description": "A baseline semantic-parsing approach where Codex generates standard SQL programs (no BINDER API calls) which are executed to obtain answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002)",
            "model_description": "Code-pretrained GPT-3 variant employed as a semantic parser to map natural language questions to SQL; generation likely uses few-shot prompting and sampling for candidates (parsing hyperparameters provided in appendix).",
            "reasoning_method_name": "Semantic parsing to SQL (single or sampled program generation)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Generate SQL (standard grammar) from question + table and execute; in the paper Codex SQL is evaluated as a baseline (parsing may use sampling but without neural API augmentation).",
            "task_name": "WikiTQ, TabFACT",
            "task_description": "Table question answering and fact verification where exact symbolic programs can be executed on tables.",
            "performance": "WikiTQ execution accuracy: dev 60.2%, test 61.1%; TabFACT small test accuracy: 80.7%. On program-unsolvable WikiTQ dev subset Codex SQL: 31.2% (vs BINDER 41.3%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to Codex BINDER: BINDER dev +4.8 pp overall on WikiTQ (65.0% vs 60.2%) and +10.1 pp on program-unsolvable subset (41.3% vs 31.2%). Compared to end-to-end QA: SQL substantially better (+9.7 pp on WikiTQ dev).",
            "key_findings": "Semantic parsing with Codex is a strong baseline and substantially better than end-to-end QA for structured table grounding, but is limited by the grammar coverage of the target language (SQL) and thus underperforms BINDER on questions requiring external knowledge or unsupported operations.",
            "counter_examples_or_negative_results": "SQL achieves reasonable performance on program-solvable questions (e.g., 68.4% on program-solvable subset) and outperforms end-to-end in that subset; however, SQL shows higher spurious program rates on program-unsolvable cases (33% spurious vs BINDER 12% in a sampled analysis), indicating susceptibility to accidental correctness.",
            "uuid": "e4944.2",
            "source_info": {
                "paper_title": "Binding Language Models in Symbolic Languages",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Majority voting / sampling strategy",
            "name_full": "Sampling multiple candidate parses and aggregating executed answers via majority voting (including answer-biased and program-biased weighting)",
            "brief_description": "An ensembling method used in BINDER: generate n candidate programs (via stochastic decoding), execute them, and aggregate answers by majority vote with optional reweighting heuristics to bias toward certain answers or program types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002) used to produce candidate programs",
            "model_description": "Codex is sampled multiple times during parsing (sampling_n set per dataset) to produce a diverse set of candidate BINDER or SQL programs; execution uses deterministic decoding for API calls.",
            "reasoning_method_name": "Sampling + majority voting (self-consistency-like ensemble)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Increase diversity by sampling multiple program candidates (sampling_n = 20 for WikiTQ/MMQA, 50 for TabFACT) with nonzero temperature; collect executed outputs and select final answer via majority voting. Variants: answer-biased weighting (e.g., weight 'entailment' answers more on TabFACT) and program-biased weighting (favoring BINDER program outputs). The paper links this to MBR-EXEC style voting.",
            "task_name": "WikiTQ, TabFACT, MMQA",
            "task_description": "Tasks requiring robust reasoning over tables/multimodal inputs, benefitting from ensembling across different programmatic decompositions.",
            "performance": "When applied in BINDER: contributed to final WikiTQ dev 65.0% (sampling_n=20), TabFACT 85.1% (sampling_n=50 for TabFACT parsing). The paper reports that increasing sampling_n and temperature for TabFACT helps determine final answer better in binary tasks.",
            "comparison_with_other_method": true,
            "performance_other_method": "Implicit comparison: single-run/deterministic parsing or end-to-end decoding yields lower performance (see Codex SQL and end-to-end QA numbers); paper reports TabFACT particularly benefits from larger sampling_n and higher temperature (empirically chosen).",
            "key_findings": "Sampling diverse candidate program decompositions and aggregating their executed answers via majority voting is an effective way to boost accuracy and robustness; for binary tasks (TabFACT) increased sampling (higher sampling_n and temperature) improved reliability of final answer. Weighted voting (answer-biased or program-biased) is used to correct for known biases of generation distributions.",
            "counter_examples_or_negative_results": "The paper does not present direct ablation numbers contrasting sampling_n extremes in all cases; it notes modest gains from retrieval of similar exemplars on TabFACT (0.9%) and indicates that effective sample selection remains an open challenge. No explicit report where sampling+voting hurts performance relative to single-deterministic parse in these tasks.",
            "uuid": "e4944.3",
            "source_info": {
                "paper_title": "Binding Language Models in Symbolic Languages",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (mention)",
            "name_full": "Chain-of-Thought prompting (generating intermediate natural-language reasoning steps)",
            "brief_description": "Mentioned in related work as an alternative large-LM reasoning paradigm where the model generates intermediate reasoning steps; acknowledged as improving reasoning but suffering from unreliability/uncontrollability.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "large language models (general)",
            "model_description": "General LLMs that can be prompted to produce chain-of-thought style intermediate natural-language reasoning traces (cited works: Wei et al., 2022; Kojima et al., 2022; Chowdhery et al., 2022a).",
            "reasoning_method_name": "Chain-of-Thought prompting",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Encourage model to produce explicit step-by-step natural language reasoning traces before giving an answer; diversity comes from varied generated chains and sampling.",
            "task_name": "General reasoning benchmarks (cited in related work)",
            "task_description": "Benchmarks where chain-of-thought has been applied to elicit multi-step reasoning in LMs.",
            "performance": "No performance numbers reported in this paper (only cited as related work).",
            "comparison_with_other_method": null,
            "performance_other_method": null,
            "key_findings": "Mentioned as a model-generated intermediate representation approach that improves complex reasoning in some settings but suffers from unreliability and uncontrollability relative to structured/executable program representations; BINDER positions itself as an interpretable executable alternative.",
            "counter_examples_or_negative_results": "The paper does not experimentally compare chain-of-thought to BINDER; it only notes general downsides (unreliability/uncontrollability) from prior work.",
            "uuid": "e4944.4",
            "source_info": {
                "paper_title": "Binding Language Models in Symbolic Languages",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2
        },
        {
            "paper_title": "MBR-EXEC: Minimum Bayes Risk Execution for Programmatic Reasoning (Shi et al., 2022)",
            "rating": 2
        },
        {
            "paper_title": "Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models",
            "rating": 1
        }
    ],
    "cost": 0.014270000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Binding Language Models in Symbolic Languages</h1>
<p>Zhoujun Cheng ${ }^{\star \text { (1) }}$ Tianbao Xie ${ }^{\star \text { a }}$ Peng Shi* Chengzu Li ${ }^{\text {a }}$ Rahul Nadkarni ${ }^{\text {a }}$<br>Yushi Hu ${ }^{\text {a }}$ Caiming Xiong ${ }^{\text {P }}$ Dragomir Radev ${ }^{\star}$ Mari Ostendorf ${ }^{\text {a }}$<br>Luke Zettlemoyer ${ }^{\text {A }}$ Noah A. Smith ${ }^{\text {A }}$ Tao Yu ${ }^{\text {A }}$<br>${ }^{\text {A }}$ The University of Hong Kong ${ }^{\circ}$ Shanghai Jiao Tong University ${ }^{\text {A }}$ University of Washington<br>${ }^{\text {® }}$ Allen Institute for AI ${ }^{\Delta}$ University of Waterloo ${ }^{\text { }}$ Salesforce Research ${ }^{\text {® }}$ Yale University ${ }^{\text {® }}$ Meta AI</p>
<h4>Abstract</h4>
<p>Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose BINDER, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few incontext exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. BINDER achieves state-of-the-art results on WIKITABLEQUESTIONS and TABFACT datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while BINDER only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/hkunlp/binder ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Performance on natural language processing tasks is dominated by neural end-to-end systems that directly map inputs to outputs (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020; Raffel et al., 2020, i.a.). These end-to-end approaches are flexible and easy-to-use while lacking interpretability and robustness. This stands in contrast to symbolic approaches that produce explicit intermediate representations such as logical forms, reasoning paths, or program code, which might then be executed to derive a final output (Zettlemoyer \&amp; Collins, 2005; Gulwani et al., 2017; Chen et al., 2019b, i.a.). The intermediate form produced by these the resulting execution makes them more robust to input changes. However, their semantic coverage is limited by the affordances of the grammar of the selected symbolic language (e.g., not being able to handle "North America?" in Fig. 1), leading to failures on real-world diverse questions, and the intermediate form annotations require expert knowledge and researcher labour.</p>
<p>A few works (Andreas et al., 2016; Gupta et al., 2019; Khot et al., 2021; Zhu et al., 2022, i.a.) have been proposed to combine neural modules and symbolic languages (neural-symbolic) to leverage advantages of both approaches. However, they require the elaborate human design of the symbolic language and the calibration of corresponding neural modules to tackle problems in a specific domain with large training data. More specifically, most of these works propose a task-specific symbolic language and corresponding modules that cover only limited semantic phenomena in a specific task and domain. Therefore, new languages and neural modules have to be introduced when adapting them</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of the BINDER pipeline of two stages: parsing and execution. (1) In the parsing stage, the language model (LM) maps the input to a BINDER program given the question and (optional) knowledge sources. The expressions with blue background in the program are API calls to acquire external results. (2) In the execution stage, an LM serves to realize the API calls given the prompt and the return values feed back into the original programming language. A deterministic program interpreter executes the program without API calls to derive the final answer.
to new tasks and domains. Their coverage is still restricted by the customized symbolic language and neural modules. Moreover, they call for various and large training data to ensure all modules are well trained. Therefore, we expect a neural-symbolic system that supports flexible neural module calls that will enable higher coverage for the symbolic language, while only requiring few annotations.</p>
<p>We propose BINDER, a training-free neural-symbolic framework that maps task inputs to an executable program in a programming language (e.g., SQL, Python) bound with a unified API to call language models (LMs; Brown et al., 2020; Chen et al., 2021) to perform versatile functionalities, i.e. a BINDER program(e.g., Binder-SQL, Binder-Python in Fig. 1), with only a few input-BINDER program annotations as in-context exemplars. More specifically, BINDER first prompts Codex, a code-pretrained of GPT-3, to parse a question input into a BINDER program, in which Codex has to decide (1) which parts in the input can be converted to the target programming language (question parts highlighted in grey in Fig. 1), (2) the corresponding task API calls (e.g., $f$ ("North America?"; Made_in)) to prompt Codex to resolve the other parts, and (3) where to insert the API calls in the BINDER program. Next, BINDER prompts Codex again to generate answers to the task API calls (given the generated task prompts), integrates the generated results back to the programming language. Specifically as in Fig. 1, the prompt (e.g., "North America?") and data (e.g., column Made_in) in API calls are fed into Codex, and the output is a new column answering the prompt based on the input data (i.e., yes/no of whether a country in Made_in column is from North America). Finally, the program with standard programming languages is executed the derive the final answer.</p>
<p>In summary, BINDER enables flexible functionality integration to the programming language to improve its coverage and requires only a few annotations. BINDER program replaces custom neural modules and task-specific languages with a unified prompting API call to Codex and general programming languages, respectively, to handle much more diverse task inputs in open domains without complex language and neural module design. BINDER is built on the advances of in-context learning with language models and does not require any training and large-scale annotations.</p>
<p>We demonstrate the effectiveness of the BINDER framework on WikiTableQuestions (WikiTQ; Pasupat \&amp; Liang, 2015) TabFACT (Chen et al., 2019a), two structured knowledge grounding datasets that require complex reasoning on the tables. Using Codex (Chen et al., 2021) as the LM, BINDER achieves state-of-the-art results on WikiTQ and TabFACT. Note that the previous state-of-the-art methods all require fine-tuning on more than 10 K annotated training examples or even massive amounts of task-related pretraining data, while our method requires only a dozen or so annotations without training. In further analysis, we find that BINDER provides the greatest performance gain on the questions that the original language grammar (SQL and Python) cannot support, indicating that BINDER effectively improves programming language coverage. We also demonstrate BINDER can be applied on multi-modal knowledge sources (text, table, images, and combined) with MulTIModALQA dataset (Talmor et al., 2021). Moreover, we show that BINDER, compared with end-to-end approaches, is more interpretable when debugging the model, more scalable to very large inputs, and more robust to noisy inputs.</p>
<h1>2 APPROACH</h1>
<p>Task Definition Given an NLP task that accepts a natural language question/statement $Q$ and optional context(s) $D$ (e.g., passages, tables, images, or a combination of the above) as inputs, the goal is to output an answer $A$ based on the inputs to respond to $Q$ correctly. For example, in passage question answering, $Q$ is a question about the passage(s) $D$; in table fact verification, $Q$ is a statement about the table(s) $D$.</p>
<h3>2.1 BINDER FRAMEWORK</h3>
<p>Overview The BINDER framework to solve NLP tasks is defined as follows: given a natural language input $Q$ and optional context(s) $D$ as the input, an executable BINDER program $Z$ is generated. Finally, the output answer $A$ is derived by executing $Z$ with a BINDER interpreter.</p>
<p>BINDER Parsing In the parsing stage, the input natural language $Q$ is parsed into a BINDER program $Z$. A BINDER program is an expression in a symbolic language that optionally includes API calls where the core symbolic language fails to provide a desired functionality. We define the API call in the program as function $f(\hat{Q} ; \hat{D})$ that accepts a question $\hat{Q}$ to be answered, and the context $\hat{D}$ to be queried on. Here $\hat{Q}$ is the unanswerable part of $Q$ with the programming language only and $\hat{D}$ is the relevant contexts in $D$ to answer $\hat{Q}$. For example, "North America?" in Fig 1 is a $\hat{Q}$, and its corresponding contexts $\hat{D}$ to answer $\hat{Q}$ is the column Made_in. Note $\hat{Q}=Q$ or $\hat{D}=D$ is also valid (if $\hat{Q}=Q$ and $\hat{D}=D$, the program is equivalent to solving the problem with an LM in end-to-end manner). The output of an API call $f(\hat{Q} ; \hat{D})$ is the answer to $\hat{Q}$, and it is represented as $a$ variable compatible with the symbolic language grammar so that the program can be executed.</p>
<p>BINDER Execution In the execution stage, the program $Z$ is executed by a BINDER interpreter to derive the answer $A$. The BINDER interpreter consists of a standard symbolic language interpreter and the model(s) realizing the API calls. The execution phase includes lexical analysis, syntax analysis, and program evaluation. In lexical and syntax analysis, $f(\hat{Q} ; \hat{D})$ is added as a new identifier in the grammar, and the program is parsed as an abstract syntax tree (AST) based on this extended grammar. In program evaluation, the API calls are evaluated by calling the underlying neural models. The API call output is saved as a variable compatible with the standard symbolic language grammar, and thus the program can be finally executed by an off-the-shelf symbolic language interpreter to derive the output. Specifically, BINDER extends the symbolic language production rules to facilitate parsing the BINDER program's AST. The AST is evaluated in bottom-up order so that nested API calls are also supported, which allows different degrees of decomposition and free combination of language models for the complex question. We provide more details about the grammar extension and a BINDER program AST in Appendix A.4.</p>
<h3>2.2 In-CONTEXT LEARNING FOR BINDER</h3>
<p>Much recent work uses large language models for in-context learning (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022b). Compared with fine-tuning, in-context learning (1) only takes a few annotations/demonstrations as a prompt, and (2) performs inference without training the model parameters, which are both longstanding issues of conventional semantic parsing. We base our experiments for BINDER on Codex, a code-pretrained version of GPT-3, which has been shown to perform proficiently on code generation tasks (Rajkumar et al., 2022; Chen et al., 2022) and even some weakly-related tasks like dialogue state tracking (Hu et al., 2022). We use Codex as both the semantic parser and the model to perform API call functionalities.</p>
<p>In the parsing stage, it is challenging to generate BINDER programs because their grammar is different from the original programming language grammar due to the inserted API calls. Thus, we take advantage of the few-shot generalization ability of Codex and find that it can learn the modified grammar effectively with only a small number of in-context examples. In the execution stage, Codex as the underlying LM serves to give the output to the API calls by concatenating the API call input, $\hat{Q}$ and $\hat{D}$ as the language model prompt (the prompt style is described in Section 3.1). The Codex output result(s) are stored as variables in the standard programming language so that a programming</p>
<p>language interpreter can execute on the combination of these variables and the rest part of the program.</p>
<p>Specifically, we apply in-context learning for BINDER in the following manner: the inputs are $k$ in-context exemplars of $\left{\left(Q_{i}, D_{i}, Z_{i}\right)\right}<em 1="1">{i=1}^{k}$ and the inference example $(Q, D)$. The $k$ examples should balance the trade-off between the diversity of question types and the model's maximum input capacity, which can either be manually selected (fixed) or automatically retrieved (dynamic) according to the inference example. The model outputs are $n$ candidate BINDER programs $\mathcal{Z}=\left{Z</em>$ (the voting method is similar to the one used by MBR-EXEC (Shi et al., 2022), which we elaborate on in Appendix A.3). The values of $k$ and $n$ are hyperparameters in the process above.}, \ldots, Z_{n}\right}$ that aim to solve the inference question $Q$. Next, the programs $\mathcal{Z}$ are executed by the BINDER interpreter producing $n$ answers $\mathcal{A}=\left{A_{1}, \ldots, A_{n}\right}$. Finally, the output answer $A$ is derived via a majority voting strategy over the set of produced answers $\mathcal{A</p>
<h1>2.3 BINDER IMPLEMENTATION</h1>
<p>In this section, we describe our implementation of BINDER with SQL and Python over structured knowledge as a demonstration. As BINDER is designed to be extensible to various programming languages and API call functionalities, we introduce a pipeline for users to quickly adapt BINDER to a new domain in Appendix D.</p>
<p>We implement two APIs $-f_{\text {col }}(\hat{Q} ; \hat{D})$ and $f_{\text {val }}(\hat{Q} ; \hat{D})$, where $\hat{D}=\left{c_{1}, \ldots, c_{|\hat{D}|}\right}$ is a (sub-)table of a set of table columns, and $c=\left{v_{i}, \ldots, v_{|c|}\right}$ is a column filled with cell values. Based on $\hat{Q}, f_{\text {col }}$ maps $\hat{D}$ into a column, and $f_{\text {val }}$ maps $\hat{D}$ into a value. Since both return types, i.e., column and value, are compatible with the grammars of SQL and Python (with the Pandas package), $f_{\text {col }}$ and $f_{\text {val }}$ APIs are inserted to replace the columns and values to form a valid BINDER program.</p>
<p>Take the question "which is the best-selling shirt made in North America and with no chemicals?" in Fig 1 as an example. The country names in the Made_in column do not provide enough information on their own to indicate what their continents are, and thus pure SQL cannot solve it (with this table's data). However, it is easy for a (large) language model to answer whether a country is from North America, which is represented by the $f_{\text {col }}$ ("North America?":Made_in) expression in the position of a column name in standard SQL. Similarly, the expression $f_{\text {col }}$ ("No chemicals?":Shirt) calls a (large) language model to identify whether the shirts consist of no chemicals (i.e., pure cotton in this case) based on the textual details in the Shirt column.</p>
<p>When a (sub-)question is too complex or infeasible to be solved by creating an intermediate new column with $f_{\text {col }}$, we turn to $f_{\text {val }}$ to directly derive the answer. For example, given the question "which shirt is the most suitable for a formal event?" on the table in Fig 1, it is hard to map Shirt to a new column of "formality value" followed by a SQL "ORDER BY" clause. Thus, the expression will be $f_{\text {val }}\left({ }^{\prime}\right.$ The most formal?":Shirt), that outputs a value as the answer. $f_{\text {val }}$ looks more like end-to-end QA, with two important differences: (1) it can be integrated into more compositional SQL queries using its result as a value, (2) it inputs the sub-table instead of the whole table which can mitigate the challenge of input capacity.</p>
<h2>3 EXPERIMENTS</h2>
<h3>3.1 EXPERIMENT SETUP</h3>
<p>Datasets We evaluate our method on three knowledge grounding datasets which were all previously dominated by end-to-end methods: WIKIYQ (Pasupat \&amp; Liang, 2015) and TABFACT (Chen et al., 2019a). WIKIYQ requires complex table reasoning skills to answer the questions. Furthermore, according to SQUALL (Shi et al., 2020b), about $20 \%$ of WIKIYQ questions are not answerable by pure SQL, either because of the need for extra knowledge or the limited coverage of the SQL grammar, both of which are issues that BINDER is designed to address. TABFACT is a binary fact verification benchmark over small tables, on which end-to-end methods have a large advantage but offer no interpretability.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Dev.</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Finetuned</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-3B (Xie et al., 2022)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">50.6</td>
</tr>
<tr>
<td style="text-align: left;">Tapex (Liu et al., 2021)</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">59.1</td>
</tr>
<tr>
<td style="text-align: left;">TaCube (Zhou et al., 2022)</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: left;">OmniTab (Jiang et al., 2022)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: left;">Without Finetuning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Codex end-to-end QA</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: left;">Codex SQL</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">Codex BINDER ${ }^{\dagger}$ (Ours)</td>
<td style="text-align: center;">$\mathbf{6 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: WiKiTQ execution accuracy on development and test sets. $\dagger$ denotes a symbolic method that outputs intermediate languages.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Finetuned</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SASP $^{\dagger}$ (Ou \&amp; Liu, 2022)</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: left;">BART-Large (Lewis et al., 2020)</td>
<td style="text-align: center;">82.5</td>
</tr>
<tr>
<td style="text-align: left;">T5-3B (Xie et al., 2022)</td>
<td style="text-align: center;">85.4</td>
</tr>
<tr>
<td style="text-align: left;">Tapex (Liu et al., 2021)</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Without Finetuning</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Codex end-to-end QA</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">Codex SQL</td>
<td style="text-align: center;">80.7</td>
</tr>
<tr>
<td style="text-align: left;">Codex BINDER ${ }^{\dagger}$ (Ours)</td>
<td style="text-align: center;">$\mathbf{8 5 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">with few-shot retriever</td>
<td style="text-align: center;">$\mathbf{8 6 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: TABFACT accuracy on the official small test set. $\dagger$ denotes a symbolic method that outputs intermediate languages.</p>
<p>Evaluation The evaluation metrics are execution accuracy (EA) for WiKiTQ and TabFACT following common practice for these datasets. Program executions are likely to be semantically correct but fail to match the gold answer exactly - for example, SQL outputs $1 / 0$ for yes/no questions. Though this is considered correct according to human evaluation, it is regarded as incorrect by the exact match evaluator. Thus, we add a pre-matching check for these semantically correct cases in WiKiTQ to the official evaluator. For a fair comparison, we compute the outputs of all baseline methods and re-evaluate them using the same evaluator. We provide more details on the evaluator in Appendix A.5, and list the results with the official evaluator for all baselines and our method in C.2.</p>
<p>Baselines We compare our method to a series of strong published methods on these datasets, including Tapex (Liu et al., 2021), OmniTab (Jiang et al., 2022), TaCube (Zhou et al., 2022), T53B (Xie et al., 2022; Raffel et al., 2020), and SASP (Ou \&amp; Liu, 2022). These baselines are fine-tuned on the full-size training set, and many of them (Liu et al., 2021; Zhou et al., 2022) are even further pretrained on a domain-relevant corpus of extra data which is specific to the target domain and task, while BINDER is training-free and only requires a few in-context exemplar annotations.</p>
<p>To further demonstrate the effectiveness of BINDER, we also evaluate Codex with additional inference modes including: (1) end-to-end QA, i.e., directly outputting the answer based on the input question and table; and (2) semantic parsing with the standard SQL language. Due to page limits, we refer readers interested in these baselines to their respective papers for details (Liu et al., 2021; Jiang et al., 2022; Zhou et al., 2022; Xie et al., 2022; Raffel et al., 2020; Ou \&amp; Liu, 2022), and we present a complete list and evaluation results of more previous systems on these datasets in Appendix C.1.</p>
<p>Implementation Details We use the OpenAI Codex (code-davinci-002) API ${ }^{2}$ model in our experiments as both the parser to generate programs and as the underlying model for API calls during the execution of each program. We annotate 14 in-context exemplars with BINDER programs for each dataset, which are selected considering the diversity of question types in demonstrations and the maximum token limit for Codex ( 8,000 tokens). The prompt format mainly follows (Rajkumar et al., 2022), which inputs the table schema and the first three table rows. The detailed prompt templates we use for each dataset are listed in Appendix A.1. On TabFACT, we further annotate a pool of 200 examples with BINDER programs from the training set using vote- $k$ selective annotation (Su et al., 2022), and use them by retrieving relevant few-shot examples for each inference example via maximum inner-product similarity of the SentenceBert (Clark et al., 2019) embeddings of the questions. The Codex hyperparameters for parsing and execution are provided in Appendix A.2. Empirically, it takes about 6 hours on average to evaluate 1,000 examples (i.e., generate and execute 20 programs per example) given that Codex allows 200 queries per minute. We evaluate on the official small test set ( 2,000 samples) of TABFACT considering the time cost.</p>
<h1>3.2 MAIN ReSULTS</h1>
<p>WiKiTQ All results on the WiKiTQ dataset are shown in Table 1. Our Codex BINDER outperforms listed strong baseline systems by a large margin, surpassing the previous state-of-the-art by absolute</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Program-unsolvable</th>
<th>Program-solvable</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5-3B (Xie et al., 2022)</td>
<td>37.6</td>
<td>56.0</td>
<td>51.9</td>
</tr>
<tr>
<td>Tapex (Liu et al., 2021)</td>
<td>33.6</td>
<td>68.0</td>
<td>60.4</td>
</tr>
<tr>
<td>TaCube (Zhou et al., 2022)</td>
<td>34.9</td>
<td>68.5</td>
<td>61.1</td>
</tr>
<tr>
<td>Codex end-to-end QA</td>
<td>40.3</td>
<td>53.4</td>
<td>50.5</td>
</tr>
<tr>
<td>w/o table inputs</td>
<td>14.2</td>
<td>11.9</td>
<td>12.4</td>
</tr>
<tr>
<td>Codex SQL</td>
<td>31.2</td>
<td>68.4</td>
<td>60.2</td>
</tr>
<tr>
<td>Codex BINDER (Ours)</td>
<td>$\mathbf{4 1 . 3}$</td>
<td>$\mathbf{7 1 . 8}$</td>
<td>$\mathbf{6 5 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Decomposition of execution accuracy on WIKITQ development set. The questions annotated with SQL by SQUALL (Shi et al., 2020b) dataset are denoted program-solvable, and the rest are program-unsolvable. OmniTab (Jiang et al., 2022) didn’t provide its result on the development set.
$1.3 \%$. Note that all baseline methods require access to the full training dataset and further fine-tuning, while our method can achieve state-of-the-art performance with only 14 example annotations. Codex BINDER improves over semantic parsing with standard SQL by $3.5 \%$ on the test set, indicating BINDER indeed mitigates the coverage limitations of the original language. Furthermore, both Codex BINDER and Codex SQL deliver dramatic advantages ( $15.9 \%$ and $12.4 \%$ ) over end-to-end QA, showing that semantic parsing with Codex is a better default choice when it comes to structured knowledge grounding and code-related tasks. A fine-grained analysis on why BINDER outperforms end-to-end QA and pure SQL is provided in Section 4.1.</p>
<p>TABFACT Table 2 presents the results on TABFACT's official small test set. Codex BINDER substantially surpasses the previous best symbolic method SASP by $8.1 \%$. Note symbolic methods usually fall behind end-to-end manners in fact verification (binary classification) since the answer space is small. When retrieving a few relevant questions from the annotated pool of 200 examples, our method achieves new state-of-the-art results, outperforming the previous best fine-tuned method. However, the improvement provided by retrieving a few similar examples is comparatively small ( $0.9 \%$ ). Effective sample selection methods for in-context learning remain an open challenge. Within the Codex few-shot setting (no few-shot retriever), BINDER shows a large advantage over standard SQL ( $4.4 \%$ ) and end-to-end QA ( $12.5 \%$ ), indicating the necessity of BINDER framework.</p>
<p>Besides the performance gain, BINDER has the additional advantages of (1) interpretability that benefits human debugging and (2) robustness that makes it stable to large or noisy inputs. We elaborate on these in Sections 4.2 and 4.3.</p>
<h1>4 ANALYSIS</h1>
<h3>4.1 Ablation Study</h3>
<p>Binding neural module API calls into a programming language can help solve queries that are unsolvable in that language alone. Therefore, we are particularly interested in the performance of BINDER on the unsolvable questions. According to SQUALL (Shi et al., 2020b), about 20\% of WIKITQ questions cannot be annotated in SQL; we call these program-unsolvable, and refer to the annotated ones as program-solvable. As presented in Table 3, Codex BINDER significantly outperforms Codex SQL by $10.1 \%$ on program-unsolvable questions, aligned with the motivation of our design. Our method even performs better than all end-to-end QA methods on the programunsolvable part, which are supposed to be more robust to this subset of the examples. We note that while SQL performs well on about $31.2 \%$ of unsolvable questions, many are spurious programs that derive the correct answer by accident. BINDER largely mitigates this phenomenon. We randomly pick 100 correct predictions in program-unsolvable set shared by SQL and BINDER and find that BINDER has a much lower spurious rate than SQL ( $12 \%$ vs. $33 \%$ ). One thing to note is that BINDER also achieves a higher score than SQL on program-solvable. We find that this is because some tables are manually cleaned (e.g., extracting numerical parts from the text) to run the annotated SQLs by SQUALL, which means there also exist unsolvable questions even in the program-solvable subset. We elaborate on our manual statistics about the proportion of WIKITQ that can be solved with SQL in Appendix C.4. We also present to what extent Codex itself can answer the question only using its internal knowledge. Codex can only answer a few WIKITQ questions ( $12.4 \%$ ) correctly, indicating it is not pretrained to overfit to the downstream dataset questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Error Type</th>
<th style="text-align: left;">Description</th>
<th style="text-align: center;">Proportion(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Syntax error</td>
<td style="text-align: left;">Incorrect program syntax (invalid grammar)</td>
<td style="text-align: center;">$\mathbf{5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Semantic error</td>
<td style="text-align: left;">Incorrect program semantics</td>
<td style="text-align: center;">$\mathbf{6 4 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Token</td>
<td style="text-align: left;">Incorrect or missing column/value/operator</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Structure</td>
<td style="text-align: left;">Incorrect program structure (valid grammar)</td>
<td style="text-align: center;">$22 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BINDER usage</td>
<td style="text-align: left;">Missing BINDER API usage</td>
<td style="text-align: center;">$32 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect execution</td>
<td style="text-align: left;">Incorrect execution answer with the correct program</td>
<td style="text-align: center;">$\mathbf{1 5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">False negative</td>
<td style="text-align: left;">Incorrect annotations or misjudge in evaluator</td>
<td style="text-align: center;">$\mathbf{1 6 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Error types of 100 samples from WiKiTQ development set of BINDER (SQL).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">Program</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">End-to-end QA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1967</td>
</tr>
<tr>
<td style="text-align: left;">SQL</td>
<td style="text-align: center;">SELECT year FROM t WHERE win_team='kansas state' AND <br> win_team - los_team &gt; 10</td>
<td style="text-align: center;"><empty></td>
</tr>
<tr>
<td style="text-align: left;">BINDER</td>
<td style="text-align: center;">SELECT year FROM t WHERE win_team='kansas state' AND <br> f("Points?";win_team) - f("Points?";los_team) &gt; 10</td>
<td style="text-align: center;"><empty></td>
</tr>
</tbody>
</table>
<p>Table 5: An example from WIKITQ development set. The query is "When was the first game that kansas state won by double digits?" and the gold answer is 1926. The incorrect segment(s) of each output are marked in red. See Appendix E for full context of this example.</p>
<h1>4.2 INTERPRETABILITY</h1>
<p>An important advantage BINDER provides is the improvement in interpretability over the end-toend approaches, where the explicit program can assist human debugging and error analysis. We sample 100 error cases from WIKI TQ dev set (approximately $10 \%$ of all incorrect examples) of the Codex BINDER (on SQL) for error analysis. We classify errors into syntax error, semantic error, incorrect execution, and false negative, as listed in Table 4. More details about the classification and example demonstrations are listed in Appendix E. For WIKI TQ, the errors mainly lie in the BINDER usage $(32 \%)$ and structure errors $(22 \%)$ of semantic errors, and incorrect execution $(15 \%)$. This indicates that $32 \%$ of incorrect examples can be further corrected if BINDER is used and $15 \%$ can be corrected through better execution, e.g., by leveraging more powerful LMs, using better in-context learning methods, and annotating more exemplars.</p>
<p>We also use an example to show the advantages of using BINDER for debugging and interpreting results. As shown in the example in Table 5, the result from the end-to-end system is 1967, which is incorrect but provides no clue to the reason behind this prediction. The result from BINDER is incorrect, since the program uses the incorrect value and operator regarding the win_team column from the table (should be "LIKE "\%kansas state\%""), and it can be potentially fixed in the future by adding similar in-context exemplars or fuzzy match postprocessing. For standard SQL, though we find the direct subtraction of win_team - los_team is incorrect (contain non-numerical text), it cannot be fixed for its limited grammar coverage. Thus, in this example, BINDER enables finding the source of the error (an advantage over the end-to-end approach) while also being expressive enough for users to find a way to fix it (an advantage over a pure symbolic system), making BINDER fit for debugging and enabling interpretability of a system that uses it. We leave it to future work to develop more systematic evaluation methods that consider ease of debugging by estimating human effort to correct system errors.</p>
<h3>4.3 Robustness</h3>
<h3>4.3.1 Scalability</h3>
<p>A great advantage of BINDER over end-to-end QA is the scalability, i.e., parsing and executing the symbolic language is always practical even when the knowledge source is too large (e.g., company databases, domain knowledge graphs) to fit into the model input capacity or memory, while end-toend QA fails or degrades since it requires the whole table as input to predict the answer. We expand tables with 100, 200, and 500 rows from a WIKI TQ development subset (random 150 samples) by prompting Codex to populate the original table with content-consistent rows, and then have two annotators annotate question-answer pairs manually. When the table is too large for Codex's maximum token limit ( 8,000 tokens), we truncate table rows until it fits. As shown in Figure 2, Codex</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Execution accuracy on WIKI TQ with very large tables (original, 100, 200, 500 rows).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Program-unsolvable</th>
</tr>
</thead>
<tbody>
<tr>
<td>Codex Python</td>
<td>30.7</td>
</tr>
<tr>
<td>Codex BINDER (Ours)</td>
<td>34.4</td>
</tr>
</tbody>
</table>
<p>Table 6: WIKI TQ accuracy on programunsolvable subset using Python as the programming language.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Execution accuracy on WIKI TQ with noisy content in tables.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>F1</th>
<th>EM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finetuned</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Implicit-Decomp (Talmor et al., 2021)</td>
<td>55.5</td>
<td>48.8</td>
</tr>
<tr>
<td>PReasM-Large (Yoran et al., 2022)</td>
<td>65.5</td>
<td>59.0</td>
</tr>
<tr>
<td>Without Finetuning</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Codex end-to-end QA</td>
<td>55.4</td>
<td>48.0</td>
</tr>
<tr>
<td>Codex BINDER (Ours)</td>
<td>57.1</td>
<td>51.0</td>
</tr>
<tr>
<td>with oracle retriever</td>
<td>64.5</td>
<td>58.1</td>
</tr>
</tbody>
</table>
<p>Table 7: MMQA F1/EM on development set.</p>
<p>end-to-end QA performance drops dramatically as table size increases, while BINDER consistently outperforms it with only slight performance decreases. Note that the input of BINDER is only <em>three table rows</em> for all table sizes.</p>
<h3>4.3.2 NOISY CONTENT</h3>
<p>End-to-end methods are more brittle to noisy inputs, especially when there exists <em>distractors</em> that are similar to the question-relevant (gold) contents. We build a noisy WIKI TQ development subset (random 150 samples) with distractors in three steps: (1) the gold table cells and columns are found using a heuristic fuzzy match, (2) replace 15% cells in gold columns with either <em>text</em> that has the smallest edit distance to the gold cell string in the WordNet corpus or <em>number</em> that equals to the gold cell value ±1, (3) have one annotator check that question-answer pairs are valid after content disturbance. As shown in Figure 3, BINDER is stable confronting distractors (1.3% ↓), while end-to-end QA is more likely to be confused by similar text and numbers (6.7% ↓).</p>
<h3>4.4 BINDER EXTENSION</h3>
<p>BINDER with Python We design BINDER to be easily extensible to various programming languages. Thus, we explore using Python (with the Pandas package) as the BINDER language on WIKI TQ. Similar to SQL, BINDER with Python is implemented by incorporating the $f(\cdot~;~\cdot)$ neural API into it. Since the neural API also calls language models with Python, we just use the original Python interpreter to execute our BINDER with Python. We evaluated it on the program-unsolvable subset of WIKI TQ to test whether our method improves Python’s capability. Though this subset is split based on SQL, Python shares some similar weaknesses with SQL, such as in cases with external knowledge requirements and unsupported functionality. As shown in Table 6, BINDER with Python effectively improves the Python coverage on the difficult subset. The gap between Python and SQL performance on WIKI TQ may lie in the properties of each programming language, as suggested by Guo et al. (2020). We conjecture that Codex is more familiar with semantic parsing to SQL for tabular data, while Python is not as commonly used in this context.</p>
<p>MultiModal Application We further explore applying BINDER on the multi-modal dataset MULTIMODAL QA (MMQA) across text, tables, and images. To input images into the LM program generation, images are converted into textual image captions with a vision-text pretrained model OFA (Wang et al., 2022) in advance. For table-related questions, the BINDER program is based</p>
<p>on SQL. For non-table questions, a program with $f_{\text {vol }}$ is generated with the targeted passage or image title(s). We follow Yoran et al. (2022) to retrieve the question-relevant passages and images in preprocessing. As far as we know, we are the first to demonstrate that multi-modal QA across text, tables, and images can be handled with interpretable and executable programs. As listed in Table 7, under the Codex few-shot setting, BINDER achieves better performance than end-to-end QA and the fine-tuned baseline Implicit-Decomp, showing the feasibility of BINDER on multi-modal knowledge sources. With the oracle retriever that assumes gold passages and images are given for each question, BINDER can achieve comparable performance with the state-of-the-art, showing the potential of BINDER approach in the future.</p>
<h1>5 Related Work</h1>
<p>Semantic Parsing Semantic parsing (Zelle \&amp; Mooney, 1996; Zettlemoyer \&amp; Collins, 2005) has been a mainstay of symbolic methods that produce an executable program given natural language input, generate intermediate structures that assist problem-solving, and improve interpretability over the neural methods which generate solution directly that came later. Structured knowledge grounding tasks mainly adopt semantic parsing since symbolic languages like SQL, SPARQL can be executed on them (Berant et al., 2013; Liang et al., 2017; Yin \&amp; Neubig, 2017; Zhong et al., 2018; Yu et al., 2018; Shaw et al., 2021; Scholak et al., 2021). Beyond structured knowledge, Chen et al. (2019b) and Thorne et al. (2021) design domain-specific languages executable on text. Recently, Chen et al. (2021) propose a generative pre-trained language model for code generation that require no additional human annotation. Many works propose methods based on this model and achieve great success (Rajkumar et al., 2022; Shi et al., 2022). However, the semantic parsing method is restricted by its grammar coverage, unable to solve problems requiring external knowledge or functions.</p>
<p>Neural-Symbolic Methods Some works integrate neural modules with symbolic languages for the advantage of both approaches, i.e., good performance and interpretability. Andreas et al. (2016); Hu et al. (2017); Das et al. (2018) and Gupta et al. (2019) generate programs that are further softly executed by the corresponding neural modules. Khot et al. (2021) propose text module networks to solve complex tasks by decomposing them into simpler ones solvable by existing QA models and a symbolic calculator. BREAK (Wolfson et al., 2020) proposes a meaningful representation, QDMR, that decomposes the question into multiple steps. However, they require the elaborate design of functions to be used in corresponding task and the calibration of corresponding neural modules which require complicated training steps and large training data (normally tens of thousands) to tackle problems in a specific domain.</p>
<p>Compared with these methods, BINDER is expressive and flexible to handle real-world diverse questions since it is able to make proper API calls to enhance its functionalities. Moreover, BINDER is training-free and requires only dozens of annotations based on certain symbolic language to perform on a domain specific task while maintaining: excellent performance, ability in scaling on input, interpretabilityand robustness over noisy content.</p>
<h2>6 CONCLUSION</h2>
<p>We propose BINDER, a training-free neural-symbolic framework that maps the task input to a program that allows binding a unified LM API for additional functionalities. BINDER aims to combine the strengths of end-to-end approaches (high coverage) and symbolic approaches (high interpretability). Using Codex as the LM, BINDER achieves state-of-the-art performance on WikiTQ and TabFACT with only dozens of in-context demonstrations and no additional training. In contrast, the best existing systems are finetuned on thousands of task-specific training samples and may require further domainspecific pertaining. We also perform a series of analyses of BINDER, decomposing performance gains, examining robustness to large or noisy inputs, applying it to multi-modal knowledge sources, and extending it to the Python language. We regard BINDER as a new, language model-focused attempt to integrate two widely-adopted paradigms in NLP: end-to-end and symbolic approaches. With the recent powerful large language models, it has become feasible that BINDER programs are generated and executed correctly in a training-free manner. In the future, we believe BINDER can be extended to many more scenarios with the appropriate programming language and functionalities, and we hope it can inspire more creative ideas on balancing model capability and interpretability.</p>
<h1>7 REPRODUCIbILITY</h1>
<p>BINDER experiments are mainly based on OpenAI Codex (code-davinci-002) API ${ }^{3}$. We provide (1) the input prompt templates we use in Appendix A.1, (2) Codex hyper-parameters for each dataset we adopt in Appendix A.2, (3) more implementation details in Appendix A. Besides, we also upload our source code in the materials making our results easy to be reproduced.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REFERENCES</h1>
<p>Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In International conference on machine learning, pp. 130-140. PMLR, 2019.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 39-48, 2016.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proc. of EMNLP, 2013. URL https://aclanthology.org/ D13-1160.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. of NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: A large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164, 2019a.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations, 2019b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022a.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022b.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proc. of ACL, July 2019. URL https://aclanthology. org/P19-1264/.</p>
<p>Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Neural modular control for embodied question answering. In Conference on Robot Learning, pp. 53-62. PMLR, 2018.</p>
<p>Pradeep Dasigi, Matt Gardner, Shikhar Murty, Luke Zettlemoyer, and Eduard Hovy. Iterative search for weakly supervised semantic parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2669-2680, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019. URL https://arxiv.org/abs/810.04805.</p>
<p>Julian Eisenschlos, Syrine Krichene, and Thomas Mueller. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 281-296, 2020.</p>
<p>Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends ${ }^{\circledR}$ in Programming Languages, 4(1-2):1-119, 2017.</p>
<p>Jiaqi Guo, Qian Liu, Jian-Guang Lou, Zhenwen Li, Xueqing Liu, Tao Xie, and Ting Liu. Benchmarking meaning representations in neural semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1520-1540, 2020.</p>
<p>Jiaqi Guo, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. Weakly supervised semantic parsing by learning from mistakes. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2603-2617, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.222. URL https://aclanthology.org/2021.findings-emnlp. 222.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. Neural module networks for reasoning over text. arXiv preprint arXiv:1912.04971, 2019.</p>
<p>Jonathan Herzig, P. Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of ACL, 2020.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 804-813, 2017.</p>
<p>Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A Smith, and Mari Ostendorf. In-context learning for few-shot dialogue state tracking. arXiv preprint arXiv:2203.08568, 2022.</p>
<p>Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. Omnitab: Pretraining with natural and synthetic data for few-shot table-based question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 932-942, 2022.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1264-1279, 2021.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL, July 2020. URL https://arxiv.org/abs/1910.13461.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23-33, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1003. URL https://aclanthology.org/P17-1003.</p>
<p>Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. Memory augmented policy optimization for program synthesis and semantic parsing. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian guang Lou. Tapex: Table pre-training via learning a neural sql executor, 2021.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834, 2015.</p>
<p>Suixin Ou and Yongmei Liu. Learning to generate programs for table fact verification via structureaware semantic parsing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7624-7638, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.525. URL https://aclanthology.org/2022.acl-long. 525.</p>
<p>Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1470-1480, Beijing, China, July 2015. Association for Computational Linguistics. doi: $10.3115 / \mathrm{v} 1 / \mathrm{P} 15-1142$. URL https://aclanthology.org/P15-1142.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JLMR, 21, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498, 2022.</p>
<p>Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9895-9901, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.779. URL https://aclanthology.org/2021.emnlp-main. 779.</p>
<p>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online, August 2021. Association for Computational Linguistics.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language to code translation with execution. arXiv preprint arXiv:2204.11454, 2022.</p>
<p>Qi Shi, Yu Zhang, Qingyu Yin, and Ting Liu. Learn to combine linguistic and symbolic information for table-based fact verification. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 5335-5346, 2020a.</p>
<p>Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daumé III, and Lillian Lee. On the potential of lexico-logical alignments for semantic parsing to sql queries. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1849-1864, 2020b.</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022.</p>
<p>Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039, 2021.</p>
<p>James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. Database reasoning over text. arXiv preprint arXiv:2106.01074, 2021.</p>
<p>Bailin Wang, Ivan Titov, and Mirella Lapata. Learning semantic parsers from denotations with latent structured alignments and abstract programs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3774-3785, 2019.</p>
<p>Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 2020.</p>
<p>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, ChienSheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.</p>
<p>Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, and Xiaodan Zhu. Program enhanced fact verification with verbalization and graph attention network. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7810-7825, 2020.</p>
<p>Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 440-450, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1041. URL https://aclanthology.org/P17-1041.</p>
<p>Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. TaBERT: Pretraining for joint understanding of textual and tabular data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, July 2020.</p>
<p>Ori Yoran, Alon Talmor, and Jonathan Berant. Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6016-6031, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.acl-long.416. URL https://aclanthology.org/2022.acl-long. 416.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3911-3921, 2018.</p>
<p>Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Caiming Xiong, et al. Grappa: Grammar-augmented pre-training for table semantic parsing. In International Conference on Learning Representations, 2020.</p>
<p>John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic programming. In AAAI 1996, pp. 1050-1055, 1996.</p>
<p>Luke S. Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. UAI, 2005.</p>
<p>Yuchen Zhang, Panupong Pasupat, and Percy Liang. Macro grammars and holistic triggering for efficient semantic parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1214-1223, 2017.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. 2018.</p>
<p>Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, and Jian Yin. Logicalfactchecker: Leveraging logical operations for fact checking with graph module network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6053-6065, 2020.</p>
<p>Fan Zhou, Mengkang Hu, Haoyu Dong, Zhoujun Cheng, Shi Han, and Dongmei Zhang. Tacube: Pre-computing data cubes for answering numerical-reasoning questions over tabular data. arXiv preprint arXiv:2205.12682, 2022.</p>
<p>Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models for logical queries on knowledge graphs. arXiv preprint arXiv:2205.10128, 2022.</p>
<h1>Appendices</h1>
<h2>A MORE IMPLEMENTATION DETAILS</h2>
<p>We present more details in our implementation of Codex BINDER for interested readers.</p>
<h2>A. 1 CODEX Input Prompts</h2>
<p>We list prompts we use in main experiments for BINDER for each dataset. As shown, the prompt starts with an instruction describing the task, followed by a few examples for in-context learning. We follow Rajkumar et al. (2022) table prompt format: (1) "CREATE TABLE" schema; (2) the first three rows of the table; (3) the question $Q$ and parsed logic form BINDER program. Besides, we also add row_id and lower case all table contents, which we find will improve the Codex parsing performance. For the sample to be inferenced, we empty the BINDER program to let Codex generate it, and input the full table to provide more information. If the full table is too large, we will shrink number of in-context shots until the table fits within the Codex max token limits.</p>
<div class="codehilite"><pre><span></span><code><span class="err">##</span><span class="w"> </span><span class="nt">WikiTQ</span>
<span class="nt">Generate</span><span class="w"> </span><span class="nt">SQL</span><span class="w"> </span><span class="nt">given</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">table</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">answer</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">correctly</span><span class="o">.</span>
<span class="o">...</span>
<span class="nt">CREATE</span><span class="w"> </span><span class="nt">TABLE</span><span class="w"> </span><span class="nt">Electoral</span><span class="w"> </span><span class="nt">district</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">Lachlan</span><span class="o">(</span>
<span class="w">    </span><span class="nt">row_id</span><span class="w"> </span><span class="nt">int</span><span class="o">,</span>
<span class="w">    </span><span class="nt">member</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">party</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">term</span><span class="w"> </span><span class="nt">text</span><span class="o">)</span>
<span class="c">/*</span>
<span class="c">3 \text { example rows:}</span>
<span class="c">SELECT * FROM w LIMIT 3;</span>
<span class="c">row_id member party term</span>
<span class="c">0 john ryan none 1859-1864</span>
<span class="c">1}\mathrm{ james martin none 1864-1869</span>
<span class="c">2}\mathrm{ james watson none 1869-1880</span>
<span class="c">*/</span>
<span class="nt">Q</span><span class="o">:</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">members</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">third</span><span class="w"> </span><span class="nt">incarnation</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">lachlan</span><span class="o">,</span>
<span class="w">    </span><span class="nt">who</span><span class="w"> </span><span class="nt">served</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">longest</span><span class="o">?</span>
<span class="nt">Binder</span><span class="o">:</span><span class="w"> </span><span class="nt">SELECT</span><span class="w"> </span><span class="nt">member</span><span class="w"> </span><span class="nt">FROM</span><span class="w"> </span><span class="nt">w</span><span class="w"> </span><span class="nt">ORDER</span><span class="w"> </span><span class="nt">BY</span>
<span class="w">    </span><span class="nt">f</span><span class="o">(</span><span class="s2">&quot;How long does it last?&quot;</span><span class="o">;</span><span class="w"> </span><span class="nt">term</span><span class="o">)</span><span class="w"> </span><span class="nt">DESC</span><span class="w"> </span><span class="nt">LIMIT</span><span class="w"> </span><span class="nt">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="err">##</span><span class="w"> </span><span class="nt">TabFact</span>
<span class="nt">Generate</span><span class="w"> </span><span class="nt">SQL</span><span class="w"> </span><span class="nt">given</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">statement</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">table</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">verify</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">statement</span><span class="w"> </span><span class="nt">correctly</span><span class="o">.</span>
<span class="o">...</span>
<span class="nt">CREATE</span><span class="w"> </span><span class="nt">TABLE</span><span class="w"> </span><span class="nt">british</span><span class="w"> </span><span class="nt">records</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">athletics</span><span class="o">(</span>
<span class="w">    </span><span class="nt">row_id</span><span class="w"> </span><span class="nt">int</span><span class="o">,</span>
<span class="w">    </span><span class="nt">event</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">data</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">athlete</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">date</span><span class="w"> </span><span class="nt">text</span><span class="o">,</span>
<span class="w">    </span><span class="nt">place</span><span class="w"> </span><span class="nt">text</span><span class="o">)</span>
<span class="o">/*</span>
<span class="nt">3</span><span class="w"> </span><span class="err">\</span><span class="nt">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="err">example</span><span class="w"> </span><span class="n">rows</span><span class="p">:}</span>
<span class="nt">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nt">FROM</span><span class="w"> </span><span class="nt">w</span><span class="w"> </span><span class="nt">LIMIT</span><span class="w"> </span><span class="nt">3</span><span class="o">;</span>
<span class="nt">row_id</span><span class="w"> </span><span class="nt">event</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="nt">athlete</span><span class="w"> </span><span class="nt">date</span><span class="w"> </span><span class="nt">place</span>
<span class="nt">0</span><span class="w"> </span><span class="nt">5</span><span class="w"> </span><span class="nt">km</span><span class="w"> </span><span class="nt">t19</span><span class="p">:</span><span class="nd">29</span><span class="w"> </span><span class="nt">andi</span><span class="w"> </span><span class="nt">drake</span><span class="w"> </span><span class="nt">1990-05-27</span><span class="w"> </span><span class="nt">00</span><span class="p">:</span><span class="nd">00</span><span class="p">:</span><span class="nd">00</span><span class="w"> </span><span class="nt">søfteland</span><span class="w"> </span><span class="o">,</span><span class="w"> </span><span class="nt">norway</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="mf">1</span><span class="w"> </span><span class="mf">5</span><span class="w"> </span><span class="n">miles</span><span class="w"> </span><span class="mf">32</span><span class="p">:</span><span class="mf">38</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ian</span><span class="w"> </span><span class="n">mccombie</span><span class="w"> </span><span class="mf">1985</span><span class="o">-</span><span class="mf">03</span><span class="o">-</span><span class="mf">23</span><span class="w"> </span><span class="mf">00</span><span class="p">:</span><span class="mf">00</span><span class="p">:</span><span class="mf">00</span>
<span class="n">york</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">united</span><span class="w"> </span><span class="n">kingdom</span>
<span class="mf">2</span><span class="w"> </span><span class="mf">10</span><span class="w"> </span><span class="n">km</span><span class="w"> </span><span class="mf">40</span><span class="p">:</span><span class="mf">17</span><span class="w"> </span><span class="n">chris</span><span class="w"> </span><span class="n">maddocks</span><span class="w"> </span><span class="mf">1989</span><span class="o">-</span><span class="mf">04</span><span class="o">-</span><span class="mf">30</span><span class="w"> </span><span class="mf">00</span><span class="p">:</span><span class="mf">00</span><span class="p">:</span><span class="mf">00</span>
<span class="n">burrator</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">united</span><span class="w"> </span><span class="n">kingdom</span>
<span class="o">*/</span>
<span class="n">Q</span><span class="p">:</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="mf">8</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">event</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="n">within</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">united</span><span class="w"> </span><span class="n">kingdom</span>
<span class="n">Binder</span><span class="p">:</span><span class="w"> </span><span class="n">SELECT</span><span class="w"> </span><span class="p">(</span><span class="n">SELECT</span><span class="w"> </span><span class="n">COUNT</span><span class="p">(</span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="n">WHERE</span>
<span class="w">    </span><span class="n">f</span><span class="p">(</span><span class="s">&quot;Is it in united kingdom?&quot;</span><span class="p">;</span><span class="w"> </span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">&#39;</span><span class="n">yes</span><span class="err">&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">8</span>
<span class="err">##</span><span class="w"> </span><span class="n">MMQA</span>
<span class="n">Generate</span><span class="w"> </span><span class="n">SQL</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="nb">tab</span><span class="n">le</span><span class="p">,</span><span class="w"> </span><span class="n">passages</span><span class="p">,</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="n">captions</span>
<span class="kr">to</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">correctly</span><span class="mf">.</span>
<span class="mf">...</span>
<span class="n">CREATE</span><span class="w"> </span><span class="nb">TAB</span><span class="n">LE</span><span class="w"> </span><span class="mf">2018</span><span class="w"> </span><span class="n">Warrington</span><span class="w"> </span><span class="n">Wolves</span><span class="w"> </span><span class="n">season</span><span class="p">(</span>
<span class="w">    </span><span class="n">row_id</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">    </span><span class="n">player</span><span class="w"> </span><span class="n">text</span><span class="p">,</span>
<span class="w">    </span><span class="n">signed</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">text</span><span class="p">,</span>
<span class="w">    </span><span class="kr">cont</span><span class="n">ract</span><span class="w"> </span><span class="nb">len</span><span class="n">gth</span><span class="w"> </span><span class="n">text</span><span class="p">,</span>
<span class="w">    </span><span class="n">announced</span><span class="w"> </span><span class="n">text</span><span class="p">)</span>
<span class="o">/*</span>
<span class="mf">3</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">rows</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="n">LIMIT</span><span class="w"> </span><span class="mf">3</span><span class="p">;</span>
<span class="n">row_id</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">signed</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="kr">cont</span><span class="n">ract</span><span class="w"> </span><span class="nb">len</span><span class="n">gth</span><span class="w"> </span><span class="n">announced</span>
<span class="mf">0</span><span class="w"> </span><span class="n">sitaleki</span><span class="w"> </span><span class="n">akauola</span><span class="w"> </span><span class="n">penrith</span><span class="w"> </span><span class="n">panthers</span><span class="w"> </span><span class="mf">2</span><span class="w"> </span><span class="mf">2017</span><span class="o">-</span><span class="mf">08</span><span class="o">-</span><span class="mf">01</span>
<span class="mf">1</span><span class="w"> </span><span class="n">bryson</span><span class="w"> </span><span class="kr">go</span><span class="n">odwin</span><span class="w"> </span><span class="n">south</span><span class="w"> </span><span class="n">sydney</span><span class="w"> </span><span class="n">rabbitohs</span><span class="w"> </span><span class="mf">2</span><span class="w"> </span><span class="mf">2017</span><span class="o">-</span><span class="mf">10</span><span class="o">-</span><span class="mf">01</span>
<span class="mf">2</span><span class="w"> </span><span class="n">tyrone</span><span class="w"> </span><span class="n">roberts</span><span class="w"> </span><span class="kr">go</span><span class="n">ld</span><span class="w"> </span><span class="n">coast</span><span class="w"> </span><span class="n">titans</span><span class="w"> </span><span class="mf">3</span><span class="w"> </span><span class="mf">2017</span><span class="o">-</span><span class="mf">10</span><span class="o">-</span><span class="mf">01</span>
<span class="o">*/</span>
<span class="n">CREATE</span><span class="w"> </span><span class="nb">TAB</span><span class="n">LE</span><span class="w"> </span><span class="n">Images</span><span class="p">(</span>
<span class="w">    </span><span class="n">row_id</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">    </span><span class="kr">go</span><span class="n">ld</span><span class="w"> </span><span class="n">coast</span><span class="w"> </span><span class="n">titans</span><span class="w"> </span><span class="n">text</span><span class="p">)</span>
<span class="o">/*</span>
<span class="n">All</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nb">tab</span><span class="n">le</span><span class="p">:</span>
<span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">w</span><span class="p">;</span>
<span class="n">row_id</span><span class="w"> </span><span class="kr">go</span><span class="n">ld</span><span class="w"> </span><span class="n">coast</span><span class="w"> </span><span class="n">titans</span>
<span class="mf">0</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nb">log</span><span class="n">o</span><span class="w"> </span><span class="kr">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="kr">go</span><span class="n">lden</span><span class="w"> </span><span class="n">knights</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">painted</span><span class="w"> </span><span class="kr">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">beach</span><span class="mf">.</span>
<span class="o">*/</span>
<span class="n">Q</span><span class="p">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">transferred</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">crossed</span>
<span class="w">    </span><span class="n">swords</span><span class="w"> </span><span class="kr">on</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="nb">log</span><span class="n">o</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Warrington</span><span class="w"> </span><span class="n">Wolves</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="mf">2018</span><span class="w"> </span><span class="n">season</span><span class="err">?</span>
<span class="n">Binder</span><span class="p">:</span><span class="w"> </span><span class="n">SELECT</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="n">WHERE</span>
<span class="w">    </span><span class="n">f</span><span class="p">(</span><span class="s">&quot;Has crossed swords on its logo?&quot;</span><span class="p">;</span><span class="w"> </span><span class="err">`</span><span class="n">signed</span><span class="w"> </span><span class="n">from</span><span class="err">`</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">&#39;</span><span class="n">yes</span><span class="err">&#39;</span>
</code></pre></div>

<p>In end-to-end QA setting, the prompt is almost the same, except the full table contents are used in all the in-context examples to since in most cases QA must see the complete table to answer the question correctly. We empirically find that full table input to Codex is a necessity for end-to-end QA, but only a small bonus for semantic parsing.</p>
<p>In the ablation study experiments, the prompt is also almost the same, except that under the very large table scalability setting, we truncate number of table rows instead of in-context shots to fit in Codex max token limits, in order to simulate a realistic situation where only parts of the table can be input into the model.</p>
<h1>A. 2 CODEX HYPER-PARAMETERS</h1>
<p>In both two phases of our Codex BINDER pipeline, parsing and execution, the Codex api is called. We set the Codex in-context learning hyper-parameters as shown in Table 8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyper-parameter</th>
<th style="text-align: center;">Parsing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Execution</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">WIKI TQ</td>
<td style="text-align: center;">TABFACT</td>
<td style="text-align: center;">MMQA</td>
<td style="text-align: center;">WIKI TQ</td>
<td style="text-align: center;">TABFACT</td>
<td style="text-align: center;">MMQA</td>
</tr>
<tr>
<td style="text-align: left;">temperature</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">top_p</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">max_output_tokens</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: left;">sampling_n</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">stop_tokens</td>
<td style="text-align: center;">\n\n</td>
<td style="text-align: center;">\n\n</td>
<td style="text-align: center;">\n\n</td>
<td style="text-align: center;">\n\n</td>
<td style="text-align: center;">\n\n</td>
<td style="text-align: center;">\n\n</td>
</tr>
<tr>
<td style="text-align: left;">num_shots</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 8: Codex hyper-parameters we set in main experiments. Codex is used in two phases: Parsing and Execution.</p>
<p>Parsing In parsing phase, i.e. parsing questions into programming languages, we annotate a dozens or so examples from training set or modified examples from development set as in-context demonstrations for each dataset. temperature is to control randomness of generations. We find 0.4 is a suitable trade-off between fidelity and diversity of generated programs in most cases. sampling_n is number of generations (programs) per sample. We heuristically set sampling_n as 20 in WIKI TQ and MMQA to balance the time cost in execution and the potential performance gain over majority voting. For TabFact, we increase temperature to 0.6 and 50 , as we empirically find that for binary classification tasks (or tasks with a small output space), more generations with more diversity can better determine the final answer.</p>
<p>Execution As mentioned in Section 2, the executor will call model(s) with various functionalities. In experiments, we use Codex as the underlining model for all the neural modules since Codex itself is a very powerful large language model which can realize most functions on texts and codes. For images, they are preprocessed into text via image captioning by OFA (Wang et al., 2022). Note that on MMQA, replacing Codex (with image captions) with a specific VQA model is likely to improve the performance on images because image captioning is question-agnostic while VQA model can predict the answer based on the question.</p>
<p>We let Codex output only one answer with temperature 0.0 per input instead of multiple answers followed by a majority vote considering the time cost. An interesting thing is that even when temperature is 0.0 , the Codex outputs may be different in two inferences. OpenAI team said the randomness inherent to GPU computations, and generally the outputs will be the consistent in most cases, which is aligned with our observations.</p>
<p>We also find it better to give Codex some in-context demonstrations for the neural module functionality. 50 samples are annotated as a small retrieve pool (shared across datasets) for mapping a column to a new one according to the question, i.e., $f_{m}$ API in BINDER grammar. A simple BLEU score retriever will retrieve 8 similar items from the pool as in-context demonstrations for each coming sample. Below are several examples demonstrating the formats of neural module functionality prompt. The output (sub-)table contains a new mapped column named after the query, which will be further merged into the original table. Note it is important to formulate the output also as a (sub-)table, otherwise if Codex outputs a list of answers, it will be confused which row it is generating answer for and perform very poorly.</p>
<div class="codehilite"><pre><span></span><code><span class="err">##</span><span class="w"> </span><span class="nt">1</span>
<span class="nt">Give</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">database</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">shown</span><span class="w"> </span><span class="nt">below</span><span class="o">:</span>
<span class="nt">Table</span><span class="o">:</span><span class="w"> </span><span class="nt">Highest</span><span class="w"> </span><span class="nt">mountain</span><span class="w"> </span><span class="nt">peaks</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">California</span>
<span class="c">/*</span>
<span class="c">row_id prominence</span>
<span class="c">1 1 0 0 8 0 \text { ft; 3 0 7 2 m}</span>
<span class="c">1 1677 ft; 511 m</span>
<span class="c">2 7 1 9 6 \text { ft; 2 1 9 3 m}</span>
<span class="c">3 2 8 9 4 \text { ft; 8 8 2 m}</span>
<span class="c">4 9 8 3 2 \text { ft; 2 9 9 7 m}</span>
<span class="c">5 2 5 6 3 \text { ft; 7 8 1 m}</span>
<span class="c">*/</span>
<span class="nt">Q</span><span class="o">:</span><span class="w"> </span><span class="nt">Answer</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="s2">&quot;What is the value of in feet?&quot;</span><span class="w"> </span><span class="nt">row</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">row</span><span class="o">.</span>
<span class="nt">QA</span><span class="w"> </span><span class="nt">map</span><span class="o">@</span><span class="w"> </span><span class="nt">output</span><span class="o">:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">/*</span>
<span class="n">row_id</span><span class="w"> </span><span class="n">prominence</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">feet</span>?
<span class="mi">0</span><span class="w"> </span><span class="mi">10080</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">3072</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">10080</span>
<span class="mi">1</span><span class="w"> </span><span class="mi">1677</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">511</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">1677</span>
<span class="mi">2</span><span class="w"> </span><span class="mi">7196</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">2193</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">7196</span>
<span class="mi">3</span><span class="w"> </span><span class="mi">2894</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">882</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">2894</span>
<span class="mi">4</span><span class="w"> </span><span class="mi">9832</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">2997</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">9832</span>
<span class="mi">5</span><span class="w"> </span><span class="mi">2563</span><span class="w"> </span><span class="n">ft</span><span class="p">;</span><span class="w"> </span><span class="mi">781</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="mi">2563</span>
<span class="o">*/</span>
##<span class="w"> </span><span class="mi">2</span>
<span class="n">Give</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">database</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">shown</span><span class="w"> </span><span class="n">below</span><span class="p">:</span>
<span class="n">Table</span><span class="p">:</span><span class="w"> </span><span class="mi">2010</span><span class="o">-</span><span class="mi">11</span><span class="w"> </span><span class="n">UAB</span><span class="w"> </span><span class="n">Blazers</span><span class="w"> </span><span class="n">men</span><span class="o">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">basketball</span><span class="w"> </span><span class="n">team</span>
<span class="o">/*</span>
<span class="n">row_id</span><span class="w"> </span><span class="n">hometown</span>
<span class="mi">0</span><span class="w"> </span><span class="n">chicago</span><span class="p">,</span><span class="w"> </span><span class="n">il</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span>
<span class="mi">1</span><span class="w"> </span><span class="n">oklahoma</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">ok</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span>
<span class="mi">2</span><span class="w"> </span><span class="n">montgomery</span><span class="p">,</span><span class="w"> </span><span class="n">al</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span>
<span class="mi">3</span><span class="w"> </span><span class="n">greenville</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span>
<span class="mi">4</span><span class="w"> </span><span class="n">birmingham</span><span class="p">,</span><span class="w"> </span><span class="n">al</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span>
<span class="o">*/</span>
<span class="n">Q</span><span class="p">:</span><span class="w"> </span><span class="n">Answer</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="s">&quot;Is it from alabama?&quot;</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">row</span><span class="p">.</span>
<span class="n">QA</span><span class="w"> </span><span class="n">map</span><span class="p">@</span><span class="w"> </span><span class="n">output</span><span class="p">:</span>
<span class="o">/*</span>
<span class="n">row_id</span><span class="w"> </span><span class="n">hometown</span><span class="w"> </span><span class="n">Is</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">alabama</span>?
<span class="mi">0</span><span class="w"> </span><span class="n">chicago</span><span class="p">,</span><span class="w"> </span><span class="n">il</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="w"> </span><span class="n">no</span>
<span class="mi">1</span><span class="w"> </span><span class="n">oklahoma</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">ok</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="w"> </span><span class="n">no</span>
<span class="mi">2</span><span class="w"> </span><span class="n">montgomery</span><span class="p">,</span><span class="w"> </span><span class="n">al</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="w"> </span><span class="n">yes</span>
<span class="mi">3</span><span class="w"> </span><span class="n">greenville</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="w"> </span><span class="n">no</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">birmingham</span><span class="p">,</span><span class="w"> </span><span class="n">al</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="w"> </span><span class="n">yes</span>
<span class="o">*/</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gu">##</span> 3
Give a database as shown below:
Table: 1963 International Gold Cup
/*
row_id driver
0 jim clark
1 richie ginther
2 graham hill
3 jack brabham
4 tony maggs
<span class="gs">*/</span>
<span class="gs">Q: Answer question &quot;What is his/her country?&quot; row by row.</span>
<span class="gs">QA map@ output:</span>
<span class="gs">/*</span>
row_id driver What is his/her country?
0 jim clark scotland
1 richie ginther united states
2 graham hill england
3 jack brabham australia
4 tony maggs south africa
*/
</code></pre></div>

<h1>A. 3 MAJORITY VOTE STRATEGY</h1>
<p>Majority vote is widely used to ensemble multiple candidate answers. A simple implementation of majority vote is to select the answer that appears most often in candidates as the final answer. For example, given a candidate answer pool $C$ with $n$ answers, the output answer $a_{\text {out }}$ is derived by:</p>
<p>$$
a_{\text {out }}=\underset{a}{\arg \max }(\operatorname{count}(a)), a \in \operatorname{set}(C)
$$</p>
<p>where $\operatorname{count}(\cdot)$ function counts the number of occurrences of the input answer in $C$, and $\operatorname{set}(\cdot)$ function de-duplicates the input list. In our case, $n$ programs are generated per sample, and their execution results compose the candidate answer pool.</p>
<p>In experiments, we adopt two variants of majority vote strategies. The first one we call answer-biased. In TABFACT, we conjecture that it is difficult to validate a statement of multiple sub-statements as entailed (i.e., answer 1) with SQL program, since a minor error in program will cause the execution to output refuted (i.e., answer 0 ). In other words, we value it when an answer 1 occurs because it indicates the statement has a high probability of being entailed. Thus, we re-weight answers by assigning four votes for an answer 1 , and one vote for an answer 0 . The second is program-biased. We assign larger weights to BINDER language when it occurs in the generated programs because BINDER consists of small portion in few-shot prompt, and thus more generations will be in standard programming language. Specifically, we re-weight BINDER program with ten votes, and the others with one vote in WIKI TQ and MMQA.</p>
<h2>A. 4 Binder Grammar AdAPtion to SQL</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An illustration of BINDER program AST and extension to SQL grammar. The blue types in the production rule are extended by BINDER.</p>
<p>BINDER extends the production rules of the programming language to ensure the return values of API calls can be compatible with its original grammar. Take SQL as an example, in Figure 4, two APIs $f_{\text {col }}$ and $f_{\text {cat }}$ are added as identifiers in the production rules to fit in as a column and a value in SQL. The AST of the shown BINDER program is quite similar to standard SQL's AST, except that API calls are placed in the column position.</p>
<h2>A. 5 Evaluator</h2>
<p>Program executions are naturally more difficult to match gold answers exactly when the answer is not a span in the input knowledge source, while end-to-end models may match this pattern by finetuning or infer phrases from the question. However, we find that in WiKiTQ, some error cases of program execution outputs are correct according to humans. These cases mainly fall into two types. The first is $A$ or $B$ choice question, e.g.,, "Are there at least 13 different components on the chart?" in Table 9. The SQL program SELECT COUNT (component) &gt; 13, returns 1 to indicate "yes" and 0 for "no". Generally in $A$ or $B$ problem, we match 1 with $A$ and 0 with $B$. The second is number with unit question, e.g.,, "what is the difference in years between constituency 1 and 2" in Table 9, where the gold answer contains unit years in it, while it is almost impossible for programs to derive years, and 4 is also a reasonable correct answer in this case. Therefore, we add pre-matching logics upon WIKI TQ official evaluator for these two problems. Besides, we also normalize the dates with Recognizers package in Python. Table 9 gives some examples to show the differences of three WiKiTQ evaluators. For fair comparison, all previous baselines are re-evaluated with the semantic-match evaluator in experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Example</th>
<th style="text-align: center;">EM(TaPEx string match)</th>
<th style="text-align: center;">EM(WikiTQ official)</th>
<th style="text-align: center;">EM(Semantic match)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(Normalization)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Question: What was the same problem that <br> Bernard Collomb had as Innes Ireland? <br> Gold Answer: oil pressure <br> Pred Answer: oil pressure (56 laps)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">(Float Precision) <br> Question: What is the difference between the <br> qualifying time in 1967 and 1965? <br> Gold Answer: 7.45 <br> Pred Answer: 7.449999999999989</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">(A or B Choice) <br> Question: Are there at least 13 <br> different components on the chart? <br> Gold Answer: Yes <br> Pred Answer: 1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">(Number with Units) <br> Question: What is the difference in years <br> between constiuency 1 and 2? <br> Gold Answer: 4 years <br> Pred Answer: 4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 9: Examples to illustrate differences among three exact match evaluators on WikiTQ. In experiments, we evaluate all baselines and our method with semantic-match evaluator.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of the BINDER method(ours) with other large language model usage paradigms: End-to-End, Chain-of-Thought, and Semantic Parsing/Code Generation.</p>
<h1>B COMPARISON WITH OTHER LARGE LANGUAGE MODEL USAGE PARADIGMS</h1>
<p>Besides our work, some other works also focus on how to leverage large language models in creative ways to achieve better performance on downstream tasks and see improvements with proper designs. We compare our Binder method with the three previous paradigms of large language model usage: End-End, Chain-of-Thought, and Semantic Parsing/Code Generation.</p>
<p>End-to-End End-to-End method (Brown et al., 2020; Hoffmann et al., 2022, i.a.) aims to use large language models to generate final answers directly, often done by providing a task description and/or a few examples for in-context learning. Despite being effective enough to reach state-of-the-art or comparable performance in a large number of benchmarks, it suffers from being uninterpretable and lacking in robustness.</p>
<p>Chain-of-thought Chain-of-thought methods (Wei et al., 2022; Chowdhery et al., 2022a; Kojima et al., 2022; Chung et al., 2022, i.a.) improve the ability of large language models to perform complex reasoning by generating a series of intermediate reasoning steps. While achieving great success in various benchmarks, as a model-generated natural language, chain-of-thought suffers from unreliability and uncontrollability.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://beta.openai.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
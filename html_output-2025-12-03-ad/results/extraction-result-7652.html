<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270123533</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.19592v1.pdf" target="_blank">Why Larger Language Models Do In-context Learning Differently?</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7652",
    "paper_id": "paper-270123533",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00722075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Why Larger Language Models Do In-context Learning Differently?
30 May 2024</p>
<p>Zhenmei Shi 
Junyi Wei 
Zhuoyan Xu 
Yingyu Liang 
Why Larger Language Models Do In-context Learning Differently?
30 May 20241657F7B1C9B1DB5F18524D460CE303F0arXiv:2405.19592v1[cs.LG]
Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of incontext learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters.One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context.This work studies this observation theoretically aiming to improve the understanding of LLM and ICL.We analyze two stylized settings: (1) linear regression with one-layer singlehead linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model).In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors.This sheds light on where transformers pay attention to and how that affects ICL.Preliminary experimental results on large base and chat models provide positive support for our analysis.</p>
<p>Introduction</p>
<p>As large language models (LLM), e.g., ChatGPT (OpenAI, 2022) and GPT4 (OpenAI, 2023), are transforming AI development with potentially profound impact on our societies, it is critical to understand their mechanism for safe and efficient deployment.An important emergent ability (Wei et al., 2022b;An et al., 2023), which makes LLM successful, is in-context learning (ICL), where models are given a few exemplars of input-label pairs as part of the prompt 1 University of Wisconsin-Madison, 2 The University of Hong Kong.Correspondence to: Zhenmei Shi, Yingyu Liang <zhmeishi, yliang@cs.wisc.edu,yingyul@hku.hk>.</p>
<p>Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).before evaluating some new input.More specifically, ICL is a few-shot (Brown et al., 2020) evaluation method without updating parameters in LLM.Surprisingly, people find that, through ICL, LLM can perform well on tasks that have never been seen before, even without any finetuning.It means LLM can adapt to wide-ranging downstream tasks under efficient sample and computation complexity.The mechanism of ICL is different from traditional machine learning, such as supervised learning and unsupervised learning.For example, in neural networks, learning usually occurs in gradient updates, whereas there is only a forward inference in ICL and no gradient updates.Several recent works, trying to answer why LLM can learn in-context, argue that LLM secretly performs or simulates gradient descent as meta-optimizers with just a forward pass during ICL empirically (Dai et al., 2022;Von Oswald et al., 2023;Malladi et al., 2023) and theoretically (Zhang et al., 2023b;Ahn et al., 2023;Mahankali et al., 2023;Cheng et al., 2023;Bai et al., 2023;Huang et al., 2023;Li et al., 2023b;Guo et al., 2024;Wu et al., 2024).Although some insights have been obtained, the mechanism of ICL deserves further research to gain a better understanding.</p>
<p>Recently, there have been some important and surprising observations (Min et al., 2022;Pan et al., 2023;Wei et al., 2023b;Shi et al., 2023a) that cannot be fully explained by existing studies.In particular, Shi et al. (2023a) finds that LLM is not robust during ICL and can be easily distracted by an irrelevant context.Furthermore, Wei et al. (2023b) shows that when we inject noise into the prompts, the larger language models may have a worse ICL ability than the small language models, and conjectures that the larger language models may overfit into the prompts and forget the prior knowledge from pretraining, while small models tend to follow the prior knowledge.On the other hand, Min et al. (2022); Pan et al. (2023) demonstrate that injecting noise does not affect the in-context learning that much for smaller models, which have a more strong pretraining knowledge bias.To improve the understanding of the ICL mechanism, to shed light on the properties and inner workings of LLMs, and to inspire efficient and safe use of ICL, we are interested in the following question:</p>
<p>Why do larger language models do in-context learning differently?</p>
<p>To answer this question, we study two settings: (1) onelayer single-head linear self-attention network (Schlag et al., 2021;Von Oswald et al., 2023;Akyurek et al., 2023;Ahn et al., 2023;Zhang et al., 2023b;Mahankali et al., 2023;Wu et al., 2024) pretrained on linear regression in-context tasks (Garg et al., 2022;Raventos et al., 2023;Von Oswald et al., 2023;Akyurek et al., 2023;Bai et al., 2023;Mahankali et al., 2023;Zhang et al., 2023b;Ahn et al., 2023;Li et al., 2023c;Huang et al., 2023;Wu et al., 2024), with rank constraint on the attention weight matrices for studying the effect of the model scale;</p>
<p>(2) two-layer multiple-head transformers (Li et al., 2023b) pretrained on sparse parity classification in-context tasks, comparing small or large head numbers for studying the effect of the model scale.In both settings, we give the closed-form optimal solutions.We show that smaller models emphasize important hidden features while larger models cover more features, e.g., less important features or noisy features.Then, we show that smaller models are more robust to label noise and input noise during evaluation, while larger models may easily be distracted by such noises, so larger models may have a worse ICL ability than smaller ones.</p>
<p>We also conduct in-context learning experiments on five prevalent NLP tasks utilizing various sizes of the Llama model families (Touvron et al., 2023a;b), whose results are consistent with previous work (Min et al., 2022;Pan et al., 2023;Wei et al., 2023b) and our analysis.</p>
<p>Our contributions and novelty over existing work:</p>
<p>• We formalize new stylized theoretical settings for studying ICL and the scaling effect of LLM.See Section 4 for linear regression and Section 5 for parity.</p>
<p>• We characterize the optimal solutions for both settings (Theorem 4.1 and Theorem 5.1).</p>
<p>• The characterizations of the optimal elucidate different attention paid to different hidden features, which then leads to the different ICL behavior (Theorem 4.2, Theorem 4.3, Theorem 5.2).</p>
<p>• We further provide empirical evidence on large base and chat models corroborating our theoretical analysis (Figure 1, Figure 2).</p>
<p>Note that previous ICL analysis paper may only focus on (1) the approximation power of transformers (Garg et al., 2022;Panigrahi et al., 2023;Guo et al., 2024;Bai et al., 2023;Cheng et al., 2023), e.g., constructing a transformer by hands which can do ICL, or (2) considering one-layer single-head linear self-attention network learning ICL on linear regression (Von Oswald et al., 2023;Akyurek et al., 2023;Mahankali et al., 2023;Zhang et al., 2023b;Ahn et al., 2023;Wu et al., 2024), and may not focus on the robustness analysis or explain the different behaviors.In this work, (1) we extend the linear model linear data analysis to the non-linear model and non-linear data setting, i.e., two-layer multiple-head transformers leaning ICL on sparse parity classification and (2) we have a rigorous behavior difference analysis under two settings, which explains the empirical observations and provides more insights into the effect of attention mechanism in ICL.</p>
<p>Related Work</p>
<p>Large language model.Transformer-based (Vaswani et al., 2017) neural networks have rapidly emerged as the primary machine learning architecture for tasks in natural language processing.Pretrained transformers with billions of parameters on broad and varied datasets are called large language models (LLM) or foundation models (Bommasani et al., 2021), e.g., BERT (Devlin et al., 2019), PaLM (Chowdhery et al., 2022), Llama (Touvron et al., 2023a), ChatGPT (Ope-nAI, 2022), GPT4 (OpenAI, 2023) and so on.LLM has shown powerful general intelligence (Bubeck et al., 2023) in various downstream tasks.To better use the LLM for a specific downstream task, there are many adaptation methods, such as adaptor (Hu et al., 2022;Zhang et al., 2023c;Gao et al., 2023;Shi et al., 2023b), calibration (Zhao et al., 2021;Zhou et al., 2023a), multitask finetuning (Gao et al., 2021b;Xu et al., 2023;Von Oswald et al., 2023;Xu et al., 2024b), prompt tuning (Gao et al., 2021a;Lester et al., 2021), instruction tuning (Li &amp; Liang, 2021;Chung et al., 2022;Mishra et al., 2022), symbol tuning (Wei et al., 2023a), black-box tuning (Sun et al., 2022), chain-of-thoughts (Wei et al., 2022c;Khattab et al., 2022;Yao et al., 2023;Zheng et al., 2024), scratchpad (Nye et al., 2021), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and many so on.</p>
<p>In-context learning.One important emergent ability (Wei et al., 2022b) from LLM is in-context learning (ICL) (Brown et al., 2020).Specifically, when presented with a brief series of input-output pairings (known as a prompt) related to a certain task, they can generate predictions for test scenarios without necessitating any adjustments to the model's parameters.ICL is widely used in broad scenarios, e.g., reasoning (Zhou et al., 2022), negotiation (Fu et al., 2023), self-correction (Pourreza &amp; Rafiei, 2023), machine translation (Agrawal et al., 2022) and so on.Many works trying to improve the ICL and zero-shot ability of LLM (Min et al., 2021;Wang et al., 2022;Wei et al., 2022a;Iyer et al., 2022).There is a line of insightful works to study the mechanism of transformer learning (Geva et al., 2021;Xie et al., 2022;Garg et al., 2022;Jelassi et al., 2022;Arora &amp; Goyal, 2023;Li et al., 2023a;d;Allen-Zhu &amp; Li, 2023;Luo et al., 2023;Tian et al., 2023a;b;Zhou et al., 2023b;Bietti et al., 2023;Xu et al., 2024a;Gu et al., 2024a;b;c;d;e) and in-context learning (Dai et al., 2022;Mahankali et al., 2023;Raventos et al., 2023;Bai et al., 2023;Ahn et al., 2023;Von Oswald et al., 2023;Pan et al., 2023;Li et al., 2023b;c;e;Akyurek et al., 2023;Zhang et al., 2023a;b;Huang et al., 2023;Cheng et al., 2023;Wibisono &amp; Wang, 2023;Wu et al., 2024;Guo et al., 2024;Reddy, 2024) empirically and theoretically.On the basis of these works, our analysis takes a step forward to show the ICL behavior difference under different scales of language models.</p>
<p>Preliminary</p>
<p>Notations.We denote [n] := {1, 2, . . ., n}.For a positive semidefinite matrix A, we denote ∥x∥ 2 A := x ⊤ Ax as the norm induced by a positive definite matrix A. We denote ∥ • ∥ F as the Frobenius norm.diag() function will map a vector to a diagonal matrix or map a matrix to a vector with its diagonal terms.</p>
<p>In-context learning.We follow the setup and notation of the problem in Zhang et al. (2023b);Mahankali et al. (2023); Ahn et al. (2023); Huang et al. (2023); Wu et al. (2024).In the pretraining stage of ICL, the model is pretrained on prompts.A prompt from a task τ is formed by N examples (x τ,1 , y τ,1 ), . . ., (x τ,N , y τ,N ) and a query token x τ,q for prediction, where for any i ∈ [N ] we have y τ,i ∈ R and x τ,i , x τ,q ∈ R d .The embedding matrix E τ , the label vector y τ , and the input matrix X τ are defined as:
E τ :=
x τ,1 x τ,2 . . .x τ,N x τ,q y τ,1 y τ,2 . . .y τ,N 0 ∈ R (d+1)×(N +1) ,
y τ :=[y τ,1 , . . . , y τ,N ] ⊤ ∈ R N , y τ,q ∈ R, X τ :=[x τ,1 , . . . , x τ,N ] ⊤ ∈ R N ×d , x τ,q ∈ R d .
Given prompts represented as E τ 's and the corresponding true labels y τ,q 's, the pretraining aims to find a model whose output on E τ matches y τ,q .After pretraining, the evaluation stage applies the model to a new test prompt (potentially from a different task) and compares the model output to the true label on the query token.</p>
<p>Note that our pretraining stage is also called learning to learn in-context (Min et al., 2021) or in-context training warmup (Dong et al., 2022) in existing work.Learning to learn in-context is the first step to understanding the mechanism of ICL in LLM following previous works (Raventos et al., 2023;Zhou et al., 2023b;Zhang et al., 2023b;Mahankali et al., 2023;Ahn et al., 2023;Huang et al., 2023;Li et al., 2023b;Wu et al., 2024).</p>
<p>Linear self-attention networks.The linear self-attention network has been widely studied (Schlag et al., 2021;Von Oswald et al., 2023;Akyurek et al., 2023;Ahn et al., 2023;Zhang et al., 2023b;Mahankali et al., 2023;Wu et al., 2024;Ahn et al., 2024), and will be used as the learning model or a component of the model in our two theoretical settings.It is defined as:
f LSA,θ (E) = E + W P V E • E ⊤ W KQ E ρ ,(1)
where θ = (W P V , W KQ ), E ∈ R (d+1)×(N +1) is the embedding matrix of the input prompt, and ρ is a normalization factor set to be the length of examples, i.e., ρ = N during pretraining.Similar to existing work, for simplicity, we have merged the projection and value matrices into W P V , and merged the key and query matrices into W KQ , and have a residual connection in our LSA network.The prediction of the network for the query token x τ,q will be the bottom right entry of the matrix output, i.e., the entry at location (d + 1), (N + 1), while other entries are not relevant to our study and thus are ignored.So only part of the model parameters are relevant.To see this, let us denote d+1) ,
W P V = W P V 11 w P V 12 (w P V 21 ) ⊤ w P V 22 ∈ R (d+1)×(W KQ = W KQ 11 w KQ 12 (w KQ 21 ) ⊤ w KQ 22 ∈ R (d+1)×(d+1) ,
where
W P V 11 , W KQ 11 ∈ R d×d ; w P V 12 , w P V 21 , w KQ 12 , w KQ 21 ∈ R d ; and w P V 22 , w KQ 22 ∈ R.
Then the prediction is:
y τ,q =f LSA,θ (E) (d+1),(N +1)
(2)
= (w P V 21 ) ⊤ w P V 22 EE ⊤ ρ W KQ 11 (w KQ 21 ) ⊤ x τ,q .</p>
<p>Linear Regression</p>
<p>In this section, we consider the linear regression task for incontext learning which is widely studied empirically (Garg et al., 2022;Raventos et al., 2023;Von Oswald et al., 2023;Akyurek et al., 2023;Bai et al., 2023) and theoretically (Mahankali et al., 2023;Zhang et al., 2023b;Ahn et al., 2023;Li et al., 2023c;Huang et al., 2023;Wu et al., 2024).</p>
<p>Data and task.For each task τ , we assume for any i ∈ [N ] tokens x τ,i , x τ,q i.i.d.</p>
<p>∼ N (0, Λ), where Λ is the covariance matrix.We also assume a d-dimension task weight w τ i.i.d.</p>
<p>∼ N (0, I d×d ) and the labels are given by y τ,i = ⟨w τ , x τ,i ⟩ and y τ,q = ⟨w τ , x τ,q ⟩.Model and loss.We study a one-layer single-head linear self-attention transformer (LSA) defined in Equation (1) and we use y τ,q := f LSA,θ (E) (d+1),(N +1) as the prediction.We consider the mean square error (MSE) loss so that the empirical risk over B independent prompts is defined as
L(f θ ) := 1 2B B τ =1 ( y τ,q − ⟨w τ , x τ,q ⟩) 2 .
Measure model scale by rank.We first introduce a lemma from previous work that simplifies the MSE and justifies our measurement of the model scale.For notation simplicity, we denote U = W KQ 11 , u = w P V 22 .Lemma 4.1 (Lemma A.1 in Zhang et al. (2023b)).Let
Γ := 1 + 1 N Λ + 1 N tr(Λ)I d×d ∈ R d×d . Let L(f LSA,θ ) = lim B→∞ L(f LSA,θ ) = 1 2 E wτ ,xτ,1,...,x τ,N ,xτ,q ( y τ,q − ⟨w τ , x τ,q ⟩) 2 , l(U, u) = tr 1 2 u 2 ΓΛUΛU ⊤ − uΛ 2 U ⊤ , we have L(f LSA,θ ) = l(U, u) + C
, where C is a constant independent with θ.</p>
<p>Lemma 4.1 tells us that the loss only depends on uU.If we consider non-zero u, w.l.o.g, letting u = 1, then we can see that the loss only depends on U ∈ R d×d ,
L(f LSA,θ ) = tr 1 2 ΓΛUΛU ⊤ − Λ 2 U ⊤ .
Note that U = W KQ 11 , then it is natural to measure the size of the model by rank of U. Recall that we merge the key matrix and the query matrix in attention together, i.e.,
W KQ = (W K ) ⊤ W Q . Thus, a low-rank U is equivalent to the constraint W K , W Q ∈ R r×d where r ≪ d.
The low-rank key and query matrix are practical and have been widely studied (Hu et al., 2022;Chen et al., 2021;Bhojanapalli et al., 2020;Fan et al., 2021;Dass et al., 2023;Shi et al., 2023c).Therefore, we use r = rank(U) to measure the scale of the model, i.e., larger r representing larger models.To study the behavior difference under different model scale, we will analyze U under different rank constraints.</p>
<p>Low Rank Optimal Solution</p>
<p>Since the token covariance matrix Λ is positive semidefinite symmetric, we have eigendecomposition Λ = QDQ ⊤ , where Q is an orthonormal matrix containing eigenvectors of Λ and D is a sorted diagonal matrix with nonnegative entries containing eigenvalues of Λ, denoting as
D = diag([λ 1 , . . . , λ d ]), where λ 1 ≥ • • • ≥ λ d ≥ 0.
Then, we have the following theorem.Then
U * = cQV * Q ⊤ , u = 1 c
, where c is any nonzero constant, and
V * = diag([v * 1 , . . . , v * d ]) satisfies for any i ≤ r, v * i = N (N +1)λi+tr(D) and for any i &gt; r, v * i = 0.
Proof sketch of Theorem 4.1.We defer the full proof to Appendix B.1.The proof idea is that we can decompose the loss function into different ranks, so we can keep the direction by their sorted "variance", i.e., argmin
U∈R d×d ,rank(U)≤r,u∈R l(U, u) = d i=1 T i λ 2 i v * i − 1 T i 2 ,
where
T i = 1 + 1 N λ i + tr(D) N . We have that v * i ≥ 0 for any i ∈ [d] and if v * i &gt; 0, we have v * i = 1 Ti . Denote g(x) = x 2 1 (1+ 1 N )x+ tr(D)</p>
<p>N</p>
<p>. We get the conclusion by g(x) is an increasing function on [0, ∞).</p>
<p>Theorem 4.1 gives the closed-form optimal rank-r solution of one-layer single-head linear self-attention transformer learning linear regression ICL tasks.Let f LSA,θ denote the optimal rank-r solution corresponding to the U * , u * above.</p>
<p>In detail, the optimal rank-r solution f LSA,θ satisfies
W * P V = 0 d×d 0 d 0 ⊤ d u , W * KQ = U * 0 d 0 ⊤ d 0 .(3)
What hidden features does the model pay attention to?Theorem 4.1 shows that the optimal rank-r solution indeed is the truncated version of the optimal full-rank solution, keeping only the most important feature directions (i.e., the first r eigenvectors of the token covariance matrix).In detail, (1) for the optimal full-rank solution, we have for any
i ∈ [d], v * i = N (N +1)λi+tr(D) ;
(2) for the optimal rank-r solution, we have for any i ≤ r, v * i = N (N +1)λi+tr(D) and for any i &gt; r, v * i = 0.That is, the small rank-r model keeps only the first r eigenvectors (viewed as hidden feature directions) and does not cover the others, while larger ranks cover more hidden features, and the large full rank model covers all features.</p>
<p>Recall that the prediction depends on U * x τ,q = cQV * Q ⊤ x τ,q ; see Equation ( 2) and (3).So the optimal rank-r model only uses the components on the first r eigenvector directions to do the prediction in evaluations.When there is noise distributed in all directions, a smaller model can ignore noise and signals along less important directions but still keep the most important directions.Then it can be less sensitive to the noise, as empirically observed.This insight is formalized in the next subsection.</p>
<p>Behavior Difference</p>
<p>We now formalize our insight into the behavior difference based on our analysis on the optimal solutions.We consider the evaluation prompt to have M examples (may not be equal to the number of examples N during pretraining for a general evaluation setting), and assume noise in labels to facilitate the study of the behavior difference (our results can be applied to the noiseless case by considering noise level σ = 0).Formally, the evaluation prompt is:
E := x 1 x 2 . . . x M x q y 1 y 2 . . . y M 0 ∈ R (d+1)×(M +1) = x 1 . . . x M x q ⟨w, x 1 ⟩ + ϵ 1 . . . ⟨w, x M ⟩ + ϵ M 0 ,
where w is the weight for the evaluation task, and for any i ∈ [M ], the label noise ϵ i i.i.d.</p>
<p>∼ N (0, σ 2 ).</p>
<p>Recall Q are eigenvectors of Λ, i.e., Λ = QDQ ⊤ and
D = diag([λ 1 , . . . , λ d ]).
In practice, we can view the large variance part of x (top r directions in Q) as a useful signal (like words "positive", "negative"), and the small variance part (bottom d − r directions in Q) as the less important or useless information (like words "even", "just").</p>
<p>Based on such intuition, we can decompose the evaluation task weight w accordingly: w = Q(s+ξ), where the r-dim truncated vector s ∈ R d has s i = 0 for any r &lt; i ≤ d, and the residual vector ξ ∈ R d has ξ i = 0 for any 1 ≤ i ≤ r.</p>
<p>The following theorem (proved in Appendix B.2) quantifies the evaluation loss at different model scales r which can explain the scale's effect.
L(f LSA,θ ; E) :=E x1,ϵ1,...,x M ,ϵ M ,xq f LSA,θ ( E) − ⟨w, x q ⟩ 2 = 1 M ∥s∥ 2 (V * ) 2 D 3 + 1 M ∥s + ξ∥ 2 D + σ 2 tr (V * ) 2 D 2 + ∥ξ∥ 2 D + i∈[r] s 2 i λ i (λ i v * i − 1) 2 .
Implications.If N is large enough with N λ r ≫ tr(D) (which is practical as we usually pretrain networks on long text), then
L(f LSA,θ ; E)≈∥ξ∥ 2 D + 1 M (r + 1)∥s∥ 2 D + r∥ξ∥ 2 D + rσ 2 .
The first term ∥ξ∥ 2 D is due to the residual features not covered by the network, so it decreases for larger r and becomes 0 for full-rank r = d.The second term 1 M (•) is significant since we typically have limited examples in evaluation, e.g., M = 16 ≪ N .Within it, (r + 1)∥s∥ 2 D corresponds to the first r directions, and rσ 2 corresponds to the label noise.These increase for larger r.So there is a trade-off between the two error terms when scaling up the model: for larger r the first term decreases while the second term increases.This depends on whether more signals are covered or more noise is kept when increasing the rank r.</p>
<p>To further illustrate the insights, we consider the special case when the model already covers all useful signals in the evaluation task: w = Qs, i.e., the label only depends on the top r features (like "positive", "negative" tokens).Our above analysis implies that a larger model will cover more useless features and keep more noise, and thus will have worse performance.This is formalized in the following theorem (proved in Appendix B.2).</p>
<p>Theorem 4.3 (Behavior difference for regression, special case).Let 0 ≤ r ≤ r ′ ≤ d and w = Qs where s is r-dim truncated vector.Denote the optimal rank-r solution as f 1 and the optimal rank-r ′ solution as f 2 .Then,
L(f 2 ; E) − L(f 1 ; E) = 1 M ∥s∥ 2 D + σ 2   r ′ i=r+1 N λ i (N + 1) λ i + tr(D) 2   .
Implications.By Theorem 4.3, in this case,
L(f 2 ; E) − L(f 1 ; E) ≈ r ′ − r M ∥s∥ 2 D input noise + r ′ − r M σ 2 label noise
.</p>
<p>We can decompose the above equation to input noise and label noise, and we know that ∥s∥ 2 D + σ 2 only depends on the intrinsic property of evaluation data and is independent of the model size.When we have a larger model (larger r ′ ), we will have a larger evaluation loss gap between the large and small models.It means larger language models may be easily affected by the label noise and input noise and may have worse in-context learning ability, while smaller models may be more robust to these noises as they only emphasize important signals.Moreover, if we increase the label noise scale σ 2 on purpose, the larger models will be more sensitive to the injected label noise.This is consistent with the observation in Wei et al. (2023b); Shi et al. (2023a) and our experimental results in Section 6.</p>
<p>Sparse Parity Classification</p>
<p>We further consider a more sophisticated setting with nonlinear data which necessitates nonlinear models.Viewing sentences as generated from various kinds of thoughts and knowledge that can be represented as vectors in some hidden feature space, we consider the classic data model of dictionary learning or sparse coding, which has been widely used for text and images (Olshausen &amp; Field, 1997;Vinje &amp; Gallant, 2000;Blei et al., 2003).Furthermore, beyond linear separability, we assume the labels are given by the (d, 2)-sparse parity on the hidden feature vector, which is the high-dimensional generalization of the classic XOR problem.Parities are a canonical family of highly non-linear learning problems and recently have been used in many recent studies on neural network learning (Daniely &amp; Malach, 2020;Barak et al., 2022;Shi et al., 2022;2023d).Data and task.Let X = R d be the input space, and Y = {±1} be the label space.Suppose G ∈ R d×d is an unknown dictionary with d columns that can be regarded as features; for simplicity, assume G is orthonormal.Let ϕ ∈ {±1} d be a hidden vector that indicates the presence of each feature.The data are generated as follows: for each task τ , generate two task indices t τ = (i τ , j τ ) which determines a distribution T τ ; then for this task, draw examples by ϕ ∼ T τ , and setting x = Gϕ (i.e., dictionary learning data), y = ϕ iτ ϕ jτ (i.e., XOR labels).</p>
<p>We now specify how to generate t τ and ϕ.As some of the hidden features are more important than others, we let A = [k] denote a subset of size k corresponding to the important features.We denote the important task set as
S 1 := A × A \ {(l, l) : l ∈ A} and less important task set as S 2 := [d] × [d] \ ({(l, l) : l ∈ [d]} ∪ S 1
).Then t τ is drawn uniformly from S 1 with probability 1 − p T , and uniformly from S 2 with probability p T , where p T ∈ [0, 1 2 ) is the less-important task rate.For the distribution of ϕ, we assume ϕ [d]{iτ ,jτ } is drawn uniformly from {±1} d−2 , and assume ϕ {iτ ,jτ } has good correlation (measured by a parameter γ ∈ (0, 1 4 )) with the label to facilitate learning.Independently, we have
Pr[(ϕ iτ , ϕ jτ ) = (1, 1)] = 1/4 + γ, Pr[(ϕ iτ , ϕ jτ ) = (1, −1)] = 1/4, Pr[(ϕ iτ , ϕ jτ ) = (−1, 1)] = 1/4, Pr[(ϕ iτ , ϕ jτ ) = (−1, −1)] = 1/4 − γ.
Note that without correlation (γ = 0), it is well-known sparse parities will be hard to learn, so we consider γ &gt; 0.</p>
<p>Model.Following Wu et al. (2024), we consider the reduced linear self-attention f LSA,θ (X, y, x q ) = y ⊤ X N W KQ x q (which is a reduced version of Equation ( 1)), and also denote W KQ as W for simplicity.It is used as the neuron in our two-layer multiple-head transformers:
g(X, y, x q ) = i∈[m] a i σ y ⊤ X N W (i) x q , where σ is ReLU activation, a = [a 1 , . . . , a m ] ⊤ ∈ [−1, 1] m , W (i) ∈ R d×d
and m is the number of attention heads.Denote its parameters as θ = (a, W (1) , . . ., W (m) ).</p>
<p>This model is more complicated as it uses non-linear activation, and also has two layers with multiple heads.</p>
<p>Measure model scale by head number.We use the attention head number m to measure the model scale, as a larger m means the transformer can learn more attention patterns.We consider hinge loss ℓ(z) = max(0, 1 − z), and the population loss with weight-decay regularization:
L λ (g) =E [ℓ (y q • g(X, y, x q ))] + λ   i∈[m] ∥W (i) ∥ 2 F   .
Suppose N → ∞ and let the optimal solution of L λ (g) be
g * = argmin g lim λ→0 +
L λ (g).</p>
<p>Optimal Solution</p>
<p>We first introduce some notations to describe the optimal.</p>
<p>Let bin(•) be the integer to binary function, e.g., bin(6) = 110.Let digit(z, i) denote the digit at the i-th position (from right to left) of z, e.g., digit(01000, 4) = 1.We are now ready to characterize the optimal solution (proved in Appendix C.1).</p>
<p>Theorem 5.1 (Optimal solution for parity).Consider k = 2 ν1 , d = 2 ν2 , and let g * 1 and g * 2 denote the optimal solutions for m = 2(ν 1 + 1) and m = 2(ν 2 + 1), respectively.When
0 &lt; p T &lt; 1 4 −γ d(d−1) 2 ( 1 4 +γ)+ 1 4 −γ
, g * 1 neurons are a subset of g * 2 neurons.Specifically, for any i ∈ [2(ν 2 + 1)], let V * ,(i) be diagonal matrix and
• For any i ∈ [ν 2 ] and i τ ∈ [d], let a * i = −1 and V * ,(i)
iτ ,iτ = (2 digit(bin(i τ − 1), i) − 1)/(4γ).</p>
<p>• For i = ν 2 + 1 and any i τ ∈ [d], let a * i = +1 and V * ,(i) iτ ,iτ = −ν j /(4γ) for g * j .</p>
<p>•
For i ∈ [2(ν 2 + 1)] \ [ν 2 + 1], let a * i = a * i−ν2−1 and V * ,(i) = −V * ,(i−ν2−1) . Let W * ,(i) = GV * ,(i) G ⊤ . Up to permutations, g * 2 has neurons (a * , W * ,(1) , . . . , W * ,(m) ) and g * 1 has the {1, . . . , ν 1 , ν 2 + 1, ν 2 + 2 . . . , ν 2 + ν 1 + 1, 2ν 2 + 2}-th neurons of g * 2 .
Proof sketch of Theorem 5.1.The proof is challenging as the non-linear model and non-linear data.We defer the full proof to Appendix C.1.The high-level intuition is transferring the optimal solution to patterns covering problems.For small p T , the model will "prefer" to cover all patterns in S 1 first.When the model becomes larger, by checking the sufficient and necessary conditions, it will continually learn to cover non-important features.Thus, the smaller model will mainly focus on important features, while the larger model will focus on all features.</p>
<p>Example for Theorem 5.1.When ν 2 = 3, the optimal has a 1 = a 2 = a 3 = −1, a 4 = +1 and,
V (1) = 1/4γ • diag([−1, +1, −1, +1, −1, +1, −1, +1]) V (2) = 1/4γ • diag([−1, −1, +1, +1, −1, −1, +1, +1]) V (3) = 1/4γ • diag([−1, −1, −1, −1, +1, +1, +1, +1]) V (4) = 3/4γ • diag([−1, −1, −1, −1, −1, −1, −1, −1]) and V (i+4) = −V (i) , a i+4 = a i for i ∈ [4].
On the other hand, the optimal g * 1 for ν 1 = 1 has the {1, 4, 5, 8}-th neurons of g * 2 .By carefully checking, we can see that the neurons in g * 1 (i.e., the {1, 4, 5, 8}-th neurons of g * 2 ) are used for parity classification task from S 1 , i.e, label determined by the first k = 2 ν1 = 2 dimensions.With the other neurons (i.e., the {2, 3, 6, 7}-th neurons of g * 2 ), g * 2 can further do parity classification on the task from S 2 , label determined by any two dimensions other than the first two dimensions.</p>
<p>What hidden features does the model pay attention to?Theorem 5.1 gives the closed-form optimal solution of twolayer multiple-head transformers learning sparse-parity ICL tasks.It shows the optimal solution of the smaller model indeed is a sub-model of the larger optimal model.In detail, the smaller model will mainly learn all important features, while the larger model will learn more features.This again shows a trade-off when increasing the model scale: larger models can learn more hidden features which can be beneficial if these features are relevant to the label, but also potentially keep more noise which is harmful.</p>
<p>Behavior Difference</p>
<p>Similar to Theorem 4.3, to illustrate our insights, we will consider a setting where the smaller model learns useful features for the evaluation task while the larger model covers extra features.That is, for evaluation, we uniformly draw a task t τ = (i τ , j τ ) from S 1 , and then draw M samples to form the evaluation prompt in the same way as during pretraining.To present our theorem (proved in Appendix C.2 using Theorem 5.1), we introduce some notations.Let
D 1 = diag(V * ,(1) ), . . . , diag(V * ,(ν1) ), diag(V * ,(ν2+1) ), . . . , diag(V * ,(ν2+ν1+1) ), diag(V * ,(2ν2+2) ) ∈ R d×2(ν1+1) D 2 = diag(V * ,(1) ), . . . , diag(V * ,(2ν2+2) ) ∈ R d×2(ν2+1) ,
where for any i ∈ [2(ν 2 + 1)], V * ,(i) is defined in Theorem 5.1.Let φτ,q ∈ R d satisfy φτ,q,iτ = ϕ τ,q,iτ , φτ,q,jτ = ϕ τ,q,jτ and all other entries being zero.For a matrix Z and a vector v, let P Z denote the projection of v to the space of Z, i.e., P Z
(v) = Z(Z ⊤ Z) −1 Z ⊤ v.
Theorem 5.2 (Behavior difference for parity).Assume the same condition as Theorem 5.1.For j ∈ {1, 2}, Let θ j denote the parameters of g * j .For l ∈ [M ], let ξ l be uniformly drawn from {±1} d , and Ξ = l∈[M ] ξ l M .Then, for any δ ∈ (0, 1), with probability at least 1 − δ over the randomness of test data, we have g * j (X τ , y τ , x τ,q ) = h(θ j , 2γ φτ,q + P Dj (Ξ)) + ϵ j
:= i∈[m]
a * i σ diag V * ,(i) ⊤ 2γ φτ,q + P Dj (Ξ) +ϵ j where ϵ j = O νj M log 1 δ and we have</p>
<p>• 2γ φτ,q is the signal useful for prediction: 0 = ℓ(y q • h(θ 1 , 2γ φτ,q )) = ℓ(y q • h(θ 2 , 2γ φτ,q )).</p>
<p>• P D1 (Ξ)) and P D2 (Ξ)) is noise not related to labels, and
E[∥P D 1 (Ξ))∥ 2 2 ] E[∥P D 2 (Ξ))∥ 2 2 ] = ν1+1 ν2+1 .
Implications.Theorem 5.2 shows that during evaluation, we can decompose the input into two parts: signal and noise.Both the larger model and smaller model can capture the signal part well.However, the smaller model has a much smaller influence from noise than the larger model, i.e., the ratio is ν1+1 ν2+1 .The reason is that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus, smaller models are more robust to noise while larger ones are easily distracted, leading to different ICL behaviors.This again sheds light on where transformers pay attention to and how that affects ICL.</p>
<p>Remark 5.1.Here, we provide a detailed intuition about Theorem 5.2.Ξ is the input noise.When we only care about the noise part, we can rewrite the smaller model as g 1 = h(θ 1 , P D1 (Ξ)), and the larger model as g 2 = h(θ 2 , P D2 (Ξ)), where they share the same h function.</p>
<p>Our conclusion says that E[∥P
D1 (Ξ)∥ 2 2 ]/E[∥P D2 (Ξ)∥ 2 2 ] = (ν 1 + 1)/(ν 2 + 1)
, which means the smaller model's "effect" input noise is smaller than the larger model's "effect" input noise.Although their original input noise is the same, as the smaller model only focuses on limited features, the smaller model will ignore part of the noise, and the "effect" input noise is small.However, the larger model is the opposite.</p>
<p>Experiments</p>
<p>Brilliant recent work (Wei et al., 2023b) runs intensive and thorough experiments to show that larger language models do in-context learning differently.Following their idea, we conduct similar experiments on binary classification datasets, which is consistent with our problem setting in the parity case, to support our theory statements.  .Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (original/without instruct turning).Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.</p>
<p>Experimental setup.Following the experimental protocols in Wei et al. (2023b); Min et al. (2022), we conduct experiments on five prevalent NLP tasks, leveraging datasets from GLUE (Wang et al., 2018) tasks and Subj (Conneau &amp; Kiela, 2018).Our experiments utilize various sizes of the Llama model families (Touvron et al., 2023a;b): 3B, 7B, 13B, 70B.We follow the prior work on in-context learning (Wei et al., 2023b) and use M = 16 in-context exemplars.We aim to assess the models' ability to use inherent semantic biases from pretraining when facing in-context examples.As part of this experiment, we introduce noise by inverting an escalating percentage of in-context example labels.To illustrate, a 100% label inversion for the SST-2 dataset implies that every "positive" exemplar is now labeled "negative".Note that while we manipulate the in-context example labels, the evaluation sample labels remain consistent.We use the same templates as (Min et al., 2021), a sample evaluation for SST-2 when M = 2: sentence: show us a good time The answer is positive.</p>
<p>sentence: as dumb and cheesy</p>
<p>The answer is negative.sentence: it 's a charming and often affecting journey The answer is</p>
<p>Behavior Difference</p>
<p>Figure 1 shows the result of model performance (chat/with instruct turning) across all datasets with respect to the proportion of labels that are flipped.When 0% label flips, we observe that larger language models have better in-context abilities.On the other hand, the performance decrease facing noise is more significant for larger models.As the percentage of label alterations increases, which can be viewed as increasing label noise σ 2 , the performance of small models remains flat and seldom is worse than random guessing while large models are easily affected by the noise, as predicted by our analysis.These results indicate that large models can override their pretraining biases in-context inputlabel correlations, while small models may not and are more robust to noise.This observation aligns with the findings in Wei et al. (2023b) and our analysis.</p>
<p>We can see a similar or even stronger phenomenon in Figure 2: larger models are more easily affected by noise (flipped labels) and override pretrained biases than smaller models for the original/without instruct turning version (see the "Average" sub-figure).On the one hand, we conclude that both large base models and large chat models suffer from ICL robustness issues.On the other hand, this is also consistent with recent work suggesting that instruction tuning will impair LLM's in-context learning capability.</p>
<p>Ablation Study</p>
<p>To further verify our analysis, we provide an ablation study.We concatenate an irrelevant sentence from GSM-IC (Shi et al., 2023a) to an input-label pair sentence from SST-2 in GLUE dataset.We use "correct" to denote the original label and "wrong" to denote the flipped label.Then, we measure the magnitude of correlation between labelinput, by computing the norm of the last row of attention maps across all heads in the final layer.We do this between "correct"/"wrong" labels and the original/irrelevant inserted sentences.Figure 3 shows the results on 100 evaluation prompts; for example, the subfigure Correct+Relevant shows the correlation magnitude between the "correct" label and the original input sentence in each prompt.The results show that the small model Llama 2-13b mainly focuses on the relevant part (original input) and may ignore the irrelevant sentence, while the large model Llama 2-70b focuses on both sentences.This well aligns with our analysis.</p>
<p>More Discussions about Noise</p>
<p>There are three kinds of noise covered in our analysis:</p>
<p>Pretraining noise.We can see it as toxic or harmful pretraining data on the website (noisy training data).The model will learn these features and patterns.It is covered by ξ in the linear regression case and S 2 in the parity case.</p>
<p>Input noise during inference.We can see it as natural noise as the user's wrong spelling or biased sampling.It is a finite sampling error as x drawn from the Gaussian distribution for the linear regression case and a finite sampling error as x drawn from a uniform distribution for the parity case.</p>
<p>Label noise during inference.We can see it as adversarial examples, or misleading instructions, e.g., deliberately letting a model generate a wrong fact conclusion or harmful solution, e.g., poison making.It is σ in the linear regression case and S 2 in the parity case.</p>
<p>For pretraining noise, it will induce the model to learn noisy or harmful features.During inference, for input noise and label noise, the larger model will pay additional attention to these noisy or harmful features in the input and label pair, i.e., y • x, so that the input and label noise may cause a large perturbation in the final results.If there is no pretraining noise, then the larger model will have as good robustness as the smaller model.Also, if there is no input and label noise, the larger model will have as good robustness as the smaller model.The robustness gap only happens when both pretraining noise and inference noise exist simultaneously.</p>
<p>Conclusion</p>
<p>In this work, we answered our research question: why do larger language models do in-context learning differently?Our theoretical study showed that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus the former are more robust to noise while the latter are more easily distracted, leading to different behaviors during in-context learning.Our empirical results provided positive support for the theoretical analysis.Our findings can help improve understanding of LLMs and ICL, and better training and application of these models.
l(U, u) − min U∈R d×d ,u∈R l(U, u) = 1 2 Γ 1 2 uΛ 1 2 UΛ 1 2 − ΛΓ −1 2 F = 1 2 Γ 1 2 Λ 1 2 uU − Γ −1 Λ 1 2 2 F = 1 2 D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F .
As W KQ is a matrix whose rank is at most r, we have V is also at most rank r.Then, we denote
V * = argmin V∈R d×d ,rank(V)≤r D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F . We can see that V * is a diagonal matrix. Denote D ′ = diag([λ ′ 1 , . . . , λ ′ d ]) and V * = diag([v * 1 , . . . , v * d ]
).Then, we have
D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F (4) = d i=1 λ ′ i 1 2 λ i v * i − 1 λ ′ i 2 (5) = d i=1 1 + 1 N λ i + tr(D) N λ 2 i v * i − 1 1 + 1 N λ i + tr(D) N 2 . (6)
As V * is the minimum rank r solution, we have that v * i ≥ 0 for any i ∈ [d] and if v * i &gt; 0, we have
v * i = 1 (1+ 1 N )λi+ tr(D) N . Denote g(x) = 1 + 1 N x + tr(D) N x 2 1 (1+ 1 N )x+ tr(D) N 2 = x 2 1 (1+ 1 N )x+ tr(D)</p>
<p>N</p>
<p>. It is easy to see that g(x) is an increasing function on [0, ∞).Now, we use contradiction to show that V * only has non-zero entries in the first r diagonal entries.Suppose i &gt; r, such that v * i &gt; 0, then we must have j ≤ r such that v * j = 0 as V * is a rank r solution.We find that if we set
v * i = 0, v * j = 1 (1+ 1 N )λj+ tr(D)
N and all other values remain the same, Equation (6) will strictly decrease as g(x) is an increasing function on [0, ∞).Thus, here is a contradiction.We finish the proof by V * = uQ ⊤ U * Q.</p>
<p>B.2. Behavior Difference</p>
<p>Theorem 4.2 (Behavior difference for regression).Let w = Q(s + ξ) ∈ R d where s, ξ ∈ R d are truncated and residual vectors defined above.The optimal rank-r solution f LSA,θ in Theorem 4.1 satisfies:
L(f LSA,θ ; E) :=E x1,ϵ1,...,x M ,ϵ M ,xq f LSA,θ ( E) − ⟨w, x q ⟩ 2 = 1 M ∥s∥ 2 (V * ) 2 D 3 + 1 M ∥s + ξ∥ 2 D + σ 2 tr (V * ) 2 D 2 + ∥ξ∥ 2 D + i∈[r] s 2 i λ i (λ i v * i − 1) 2 .
Proof of Theorem 4.2.By Theorem 4.1, w.l.o.g, letting c = 1, the optimal rank-r solution f LSA,θ satisfies θ = (W P V , W KQ ), and
W * P V = 0 d×d 0 d 0 ⊤ d 1 , W * KQ = U * 0 d 0 ⊤ d 0 ,
where
U * = QV * Q ⊤ .
We can see that U * and Λ commute.Denote Λ := 1 M M i=1 x i x ⊤ i .Note that we have
y q =f LSA,θ ( E) = 0 d×d 0 d 0 ⊤ d 1 E E ⊤ M U * 0 d 0 ⊤ d 0 x q = 0 d×d 0 d 0 ⊤ d 1   1 M x q x ⊤ q + M i=1 x i x ⊤ i 1 M M i=1 x i x ⊤ i w + M i=1 ϵ i x i 1 M M i=1 w ⊤ x i x ⊤ i + M i=1 ϵ i x ⊤ i 1 M M i=1 (w ⊤ x i + ϵ i ) 2   • U * 0 d 0 ⊤ d 0 x q = w ⊤ Λ + 1 M M i=1 ϵ i x ⊤ i U * x q .
Then, we have
E x1,ϵ1,...,x M ,ϵ M ,xq ( y q − ⟨w, x q ⟩) 2 =E x1,ϵ1,...,x M ,ϵ M ,xq w ⊤ ΛU * x q + 1 M M i=1 ϵ i x ⊤ i U * x q − w ⊤ x q 2 = E w ⊤ ΛU * x q − w ⊤ x q 2 (I) + E   1 M M i=1 ϵ i x ⊤ i U * x q 2   (II)
,</p>
<p>where the last equality is due to i.i.d. of ϵ i .We see that the label noise can only have an effect in the second term.For the term (I) we have,
(I) =E w ⊤ ΛU * x q − w ⊤ ΛU * x q + w ⊤ ΛU * x q − w ⊤ x q 2 = E w ⊤ ΛU * x q − w ⊤ ΛU * x q 2 (III) + E w ⊤ ΛU * x q − w ⊤ x q 2 (IV)
,</p>
<p>where the last equality is due to E[ Λ] = Λ and Λ is independent with x q .Note the fact that U * and Λ commute.For the (III) term, we have
(III) =E E w ⊤ ΛU * x q 2 + w ⊤ ΛU * x q 2 − 2 w ⊤ ΛU * x q w ⊤ ΛU * x q x q =E w ⊤ ΛU * x q 2 − w ⊤ ΛU * x q 2 .
By the property of trace, we have,
(III) =E tr Λww ⊤ Λ(U * ) 2 Λ − ∥w∥ 2 (U * ) 2 Λ 3 =E 1 M 2 tr M i=1 x i x ⊤ i ww ⊤ M i=1 x i x ⊤ i (U * ) 2 Λ − ∥w∥ 2 (U * ) 2 Λ 3 =E M − 1 M tr Λww ⊤ Λ(U * ) 2 Λ + 1 M tr x 1 x ⊤ 1 ww ⊤ x 1 x ⊤ 1 (U * ) 2 Λ − ∥w∥ 2 (U * ) 2 Λ 3 = − 1 M ∥w∥ 2 (U * ) 2 Λ 3 + 1 M E tr x 1 x ⊤ 1 ww ⊤ x 1 x ⊤ 1 (U * ) 2 Λ = − 1 M ∥w∥ 2 (U * ) 2 Λ 3 + 1 M E tr ∥w∥ 2 Λ Λ + 2Λw ⊤ wΛ (U * ) 2 Λ = 1 M ∥w∥ 2 (U * ) 2 Λ 3 + 1 M ∥w∥ 2 Λ tr (U * ) 2 Λ 2 ,
where the third last equality is by Lemma B.2. Furthermore, injecting w = Q(s + ξ), as ξ ⊤ V * is a zero vector, we have
(III) = 1 M ∥s + ξ∥ 2 (V * ) 2 D 3 + 1 M ∥s + ξ∥ 2 D tr (V * ) 2 D 2 = 1 M ∥s∥ 2 (V * ) 2 D 3 + 1 M ∥s + ξ∥ 2 D tr (V * ) 2 D 2 .
Similarly, for the term (IV), we have
(IV) =E (s + ξ) ⊤ Q ⊤ ΛU * x q − (s + ξ) ⊤ Q ⊤ x q 2 =E s ⊤ DV * Q ⊤ x q − s ⊤ Q ⊤ x q − ξ ⊤ Q ⊤ x q 2 =s ⊤ (V * ) 2 D 3 s + s ⊤ Ds + ξ ⊤ Dξ − 2s ⊤ V * D 2 s =ξ ⊤ Dξ + i∈[r] s 2 i λ i λ 2 i (v * i ) 2 − 2λ i v * i + 1 =∥ξ∥ 2 D + i∈[r] s 2 i λ i (λ i v * i − 1) 2 ,
where the third equality is due to s ⊤ Aξ = 0 for any diagonal matrix A ∈ R d×d .Now, we analyze the label noise term.By U * and Λ being commutable, for the term (II), we have
(II) = σ 2 M 2 E   M i=1 x ⊤ i U * x q 2   = σ 2 M 2 E   tr   M i=1 x i ⊤ U * ΛU * M i=1 x i     = σ 2 M E tr x ⊤ 1 U * ΛU * x 1 = σ 2 M tr (V * ) 2 D 2 ,
where all cross terms vanish in the second equality.We conclude by combining four terms.</p>
<p>Theorem 4.3 (Behavior difference for regression, special case).Let 0 ≤ r ≤ r ′ ≤ d and w = Qs where s is r-dim truncated vector.Denote the optimal rank-r solution as f 1 and the optimal rank-r ′ solution as f 2 .Then,
L(f 2 ; E) − L(f 1 ; E) = 1 M ∥s∥ 2 D + σ 2   r ′ i=r+1 N λ i (N + 1) λ i + tr(D) 2   . Proof of Theorem 4.3. Let V * = diag([v * 1 , . . . , v * d ]) satisfying for any i ≤ r, v * i = N (N +1)λi+tr(D) and for any i &gt; r, v * i = 0. Let V ′ * = diag([v ′ * 1 , . . . , v ′ * d ]) be satisfied for any i ≤ r ′ , v ′ * i = N (N +1
)λi+tr(D) and for any i &gt; r ′ , v ′ * i = 0. Note that V * is a truncated diagonal matrix of V ′ * .By Theorem 4.1 and Theorem 4.2, we have
L(f 2 ; E) − L(f 1 ; E) =   1 M ∥s∥ 2 (V ′ * ) 2 D 3 + 1 M ∥s∥ 2 D + σ 2 tr (V ′ * ) 2 D 2 + i∈[r ′ ] s 2 i λ i λ i v ′ * i − 1 2   −   1 M ∥s∥ 2 (V * ) 2 D 3 + 1 M ∥s∥ 2 D + σ 2 tr (V * ) 2 D 2 + i∈[r] s 2 i λ i (λ i v * i − 1) 2   = 1 M ∥s∥ 2 D + σ 2 tr (V ′ * ) 2 D 2 − tr (V * ) 2 D 2 = 1 M ∥s∥ 2 D + σ 2   r ′ i=r+1 N λ i (N + 1) λ i + tr(D) 2   .(U, u) = − 1 2 tr[Λ 2 Γ −1 ],
where U = cΓ −1 , u = 1 c for any non-zero constant c are minimum solution.We also have
l(U, u) − min U∈R d×d ,u∈R l(U, u) = 1 2 Γ 1 2 uΛ 1 2 UΛ 1 2 − ΛΓ −1 2 F .(7)
Lemma B.2.Let x ∼ N (0, Λ), ϵ ∼ N (0, σ 2 ) and y = ⟨w, x⟩ + ϵ, where w ∈ R d is a fixed vector.Then we have
E y 2 xx ⊤ =σ 2 Λ + ∥w∥ 2 Λ Λ + 2Λw ⊤ wΛ, E(yx)E(yx) ⊤ =Λ ⊤ ww ⊤ Λ, E (yx − E(yx))(yx − E(yx)) ⊤ =σ 2 Λ + ∥w∥ 2 Λ Λ + Λw ⊤ wΛ.
Proof of Lemma B.2.As y is a zero mean Gaussian, by Isserlis' theorem (Wick, 1950;Michalowicz et al., 2009), for any i, j ∈ [d] we have
E[y 2 x i x j ] =E[y 2 ]E[x i x j ] + 2E[yx i ]E[yx j ] = σ 2 + w ⊤ Λw Λ i,j + 2Λ ⊤ i ww ⊤ Λ j .
Thus, we have E y 2 xx ⊤ = σ 2 + w ⊤ Λw Λ + 2Λw ⊤ wΛ.Similarly, we also have E(yx)E(yx) ⊤ = Λ ⊤ ww ⊤ Λ.Thus, we have Here, we provide the proof of Theorem 5.1.Theorem 5.1 (Optimal solution for parity).Consider k = 2 ν1 , d = 2 ν2 , and let g * 1 and g * 2 denote the optimal solutions for m = 2(ν 1 + 1) and m = 2(ν 2 + 1), respectively.
E (yx − E(yx))(yx − E(yx)) ⊤ =E y 2 xx ⊤ − yxE(yx) ⊤ − E(yx)yx ⊤ + E(yx)E(yx) ⊤ =E y 2 xx ⊤ − E(yx)E(yx) ⊤ = σ 2 + w ⊤ Λw Λ + Λw ⊤ wΛ.When 0 &lt; p T &lt; 1 4 −γ d(d−1)2
( 1 4 +γ)+ 1 4 −γ , g * 1 neurons are a subset of g * 2 neurons.Specifically, for any i ∈ [2(ν 2 + 1)], let V * ,(i) be diagonal matrix and
• For any i ∈ [ν 2 ] and i τ ∈ [d], let a * i = −1 and V * ,(i)
iτ ,iτ = (2 digit(bin(i τ − 1), i) − 1)/(4γ).</p>
<p>• For i = ν 2 + 1 and any i τ ∈ [d], let a * i = +1 and V * ,(i)</p>
<p>iτ ,iτ = −ν j /(4γ) for g * j .</p>
<p>•
For i ∈ [2(ν 2 + 1)] \ [ν 2 + 1], let a * i = a * i−ν2−1 and V * ,(i) = −V * ,(i−ν2−1) .
Let W * ,(i) = GV * ,(i) G ⊤ .Up to permutations, g * 2 has neurons (a * , W * ,(1) , . . ., W * ,(m) ) and g * 1 has the {1, . . ., ν 1 , ν 2 + 1, ν 2 + 2 . . ., ν 2 + ν 1 + 1, 2ν 2 + 2}-th neurons of g * 2 .</p>
<p>Proof of Theorem 5.1.Recall t τ = (i τ , j τ ).Let z τ ∈ R d satisfy z τ,iτ = z τ,jτ = 2γ and all other entries are zero.Denote
V (i) = G ⊤ W (i) G. Notice that ∥W (i) ∥ 2 F = ∥V (i) ∥ 2 F . Thus, we denote V * ,(i) = G ⊤ W * ,(i) G. Then, we have E τ [ℓ (y τ,q • g(X τ , y τ , x τ,q ))] =E τ   ℓ   y τ,q   i∈[m] a i σ y ⊤ τ X τ N W (i) x τ,q       =E τ   ℓ   y τ,q   i∈[m] a i σ z ⊤ τ V (i) ϕ τ,q       =E τ   ℓ   y τ,q   i∈[m] a i σ 2γ(V (i) iτ ,: + V (i) jτ ,: )ϕ τ,q       .
We can see that for any
i ∈ [m], |a * i | = 1 and V * ,(i) j,l
= 0 when j ̸ = l.As ReLU is a homogeneous function, we have
E τ [ℓ (y τ,q • g * (X τ , y τ , x τ,q ))] = (1 − p T )E   ℓ   2γϕ τ,q,iτ ϕ τ,q,jτ   i∈[m] a * i σ V * ,(i) iτ ,iτ ϕ τ,q,iτ + V * ,(i) jτ ,jτ ϕ τ,q,jτ     t τ ∈ S 1   (I) + p T E   ℓ   2γϕ τ,q,iτ ϕ τ,q,jτ   i∈[m]
a * i σ V * ,(i) iτ ,iτ ϕ τ,q,iτ + V * ,(i)
jτ ,jτ ϕ τ,q,jτ     t τ ∈ S 2   (II)
.</p>
<p>We have We can get a similar equation for (II).</p>
<p>We make some definitions to be used.We define a pattern as (z 1 , {(i τ , z 2 ), (j τ , z 3 )}), where z 1 , z 2 , z 3 ∈ {±1}.We define a pattern is covered by a neuron means there exists i ∈ [m], such that a * i = z 1 and sign(V * ,(i) iτ ,iτ ) = z 2 and sign(V * ,(i) jτ ,jτ ) = z 3 .We define a neuron as being positive when its a * i = +1 and being negative when its a * i = −1.We define a pattern as being positive if z 1 = +1 and being negative if z 1 = −1.</p>
<p>Then all terms in (I) and (II) can be written as: probability at least 1 − δ over the randomness of test data, we have g * j (X τ , y τ , x τ,q ) = h(θ j , 2γ φτ,q + P Dj (Ξ)) + ϵ j
:= i∈[m]
a * i σ diag V * ,(i) ⊤ 2γ φτ,q + P Dj (Ξ) +ϵ j where ϵ j = O νj M log 1 δ and we have</p>
<p>• 2γ φτ,q is the signal useful for prediction: 0 = ℓ(y q • h(θ 1 , 2γ φτ,q )) = ℓ(y q • h(θ 2 , 2γ φτ,q )).</p>
<p>• P D1 (Ξ)) and P D2 (Ξ)) is noise not related to labels, and
E[∥P D 1 (Ξ))∥ 2 2 ] E[∥P D 2 (Ξ))∥ 2 2 ] = ν1+1 ν2+1 .
Proof of Theorem 5.2.Let Φ τ = [ϕ τ,1 , . . ., ϕ τ,M ] ⊤ ∈ R M ×d .Recall t τ = (i τ , j τ ).Let z τ ∈ R d satisfy z τ,iτ = z τ,jτ = 2γ and all other entries are zero.We see t τ as an index set and let r τ = [d] \ t τ .Then, we have
g * 2 (X τ , y τ , x τ,q ) = i∈[m] a * i σ y ⊤ τ X τ M W * ,(i) x τ,q = i∈[m] a * i σ y ⊤ τ Φ τ M V * ,(i) ϕ τ,q = i∈[m] a * i σ y ⊤ τ Φ τ :,tτ M V * ,(i)
tτ ,: ϕ τ,q,tτ + y ⊤ τ Φ τ :,rτ M V * ,(i) rτ ,: ϕ τ,q,rτ .</p>
<p>Note that we can absorb the randomness of y τ , Φ τ :,rτ , ϕ τ,q,rτ together.Let z i for i ∈ [n] uniformly draw from {−1, +1}.By Chernoff bound for binomial distribution (Lemma C.1), for any 0 &lt; ϵ &lt; 1, we have
Pr i∈[n] z i n ≥ ϵ ≤ 2 exp − ϵ 2 n 6 .
Thus, for any 0 &lt; δ &lt; 1, with probability at least 1 − δ over the randomness of evaluation data, such that
Ξ ⊤ tτ diag(V * ,(i) tτ ,tτ ) ≤ O 1 M log 1 δ .
Then, for any 0 &lt; δ &lt; 1, with probability at least 1 − δ over the randomness of evaluation data, we have
g * 2 (X τ , y τ , x τ,q ) = i∈[m] a * i σ y ⊤ τ Φ τ :,tτ M V * ,(i)
tτ ,: ϕ τ,q,tτ + Ξ ⊤ diag(V * ,(i) ) − Ξ ⊤ tτ diag(V * ,(i)
tτ ,tτ ) = i∈[m]
a * i σ z ⊤ τ V * ,(i) tτ ,: ϕ τ,q,tτ + Ξ ⊤ diag(V * ,(i) ) − Ξ ⊤ tτ diag(V * ,(i)
tτ ,tτ ) = i∈[m]
a * i σ 2γ diag V * ,(i) tτ ,tτ ⊤ ϕ τ,q,tτ + Ξ ⊤ diag(V * ,(i) ) − Ξ ⊤ tτ diag(V * ,(i)
tτ ,tτ ) = i∈[m]
a * i σ diag V * ,(i) ⊤ 2γ φτ,q + Ξ − Ξ ⊤ tτ diag(V * ,(i)
tτ ,tτ ) = i∈[m] a * i σ diag V * ,(i) ⊤ 2γ φτ,q + Ξ + O 1 M log 1 δ = i∈[m]
a * i σ diag V * ,(i) ⊤ 2γ φτ,q + P D2 (Ξ) + O 1 M log 1 δ = h(θ 2 , 2γ φτ,q + P D2 (Ξ)) + O ν 2 M log 1 δ .</p>
<p>Similarly, we have g * 1 (X τ , y τ , x τ,q ) = h(θ 1 , 2γ φτ,q + P D1 (Ξ)) + O ν1 M log 1 δ .</p>
<p>As t τ ∈ S 1 and the number of (ϕ iτ , ϕ jτ ) being balanced as training, by careful checking, we can see that ℓ(y q • h(θ 1 , 2γ φτ,q )) = ℓ(y q • h(θ 2 , 2γ φτ,q )) = 0 and we have 2γ φτ,q is the signal part.</p>
<p>On the other hand, we know that all the first half columns in D 2 are orthogonal with each other, and the second half columns in D 2 are opposite to the first half columns.We have the same fact to D 1 .As Ξ is a symmetric noise distribution, we have</p>
<p>Theorem 4.1 (Optimal rank-r solution for regression).Recall the loss function l in Lemma 4.1.Let U * , u * = argmin U∈R d×d ,rank(U)≤r,u∈R l(U, u).</p>
<p>Theorem 4.2 (Behavior difference for regression).Let w = Q(s + ξ) ∈ R d where s, ξ ∈ R d are truncated and residual vectors defined above.The optimal rank-r solution f LSA,θ in Theorem 4.1 satisfies:</p>
<p>Figure 1 .
1
Figure 1.Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (chat/with instruct turning).Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.0.0 20.0 40.0 60.0 80.0 100.0 Accuracy(%)</p>
<p>Figure 2
2
Figure2.Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (original/without instruct turning).Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.</p>
<p>Figure 3 .
3
Figure3.The magnitude of attention between the labels and input sentences in Llama 2-13b and 70b on 100 evaluation prompts; see the main text for the details.x-axis: indices of the prompts.y-axis: the norm of the last row of attention maps in the final layer.Correct: original label; wrong: flipped label; relevant: original input sentence; irrelevant: irrelevant sentence from other datasets.The results show that larger models focus on both sentences, while smaller models only focus on relevant sentences.</p>
<p>the structure of the quadratic form of our MSE loss.Lemma B.1 (Corollary A.2 in Zhang et al. (2023b)).The loss function l in Lemma 4.1 satisfies min U∈R d×d ,u∈R l</p>
<p>C. Deferred Proof for Parity Classification C.1.Proof of Theorem 5.1</p>
<p>(</p>
<p>a * i σ z 2 V * ,(i) iτ ,iτ + z 3 V * ,(i)</p>
<p>D 2 (Ξ))∥ 2 2 ] = ν1+1 ν2+1 and we have P D1 (Ξ)) and P D2 (Ξ)) is the noise part.C.3.Auxiliary Lemma Lemma C.1 (Chernoff bound for binomial distribution).Let Z ∼ Bin(n, p) and let µ = E[Z].For any 0 &lt; ϵ &lt; 1, we have Pr(|Z − µ| ≥ ϵµ) ≤ 2 exp − ϵ 2 µ 3 .</p>
<p>AcknowledgementsThe work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants 2008559-IIS, 2023239-DMS, and CCF-2046710.Impact StatementOur work aims to improve the understanding of the incontext learning mechanism and to inspire efficient and safe use of ICL.Our paper is purely theoretical and empirical in nature and thus we foresee no immediate negative ethical impact.We hope our work will inspire effective algorithm design and promote a better understanding of large language model learning mechanisms.AppendixA. LimitationsWe study and understand an interesting phenomenon of in-context learning: smaller models are more robust to noise, while larger ones are more easily distracted, leading to different ICL behaviors.Although we study two stylized settings and give the closed-form solution, our analysis cannot extend to real Transformers easily due to the high non-convex function and complicated design of multiple-layer Transformers.Also, our work does not study optimization trajectory, which we leave as future work.On the other hand, we use simple binary classification real-world datasets to verify our analysis, which still has a gap for the practical user using the LLM scenario.ThenB. Deferred Proof for Linear Regression, where c is any nonzero constant, and Thus, we may consider Equation (7) in Lemma B.1 only.On the other hand, we haveWe denoteSince Γ and Λ are commutable and the Frobenius norm (F -norm) of a matrix does not change after multiplying it by an orthonormal matrix, we have Equation (7) as where α is the scalar term.Note that there are total k(k−1) 2 × 4 patterns in (I) andThe loss depends on the weighted sum of non-covered patterns.To have zero loss, we need all patterns to be covered by m neurons, i.e., (a * , V * ,(1) , . . ., V * ,(m) ).Note that one neuron at most coverwhich means the model will only cover all patterns in (I) before covering a pattern in (II) in purpose.Now, we show that the minimum number of neurons to cover all patterns in (I) and (II) is 2(ν 2 + 1).First, we show that 2(ν 2 + 1) neurons are enough to cover all patterns in (I) and (II).iτ ,iτ = (2 digit(bin(i τ − 1), i) − 1)/(4γ) and all non-diagonal entries in V (i) being zero andiτ ,iτ = −ν 2 /(4γ) and all non-diagonal entries in V (i) being zero and a i = +1.We can check that this construction can cover all patterns in (I) and (II) and only needs 2(ν 2 + 1) neurons.V (ν2+1) and V (2(ν2+1)) cover all positive patterns.All other neurons cover all negative patterns.This is because bin(i τ ) and bin(j τ ) have at least one digit difference.If bin(i τ ) and bin(j τ ) are different in the i-th digit, then (−1, {(i τ , −1), (j τ , +1)}) and (−1, {(i τ , +1), (j τ , −1)}) are covered by the i-th and i + ν 2 + 1-th neuron.We can also check that the scalar 1 4γ and ν2 4γ is the optimal value.Note that(1) For any negative patterns, the positive neurons will not have a cancellation effect on the negative neurons, i.e., when y q = −1, the positive neurons will never activate.(2) For each negative neuron, there exist some patterns that are uniquely covered by it.(3) For any positive patterns, there are at most ν 2 − 1 negative neurons that will have a cancellation effect on the positive neurons, i.e., when y q = +1, these negative neurons will activate simultaneously.Also, we can check that there is a positive pattern such that there are ν 2 − 1 negative neurons that will have a cancellation effect.(4) For two positive neurons, there exist some patterns that are uniquely covered by one of them.Due to hinge loss, we can see that 1 4γ is tight for negative neurons as (1) and (2).Similarly, we can also see that ν2 4γ is tight for positive neurons as (3) and (4).Second, we prove that we need at least 2(ν 2 + 1) neurons to cover all patterns in (I) and (II).We can see that we need at least 2 positive neurons to cover all positive patterns.Then, we only need to show that 2ν 2 − 1 neurons are not enough to cover all negative patterns.We can prove that all negative patterns are covered equivalent to all numbers from {0, 1, . . .,Therefore, the minimum number of neurons to cover all patterns in (I) and (II) is 2(ν 2 + 1).Thus, when m = 2(ν 1 + 1), the optimal solution will cover all patterns in (I) but not all in (II).When m ≥ 2(ν 2 + 1), the optimal solution will cover all patterns in (I) and (II).We see that g * 1 neurons as the subset of g * 2 neurons, while the only difference is that the scalar of positive neurons is ν1 4γ for g * 1 and ν2 4γ for g * 2 .Thus, we finished the proof.C.2. Proof of Theorem 5.2Here, we provide the proof of Theorem 5.2.Theorem 5.2 (Behavior difference for parity).Assume the same condition as Theorem 5.1.For j ∈ {1, 2}, Let θ j denote the parameters of g * j .For l ∈ [M ], let ξ l be uniformly drawn from {±1} d , and Ξ = l∈[M ] ξ lM. Then, for any δ ∈ (0, 1), with
In-context examples selection for machine translation. S Agrawal, C Zhou, M Lewis, L Zettlemoyer, M Ghazvininejad, arXiv:2212.024372022arXiv preprint</p>
<p>Transformers learn to implement preconditioned gradient descent for in-context learning. K Ahn, X Cheng, H Daneshmand, S Sra, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Linear attention is (maybe) all you need (to understand transformer optimization). K Ahn, X Cheng, M Song, C Yun, A Jadbabaie, S Sra, The Twelfth International Conference on Learning Representations. 2024</p>
<p>What learning algorithm is in-context learning? investigations with linear models. E Akyurek, D Schuurmans, J Andreas, T Ma, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Physics of language models: Part 1, context-free grammar. Z Allen-Zhu, Y Li, arXiv:2305.136732023arXiv preprint</p>
<p>How do in-context examples affect compositional generalization?. S An, Z Lin, Q Fu, B Chen, N Zheng, J.-G Lou, D Zhang, arXiv:2305.048352023arXiv preprint</p>
<p>A theory for emergence of complex skills in language models. S Arora, A Goyal, arXiv:2307.159362023arXiv preprint</p>
<p>Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Y Bai, F Chen, H Wang, C Xiong, S Mei, B Barak, B Edelman, S Goel, S Kakade, E Malach, C Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023. 202235Advances in Neural Information Processing Systems</p>
<p>Low-rank bottleneck in multi-head attention models. S Bhojanapalli, C Yun, A S Rawat, S Reddi, S Kumar, International conference on machine learning. PMLR2020</p>
<p>Birth of a transformer: A memory viewpoint. A Bietti, V Cabannes, D Bouchacourt, H Jegou, L Bottou, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 32003</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 2020</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Scatterbrain: Unifying sparse and low-rank attention. B Chen, T Dao, E Winsor, Z Song, A Rudra, C Ré, Advances in Neural Information Processing Systems. 2021</p>
<p>Transformers implement functional gradient descent to learn non-linear functions in context. X Cheng, Y Chen, S Sra, arXiv:2312.065282023arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>SentEval: An evaluation toolkit for universal sentence representations. A Conneau, D Kiela, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)European Language Resources Association (ELRA)2018</p>
<p>Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. D Dai, Y Sun, L Dong, Y Hao, Z Sui, F Wei, arXiv:2212.105592022arXiv preprint</p>
<p>Learning parities with neural networks. A Daniely, E Malach, Advances in Neural Information Processing Systems. 202033</p>
<p>Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention. J Dass, S Wu, H Shi, C Li, Z Ye, Z Wang, Y Lin, 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2023</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2019</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Lighter and better: low-rank decomposed selfattention networks for next-item recommendation. X Fan, Z Liu, J Lian, W X Zhao, X Xie, J.-R Wen, Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>Improving language model negotiation with self-play and incontext learning from ai feedback. Y Fu, H Peng, T Khot, M Lapata, arXiv:2305.101422023arXiv preprint</p>
<p>P Gao, J Han, R Zhang, Z Lin, S Geng, A Zhou, W Zhang, P Lu, C He, X Yue, arXiv:2304.15010Parameter-efficient visual instruction model. 20232arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021a</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021b</p>
<p>What can transformers learn in-context? a case study of simple function classes. S Garg, D Tsipras, P S Liang, G Valiant, Advances in Neural Information Processing Systems. 2022</p>
<p>Transformer feed-forward layers are key-value memories. M Geva, R Schuster, J Berant, O Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Exploring the frontiers of softmax: Provable optimization, applications in diffusion model. J Gu, C Li, Y Liang, Z Shi, Z Song, arXiv:2405.032512024aarXiv preprint</p>
<p>J Gu, C Li, Y Liang, Z Shi, Z Song, T Zhou, arXiv:2402.09469Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. 2024barXiv preprint</p>
<p>Convbasis: A new paradigm for efficient attention inference and gradient computation in transformers. J Gu, Y Liang, H Liu, Z Shi, Z Song, J Yin, arXiv:2405.052192024carXiv preprint</p>
<p>Tensor attention training: Provably efficient learning of higherorder transformers. J Gu, Y Liang, Z Shi, Z Song, Y Zhou, arXiv:2405.164112024darXiv preprint</p>
<p>Unraveling the smoothness properties of diffusion models: A gaussian mixture perspective. J Gu, Y Liang, Z Shi, Z Song, Y Zhou, arXiv:2405.164182024earXiv preprint</p>
<p>How do transformers learn in-context beyond simple functions? a case study on learning with representations. T Guo, W Hu, S Mei, H Wang, C Xiong, S Savarese, Y Bai, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Low-rank adaptation of large language models. E J Hu, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, International Conference on Learning Representations. 2022</p>
<p>Y Huang, Y Cheng, Y Liang, arXiv:2310.05249-context convergence of transformers. 2023arXiv preprint</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.120172022arXiv preprint</p>
<p>Vision transformers provably learn spatial structure. S Jelassi, M Sander, Y Li, Advances in Neural Information Processing Systems. 2022</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledgeintensive nlp. O Khattab, K Santhanam, X L Li, D Hall, P Liang, C Potts, M Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. H Li, M Wang, S Liu, P.-Y Chen, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Transformers as multi-task feature selectors: Generalization analysis of in-context learning. H Li, M Wang, S Lu, H Wan, X Cui, P.-Y Chen, NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning. 2023b</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Transformers as algorithms: Generalization and stability in in-context learning. Y Li, M E Ildiz, D Papailiopoulos, S Oymak, Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. the 40th International Conference on Machine Learning, Machine Learning ResearchPMLR2023c</p>
<p>How do transformers learn topic structure: Towards a mechanistic understanding. Y Li, Y Li, A Risteski, Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. the 40th International Conference on Machine Learning, Machine Learning ResearchPMLR2023d</p>
<p>Dissecting chain-of-thought: Compositionality through in-context filtering and learning. Y Li, K Sreenivasan, A Giannou, D Papailiopoulos, S Oymak, Thirty-seventh Conference on Neural Information Processing Systems. 2023e</p>
<p>One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. Z Luo, S Wu, C Weng, M Zhou, R Ge, A Mahankali, T B Hashimoto, T Ma, arXiv:2307.03576The Eleventh International Conference on Learning Representations. 2023. 2023arXiv preprintUnderstanding the robustness of self-supervised learning through topic modeling</p>
<p>Fine-tuning language models with just forward passes. S Malladi, T Gao, E Nichani, A Damian, J D Lee, D Chen, S Arora, Advances in Neural Information Processing Systems. 2023</p>
<p>An isserlis' theorem for mixed gaussian variables: Application to the auto-bispectral density. J Michalowicz, J Nichols, F Bucholtz, C Olson, Journal of Statistical Physics. 2009</p>
<p>S Min, M Lewis, L Zettlemoyer, H Hajishirzi, Metaicl, arXiv:2110.15943Learning to learn in context. 2021arXiv preprint</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Crosstask generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research. B Olshausen, D Field, 199737</p>
<p>. Introducing Openai, Chatgpt, 2022</p>
<p>arxiv:2303.08774OpenAI. GPT-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 2022</p>
<p>What in-context learning 'learns' in-context: Disentangling task recognition and task learning. J Pan, T Gao, H Chen, D Chen, Findings of Association for Computational Linguistics2023</p>
<p>A Panigrahi, S Malladi, M Xia, S Arora, arXiv:2307.01189Trainable transformer in transformer. 2023arXiv preprint</p>
<p>Din-sql: Decomposed incontext learning of text-to-sql with self-correction. M Pourreza, D Rafiei, arXiv:2304.110152023arXiv preprint</p>
<p>Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. A Raventos, M Paul, F Chen, S Ganguli, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>The mechanistic basis of data dependence and abrupt learning in an in-context classification task. G Reddy, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Linear transformers are secretly fast weight programmers. I Schlag, K Irie, J Schmidhuber, International Conference on Machine Learning. PMLR2021</p>
<p>Large language models can be easily distracted by irrelevant context. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, N Schärli, D Zhou, International Conference on Machine Learning. PMLR2023a</p>
<p>A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. Z Shi, J Wei, Y Liang, International Conference on Learning Representations. 2022</p>
<p>The trade-off between universality and label efficiency of representations from contrastive learning. Z Shi, J Chen, K Li, J Raghuram, X Wu, Y Liang, S Jha, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Domain generalization via nuclear norm regularization. Z Shi, Y Ming, Y Fan, F Sala, Y Liang, Conference on Parsimony and Learning (Proceedings Track). 2023c</p>
<p>Provable guarantees for neural networks via gradient feature learning. Z Shi, J Wei, Y Liang, Thirty-seventh Conference on Neural Information Processing Systems. 2023d</p>
<p>Blackbox tuning for language-model-as-a-service. T Sun, Y Shao, H Qian, X Huang, X Qiu, International Conference on Machine Learning. PMLR2022</p>
<p>Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Y Tian, Y Wang, B Chen, S Du, Advances in Neural Information Processing Systems. 2023a</p>
<p>Demystifying multilayer transformers via joint dynamics of mlp and attention. Y Tian, Y Wang, Z Zhang, B Chen, S Du, Joma, 2023b</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Sparse coding and decorrelation in primary visual cortex during natural vision. W E Vinje, J L Gallant, Science. 28754562000</p>
<p>Transformers learn in-context by gradient descent. Von Oswald, J Niklasson, E Randazzo, E Sacramento, J Mordvintsev, A Zhmoginov, A Vladymyrov, M , International Conference on Machine Learning. PMLR2023</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPAssociation for Computational Linguistics2018</p>
<p>Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2022a</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Transactions on Machine Learning Research. 2022b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022c</p>
<p>Symbol tuning improves in-context learning in language models. J Wei, L Hou, A K Lampinen, X Chen, D Huang, Y Tay, X Chen, Y Lu, D Zhou, T Ma, Q V Le, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023a</p>
<p>Larger language models do in-context learning differently. J Wei, J Wei, Y Tay, D Tran, A Webson, Y Lu, X Chen, H Liu, D Huang, D Zhou, arXiv:2303.038462023barXiv preprint</p>
<p>On the role of unstructured training data in transformers' in-context learning capabilities. K C Wibisono, Y Wang, NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning. 2023</p>
<p>The evaluation of the collision matrix. G.-C Wick, Physical review. 1950</p>
<p>How many pretraining tasks are needed for in-context learning of linear regression?. J Wu, D Zou, Z Chen, V Braverman, Q Gu, P L Bartlett, The Twelfth International Conference on Learning Representations. 2024</p>
<p>An explanation of in-context learning as implicit bayesian inference. S M Xie, A Raghunathan, P Liang, T Ma, International Conference on Learning Representations. 2022</p>
<p>Improving foundation models for few-shot learning via multitask finetuning. Z Xu, Z Shi, J Wei, Y Li, Y Liang, ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Z Xu, Z Shi, Y Liang, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024a</p>
<p>Towards few-shot adaptation of foundation models via multitask finetuning. Z Xu, Z Shi, J Wei, F Mu, Y Li, Y Liang, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K R Narasimhan, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>H Zhang, Y.-F Zhang, Y Yu, D Madeka, D Foster, E Xing, H Lakkaraju, S Kakade, arXiv:2312.04021A study on the calibration of in-context learning. 2023aarXiv preprint</p>
<p>Trained transformers learn linear models in-context. R Zhang, S Frei, P L Bartlett, arXiv:2306.099272023barXiv preprint</p>
<p>Llama-adapter: Efficient finetuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv:2303.161992023carXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, International Conference on Machine Learning. PMLR2021</p>
<p>Step-back prompting enables reasoning via abstraction in large language models. H S Zheng, S Mishra, X Chen, H.-T Cheng, E H Chi, Q V Le, D Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>LIMA: Less is more for alignment. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, S Zhang, G Ghosh, M Lewis, L Zettlemoyer, O Levy, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>Teaching algorithmic reasoning via in-context learning. H Zhou, A Nova, H Larochelle, A Courville, B Neyshabur, H Sedghi, arXiv:2211.090662022arXiv preprint</p>
<p>What algorithms can transformers learn? a study in length generalization. H Zhou, A Bradley, E Littwin, N Razin, O Saremi, J Susskind, S Bengio, P Nakkiran, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-272828154</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14908v2.pdf" target="_blank">KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems</a></p>
                <p><strong>Paper Abstract:</strong> Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates longterm and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. Karma distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dualmemory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by $1.3 \times$ and $2.3 \times$ in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by $3.4 \times$ and $62.7 \times$. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms. Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. Our code is available at https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KARMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play memory system that augments an LLM-based planner for embodied agents with a dual long-term (3D scene graph) and short-term (vector-embedded recent object states) memory, plus explicit replacement policies (FIFO, merged-FIFO, W-TinyLFU) to improve planning accuracy and efficiency in long-horizon household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KARMA-augmented LLM planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied agent that uses a pretrained LLM (used as the high-level planner) which receives a prompt composed of: serialized long-term memory (a 3D scene graph describing areas, adjacency, objects and attributes), retrieved short-term memory units (text/image/state strings), the task instruction, example decompositions, and parameterized basic-skill APIs. The LLM generates action code (calls to skill APIs) for navigation and manipulation; perception feeds update short-term memory via a VLM and multimodal embedding model. The system runs open-loop planning: perception → memory write (short-term, occasional long-term) → embed/retrieve relevant memories → LLM prompt → generated action code → execution.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>OpenAI GPT-4o (used as planner; paper's Table 4 lists GPT-4o as the LLM-as-planner)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual memory: long-term semantic/topological memory (3D scene graph) + short-term episodic/working memory (vector-embedded recent object states and images)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Long-term memory: a non-volatile hierarchical 3D scene graph (3DSG) encoding floors, areas, object nodes with 3D positions and attributes; serialized into plain-text prompt and included wholesale in LLM context. Short-term memory: small volatile units containing objectId, world coordinates, object state (inferred by VLM), and raw image path; each unit embedded by a multimodal embedding model (OpenAI text-embedding-3-large used for recall) and stored in a vector index. At query time the instruction is embedded and top-K nearest short-term memories (cosine similarity) are retrieved and their textual content appended to the prompt. Replacement policies (FIFO with ID-merging, and W-TinyLFU with window/main segments implemented using counting Bloom filters) manage short-term memory capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Short-term memory sizes evaluated: examples include 5, 10, 25 units (experiments report results for memory sizes of 5, 10, 25). Long-term memory is non-volatile and grows with environment (no fixed limit reported).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic similarity via embedding: input instruction embedded with pre-trained embedding model (OpenAI text-embedding-3-large); retrieve top-K short-term memory units by cosine similarity. Long-term 3DSG serialized and directly included in prompt (no embedding retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Short-term memory: refreshed every agent start and updated frequently after perception/action: new memory units (objectId, state, coords, image) are embedded and inserted; if capacity exceeded, replacement policy (merged-FIFO or W-TinyLFU) is applied. Long-term memory: built incrementally while exploring and updated infrequently when environment changes (add/delete nodes via topology).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFRED-L (derived from ALFRED tasks executed in the AI2-THOR simulator); additional replacement-policy evaluation dataset ALFWorld-R (long-sequence tasks sampled from ALFRED/ALFWorld style tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Long-horizon indoor household instruction-following tasks with multi-step dependencies; three categories: Simple (short, unrelated subtasks, <5 steps), Composite (highly related multi-object subtasks requiring reuse of past memories), Complex (multiple loosely related tasks, ambiguous object descriptions, longer sequences). Ground-truth symbolic subgoal conditions (heated, cooked, sliced, cleaned, inside(fridge), etc.) used for success determination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported improvements on ALFRED-L vs state-of-the-art baselines: Composite Tasks: 43% absolute task success rate improvement and 68.7% reduction in time (reported as relative improvement of 1.3x SR and 3.4x efficiency vs CAPEAM). Complex Tasks: 21% absolute task success rate improvement and 69% reduction in time (reported as relative improvement of 2.3x SR and 62.7x efficiency vs HELPER). Simple Tasks: 42% SR improvement and 61.2% reduction in time (relative 1.1x SR and 2.3x efficiency vs HELPER). Memory retrieval accuracy (MRA) and memory hit rate (MHR) metrics reported: e.g., recall accuracy for composite tasks is 2.2x higher than for complex tasks. (Exact baseline absolute SR numbers are in Table 1 of the paper; the percentages and relative multipliers above are reported in the text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation: removing short-term memory caused success-rate drops of 1.9x (complex tasks) and 4.2x (composite tasks) compared to full KARMA; removing long-term memory reduced execution efficiency (RT) by 2.7x and caused a smaller SR drop (~1.2x). When compared to baselines without KARMA, KARMA achieved the improvements listed in performance_with_memory (baselines include LoTa-Bench (modified), HELPER, CAPEAM).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Short-term memory is crucial for success rate: removal caused large SR drops (1.9x on complex, 4.2x on composite). Long-term memory primarily improves efficiency: removing it degraded reduced-time (RT) by ~2.7x while SR dropped modestly (~1.2x).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper evaluates replacement policies (FIFO, merged-FIFO, W-TinyLFU) and shows W-TinyLFU (with large window segment, e.g., window=9, main=1 for a total capacity of 10) attains highest memory hit rate after warm-up; larger memory sizes (25 vs 5) yield substantially higher hit rates (FIFO: 4.6x higher; W-TinyLFU: 3.9x higher). No direct comparison with fundamentally different memory paradigms (e.g., transformer internal memory vs external vector DB) beyond these cache replacement schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Dual memory (persistent 3D scene graph long-term + compact, updated short-term episodic units) improves both success rate and efficiency: short-term memory substantially increases success on tasks requiring reuse of recent object states, while long-term 3DSG substantially reduces redundant exploration and time. Memory hit rate correlates linearly with reduced exploration, and replacement policy (W-TinyLFU with large window) improves hit rate over FIFO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoTa-Bench (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoTa-Bench (modified) - Benchmarking language-oriented task planners for embodied agents (modified variant used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompt-based LLM task planner that provides prefix and in-context examples to an LLM which then computes probabilities of executable skills and selects actions; used as a baseline in KARMA experiments (LoTa-Bench modified).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LoTa-Bench (modified) planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prompt-based LLM planner that uses a prefix and in-context examples; LLM calculates probabilities over pre-defined skill APIs and picks the most probable skill to execute (used as baseline). The paper used an optimized version referencing SMART-LLM skill configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFRED-L (used as baseline evaluation set)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same ALFRED-L task categories: Simple, Composite, Complex long-horizon household tasks in AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HELPER (Sarch et al., 2023b) - baseline embodied agent referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced state-of-the-art embodied agent baseline used for comparison in KARMA experiments; reported as best-performing baseline in some task categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HELPER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not described in detail in this paper; used as a comparative baseline for success rate and efficiency on ALFRED-L tasks. Paper states KARMA outperforms HELPER on complex and simple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFRED-L (evaluated in AI2-THOR simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Long-sequence indoor household tasks with subgoal predicates (heated, sliced, cleaned, inside(fridge), etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in-paper comparisons: KARMA achieves a relative improvement of 2.3x SR and 62.7x task execution efficiency vs HELPER on complex tasks; 1.1x SR and 2.3x efficiency on simple tasks (numerical baseline values not reproduced verbatim in the paper text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2823.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2823.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAPEAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAPEAM (Kim et al., 2023) - referenced baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced baseline embodied agent used for comparison in KARMA experiments; cited as best-performing baseline in the composite-task setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CAPEAM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not described in detail within this paper; referenced as a baseline whose performance KARMA improves upon for composite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFRED-L (AI2-THOR simulator tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Composite tasks requiring reuse of previously observed object states and locations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>KARMA reported to achieve 43% SR improvement and 68.7% time reduction relative to CAPEAM on composite tasks (relative improvement reported as 1.3x SR and 3.4x efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2823.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2823.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>W-TinyLFU (replacement policy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>W-TinyLFU cache admission and replacement policy (approximate LFU with LRU segments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cache replacement policy used by KARMA for short-term memory management, combining a window segment and a protected main segment with frequency estimation (counting Bloom filters) and periodic counter aging to approximate LFU behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KARMA (replacement-policy component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Short-term memory admission uses W-TinyLFU: incoming memory entry enters the window segment; when full, comparisons between window and elimination segments determine evictions based on approximate frequency counts maintained by counting Bloom filters; periodic halving keeps counts fresh.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>cache admission / replacement policy for short-term episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Two-segment main memory (protection and elimination) + window segment; frequency statistics via counting Bloom filters; approximate LFU achieved by comparing frequencies across segments upon admission/eviction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Experiments illustrate total short-term memory budgets like 10 units (configurations: e.g., [9 window,1 main], [7,3], [4,6], [1,9]) and other sizes up to 25 units in separate tests.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Used in combination with vector-similarity retrieval; W-TinyLFU controls which units remain resident for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Admission on write; window/main segment logic; global counter increments on each addition; when counter reaches threshold W, all counters halved to age frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFWorld-R and ALFRED-L used for replacement policy evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Long-sequence tasks where re-use of recent memories matters; replacement policy evaluated by memory hit rate over sequences of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>W-TinyLFU with larger window (e.g., window=9, main=1 for capacity=10) attained the highest memory hit rate after warm-up; higher hit rate linearly correlated with higher reduced exploration and greater task efficiency. Specific reported comparisons: after warm-up, W-TinyLFU [9,1] outperformed FIFO [10].</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>W-TinyLFU (with larger window segment) produces higher hit rates than FIFO; larger memory sizes (25 vs 5) markedly increase hit rate (FIFO: 4.6x improvement; W-TinyLFU: 3.9x improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared W-TinyLFU vs FIFO and variants (merged-FIFO); W-TinyLFU showed superior memory hit rate in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Replacement policy choice strongly affects hit rate and thus reduces exploration: W-TinyLFU with a larger window segment is more effective than FIFO for maintaining useful short-term memories in long-sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-ended instructable embodied agents with memory-augmented large language models <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>LoTa-bench: Benchmarking language-oriented task planners for embodied agents <em>(Rating: 2)</em></li>
                <li>Context-aware planning and environment-aware memory for instruction following embodied agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2823",
    "paper_id": "paper-272828154",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "KARMA",
            "name_full": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems",
            "brief_description": "A plug-and-play memory system that augments an LLM-based planner for embodied agents with a dual long-term (3D scene graph) and short-term (vector-embedded recent object states) memory, plus explicit replacement policies (FIFO, merged-FIFO, W-TinyLFU) to improve planning accuracy and efficiency in long-horizon household tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KARMA-augmented LLM planner",
            "agent_description": "An embodied agent that uses a pretrained LLM (used as the high-level planner) which receives a prompt composed of: serialized long-term memory (a 3D scene graph describing areas, adjacency, objects and attributes), retrieved short-term memory units (text/image/state strings), the task instruction, example decompositions, and parameterized basic-skill APIs. The LLM generates action code (calls to skill APIs) for navigation and manipulation; perception feeds update short-term memory via a VLM and multimodal embedding model. The system runs open-loop planning: perception → memory write (short-term, occasional long-term) → embed/retrieve relevant memories → LLM prompt → generated action code → execution.",
            "base_llm": "OpenAI GPT-4o (used as planner; paper's Table 4 lists GPT-4o as the LLM-as-planner)",
            "uses_memory": true,
            "memory_type": "dual memory: long-term semantic/topological memory (3D scene graph) + short-term episodic/working memory (vector-embedded recent object states and images)",
            "memory_architecture": "Long-term memory: a non-volatile hierarchical 3D scene graph (3DSG) encoding floors, areas, object nodes with 3D positions and attributes; serialized into plain-text prompt and included wholesale in LLM context. Short-term memory: small volatile units containing objectId, world coordinates, object state (inferred by VLM), and raw image path; each unit embedded by a multimodal embedding model (OpenAI text-embedding-3-large used for recall) and stored in a vector index. At query time the instruction is embedded and top-K nearest short-term memories (cosine similarity) are retrieved and their textual content appended to the prompt. Replacement policies (FIFO with ID-merging, and W-TinyLFU with window/main segments implemented using counting Bloom filters) manage short-term memory capacity.",
            "memory_capacity": "Short-term memory sizes evaluated: examples include 5, 10, 25 units (experiments report results for memory sizes of 5, 10, 25). Long-term memory is non-volatile and grows with environment (no fixed limit reported).",
            "memory_retrieval_method": "Semantic similarity via embedding: input instruction embedded with pre-trained embedding model (OpenAI text-embedding-3-large); retrieve top-K short-term memory units by cosine similarity. Long-term 3DSG serialized and directly included in prompt (no embedding retrieval).",
            "memory_update_strategy": "Short-term memory: refreshed every agent start and updated frequently after perception/action: new memory units (objectId, state, coords, image) are embedded and inserted; if capacity exceeded, replacement policy (merged-FIFO or W-TinyLFU) is applied. Long-term memory: built incrementally while exploring and updated infrequently when environment changes (add/delete nodes via topology).",
            "text_game_benchmark": "ALFRED-L (derived from ALFRED tasks executed in the AI2-THOR simulator); additional replacement-policy evaluation dataset ALFWorld-R (long-sequence tasks sampled from ALFRED/ALFWorld style tasks).",
            "game_characteristics": "Long-horizon indoor household instruction-following tasks with multi-step dependencies; three categories: Simple (short, unrelated subtasks, &lt;5 steps), Composite (highly related multi-object subtasks requiring reuse of past memories), Complex (multiple loosely related tasks, ambiguous object descriptions, longer sequences). Ground-truth symbolic subgoal conditions (heated, cooked, sliced, cleaned, inside(fridge), etc.) used for success determination.",
            "performance_with_memory": "Reported improvements on ALFRED-L vs state-of-the-art baselines: Composite Tasks: 43% absolute task success rate improvement and 68.7% reduction in time (reported as relative improvement of 1.3x SR and 3.4x efficiency vs CAPEAM). Complex Tasks: 21% absolute task success rate improvement and 69% reduction in time (reported as relative improvement of 2.3x SR and 62.7x efficiency vs HELPER). Simple Tasks: 42% SR improvement and 61.2% reduction in time (relative 1.1x SR and 2.3x efficiency vs HELPER). Memory retrieval accuracy (MRA) and memory hit rate (MHR) metrics reported: e.g., recall accuracy for composite tasks is 2.2x higher than for complex tasks. (Exact baseline absolute SR numbers are in Table 1 of the paper; the percentages and relative multipliers above are reported in the text.)",
            "performance_without_memory": "Ablation: removing short-term memory caused success-rate drops of 1.9x (complex tasks) and 4.2x (composite tasks) compared to full KARMA; removing long-term memory reduced execution efficiency (RT) by 2.7x and caused a smaller SR drop (~1.2x). When compared to baselines without KARMA, KARMA achieved the improvements listed in performance_with_memory (baselines include LoTa-Bench (modified), HELPER, CAPEAM).",
            "has_ablation_study": true,
            "memory_ablation_results": "Short-term memory is crucial for success rate: removal caused large SR drops (1.9x on complex, 4.2x on composite). Long-term memory primarily improves efficiency: removing it degraded reduced-time (RT) by ~2.7x while SR dropped modestly (~1.2x).",
            "comparison_with_other_memory_types": "Paper evaluates replacement policies (FIFO, merged-FIFO, W-TinyLFU) and shows W-TinyLFU (with large window segment, e.g., window=9, main=1 for a total capacity of 10) attains highest memory hit rate after warm-up; larger memory sizes (25 vs 5) yield substantially higher hit rates (FIFO: 4.6x higher; W-TinyLFU: 3.9x higher). No direct comparison with fundamentally different memory paradigms (e.g., transformer internal memory vs external vector DB) beyond these cache replacement schemes.",
            "key_findings_about_memory_effectiveness": "Dual memory (persistent 3D scene graph long-term + compact, updated short-term episodic units) improves both success rate and efficiency: short-term memory substantially increases success on tasks requiring reuse of recent object states, while long-term 3DSG substantially reduces redundant exploration and time. Memory hit rate correlates linearly with reduced exploration, and replacement policy (W-TinyLFU with large window) improves hit rate over FIFO.",
            "uuid": "e2823.0",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LoTa-Bench (modified)",
            "name_full": "LoTa-Bench (modified) - Benchmarking language-oriented task planners for embodied agents (modified variant used as baseline)",
            "brief_description": "Baseline prompt-based LLM task planner that provides prefix and in-context examples to an LLM which then computes probabilities of executable skills and selects actions; used as a baseline in KARMA experiments (LoTa-Bench modified).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LoTa-Bench (modified) planner",
            "agent_description": "Prompt-based LLM planner that uses a prefix and in-context examples; LLM calculates probabilities over pre-defined skill APIs and picks the most probable skill to execute (used as baseline). The paper used an optimized version referencing SMART-LLM skill configurations.",
            "base_llm": null,
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "ALFRED-L (used as baseline evaluation set)",
            "game_characteristics": "Same ALFRED-L task categories: Simple, Composite, Complex long-horizon household tasks in AI2-THOR.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": null,
            "uuid": "e2823.1",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HELPER",
            "name_full": "HELPER (Sarch et al., 2023b) - baseline embodied agent referenced",
            "brief_description": "Referenced state-of-the-art embodied agent baseline used for comparison in KARMA experiments; reported as best-performing baseline in some task categories.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "HELPER",
            "agent_description": "Not described in detail in this paper; used as a comparative baseline for success rate and efficiency on ALFRED-L tasks. Paper states KARMA outperforms HELPER on complex and simple tasks.",
            "base_llm": null,
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "ALFRED-L (evaluated in AI2-THOR simulator)",
            "game_characteristics": "Long-sequence indoor household tasks with subgoal predicates (heated, sliced, cleaned, inside(fridge), etc.).",
            "performance_with_memory": "Reported in-paper comparisons: KARMA achieves a relative improvement of 2.3x SR and 62.7x task execution efficiency vs HELPER on complex tasks; 1.1x SR and 2.3x efficiency on simple tasks (numerical baseline values not reproduced verbatim in the paper text excerpt).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": null,
            "uuid": "e2823.2",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CAPEAM",
            "name_full": "CAPEAM (Kim et al., 2023) - referenced baseline",
            "brief_description": "Referenced baseline embodied agent used for comparison in KARMA experiments; cited as best-performing baseline in the composite-task setting.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "CAPEAM",
            "agent_description": "Not described in detail within this paper; referenced as a baseline whose performance KARMA improves upon for composite tasks.",
            "base_llm": null,
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "ALFRED-L (AI2-THOR simulator tasks)",
            "game_characteristics": "Composite tasks requiring reuse of previously observed object states and locations.",
            "performance_with_memory": "KARMA reported to achieve 43% SR improvement and 68.7% time reduction relative to CAPEAM on composite tasks (relative improvement reported as 1.3x SR and 3.4x efficiency).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": null,
            "uuid": "e2823.3",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "W-TinyLFU (replacement policy)",
            "name_full": "W-TinyLFU cache admission and replacement policy (approximate LFU with LRU segments)",
            "brief_description": "A cache replacement policy used by KARMA for short-term memory management, combining a window segment and a protected main segment with frequency estimation (counting Bloom filters) and periodic counter aging to approximate LFU behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "KARMA (replacement-policy component)",
            "agent_description": "Short-term memory admission uses W-TinyLFU: incoming memory entry enters the window segment; when full, comparisons between window and elimination segments determine evictions based on approximate frequency counts maintained by counting Bloom filters; periodic halving keeps counts fresh.",
            "base_llm": null,
            "uses_memory": true,
            "memory_type": "cache admission / replacement policy for short-term episodic memory",
            "memory_architecture": "Two-segment main memory (protection and elimination) + window segment; frequency statistics via counting Bloom filters; approximate LFU achieved by comparing frequencies across segments upon admission/eviction.",
            "memory_capacity": "Experiments illustrate total short-term memory budgets like 10 units (configurations: e.g., [9 window,1 main], [7,3], [4,6], [1,9]) and other sizes up to 25 units in separate tests.",
            "memory_retrieval_method": "Used in combination with vector-similarity retrieval; W-TinyLFU controls which units remain resident for retrieval.",
            "memory_update_strategy": "Admission on write; window/main segment logic; global counter increments on each addition; when counter reaches threshold W, all counters halved to age frequencies.",
            "text_game_benchmark": "ALFWorld-R and ALFRED-L used for replacement policy evaluation",
            "game_characteristics": "Long-sequence tasks where re-use of recent memories matters; replacement policy evaluated by memory hit rate over sequences of tasks.",
            "performance_with_memory": "W-TinyLFU with larger window (e.g., window=9, main=1 for capacity=10) attained the highest memory hit rate after warm-up; higher hit rate linearly correlated with higher reduced exploration and greater task efficiency. Specific reported comparisons: after warm-up, W-TinyLFU [9,1] outperformed FIFO [10].",
            "performance_without_memory": null,
            "has_ablation_study": true,
            "memory_ablation_results": "W-TinyLFU (with larger window segment) produces higher hit rates than FIFO; larger memory sizes (25 vs 5) markedly increase hit rate (FIFO: 4.6x improvement; W-TinyLFU: 3.9x improvement reported).",
            "comparison_with_other_memory_types": "Compared W-TinyLFU vs FIFO and variants (merged-FIFO); W-TinyLFU showed superior memory hit rate in experiments.",
            "key_findings_about_memory_effectiveness": "Replacement policy choice strongly affects hit rate and thus reduces exploration: W-TinyLFU with a larger window segment is more effective than FIFO for maintaining useful short-term memories in long-sequence tasks.",
            "uuid": "e2823.4",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-ended instructable embodied agents with memory-augmented large language models",
            "rating": 2,
            "sanitized_title": "openended_instructable_embodied_agents_with_memoryaugmented_large_language_models"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "LoTa-bench: Benchmarking language-oriented task planners for embodied agents",
            "rating": 2,
            "sanitized_title": "lotabench_benchmarking_languageoriented_task_planners_for_embodied_agents"
        },
        {
            "paper_title": "Context-aware planning and environment-aware memory for instruction following embodied agents",
            "rating": 1,
            "sanitized_title": "contextaware_planning_and_environmentaware_memory_for_instruction_following_embodied_agents"
        }
    ],
    "cost": 0.0159,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</p>
<p>Zixuan Wang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Bo Yu2 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Junzhe Zhao 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Wenhao Sun 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Sai Hou® 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Shuai Liang' 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Xing Hu' 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yinhe Han 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yiming Gan 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Joon Sung Park 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Joseph O'brien 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Carrie Jun Cai 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Mered- Ith Ringel Morris 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Percy Liang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Chen Qian 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Xin Cong 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Cheng Yang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Weize Chen 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yusheng Su 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Juyuan Xu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Zhiyuan Liu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Krishan Rana 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Jesse Haviland 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Sourav Garg 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Jad Abou- Chakra 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Ian Reid 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Niko 2023 Suenderhauf 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Say 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Gabriel Sarch 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yue Wu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Michael J Tarr 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems
4C73EFA125FF85EDFA6D18FB8B141051unified instructable embodied agent to tackle four interactive vision-language domains with memoryaugmented language models. arXiv preprint arXiv: 2404.19065.
Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution.To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting.KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while shortterm memory dynamically records changes in objects' positions and states.This dualmemory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning.Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data.Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by 1.3x and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by 3.4x and 62.7x.Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms.Through this plug-andplay memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient.The experimental videos from the work can be found at https://youtu.be/4BT7fnw9ehs.Our code is available at https://github.com/WZX®@Swarm@Robotics/KARMA/tree/master.</p>
<p>Introduction</p>
<p>Robotic applications are evolving towards longer and more complex tasks.Using an LLM as its core planning module can effectively decompose long and complex tasks into multiple short and fixed movements (Choi et al., 2024;Sarch et al., 2024;Chen et al., 2023b;Vemprala et al., 2023;Rana et al., 2023;Brohan et al., 2022Brohan et al., , 2023;;Belkhale et al., 2024), increasing the success rate.</p>
<p>Yet, simply equipping an embodied agent or a robot with an LLM is not enough.Take indoor household tasks as an example, they usually require a sequence of interrelated instructions where later ones have strong or weak dependencies on previous ones.When the amount of in-context examples and task descriptions necessary to cover the task constraints increases, even advanced models like GPT-40 can blur critical details, such as the location of a previously used object.Thus, there is a growing need to enhance the power of LLMs with "memory-augmented prompting" (Sarch et al., 2023a;Lewis et al., 2020;Mao et al., 2020).</p>
<p>We introduce KARMA, a plug and play memory system tailored for indoor embodied agents.The memory system comprises both long-term memory, 'Institute of Automation, Chinese Academy of Science -represented as a non-volatile 3D scene graph, and for Society ate information about objects encountered during instruction execution.The memory system allows agents to accurately recall the positions and states of objects during complex household tasks, reducing task redundancy and enhancing execution efficiency and success rates.</p>
<p>On top of the memory system design, we propose to effectively maintain the contents of the memory given the capacity constraints.Specifically, we use the metric hit rate that measures how often a memory recall requirement is satisfied.We demonstrate that a higher hit rate indicates an improved replacement policy and enhanced system performance.Using this metric, we propose replacing the least recently used (LRU) unit whenever a new unit needs to be incorporated into a full memory.Our findings show that this approach achieves a higher hit rate compared to a naive first-in-firstout policy.</p>
<p>In summary, the paper makes following contributions to the community: Large language models have been widely used in robotic applications (Huang et al., 2022;Ahn et al., 2022) due to their impressive generalization abilities and common-sense reasoning capabilities (Brown et al., 2020;Madaan et al., 2022;Achiam et al., 2023).In most cases, LLMs replace the task planning and decision making modules in traditional robotic computing pipeline.Most robotic applications now encode sensor inputs into the format of LLM-accepted tokens and use LLMs to generate the next instructions, which further connect to robots through predefined skills or basic movements across different degrees of freedom (Ahn et al., 2022;Jin et al., 2023;Wu et al.. 2023a,c).</p>
<p>2.2</p>
<p>Memory-Augmented Prompting of LLM-Based Agent Using LLMs as task planner for robots face the challenge of accurately retaining information across multiple interdependent tasks.Thus, augmenting LLM-based agents with different forms of memory is a common approach in role-playing games (Shao et al., 2023;Li et al., 2023a;Wang et al., 2023e;Zhou et al., 2023;Zhao et al., 2023), social simulations (Kaiya et al., 2023;Park et al., 2023;Gao et al., 2023;Li et al., 2023b;Hua et al., 2023), personal assistants (Zhong et al., 2024;Modarressi et al., 2023;Lu et al., 2023;Packer et al., 2023;Lee et al., 2023;Wu et al., 2023b;Hu et al., 2023;Liu et al., 2023;Liang et al., 2023), open-world games (Wang et al., 2023a;Zhu et al., 2023;Wang et al., 2023f;Yan et al., 2023), code generation (Tsai et al., 2023;Chen et al., 2023a;Qian et al., 2023;Li et al., 2023b;Zhang et al., 2024b), recommendations (Wang et al., 2023d,c; Zhang et al., 2024a), and domain-specific expert systems (Wang et al., 2023b;Yang et al., 2023;Zhao et al., 2024b).</p>
<p>The definition and formats of the memory is distinctive in different works.Historical actions (Park et al., 2023), thoughts (Liu et al., 2023), contexts (Liang et al., 2023;Packer et al., 2023) are explored.Different memory management mechanisms are also designed and evaluated.For example, agents can simply use text indexing to match relevant memory; the memory recall and management can also be much more complicated, involving text embedding, semantic retrieval (Zhao et al., 2024a) and Graph RAG (Edge et al., 2024).</p>
<p>In the field of embodied agents, much of the research (Kagaya et al., 2024;Zhang et al., 2023;Sarch et al., 2023a;Wang et al., 2023f) focuses on storing and recalling past experiences, allowing agents to learn from previous interactions and make more informed decisions.Research (Kim et al., 2023) uses short-term memory to maintain and continuously track the positions of objects through semantic labels.Additionally, other studies utilizes structured maps as long-term memory, enabling agents to more efficiently locate places or objects in vision-language navigation tasks (Zhan et al., 2024;Chiang et al., 2024).However, these approaches fail to address challenges such as hallucinations or memory inconsistencies that often arise long-sequence task planning with LLMs.Furthermore, integrating memory mechanisms into LLMs remains at a preliminary stage, particularly regarding memory saving and updating mechanisms.For example, saving everything permanently can result in unaffordable storage requirements, while refreshing the memory every time agents restart will lose any long-term capability.Additionally, the decision of which memory unit to replace remains unsolved.Most approaches use either a forgetting curve (Zhong et al., 2024) or the simple first-infirst-out principle (Packer et al., 2023) without detailed discussions on context-specific updates.</p>
<p>Our work addresses these limitations by incorporating a tailored memory framework for embodied Al agents.This system includes long-term memory in the form of a 3D scene graph representing static objects and short-term memory for instant information about recent activities.This long-short memory approach helps the agent better understand its environment and recent actions.Various exit and update mechanisms are discussed to maintain effectiveness even under fixed memory capacity, providing a comprehensive solution for long sequential tasks in household environments.</p>
<p>Method</p>
<p>We describe the methodology in this section, with start on elaborating the problem setup (Sec.3.1), Sec.3.2 gives an overview of the framework and Sec.3.3 and Sec.3.4 reveals the long-term and short-term memory design.We wrap Sec.3.6 with the novel memory exit and replacement mechanism.</p>
<p>Problem setup</p>
<p>Although generalizable, our work focuses on indoor environment where users send instructions to an agent to perform a series of tasks, H = Lig, Lt, ,.-., Tt), These tasks are typically related in terms of both time and order of completion.For instance, if the agent is asked to prepare a salad, it must first wash an apple (/;,) and cut it (J;,), then repeat the process with a tomato (J;,,J¢,), and finally place the ingredients into a bowl and mix them.During this process, an large volume of highdimensional data is incorporated through various sensors, such as the agent's location and the po-sition and status of different objects.Even when equipped with a large language model as its planner, the agent may lose track of its tasks and need to re-explore the environment, which motivates our work to customize a memory system to augment the agent.</p>
<p>In this paper, we use S € {Smanipulation U Snavigation } to represent the set of skills that the agent can perform, which should be executed by a LLM through pre-defined APIs.The instruction I can be further decomposed into an ordered set of K sub-tasks, T = {T), T2,...,Tk}, where Kk represents the sequence of sub-tasks over time.</p>
<p>Overview</p>
<p>KARMA is a memory system tailored for embodied Al agents, incorporating memory design, recall using context embedding with a pre-trained LLM and an accurate replacement policy.Specifically, we design two memory modules: long-term memory and short-term memory.The long-term memory comprises a 3D scene graph (3DSG) representing static objects in the environment, while the shortterm memory stores instant information about used or witnessed objects.The long-term memory aids the agent in better understanding the environment, and the short-term memory helps the agent understand its recent activities.Due to fixed memory capacity, we also discuss various exit and update mechanisms.Fig. 1 provides an overview of our work.</p>
<p>Long-Term Memory Design</p>
<p>Long-term memory is large in size, non-volatile, and task-irrelevant.It should be built incrementally and updated infrequently.This type of memory is designed to store static information that remains constant over extended periods, such as the layout of the environment and the positions of immovable objects.In the context of an indoor agent, semantic maps serve as an appropriate carrier for it.In many forms of semantic maps, KARMA uses a 3D scene graph to represent the environment.The main reason we choose a 3DSG instead of 2D semantic maps or voxel grids is that 3DSG offers a more accurate and comprehensive representation of the environment and features a topological structure, which is essential for tasks that require precise navigation and manipulation.Also, even a state-ofthe-art multi-modality LLM has difficulties understanding the geographic relationships from a 2D semantic map, while a 3DSG display it explicitly.The 3DSG utilizes a hierarchical structure encompassing floors, areas, and objects, not only capturing the spatial relationships and attributes of objects but also leveraging the benefits of a topological graph.This structure is particularly advantageous when expanding the map to represent the environment, as its sparse topological nature effectively mitigates the impact of accumulated drifts compared to dense semantic maps.Thus, 3DSG is better suited to meet the navigation needs in unknown environments.The construction process of the 3DSG is similar to existing works (Rosinol et al., 2021;Armeni et al., 2019;Rana et al., 2023), as illustrated in Figure 2. We establish and manage a hierarchical topological graph G = (V,E), where the set of vertices V is composed of Vj U... UV, with &amp; = 3, Each V; represents the set of vertices at a particular level of the hierarchy.The area nodes, V2 = {V3}, V?,..., ViV}, are evenly distributed across the reachable regions in the indoor environment, with their world coordinates acquired through a simulator.If two area nodes are navigable to each other, an edge is established between them.For each area node, we detect the types and additional information of ob-jects within a certain radius, using data acquired through a simulator.In real-world applications,this object detection can be performed using methods such as Faster R-CNN.The detected immovable entities are then assigned as object nodes to their respective area nodes.These object nodes encode detailed attributes such as volume and 3D position.</p>
<p>In our framework, the agent gradually builds and maintains a 3DSG as it explores the indoor environment.The graph remains unchanged unless the indoor environment change.When being used by the planner, we transform the 3DSG into a topological graph and serialized it into a text data format that can be directly parsed by a pretrained LLM.An example of a single area node from the 3DSG is as follows: {name: node_1l, type: Area, contains: [bed, table, window, ...], adjacent nodes: [node_2, node_8], position: [2.34, 0.00, 2.23]} with edges between nodes captured as {node_1 ++ node_2, node_1 &lt;&gt; node_8}.</p>
<p>Our design and use of long-term memory aim to provide accurate geometric relationships within the indoor environment.With this information, the agent is able to reduce the cost for repetitive environment exploration by allowing the addition or deletion of nodes through topological relationships, thus updating the environment representation seamlessly.This approach effectively avoids the drift errors typically caused by loop closure detection in traditional SLAM methods, and it minimizes the need for extensive place recognition processes, saving computational resources, storage, and time.</p>
<p>Moreover, long-term memory enhances the agent's ability to make informed decisions based on a comprehensive understanding of the environment.This capability is particularly useful for planning complex, multi-step tasks.By accessing detailed and persistent environmental data, the agent can predict potential obstacles and plan its actions more effectively, thereby improving both task completion success rates and execution efficiency.Also, the 3DSG is updated when the indoor environment changes, capturing the up-to-date information.</p>
<p>Short-Term Memory</p>
<p>Short-term memory is small, volatile, and frequently updated.It is refreshed every time the agent starts and provides instant memorization of recently used objects and their status during task execution.This ensures that the same objects or relevant information are readily available for subsequent tasks.</p>
<p>Among all the information the agent captures during tasks, vision data is relied upon, as it provides the highest information density compared to other sensor inputs.After capturing an image, we use a vision language model (VLM) to analyze the image and extract the state of the object of interest (OOT).This process is task-specific, meaning the VLM is fed both the task and the image to handle multiple objects in the image.Subsequently, the world coordinates (acquired through a simulator), the state (generated by the VLM), and the raw image form a memory unit in the short-term memory, akin to a line of data in a cache.Finally, a multimodality embedding model converts the memory unit into a vector for later recall.</p>
<p>We use an example to illustrate the design of KARMA's short-term memory.Given a task asking the agent to 'wash an apple and place it in a bowl, the agent will memorize the coordinates of the apple and its state (cleaned) at the end.If a subsequent task asks the agent to "bring an apple,' KARMA will retrieve the apple's memory from short-term memory, include it in the prompt, and query the LLM to generate a more efficient task plan.This saves the agent from exploring the kitchen to find the apple, reduces interactions with the LLM, and speeds up the process.KARMA's planner uses both long-term and shortterm memory when interacting with the LLM.As mentioned earlier, the entire long-term memory is directly serialized into the prompt, while only one unit of the short-term memory can be selected.KARMA uses vector similarity to select from the entire short-term memory.Each short-term memory is embedded into a set of vectors using a pre-trained embedding model.For the current instruction J, KARMA retrieves the top-K most similar memories-those with the smallest cosine distance to the embedding of the input instruction J.The corresponding text content of these memories is then added as context to the LLM prompt.</p>
<p>We show an example prompt in Apdx. A. It includes the action code for the basic skills S (parameterized as Python functions), examples of task decomposition, the input instruction J, and the retrieves short-term memory and long-term memory.The LLM is tasked with generating action code based on the parameterized basic skills S.</p>
<p>Memory Replacement</p>
<p>Unlike long-term memory that can be stored in nonvolatile storage, short-term memory has a fixed capacity and can easily become full.An effective short-term replacement policy ensures it remains highly relevant to subsequent tasks.</p>
<p>Hit rate.We use memory hit rate to evaluate the effectiveness of memory replacement policies.This metric is defined as the ratio of the number of times the required memory units are found in short-term memory to the total number of queries.It is widely used in evaluating cache replacement policies (Einziger and Friedman, 2014), with higher values indicating better performance.</p>
<p>First-In-First-Out (FIFO).The FIFO replacement policy is the most straightforward.It manages memory units as a queue.When the queue is full and a new memory unit needs to be added, the earliest entry will be removed from the queue.</p>
<p>We improve the FIFO policy to better suit our application by adding a merging option.When a new memory unit needs to join the queue and the queue is full, we first check the object's ID in all memory units in the queue.If the same ID exists, the new unit will replace the old one with the same object's ID, instead of replacing the oldest unit.</p>
<p>Least Frequently Used.A more complex yet accurate replacement policy is Least Frequently Used (LFU).The design principle of LFU is based on the usage frequency of each memory unit.Whenever a new memory unit needs to join, the existing unit with the lowest usage frequency is replaced.This results in a high hit rate, as the memory retains frequently-used units.Since perfect LFU is not feasible, we use an approximate method called W-TinyLFU.</p>
<p>W-TinyLFU maintains two segments of memory: a main segment and a window segment.The main segment is organized in a two-segment Least Recently Used (LRU) manner, containing a protection segment and an elimination segment.Units in the protection segment are the safest; even if they are picked for replacement, they first move to the elimination segment.</p>
<p>Every time a unit needs to join the memory, it enters the window segment first.When the memory is full and a unit needs to be evicted, a comparison occurs among all units in the window segment and the elimination segment.The memory then selects the unit whose eviction would minimally impact the overall usage frequency and evicts it.W-TinyLFU uses counting Bloom filters (Luo et al., 2018) as the basic data structure to count the usage of memory units.To keep frequency statistics fresh, W-TinyLFU applies a reset method.Each time a memory unit is added, a global counter is incremented.When the counter reaches a threshold W, all counters are halved:c; &lt;-3.</p>
<p>Experiments</p>
<p>We discuss the setup Sec.4.1 and metrics Sec.4.2 first, followed by extensive experiments.This includes success rate and efficiency (Sec.4.3), different replacement policies (Sec.4.4), ablation study (Sec.4.5) and real-world deployment(Sec.4.6).</p>
<p>Experimental Setup and Metrics</p>
<p>Experimental Settings.We use the widelyadopted AI2-THOR simulator (Kolve et al., 2017) for evaluation.The simulator's built-in object detection algorithm provided the label of objects and their relevant information for both long-term and short-term memory.Additionally, we employ Ope-nAI's text-embedding-3-large model as the embedding model for memory recall.</p>
<p>Baseline.To our best knowledge, most current methods using LLMs for task planning are very similar with LoTa-Bench (Choi et al., 2024).It provides a prompt that includes a prefix, in-context examples to the LLM, and then the LLM calculates the probabilities of all executable skills based on this prompt and selects the skill from skill sets most likely to complete the task.We also use it as our baseline.Additionally, we optimize the efficiency and success rate of planning and executing tasks in LoTa-Bench by referring to the skill sets configurations and selection described in SMART-LLM (Kannan et al., 2023).</p>
<p>Dataset.</p>
<p>The dataset construction utilizes tasks from the ALFRED benchmark (Shridhar et al., 2021).By extracting its typical tasks and reorganizing them into long sequence tasks that align with everyday human needs, we ensured a more accurate assessment.More details of the dataset are provided in supplementary material.</p>
<p>This new dataset, ALFRED-L, includes 48 highlevel instructions that detail the length, relevance, and complexity of sequential tasks.Additionally, it provides corresponding AI2-THOR floor plans to offer spatial context for task execution.We also include the ground truth states and corresponding location of objects after the completion of each subtask.This ground truth is used as symbolic goal conditions to determine whether the tasks are successfully completed.For example, conditions such as heated, cooked, sliced, or cleaned are specified.Our dataset comprises three task categories: Simple Tasks have multiple unrelated tasks.The agent is assumed to perform sequential tasks with a length of less than five, without requiring specific memory to assist in task completion.</p>
<p>Composite Tasks include highly related tasks.These tasks involve multiple objects, and the agent needs to utilize memories generated from previous related tasks to execute subsequent subtasks.</p>
<p>Complex Tasks consist of multiple loosely related tasks.Some of these tasks involve specific objects, while others involve vague object concepts.For example, the agent be instructed to wash an apple(J;,) and cut it(/;, ), then to place a red food on the plate(J;,).ALFRED-L comprises 15 tasks categorized as simple tasks, 15 tasks as composite tasks, 18 tasks as complex tasks.</p>
<p>Additionally, we use another dataset to better assess the performance of the memory replacement mechanism.</p>
<p>The new dataset, ALFWorld-R, consists of long-sequence tasks A = {Ii,,1h,,..-, Jey}.with each task [;,,7 € {0, 1,2, ..., N} in the sequence randomly selected from tasks in ALFRED.</p>
<p>4.2</p>
<p>Evaluation Metrics.</p>
<p>Success Rate (SR) is the percentage of tasks fully completed by the agent.A task is considered complete only when all subtasks are achieved.</p>
<p>Memory Retrieval Accuracy (MRA) is a binary variable determines if related memory can be successfully retrieved.</p>
<p>Memory Hit Rate (MHR).The definition is the same as the hit rate described in Sec.3.6.</p>
<p>Reduced Exploration (RE).This metric measures the effectiveness of the system in reducing unnecessary exploration attempts.RE = Fegieel where Fotai is the total number of exploration attempts, Freduced 1s the number of exploration attempts that were reduced.</p>
<p>Reduced Time (RT).This metric measures the proportion of time saved by reducing unnecessary actions during task execution.RT' = Tlic where Tiotal 18 the total time taken for the task, Theducea 18 the time that was reduced.</p>
<p>Success Rate and Efficiency Evaluation</p>
<p>Success Rate &amp; Task Efficiency.In Tbl. 1, we present the quantitative results of KARMA and the baselines on the sequence tasks dataset ALFRED-L.For complex tasks, KARMA achieves a 21% task success rate improvement and a 69% reduction in time, which represent a relative improvement of 2.3x and 62.7x, respectively, compared to HELPER (the best-performing baseline in this setting).For composite tasks, KARMA achieves a 43% task success rate improvement and a 68.7% reduction in time, which represent a relative improvement of 1.3x and 3.4x, respectively, compared to CAPEAM (the best-performing baseline in this setting).For simple tasks, KARMA achieves a 42% task success rate improvement and a 61.2% reduction in time, which represent a relative improvement of 1.1x and 2.3x, respectively, compared to HELPER (the best-performing baseline in this setting).It is worth noting that since simple tasks do not require the use of short-term memory, KARMA does not show a significant improvement in task success rate over other baselines.</p>
<p>Memory Retrieval Accuracy.We show the accuracy of memory recall in the MRA column of Tbl. 1.Our memory system achieves a recall accuracy that is 2.2x higher for composite tasks compared to complex tasks, as the recall method has certain limitations when instructions contain ambiguous information.We believe this is due to the inherent performance limitations of the commonly used models for semantic matching.For complex tasks, instructions may contain particularly ambiguous semantics, such as "get me a highcalorie food," where even the most advanced semantic matching models perform poorly.</p>
<p>Replacement Policy Evaluation</p>
<p>Fig. 4 illustrates the efficiency of the FIFO policy compared to the W-TinyLFU policy under various configurations of window segment size and main segment size, with a total of 10 memory units.We show the number of consecutive tasks performed by the agent on the x-axis.The y-axis shows the memory hit rate for each memory replacement policy, representing the effectiveness of each policy.Vertical lines of different colors indicate whether the corresponding policy has undergone a warm-up phase.We consider memory to be warmed up when the occupancy rate of the memory units exceeds  [10] means the memory size of FIFO is 10, [9,1] means the memory size of W-TinyLFU is also 10, the main segment is 1, window segment is 9.
@-W-TinyLFU [9,1] W-TinyLFU [7,3] W-TinyLFU [4,6] -@-W-TinyLFU [1,9] -@-FIFO [10]
95%.After all replacement policies have undergone their warm-up phases, the W-TinyLFU policy with a window segment size of 9 achieves the highest memory hit rate.This indicates that, on the ALFRED-R dataset, a larger window segment size in the W-TinyLFU policy allows for more effective utilization of memory units.For W-TinyLFU, a larger window size typically covers a broader time range, capturing more memory units that are likely to be frequently recalled.These memory units have a high probability of being reused in the task sequence, thereby increasing the memory hit rate.</p>
<p>-e@-FIFO [5] -@-FIFO [10] FIFO the memory is with size equals to 10. Fig. 5 illustrates the memory hit rate of FIFO pol-icy with different numbers of memory units, with X-axis represents the number of tasks.As expected, larger memory size brings higher hit rate, the memory hit rate with 25 memory units is 4.6 higher than with only 5 memory units.Similar results can be extracted through Fig. 6, where memory hit rate with 25 memory units is 3.9 higher than with only 5 memory units.</p>
<p>-e@-W-TinyLFU means the memory size of wlinyLFU is 10, the main segment is 1, window segment is 9.In Fig. 7, we illustrate the impact of memory hit rate on the efficiency of task execution.The x-axis shows the memory hit rate of the W-TinyLFU policy with a window segment size of 9 and a main segment size of 1.The y-axis displays the proportion of reduced exploration.We demonstrate that the memory hit rate and the proportion of reduced exploration are linearly correlated.This means that increasing the memory hit rate enhances the agent's task execution efficiency.A higher memory hit rate signifies more efficient use of memory units.This enhances the agent's ability to recall relevant information, reducing the amount of action code needed for task execution, and ultimately improving overall task performance.
[4,1] -@-W-TinyLFU [9,1] W-TinyLFU [14,1] W-TinyLFU [19,1] W-TinyLFU [24,1] 0.4 | 0.3+ | oO I © 0.2- = oat - 0.0 1 1 L 1 L rT 0 10</p>
<p>Ablation Study</p>
<p>In Tbl. 2, we evaluate the performance of KARMA after removing short-term memory or long-term memory.The removal of short-term memory significantly affected the agent's ability to handle com- plex and composite tasks, with the success rate dropping by 1.9x and 4.2x, respectively.However, this did not greatly impact the agent's task execution efficiency, which decreased only by 1.2 and 1.1.On the other hand, removing long-term memory had a notable impact on task execution efficiency, with the RT decreasing by 2.7, but its effect on success rate was less pronounced, with SR only dropping by 1.2x.</p>
<p>In summary, short-term memory plays a key role in improving task success rates, while long-term memory has a greater impact on task efficiency.</p>
<p>Long-term memory retains 3D scene maps representing the environment, helping to reduce the action code generated by the LLM during task planning, thereby enhancing task execution efficiency.Meanwhile, short-term memory stores information about recently used objects, ensuring that these objects or relevant details are readily accessible for future tasks.</p>
<p>4.6</p>
<p>Real-world deployment</p>
<p>We deploy KARMA ona mobile manipulation robot consisting of a UR3 robotic arm and a six-wheeled chassis to demonstrate KARMA's ability to store and retrieve memory, enhancing the LLM's capability for planning long-sequence tasks in real-world environments.For the robot's navigation and obstacle avoidance, we utilize Google Cartographer for simultaneous localization and mapping (SLAM).</p>
<p>The camera mounted on the robotic arm's gripper ing embodied AI agents by integrating external long-and-short term memory systems.Through the implementation of a customized memory system, recall mechanism, and replacement policy, we demonstrate significant improvements over stateof-the-art embodied agents that also utilize memory.Specifically, our memory-augmented AI agent achieves success rates that are 1.3x higher in composite tasks and 2.3 higher in complex tasks.Additionally, task execution efficiency is improved by 3.4x in composite tasks and an impressive 62.7 x in complex tasks.This memory system streamlines the transition from simulation to real-world robotic applications, allowing long-and short-term memory storage and recall methods to be seamlessly integrated into task planning for real robotic systems (Sun et al., 2024).</p>
<p>Limitations</p>
<p>Ideal Simulation Environments.</p>
<p>In this work, all evaluations are performed under ideal simulation environments, free from interruptions by other agents or humans.However, this ideal situation is not reflective of real life.Although this paper includes extensive experiments, it lacks evaluation of how the memory system will behave in real-world scenarios.Specifically, the number of objects in the real world will significantly increase compared to a simulation environment, making the effectiveness of recall and replacement mechanisms crucial to final performance.Additionally, we have not tested the system's response to intentional disturbances by humans.These factors constitute the primary limitation of this paper.</p>
<p>Lack of Biological Theory.Although effective, the current design of the memory system is analogous to the memory systems of existing computing platforms.For instance, the concept of short-term memory and its replacement can be found in cache design.However, human memory may not function in this manner.This work borrows terminology from human memory yet lacks theoretical support from a biological perspective, which constitutes its second limitation.</p>
<p>Open-loop Planning.In this work, all memory operations and planning are open-loop, meaning there is no feedback.However, in most robot system designs, feedback is necessary.For example, if the memory is incorrect, there is no mechanism designed for eviction or updating.The lack of feedback constitutes the third limitation of this paper.We provide detailed skill APIs and their corresponding action codes in the Listing?2.</p>
<p>E LANGUAGE MODELS</p>
<p>Tbl. 4 lists the language models used in experiments and outlines their core functions.</p>
<p>F Details of image analysis in short-term memory</p>
<p>In Fig. 10, we present the prompt used to analyze images stored in short-term memory by the Vision-Language Model (VLM).The text highlighted in blue, [Image], represents the placeholder that will be filled with an image, while [task] will be replaced with the actual instruction.We employed a step-by-step Chain of Thought approach to guide the VLM in identifying the relevant objects and their corresponding states.</p>
<p>G Anexample result of KARMA on the</p>
<p>ALFRED-L dataset</p>
<p>In Fig. 11, we present images of the agent performing tasks in the AI2-THOR simulator.</p>
<p>Listing 2: Full Skill API and Action CODE used in the prompts.</p>
<p>Fig. 2 :
2
Fig. 2: Transforming 3D scene graphs into prompts.</p>
<p>Fig. 3 :
3
Fig. 3: Recalling long-term and short-term memory</p>
<p>Fig. 4 :
4
Fig. 4: The memory hit rate of FIFO and W-TinyLFU.</p>
<p>Fig. 5 :
5
Fig. 5: Evaluation on different FIFO sizes.[10] means</p>
<p>Fig. 6 :
6
Fig.6: Evaluation on W-TinyLFU configurations.[9,1]means the memory size of wlinyLFU is 10, the main segment is 1, window segment is 9.In Fig.7, we illustrate the impact of memory hit rate on the efficiency of task execution.The x-axis shows the memory hit rate of the W-TinyLFU policy with a window segment size of 9 and a main segment size of 1.The y-axis displays the proportion of reduced exploration.We demonstrate that the memory hit rate and the proportion of reduced exploration are linearly correlated.This means that increasing the memory hit rate enhances the agent's task execution efficiency.A higher memory hit rate signifies more efficient use of memory units.This enhances the agent's ability to recall relevant information, reducing the amount of action code needed for task execution, and ultimately improving overall task performance.</p>
<p>Fig.9: " /short_term/images/Bread.jpq" tured after the task of putting bread on the countertop was executed.stores at was cap-</p>
<p>Table 1 :
1
Evaluation of KARMA and baseline for different categories of tasks in ALFRED-L.
MethodsSimple TasksComposite TasksComplex TasksSRMRARERTSRMRARERTSRMRARERTLoTa-Bench(Modified) HELPER(Sarch et al.,2023b)0.41 0.40-0.251-0.2630.23 = 0.21---0.243-0.1780.04 0.09-0.018-0.011CAPEAM(Kim et al., 2023) KARMA0.35 0.42-0.054 0.582-0.002 0.6120.33 0.43-0.930.293 0.9020.201 0.6870.07 0.21-0420.012 0.8670.008 0.690</p>
<p>Table 2 :
2
Ablation Study.
MethodsSimple TasksComposite TasksComplex TasksSR. 041 KARMA(w/o long term memory) -<em> 0.40 LoTa-Bench(Modified) KaRMA(w/o short term memory) 0.44 KARMA 0.42MRA ----RE ; O01 0573 0582 0.612 RT -0.002 0.605SR 0.23 0.35 0.22 043MRA 5 1 -0.93RE 5 0.329 «0.210 RI -0.774 0.624 0.902 0687SR. 0.04 «0.12 0.05 0.21MRA -043 -042RE _ RT ; 0.021</em>-(0.013 0.784 (0.654 0.867 0.690© 046012was used to detect objects, feeding the input intooceLangSAM(Kirillov et al., 2023) for segmentation© oral = 0 .79.10 Sand semantic matching to locate the object to be2 oo &amp; 0.08no &amp;ad eo?70.08 a =! 710.06 3grasped. And then AnyGrasp(Fang et al., 2023) . . tos generates the most suitable grasping position anda fii o.o4to °70.04 B. plans the arm's motion path.oe Q = 0.008Rg a-40.02 &gt; 0.005 Conclusionwy90.000.050.100.150.20Hit RateIn this paper, we explore the potential of enhanc-Fig. 7: The impact of memory hit rate on the agent'stask execution efficiency.
Alibaba Group, Hangzhou, China
Supplementary MaterialA PromptsIn Fig.8, we provide a prompt template that integrates both long-term and short-term memory, specifically designed to enhance the capabilities of LLMs in planning long-sequence tasks.We present the contents stored in short-term (Listingl) during task execution .In Listing], we present the text and image stored in short-term memory after executing the sequential tasks of washing a potato and placing it on the countertop, washing a tomato and placing it on the countertop, putting bread on the countertop, and throwing the knife in the trash.In short-term memory, the "objectId" is a unique identifier for each object that remains constant over time.This identifier is used to determine if the object is the same before and after memory updates.The "position" records the current location of the object after the agent's interaction or the location of objects the agent has encountered during task execution.The "imagePath" stores images of objects captured by the agent, which are used for subsequent analysis by the Vision-Language Model (VLM).In Fig.9, we present the image of bread captured by the agent after executing the task of putting bread on the countertop.This image is stored at " [short_term/images/ Bread.jpq' .Listing 1: The detailed content of short-term memory during task execution.short_term_memory=[ { "objectType":"Tomato", "position": { "x": @.9792354106903076, "vy":1.7150063514709473, "2":-2.606173276901245 },"objectId": "Tomato | -@0.39|]+01.14]-00.81" "imagePath": "/short_term/images /Tomato.jpg" }, { "objectType":"Apple", "position": { "x":1.0981664657592773 , "vy": @.9569252133369446, "2":-2.4071836471557617 }, "objectId": "Apple | -@0.47|+01.15|+00.48""imagePath": "/short_term/images /Apple.jpg" Be { "objectType": "DishSponge", "position": { "x":-1,.8567615747451782, "y": @.14490127563476562, "Zz":-1.619217514991 7603 }, "objectId": "DishSponge | -@1.94|+@0.75|-@1.71""imagePath": ""/short_term/ images /DishSponge.jpg" be { "objectType": "Potato", "position": { "x":1.098166584968567, "vy": @.9390283823013306, "2"s -2.2535505294799805 Be "objectId": "Potato |-@1.66|+00.93]-@2.15""imagePath": "/short_term/images /Potato.jpg" }, { "objectType": "Book", "position": { "x": -1.35060715675354, "y": 1.1669094562530518, "Zz":1.970085859298706 }, "objectId": "Book |+0@.15/+01.10]+00.62""imagePath": "/short_term/ images /Book.jpg" }; { "objectType": "Bread", "position": { "x": @.9692967534065247 , "y": @.9761490225791931, "2": -2.330367088317871Io "objectId": "Bread |-@0.52]+01.17|-00.03""imagePath": "/short_term/ images /Bread.jpg" }, { "objectType":"Knife", "position": { "x": -2.0168256759643555, "y": @.24547088146209717, "2": 2.1725265979766846 }, "objectId": "Knife | -@1.70|+00.79|]-00,.22""imagePath": "/short_term/images /Knife.jpg" }, { "objectType":"Lettuce", "position": { "x":-1.6119909286499023, "y": @.9801480174064636, "Zz":-@.6989647150039673 }, "objectId": "Lettuce |-@1.81]+00.97|-00.94""imagePath": "/short_term/ images /Lettuce.jpg" }CMore Details on ALFRED-L ALFRED-L includes three types of tasks: simple tasks, composite tasks, and complex tasks.These tasks are adapted from the original ALFRED dataset.In ALFRED-L, placing an object inside the fridge was deemed successful when the object is in the fridge.We enhanced this by adding a subgoal "INSIDE(Fridge): 1" to ensure the object is correctly placed inside fridge.For tasks like "wash an apple" in ALFRED-L, the goal conditions involve the apple being rinsed in the sink.The Table3: Task types and samples for each type in the ALFRED-L dataset.place potato on the plate -wash an apple &gt; get a frying pan.Composite Tasks wash a tomato -wash a potato -slice a bread -put the bread in the fridge -place the clean, red food on the plate.Table4: List of language models used in the experiments and their respective roles.Language Model Role FunctionOpenAlI GPT-40 VLM Analyzes the state of objects within the image of short-term memory.OpenAI GPT-4o0 LLM as Planner Task decomposition.OpenAI text-embedding-3-large Embedding Model Recalls memory units.<System Rolo> As an image analysis expert, your task is to infer the state of objects in the image through step-by-step reasoning.<User Role> 1.Provide a detailed description of this image|!mage}.2.From the given task{ Task], extract the relevant content from the first step's image description that pertains to the mentioned objects.3.Based on the object descriptions extracted in the second step, match each object to one of the following states: heated, cooked, sliced, cleaned, dirty, filled, used up, off, on, opened, closed, none.4,Summarize the results from step three in the following format: object: state.Fig.10: The prompt template for GPT-4, utilizing a step-by-step approach to guide VLM in identifying the relevant objects and their corresponding states.Instruction: wash an tomato and place it on the countertop &gt;&gt; find an apple and place it on the countertop =&gt; slice the clean tomato
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Do As I Can. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Grounding Language in Robotic Affordances. Not As I Say2022arXiv preprint</p>
<p>3d scene graph: A structure for unified semantics, 3d space, and camera. Iro Armeni, Zhi-Yang He, Amir Zamir, Junyoung Gwak, Jitendra Malik, Martin Fischer, Silvio Savarese, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. 2024. Rt-h: Action hierarchies using language. arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:22122022arXiv preprint</p>
<p>Language Models are Few-shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020</p>
<p>Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, Haoyang Zhang, arXiv:23 10.08067Gamegpt: Multi-agent collaborative framework for game development. 2023aarXiv preprint</p>
<p>Siwei Chen, Anxing Xiao, David Hsu, arXiv:23 11.17406Llm-state: Expandable state representation for longhorizon task planning in the open world. 2023barXiv preprint</p>
<p>Lewis Hao-Tien, Zhuo Chiang, Zipeng Xu, Mithun Fu, George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, arXiv:2407.07775Mobility vla: Multimodal instruction navigation with long-context vims and topological graphs. 2024arXiv preprint</p>
<p>Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang, arXiv:2402.08178Lota-bench: Benchmarking language-oriented task planners for embodied agents. 2024arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Tinylfu: A highly efficient cache admission policy. Gil Einziger, Roy Friedman, 201420</p>
<p>22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. </p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. Chenxi Hao-Shu Fang, Hongjie Wang, Minghao Fang, Jirong Gou, Hengxu Liu, Wenhai Yan, Yichen Liu, Cewu Xie, Lu, EEE Transactions on Robotics. 2023</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.149842023. s*: Social-network simulation system with large language model-empowered agents. arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao ; Wenyue, Lizhou Hua, Lingyao Fan, Kai Li, Jianchao Mei, Yingqiang Ji, Libby Ge, Yongfeng Hemphill, Zhang, arXiv:2306.03901arX1v:2311.17227Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. 2023. 2023arXiv preprintInternational Conference on Machine Learning</p>
<p>Adapt: Action-aware driving caption transformer. Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Rap: Retrieval-augmented planning with contextual memory for multimodal Ilm agents. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You, arXiv:2402.036102024arXiv preprint</p>
<p>Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn, arXiv:2310.02172Lyfe agents: Generative agents for low-cost real-time social interactions. 2023arXiv preprint</p>
<p>Shyam Sundar Kannan, L N Vishnunandan, Byung-Cheol Venkatesh, Min, arXiv:2309.10062Smart-llm: Smart multi-agent robot task planning using large language models. 2023arXiv preprint</p>
<p>Context-aware planning and environment-aware memory for instruction following embodied agents. Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi, Proceedings of the LEEE/CVF International Conference on Computer Vision. the LEEE/CVF International Conference on Computer Vision2023</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXivAI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>Prompted Ilms as chatbot modules for long open-domain conversation. Gibbeum Lee, Jongho Volker Hartmann, Dimitris Park, Kangwook Papailiopoulos, Lee, arXiv:2305.045332023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kiittler, Mike Lewis, Wen-Tau Yih, Tim Rocktaschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Chatharuhi: Reviving anime character in reality via large language model. 2023aarXiv preprint</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, arXiv:2310.06500Metaagents: Simulating interactions of human behavyiors for llm-based task-oriented coordination via collaborative generative agents. 2023barXiv preprint</p>
<p>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhigiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Thinkin-memory: Recalling and post-thinking enable ms with long-term memory. 2023arXiv preprint</p>
<p>Memochat: Tuning Ilms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Optimizing bloom filter: Challenges, solutions, and comparisons. Lailong Luo, Deke Guo, Richard Ma, Ori Rottenstreich, Xueshan Luo, 2018JEEE Communications Surveys &amp; Tutorials</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, arXtv:2210.07128Language Models of Code are Few-shot Commonsense Learners. 2022arXiv preprint</p>
<p>Generation-augmented retrieval for open-domain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, arXiv:2009.085532020arXiv preprint</p>
<p>Ret-IIm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schiitze, arXiv:2305.143222023arXiv preprint</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXtv:23 10.08560Memegpt: Towards Ilms as operating systems. 2023arXiv preprint</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Fragkiadaki, arXiv:2310.151272023aarXiv preprint</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Gabriel Sarch, Yue Wu, Michael J Tarr, Katerina Fragkiadaki, arXiv:2310.151272023barXiv preprint</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, arXiv:2310.10158Character-llm: A trainable agent for roleplaying. 2023arXiv preprint</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Coté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Wenhao Sun, Sai Hou, Zixuan Wang, Bo Yu, Shaoshan Liu, Xu Yang, Shuai Liang, Yiming Gan, Yinhe Han, arXiv:2412.01663Dadu-e: Rethinking the role of large language model in robotic computing pipeline. 2024arXiv preprint</p>
<p>Rtlfixer: Automatically fixing rtl syntax errors with large language models. Yunda Tsai, Mingjie Liu, Haoxing Ren, arXiv:2311.165432023arXiv preprint</p>
<p>Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, ChatGPT for Robotics: Design Principles and Model Abilities. 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv:2304.06975Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023b. Huatuo: Tuning llama model with chinese medical knowledge. 2023aarXiv preprintLinxi Fan, and Anima Anandkumar</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, ArXiv:2306.02552c. when large language model based agent meets user behavior analysis: A novel user simulation paradigm. 2023carXiv preprint</p>
<p>Recmind: Large language model powered agent for recommendation. Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, arXiv:2308.142962023darXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746arXiv:23 11.05997Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2023f. Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models. 2023earXiv preprintRolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models</p>
<p>TidyBot: Personalized Robot Assistance with Large Language Models. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser, Autonomous Robots. 2023a</p>
<p>Autogen: Enabling next-gen Ilm applications via multiagent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv: 2308.08 1552023barXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023carXiv preprint</p>
<p>Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan, arXiv:2312.17653Larp: Language-agent role play for open-world games. 2023arXiv preprint</p>
<p>Yi Yang, Yixuan Tang, Kar Yan, Tam , arXiv:2309.13064vestlm: A large language model for investment using financial domain instruction tuning. 2023arXiv preprint</p>
<p>Mc-gpt: Empowering vision-and-language navigation with memory map and reasoning chains. Zhaohuan Zhan, Lisha Yu, Sijie Yu, Guang Tan, arXiv:2405.106202024arXiv preprint</p>
<p>Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.02485Building cooperative embodied agents modularly with large language models. 2023arXiv preprint</p>
<p>Agentcf: Collaborative learning with autonomous language agents for recommender systems. Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian Mcauley, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024a</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for realworld repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, arXiv:2401.073392024barXiv preprint</p>
<p>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, arXiv:2402.19473Wentao Zhang, and Bin Cui. 2024a._ Retrievalaugmented generation for ai-generated content: A survey. arXiv preprint</p>
<p>Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, Lin Gui, arXiv:2310.01459Narrativeplay: Interactive narrative understanding. 2023arXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Chemdfm: Dialogue foundation model for chemistry. 2024barXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Characterglm: Customizing chinese conversational ai characters with large language models. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, arXiv:2311.168322023arXiv preprint</p>
<p>Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.17144Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
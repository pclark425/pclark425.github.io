<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270869417</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01437v2.pdf" target="_blank">Needle in the Haystack for Memory Based Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Current large language models (LLMs) often perform poorly on simple fact retrieval tasks. Here we investigate if coupling a dynamically adaptable external memory to a LLM can alleviate this problem. For this purpose, we test Larimar, a recently proposed language model architecture which uses an external associative memory, on long-context recall tasks including passkey and needle-in-the-haystack tests. We demonstrate that the external memory of Larimar, which allows fast write and read of an episode of text samples, can be used at test time to handle contexts much longer than those seen during training. We further show that the latent readouts from the memory (to which long contexts are written) control the decoder towards generating correct outputs, with the memory stored off of the GPU. Compared to existing transformer-based LLM architectures for long-context recall tasks that use larger parameter counts or modified attention mechanisms, a relatively smaller size Larimar is able to maintain strong performance without any task-specific training or training on longer contexts.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Larimar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Larimar (Large language models with episodic memory control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder–decoder LLM augmented with a linear associative external memory that is written with encoded context segments and read via a key-based associative lookup; memory readouts condition the decoder at generation time and memory can be enlarged at test time and kept off-GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Larimar: Large language models with episodic memory control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Larimar</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encoder–decoder LM trained jointly with a linear associative memory module; during inference context is segmented, each segment encoded and written to an external memory matrix M (computed as a least-squares pseudoinverse solution), and at query time a read key (computed from the query prefix) retrieves a single latent readout that conditions the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External linear associative memory (matrix-based associative memory inspired by Kanerva Machine / generative pseudoinverse memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Latent segment encodings stored as rows of a memory matrix M (i.e., vectors / key-value-like rows where rows are encodings); a separate key-memory stores prefix encodings used for nearest-neighbor key selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: solve W† Z (pseudoinverse least-squares) to compute memory matrix rows; Read: compute a reading key w from the query (nearest-neighbor lookup in a key-memory to produce a one-hot key), then read z_read = w M (row lookup). Keys can be based on fixed-length prefixes to improve matching; all read/write ops performed on CPU in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passkey test; Needle-in-the-haystack (haystack dataset / 'San Francisco' needle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context retrieval / recall (needle-in-haystack retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Passkey (context up to ~1.2M tokens): reported examples include 1.2M context: 3-digit 100%, 4-digit 86%, 5-digit 87%, 6-digit 79%, 7-digit 59% (Table 1). For 137K context (Table 2): 3-digit 95%, 4-digit 64%, 'San Francisco' needle 100%. Needle-in-the-haystack: reported strong recall at >100K context (detailed Table 3 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No direct ablation with memory fully disabled reported. Comparisons to baseline models without Larimar memory: Mistral 7B (24K context) showed ~66% (3-digit), 62% (4-digit), 80% (SF); Phi-3-mini-128K (100K context) ~27% (3-digit), 26% (4-digit), 37% (SF). An ablation changing write-key computation ('Larimar (no prefix)') degraded performance at 137K to 88% (3-digit), 14% (4-digit), 0% (SF).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Recall / percent successful recall (success rate); Rouge-L used for San Francisco needle in needle-in-the-haystack (reported as Rouge-L recall)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Memory storage cost scales linearly with number of unique segments (O(N) memory rows); writing via pseudoinverse is O(K^3) in general but reduces to O(N) when keys are one-hot and unique; memory can be offloaded to CPU so GPU memory footprint does not grow with context length, but CPU–GPU transfers are required (encode to CPU for write, readout back to GPU). Using fixed-length prefixes improves matching but may reduce fidelity for complex needles. External memory growth trades space (CPU/RAM/disk) for ability to handle arbitrarily long contexts without increasing GPU memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Writes segments in isolation, losing cross-segment correlations and sequence order — so method is best when relevant information is contained within single segments; when number of unique segments grows with context length, memory size grows and retrieval costs grow; mismatch between query prefix and full segment encoding can cause retrieval of wrong (nearest) segment (noted for 4-digit needles and SF needle); no experiment shown with memory fully disabled in the same model for exact ablation of the memory effect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Nelson, E., Kollias, G., Das, P., Chaudhury, S., & Dan, S. (2024). Needle in the Haystack for Memory Based Large Language Models. arXiv:2407.01437v2 [cs.CL].</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Needle in the Haystack for Memory Based Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6444.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6444.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kanerva Machine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Kanerva Machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distributed associative memory architecture referenced as structural inspiration for Larimar's external memory; stores and retrieves vector encodings in an associative memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The kanerva machine: A generative distributed memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Kanerva Machine (memory architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An associative memory storing distributed vector representations that supports reading/writing of latent encodings; Larimar's external memory is described as being structured similarly to the Kanerva Machine.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Associative distributed memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Latent vector encodings stored in memory matrix rows (distributed representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Associative read/write of encodings (nearest-neighbor key-based access in Larimar adaptation); in referenced work typically probabilistic/latent generative memory updates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Memory architecture (background)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned as inspiration; Larimar departs by using deterministic pseudoinverse least-squares updates rather than Kanerva Machine's multivariate Gaussian posterior updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wu, Y., & Wayne, G. (2018). The kanerva machine: A generative distributed memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Needle in the Haystack for Memory Based Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6444.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6444.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Pseudoinverse Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative pseudoinverse memory (Pham et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory update method that computes memory updates via a pseudoinverse/least-squares solution; Larimar uses the same least-squares solving approach for memory updates instead of probabilistic Gaussian updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative pseudoinverse memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative pseudoinverse memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory update algorithm that finds the least-squares solution to minimize readout error (compute M = W† Z), providing deterministic read/write behavior; Larimar uses this update rule for its external memory writes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Pseudoinverse linear associative memory update (least-squares)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key and value matrices (W and Z); resulting memory matrix M stores encodings as rows</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: compute pseudoinverse W† and solve M = W† Z to minimize ||Z − W M||^2; Read: linear readout using key vector w (z_read = w M).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Memory algorithm / module</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Algorithmic write complexity can be O(K^3) for general W but simplifies to O(N) when keys are one-hot and unique; deterministic when variance terms are set to zero (σ_W = σ_ξ = 0 as in Larimar experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computational cost of pseudoinverse for large non-sparse W; reliance on choice of keys for efficient computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Pham, K., Le, H., Ngo, M., Tran, T., Ho, B., & Venkatesh, S. (2021). Generative pseudoinverse memory. ICLR 2021.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Needle in the Haystack for Memory Based Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6444.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6444.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Infini-Transformer / Infini-attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leave no context behind: Efficient infinite context transformers with infiniattention (Infini-Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer variant that retains memory of all past segments (infinite/very long context) via attention/memory mechanisms; cited as a baseline and comparison for long-context recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leave no context behind: Efficient infinite context transformers with infiniattention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Infini-Transformer (Infini-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer architecture that uses a memory of all past segments (rather than only the last segment) enabling very long-context processing; evaluated by its authors on passkey-like tests and compared here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Attention-based memory (retain memory of all past segments via modified attention)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Cached past-segment representations used as attention context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention over stored past-segment states (infini-attention variant); requires task-specific fine-tuning on long-context instances for best results per cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passkey test</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context retrieval / recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Cited: With fine-tuning on the passkey test, Infini-attention achieved 100% recall up to 1M tokens; without fine-tuning, recall drops to ~O(10%) at 128K (cited from Munkhdalai et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Recall / Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Often requires task-specific fine-tuning on long-context instances and can incur increased compute/memory costs; without fine-tuning performance can degrade substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without fine-tuning, recall drops sharply at long contexts (reported ~10% at 128K).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave no context behind: Efficient infinite context transformers with infiniattention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Needle in the Haystack for Memory Based Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6444.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6444.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT / RMT-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer (RMT) and RMT-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory-augmented transformer approaches (recurrent memory transformer and retrieval-augmented variants) previously shown to achieve near-perfect recall on long-context needle-in-the-haystack tasks after task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrent Memory Transformer (RMT) / RMT-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer variants that maintain recurrent memory (cache of past segments) across segments and/or use retrieval to locate needles; reported to achieve strong recall with task-specific training on needle-in-the-haystack tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Example cited: GPT-2 (137M) when fine-tuned with RMT in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent memory / cached past-segment memory; retrieval-augmented variants</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Cached past-segment representations or retrieval index entries (depending on variant)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Recurrent caching of segment representations and retrieval mechanisms; task-specific fine-tuning to use memory effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Needle-in-the-haystack (long-context recall up to 1M tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context retrieval / recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Cited: Near-perfect recall out to 1M tokens with task-specific training of GPT-2 (137M) in RMT / RMT-Retrieval studies (Bulatov et al., 2022; Kuratov et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Recall / Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Achieves strong long-context recall but requires task-specific fine-tuning of base models; may rely on increased training on long-context instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Often requires task-specific training and may not generalize without such fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Needle in the Haystack for Memory Based Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Larimar: Large language models with episodic memory control <em>(Rating: 2)</em></li>
                <li>Leave no context behind: Efficient infinite context transformers with infiniattention <em>(Rating: 2)</em></li>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Generative pseudoinverse memory <em>(Rating: 2)</em></li>
                <li>The kanerva machine: A generative distributed memory <em>(Rating: 2)</em></li>
                <li>In search of needles in a 11m haystack: Recurrent memory finds what llms miss <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6444",
    "paper_id": "paper-270869417",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Larimar",
            "name_full": "Larimar (Large language models with episodic memory control)",
            "brief_description": "An encoder–decoder LLM augmented with a linear associative external memory that is written with encoded context segments and read via a key-based associative lookup; memory readouts condition the decoder at generation time and memory can be enlarged at test time and kept off-GPU.",
            "citation_title": "Larimar: Large language models with episodic memory control",
            "mention_or_use": "use",
            "agent_name": "Larimar",
            "agent_description": "Encoder–decoder LM trained jointly with a linear associative memory module; during inference context is segmented, each segment encoded and written to an external memory matrix M (computed as a least-squares pseudoinverse solution), and at query time a read key (computed from the query prefix) retrieves a single latent readout that conditions the decoder.",
            "model_size": "1.3B",
            "memory_used": true,
            "memory_type": "External linear associative memory (matrix-based associative memory inspired by Kanerva Machine / generative pseudoinverse memory)",
            "memory_representation": "Latent segment encodings stored as rows of a memory matrix M (i.e., vectors / key-value-like rows where rows are encodings); a separate key-memory stores prefix encodings used for nearest-neighbor key selection.",
            "memory_access_mechanism": "Write: solve W† Z (pseudoinverse least-squares) to compute memory matrix rows; Read: compute a reading key w from the query (nearest-neighbor lookup in a key-memory to produce a one-hot key), then read z_read = w M (row lookup). Keys can be based on fixed-length prefixes to improve matching; all read/write ops performed on CPU in experiments.",
            "task_name": "Passkey test; Needle-in-the-haystack (haystack dataset / 'San Francisco' needle)",
            "task_category": "Long-context retrieval / recall (needle-in-haystack retrieval)",
            "performance_with_memory": "Passkey (context up to ~1.2M tokens): reported examples include 1.2M context: 3-digit 100%, 4-digit 86%, 5-digit 87%, 6-digit 79%, 7-digit 59% (Table 1). For 137K context (Table 2): 3-digit 95%, 4-digit 64%, 'San Francisco' needle 100%. Needle-in-the-haystack: reported strong recall at &gt;100K context (detailed Table 3 in paper).",
            "performance_without_memory": "No direct ablation with memory fully disabled reported. Comparisons to baseline models without Larimar memory: Mistral 7B (24K context) showed ~66% (3-digit), 62% (4-digit), 80% (SF); Phi-3-mini-128K (100K context) ~27% (3-digit), 26% (4-digit), 37% (SF). An ablation changing write-key computation ('Larimar (no prefix)') degraded performance at 137K to 88% (3-digit), 14% (4-digit), 0% (SF).",
            "has_comparative_results": true,
            "performance_metric": "Recall / percent successful recall (success rate); Rouge-L used for San Francisco needle in needle-in-the-haystack (reported as Rouge-L recall)",
            "tradeoffs_reported": "Memory storage cost scales linearly with number of unique segments (O(N) memory rows); writing via pseudoinverse is O(K^3) in general but reduces to O(N) when keys are one-hot and unique; memory can be offloaded to CPU so GPU memory footprint does not grow with context length, but CPU–GPU transfers are required (encode to CPU for write, readout back to GPU). Using fixed-length prefixes improves matching but may reduce fidelity for complex needles. External memory growth trades space (CPU/RAM/disk) for ability to handle arbitrarily long contexts without increasing GPU memory.",
            "limitations_or_failure_cases": "Writes segments in isolation, losing cross-segment correlations and sequence order — so method is best when relevant information is contained within single segments; when number of unique segments grows with context length, memory size grows and retrieval costs grow; mismatch between query prefix and full segment encoding can cause retrieval of wrong (nearest) segment (noted for 4-digit needles and SF needle); no experiment shown with memory fully disabled in the same model for exact ablation of the memory effect.",
            "citation": "Nelson, E., Kollias, G., Das, P., Chaudhury, S., & Dan, S. (2024). Needle in the Haystack for Memory Based Large Language Models. arXiv:2407.01437v2 [cs.CL].",
            "uuid": "e6444.0",
            "source_info": {
                "paper_title": "Needle in the Haystack for Memory Based Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Kanerva Machine",
            "name_full": "The Kanerva Machine",
            "brief_description": "A distributed associative memory architecture referenced as structural inspiration for Larimar's external memory; stores and retrieves vector encodings in an associative memory.",
            "citation_title": "The kanerva machine: A generative distributed memory",
            "mention_or_use": "mention",
            "agent_name": "Kanerva Machine (memory architecture)",
            "agent_description": "An associative memory storing distributed vector representations that supports reading/writing of latent encodings; Larimar's external memory is described as being structured similarly to the Kanerva Machine.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Associative distributed memory",
            "memory_representation": "Latent vector encodings stored in memory matrix rows (distributed representations)",
            "memory_access_mechanism": "Associative read/write of encodings (nearest-neighbor key-based access in Larimar adaptation); in referenced work typically probabilistic/latent generative memory updates.",
            "task_name": null,
            "task_category": "Memory architecture (background)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Mentioned as inspiration; Larimar departs by using deterministic pseudoinverse least-squares updates rather than Kanerva Machine's multivariate Gaussian posterior updates.",
            "limitations_or_failure_cases": null,
            "citation": "Wu, Y., & Wayne, G. (2018). The kanerva machine: A generative distributed memory.",
            "uuid": "e6444.1",
            "source_info": {
                "paper_title": "Needle in the Haystack for Memory Based Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Generative Pseudoinverse Memory",
            "name_full": "Generative pseudoinverse memory (Pham et al., 2021)",
            "brief_description": "A memory update method that computes memory updates via a pseudoinverse/least-squares solution; Larimar uses the same least-squares solving approach for memory updates instead of probabilistic Gaussian updates.",
            "citation_title": "Generative pseudoinverse memory",
            "mention_or_use": "mention",
            "agent_name": "Generative pseudoinverse memory",
            "agent_description": "Memory update algorithm that finds the least-squares solution to minimize readout error (compute M = W† Z), providing deterministic read/write behavior; Larimar uses this update rule for its external memory writes.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Pseudoinverse linear associative memory update (least-squares)",
            "memory_representation": "Key and value matrices (W and Z); resulting memory matrix M stores encodings as rows",
            "memory_access_mechanism": "Write: compute pseudoinverse W† and solve M = W† Z to minimize ||Z − W M||^2; Read: linear readout using key vector w (z_read = w M).",
            "task_name": null,
            "task_category": "Memory algorithm / module",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Algorithmic write complexity can be O(K^3) for general W but simplifies to O(N) when keys are one-hot and unique; deterministic when variance terms are set to zero (σ_W = σ_ξ = 0 as in Larimar experiments).",
            "limitations_or_failure_cases": "Computational cost of pseudoinverse for large non-sparse W; reliance on choice of keys for efficient computation.",
            "citation": "Pham, K., Le, H., Ngo, M., Tran, T., Ho, B., & Venkatesh, S. (2021). Generative pseudoinverse memory. ICLR 2021.",
            "uuid": "e6444.2",
            "source_info": {
                "paper_title": "Needle in the Haystack for Memory Based Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Infini-Transformer / Infini-attention",
            "name_full": "Leave no context behind: Efficient infinite context transformers with infiniattention (Infini-Transformer)",
            "brief_description": "A transformer variant that retains memory of all past segments (infinite/very long context) via attention/memory mechanisms; cited as a baseline and comparison for long-context recall.",
            "citation_title": "Leave no context behind: Efficient infinite context transformers with infiniattention",
            "mention_or_use": "mention",
            "agent_name": "Infini-Transformer (Infini-attention)",
            "agent_description": "Transformer architecture that uses a memory of all past segments (rather than only the last segment) enabling very long-context processing; evaluated by its authors on passkey-like tests and compared here as a baseline.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Attention-based memory (retain memory of all past segments via modified attention)",
            "memory_representation": "Cached past-segment representations used as attention context",
            "memory_access_mechanism": "Attention over stored past-segment states (infini-attention variant); requires task-specific fine-tuning on long-context instances for best results per cited work.",
            "task_name": "Passkey test",
            "task_category": "Long-context retrieval / recall",
            "performance_with_memory": "Cited: With fine-tuning on the passkey test, Infini-attention achieved 100% recall up to 1M tokens; without fine-tuning, recall drops to ~O(10%) at 128K (cited from Munkhdalai et al., 2024).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Recall / Accuracy",
            "tradeoffs_reported": "Often requires task-specific fine-tuning on long-context instances and can incur increased compute/memory costs; without fine-tuning performance can degrade substantially.",
            "limitations_or_failure_cases": "Without fine-tuning, recall drops sharply at long contexts (reported ~10% at 128K).",
            "citation": "Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave no context behind: Efficient infinite context transformers with infiniattention.",
            "uuid": "e6444.3",
            "source_info": {
                "paper_title": "Needle in the Haystack for Memory Based Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RMT / RMT-Retrieval",
            "name_full": "Recurrent Memory Transformer (RMT) and RMT-Retrieval",
            "brief_description": "Memory-augmented transformer approaches (recurrent memory transformer and retrieval-augmented variants) previously shown to achieve near-perfect recall on long-context needle-in-the-haystack tasks after task-specific training.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "mention",
            "agent_name": "Recurrent Memory Transformer (RMT) / RMT-Retrieval",
            "agent_description": "Transformer variants that maintain recurrent memory (cache of past segments) across segments and/or use retrieval to locate needles; reported to achieve strong recall with task-specific training on needle-in-the-haystack tasks.",
            "model_size": "Example cited: GPT-2 (137M) when fine-tuned with RMT in prior work",
            "memory_used": true,
            "memory_type": "Recurrent memory / cached past-segment memory; retrieval-augmented variants",
            "memory_representation": "Cached past-segment representations or retrieval index entries (depending on variant)",
            "memory_access_mechanism": "Recurrent caching of segment representations and retrieval mechanisms; task-specific fine-tuning to use memory effectively.",
            "task_name": "Needle-in-the-haystack (long-context recall up to 1M tokens)",
            "task_category": "Long-context retrieval / recall",
            "performance_with_memory": "Cited: Near-perfect recall out to 1M tokens with task-specific training of GPT-2 (137M) in RMT / RMT-Retrieval studies (Bulatov et al., 2022; Kuratov et al., 2024).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Recall / Success rate",
            "tradeoffs_reported": "Achieves strong long-context recall but requires task-specific fine-tuning of base models; may rely on increased training on long-context instances.",
            "limitations_or_failure_cases": "Often requires task-specific training and may not generalize without such fine-tuning.",
            "citation": "Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer.",
            "uuid": "e6444.4",
            "source_info": {
                "paper_title": "Needle in the Haystack for Memory Based Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Larimar: Large language models with episodic memory control",
            "rating": 2,
            "sanitized_title": "larimar_large_language_models_with_episodic_memory_control"
        },
        {
            "paper_title": "Leave no context behind: Efficient infinite context transformers with infiniattention",
            "rating": 2,
            "sanitized_title": "leave_no_context_behind_efficient_infinite_context_transformers_with_infiniattention"
        },
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2,
            "sanitized_title": "recurrent_memory_transformer"
        },
        {
            "paper_title": "Generative pseudoinverse memory",
            "rating": 2,
            "sanitized_title": "generative_pseudoinverse_memory"
        },
        {
            "paper_title": "The kanerva machine: A generative distributed memory",
            "rating": 2,
            "sanitized_title": "the_kanerva_machine_a_generative_distributed_memory"
        },
        {
            "paper_title": "In search of needles in a 11m haystack: Recurrent memory finds what llms miss",
            "rating": 2,
            "sanitized_title": "in_search_of_needles_in_a_11m_haystack_recurrent_memory_finds_what_llms_miss"
        }
    ],
    "cost": 0.014300249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Needle in the Haystack for Memory Based Large Language Models
12 Jul 2024</p>
<p>Elliot Nelson 
IBM Research</p>
<p>Georgios Kollias 
IBM Research</p>
<p>Payel Das 
IBM Research</p>
<p>Subhajit Chaudhury 
IBM Research</p>
<p>Soham Dan 
IBM Research</p>
<p>Needle in the Haystack for Memory Based Large Language Models
12 Jul 20248B3289AB3BB803F8446631FEAAFAF4B7arXiv:2407.01437v2[cs.CL]
Current large language models (LLMs) often perform poorly on simple fact retrieval tasks.Here we investigate if coupling a dynamically adaptable external memory to a LLM can alleviate this problem.For this purpose, we test Larimar, a recently proposed language model architecture which uses an external associative memory, on long-context recall tasks including passkey and needle-in-the-haystack tests.We demonstrate that the external memory of Larimar, which allows fast write and read of an episode of text samples, can be used at test time to handle contexts much longer than those seen during training.We further show that the latent readouts from the memory (to which long contexts are written) control the decoder towards generating correct outputs, with the memory stored off of the GPU.Compared to existing transformer-based LLM architectures for longcontext recall tasks that use larger parameter counts or modified attention mechanisms, a relatively smaller size Larimar is able to maintain strong performance without any task-specific training or training on longer contexts.</p>
<p>Introduction</p>
<p>One of the most important abilities of large language models (LLMs) is retrieving information from the text input that is included in the prompt, which enables generating contextually grounded responses.The length of the context an LLM can process during inference is therefore an important factor controlling the generation quality.Vanilla transformer models suffer from quadratic memory and computation complexity with respect to the length of the input sequence due to the selfattention mechanism.Further, the global and local information get mixed and blurred while processing long context.Recent works have suggested different attention mechanisms in transformer-based LLM architectures to address such issues with longcontext modeling (Munkhdalai et al., 2024;Hwang et al., 2024;Beltagy et al., 2020;Dai et al., 2019;Bulatov et al., 2022).For example, transformers with Sliding Window Attention have been proposed that show O(L×W ) complexity for input length L and window size W (Beltagy et al., 2020).Memory of the past segments has been stored in a cache to be used as a context for the next segment (Dai et al., 2019) or models have been trained to learn explicit global memory tokens aka.soft prompts (Burtsev et al., 2021;Bulatov et al., 2022;Hwang et al., 2024).Recently, Infini-transformer (Munkhdalai et al., 2024) has been proposed to use memory of all past segments, as opposed to considering only the memory of the last segment in processing the current segment and discarding all other past segments.Those works often require task-specific training on long-context instances and lack generalization.</p>
<p>As an alternative, here we investigate test-time long-context adaptation of LARIMAR, a recently proposed (Das et al., 2024) LLM decoder model augmented with dynamically updatable memory mechanisms, and test its in-context recall performance on long-context modeling tasks.The external memory is structured similarly to the Kanerva Machine (Wu et al., 2018), but updates the memory by finding the least-squares solution to a linear system, following (Pham et al., 2021), instead of updating a multivariate Gaussian posterior distribution.While the model's training used relatively short contexts, we show that it can generalize to much longer contexts when only a small part of the context is task-relevant.This is because the external memory can be enlarged at test time to store and retrieve information from arbitrarily long contexts.Even if the context length vastly exceeds the training distribution, our model will generalize to the task as long as the memory readout (a single encoding) is in distribution as an input to the decoder.</p>
<p>In this paper, we make the following contributions:</p>
<p>• We introduce a method for writing long contexts to an external associative memory, with read/write memory operations that use a prefix or subset of the written text to ease retrieval when reading.</p>
<p>• We show that this method is able to perform long-context retrieval tasks at context length 100K-1M tokens, without any task-specific training, with the training context length limited to only 384 tokens, and with a relatively small 1.3B parameter model.</p>
<p>• By performing all memory operations on the CPU, we demonstrate the feasibility of scaling to longer contexts without increasing the GPU memory space footprint.</p>
<p>Background</p>
<p>In this section, we review our model architecture and describe memory operations used for longcontext recall tasks.</p>
<p>Notation.We use lower-case characters to denote individual key or value vectors: z ∈ R C , w ∈ R K , with latent embedding dimension C and memory size K, and upper-case for matrices of N ≥ 1 keys or values:
Z ∈ R N ×C , W ∈ R N ×K .</p>
<p>Larimar Architecture Overview</p>
<p>The language model (LM) employed here is Larimar (Das et al., 2024), which uses an encoderdecoder architecture (Li et al., 2020) trained together with a linear associative memory module (Pham et al., 2021).The LM encoder and decoder are trained to reconstruct an episode of input text samples as follows: The text is encoded in the latent space and written to the memory, the memory is queried, and the readout from the memory conditions the decoder (see (Das et al., 2024) for details).(Note that the decoder receives as input text a query about the context, but only accesses the full context -which is stored in the external memory -via the memory readout channel.)The loss objective used is a combination of a cross-entropy reconstruction loss and an auto-encoder loss.During inference, the context is divided into N segments, each of which are encoded and then written to memory.In our experiments, we use the following memory operations, differing slightly from (Das et al., 2024). 1riting.To write a segment of text to memory, we first compute its encoding z, along with a writing key vector w defined below.Given arrays of new encodings Z and corresponding key vectors W, the memory matrix
M ∈ R K×C is obtained as the solution 2 M = W † Z (1)
to the least-squares problem of minimizing the readout error ||Z − WM|| 2 2 (Kohonen and Ruohonen, 1973;Bau et al., 2020).</p>
<p>Reading.We assume the context ends in a final query (partial sentence), which is treated differently from the preceding context.Instead of writing to memory, the query is used to compute a reading key w, with which an encoding
z read = wM (2)
is read out from memory and passed as input to the decoder.</p>
<p>Reading and Writing Keys.The reading and writing keys used with a given encoding z are computed as a function w = f (z| M) of an encoding z (which may differ from the encoding z written to memory), conditional on a fixed "key memory" M ∈ R K×C (distinct from M) which is used exclusively to compute key vectors w.The encoding z can be obtained using a fixed-length prefix of the text to be written (when writing) or the query text (when reading), or alternatively can be the full text encoding z.Using a prefix can lead to reading and writing key vectors which are more similar. 3The function f is defined as follows.Given the encoding z, we select the nearest neighbor row in the key memory matrix,
k ⋆ (z) := argmin k ||z − Mk,: || 2 .
(3)</p>
<p>The corresponding one-hot key vector is
w k = 1(k = k ⋆ (z)),(4)
where 1(x) = 1 when x is True, and 0 otherwise.This ensures that the rows of M in Eq. ( 1) are simply the encodings Z, and that the memory readout vector in Eq. ( 2) is simply the k ⋆ (z)'th row of M, that is, z read = M k ⋆ (z),: .Lastly, we set the rows of the key memory M to the prefix encodings z of each in-context sentence.This ensures that, in the limit where the query prefix encoding z = zquery is very close to a particular prefix encoding used when writing to memory (e.g.z = zneedle ), the nearest neighbor row, k ⋆ (z) is in fact the row of M where the corresponding sentence (e.g.z = z needle ) was written.</p>
<p>In general, the time complexity of writing to memory is set by the O(K 3 ) cost of the pseudoinverse W † .However, when key vectors are one-hot, Eq. ( 4), and furthermore when the nearest neighbor locations k ⋆ (z) are unique for each segment encoding z (as in the case where these encodings populate the rows of M) and the memory size K is set to the O(N ) number of unique segments, then W is a permutation of the identity matrix.In this case, and with the O(N ) computations of key vectors running in parallel, the overall runtime of computing M (as well as reading from it) is O(N ).</p>
<p>Experiments</p>
<p>We conducted two experiments to evaluate longcontext recall.These experiments collectively involved O(10) GPU hours with an A100 40GB GPU.</p>
<p>Passkey test.</p>
<p>We test Larimar on the passkey test as defined in (Mohtashami and Jaggi, 2023). 4The context is divided into sentences, which are each written to memory, with the exception of the final prompt "The pass key is," which is fed into the decoder along with the memory readout.</p>
<p>In Table 1 we report the the average retrieval accuracy compared to the Zero-shot accuracy of Infini-Transformer (Munkhdalai et al., 2024).Importantly, because the context has only a small number of distinct sentences, regardless of the context length, only a small number of memory slots are used.All repeats of a given sentence are written to the same memory slot k ⋆ (z) in Eq. (3).Consequently, the memory readout and model generation are independent of the context length; while we 4 The exact context is as follows: "There is an important info hidden inside a lot of irrelevant text.Find it and memorize them.I will quiz you about the important information there.The grass is green.The sky is blue.The sun is yellow.Here we go.There and back again.(repeat x times) The pass key is 9054.Remember it.9054 is the pass key.The grass is green.The sky is blue.The sun is yellow.Here we go.There and ack again.(repeat y times) What is the pass key?The pass key is" only report up to 1M tokens in Table 1, the same results will hold for arbitrarily long contexts.(This will not hold for contexts where the number of unique text segments grows with context length.)While Table 1 assumes a single random passkey number, we also evaluated (at 1.2M tokens) the average recall rate over 100 random numbers with n digits, with results shown in Table 2.</p>
<p>Note that our method is invariant to the order in which sentences are written to memory, resulting in equivalent performance for any position of the passkey (or needle sentence, below) within the context.Needle-in-the-Haystack.</p>
<p>We follow (Kamradt, 2023), using the "haystack" dataset of Paul Graham essays, for which the total context length is ≈ 137K tokens.We test completion of needle sentences of the form "The magic number is X" with final prompt "The magic number is" following the context, as well as the "San Francisco" needle5 (Kamradt, 2023).</p>
<p>For Larimar, when computing key vectors, we used the encoding z of the four-word prefix of each sentence. 6We set the memory size K to the total number of sentences N , and write each encoded sentence to a unique memory slot using the key vector method of section 2.1.This incurs a memory storage cost that scales linearly with the context length, similar to (Kuratov et al., 2024).However, by keeping the external memory and read/write operations on the CPU (with encoding and memoryconditioned decoding on the GPU), we avoid an increased GPU memory cost and are able to handle much longer context lengths.</p>
<p>We compare to Mistral 7B v0.2 and Phi-3-Mini-128K-Instruct (3.8B parameters) as baseline methods for long-context recall.For the Mistral model, we use 1200-sentence (≈ 24K tokens) subsets of the full haystack dataset to evaluate the model at slightly less than its 32K token context limit.For the Phi-3 model, we use a 5000-sentence subset (≈ 100K tokens).We average over needle positions distributed uniformly throughout the context.</p>
<p>Our results are reported in Table 3, and show Larimar's ability to maintain strong recall at over 100K context tokens, while baseline models strug-gle with shorter contexts.For Larimar, note that the drop in recall from 3 to 4 digits reflects the increased difficulty of reconstructing the needle from the original encoding, rather than an increase in difficulty in locating the needle encoding in memory.The benefit of using a shorter, fixed-length prefix to compute the writing key is more significant for longer or more complex needle sentences (4-digit numbers, SF needle) in which case the query encoding may differ more from the full, untruncated needle encoding.(When the full needle encoding is too different from the query encoding, the reading key finds a different "haystack" sentence which is closer in the latent space.)</p>
<p>Discussion</p>
<p>Recent approaches to long-context retrieval have shown good performance after fine-tuning smaller models on needle-in-the-haystack tasks.RMT (Bulatov et al., 2022) and RMT-Retrieval (Kuratov et al., 2024) have shown near-perfect recall out to 1M tokens, with task-specific training of GPT2 (137M parameters).Infini-attention (Munkhdalai et al., 2024) with fine-tuning on the passkey test obtains 100% recall with context lengths up to 1M tokens.Without fine-tuning, however, recall drops to O(10%) at context length 128K (Table 1).</p>
<p>On the other hand, larger models have shown strong performance on needle-in-the-haystack tests without task-specific fine-tuning, but incur additional training and inference costs due to their larger size.</p>
<p>In comparison to these approaches, we aim for strong recall performance with a more compact model (1.3B parameters), without resorting to taskspecific training.Our model was trained on a generic text dataset using a subset of Wikipedia data, and can be adapted to novel context data during inference, with the reduced latency and inference costs of a smaller model (Das et al., 2024).</p>
<p>In our experiments, we allow the memory space to grow in proportion with the context length.While this increases space complexity, we emphasize that it does not increase the storage space needed on the GPU, since all memory operations can be performed on one or more CPUs.(A single long-context query requires (i) the encoded context to be moved to the CPU for writing to memory, and (ii) the memory readout to be moved back to the GPU for decoding, with the decoder input sequence length being ≲ O(100) tokens regardless of the context length.)Overall, we emphasize that the external memory size can be adjusted as needed depending on the task and context, with the memory-conditioned decoder training allowing the model to adapt to variable-sized contexts with unseen data.Deepening memory hierarchy in hardware by adding disk storage to CPU RAM can further expand available space and flexibility in offloading limited GPU memory (Sheng et al., 2023).</p>
<p>In the future, we aim to explore more general methods for computing reading and writing keys conditional on the full context, such that memory space can be dynamically allocated to context data that is more task-relevant and/or more surprising to the model, allowing for more predictable parts of the context to be stored in memory with correspondingly fewer bytes of information.</p>
<p>Limitations</p>
<p>An algorithmic limitation of our approach is that, after dividing the context into segments (e.g.sentences), each segment is written to memory in isolation, losing the information in cross-segment correlations and the sequence order of segments.Our method is thus most relevant for tasks where the relevant information is within individual segments.It could also be incorporated into more general architectures that extract context information in long-range correlations before writing to an external memory.</p>
<p>A Potential Risks</p>
<p>Our paper describes an approach to long-context recall for language models using an external memory.Language models with increased memory and recall capabilities introduce potential risks via misuse, and should only be deployed with appropriate guardrails.Our experiments only involved publicly available data without sensitive information, and we only applied our method on a relatively small 1.3B parameter model, with lower capability levels and potential for misuse compared to larger language models.</p>
<p>Table 1 :
1
(Munkhdalai et al., 2024)recall for Larimar 1.3B on the passkey test as defined in Appendix B of(Munkhdalai et al., 2024)
128K1MLarimar100/100/100 100/100/100Infini-Transformer (Linear)11/14/1008/6/98Infini-Transformer (Linear + δ) 6/9/997/6/97Passkey 1.2M3 digits 1004 digits 865 digits 876 digits 797 digits 598 digits 50</p>
<p>Table 2
2: Percentage of successful recall for Larimar1.3B on the passkey test with context length up to1,200,057, averaged over 100 random passkey numbersin each case.context 3 digits 4 digits SFLarimar137K0.950.641.0Larimar (no prefix) 137K0.880.140.0Mistral 7B v0.224K0.660.620.80Phi-3-mini-128K100K0.270.260.37Table 3: Percentage of successful recall for Larimar1.3B and baseline models on the needle-in-the-haystacktest. The "context" column indicates the context lengthused, with the remaining columns showing results fordifferent "needle" sentences. For 3 or 4 digit "magicnumber" needles, we average over 50 random numbers,checking the presence of the number within the modelresponse. For the "San Francisco" needle, we evaluatethe rougeL recall (length of the longest common sub-sequence of words, divided by the length of the targetcompletion). For Larimar, we either truncate each sen-tence to a fixed-length prefix when computing writingkeys, or do not ("no prefix").
 enelson@ibm.com <br />
gkollias@us.ibm.com
daspa@us.ibm.com
 subhajit@ibm.com <br />
soham.dan@ibm.com
We also set σ W = σ ξ = 0 as defined in(Das et al.,<br />
), resulting in deterministic read and write operations.2 W † indicates the pseudoinverse of the non-square matrix W.3 For instance, if the written text is "The pass key is 732" and the query text is "The pass key is", using the same threeor four-word prefix of each will result in equivalent reading and writing keys.
The needle text is "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." After writing the context to memory we use the prompt "'The best thing to do in San Francisco is ".
Or the full sentence, if it was shorter.</p>
<p>Rewriting a deep generative model. David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, arXiv:2007.156462020Preprint</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020Preprint</p>
<p>Recurrent memory transformer. Aydar Bulatov, Yury Kuratov, Mikhail Burtsev, Advances in Neural Information Processing Systems. 202235</p>
<p>. Mikhail S Burtsev, Yuri Kuratov, Anton Peganov, Grigory V Sapunov, arXiv:2006.115272021Memory transformer. Preprint</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov, arXiv:1901.028602019Preprint</p>
<p>Larimar: Large language models with episodic memory control. Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Soham Navrátil, Pin-Yu Dan, Chen, arXiv:2403.119012024Preprint</p>
<p>Transformerfam: Feedback attention is working memory. Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, Pedro Moreno Mengibar, arXiv:2404.091732024Preprint</p>
<p>Needle In A Haystack -pressure testing LLMs. Gregory Kamradt, 2023Github</p>
<p>Representation of associated data by matrix operators. Teuvo Kohonen, Matti Ruohonen, 10.1109/TC.1973.5009138IEEE Trans. Comput. 2271973</p>
<p>In search of needles in a 11m haystack: Recurrent memory finds what llms miss. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev, arXiv:2402.107902024Preprint</p>
<p>Optimus: Organizing sentences via pre-trained modeling of a latent space. Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Landmark attention: Random-access infinite context length for transformers. Amirkeivan Mohtashami, Martin Jaggi, arXiv:2305.163002023Preprint</p>
<p>Leave no context behind: Efficient infinite context transformers with infiniattention. Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal, arXiv:2404.071432024Preprint</p>
<p>Generative pseudoinverse memory. Kha Pham, Hung Le, Man Ngo, Truyen Tran, Bao Ho, Svetha Venkatesh, International Conference on Learning Representations. 2021</p>
<p>Flexgen: high-throughput generative inference of large language models with a single gpu. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023Christopher Ré, Ion Stoica, and Ce Zhang</p>
<p>The kanerva machine: A generative distributed memory. Yan Wu, Greg Wayne, arXiv:1804.017562018PreprintAlex Graves, and Timothy Lillicrap</p>            </div>
        </div>

    </div>
</body>
</html>
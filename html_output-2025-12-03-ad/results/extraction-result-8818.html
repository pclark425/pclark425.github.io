<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8818 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8818</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8818</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a4c40532e68728fbeab5d9415f6ad8e9530db360</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4c40532e68728fbeab5d9415f6ad8e9530db360" target="_blank">The WebNLG Challenge: Generating Text from RDF Data</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> The microplanning task is introduced, data preparation, evaluation methodology, participant results and a brief description of the participating systems are provided.</p>
                <p><strong>Paper Abstract:</strong> The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare “microplanners”, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8818.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8818.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearised+Delex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearisation of RDF triples with delexicalisation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert a set of RDF triples into a linear token sequence (a serialization) and replace entity surface forms by abstract placeholders (delexicalisation) before training a seq2seq model; relexicalisation is applied as post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (delexicalised)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples are serialized into a single sequence by concatenating subject-predicate-object tuples (often with explicit tuple-separator tokens). Entities and/or objects are replaced by placeholders (e.g., category tokens or property-based placeholders) using exact matching; the sequence is used as the encoder input to an encoder-decoder model. After generation the placeholders are re-substituted by original entity strings (relexicalisation).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (knowledge graph triples extracted from DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Set of triples -> ordered list (linearization). Delexicalisation performed via exact string matching: subjects replaced by their category token (e.g., Alan_Bean -> ASTRONAUT), objects often replaced by placeholder tokens (e.g., COUNTRY). Tuple separation special tokens may be inserted. Ordering heuristics (e.g., discourse-preserving ordering) are optionally applied before linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning (verbalising RDF triples as natural language text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline system (vanilla seq2seq with attention trained on linearised+delex data): BLEU = 33.24 (global), TER = 0.61, METEOR = 0.23. Variants using similar linearised+delex pipelines: UTilburg-NMT global BLEU = 34.60; UTilburg-Pipeline/UTILBURG-related pipeline systems show other scores (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to approaches that keep entities lexicalised (e.g., ADAPT subword approach) this representation simplifies surface form generation and reduces vocabulary size; compared to graph-structure representations (PredArg graph) it is simpler but may lose explicit relation between triples. Systems using delexicalisation (SMT and NMT) generally outperform naïve pipeline rule systems on seen data, though pipeline and graph-based template systems can be more robust on unseen properties.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces vocabulary sparsity and rare-word issues; straightforward to feed into sequence models; enables use of standard seq2seq toolkits and relexicalisation restores original entity forms; strong baseline performance on seen data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on exact matching for delexicalisation (some entities not replaced if no exact match); can lose structural information present in graph (ordering may be arbitrary); requires correct post hoc relexicalisation; limited generalisation to unseen properties if placeholders don't capture needed lexical cues.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor relexicalisation when generated output omits placeholders or generates altered placeholders; lower performance on unseen categories/properties when missing lexical cues (baseline unseen BLEU reported as low as 6.13 in the paper for some systems using delexicalisation without adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8818.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SubwordLinear</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword-linearized representation (no delexicalisation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Linearize triple input and represent tokens at subword level (BPE/subword) to avoid delexicalisation and better handle rare/new entity surface forms during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization + subword tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearization of RDF triples into a sequence with special tuple separators, but instead of replacing entity strings with placeholders, the system applies subword segmentation (subword units) to both input and output tokens so that rare words/entities are represented as sequences of subword units and can be generated without explicit placeholders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (DBpedia-derived knowledge graph triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Triples are serialized into a sequence with tuple separators; input and output text are segmented into subword units (e.g., BPE) and fed into an attention-based encoder-decoder (Nematus).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ADAPTCentre (Nematus, subword, no delexicalisation, tuple separators): global BLEU = 31.06, TER = 0.84, METEOR = 0.31. Seen-categories performance strong (ranked highly on seen data in the paper), but poor on unseen categories (unseen BLEU = 10.53).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to delexicalisation+linearization, the subword approach avoids a separate relexicalisation step and can generate novel surface forms; however, in the WebNLG results it generalised poorly to unseen properties/categories compared to grammar/template-based systems (UPF-FORGE) and some delexicalised SMT/NMT systems.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Removes need for handcrafted placeholder mapping and relexicalisation; handles rare tokens and morphological variants via subword units; straightforward integration with NMT toolkits.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Subword models may still struggle to lexicalise unseen properties that require new lexicalisations (predicates), and can produce incorrect entity forms; observed decreased robustness on unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Did not generalise well to unseen categories/properties in the test set (noted by large performance drop on unseen data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8818.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntityID+Type</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalisation with Entity IDs and DBpedia type enrichment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entities are delexicalised to an ENTITY-ID token and enriched with their DBpedia type when available before linearisation and encoding; aims to preserve type information to help generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>entity-ID delexicalisation + type enrichment</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity in the input is replaced by an identifier token (ENTITY-ID); when available, the DBpedia type of that entity is appended to the identifier to give the model additional type/context information. N-gram matching is used to ensure accurate delexicalisation of the target side.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / DBpedia knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Delexicalise entities in input and corresponding referring expressions in target using ENTITY-ID tokens; attach DBpedia type to entity token; linearize triples and feed into encoder-decoder with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (microplanning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UMelbourne (delex with entity-ID + type enrichment): global BLEU = 45.13, TER = 0.47, METEOR = 0.37; seen categories BLEU = 54.52 (paper table 6). Unseen categories BLEU = 33.27 (table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed many other systems overall (highest global BLEU in the challenge), showing that enriching delexicalised tokens with type information improves generation compared to plain delexicalisation or subword-only approaches; less robust than UPF-FORGE on unseen categories but stronger on seen.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves useful semantic/type information that aids lexicalisation while still reducing sparsity; strong empirical performance on seen test data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Depends on availability and accuracy of DBpedia type information; still suffers performance degradation on unseen categories/properties (but less so than pure subword/no-delex approach).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When DBpedia type is missing or wrong, enrichment may be unavailable or misleading; still fails to fully generalise to novel predicates in unseen test set (performance drop observed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8818.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PredArg+FORGe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicate-Argument (PredArg) graph representation with FORGe generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert RDF triples to linguistic predicate-argument structures (PredArg templates) which are aggregated into a PredArg graph; a linguistic generator (FORGe) takes this graph and produces fluent text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PredArg graph (predicate-argument templates)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each DBpedia property, manually defined PredArg templates encode DBpedia-specific and linguistic features mapping triples to predicate-argument structures; multiple PredArg structures are aggregated into a PredArg graph which is input to the FORGe generator (sequence of graph transducers) to produce surface text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples converted into predicate-argument graph structures (linguistic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Manual mapping from DBpedia properties to PredArg templates -> convert triples to PredArg structures -> aggregate PredArg structures into a PredArg graph -> generate text with the FORGe generator (graph-transducer-based pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning (verbalising RDF content)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UPF-FORGE (PredArg + FORGe): global BLEU = 38.65, METEOR = 0.39 (highest METEOR); unseen categories BLEU = 35.70 (ranked 1st on unseen), TER (unseen) = 0.55. Seen-categories METEOR ranking also high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed many neural and SMT systems on unseen categories (best BLEU on unseen) and obtained top METEOR overall, showing stronger generalisation to properties not seen in training; compared to purely data-driven linearization+NMT it is more adaptable to unseen predicates but requires manual template creation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Robust generalisation to unseen properties and categories; generates morpho-syntactically and lexical-variational outputs yielding high METEOR; explicit linguistic structure aids handling novel predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires manual construction of PredArg templates per property (labor-intensive and not fully scalable); engineering effort for mapping and templates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Scalability limitations when porting to many new properties/domains due to manual template creation; potential coverage gaps for properties without templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8818.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TypedDepRules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Typed-dependency-based rule/template extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extract generation rules from the typed-dependency parses of training texts, mapping dependency patterns to templates for surface realisation from triples; at runtime use lexical similarity (WordNet) to match predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>typed-dependency template extraction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of delexicalising, the system uses dependency parses of reference texts to extract templates or rules that map dependency templates to triple realizations. At runtime WordNet-based similarity is used to match predicates between train and test.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (DBpedia) paired with dependency structures of reference texts</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>From training data, collect typed-dependency structures aligned with triples to form extraction rules/templates; at generation time, match test predicates to train predicates using WordNet similarity and apply extracted templates to produce surface text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UIT-VNU-HCM pipeline: global BLEU = 7.07, TER = 0.82, METEOR = 0.09; unseen categories BLEU = 0.11 (very low).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared unfavourably to learned SMT/NMT and PredArg methods in this challenge (much lower BLEU/METEOR), suggesting that the typed-dependency template extraction used here did not capture sufficient generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Tries to exploit syntactic structure from training texts and avoid delexicalisation; uses lexical similarity measures (WordNet) to handle predicate mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Poor empirical performance in the challenge; likely brittle matching between observed dependency templates and novel test inputs; low robustness on unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Extremely low scores on unseen categories and overall poor BLEU/METEOR, indicating failure to generalise from extracted dependency templates to diverse test inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8818.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMT+EntityID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical Machine Translation with delexicalisation and entity-ID annotation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Moses-based SMT approach where inputs/outputs are delexicalised, entity mentions on both sides annotated with Wikipedia IDs, alignments learned with MGIZA, tuned with MIRA, and decoded with a KenLM language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMT on linearised delexicalised triples with entity-ID annotation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Preprocess training data by delexicalising entities and annotating them with their Wikipedia IDs; linearize (serialize) triple sets into sequences; train phrase-based SMT (Moses) with MGIZA alignments and tune weights with batch MIRA using BLEU; use large n-gram LM (6-gram KenLM) for decoding and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Delexicalise input triples and output referring expressions, annotate entity tokens with Wikipedia IDs, obtain alignments with MGIZA, train Moses SMT model on serialized sequences, decode and relexicalise.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (microplanning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UTilburg-SMT: global BLEU = 44.28, TER = 0.53, METEOR = 0.38; seen categories BLEU = 54.29; unseen categories BLEU = 29.88.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>SMT performed competitively with best neural and pipeline systems on seen data (one of the top BLEU scores), outperforming some NMT systems globally; however on unseen categories it fell behind template/grammar-based UPF-FORGE.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Strong empirical performance on seen domains; benefits from entity-ID annotation and strong n-gram LM for fluency; interpretable phrase tables and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still suffers performance drop on unseen properties/categories relative to grammar-based adaptation; requires careful preprocessing and alignment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance drops on unseen categories (BLEU lower than UPF-FORGE), showing limited generalisation to novel predicates not observed in training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8818.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PredTemplateRules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template/rule extraction mapping triples to delexicalised texts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extract rules or templates from training data that map single triples or triple-sets to delexicalised text fragments; at generation time apply rules, order triples for discourse, apply referring expression generation and rank outputs with an n-gram LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>template-based mapping (delexicalised triple->text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>From training pairs extract templates/rules mapping (possibly sets of) triples to delexicalised surface text fragments. At runtime order triples to preserve discourse order, apply extracted templates to generate delexicalised text, add missing entities using a GRE module, and rank candidate outputs using an n-gram language model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Rule/template extraction from (delexicalised) training data; triple ordering; template application; referring expression generation for missing entities; language model ranking (6-gram Gigaword) to select final output.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UTILBURG-PIPELINE: global BLEU = 35.29, TER = 0.56, METEOR = 0.30; seen BLEU = 44.34; unseen BLEU = 20.65.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performed worse than top SMT and top NMT systems on seen data, but better than some systems on certain metrics; not as robust on unseen categories as UPF-FORGE.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Interpretable mapping rules; can directly reuse observed surface realizations; GRE module can add missing entities deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Limited coverage for unseen combinations and properties; dependent on extraction quality and delexicalisation match.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower BLEU/METEOR relative to best systems on global evaluation and especially on unseen categories where template coverage is lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8818.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8818.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble+Rank+RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembled NMT with supervised ranking and reinforcement learning objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ensemble multiple attention-based encoder-decoder models; apply a learned ranker to choose best verbalisation among model outputs and incorporate an RL objective to encourage including input subjects in outputs; use hand-crafted rules for failure cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>ensemble NMT outputs + supervised ranker + RL fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train multiple seq2seq (attention) models (ensemble of 7), generate candidate verbalizations, score candidate-output pairs with a supervised ranker trained on features (BLEU-based quality labels) to pick best output; augment model training with an RL reward encouraging outputs to include input subjects; fallback hand-crafted rules for certain failure cases.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples (DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Input triple sets are linearised/delexicalised; ensemble models produce multiple candidate texts; a supervised ranker (trained on automatically scored examples using BLEU against references) selects the best candidate; RL objective used during model training to bias outputs toward including input items.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / microplanning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PKUWRITER: global BLEU = 39.88, TER = 0.55, METEOR = 0.31; seen BLEU = 51.23; unseen BLEU = 25.36.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Improves over single-model NMT baselines by leveraging ensemble diversity and ranking, but did not surpass the very top system (UMelbourne) in global BLEU; better on seen than unseen.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Ranker helps select higher-quality outputs among candidates; RL objective can encourage faithfulness to input (e.g., inclusion of subjects); ensemble gives diverse outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex training and generation pipeline; additional supervised data needed to train ranker (automatically generated labels); RL objective design and hand-crafted fallbacks indicate remaining failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Relies on ranker quality — if candidate pool lacks correct wording or placeholders are missing, ranking cannot recover; hand-crafted rules required to handle certain cases where models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Creating training corpora for nlg micro-planners <em>(Rating: 2)</em></li>
                <li>Building rdf content for data-to-text generation <em>(Rating: 2)</em></li>
                <li>FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers <em>(Rating: 2)</em></li>
                <li>Split and rephrase <em>(Rating: 2)</em></li>
                <li>Multi-domain neural network language generation for spoken dialogue systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8818",
    "paper_id": "paper-a4c40532e68728fbeab5d9415f6ad8e9530db360",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Linearised+Delex",
            "name_full": "Linearisation of RDF triples with delexicalisation",
            "brief_description": "Convert a set of RDF triples into a linear token sequence (a serialization) and replace entity surface forms by abstract placeholders (delexicalisation) before training a seq2seq model; relexicalisation is applied as post-processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization (delexicalised)",
            "representation_description": "Triples are serialized into a single sequence by concatenating subject-predicate-object tuples (often with explicit tuple-separator tokens). Entities and/or objects are replaced by placeholders (e.g., category tokens or property-based placeholders) using exact matching; the sequence is used as the encoder input to an encoder-decoder model. After generation the placeholders are re-substituted by original entity strings (relexicalisation).",
            "graph_type": "RDF triples (knowledge graph triples extracted from DBpedia)",
            "conversion_method": "Set of triples -&gt; ordered list (linearization). Delexicalisation performed via exact string matching: subjects replaced by their category token (e.g., Alan_Bean -&gt; ASTRONAUT), objects often replaced by placeholder tokens (e.g., COUNTRY). Tuple separation special tokens may be inserted. Ordering heuristics (e.g., discourse-preserving ordering) are optionally applied before linearization.",
            "downstream_task": "Data-to-text generation / microplanning (verbalising RDF triples as natural language text)",
            "performance_metrics": "Baseline system (vanilla seq2seq with attention trained on linearised+delex data): BLEU = 33.24 (global), TER = 0.61, METEOR = 0.23. Variants using similar linearised+delex pipelines: UTilburg-NMT global BLEU = 34.60; UTilburg-Pipeline/UTILBURG-related pipeline systems show other scores (see paper tables).",
            "comparison_to_others": "Compared to approaches that keep entities lexicalised (e.g., ADAPT subword approach) this representation simplifies surface form generation and reduces vocabulary size; compared to graph-structure representations (PredArg graph) it is simpler but may lose explicit relation between triples. Systems using delexicalisation (SMT and NMT) generally outperform naïve pipeline rule systems on seen data, though pipeline and graph-based template systems can be more robust on unseen properties.",
            "advantages": "Reduces vocabulary sparsity and rare-word issues; straightforward to feed into sequence models; enables use of standard seq2seq toolkits and relexicalisation restores original entity forms; strong baseline performance on seen data.",
            "disadvantages": "Relies on exact matching for delexicalisation (some entities not replaced if no exact match); can lose structural information present in graph (ordering may be arbitrary); requires correct post hoc relexicalisation; limited generalisation to unseen properties if placeholders don't capture needed lexical cues.",
            "failure_cases": "Poor relexicalisation when generated output omits placeholders or generates altered placeholders; lower performance on unseen categories/properties when missing lexical cues (baseline unseen BLEU reported as low as 6.13 in the paper for some systems using delexicalisation without adaptation).",
            "uuid": "e8818.0",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "SubwordLinear",
            "name_full": "Subword-linearized representation (no delexicalisation)",
            "brief_description": "Linearize triple input and represent tokens at subword level (BPE/subword) to avoid delexicalisation and better handle rare/new entity surface forms during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization + subword tokenization",
            "representation_description": "Linearization of RDF triples into a sequence with special tuple separators, but instead of replacing entity strings with placeholders, the system applies subword segmentation (subword units) to both input and output tokens so that rare words/entities are represented as sequences of subword units and can be generated without explicit placeholders.",
            "graph_type": "RDF triples (DBpedia-derived knowledge graph triples)",
            "conversion_method": "Triples are serialized into a sequence with tuple separators; input and output text are segmented into subword units (e.g., BPE) and fed into an attention-based encoder-decoder (Nematus).",
            "downstream_task": "Data-to-text generation / microplanning",
            "performance_metrics": "ADAPTCentre (Nematus, subword, no delexicalisation, tuple separators): global BLEU = 31.06, TER = 0.84, METEOR = 0.31. Seen-categories performance strong (ranked highly on seen data in the paper), but poor on unseen categories (unseen BLEU = 10.53).",
            "comparison_to_others": "Compared to delexicalisation+linearization, the subword approach avoids a separate relexicalisation step and can generate novel surface forms; however, in the WebNLG results it generalised poorly to unseen properties/categories compared to grammar/template-based systems (UPF-FORGE) and some delexicalised SMT/NMT systems.",
            "advantages": "Removes need for handcrafted placeholder mapping and relexicalisation; handles rare tokens and morphological variants via subword units; straightforward integration with NMT toolkits.",
            "disadvantages": "Subword models may still struggle to lexicalise unseen properties that require new lexicalisations (predicates), and can produce incorrect entity forms; observed decreased robustness on unseen categories.",
            "failure_cases": "Did not generalise well to unseen categories/properties in the test set (noted by large performance drop on unseen data).",
            "uuid": "e8818.1",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "EntityID+Type",
            "name_full": "Delexicalisation with Entity IDs and DBpedia type enrichment",
            "brief_description": "Entities are delexicalised to an ENTITY-ID token and enriched with their DBpedia type when available before linearisation and encoding; aims to preserve type information to help generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "entity-ID delexicalisation + type enrichment",
            "representation_description": "Each entity in the input is replaced by an identifier token (ENTITY-ID); when available, the DBpedia type of that entity is appended to the identifier to give the model additional type/context information. N-gram matching is used to ensure accurate delexicalisation of the target side.",
            "graph_type": "RDF triples / DBpedia knowledge graph",
            "conversion_method": "Delexicalise entities in input and corresponding referring expressions in target using ENTITY-ID tokens; attach DBpedia type to entity token; linearize triples and feed into encoder-decoder with attention.",
            "downstream_task": "Data-to-text generation (microplanning)",
            "performance_metrics": "UMelbourne (delex with entity-ID + type enrichment): global BLEU = 45.13, TER = 0.47, METEOR = 0.37; seen categories BLEU = 54.52 (paper table 6). Unseen categories BLEU = 33.27 (table 7).",
            "comparison_to_others": "Outperformed many other systems overall (highest global BLEU in the challenge), showing that enriching delexicalised tokens with type information improves generation compared to plain delexicalisation or subword-only approaches; less robust than UPF-FORGE on unseen categories but stronger on seen.",
            "advantages": "Preserves useful semantic/type information that aids lexicalisation while still reducing sparsity; strong empirical performance on seen test data.",
            "disadvantages": "Depends on availability and accuracy of DBpedia type information; still suffers performance degradation on unseen categories/properties (but less so than pure subword/no-delex approach).",
            "failure_cases": "When DBpedia type is missing or wrong, enrichment may be unavailable or misleading; still fails to fully generalise to novel predicates in unseen test set (performance drop observed).",
            "uuid": "e8818.2",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "PredArg+FORGe",
            "name_full": "Predicate-Argument (PredArg) graph representation with FORGe generator",
            "brief_description": "Convert RDF triples to linguistic predicate-argument structures (PredArg templates) which are aggregated into a PredArg graph; a linguistic generator (FORGe) takes this graph and produces fluent text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "PredArg graph (predicate-argument templates)",
            "representation_description": "For each DBpedia property, manually defined PredArg templates encode DBpedia-specific and linguistic features mapping triples to predicate-argument structures; multiple PredArg structures are aggregated into a PredArg graph which is input to the FORGe generator (sequence of graph transducers) to produce surface text.",
            "graph_type": "RDF triples converted into predicate-argument graph structures (linguistic graphs)",
            "conversion_method": "Manual mapping from DBpedia properties to PredArg templates -&gt; convert triples to PredArg structures -&gt; aggregate PredArg structures into a PredArg graph -&gt; generate text with the FORGe generator (graph-transducer-based pipeline).",
            "downstream_task": "Data-to-text generation / microplanning (verbalising RDF content)",
            "performance_metrics": "UPF-FORGE (PredArg + FORGe): global BLEU = 38.65, METEOR = 0.39 (highest METEOR); unseen categories BLEU = 35.70 (ranked 1st on unseen), TER (unseen) = 0.55. Seen-categories METEOR ranking also high.",
            "comparison_to_others": "Outperformed many neural and SMT systems on unseen categories (best BLEU on unseen) and obtained top METEOR overall, showing stronger generalisation to properties not seen in training; compared to purely data-driven linearization+NMT it is more adaptable to unseen predicates but requires manual template creation.",
            "advantages": "Robust generalisation to unseen properties and categories; generates morpho-syntactically and lexical-variational outputs yielding high METEOR; explicit linguistic structure aids handling novel predicates.",
            "disadvantages": "Requires manual construction of PredArg templates per property (labor-intensive and not fully scalable); engineering effort for mapping and templates.",
            "failure_cases": "Scalability limitations when porting to many new properties/domains due to manual template creation; potential coverage gaps for properties without templates.",
            "uuid": "e8818.3",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "TypedDepRules",
            "name_full": "Typed-dependency-based rule/template extraction",
            "brief_description": "Extract generation rules from the typed-dependency parses of training texts, mapping dependency patterns to templates for surface realisation from triples; at runtime use lexical similarity (WordNet) to match predicates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "typed-dependency template extraction",
            "representation_description": "Instead of delexicalising, the system uses dependency parses of reference texts to extract templates or rules that map dependency templates to triple realizations. At runtime WordNet-based similarity is used to match predicates between train and test.",
            "graph_type": "RDF triples (DBpedia) paired with dependency structures of reference texts",
            "conversion_method": "From training data, collect typed-dependency structures aligned with triples to form extraction rules/templates; at generation time, match test predicates to train predicates using WordNet similarity and apply extracted templates to produce surface text.",
            "downstream_task": "Data-to-text generation / microplanning",
            "performance_metrics": "UIT-VNU-HCM pipeline: global BLEU = 7.07, TER = 0.82, METEOR = 0.09; unseen categories BLEU = 0.11 (very low).",
            "comparison_to_others": "Compared unfavourably to learned SMT/NMT and PredArg methods in this challenge (much lower BLEU/METEOR), suggesting that the typed-dependency template extraction used here did not capture sufficient generalisation.",
            "advantages": "Tries to exploit syntactic structure from training texts and avoid delexicalisation; uses lexical similarity measures (WordNet) to handle predicate mismatches.",
            "disadvantages": "Poor empirical performance in the challenge; likely brittle matching between observed dependency templates and novel test inputs; low robustness on unseen categories.",
            "failure_cases": "Extremely low scores on unseen categories and overall poor BLEU/METEOR, indicating failure to generalise from extracted dependency templates to diverse test inputs.",
            "uuid": "e8818.4",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "SMT+EntityID",
            "name_full": "Statistical Machine Translation with delexicalisation and entity-ID annotation",
            "brief_description": "A Moses-based SMT approach where inputs/outputs are delexicalised, entity mentions on both sides annotated with Wikipedia IDs, alignments learned with MGIZA, tuned with MIRA, and decoded with a KenLM language model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "SMT on linearised delexicalised triples with entity-ID annotation",
            "representation_description": "Preprocess training data by delexicalising entities and annotating them with their Wikipedia IDs; linearize (serialize) triple sets into sequences; train phrase-based SMT (Moses) with MGIZA alignments and tune weights with batch MIRA using BLEU; use large n-gram LM (6-gram KenLM) for decoding and ranking.",
            "graph_type": "RDF triples (DBpedia)",
            "conversion_method": "Delexicalise input triples and output referring expressions, annotate entity tokens with Wikipedia IDs, obtain alignments with MGIZA, train Moses SMT model on serialized sequences, decode and relexicalise.",
            "downstream_task": "Data-to-text generation (microplanning)",
            "performance_metrics": "UTilburg-SMT: global BLEU = 44.28, TER = 0.53, METEOR = 0.38; seen categories BLEU = 54.29; unseen categories BLEU = 29.88.",
            "comparison_to_others": "SMT performed competitively with best neural and pipeline systems on seen data (one of the top BLEU scores), outperforming some NMT systems globally; however on unseen categories it fell behind template/grammar-based UPF-FORGE.",
            "advantages": "Strong empirical performance on seen domains; benefits from entity-ID annotation and strong n-gram LM for fluency; interpretable phrase tables and alignment.",
            "disadvantages": "Still suffers performance drop on unseen properties/categories relative to grammar-based adaptation; requires careful preprocessing and alignment steps.",
            "failure_cases": "Performance drops on unseen categories (BLEU lower than UPF-FORGE), showing limited generalisation to novel predicates not observed in training.",
            "uuid": "e8818.5",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "PredTemplateRules",
            "name_full": "Template/rule extraction mapping triples to delexicalised texts",
            "brief_description": "Extract rules or templates from training data that map single triples or triple-sets to delexicalised text fragments; at generation time apply rules, order triples for discourse, apply referring expression generation and rank outputs with an n-gram LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "template-based mapping (delexicalised triple-&gt;text)",
            "representation_description": "From training pairs extract templates/rules mapping (possibly sets of) triples to delexicalised surface text fragments. At runtime order triples to preserve discourse order, apply extracted templates to generate delexicalised text, add missing entities using a GRE module, and rank candidate outputs using an n-gram language model.",
            "graph_type": "RDF triples (DBpedia)",
            "conversion_method": "Rule/template extraction from (delexicalised) training data; triple ordering; template application; referring expression generation for missing entities; language model ranking (6-gram Gigaword) to select final output.",
            "downstream_task": "Data-to-text generation / microplanning",
            "performance_metrics": "UTILBURG-PIPELINE: global BLEU = 35.29, TER = 0.56, METEOR = 0.30; seen BLEU = 44.34; unseen BLEU = 20.65.",
            "comparison_to_others": "Performed worse than top SMT and top NMT systems on seen data, but better than some systems on certain metrics; not as robust on unseen categories as UPF-FORGE.",
            "advantages": "Interpretable mapping rules; can directly reuse observed surface realizations; GRE module can add missing entities deterministically.",
            "disadvantages": "Limited coverage for unseen combinations and properties; dependent on extraction quality and delexicalisation match.",
            "failure_cases": "Lower BLEU/METEOR relative to best systems on global evaluation and especially on unseen categories where template coverage is lacking.",
            "uuid": "e8818.6",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Ensemble+Rank+RL",
            "name_full": "Ensembled NMT with supervised ranking and reinforcement learning objective",
            "brief_description": "Ensemble multiple attention-based encoder-decoder models; apply a learned ranker to choose best verbalisation among model outputs and incorporate an RL objective to encourage including input subjects in outputs; use hand-crafted rules for failure cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "ensemble NMT outputs + supervised ranker + RL fine-tuning",
            "representation_description": "Train multiple seq2seq (attention) models (ensemble of 7), generate candidate verbalizations, score candidate-output pairs with a supervised ranker trained on features (BLEU-based quality labels) to pick best output; augment model training with an RL reward encouraging outputs to include input subjects; fallback hand-crafted rules for certain failure cases.",
            "graph_type": "RDF triples (DBpedia)",
            "conversion_method": "Input triple sets are linearised/delexicalised; ensemble models produce multiple candidate texts; a supervised ranker (trained on automatically scored examples using BLEU against references) selects the best candidate; RL objective used during model training to bias outputs toward including input items.",
            "downstream_task": "Data-to-text generation / microplanning",
            "performance_metrics": "PKUWRITER: global BLEU = 39.88, TER = 0.55, METEOR = 0.31; seen BLEU = 51.23; unseen BLEU = 25.36.",
            "comparison_to_others": "Improves over single-model NMT baselines by leveraging ensemble diversity and ranking, but did not surpass the very top system (UMelbourne) in global BLEU; better on seen than unseen.",
            "advantages": "Ranker helps select higher-quality outputs among candidates; RL objective can encourage faithfulness to input (e.g., inclusion of subjects); ensemble gives diverse outputs.",
            "disadvantages": "More complex training and generation pipeline; additional supervised data needed to train ranker (automatically generated labels); RL objective design and hand-crafted fallbacks indicate remaining failure modes.",
            "failure_cases": "Relies on ranker quality — if candidate pool lacks correct wording or placeholders are missing, ranking cannot recover; hand-crafted rules required to handle certain cases where models fail.",
            "uuid": "e8818.7",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Creating training corpora for nlg micro-planners",
            "rating": 2,
            "sanitized_title": "creating_training_corpora_for_nlg_microplanners"
        },
        {
            "paper_title": "Building rdf content for data-to-text generation",
            "rating": 2,
            "sanitized_title": "building_rdf_content_for_datatotext_generation"
        },
        {
            "paper_title": "FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers",
            "rating": 2,
            "sanitized_title": "forge_at_semeval2017_task_9_deep_sentence_generation_based_on_a_sequence_of_graph_transducers"
        },
        {
            "paper_title": "Split and rephrase",
            "rating": 2,
            "sanitized_title": "split_and_rephrase"
        },
        {
            "paper_title": "Multi-domain neural network language generation for spoken dialogue systems",
            "rating": 1,
            "sanitized_title": "multidomain_neural_network_language_generation_for_spoken_dialogue_systems"
        }
    ],
    "cost": 0.016648,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The WebNLG Challenge: Generating Text from RDF Data</h1>
<p>Claire Gardent Anastasia Shimorina<br>CNRS, LORIA, UMR 7503<br>Vandoeuvre-lès-Nancy, F-54500, France<br>claire.gardent@loria.fr<br>anastasia.shimorina@loria.fr</p>
<p>Shashi Narayan Laura Perez-Beltrachini School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh, EH8 9AB, UK shashi.narayan@ed.ac.uk lperez@ed.ac.uk</p>
<h4>Abstract</h4>
<p>The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare "microplanners", i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
<h2>1 Introduction</h2>
<p>Previous Natural Language Generation (NLG) challenges have focused on surface realisation (Banik et al., 2013; Belz et al., 2011), referring expression generation (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009; Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) and content selection (BouayadAgha et al., 2013).</p>
<p>In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content. Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN.E.BLAHA
entity only once (referring expression generation), lexicalising the occupation property as the phrase worked as (lexicalisation), using PP coordination to avoid repeating the word born (aggregation) and verbalising the three triples by a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation).
(1) a. Data: (John.E.BlaHa birthDate 1942_08_26) (John.E.BlaHa birthPlace San_Antonio) (John.E.BlaHa occupation Fighter_pilot)
b. Text: John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot</p>
<h2>2 Data</h2>
<p>As illustrated by the above example, the WebNLG dataset was designed to exercise the ability of NLG systems to handle the whole range of microplanning operations and their interactions. It was created using a content selection procedure specifically designed to enhance data and text variety (Perez-Beltrachini et al., 2016). In (Gardent et al., 2017), we compared a dataset created using the WebNLG process with existing benchmarks in particular, (Wen et al., 2016)'s dataset (RNNLG) which was produced using a similar process. In what follows, we give various statistics about the WebNLG dataset using the RNNLG dataset as a reference point.</p>
<p>Size. The WebNLG dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units.</p>
<p>Lexicalisation. As illustrated by the examples in (2), different properties can induce different lexical forms (a property might be lexicalised as a verb, a relational noun, a preposition or an adjective). Therefore, the larger the number of properties, the more likely the data is to allow for a wider range of lexicalisation patterns.
(2) X title $\mathrm{Y} \Rightarrow X$ served as $Y$</p>
<p>X Nationality $\mathrm{Y} \Rightarrow X$ 's nationality is $Y$
Relational noun
X country $\mathrm{Y} \Rightarrow X$ is in $Y \quad$ Preposition
X nationality USA $\Rightarrow X$ is American Adjective
To promote diverse lexicalisation patterns, we extracted data from 15 DBPedia categories (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, WrittenWork, Athlete, Artist, City, MeanOfTransportation, CelestialBody, Politician) resulting in a set of 373 distinct RDF properties (more than three times the number of properties contained in the RNNLG dataset). The corrected type token ratio $\left(\right.$ CTTR $^{1}$ ) and the number of word types is roughly twice as large in theWebNLG dataset than in RNNLG.</p>
<p>Surface Realisation. To increase syntactic variety, we use a content selection procedure which extracts data units of various shapes. The intuition is that different input shapes may induce distinct linguistic constructions. This is illustrated in Figure 2. Typically, while triples sharing a subject (SIBLING configuration) are likely to induce a VP or a sentence coordination, a CHAIN configuration (where the object of one triple is the subject of the other) will more naturally give rise to object relative clauses or participials.</p>
<p>Another factor impacting syntactic variation is the set of properties (input patterns) cooccuring in a given input. This is illustrated by the examples in (3) where two inputs of the same length ( 3 triples hence 3 properties) result in text with different syntax. That is, a larger number of input patterns is more likely to induce texts with greater syntactic variety. By extracting data units from a large number of distinct domains (DBPedia categories), we seeked to produce a large number of distinct input patterns.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>A was born in E. She worked as a D. A was born in $E$ and worked as a $D$.
(3) a. location-country-startDate
$\Rightarrow$ Passive-Apposition-Active
108 St. Georges Terrace is located in Perth, Australia. Its construction began in 1981.
b. BIRTHPLACE-ALMA MATER-SELECTION
$\Rightarrow$ Passive-VP coordination
William Anders was born in British Hong Kong, graduated from AFIT in 1962, and joined NASA in 1963.</p>
<p>As shown in Table 3, the WebNLG dataset contains twice as many distinct input patterns and ten times more input shapes than the RNNLG dataset. It is also less redundant with a ratio between number of inputs and number of input patterns of 2.34 against 10.31 for RNNLG.</p>
<p>Aggregation, Sentence Segmentation and Referring Expression Generation. Finally, the need for aggregation, sentence segmentation and referring expression generation mainly arise when texts contains more than one sentence. As Table 3 shows, although data units are overall smaller in the WebNLG dataset than in RNNLG, the WebNLG dataset has a higher number of texts containing more than one sentence and contains texts of longer length.</p>
<h2>3 Participating Systems</h2>
<p>The WebNLG challenge received eight submissions from six participating teams: the ADAPT Centre, Ireland (ADAPTCentre), the University of Melbourne, Australia (UMelbourne), Peking University, China (PKUWriter), Tilburg University, The Netherlands (UTilburg), University of Information Technology, VNU-HCM, Vietnam (UIT-VNU-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">WebNLG</th>
<th style="text-align: right;">RNNLG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># data-text pairs</td>
<td style="text-align: right;">25,298</td>
<td style="text-align: right;">30,842</td>
</tr>
<tr>
<td style="text-align: left;"># distinct inputs</td>
<td style="text-align: right;">9,674</td>
<td style="text-align: right;">22,225</td>
</tr>
<tr>
<td style="text-align: left;">Lexicalisation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># properties</td>
<td style="text-align: right;">373</td>
<td style="text-align: right;">108</td>
</tr>
<tr>
<td style="text-align: left;"># domains</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;"># CTTR</td>
<td style="text-align: right;">6.51</td>
<td style="text-align: right;">3.42</td>
</tr>
<tr>
<td style="text-align: left;"># Words (Type)</td>
<td style="text-align: right;">6,547</td>
<td style="text-align: right;">3,524</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic Variety</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input patterns</td>
<td style="text-align: right;">4,129</td>
<td style="text-align: right;">2,155</td>
</tr>
<tr>
<td style="text-align: left;"># input / # input patterns</td>
<td style="text-align: right;">2.34</td>
<td style="text-align: right;">10.31</td>
</tr>
<tr>
<td style="text-align: left;"># input shapes</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Aggregation, GRE, Segmentation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input with 1 or 2 triples</td>
<td style="text-align: right;">11,111</td>
<td style="text-align: right;">4,087</td>
</tr>
<tr>
<td style="text-align: left;"># input with 3 or 4 triples</td>
<td style="text-align: right;">8,172</td>
<td style="text-align: right;">6,690</td>
</tr>
<tr>
<td style="text-align: left;"># input with 5 to 7 triples</td>
<td style="text-align: right;">6,015</td>
<td style="text-align: right;">20,065</td>
</tr>
<tr>
<td style="text-align: left;"># text with 1 sentence</td>
<td style="text-align: right;">16,740</td>
<td style="text-align: right;">24,234</td>
</tr>
<tr>
<td style="text-align: left;"># text with 2 sentences</td>
<td style="text-align: right;">6,798</td>
<td style="text-align: right;">5,729</td>
</tr>
<tr>
<td style="text-align: left;"># text with $\geq 3$ sentences</td>
<td style="text-align: right;">1,760</td>
<td style="text-align: right;">879</td>
</tr>
<tr>
<td style="text-align: left;"># words/text (avg/min/max)</td>
<td style="text-align: right;">$22.69 / 4 / 80$</td>
<td style="text-align: right;">$18.37 / 1 / 76$</td>
</tr>
</tbody>
</table>
<p>Table 1: Some Statistics about the WebNLG Dataset</p>
<p>HCM) and Universitat Pompeu Fabra, Barcelona, Spain (UPF-FORGE). Each team submitted outputs from a single system except UTILBURG who submitted outputs from three different systems. As a result, there were nine systems in total: eight participating systems and our baseline (BASELINE) system. These can be grouped into three categories: pipeline systems, statistical machine translation (SMT) and neural machine translation (NMT) systems. Table 3 shows the system categorisations.</p>
<p>Pipeline Systems. Three submissions used a template or grammar-based pipeline framework with some NLG module: UTILBURG-PIPELINE, UIT-VNU-HCM and UPF-FORGE.</p>
<p>The first two systems, UTILBURG-PIPELINE and UIT-VNU-HCM, extracted rules or templates from the training data for surface realisation, whereas the third system, UPF-FORGE, used the FORGe grammar (Mille et al., 2017).</p>
<p>UTILBURG-PIPELINE extracted rules mapping a triple (or a triple set) to a text observed in the training data; both the triple and the associated text were delexicalised. Given a RDF triple set to generate</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System ID</th>
<th style="text-align: center;">Institution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PIPELINE Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">UIT-VNU-HCM</td>
<td style="text-align: center;">University of Information Technology</td>
</tr>
<tr>
<td style="text-align: center;">UPF-FORGE</td>
<td style="text-align: center;">Universitat Pompeu Fabra</td>
</tr>
<tr>
<td style="text-align: center;">SMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">NMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADAPTCENTRE</td>
<td style="text-align: center;">ADAPT Centre, Ireland</td>
</tr>
<tr>
<td style="text-align: center;">UMELBOURNE</td>
<td style="text-align: center;">University of Melbourne</td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-NMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">PKUWRITER</td>
<td style="text-align: center;">Peking University</td>
</tr>
<tr>
<td style="text-align: center;">BASELINE</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Categorisation of participating systems.
from, UTILBURG-PIPELINE first ordered triples to maintain discourse order. Extracted rules were then applied to generate a delexicalised text. Missing entities were added using a referring expression generation module (Castro Ferreira et al., 2016). Finally, a 6 -gram language model trained on the Gigaword corpus was used to rank the system output.</p>
<p>UIT-VNU-HCM did not resort to delexicalisation in their rules. Instead of using the text to extract templates, it used the typed-dependency structure of the text to facilitate rule extraction from the training data. In addition, at run time, WordNet was used to estimate similarity between predicates in the test and train sets.</p>
<p>UPF-FORGE mostly focused on sentence planning with predicate-argument (PredArg) templates. For each of the DBPedia properties found in the training and evaluation data, they manually defined PredArg templates encoding various DBPediaspecific and linguistic features. Given a RDF triple set to generate from, PredArg templates were used to convert these triples to PredArg structures and to further aggregate them to form a PredArg graph structure. The FORGe generator took this linguistic PredArg structure as input and generated a text.</p>
<p>SMT Systems. UTilburg-SMT was the only system which used the statistical machine translation framework. It was trained on the WebNLG dataset using the Moses toolkit (Koehn et al., 2007). The dataset was pre-processed whereby each entity in the input and each corresponding referring expression in the output were delexicalised and annotated with the entity Wikipedia ID. The alignments from the training set were obtained using MGIZA and model weights were tuned using 60batch MIRA with BLEU as the evaluation metric. Similar to UTilburg-Pipeline, the system used a 6-gram language model trained on the Gigaword corpus using KenLM.</p>
<p>NMT Systems. Four systems (ADAPTCentre, UMElbourne, UTilburg-NMT and PKUWRIter) build upon the attention-based encoder-decoder architecture proposed in (Bahdanau et al., 2014). Most of them make use of existing NMT frameworks. There are however important differences among systems with respect to both the concrete architecture and the sequence representations they use.</p>
<p>ADAPTCentre makes use of the Nematus (Sennrich et al., 2017) system. They opt for subword representations rather than delexicalisation to deal with rare words and sparsity. They linearise the input sequence and insert tuple separation special tokens.</p>
<p>UMelbourne does a combined delexicalisation procedure and enrichment of the input sequence. Entities are delexicalised using an entity identifier (ENTITY-ID). When available, the DBPedia type of the entity is appended. An n-gram search is used to assure the most accurate target sequence delexicalisation. They use a standard encoder-decoder with attention model.</p>
<p>UTilburg-NMT is based on the Edinburgh Neural Machine Translation submission for the 2016 machine translation shared task (WMT 2016). The target sequences are the delexicalised texts (cf. UTilburg-PiPEline) and the input sequences are the linearisation of the delexicalised input set of triples. The REG module from their pipeline system is used to post-process the decoder outputs.</p>
<p>The PKUWRITER system relies upon two extra mechanisms, namely a ranking module and an extra Reinforcement Learning (RL) training objective. It uses an ensemble of attention-based encoderdecoder models based on the TensorFlow seq2seq API in addition to the baseline ( 7 models in total). They propose an output ranking module to choose the best verbalisation among those output by the generation models. The ranker is trained on supervised data generated automatically. Input triple sets are paired with verbalisations produced by each of the generation models. Then, each pair is associated with a quality score, i.e. the BLEU score of the verbalisation and the reference. Word and sentence level features are extracted to train the ranker. The generation models and ranker are trained on different data partitions. The RL objective encourages the generation of output texts which include subjects occurring in the input RDF triples. In addition, PKUWRITER uses a set of hand-crafted rules to handle input cases where the model fails.</p>
<h2>4 Evaluation Methodology</h2>
<p>The WebNLG challenge includes both an automatic and a human-based evaluation. Due to time constraints, only the results of the automatic evaluation are presented in this paper. The results of the humanbased evaluation will be provided on the WebNLG website ${ }^{2}$ in October 2017.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Automatic Evaluation</h3>
<p>Three automatic metrics were used to evaluate the participating systems:</p>
<ul>
<li>BLEU-4 ${ }^{3}$ (Papineni et al., 2002). BLEU scores were computed using up to three references.</li>
<li>METEOR (v1.5) ${ }^{4}$ (Denkowski and Lavie, 2014);</li>
<li>$\mathrm{TER}^{5}$ (Snover et al., 2006).</li>
</ul>
<p>For statistical significance testing, we followed the bootstrapping algorithm described in (Koehn and Monz, 2006).</p>
<p>To assess the ability of the participating systems to generalise to out of domain data, the test dataset consists of two sets of roughly equal size: a test set containing inputs created for entities belonging to DBpedia categories that were seen in the training data (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, City, and WrittenWork), and a test set containing inputs extracted for entities belonging to 5 unseen categories (Athlete, Artist, MeanOfTransportation, CelestialBody, Politician). We call the first type of data seen categories, the second, unseen categories. Correspondingly, we report results for 3 datasets: the seen category dataset, the unseen category dataset and the total test set made of both the seen and the unseen category datasets.</p>
<p>Table 3 gives more detailed statistics about the number of properties, objects and subject entities occurring in each test set.</p>
<ul>
<li>$|$ Test $\mid$ is the number of distinct properties, subjects and objects in the test set;</li>
<li>$|$ Test $\cap T n D v \mid$ is the number of distinct properties, subjects and objects which are in the test set and were seen in the training or the development set;</li>
<li>$|$ Test $\backslash T n D v \mid$ is the number of distinct properties, subjects and objects which occur in the</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>test set, but not in the training and development set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Seen</th>
<th style="text-align: center;">Unseen</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prop.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">108</td>
</tr>
<tr>
<td style="text-align: center;">Obj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">1033</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">1888</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">1011</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">1025</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">841</td>
<td style="text-align: center;">863</td>
</tr>
<tr>
<td style="text-align: center;">Subj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">343</td>
<td style="text-align: center;">238</td>
<td style="text-align: center;">575</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">342</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">342</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">233</td>
</tr>
</tbody>
</table>
<p>Table 3: Test data statistics on properties, objects and subjects for seen, unseen and all datasets.</p>
<p>While in the seen test data (first column) almost all triple elements are present in the training and development sets, in the unseen test data (second column) the vast majority of subjects, objects, and, more importantly, properties (which need to be lexicalised) has not been seen in the training and development data.</p>
<p>Participants were requested to submit tokenised and lowercased texts. To ensure consistency between submissions, we pre-processed the submitted results one more time to double-check that those requirements were fullfilled. As teams used different strategies of tokenisation, we had to modify submissions using our own scripts. In particular, all punctuation signs were separated from alphanumeric sequences (e.g. a two-token group 65.6 feet was modified to a four-token 65.6 feet). Moreover, we converted both references and submission outputs to the ASCII character set.</p>
<h3>4.2 Baseline System</h3>
<p>We developed a baseline system using neural networks and delexicalisation. Before training, we preprocess the data by linearising triples, performing tokenisation and delexicalisation using exact matching.</p>
<p>While delexicalising, we make the following replacements:</p>
<ul>
<li>
<p>given a triple of the form ( $s p o$ ) where $s$ is of the category $C$ for which the triple set has been produced (e.g., Alan_Bean for the category Astronaut), we replace $s$ by $C$.</p>
</li>
<li>
<p>given a triple of the form $s p o$, we replace $o$ by p. E.g., (s country Indonesia) becomes (s country COUNTRY). The replacements were made using the exact match and as a result not all the entities were replaced.</p>
</li>
</ul>
<p>Examples 4 and 5 show a (data,text) pair before and after delexicalisation. Note that noodles was not substituted by the corresponding entity category in the target text (because there is no exact match with the Noodle object in the input). Table 4 shows the number of distinct tokens occurring in the original and delexicalised data.
(4) a. Set of triples: (INDONESIA LEADERNAME Jusuf.KALLA) (BAKSO INGREDIENT NOODLE) (BAKSO COUNTRY INDONESIA)
b. Text: Bakso is a food containing noodles; it is found in Indonesia where Jusuf Kalla is the leader.
(5) a. Source: (COUNTRY LEADERNAME LEADERNAME) (FOOD INGREDIENT INGREDIENT) (FOOD COUNTRY COUNTRY)
b. Target: FOOD is a food containing noodles; it is found in COUNTRY where LEADERNAME is the leader.</p>
<p>On this delexicalised data-to-text corpus, we trained a vanilla sequence-to-sequence model with attention mechanism using the OpenNMT toolkit (Klein et al., 2017) with default parameters for training and decoding. The network consists of a twolayered bidirectional encoder-decoder model with LSTM units. We use a batch size of 64 and a starting learning rate of 1.0. The size of the hidden layers is 500. The network was trained for 13 epochs with a stochastic gradient descent optimisation method and a dropout probability of 0.3 . We used the entire vocabulary for the baseline due to its rather small size.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Delexicalised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: center;">2703</td>
<td style="text-align: center;">1300</td>
</tr>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: center;">5374</td>
<td style="text-align: center;">5013</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">8077</td>
<td style="text-align: center;">6313</td>
</tr>
</tbody>
</table>
<p>Table 4: Vocabulary size in tokens.
After training we relexicalised sentences with corresponding entities if of course their counterparts are
present in generated output. The performance of the baseline is shown in Tables 5, 6, 7 along with other teams' results.</p>
<h2>5 Results</h2>
<p>We briefly discuss the automatic scores distinguishing between results on the whole dataset, on data extracted from previously unseen categories and on data extracted from seen categories.</p>
<p>Global Scores. Table 5 shows the global results that is, results on the whole test set. Horizontal lines group together systems for which the difference in scores is not statistically significant. The names of the teams are coloured according to system type: neural-based systems are in red, pipeline systems in blue, and SMT systems in light grey.</p>
<p>Most systems (6 out of 8 ) outperform the baseline, four of them obtaining scores well above it. In terms of BLEU and TER scores, the first four systems include systems of each type (neural, SMT-based and pipelines).</p>
<p>While BLEU and METEOR yield almost identical rankings, METEOR does not, suggesting that the systems handle synonyms and morphological variation differently. In particular, the fact that UPFFORGE ranks first under the METEOR score suggests that it often generates text that differs from the references because of synonymic or morphological variation.</p>
<p>Scores on Seen Categories. For data extracted from DBPedia categories that were seen in the training data, machine learning based systems (neural and SMT) mostly outperform rule-based systems. In particular, in terms of BLEU and TER scores, the three pipeline systems are at the low end of the ranking. Again though, the METEOR scores show a much higher ranking (3rd rather than 6th) for the UPF-FORGE systems.</p>
<p>Scores on Unseen Categories. On unseen categories, the UPF-FORGE systems ranks first as the system could quickly be adapted to handle properties that had not been seen in the training data. The ranking of the other systems is more or less unchanged with the exception of the ADAPTCENTRE system. This neural system does not use delexicalisation and the subword approach that was adopted</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 45.13</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 44.28</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 39.88</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 38.65</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-PiPELine 35.29</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 34.60</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 33.24</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">ADAPT 31.06</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 7.07</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 0.47</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.53</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$3-5$</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.56</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-NMT 0.60</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">BASELINE 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">UIT-VNU 0.82</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">ADAPT 0.84</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.39</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.37</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.34</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">ADAPT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$5-7$</td>
<td style="text-align: center;">PKUWritER 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-PiPELine 0.30</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.23</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.09</td>
</tr>
</tbody>
</table>
<p>Table 5: Results for all categories. Lines between systems indicate a difference in scores which is statistically significant ( $p&lt;$ 0.05 ). A colour for a team name indicates a type of the system used (NMT, SMT, Pipeline).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: center;">$2-3$</td>
<td style="text-align: center;">Melbourne 54.52</td>
</tr>
<tr>
<td style="text-align: center;">$2-4$</td>
<td style="text-align: center;">Tilb-SMT 54.29</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 52.39</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 51.23</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 44.34</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 43.28</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 40.88</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 19.87</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 0.44</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.45</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Tilb-SMT 0.47</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.48</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 0.51</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.78</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.44</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.42</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">Melbourne 0.41</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$5-8$</td>
<td style="text-align: center;">Tilb-PiPELine 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">PKUWritER 0.37</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">BASELINE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.15</td>
</tr>
</tbody>
</table>
<p>Table 6: Results for seen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 35.70</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 33.27</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 29.88</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 25.36</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-NMT 25.12</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 20.65</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 10.53</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 06.13</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.11</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.55</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.65</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 0.67</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-NMT 0.72</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 0.80</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UIT-VNU 0.87</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">ADAPT 1.4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.33</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.33</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 0.24</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.21</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 0.19</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.07</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.03</td>
</tr>
</tbody>
</table>
<p>Table 7: Results for unseen categories.</p>
<p>S John Clancy is a labour politican who leads Birmingham, where architect John Madin, who designed 103 Colmore Row, was born.
$\mathbf{M}<em>{S} \quad{$ Birmingham|LeaderName|John_Clancy</em>(Labour_politician),
John_Madin|birthPlace|Birmingham,
103_Colmore_Row|architect|John_Madin $}$
$\mathbf{T}<em T__1="T_{1">{1} \quad$ Labour politician, John Clancy is the leader of Birmingham.
$\mathbf{M}</em> \quad{$ Birmingham|LeaderName|John_Clancy_(Labour_politician) $}$
$\mathbf{T}}<em T__2="T_{2">{2} \quad$ John Madin was born in Birmingham.
$\mathbf{M}</em> \quad{$ John_Madin|birthPlace|Birmingham $}$
$\mathbf{T}}<em T__3="T_{3">{3} \quad$ He was the architect of 103 Colmore Row.
$\mathbf{M}</em> \quad{$ 103_Colmore_Row|architect|John_Madin $}$
Figure 1: An example pair out of the Split-and-Rephrase Dataset. $\mathbf{S}$ is a single complex sentence with meaning $\mathbf{M}}<em 1="1">{S} . \mathbf{T}</em>}, \mathbf{T<em 1="1">{1}, \mathbf{T}</em>}$ form a text of three simple sentences whose joint meaning $\mathbf{M<em 1="1">{T</em>}} \cup \mathbf{M<em 2="2">{T</em>}} \cup \mathbf{M<em 3="3">{T</em>$.}}$ is the same as the meaning $\mathbf{M}_{S}$ of the corresponding single complex sentence $\mathbf{S</p>
<p>to handle unseen data does not seem to work well.</p>
<h2>6 Conclusion</h2>
<p>The WebNLG challenge was novel in that it was the first challenge to provide a benchmark on which to evaluate and compare microplanners. Despite a tight schedule (we released the training data in April for a submission in August), it generated a high level of interest among the NLG community: 62 groups from 18 countries ${ }^{6}$ downloaded the data, 6 groups submitted 8 systems and 3 groups developped a system but did not submit.</p>
<p>The training data for the WebNLG 2017 challenge is available on the WebNLG website ${ }^{7}$ and evaluation on the test data can be run by the organisers on demand. A larger dataset consisting of 40,049 (data, text) pairs, 15,095 distinct data input and 15 DBpedia categories is also available. Both datasets are under the creative common licence "CC Attribution-Noncommercial-Share Alike 4.0 International license". We hope that these resources will enable a long and fruitful strand of research on microplanning.</p>
<p>The usefulness of the WebNLG dataset reaches far beyond the WebNLG challenge. It can be used for instance to train a semantic parser which would convert a sentence into a set of RDF triples. It can also be used to derive new datasets for related tasks. Thus in (Narayan et al., 2017), we show how to derive from the WebNLG dataset, a dataset for sentence simplification which we call the Split-andRephrase dataset. In this dataset, each pair consists of (i) a single, complex sentence with its meaning representation in terms of RDF triples and (ii) a sequence of at least two sentences and their corresponding sets of RDF triples whereby these sets form a partition on the set of RDF triples associated with the input complex sentence. In other words, the Split-and-Rephrase dataset associates a complex sentence with a sequence of at least two sentences whose meaning is the same as that of the complex sentence. As explained in (Narayan et al., 2017), this dataset was created using the meaning represen-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tations (sets of RDF triples) as pivot. The Split-and-Rephrase dataset consists of 1,100,166 pairs of the form $\left\langle\left(M_{C}, T_{C}\right),\left{\left(M_{1}, T_{1}\right) \ldots\left(M_{n}, T_{n}\right)\right}\right\rangle$ where $T_{C}$ is a complex sentence and $T_{1} \ldots T_{n}$ is a sequence of texts with semantics $M_{1}, \ldots M_{n}$ expressing the same content $M_{C}$ as $T_{C}$. Figure 1 shows an example pair. It was used to train four neural systems and the associated meaning representations were shown to improve performance.</p>
<p>In the future, we are planning to build a multilingual resource in which the English text present in the WebNLG dataset will be translated into French, Russian and Maltese. In this way, morphological variation can be explored which is an interesting avenue of research in particular for neural systems which have a limited ability to handle unseen input: how well will these systems be able to handle the generation of morphologically rich languages ?</p>
<p>The analysis of the participants results presented in this paper will be complemented in an arxiv report by the results of a human-based evaluation. Using human judgements obtained through crowdsourcing, this human evaluation will assess the system results on three criteria, namely fluency, grammaticality and appropriateness (does the text correctly verbalise the input data?). We will also provide a more in depth analysis of the participant results on data extracted from different categories and data of various length.</p>
<h2>Acknowledgments</h2>
<p>The research presented in this paper has been supported by the following grants and projects: "WebNLG", Project ANR-14-CE24-0033 of the French National Research Agency and "SUMMA", H2020 project No. 688139.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR-2015 (abs/1409.0473).
Eva Banik, Claire Gardent, and Eric Kow. 2013. The kbgen challenge. In the 14th European Workshop on Natural Language Generation (ENLG), pages 94-97.
Anja Belz and Albert Gatt. 2007. The attribute selection for gre challenge: Overview and evaluation results.</p>
<p>Proceedings of UCNLG+ MT: Language Generation and Machine Translation, pages 75-83.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2008. The grec challenge: Overview and evaluation results.
Anja Belz, Eric Kow, and Jette Viethen. 2009. The grec named entity generation challenge 2009: overview and evaluation results. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 88-98. Association for Computational Linguistics.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2010. Generating referring expressions in context: The grec task evaluation challenges. In Empirical methods in natural language generation, pages 294327. Springer.</p>
<p>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, ENLG '11, pages 217-226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner, and Chris Mellish. 2013. Overview of the first content selection challenge from open semantic web data. In ENLG, pages 98-102.
J. B. Carroll. 1964. Language and thought. NJ: PrenticeHall. Englewood Cliffs.
Thiago Castro Ferreira, Emiel Krahmer, and Sander Wubben. 2016. Towards more variation in text generation: Developing and evaluating variation models for choice of referential form. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 568-577.
Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 179-188.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The tuna challenge 2008: Overview and evaluation results. In Proceedings of the Fifth International Natural Language Generation Conference, pages 198-206. Association for Computational Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tunareg challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on</p>
<p>Natural Language Generation, pages 174-182. Association for Computational Linguistics.
G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. ArXiv e-prints.
Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings of the Workshop on Statistical Machine Translation, StatMT '06, pages 102-121.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177180.</p>
<p>Xiaofei Lu. 2008. Automatic measurement of syntactic complexity using the revised developmental level scale. In FLAIRS Conference, pages 153-158.
Simon Mille, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of SemEval-2017, pages 917-920.
Shashi Narayan, Claire Gardent, Shay B. Cohen, and Anastasia Shimorina. 2017. Split and rephrase. In Proceedings of EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.
Laura Perez-Beltrachini and Claire Gardent. 2017. Analysing data-to-text generation benchmarks. In Proceedings of the tenth International Natural Language Generation Conference, INLG.
Laura Perez-Beltrachini, Rania Sayed, and Claire Gardent. 2016. Building rdf content for data-to-text generation. In COLING, pages 1493-1502.
Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch-Mayne, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Laubli, Antonio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus: A toolkit for neural machine translation. In Proceedings of the EACL 2017 Software Demonstrations, pages 65-68, 4.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation.</p>
<p>In Proceedings of association for machine translation in the Americas, volume 200.
Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, and Steve Young. 2016. Multi-domain neural network language generation for spoken dialogue systems. In Proceedings of NAACL-HLT.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Australia, Canada, China, Croatie, France, Germany, India, Iran, Ireland, Italy, Netherlands, Norway, Poland, Spain, Tunisia, UK, USA, Vietnam
${ }^{7}$ http://talcl.loria.fr/webnlg/stories/ challenge.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6805 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6805</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6805</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-8236010c2ecc94d826be6010ff187fdc000e7df6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8236010c2ecc94d826be6010ff187fdc000e7df6" target="_blank">Deductive Verification of Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Natural Program, a natural language-based deductive reasoning format that enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps and significantly enhances the rigor and trustfulness of generated reasoning steps.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6805.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6805.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural Program</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural Program deductive reasoning format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A natural-language-based format that requires models to list labeled question-related premises and produce numbered reasoning steps that cite the minimal set of premises used, enabling per-step deductive verification via in‑context LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (ChatGPT) (primary evaluated model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM used via API to generate Natural Program outputs and to perform one-shot per-step verification; paper does not supply internal training specifics for the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (natural-language deductive formatting + in‑context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Natural Program: explicit premise extraction + numbered steps citing minimal premises; in‑context one-shot verification of each step; generation via in‑context learning (CoT-style prompts adapted to Natural Program).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Last Letters (Last Letter Concatenation), Date (BIG-bench date subset)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Arithmetic/math word problems (GSM8K, AddSub, MATH, AQuA), symbol/string manipulation (Last Letters), and date‑understanding tasks (Date). Human-labeled ground-truth deductive validity used for verification evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Per-step deductive validity verification; multi-step arithmetic reasoning; symbol manipulation; date reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Per-step deductive verification accuracy (binary yes/no); final answer accuracy (majority/plurality voting over generated chains).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Deductive verification (Natural Program, GPT-3.5-turbo, one-shot, using minimal premises): overall verification accuracy 69% (Table 3). Final answer accuracies with NP (no verification) roughly comparable to CoT baselines (e.g., GSM8K 87.05%, AQuA 70.34%, MATH 36.75%, AddSub 93.67%, Date 72.49%, Last Letters 92.98%); with NP + Deductive Verification + UPV slight reductions (e.g., GSM8K 86.01%, AQuA 69.49%, MATH 36.48%, AddSub 93.54%, Date 71.45%, Last Letters 92.60%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Natural Program verification (69% overall) outperforms verifying entire CoT chains at once (≈52% overall) by ~17 percentage points for per-step verification; final-answer accuracy with NP is on-par with or slightly better than CoT baselines before verification, but applying verification+filtering can slightly reduce final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Formatting reasoning as Natural Programs (explicit premises + minimal citations) substantially improves LLM per-step verification accuracy versus verifying whole CoT traces, and yields reasoning chains that are more rigorous/trustworthy while preserving similar final-answer accuracy before filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verification still imperfect (≈69% overall); filtering verified chains removes many chains that had correct final answers but incorrect grounding, which can reduce final-answer accuracy; struggles on tasks with heavy sequential dependency (Last Letters); cannot detect some semantic or contextual ambiguities (e.g., ambiguous word sense like 'pennies').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6805.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6805.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UPV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unanimity-Plurality Voting (UPV)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase sequence selection strategy: (1) unanimity phase—perform per-step deductive validation on sampled chains and retain only chains where all steps are verified; (2) plurality phase—majority/plurality voting among retained, verified chains to select final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM used to generate k candidate Natural Program chains and to perform k' per-step verification votes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + sampling + verification-voting pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sampling k candidate chains (k=10), per-step verification with majority vote over k' (default k'=3) for each step, retain fully-verified chains, then plurality voting on answers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Date, Last Letters (same benchmarks as Natural Program experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same as Natural Program; used to evaluate impact of verification+voting on final answer accuracy and trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Final answer selection from multiple candidate reasoning chains after deductive verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final answer accuracy after UPV filtering; verification accuracy as internal metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Applying UPV (NP + Deductive Verification + UPV) produced slight decreases in final-answer accuracy vs NP without verification (example deltas: GSM8K 87.05% -> 86.01%; AQuA 70.34% -> 69.49%; MATH 36.75% -> 36.48%; AddSub 93.67% -> 93.54%; Date 72.49% -> 71.45%; Last Letters 92.98% -> 92.60%). Verification voting ablation: increasing k' increased verification accuracy (k'=1 correct detection 86%; k'=3 90%; k'=5 90%; k'=10 92% for AddSub, Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>UPV increases trustworthiness by eliminating unverified chains, but can reduce final-answer accuracy because many chains with correct final answers are filtered out when their reasoning is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>UPV produces more trustworthy final answers by selecting only fully-verified reasoning chains; increasing k' improves verification reliability but costs more compute; in practice UPV slightly reduces final-answer accuracy due to removal of correct-but-unverified chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Filtering can remove many correct final answers whose supporting reasoning is imperfect, reducing measured accuracy; computational cost grows with k and k'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6805.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6805.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned large transformer language model from OpenAI used in experiments both for generating Natural Program reasoning traces and for performing one-shot per-step deductive verification via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned transformer LLM (API-served); specifics of pretraining corpus/parameter count not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not reported in paper (OpenAI internal pretraining + instruction tuning / RLHF presumed).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought prompting, Natural Program formatting, in‑context one-shot per-step verification, sampling-based voting (self-consistency style sampling used in baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Date, Last Letters</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Arithmetic and math word problems, multiple choice math (AQuA), symbolic string manipulation (Last Letters), date arithmetic (Date).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic reasoning, deductive step verification, symbol manipulation, date reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Per-step deductive verification accuracy; final answer accuracy (majority voting over k=10 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per-step verification (verifying whole chain at once) performed poorly (~50–52% average); Natural Program per-step verification (one-shot, minimal premises) achieved overall ~69% verification accuracy (Table 3). Final answer accuracy examples (CoT+Voting baseline vs NP variants): CoT+Voting GSM8K 87.62%, NP (no verification) GSM8K 87.05%, NP+UPV GSM8K 86.01% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Natural Program + per-step verification substantially improves verification accuracy vs verifying entire chains at once (≈69% vs ≈52%); final-answer accuracy remains similar to CoT baselines, with slight reductions after filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5-turbo can reliably verify short, locally-grounded deductive steps when provided only minimal necessary premises and well-structured Natural Program formatting; verifying entire CoT traces at once yields near-random or biased verification behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verification still imperfect; model tends to be distracted by irrelevant premises when verifying entire chains; fails to detect some semantic/contextual ambiguity errors; limited on tasks with long chain dependencies (Last Letters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6805.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6805.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B/13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-7B and Vicuna-13B (LLaMA-based chat models fine-tuned on ShareGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-weight conversational models fine-tuned from LLaMA-7B/13B on ShareGPT-style data; evaluated for one-shot Natural Program per-step deductive verification and further fine-tuned on an automatically generated verification dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B, Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLaMA-7B and LLaMA-13B variants (ShareGPT fine-tuning); tested both original and further fine-tuned versions on a deductive verification dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (LLaMA family) with conversational fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ShareGPT fine-tuning data; additional fine-tuning performed on a 2000-step deductive verification dataset generated using GPT-3.5-turbo for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Natural Program one-shot per-step verification; additional supervised fine-tuning on labeled verification examples.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Date, Last Letters (evaluated same split as GPT-3.5 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same arithmetic/math/string/date tasks used to measure per-step verification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Per-step deductive verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Per-step deductive verification accuracy (one-shot Natural Program verification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Original Vicuna-7B overall verification ≈54% (Table 9). Vicuna-7B after fine-tuning on verification data overall ≈61%. Original Vicuna-13B overall ≈57%; Vicuna-13B fine-tuned ≈66%. (Per-dataset variations reported in Table 9.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuning on a small supervised verification dataset improves Vicuna verification performance (e.g., Vicuna-7B 54% -> 61%, Vicuna-13B 57% -> 66%), but still generally lags behind GPT-3.5-turbo (≈69% overall), indicating model capacity and pretraining matter.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model capacity and pretraining/fine-tuning both affect deductive verification capability; small open models can be improved by supervised fine-tuning on verification data but may not match larger closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even after fine-tuning, smaller models show lower verification reliability than GPT-3.5; fine-tuning dataset construction depended on GPT-3.5 outputs (potentially propagating biases); verification performance varies materially by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6805.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6805.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural language reasoning from LLMs to improve performance on multi-step tasks by exposing intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (used as baseline prompting paradigm in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering technique applied to transformer LLMs to elicit multi-step reasoning in outputs (no new model introduced).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (prompting technique, not an architecture change)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate unconstrained CoT traces; used as baseline for generation and for end-to-end verification attempts (verifying entire chain at once).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Date, Last Letters (used as baseline generation format)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same reasoning/math/string/date benchmarks used in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning generation and end-to-end verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy (majority voting) and verification accuracy when attempting to verify entire chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Final-answer (CoT+Voting) reported in Table 4: GSM8K 87.62%, AQuA 70.18%, MATH 35.93%, AddSub 92.36%, Date 69.97%, Last Letters 81.60%. End-to-end chain verification accuracy (verifying entire chain at once) ~52% average (Table 3 / Table 2 results show ~50–52%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT improves final-answer performance relative to direct answering, but verifying entire CoT traces at once performs poorly compared to stepwise verification under Natural Program formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT remains effective for improving final-answer accuracy but is vulnerable to hallucinations and error accumulation; naive whole-chain self-verification by LLMs is unreliable due to distraction by irrelevant premises.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT traces can contain hallucinated or ungrounded intermediate steps; verifying whole CoT traces is challenging for LLMs and yields low verification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6805.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6805.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithful CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed method that aims to produce more faithful/grounded CoT traces (cited and evaluated as a baseline in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (baseline prompting/approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method/prompting strategy intended to increase faithfulness of CoT traces (paper compared against it as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + faithful CoT prompt/regimen</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting approach designed to make step-by-step reasoning more faithful; used in comparison for final-answer voting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, MATH, AddSub, Date (reported for some datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same reasoning/math benchmarks used for comparative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning with an emphasis on faithful/grounded steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy with majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Faithful CoT + Voting reported in Table 4: GSM8K 75.80%, AQuA 61.80%, MATH 31.78% (note some dataset numbers were taken from official repo and some were reimplemented), AddSub 88.35%, Date 73.50% (Last Letters not available).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Faithful CoT sometimes underperforms vanilla CoT voting or Natural Program on final-answer accuracy for some datasets in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Faithfulness-focused CoT approaches can improve interpretability but do not uniformly outperform Natural Program formatting combined with voting on final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not all results directly comparable across datasets due to prompt and implementation differences; some dataset results unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Verification of Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Natural language deduction through search over statement compositions <em>(Rating: 1)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6805",
    "paper_id": "paper-8236010c2ecc94d826be6010ff187fdc000e7df6",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "Natural Program",
            "name_full": "Natural Program deductive reasoning format",
            "brief_description": "A natural-language-based format that requires models to list labeled question-related premises and produce numbered reasoning steps that cite the minimal set of premises used, enabling per-step deductive verification via in‑context LLM prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (ChatGPT) (primary evaluated model)",
            "model_description": "Instruction-tuned transformer LLM used via API to generate Natural Program outputs and to perform one-shot per-step verification; paper does not supply internal training specifics for the model.",
            "model_size": null,
            "architecture_type": "Transformer (natural-language deductive formatting + in‑context learning)",
            "training_data": null,
            "reasoning_method": "Natural Program: explicit premise extraction + numbered steps citing minimal premises; in‑context one-shot verification of each step; generation via in‑context learning (CoT-style prompts adapted to Natural Program).",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Last Letters (Last Letter Concatenation), Date (BIG-bench date subset)",
            "benchmark_description": "Arithmetic/math word problems (GSM8K, AddSub, MATH, AQuA), symbol/string manipulation (Last Letters), and date‑understanding tasks (Date). Human-labeled ground-truth deductive validity used for verification evaluation.",
            "task_type": "Per-step deductive validity verification; multi-step arithmetic reasoning; symbol manipulation; date reasoning.",
            "performance_metric": "Per-step deductive verification accuracy (binary yes/no); final answer accuracy (majority/plurality voting over generated chains).",
            "performance_value": "Deductive verification (Natural Program, GPT-3.5-turbo, one-shot, using minimal premises): overall verification accuracy 69% (Table 3). Final answer accuracies with NP (no verification) roughly comparable to CoT baselines (e.g., GSM8K 87.05%, AQuA 70.34%, MATH 36.75%, AddSub 93.67%, Date 72.49%, Last Letters 92.98%); with NP + Deductive Verification + UPV slight reductions (e.g., GSM8K 86.01%, AQuA 69.49%, MATH 36.48%, AddSub 93.54%, Date 71.45%, Last Letters 92.60%).",
            "comparison_with_baseline": "Natural Program verification (69% overall) outperforms verifying entire CoT chains at once (≈52% overall) by ~17 percentage points for per-step verification; final-answer accuracy with NP is on-par with or slightly better than CoT baselines before verification, but applying verification+filtering can slightly reduce final-answer accuracy.",
            "key_findings": "Formatting reasoning as Natural Programs (explicit premises + minimal citations) substantially improves LLM per-step verification accuracy versus verifying whole CoT traces, and yields reasoning chains that are more rigorous/trustworthy while preserving similar final-answer accuracy before filtering.",
            "limitations": "Verification still imperfect (≈69% overall); filtering verified chains removes many chains that had correct final answers but incorrect grounding, which can reduce final-answer accuracy; struggles on tasks with heavy sequential dependency (Last Letters); cannot detect some semantic or contextual ambiguities (e.g., ambiguous word sense like 'pennies').",
            "uuid": "e6805.0",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "UPV",
            "name_full": "Unanimity-Plurality Voting (UPV)",
            "brief_description": "A two-phase sequence selection strategy: (1) unanimity phase—perform per-step deductive validation on sampled chains and retain only chains where all steps are verified; (2) plurality phase—majority/plurality voting among retained, verified chains to select final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-tuned transformer LLM used to generate k candidate Natural Program chains and to perform k' per-step verification votes.",
            "model_size": null,
            "architecture_type": "Transformer + sampling + verification-voting pipeline",
            "training_data": null,
            "reasoning_method": "Sampling k candidate chains (k=10), per-step verification with majority vote over k' (default k'=3) for each step, retain fully-verified chains, then plurality voting on answers.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Date, Last Letters (same benchmarks as Natural Program experiments)",
            "benchmark_description": "Same as Natural Program; used to evaluate impact of verification+voting on final answer accuracy and trustworthiness.",
            "task_type": "Final answer selection from multiple candidate reasoning chains after deductive verification.",
            "performance_metric": "Final answer accuracy after UPV filtering; verification accuracy as internal metric.",
            "performance_value": "Applying UPV (NP + Deductive Verification + UPV) produced slight decreases in final-answer accuracy vs NP without verification (example deltas: GSM8K 87.05% -&gt; 86.01%; AQuA 70.34% -&gt; 69.49%; MATH 36.75% -&gt; 36.48%; AddSub 93.67% -&gt; 93.54%; Date 72.49% -&gt; 71.45%; Last Letters 92.98% -&gt; 92.60%). Verification voting ablation: increasing k' increased verification accuracy (k'=1 correct detection 86%; k'=3 90%; k'=5 90%; k'=10 92% for AddSub, Table 7).",
            "comparison_with_baseline": "UPV increases trustworthiness by eliminating unverified chains, but can reduce final-answer accuracy because many chains with correct final answers are filtered out when their reasoning is imperfect.",
            "key_findings": "UPV produces more trustworthy final answers by selecting only fully-verified reasoning chains; increasing k' improves verification reliability but costs more compute; in practice UPV slightly reduces final-answer accuracy due to removal of correct-but-unverified chains.",
            "limitations": "Filtering can remove many correct final answers whose supporting reasoning is imperfect, reducing measured accuracy; computational cost grows with k and k'.",
            "uuid": "e6805.1",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (ChatGPT)",
            "brief_description": "An instruction-tuned large transformer language model from OpenAI used in experiments both for generating Natural Program reasoning traces and for performing one-shot per-step deductive verification via prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-finetuned transformer LLM (API-served); specifics of pretraining corpus/parameter count not provided in paper.",
            "model_size": null,
            "architecture_type": "Transformer",
            "training_data": "Not reported in paper (OpenAI internal pretraining + instruction tuning / RLHF presumed).",
            "reasoning_method": "Chain-of-Thought prompting, Natural Program formatting, in‑context one-shot per-step verification, sampling-based voting (self-consistency style sampling used in baselines).",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Date, Last Letters",
            "benchmark_description": "Arithmetic and math word problems, multiple choice math (AQuA), symbolic string manipulation (Last Letters), date arithmetic (Date).",
            "task_type": "Multi-step arithmetic reasoning, deductive step verification, symbol manipulation, date reasoning.",
            "performance_metric": "Per-step deductive verification accuracy; final answer accuracy (majority voting over k=10 samples).",
            "performance_value": "Per-step verification (verifying whole chain at once) performed poorly (~50–52% average); Natural Program per-step verification (one-shot, minimal premises) achieved overall ~69% verification accuracy (Table 3). Final answer accuracy examples (CoT+Voting baseline vs NP variants): CoT+Voting GSM8K 87.62%, NP (no verification) GSM8K 87.05%, NP+UPV GSM8K 86.01% (Table 4).",
            "comparison_with_baseline": "Natural Program + per-step verification substantially improves verification accuracy vs verifying entire chains at once (≈69% vs ≈52%); final-answer accuracy remains similar to CoT baselines, with slight reductions after filtering.",
            "key_findings": "GPT-3.5-turbo can reliably verify short, locally-grounded deductive steps when provided only minimal necessary premises and well-structured Natural Program formatting; verifying entire CoT traces at once yields near-random or biased verification behavior.",
            "limitations": "Verification still imperfect; model tends to be distracted by irrelevant premises when verifying entire chains; fails to detect some semantic/contextual ambiguity errors; limited on tasks with long chain dependencies (Last Letters).",
            "uuid": "e6805.2",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Vicuna-7B/13B",
            "name_full": "Vicuna-7B and Vicuna-13B (LLaMA-based chat models fine-tuned on ShareGPT)",
            "brief_description": "Open-weight conversational models fine-tuned from LLaMA-7B/13B on ShareGPT-style data; evaluated for one-shot Natural Program per-step deductive verification and further fine-tuned on an automatically generated verification dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B, Vicuna-13B",
            "model_description": "Fine-tuned LLaMA-7B and LLaMA-13B variants (ShareGPT fine-tuning); tested both original and further fine-tuned versions on a deductive verification dataset.",
            "model_size": "7B / 13B",
            "architecture_type": "Transformer (LLaMA family) with conversational fine-tuning",
            "training_data": "ShareGPT fine-tuning data; additional fine-tuning performed on a 2000-step deductive verification dataset generated using GPT-3.5-turbo for experiments.",
            "reasoning_method": "Natural Program one-shot per-step verification; additional supervised fine-tuning on labeled verification examples.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Date, Last Letters (evaluated same split as GPT-3.5 experiments)",
            "benchmark_description": "Same arithmetic/math/string/date tasks used to measure per-step verification accuracy.",
            "task_type": "Per-step deductive verification",
            "performance_metric": "Per-step deductive verification accuracy (one-shot Natural Program verification).",
            "performance_value": "Original Vicuna-7B overall verification ≈54% (Table 9). Vicuna-7B after fine-tuning on verification data overall ≈61%. Original Vicuna-13B overall ≈57%; Vicuna-13B fine-tuned ≈66%. (Per-dataset variations reported in Table 9.)",
            "comparison_with_baseline": "Fine-tuning on a small supervised verification dataset improves Vicuna verification performance (e.g., Vicuna-7B 54% -&gt; 61%, Vicuna-13B 57% -&gt; 66%), but still generally lags behind GPT-3.5-turbo (≈69% overall), indicating model capacity and pretraining matter.",
            "key_findings": "Model capacity and pretraining/fine-tuning both affect deductive verification capability; small open models can be improved by supervised fine-tuning on verification data but may not match larger closed models.",
            "limitations": "Even after fine-tuning, smaller models show lower verification reliability than GPT-3.5; fine-tuning dataset construction depended on GPT-3.5 outputs (potentially propagating biases); verification performance varies materially by dataset.",
            "uuid": "e6805.3",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural language reasoning from LLMs to improve performance on multi-step tasks by exposing intermediate steps.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Various LLMs (used as baseline prompting paradigm in experiments)",
            "model_description": "Prompt-engineering technique applied to transformer LLMs to elicit multi-step reasoning in outputs (no new model introduced).",
            "model_size": null,
            "architecture_type": "Transformer (prompting technique, not an architecture change)",
            "training_data": null,
            "reasoning_method": "Generate unconstrained CoT traces; used as baseline for generation and for end-to-end verification attempts (verifying entire chain at once).",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Date, Last Letters (used as baseline generation format)",
            "benchmark_description": "Same reasoning/math/string/date benchmarks used in this paper's comparisons.",
            "task_type": "Multi-step reasoning generation and end-to-end verification",
            "performance_metric": "Final-answer accuracy (majority voting) and verification accuracy when attempting to verify entire chains.",
            "performance_value": "Final-answer (CoT+Voting) reported in Table 4: GSM8K 87.62%, AQuA 70.18%, MATH 35.93%, AddSub 92.36%, Date 69.97%, Last Letters 81.60%. End-to-end chain verification accuracy (verifying entire chain at once) ~52% average (Table 3 / Table 2 results show ~50–52%).",
            "comparison_with_baseline": "CoT improves final-answer performance relative to direct answering, but verifying entire CoT traces at once performs poorly compared to stepwise verification under Natural Program formatting.",
            "key_findings": "CoT remains effective for improving final-answer accuracy but is vulnerable to hallucinations and error accumulation; naive whole-chain self-verification by LLMs is unreliable due to distraction by irrelevant premises.",
            "limitations": "CoT traces can contain hallucinated or ungrounded intermediate steps; verifying whole CoT traces is challenging for LLMs and yields low verification accuracy.",
            "uuid": "e6805.4",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Faithful CoT",
            "name_full": "Faithful Chain-of-Thought",
            "brief_description": "A recently proposed method that aims to produce more faithful/grounded CoT traces (cited and evaluated as a baseline in the paper).",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "use",
            "model_name": "Various LLMs (baseline prompting/approach)",
            "model_description": "A method/prompting strategy intended to increase faithfulness of CoT traces (paper compared against it as a baseline).",
            "model_size": null,
            "architecture_type": "Transformer + faithful CoT prompt/regimen",
            "training_data": null,
            "reasoning_method": "Prompting approach designed to make step-by-step reasoning more faithful; used in comparison for final-answer voting experiments.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "GSM8K, AQuA, MATH, AddSub, Date (reported for some datasets)",
            "benchmark_description": "Same reasoning/math benchmarks used for comparative evaluation.",
            "task_type": "Multi-step reasoning with an emphasis on faithful/grounded steps",
            "performance_metric": "Final-answer accuracy with majority voting",
            "performance_value": "Faithful CoT + Voting reported in Table 4: GSM8K 75.80%, AQuA 61.80%, MATH 31.78% (note some dataset numbers were taken from official repo and some were reimplemented), AddSub 88.35%, Date 73.50% (Last Letters not available).",
            "comparison_with_baseline": "Faithful CoT sometimes underperforms vanilla CoT voting or Natural Program on final-answer accuracy for some datasets in these experiments.",
            "key_findings": "Faithfulness-focused CoT approaches can improve interpretability but do not uniformly outperform Natural Program formatting combined with voting on final answers.",
            "limitations": "Not all results directly comparable across datasets due to prompt and implementation differences; some dataset results unavailable.",
            "uuid": "e6805.5",
            "source_info": {
                "paper_title": "Deductive Verification of Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "faithful_chainofthought_reasoning"
        },
        {
            "paper_title": "Natural language deduction through search over statement compositions",
            "rating": 1,
            "sanitized_title": "natural_language_deduction_through_search_over_statement_compositions"
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        }
    ],
    "cost": 0.01787425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deductive Verification of Chain-of-Thought Reasoning</h1>
<p>Zhan Ling ${ }^{1 <em>}$ Yunhao Fang ${ }^{1 </em>}$ Xuanlin Li ${ }^{1}$ Zhiao Huang ${ }^{1}$ Mingu Lee ${ }^{2}$<br>Roland Memisevic ${ }^{2}$ Hao Su ${ }^{1}$<br>${ }^{1}$ UC San Diego, ${ }^{2}$ Qualcomm AI Research ${ }^{\dagger}$</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lzloceani/verify_cot.</p>
<h2>1 Introduction</h2>
<p>The transformative power of large language models, enhanced by Chain-of-Thought (CoT) prompting [50, 21, 59, 42], has significantly reshaped the landscape of information processing [14, 26, 49, 56, 13, 55, 23, 29], fostering enhanced abilities across a myriad of disciplines and sectors. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations [4, 30, 16, 20] and accumulated errors [4, 51, 1], thereby limiting models' ability to produce cogent reasoning processes.
In fact, the pursuit of reliable reasoning is not a contemporary novelty; indeed, it is an intellectual endeavor that traces its roots back to the time of Aristotle's ancient Greece. Motivated by the desire to establish a rigorous reasoning process, in his "Organon," Aristotle introduced principles of logic, in particular, syllogism, a form of logical argument that applies deductive reasoning to arrive at a conclusion based on two or more propositions assumed to be true. In disciplines that rigorous reasoning is critical, such as judical reasoning and mathematical problem solving, documents must be written in a formal language with a logical structure to ensure the validity of the reasoning process.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our proposed deductive reasoning and verification process. In response to an input question, LLMs generate deductive reasoning chains using the Natural Program format (bottom 3 boxes), a natural language-based deductive reasoning approach. The Natural Program format allows individual reasoning steps (an example in purple) and their corresponding minimal set of premises (an example in orange) to be easily extracted. This streamlined extraction process facilitates the step-by-step decomposition and verification of deductive reasoning (top-right box).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Through our Natural Program-based deductive reasoning verification approach, we identify and eliminate reasoning chains that contain errors in reasoning and grounding (we define grounding error as utilizing information that is not present in cited premises). By alleviating such errors, we significantly enhance the rigor, trustworthiness, and interpretability of the generated reasoning outputs.</p>
<p>We yearn for this sequence of reliable knowledge when answering questions. Our goal is to develop language models that can propose potential solutions through reasoning in logical structures. Simultaneously, we aim to establish a verifier capable of accurately assessing the validity of these reasoning processes. Despite recent significant explorations in the field, such as [48]'s emphasis on self-consistency and [27, 5]'s innovative use of codes to represent the reasoning process, these approaches still exhibit considerable limitations. For example, consistency and reliability are not inherently correlated; as for program codes, they are not powerful enough to represent many kinds of reasoning process, e.g., in the presence of quantifiers ("for all", "if there exists") or nuances of natural language (moral reasoning, "likely", …).</p>
<p>We propose leveraging the power of natural language to achieve the deductive reasoning emphasized in ancient Greek logic, introducing a "natural program". This involves retaining natural language for its inherent power and avoiding the need for extensive retraining with large data sets. A natural program</p>
<p>represents a rigorous reasoning sequence, akin to a computer program. We expect implementations of the idea to have two properties: 1) that natural programs are generated with minimal effort from an existing language model capable of CoT reasoning, preferably through in-context learning; 2) that the natural program can be easily verified for reliability in the reasoning process.</p>
<p>Through a step-by-step investigation, we discovered that large language models have the potential to meet our expectation. Naïve CoT prompts like "Let us think step by step." has many flaws, and entrusting the entire verification process to a large model like ChatGPT can still lead to significant error rates. However, we found that, if the reasoning process is very short, and only based on necessary premises and contexts, the verification of existing large language models is already quite reliable. Therefore, our approach is to design prompts that induce CoT processes comprised of rigorous premises/conditions and conclusions with statement labels, and verification can be done by gradually isolating very few statements within the long thought chain. Experimentally, we found that most reasoning that passed the verification was rigorous, and many that did not pass had elements of imprecision in the reasoning process, even if they occasionally arrived at correct answers.</p>
<p>It is worth emphasizing that, we are not looking for a method to just maximize the correctness rate of final answers; instead, we aspire to generate a cogent reasoning process, which is more aligned with the spirit of judical reasoning. When combined with sampling-based methods, our method can identify low-probability but rigorous reasoning processes. When repeated sampling fails to yield a rigorous reasoning process, we can output "unknown" to prevent hallucinations that mislead users.</p>
<p>We demonstrate the efficacy of our natural program-based verification approach across a range of arithmetic and common sense datasets on publicly-available models like OpenAI's GPT-3.5-turbo. Our key contributions are as follows:</p>
<ol>
<li>We propose a novel framework for rigorous deductive reasoning by introducing a "Natural Program" format (Fig. 1), which is suitable for verification and can be generated by just in-context learning;</li>
<li>We show that reliable self-verification of long deductive reasoning processes written in our Natural Program format can be achieved through step-by-step subprocesses that only cover necessary context and premises;</li>
<li>Experimentally, we demonstrate the superiority of our framework in improving the rigor, trustworthiness, and interpretability of LLM-generated reasoning steps and answers (Fig. 2).</li>
</ol>
<h1>2 Related work</h1>
<p>Reasoning with large language models. Recent large language models (LLMs) [3, 8, 57, 47, 38, 18, 9,37 ] have shown incredible ability in solving complex reasoning tasks. Instead of letting LLMs directly generate final answers as output, prior work have shown that by encouraging step-by-step reasoning through proper prompting, such as Chain-of-Thought (CoT) prompting [50] and many others [21, 59, 58, 44, 48, 60, 25, 54], LLMs exhibit significantly better performance across diverse reasoning tasks. To further improve the step-by-step reasoning process, some recent studies have investigated leveraging external solvers such as program interpreters [39, 5, 27], training and calling external reasoning modules [11], or performing explicit search to generate deductive steps [2, 46]. Parallel to these works, we do not rely on external modules and algorithms, and we directly leverage the in-context learning ability of LLMs to generate more precise and rigorous deductive reasonings.</p>
<p>Large language models as verifiers. Using language models to evaluate model generations has been a long standing idea [22, 36, 40, 4]. As LLMs exhibit impressive capabilities across diverse tasks, it becomes a natural idea to use LLMs as evaluation and verification tools. For example, [10, 11, 33] finetune LLMs to verify solutions and intermediate steps. LLMs aligned with RLHF [32, 31, 48] have also been employed to compare different model generations. In addition, recent works like [43, 52, 28, 6] leverage prompt designs to allow LLMs to self-verify, self-refine, and self-debug without the need for finetuning. However, these works do not focus on the rigor and trustworthiness of the deductive reasoning processes at every reasoning step. In this work, we propose a natural language-based deductive reasoning format that allows LLMs to self-verify every intermediate step of a deductive reasoning process, thereby improving the rigor and trustfulness of reasoning.</p>
<p>Question: Jerry is rolling a six-sided die. How much more likely is it (expressed as a percentage) that he rolls a number greater than $\$ 3$ than that he rolls two even numbers in a row?
Ground Truth: There are 3 numbers greater than 3 on the dice, so the chances of rolling one of them are $3 / 6=50 \%$. The chance of rolling one even number is $50 \%$, so the chance of rolling two in a row is $50 \% * 50 \%=25 \%$. The difference between these two probabilities is $50 \%$ - $25 \%$ $=25 \% . # # # # 25$
Output: There are two possible outcomes for rolling a number greater than 3 out of 6 total outcomes, so the probability is $2 / 6$ or $1 / 3$. For rolling two even numbers in a row, there are three possible outcomes (2-4, 4-6, 6-2) out of 36 total outcomes, so the probability is $3 / 36$ or $1 / 12$. To find the difference in probability, we subtract the probability of rolling two even numbers in a row from the probability of rolling a number greater than 3: $1 / 3$ - $1 / 12=1 / 4$. This means that it is $25 \%$ more likely to roll a number greater than 3 than to roll two even numbers in a row.</p>
<p>Table 1: An example question from GSM8K with a generated CoT reasoning path with GPT3.5 (turbo), where the output provides a wrong reasoning chain with the correct answer.</p>
<p>Additionally, while some recent works [12, 53, 15, 34] have proposed methods to verify individual steps in a reasoning process, our approach distinguishes from these works in the following perspectives: (1) Our approach leverages in-context learning to achieve reasoning verification, without the need for language model finetuning. (2) Our Natural Program-based LLM verification approach not only identifies invalid reasoning steps, but also provides explicit explanations for why they are invalid, detailing the specific reasoning errors involved. (3) Our Natural Program-based reasoning and verification approach is compatible with in-context abstract reasoning tasks where reasoning steps do not possess proof-like entailment structures. For example, our approach is compatible with the Last Letters task, where the LLM is instructed to output the concatenation of the last letters of all words in a sequence as the final answer. (4) Our Natural Program approach allows the use of commonsense knowledge not explicitly listed in premises. For example, consider this problem: "Marin eats 4 apples a day. How many apples does he eat in November?" Even though "November has 30 days" is not explicitly listed in the premises, Natural Program permits the use of such common knowledge within a reasoning step. Our in-context verification process is also capable of handling these implicit premises (e.g., if LLM outputs "November has 29 days" in a reasoning step, it will be marked as invalid).</p>
<h1>3 Motivation and Problem Formulation</h1>
<p>A reasoning-based question-answering (QA) task can be defined as a tuple $(Q, C, O, A)$ [35], where $Q$ is the target question; $C$ is the context of a question, such as the necessary background for answering a question; $O=\left(o_{1}, o_{2}, \cdots, c_{k}\right)$ are optional answer choices if $Q$ is a K-way multiple choice problem; and $A$ is the ground-truth answer. Given $Q$ and $C$ as inputs, large language models (LLMs) [3, 8, 47] generate a sequence of tokens $T=\left(t_{1}, t_{2}, \cdots, t_{n}\right)$ to answer the question. Recent works like Chain-of-Thought (CoT) [50, 21] leverage prompt engineering in the context $C$ to encourage models to generate the intermediate reasoning process in $T$, which benefits LLM performance across diverse reasoning tasks. In this case, $T$ consists of a set of $m$ intermediate reasoning steps, which we denote as $S=\left(s_{1}, s_{2}, \cdots, s_{m}\right)$. Each step $s_{i}$ can be represented by a subsequence of the generated tokens $\left(t_{l_{i}}, t_{r_{i}}\right) \subseteq T$. In much prior work, a generated solution is "correct" if and only if the predicted final answer in $s_{m}$ matches the ground truth $A$, which we call answer correct(ness).
We observe that for all cases where LLMs produce erroneous final answers, there exists at least one mistake among the intermediate reasoning steps $S$. Moreover, even when the final answer is correct, there might still exist some mistakes among $S$. This phenomenon, as illustrated in Tab. 1, occurs for all LLMs we tested, including state-of-the-art models such as ChatGPT and GPT-4 [32]. Since later reasoning steps are conditioned on prior reasoning steps, these mistakes often initiate a snowball effect, causing subsequent mistakes to compound. This significantly diminishes the likelihood of correct problem-solving and impedes the progress towards achieving human-level complex reasoning.</p>
<p>Therefore, in this work, we place significant emphasis on ensuring the validity of every reasoning step, not just the correctness of the final answer. In particular, we focus on the validity of deductive reasoning, an essential component of a logical reasoning process. In deductive reasoning, we are</p>
<table>
<thead>
<tr>
<th>Prompting</th>
<th>Reasoning Correctness</th>
<th>GSM8K</th>
<th>AQuA</th>
<th>MATH</th>
<th>AddSub</th>
<th>Date</th>
<th>Last Letters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot</td>
<td>Correct</td>
<td>0.98</td>
<td>0.96</td>
<td>1.00</td>
<td>0.98</td>
<td>0.98</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Incorrect</td>
<td>0.04</td>
<td>0.06</td>
<td>0.04</td>
<td>0.02</td>
<td>0.04</td>
<td>0.04</td>
</tr>
<tr>
<td></td>
<td>(Average)</td>
<td>0.51</td>
<td>0.51</td>
<td>0.52</td>
<td>0.50</td>
<td>0.51</td>
<td>0.52</td>
</tr>
<tr>
<td>Two-shot</td>
<td>Correct</td>
<td>0.98</td>
<td>0.96</td>
<td>1.00</td>
<td>0.92</td>
<td>1.00</td>
<td>0.96</td>
</tr>
<tr>
<td></td>
<td>Incorrect</td>
<td>0.02</td>
<td>0.04</td>
<td>0.00</td>
<td>0.06</td>
<td>0.26</td>
<td>0.06</td>
</tr>
<tr>
<td></td>
<td>(Average)</td>
<td>0.50</td>
<td>0.50</td>
<td>0.50</td>
<td>0.49</td>
<td>0.63</td>
<td>0.51</td>
</tr>
</tbody>
</table>
<p>Table 2: Zero-shot and two-shot reasoning chain verification accuracy for GPT-3.5-turbo (ChatGPT), where an entire reasoning chain is verified at once. The two shot prompt we used is presented in Appendix D.1. To generate verification inputs, for each dataset, we perform Chain-of-Thought (CoT) prompting and randomly sample 50 reasoning chains that are valid and 50 reasoning chains that exhibit mistakes. We observe that when given an entire reasoning process, where the deductive graphs for all reasoning steps are entangled together, it is challenging even for strong language models like ChatGPT to verify its validity.
given a (premise, conclusion) pair, and we are interested in determining whether the conclusion follows from the premises. In the context of reasoning-based QA tasks, for each reasoning step $s_{i}$, we define its deductive validity $V\left(s_{i}\right)$ as a binary variable. A reasoning step is deductively valid $\left(V\left(s_{i}\right)=1\right)$ if and only if $s_{i}$ can be logically deduced from its corresponding premises $p_{i}$, which consist of the context $C$, the question $Q$, and all the previous reasoning steps $s_{j}(j&lt;i)$. Then, we can also define the deductive validity for the entire reasoning chain $S$ as $V(S)=\wedge_{i=1}^{M} V\left(s_{i}\right)$. Compared to evaluating answer correctness, which can be accomplished by simple functions such as exact string match, evaluating deductive validity is a lot more challenging. Thanks to the recent progress on LLMs, which demonstrate impressive in-context learning capabilities across diverse scenarios, we propose to use LLMs to examine reasoning chains and predict the deductive reasoning validity.</p>
<h1>4 Deductively Verifiable Chain-of-Thought Reasoning</h1>
<p>In this section, we introduce our specific approaches to performing deductive verification of reasoning chains. Specifically, we first introduce our motivation and method for decomposing a deductive verification process into a series of step-by-step processes, each only receiving contexts and premises that are necessary. Then, we propose Natural Program, a natural language-based deductive reasoning format, to facilitate local step-by-step verification. Finally, we show that by integrating deductive verification with unanimity-plurality voting, we can improve the trustworthiness of reasoning processes along with final answers. An overview of our approach is illustrated in Fig. 1 and Fig. 2.</p>
<h3>4.1 Decomposition of Deductive Verification Process</h3>
<p>Given a reasoning chain $S=\left(s_{1}, s_{2}, \cdots, s_{n}\right)$, a straightforward idea to verify its deductive validity is to ask LLMs to examine the entire reasoning chain at once. To assess the effectiveness of this approach, we conduct a preliminary experiment: for a dataset problem and its reasoning chain $S$ generated by ChatGPT, we prompt ChatGPT with "Do you think the above reasoning process is correct? Let's think step by step" such that its outputs whether there exists any mistake among any reasoning step in $S$. However, as demonstrated in Tab. 2, the verification accuracy is $50 \%$ for most datasets, and ChatGPT struggles at finding out mistaken reasonings. Notably, it persistently outputs "Correct" for most reasoning chain queries, regardless of their actual validity.
We conjecture that such phenomenon is caused by the abundance of irrelevant premises for each reasoning step. Recall that the premises $p_{i}$ for a reasoning step $s_{i}$ consist of the the question $Q$, the question context $C$, along with the prior reasoning steps $s_{\leq j}=\left{s_{j}: j&lt;i\right}$. For $Q$ and $C$, we can further extract and decompose $Q \cup C$ into a set of "question-related premises" $Q C=\left{q c_{1}, q c_{2}, \cdots, q c_{m}\right}$, where $q c_{i}$ is a premise or condition inferred from $Q \cup C$. Then, it is often the case that most elements of $p_{i}=Q C \cup s_{\leq j}$ are irrelevant to the validity of $s_{i}$, leading to erroneous verifications from language models. A very recent work [41] also observes a similar phenomenon where LLMs are easily distracted by irrelevant context.
Hence, we propose a decomposition of the reasoning chain verification process into a series of step-by-step processes, where each step only considers the premises that are necessary. The overall validity</p>
<p>of the reasoning chain, denoted as $V(S)=\wedge_{i=1}^{M} V\left(s_{i}\right)$, can be naturally decomposed into individual step validity $V\left(s_{i}\right)$. However, achieving such decomposition is highly challenging without imposing constraints on the format of reasoning chains. Additionally, for each $s_{i} \in S$, we aim to ensure that it explicitly lists the minimal subset of premises $\bar{p}<em i="i">{i} \subseteq p</em>$ required for deductive reasoning to avoid potential ambiguities during verification. This motivates us to introduce a natural-language-based deductive reasoning format in Section 4.2.</p>
<h1>4.2 Natural Program Deductive Reasoning Format</h1>
<p>As previously mentioned in Sec. 4.1, we desire LLMs to output deductive reasoning processes that can be easily verified by themselves, specifically by listing out the minimal set of necessary premises $p_{i}$ at each reasoning step $s_{i}$. To accomplish its goal, we propose to leverage the power of natural language, which is capable of rigorously representing a large variety of reasoning processes and can be generated with minimal effort. In particular, we introduce Natural Program , a novel deductive reasoning format for LLMs. More formally, Natural Program consists of the following components:</p>
<ul>
<li>An instruction for models to extract question-related premises $Q C$. We use the following instruction: "First, let's write down all the statements and relationships in the question with labels".</li>
<li>A numbered-list of question-related premises, each prefixed with "#{premise_number}".</li>
<li>An instruction for models to generate the reasoning chain $S$ based on the question-related premises $Q C$. We use the following instruction: "Next, let's answer the question step by step with reference to the question and reasoning process".</li>
<li>A list of prefixed reasoning steps $S_{i}$. The prefix has the following format:
#{number} (by {list_of_premises_used}}. Here "number" equals $|Q C|+i$, and "list_of_premises_used" consists of numbers from the smallest subset of premises among $Q C \cup s_{\leq j}$ that are used for the deductive reasoning of $s_{i}$. In addition, for the last reasoning step $s_{m}$, we ensure that it (1) includes a special tag Final Step; (2) refers to the premise number of the target question to be answered; (3) explicitly gives the final answer to a question.</li>
</ul>
<p>To encourage language models to reason in the Natural Program format, we have designed oneshot prompts for different datasets, which are shown Appendix D.2. Given that LLM's reasoning outputs follow the Natural Program format, we can then verify the deductive validity of a single reasoning step $s_{i}$ through an instruction that consists of (1) the full descriptions of premises used for the reasoning of $s_{i}$; (2) the full description of $s_{i}$; (3) an instruction for validity verification, such as "Double-check the reasoning process, let's analyze its correctness, and end with "yes" or "no"." Note that throughout this verification process, we only retain the minimal necessary premise and context for $s_{i}$, thereby avoiding irrelevant context distraction and significantly improving the effectiveness of validation. Additionally, we employ a one-shot prompt for this verification process, which we find very helpful for improving the verification accuracy. The prompt is shown in Appendix D.3.</p>
<p>Figure 1 provides an overview of the complete Natural Program-based deductive reasoning and verification process. By using the Natural Program approach, we demonstrate that LLMs are capable of performing explicit, rigorous, and coherent deductive reasoning. Furthermore, Natural Program enables LLMs to self-verify their reasoning processes more effectively, enhancing the reliability and trustworthiness of the generated responses.</p>
<h3>4.3 Integrating Deductive Verification with Unanimity-Plurality Voting</h3>
<p>Given that we can effectively verify a deductive reasoning process, we can naturally integrate verification with LLM's sequence generation strategies to enhance the trustworthiness of both the intermediate reasoning steps and the final answers. In this work, we propose Unanimity-Plurality Voting, a 2-phase sequence generation strategy described as follows. Firstly, similar to prior work like [48], we sample $k$ reasoning chain candidates along with their final answers. In the unanimity phase, we perform deductive validation on each reasoning chain. Recall that a chain $S$ is valid (i.e., $V(S)=1$ ) if and only if all of its intermediate reasoning steps are valid (i.e., $\forall i, V\left(s_{i}\right)=1$ ). For each intermediate reasoning step $s_{i}$, we perform majority voting over $k^{\prime}$ sampled single-step validity</p>
<p>predictions to determine its final validity $V(s_{i})$. We then only retain the verified chain candidates ${S: V(S)=1}$. In the plurality voting stage, we conduct a majority-based voting among the verified chain candidates to determine the final answer. This voting process ensures that the final answer is selected based on a consensus among the trustworthy reasoning chains.</p>
<h1>5 Experiments</h1>
<p>In this section, we perform evaluations to demonstrate the effectiveness of our Natural Program-based deductive reasoning verification approach over diverse reasoning datasets. Firstly, we show that our deductive verification process leads to substantial improvements in the rigor and reliability of reasoning chains. Subsequently, we will examine the impact of deductive verification on the accuracy of final answers. Our findings reveal that by adopting our Natural Program reasoning format without verification, we improve answer correctness on challenging benchmarks. Further applying deductive verification leads to slight reductions in final answer accuracy. One reason for this phenomenon is that the verification process effectively identifies and eliminates flawed reasoning chains that still produce correct answers.</p>
<h3>5.1 Experimental Setup</h3>
<p>Benchmarks. We evaluate the deductive verification accuracy and the answer correctness of reasoning chains over a diverse set of reasoning tasks: arithmetic reasoning, symbol manipulation, and date understanding. For arithmetic reasoning, we utilize the following benchmarks: 1) AddSub [19]; 2) GSM8K [10]; 3) MATH [17]; 4) AQuA [24]. Among these benchmarks, the AddSub and GSM8K datasets involve middle school-level multi-step calculations to arrive at a single number as the final answer. The MATH dataset presents more challenging problems that require expressing the answer as a mathematical expression in LaTeX format. These problems involve concepts from linear algebra, algebra, geometry, calculus, statistics, and number theory. AQuA also features similarly challenging problems, except that questions are in a multiple-choice format. For symbol manipulation, we use Last Letter Concatenation [50], where the model is tasked with concatenate the last letters of all the words provided in the question. For date understanding, we use the one from BIG-bench [45]
Deductive verfication evaluation setup. For each of the above benchmarks, we select 100 reasoning chains, where 50 of them are deductively valid and 50 of them exhibit reasoning mistakes. The ground-truth deductive validity of each reasoning chain is determined by human annotators.
Answer extraction. To extract answers from reasoning solutions, we first perform text splitting based on answer prefix patterns such as "answer is" or "option is". Then, using problem type-specific regular expressions, we extract the final answer. To extract the validity results from deductive verification processes, we only keep the last sentence of model response. We then extract the validity answer with regular expressions to obtain attitude words, e.g., "yes" or "no", to determine the validity answer. Sometimes, language models may not provide a direct answer and instead output phrases like "not applicable" at the end of the response. In such cases, we consider the answer from the model as "yes". Please refer to Appendix C for more details.
Model and Hyperparameters. We conduct our main experiments with GPT-3.5-turbo (ChatGPT) [32]. We also present results for the LLama model-family [47]) in Appendix A, where we find the deductive verification accuracy to be worse than larger models even after finetuning. For ChatGPT, we use a generation temperature of $T=0.7$. For Unanimity-Plurality Voting, we set $k=10$ and $k^{\prime}=3$ by default. We use 1-shot prompting for both reasoning chain generation and deductive verification (except reasoning chain generation for the date understanding task where we use 2-shot). See Appendix D. 2 and Appendix D. 3 for more details.</p>
<h3>5.2 Comparison of Deductive Verification Accuracy</h3>
<p>We compare the verification accuracy of reasoning chains using two methods: (1) verifying the entire reasoning chain at once (as described in Section 4.1) without utilizing the Natural Program, and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Verification Method</th>
<th style="text-align: center;">Reasoning Correctness</th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Last Letters</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$96 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$92 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$96 \%$</td>
<td style="text-align: center;">$97 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Two-shot</td>
<td style="text-align: center;">Incorrect</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$26 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$7 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$49 \%$</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$51 \%$</td>
<td style="text-align: center;">$52 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Natural Program</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">$84 \%$</td>
<td style="text-align: center;">$72 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$95 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$96 \%$</td>
<td style="text-align: center;">$85 \%$</td>
</tr>
<tr>
<td style="text-align: center;">One-shot</td>
<td style="text-align: center;">Incorrect</td>
<td style="text-align: center;">$84 \%$</td>
<td style="text-align: center;">$62 \%$</td>
<td style="text-align: center;">$76 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$56 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$54 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">$\mathbf{8 4 \%}$</td>
<td style="text-align: center;">$\mathbf{6 7 \%}$</td>
<td style="text-align: center;">$\mathbf{7 3 \%}$</td>
<td style="text-align: center;">$\mathbf{6 8 \%}$</td>
<td style="text-align: center;">$\mathbf{7 3 \%}$</td>
<td style="text-align: center;">$51 \%$</td>
<td style="text-align: center;">$\mathbf{6 9 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of deductive verification accuracy of reasoning chains for GPT-3.5-turbo (ChatGPT). We compare two approaches: (1) verifying entire reasoning chains generated by Chain-ofThought prompting; (2) verifying reasoning chains generated in the Natural Program format with step-by-step decomposition. In the latter case, when we verify each reasoning step $s_{i}$, we only keep the necessary subset of premises $\bar{p}<em i="i">{i} \subseteq p</em>$. To calculate verification accuracy, for each dataset, we randomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">MATH $^{+}$</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Last Letters</td>
</tr>
<tr>
<td style="text-align: center;">CoT + Voting</td>
<td style="text-align: center;">87.62\%</td>
<td style="text-align: center;">70.18\%</td>
<td style="text-align: center;">35.93\%</td>
<td style="text-align: center;">92.36\%</td>
<td style="text-align: center;">69.97\%</td>
<td style="text-align: center;">81.60\%</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT + Voting</td>
<td style="text-align: center;">75.80\%</td>
<td style="text-align: center;">61.80\%</td>
<td style="text-align: center;">31.78\% ${ }^{1}$</td>
<td style="text-align: center;">88.35\% ${ }^{1}$</td>
<td style="text-align: center;">73.50\%</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ours (Natural Program (NP), No Verification)</td>
<td style="text-align: center;">87.05\%</td>
<td style="text-align: center;">70.34\%</td>
<td style="text-align: center;">36.75\%</td>
<td style="text-align: center;">93.67\%</td>
<td style="text-align: center;">72.49\%</td>
<td style="text-align: center;">92.98\%</td>
</tr>
<tr>
<td style="text-align: center;">Ours (NP + Deductive Verification + UPV)</td>
<td style="text-align: center;">86.01\%</td>
<td style="text-align: center;">69.49\%</td>
<td style="text-align: center;">36.48\%</td>
<td style="text-align: center;">93.54\%</td>
<td style="text-align: center;">71.45\%</td>
<td style="text-align: center;">92.60\%</td>
</tr>
</tbody>
</table>
<p>Table 4: Final answer accuracy comparison on GPT-3.5-turbo (ChatGPT). All approaches generate $k=10$ reasoning chains for each problem before performing majority voting or reasoning chain filtering with our deductive verification approach.
(2) our Natural Program-based verification approach with step-by-step decomposition. The results, presented in Table 3, indicate that our approach achieves significantly higher reasoning verification accuracy across most datasets. It effectively identifies erroneous reasoning in faulty chains while maintaining a low rate of false positives for valid chains. However, we observe that our approach's effectiveness is limited on the "Last Letters" task. We hypothesize that this is due to the task's nature, where each subsequent reasoning step is conditioned on all previous steps, presenting greater challenges for reasoning verification due to the increased dependency among premises.</p>
<h1>5.3 Impact of Natural Program and Deductive Verification on Final Answer Correctness</h1>
<p>We then investigate the impact of our Natural Program reasoning format and our deductive verification process on final answer correctness. We conduct two experiments: (1) for each problem, we instruct language models to generate $k=10$ reasoning chain candidates in the Natural Program (NP) format and perform simple majority voting on final answers, without using deductive verification to filter out reasoning chain candidates; (2) applying our deductive verification approach to filter out reasoning chain candidates, and apply Unanimity-Plurality Voting (UPV) along the process to determine the final answer. As a reference, we also report the performance of Chain-of-Thought (CoT) [50] and Faithful CoT [27]. For these baselines, we perform simple answer-based majority voting with $k=10$ for fair comparison.</p>
<p>Results are presented in Tab. 4. While our major goal is to improve the trustworthiness and reliability of deductive reasoning, we find that prompting language models to reason in our Natural Program format achieves on-par or better final answer accuracy than baselines over many reasoning tasks. Upon further applying our deductive verification approach to filter out invalid reasoning chains, we observe a slight decrease in final answer accuracy. One major contributing factor to this decrease is the filtering out of reasoning chain candidates that provide correct answers but exhibit incorrect reasoning. We illustrate an example in Table 5, where ChatGPT generates the correct final answer but assigns incorrect premise numbers to support the first reasoning step. We note that in many such cases, our approach effectively identifies these reasoning errors, thereby enhancing the rigor and</p>
<p>Question: In March it rained 0.81 inches. It rained 0.35 inches less in April than in March.
How much did it rain in April?
Ground Truth: It rained $0.81-0.35=0.46$ inches.
GPT-3.5-turbo (ChatGPT):
Question-related premises:
#1. It rained 0.81 inches in March.
#2. It rained 0.35 inches less in April than in March.
#3. How much did it rain in April?</p>
<h1>The first reasoning step:</h1>
<p>#4. (by #2) Step 1: Calculate how much it rained in April by subtracting the difference of 0.35 inches from the amount in March.
Amount of rain in April: $0.81-0.35=0.46$ inches
Table 5: For this reasoning chain candidate of this GSM8k problem, ChatGPT generates the correct final answer but provides incorrect premise numbers for grounding the first reasoning step. In "Step 1", the correct premise numbers should be $# 1$ and $# 2$. Our deductive reasoning verification approach effectively identifies these reasoning errors, enhancing the rigor and trustworthiness of the reasoning process. It is worth noting that removing a correct answer from the candidate reasoning chains has a slightly negative impact on the overall final answer correctness.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Premise Context</th>
<th style="text-align: center;"># Shots</th>
<th style="text-align: center;">Reasoning Correctness</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Last Letters</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Full Premises</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">64\%</td>
<td style="text-align: center;">54\%</td>
<td style="text-align: center;">58\%</td>
<td style="text-align: center;">95\%</td>
<td style="text-align: center;">26\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">66\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">24\%</td>
<td style="text-align: center;">76\%</td>
<td style="text-align: center;">5\%</td>
<td style="text-align: center;">48\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">61\%</td>
<td style="text-align: center;">57\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">51\%</td>
<td style="text-align: center;">51\%</td>
<td style="text-align: center;">57\%</td>
</tr>
<tr>
<td style="text-align: center;">Minimal Premises</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">78\%</td>
<td style="text-align: center;">90\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">90\%</td>
<td style="text-align: center;">12\%</td>
<td style="text-align: center;">75\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">26\%</td>
<td style="text-align: center;">12\%</td>
<td style="text-align: center;">28\%</td>
<td style="text-align: center;">20\%</td>
<td style="text-align: center;">20\%</td>
<td style="text-align: center;">80\%</td>
<td style="text-align: center;">31\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">55\%</td>
<td style="text-align: center;">45\%</td>
<td style="text-align: center;">59\%</td>
<td style="text-align: center;">58\%</td>
<td style="text-align: center;">55\%</td>
<td style="text-align: center;">46\%</td>
<td style="text-align: center;">53\%</td>
</tr>
<tr>
<td style="text-align: center;">Minimal Premises</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">95\%</td>
<td style="text-align: center;">90\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">85\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">62\%</td>
<td style="text-align: center;">76\%</td>
<td style="text-align: center;">40\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">54\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">51\%</td>
<td style="text-align: center;">69\%</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study on the impact of (1) premise context and (2) zero-shot vs. few-shot scenarios on deductive verification accuracy using our Natural Program-based approach with step-by-step reasoning chain decomposition. To verify each reasoning step $s_{i}$, we either the full premises $p_{i}=Q C \cup S_{\leq j}$, or use the minimal subset of premises $\bar{p}<em i="i">{i} \subseteq p</em>$ necessary as outlined in Sec. 4.1 The one-shot prompt we used is shown in Appendix D.3. For each dataset, we randomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.
reliability of the language models' reasoning processes, albeit with a slight negative impact on the overall final answer correctness. Further discussions are presented in Appendix B.</p>
<h3>5.4 Ablation Study</h3>
<p>In addition, we perform several ablation studies to gain further insights into the designs of our deductive verification approach. In Tab. 6, we compare two different approaches to verify a single reasoning step $s_{i} \in S$ following our Natural Program format. The first approach utilizes all premises $p_{i}=Q C \cup S_{\leq j}$ for verification regardless of their relevance to $s_{i}$, potentially introducing irrelevant contexts. The second approach follows our design in Sec. 4.1 and only includes the necessary context and premises $\bar{p}<em i="i">{i} \subseteq p</em>$. We observe that removing irrelevant premises significantly improves the reasoning chain verification accuracy on many datasets, highlighting the importance of this technique.
We also ablate on our Unanimity-Plurality Voting strategy by investigating the impact of different $k^{\prime}$. Recall that $k^{\prime}$ determines the number of votes to produce validity predictions of single-step reasoning. Results are shown in Tab. 7. We observe that increasing $k^{\prime}$ generally enhances reasoning validation accuracy, though we note that this is at the expense of more compute.</p>
<h2>6 Limitations</h2>
<p>While we have demonstrated the effectiveness of Natural Program-based deductive reasoning verification to enhance the trustworthiness and interpretability of reasoning steps and final answers, it is</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer Correctness</th>
<th style="text-align: center;">$k^{\prime}=1$</th>
<th style="text-align: center;">$k^{\prime}=3$</th>
<th style="text-align: center;">$k^{\prime}=5$</th>
<th style="text-align: center;">$k^{\prime}=10$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">$86 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$92 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">$40 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation of different values of $k^{\prime}$ on the verification accuracy of reasoning chains using our Unanimity-Plurality Voting strategy. Experiments are performed on AddSub using GPT-3.5-turbo (ChatGPT).</p>
<p>Question: Melanie had 10 quarters and 17 pennies in her bank. Her dad gave her 27 pennies and her mother gave her 19 pennies. How many pennies does Melanie have now?
Ground Truth: Melanie have $17+27+19=63$ pennies.</p>
<h1>ChatGPT's reasoning step:</h1>
<p>#5. (by #1) Step 1: Calculate the number of pennies Melanie had initially.
Number of pennies in 10 quarters: $10 * 25=250$
Number of pennies initially: $250+17=267$
Table 8: An example question with ambiguous wordings. The term "pennies" in this question can be interpreted as either a type of coin or a unit of currency. In this particular question, "pennies" is treated as a type of coin. However, the initial reasoning step by ChatGPT mistakenly treats "pennies" as a unit of currency, resulting in the conversion of all Melanie's money into "pennies" (highlighted in red). Consequently, all subsequent reasoning steps follow this flawed logic, leading to an incorrect reasoning trace. Our deductive verification is not yet able to detect such errors.
important to acknowledge that our approach has limitations. In this section, we analyze a common source of failure cases to gain deeper insights into the behaviors of our approach. The failure case, as shown in Tab. 8, involves the ambiguous interpretation of the term "pennies," which can be understood as either a type of coin or a unit of currency depending on the context. The ground truth answer interprets "pennies" as coins, while ChatGPT interprets it as a unit of currency. In this case, our deductive verification process is incapable of finding such misinterpretations. Contextual ambiguities like this are common in real-world scenarios, highlighting the current limitation of our approach.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we aim to enable Large Language Models (LLMs) to perform explicit and rigorous deductive reasoning while ensuring the trustworthiness of their reasoning processes through selfverification. To this end, we have proposed a novel framework based on "Natural Program", a natural language-based deductive reasoning format that facilitates reasoning verification and can be easily generated through in-context learning. Within this framework, we decompose the verification process of complex reasoning chains into step-by-step subprocesses that focus solely on necessary context and premises, allowing us to significantly enhance the accuracy of verification. Additionally, we introduce a Unanimity-Plurality Voting strategy to further improve verification accuracy. Experimentally, we demonstrate the superiority of our framework in improving the rigor, trustworthiness, and interpretability of reasoning steps and answers.
Broader Impact. While our deductive verification approach can mitigate hallucinations and reasoning errors of Large Language Models (LLMs), it does not completely eliminate these phenomena. LLMs can still produce harmful and biased content, make incorrect claims, and produce wrongful advice. This issue becomes particularly significant when LLMs engage in complex reasoning chains, increasing the risk of misleading users. Consequently, it is still crucial for users to exercise great caution when interacting with, deploying, or developing LLM-based applications.</p>
<h2>Acknowledgements</h2>
<p>We would like to express our sincere gratitude to Tongzhou Mu and Caiwei Xiao from UC San Diego, Kairong Luo from Tsinghua University, and Pulkit Madan, Reza Pourreza, Sunny Panchal, and Apratim Bhattacharyya from Qualcomm for their valuable discussions and feedback.</p>
<h1>References</h1>
<p>[1] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022.
[2] Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. Natural language deduction through search over statement compositions. arXiv preprint arXiv:2201.06028, 2022.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[4] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.
[6] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.
[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023.
[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[11] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.
[12] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, 2023.
[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.
[14] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.
[15] Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.
[16] Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André FT Martins. Hallucinations in large multilingual translation models. arXiv preprint arXiv:2303.16104, 2023.
[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[18] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>[19] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523-533, 2014.
[20] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.
[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[22] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281, Baltimore, Maryland, June 2014. Association for Computational Linguistics.
[23] Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 537-563, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
[24] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
[25] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.
[26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.
[27] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.
[28] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
[29] Ana Marasović, Iz Beltagy, Doug Downey, and Matthew E. Peters. Few-shot self-rationalization with natural language prompts, 2022.
[30] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.
[31] OpenAI. Gpt-4 technical report, 2023.
[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[33] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.
[34] Archiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. Receval: Evaluating reasoning chains via correctness and informativeness. 2023.
[35] Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Wang, Zhiheng Huang, et al. Street: A multi-task structured reasoning and explanation benchmark. arXiv preprint arXiv:2302.06729, 2023.
[36] Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.
[37] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[39] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[40] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.
[41] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. arXiv preprint arXiv:2302.00093, 2023.
[42] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022.
[43] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[44] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022.
[45] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
[46] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 3621-3634. Association for Computational Linguistics, 2021.
[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[48] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[51] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.
[52] Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.
[53] Kaiyu Yang, Jia Deng, and Danqi Chen. Generating natural language proofs with verifier-guided search. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.
[54] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[55] Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner bootstrapping reasoning with reasoning. 2022.
[56] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[58] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.
[59] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
[60] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.</p>
<h1>A Deductive Verification with Vicuna Models</h1>
<p>We further explore the efficacy of deductive verification for open-source models. We select two popular models: Vicuna-7B and Vicuna-13B [7]. These models are fine-tuned versions of LLaMA-7B and LLaMA-13B [47] using the ShareGPT data ${ }^{3}$. We use the same Natural Program-based one-shot verification method we used in the main paper. Results are shown in the first and the third rows of Table 9. We observe for the original Vicuna models without finetuning, Vicuna-7B exhibits poor performance in deductive verification and fails to find out reasoning mistakes, while the larger Vicuna-13B exhibits better verification accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Reasoning Correctness</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Last Letters</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">80\%</td>
<td style="text-align: center;">86\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">98\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">80\%</td>
<td style="text-align: center;">89\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">14\%</td>
<td style="text-align: center;">22\%</td>
<td style="text-align: center;">16\%</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">20\%</td>
<td style="text-align: center;">34\%</td>
<td style="text-align: center;">19\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">47\%</td>
<td style="text-align: center;">54\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">52\%</td>
<td style="text-align: center;">58\%</td>
<td style="text-align: center;">57\%</td>
<td style="text-align: center;">54\%</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B <br> (fine-tuned)</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">48\%</td>
<td style="text-align: center;">46\%</td>
<td style="text-align: center;">76\%</td>
<td style="text-align: center;">46\%</td>
<td style="text-align: center;">32\%</td>
<td style="text-align: center;">53\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">86\%</td>
<td style="text-align: center;">54\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">69\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">50\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">61\%</td>
<td style="text-align: center;">50\%</td>
<td style="text-align: center;">61\%</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">86\%</td>
<td style="text-align: center;">82\%</td>
<td style="text-align: center;">92\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">74\%</td>
<td style="text-align: center;">84\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">32\%</td>
<td style="text-align: center;">36\%</td>
<td style="text-align: center;">20\%</td>
<td style="text-align: center;">20\%</td>
<td style="text-align: center;">34\%</td>
<td style="text-align: center;">30\%</td>
<td style="text-align: center;">29\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">59\%</td>
<td style="text-align: center;">59\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">58\%</td>
<td style="text-align: center;">53\%</td>
<td style="text-align: center;">52\%</td>
<td style="text-align: center;">57\%</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B <br> (fine-tuned)</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">74\%</td>
<td style="text-align: center;">50\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">86\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">12\%</td>
<td style="text-align: center;">58\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">76\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">62\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">74\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">63\%</td>
<td style="text-align: center;">64\%</td>
<td style="text-align: center;">77\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">54\%</td>
<td style="text-align: center;">66\%</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT <br> (GPT-3.5-Turbo)</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">95\%</td>
<td style="text-align: center;">90\%</td>
<td style="text-align: center;">96\%</td>
<td style="text-align: center;">85\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">62\%</td>
<td style="text-align: center;">76\%</td>
<td style="text-align: center;">40\%</td>
<td style="text-align: center;">56\%</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">54\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Average)</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">68\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">51\%</td>
<td style="text-align: center;">69\%</td>
</tr>
</tbody>
</table>
<p>Table 9: One-shot Deductive Verification Accuracy of Vicuna-7B and Vicuna-13B. The models are evaluated with or without finetuning on our deductive verification dataset. For each dataset, we randomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.</p>
<p>We therefore conduct an additional experiment to investigate if the verification accuracy of Vicuna models can be improved by fine-tuning. To this end, we generate a deductive verification dataset, which consists of 2000 reasoning steps evenly distributed between correct and incorrect categories. We automatically generate this dataset using GPT-3.5-turbo since it exhibits a very high accuracy of single-step verification. We first use GPT-3.5-turbo to generate solutions for problems in GSM8K's training set. We then execute step-by-step deductive verification on these solutions using GPT-3.5turbo. For solutions that result in correct final answers, we retain the reasoning steps that pass deductive verification. For solutions that yield incorrect final answers, we retain the reasoning steps that cannot pass deductive verification. After constructing our dataset, we then fine-tune the Vicuna models using the verifications of the 2000 reasoning steps. Models were fine-tuned with 4 A100-80GB over 3 epochs. Training parameters are shown in Table 10.</p>
<p>As shown in Tab. 9, we observe that fine-tuning with our dataset can enhance the deductive verification accuracy of Vicuna models not only on the dataset where the training dataset is constructed (GSM8K), but also on many other datasets. However, the accuracy is still worse than non-finetuned GPT-3.5, which suggests that model capacity has a significant impact on deductive verification capabilities.</p>
<h2>B More Discussion on Improvements of Deductive Verification Accuracy Versus Improvements on Final Answer Correctness</h2>
<p>In the main paper, we demonstrated that our verification approach significantly improves the verification accuracy of reasoning chains (Tab. 3, 6, but barely improves the final answer accuracy (Tab. 4). We further analyze this phenomenon below:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameters</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$1 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Weight decay</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">Num epochs</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate schedule</td>
<td style="text-align: center;">Linear</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyperparameters for finetuning Vicuna models with our deductive verification dataset.</p>
<p>Consider the GSM8K dataset as an example (recall that the final answer for a problem is obtained through majority voting). Among all problems, $91.6 \%$ of problems have |(number of votes received by the correct answer) - (largest number of votes received by a single wrong answer) $&gt;2$, and their final answers are unlikely to be changed through our deductive verification approach. For the rest of the cases $(8.4 \%)$, where deductive verification is more likely to impact their final answers, we found that:</p>
<ul>
<li>Among all reasoning chains that arrive at correct answers (these correct-answer chains account for $49.4 \%$ of all reasoning chain candidates), $46.2 \%$ of reasoning chains are filtered out by our verification process.</li>
<li>Among the reasoning chains that arrive at correct answer but are filtered out by our verification process, $76.3 \%$ indeed exhibit incorrect reasoning.</li>
<li>Among the reasoning chains that arrive at correct answer and are not filtered out by our verification process, $78.0 \%$ indeed have correct reasonings.</li>
<li>Among the reasoning chains that do not arrive at correct answer and exhibit incorrect reasonings (these account for $50.6 \%$ of all reasoning chain candidates), $40.6 \%$ are filtered out by our verification process.</li>
</ul>
<p>The above statistics shows that a significant portion of reasoning chains that arrive at correct answers but exhibit incorrect reasoning are successfully eliminated. Therefore, the reliability and trustfulness of reasoning chains that arrive at the correct answers are significantly improved. Combined with the fact that a significant proportion of reasoning chains that exhibit incorrect answers are eliminated, and that our approach's verification accuracy significantly improves over naive verification approaches, our primary goal to improve LLM reasoning reliability is accomplished.
Nevertheless, the removals of many reasoning chains yielding correct answers (specifically, a significant $46.2 \% \times 49.4 \%$ of all chains) has a notable impact. This even exceeds the removals of reasoning chains with incorrect reasonings and answers ( $40.6 \% \times 50.6 \%$ of all chains). As a result, there are fewer votes for the correct answer when generating final answers through majority voting, which limits the final answer accuracy. In the future, we believe that when a greater proportion of incorrect reasoning chains with incorrect answers are filtered out, we can improve the final answer accuracy.</p>
<h1>C More Details on Answer Extraction</h1>
<p>In this section, we describe our process to extract the final answer from language models' responses. The process begins by selecting the last three non-empty lines. Then, these lines are processed through the following pipeline:</p>
<ol>
<li>Firstly, we use a list of regular expressions to identify "No-Answer" patterns within the text, such as "we cannot answer (this|the) question". This process helps us ascertain whether the model can provide a conclusive answer. If any such patterns appear in the text, we mark "No answer!" as the final answer. However, if we don't detect these patterns, we proceed to the next steps for extracting the final answer.</li>
<li>
<p>Secondly, if any "Answer-Split" patterns are found in the text, we divide the text into several blocks using the identified pattern. The last block of text is then utilized for extracting the answer.</p>
</li>
<li>
<p>Lastly, we use regular expressions, as outlined in Tab. 11, to scan the remaining text for possible final answers. If multiple matches are found for the pattern, we select the first match as the final answer. If no pattern matches are found in the remaining text, we default the final response to "No answer!".
"No-Answer" Patterns: "we cannot provide an answer to this question with (this|the) given information", "we cannot answer (this|the) question", "we cannot determine", "we can't determine", "we do not have enough information to answer (this|the) question", "we do not have enough information to provide a definitive answer to (this|the) question", "the answer(.*?)is unknown", "answer is not listed among the answer choices".
"Answer-Split" Patterns: "answer is", "final answer:", "answer to the question is", "answer to this question is", "concatenated letters are", "concatenate the letters -", "The answer of ".</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer Type</th>
<th style="text-align: center;">Regular Expression</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">$(-? \backslash \mathrm{d}[\backslash \mathrm{d}, \backslash,] *)$</td>
</tr>
<tr>
<td style="text-align: center;">Fractional number</td>
<td style="text-align: center;">$(-? \backslash(\backslash \mathrm{d}+\backslash / \backslash \mathrm{d}+\backslash) \backslash / \backslash \mathrm{d}+ \mid-? \backslash \mathrm{d}+\backslash / \backslash \mathrm{d}+)$</td>
</tr>
<tr>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">$(\backslash \mathrm{d} \backslash \mathrm{d} \backslash / \backslash \mathrm{d} \backslash \mathrm{d} \backslash / \backslash \mathrm{d} \backslash \mathrm{d} \backslash \mathrm{d})$</td>
</tr>
<tr>
<td style="text-align: center;">Yes or No</td>
<td style="text-align: center;">(?:Yes</td>
</tr>
</tbody>
</table>
<p>Table 11: Regular Expression for extracting the final answers of different kinds of questions.</p>
<h1>D Prompts</h1>
<h2>D. 1 Prompt for Direct Reasoning Chain Verification Without Natural Program Format</h2>
<p>For the results in Tab. 2 of the main paper, We use "Do you think the above reasoning process is correct? Let's think step by step." as the zero-shot prompt to verify an entire reasoning chain at once. We also design a two-shot prompt for reasoning chain verification as shown in Tab. 12, which covers one correct reasoning chain and one incorrect reasoning chain.</p>
<h2>D. 2 Prompts for Reasoning Chain Generation in the Natural Program Format</h2>
<p>To instruct models to generate reasoning chains in the Natural Program format that facilitates step-by-step deductive verification, we have designed four distinct prompts to address different types of problems. These include:</p>
<ol>
<li>Math word problems, as illustrated in Tab. 13, covering GSM8K, MATH, and AddSub datasets.</li>
<li>Math word problems with multiple-choice options, illustrated in Tab. 14, covering the AQuA dataset.</li>
<li>Date-related problems, illustrated in Tab. 15, covering the Date dataset.</li>
<li>Last Letters problems, illustrated in Tab. 16, covering the Last Letters dataset.</li>
</ol>
<h2>D. 3 Prompt for Deductive Verification Following Natural Program Format and Step-by-Step Decomposition</h2>
<p>We have designed a general one-shot prompt for the deductive verification of a single reasoning step on different datasets, as shown in Tab. 17. This prompt serves to instruct language models to generate the deductive validity of each reasoning step as illustrated in Sec. 4.2 and the top-right box of Fig. 1 of the main paper.</p>
<h2>E More Deductive Verification Examples</h2>
<p>In this section, we present more deductive verification examples using our Natural Program-based approach on single reasoning steps.</p>
<p>In Tab. 18, we demonstrate that the language model (ChatGPT) not only successfully identifies ungrounded information, but also identifies logical errors within the given solutions.</p>
<p>In Tab. 19, we illustrate a case where the language model fails to detect ungrounded premise numbers, mistakenly assuming that these numbers can be derived from grounded ones.</p>
<p>Lastly, in Tab. 20, we illustrate a case where the language model is sometimes unable to correctly identify grounded numbers.</p>
<h1>Here is a math question and its solution:</h1>
<p>"Question:
Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?</p>
<p>Answer:
Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny $20+12=34$. The answer is 34 ."</p>
<p>You are a math teacher. Do you think the reasoning process is correct?
Let's check the "Answer" step by step, and then end with "yes" or "no".</p>
<h2>Answer:</h2>
<p>The problem statement is clear, but the reasoning process has a major error in it.
Let's go through it step by step:</p>
<ol>
<li>"Jason started with 20 lollipops." This is correct according to the problem statement.</li>
<li>"Then he had 12 after giving some to Denny." This is also correct as per the problem.</li>
<li>"So he gave Denny $20+12=34$. The answer is 34 ." This is incorrect. The correct reasoning process should be subtracting the number of lollipops he ended up with from the number he started with. That would be $20-12=8$ lollipops.</li>
</ol>
<p>So, Jason gave Denny 8 lollipops, not 34. No, the reasoning process is incorrect.</p>
<h2>Here is a math question and its solution:</h2>
<p>"Question:
Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?
Answer:
Olivia had 23 dollars. 5 bagels for 3 dollars each will be $5 \times 3=15$ dollars. So she has 23 - 15 dollars left. $23-15$ is 8 . The answer is 8 ."</p>
<p>You are a math teacher. Do you think the reasoning process is correct?
Let's check the "Answer" step by step, and then end with "yes" or "no".</p>
<h2>Answer:</h2>
<p>Let's go through this reasoning process step by step.</p>
<ol>
<li>"Olivia had 23 dollars." This is the given starting amount of money that Olivia has. This step is correct.</li>
<li>"5 bagels for 3 dollars each will be $5 \times 3=15$ dollars." This is correct as well. Olivia spends 3 dollars for each bagel, and she buys 5 of them. So the total cost of the bagels is 3 dollars/bagel x 5 bagels $=15$ dollars.</li>
<li>"So she has 23 - 15 dollars left." This statement is also correct. After purchasing the bagels, Olivia will have her initial amount of money minus the cost of the bagels.</li>
<li>"23 - 15 is 8 . The answer is 8 ." This step is correct as well. 23 dollars (Olivia's initial amount) - 15 dollars (the cost of the bagels) $=8$ dollars. This means that Olivia has 8 dollars left after buying the bagels.</li>
</ol>
<p>So overall, Olivia has 8 dollars left after buying five bagels for 3 dollars each. Yes, the reasoning process is correct.</p>
<p>Table 12: Two-shot prompt for direct reasoning chain verification without Natural Program format.</p>
<h1>Question:</h1>
<p>Alice buys 5 apples. Each apple costs her $\$ 2$. She gets them for $50 \%$ off. After she bought she decides she doesn't like 2 of them and sells them to her friends for 1 . How much money was she out?</p>
<h2>Answer:</h2>
<p>First, let's write down all the statements and relationships in the question with labels. #1. Alice gets 5 apples.
#2. Each apple costs $\$ 2$, but Alice gets apples for $50 \%$ off.
#3. Alice sells 2 of 5 apples for 1 .
#4. How much money was Alice out?
Next, let's answer the question step by step with reference to the question and reasoning process:
#5. (by #2) Step 1: Calculate the price per apple after a $50 \%$ discount.
Original price per apple: $\$ 2$
Discount: $50 \%$
Price per apple after discount: $\$ 2 *(1-0.50)=\$ 2 * 0.50=\$ 1$
#6. (by #1 #5) Step 2: Calculate Alice's cost of the 5 apples after a $50 \%$ discount.
Price per apple after discount: $\$ 1$
Total apples: 5
Alice's total cost of 5 apples: $\$ 1 * 5=\$ 5$
#7. (by #3) Step 3: Calculate the money Alice gets back from selling 2 apples.
Money that Alice gets back from selling 2 apples: $\$ 1$
#8. (by #6 #7) Step 4: Calculate the total amount of money Alice is out.
Alice's total cost of 5 apples: $\$ 5$
Money that Alice gets back from selling 2 apples: $\$ 1$
Money that Alice is out: $\$ 5-\$ 1=\$ 4$
#9. (by #4 #8) The original question is #4.
How much money was Alice out? We do not miss information on the rewritten labels. So the answer to this question is Alice is out $\$ 4$.</p>
<p>Please answer the following question by following the above template.
Table 13: One-shot Natural Program prompt for reasoning chain generation on math word problems.</p>
<h1>Question:</h1>
<p>Alice buys 5 apples. Each apple costs her $\$ 2$.
She gets them for $50 \%$ off. After she bought She decides she doesn't like 2 of them and sells them to her friends for 1.
How much money was she out?
Answer Choices:
A) 4
B) 2
C) None of the above</p>
<h2>Answer:</h2>
<p>First, let's write down all the statements and relationships in the question with labels.
#1. Alice gets 5 apples.
#2. Each apple costs $\$ 2$, but Alice gets apples for $50 \%$ off.
#3. Alice sells 2 of 5 apples for 1 .
#4. How much money was Alice out?
Next, let's answer the question step by step with reference to the question and reasoning process:
#5. (by #2) Step 1: Calculate the price per apple after a 50\% discount.
Original price per apple: $\$ 2$
Discount: $50 \%$
Price per apple after discount: $\$ 2 *(1-0.50)=\$ 2 * 0.50=\$ 1$
#6. (by #1 #5) Step 2: Calculate Alice's cost of the 5 apples after a 50\% discount.
Price per apple after discount: $\$ 1$
Total apples: 5
Alice's total cost of 5 apples: $\$ 1 * 5=\$ 5$
#7. (by #3) Step 3: Calculate the money Alice gets back from selling 2 apples.
Money that Alice gets back from selling 2 apples: $\$ 1$
#8. (by #6 #7) Step 4: Calculate the total amount of money Alice is out.
Alice's total cost of 5 apples: $\$ 5$
Money that Alice gets back from selling 2 apples: $\$ 1$
Money that Alice is out: $\$ 5-\$ 1=\$ 4$
#9. (by #4 #8) The original question is #4. How much money was Alice out? We do not miss information on the rewritten labels. So the answer to this question is Alice is out $\$ 4$. Among all the answer choices, the best option is A) 4 .</p>
<p>Please answer the following question by following the above template.
Table 14: One-shot Natural Program prompt for reasoning chain generation on math word problems with multiple choice.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/domeccleston/sharegpt&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8555 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8555</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8555</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e3cd9f01f87a601b274b4ef6513a84c8cde03214</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e3cd9f01f87a601b274b4ef6513a84c8cde03214" target="_blank">Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work provides two new data resources on multiple spatial language processing tasks and shows pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
                <p><strong>Paper Abstract:</strong> Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8555.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8555.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT_on_StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (evaluated on the StepGame multi-hop spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-base (pretrained transformer encoder) fine-tuned as a classifier on story+question inputs to answer multi-hop spatial reasoning questions in the StepGame benchmark; evaluated with and without further pretraining on synthetic spatial corpora (SPARTQA-AUTO, SPARTUN, StepGame synthetic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (BERT-base, HuggingFace implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained bidirectional Transformer encoder (BERT-base) used as a PLM; the [CLS] token output is fed to classification layers (binary or multi-class) for QA labels; further pretraining (transfer learning) on synthetic corpora such as SPARTUN / SPARTQA-AUTO / StepGame was performed before fine-tuning on target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (commonly 110M parameters) - paper refers to BERT-base via HuggingFace</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (multi-step spatial reasoning text benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic multi-hop spatial reasoning task (textual, k-step spatial relation inference; grid-like/directional reasoning over chains of relations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: concatenation of story (context) and question passed to BERT; output: multi-class classification for relation answers (StepGame uses FR questions); training used focal loss or cross-entropy depending on setting; experiments include BERT trained from scratch on target and BERT further pretrained (SynSup) on synthetic corpora (SPARTQA-AUTO, SPARTUN, StepGame) before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Pure PLM classification: BERT representations with a final classifier; transfer learning via additional pretraining on synthetic spatial corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) to inject spatial knowledge; no chain-of-thought or explicit symbolic reasoning used at inference time (the authors also build a symbolic spatial reasoner but it is used for dataset generation and answer labeling, not as part of the BERT inference pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported per number of reasoning steps k (k=1..10). BERT (no SynSup) accuracies: k=1 98.44%, k=2 94.77%, k=3 91.78%, k=4 71.70%, k=5 57.56%, k=6 50.34%, k=7 45.17%, k=8 39.69%, k=9 35.41%, k=10 33.62%. BERT + SPARTUN (best SynSup) accuracies: k=1 98.55%, k=2 95.02%, k=3 92.04%, k=4 79.10%, k=5 70.34%, k=6 63.39%, k=7 58.74%, k=8 52.09%, k=9 48.36%, k=10 45.68% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Empirical improvement on multi-hop (k>3) examples after further pretraining on SPARTUN indicates that BERT can internalize multi-step spatial relation composition to some extent; steep accuracy decline with increasing k shows limited ability to chain many spatial inferences; no explicit probing/ablation demonstrating internal symbolic reasoning representation beyond transfer-learning gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against TP-MANN (neural memory network from the StepGame paper). TP-MANN accuracies are substantially lower for larger k (e.g., TP-MANN k=1 85.77%, k=10 21.46%), while BERT (with and without SynSup) substantially outperforms TP-MANN across k. Comparisons between different synthetic supervision sources (SPARTQA-AUTO, SPARTUN-S, SPARTUN, StepGame) show SPARTUN yields best transfer for k>3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades rapidly as the number of reasoning steps increases (e.g., BERT drops from ≈98% at k=1 to ≈34% at k=10 without SynSup, and to ≈46% with SPARTUN). Models operate as black-box PLMs and lack explicit neuro-symbolic proof tracing; authors note models are far from human-level and resource intensive (GPU). No specific stepwise error analyses are provided beyond per-k accuracy; failure cases include inability to reliably chain many hops and sensitivity to limited relation-expression coverage in pretraining corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8555.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8555.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT_on_bAbI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (evaluated on bAbI spatial tasks 17 and 19)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-base fine-tuned for bAbI spatial reasoning tasks (Task 17: spatial reasoning YN, Task 19: path finding FR) using small training subsets to evaluate transfer from synthetic spatial corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards ai-complete question answering: A set of prerequisite toy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (BERT-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Transformer encoder fine-tuned with classification heads on concatenated story+question; experiments include baseline fine-tuning and further pretraining on synthetic spatial corpora before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (commonly 110M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>bAbI spatial tasks (Task 17, Task 19)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Toy synthetic spatial reasoning puzzles requiring 1-2 hop inference or path-finding over small relation chains (textual logic puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Authors fine-tuned on limited subsets (1k examples for Task 17 YN, 500 for Task 19 FR) to study the effect of synthetic supervision; input is story+question and output is Yes/No or directional relation labels; models compared when further pretrained on SPARTUN, SPARTQA-AUTO, StepGame, SPARTUN-S.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard PLM fine-tuning classifier; transfer learning via synthetic corpora pretraining; post-processing to remove inconsistent multi-label outputs (e.g., can't be LEFT and RIGHT simultaneously).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (Table 4) on the small-data experimental setup: Majority baseline (MB) YN 53.60%, FR 24.52%. BERT (no SynSup) YN 49.65%, FR 18.18%. BERT + SPARTUN achieves YN 48.25% and FR 50.64% (best FR). Other SynSup variants show mixed effects. Human accuracy reported (for the full bAbI tasks with sufficient data) is high (Human YN 90.69%, FR 95.23%) but not directly comparable to the small-data setups.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>When trained on small data, BERT struggles on these toy spatial tasks without suitable synthetic pretraining; pretraining on SPARTUN improves FR performance substantially (e.g., FR up to 50.64%), indicating learned spatial composition helps. The paper also notes that with full bAbI data PLMs can reach near 100%, indicating capacity but data-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared across different synthetic supervision sources (SPARTQA-AUTO, StepGame, SPARTUN-S, SPARTUN) showing different transferability: SPARTUN gives best FR results; SPARTUN-S (simpler expression set) performs better on Task 17 YN than some other SynSup sets. Human performance on bAbI (with full data) far exceeds PLM performance on small-data experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On small-data settings BERT sometimes underperforms a simple majority baseline for YN (e.g., BERT 49.65% vs MB 53.60% for YN in Table 4), indicating fragility; improvements depend strongly on the match between synthetic pretraining expression coverage and target dataset language; models still far from human-level reasoning on ambiguous or low-resource setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8555.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8555.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT_on_RESQ_and_SPARTQA-HUMAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (evaluated on RESQ and SPARTQA-HUMAN human-generated spatial QA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-base fine-tuned (with and without further pretraining on synthetic corpora) on two human-generated SQA datasets: RESQ (created in this paper from MSPRL) and SPARTQA-HUMAN (small human subset); used to evaluate transferability of synthetic pretraining to realistic spatial language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (BERT-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Transformer encoder fine-tuned for Yes/No (YN) and Find-Relations (FR) QA types; additional pretraining (SynSup) on SPARTUN or other synthetic corpora evaluated for transfer benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (commonly 110M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>RESQ (Real-world Spatial Questions) and SPARTQA-HUMAN</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Human-generated spatial question answering tasks over image descriptions (realistic spatial language requiring multi-hop reasoning and real-world spatial semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: story (image descriptions) + question; output: binary YN or multi-label FR answers; evaluation metric: accuracy; experiments compare models fine-tuned only on the small human dataset vs. models further pretrained on synthetic corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>PLM fine-tuning with classification heads; transfer learning from synthetic spatial corpora to improve robustness to diverse spatial expressions; post-processing to enforce consistency in multi-label outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RESQ (Table 7): Majority baseline 50.21%, BERT (no SynSup) 57.37% accuracy, BERT + SPARTQA-AUTO 55.08%, BERT + StepGame 60.14%, BERT + SPARTUN-S 58.03%, BERT + SPARTUN 63.60% (best). Human accuracy on a subset: 90.38%. SPARTQA-HUMAN: authors report that pretraining on SPARTUN achieves +2.6% on FR and +9% on YN compared to SPARTQA-AUTO pretraining (exact numbers not fully tabulated in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improvements from SPARTUN pretraining (esp. +5.5% absolute on RESQ vs. baseline) support the claim that broader coverage of relation types and lexical expressions helps models learn transferable spatial reasoning patterns; however human accuracy remains substantially higher, indicating incomplete spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared different synthetic supervision sources: SPARTUN yields the best transfer to RESQ and SPARTQA-HUMAN, outperforming SPARTQA-AUTO and StepGame in some settings. Human performance far exceeds model performance (e.g., RESQ human 90.38% vs best model 63.60%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with SPARTUN pretraining, BERT remains well below human performance on realistic questions. Authors note small size of human datasets limits training and that RESQ sometimes requires spatial commonsense beyond explicit text, which the PLM does not model. Failures include difficulty with YN questions in SPARTQA-HUMAN where PLMs often cannot beat the simple majority baseline without appropriate SynSup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8555.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8555.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TP-MANN_reference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TP-MANN (neural memory network reported in the StepGame paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural memory-network based model (TP-MANN) reported in the StepGame benchmark paper; used here as a prior state-of-the-art baseline for multi-hop textual spatial reasoning and compared to BERT results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TP-MANN (neural memory network from StepGame paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Memory-augmented neural network architecture (from the StepGame benchmark authors) designed for multi-step spatial inference over textual descriptions; specifics come from the StepGame paper (Shi et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic multi-hop spatial reasoning tasks (textual, k-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Reported in the StepGame paper; here used as a published baseline for comparison against BERT across k-step difficulty levels (k=1..10).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Neural memory network designed to store and retrieve facts across multiple reasoning steps (details in StepGame paper); not further modified or pretrained by the authors of the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (Table 6) TP-MANN accuracies on StepGame: k=1 85.77%, k=2 60.31%, k=3 50.18%, k=4 37.45%, k=5 31.25%, k=6 28.53%, k=7 26.45%, k=8 23.67%, k=9 22.52%, k=10 21.46%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>TP-MANN's rapidly decreasing accuracy with increasing k indicates limited capacity for long chains of spatial inference compared to PLMs like BERT; used as a baseline to show PLM advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared in this paper to BERT (with and without synthetic pretraining); BERT substantially outperforms TP-MANN across nearly all k values, especially for small k where BERT nearly solves 1-3 step cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>TP-MANN shows steep degradation as reasoning steps increase (k), achieving only ~21% at k=10, demonstrating difficulty scaling memory-based neural approaches to many-hop spatial reasoning in text; no further analysis or error cases beyond reported accuracies in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 2)</em></li>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Towards ai-complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8555",
    "paper_id": "paper-e3cd9f01f87a601b274b4ef6513a84c8cde03214",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "BERT_on_StepGame",
            "name_full": "BERT (evaluated on the StepGame multi-hop spatial reasoning benchmark)",
            "brief_description": "BERT-base (pretrained transformer encoder) fine-tuned as a classifier on story+question inputs to answer multi-hop spatial reasoning questions in the StepGame benchmark; evaluated with and without further pretraining on synthetic spatial corpora (SPARTQA-AUTO, SPARTUN, StepGame synthetic).",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (BERT-base, HuggingFace implementation)",
            "model_description": "Pretrained bidirectional Transformer encoder (BERT-base) used as a PLM; the [CLS] token output is fed to classification layers (binary or multi-class) for QA labels; further pretraining (transfer learning) on synthetic corpora such as SPARTUN / SPARTQA-AUTO / StepGame was performed before fine-tuning on target tasks.",
            "model_size": "BERT-base (commonly 110M parameters) - paper refers to BERT-base via HuggingFace",
            "puzzle_name": "StepGame (multi-step spatial reasoning text benchmark)",
            "puzzle_type": "Synthetic multi-hop spatial reasoning task (textual, k-step spatial relation inference; grid-like/directional reasoning over chains of relations)",
            "task_setup": "Input: concatenation of story (context) and question passed to BERT; output: multi-class classification for relation answers (StepGame uses FR questions); training used focal loss or cross-entropy depending on setting; experiments include BERT trained from scratch on target and BERT further pretrained (SynSup) on synthetic corpora (SPARTQA-AUTO, SPARTUN, StepGame) before fine-tuning.",
            "mechanisms_or_strategies": "Pure PLM classification: BERT representations with a final classifier; transfer learning via additional pretraining on synthetic spatial corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) to inject spatial knowledge; no chain-of-thought or explicit symbolic reasoning used at inference time (the authors also build a symbolic spatial reasoner but it is used for dataset generation and answer labeling, not as part of the BERT inference pipeline).",
            "performance_metrics": "Accuracy reported per number of reasoning steps k (k=1..10). BERT (no SynSup) accuracies: k=1 98.44%, k=2 94.77%, k=3 91.78%, k=4 71.70%, k=5 57.56%, k=6 50.34%, k=7 45.17%, k=8 39.69%, k=9 35.41%, k=10 33.62%. BERT + SPARTUN (best SynSup) accuracies: k=1 98.55%, k=2 95.02%, k=3 92.04%, k=4 79.10%, k=5 70.34%, k=6 63.39%, k=7 58.74%, k=8 52.09%, k=9 48.36%, k=10 45.68% (Table 6).",
            "evidence_of_spatial_reasoning": "Empirical improvement on multi-hop (k&gt;3) examples after further pretraining on SPARTUN indicates that BERT can internalize multi-step spatial relation composition to some extent; steep accuracy decline with increasing k shows limited ability to chain many spatial inferences; no explicit probing/ablation demonstrating internal symbolic reasoning representation beyond transfer-learning gains.",
            "comparisons": "Compared against TP-MANN (neural memory network from the StepGame paper). TP-MANN accuracies are substantially lower for larger k (e.g., TP-MANN k=1 85.77%, k=10 21.46%), while BERT (with and without SynSup) substantially outperforms TP-MANN across k. Comparisons between different synthetic supervision sources (SPARTQA-AUTO, SPARTUN-S, SPARTUN, StepGame) show SPARTUN yields best transfer for k&gt;3.",
            "limitations_or_failure_cases": "Performance degrades rapidly as the number of reasoning steps increases (e.g., BERT drops from ≈98% at k=1 to ≈34% at k=10 without SynSup, and to ≈46% with SPARTUN). Models operate as black-box PLMs and lack explicit neuro-symbolic proof tracing; authors note models are far from human-level and resource intensive (GPU). No specific stepwise error analyses are provided beyond per-k accuracy; failure cases include inability to reliably chain many hops and sensitivity to limited relation-expression coverage in pretraining corpora.",
            "uuid": "e8555.0",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BERT_on_bAbI",
            "name_full": "BERT (evaluated on bAbI spatial tasks 17 and 19)",
            "brief_description": "BERT-base fine-tuned for bAbI spatial reasoning tasks (Task 17: spatial reasoning YN, Task 19: path finding FR) using small training subsets to evaluate transfer from synthetic spatial corpora.",
            "citation_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "mention_or_use": "use",
            "model_name": "BERT (BERT-base)",
            "model_description": "Pretrained Transformer encoder fine-tuned with classification heads on concatenated story+question; experiments include baseline fine-tuning and further pretraining on synthetic spatial corpora before fine-tuning.",
            "model_size": "BERT-base (commonly 110M parameters)",
            "puzzle_name": "bAbI spatial tasks (Task 17, Task 19)",
            "puzzle_type": "Toy synthetic spatial reasoning puzzles requiring 1-2 hop inference or path-finding over small relation chains (textual logic puzzles).",
            "task_setup": "Authors fine-tuned on limited subsets (1k examples for Task 17 YN, 500 for Task 19 FR) to study the effect of synthetic supervision; input is story+question and output is Yes/No or directional relation labels; models compared when further pretrained on SPARTUN, SPARTQA-AUTO, StepGame, SPARTUN-S.",
            "mechanisms_or_strategies": "Standard PLM fine-tuning classifier; transfer learning via synthetic corpora pretraining; post-processing to remove inconsistent multi-label outputs (e.g., can't be LEFT and RIGHT simultaneously).",
            "performance_metrics": "Accuracy (Table 4) on the small-data experimental setup: Majority baseline (MB) YN 53.60%, FR 24.52%. BERT (no SynSup) YN 49.65%, FR 18.18%. BERT + SPARTUN achieves YN 48.25% and FR 50.64% (best FR). Other SynSup variants show mixed effects. Human accuracy reported (for the full bAbI tasks with sufficient data) is high (Human YN 90.69%, FR 95.23%) but not directly comparable to the small-data setups.",
            "evidence_of_spatial_reasoning": "When trained on small data, BERT struggles on these toy spatial tasks without suitable synthetic pretraining; pretraining on SPARTUN improves FR performance substantially (e.g., FR up to 50.64%), indicating learned spatial composition helps. The paper also notes that with full bAbI data PLMs can reach near 100%, indicating capacity but data-dependence.",
            "comparisons": "Compared across different synthetic supervision sources (SPARTQA-AUTO, StepGame, SPARTUN-S, SPARTUN) showing different transferability: SPARTUN gives best FR results; SPARTUN-S (simpler expression set) performs better on Task 17 YN than some other SynSup sets. Human performance on bAbI (with full data) far exceeds PLM performance on small-data experiments.",
            "limitations_or_failure_cases": "On small-data settings BERT sometimes underperforms a simple majority baseline for YN (e.g., BERT 49.65% vs MB 53.60% for YN in Table 4), indicating fragility; improvements depend strongly on the match between synthetic pretraining expression coverage and target dataset language; models still far from human-level reasoning on ambiguous or low-resource setups.",
            "uuid": "e8555.1",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BERT_on_RESQ_and_SPARTQA-HUMAN",
            "name_full": "BERT (evaluated on RESQ and SPARTQA-HUMAN human-generated spatial QA datasets)",
            "brief_description": "BERT-base fine-tuned (with and without further pretraining on synthetic corpora) on two human-generated SQA datasets: RESQ (created in this paper from MSPRL) and SPARTQA-HUMAN (small human subset); used to evaluate transferability of synthetic pretraining to realistic spatial language.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (BERT-base)",
            "model_description": "Pretrained Transformer encoder fine-tuned for Yes/No (YN) and Find-Relations (FR) QA types; additional pretraining (SynSup) on SPARTUN or other synthetic corpora evaluated for transfer benefits.",
            "model_size": "BERT-base (commonly 110M parameters)",
            "puzzle_name": "RESQ (Real-world Spatial Questions) and SPARTQA-HUMAN",
            "puzzle_type": "Human-generated spatial question answering tasks over image descriptions (realistic spatial language requiring multi-hop reasoning and real-world spatial semantics).",
            "task_setup": "Input: story (image descriptions) + question; output: binary YN or multi-label FR answers; evaluation metric: accuracy; experiments compare models fine-tuned only on the small human dataset vs. models further pretrained on synthetic corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) before fine-tuning.",
            "mechanisms_or_strategies": "PLM fine-tuning with classification heads; transfer learning from synthetic spatial corpora to improve robustness to diverse spatial expressions; post-processing to enforce consistency in multi-label outputs.",
            "performance_metrics": "RESQ (Table 7): Majority baseline 50.21%, BERT (no SynSup) 57.37% accuracy, BERT + SPARTQA-AUTO 55.08%, BERT + StepGame 60.14%, BERT + SPARTUN-S 58.03%, BERT + SPARTUN 63.60% (best). Human accuracy on a subset: 90.38%. SPARTQA-HUMAN: authors report that pretraining on SPARTUN achieves +2.6% on FR and +9% on YN compared to SPARTQA-AUTO pretraining (exact numbers not fully tabulated in the main text).",
            "evidence_of_spatial_reasoning": "Improvements from SPARTUN pretraining (esp. +5.5% absolute on RESQ vs. baseline) support the claim that broader coverage of relation types and lexical expressions helps models learn transferable spatial reasoning patterns; however human accuracy remains substantially higher, indicating incomplete spatial reasoning.",
            "comparisons": "Compared different synthetic supervision sources: SPARTUN yields the best transfer to RESQ and SPARTQA-HUMAN, outperforming SPARTQA-AUTO and StepGame in some settings. Human performance far exceeds model performance (e.g., RESQ human 90.38% vs best model 63.60%).",
            "limitations_or_failure_cases": "Even with SPARTUN pretraining, BERT remains well below human performance on realistic questions. Authors note small size of human datasets limits training and that RESQ sometimes requires spatial commonsense beyond explicit text, which the PLM does not model. Failures include difficulty with YN questions in SPARTQA-HUMAN where PLMs often cannot beat the simple majority baseline without appropriate SynSup.",
            "uuid": "e8555.2",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "TP-MANN_reference",
            "name_full": "TP-MANN (neural memory network reported in the StepGame paper)",
            "brief_description": "A neural memory-network based model (TP-MANN) reported in the StepGame benchmark paper; used here as a prior state-of-the-art baseline for multi-hop textual spatial reasoning and compared to BERT results.",
            "citation_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "mention_or_use": "mention",
            "model_name": "TP-MANN (neural memory network from StepGame paper)",
            "model_description": "Memory-augmented neural network architecture (from the StepGame benchmark authors) designed for multi-step spatial inference over textual descriptions; specifics come from the StepGame paper (Shi et al., 2022).",
            "model_size": null,
            "puzzle_name": "StepGame",
            "puzzle_type": "Synthetic multi-hop spatial reasoning tasks (textual, k-step reasoning)",
            "task_setup": "Reported in the StepGame paper; here used as a published baseline for comparison against BERT across k-step difficulty levels (k=1..10).",
            "mechanisms_or_strategies": "Neural memory network designed to store and retrieve facts across multiple reasoning steps (details in StepGame paper); not further modified or pretrained by the authors of the current paper.",
            "performance_metrics": "Reported (Table 6) TP-MANN accuracies on StepGame: k=1 85.77%, k=2 60.31%, k=3 50.18%, k=4 37.45%, k=5 31.25%, k=6 28.53%, k=7 26.45%, k=8 23.67%, k=9 22.52%, k=10 21.46%.",
            "evidence_of_spatial_reasoning": "TP-MANN's rapidly decreasing accuracy with increasing k indicates limited capacity for long chains of spatial inference compared to PLMs like BERT; used as a baseline to show PLM advantages.",
            "comparisons": "Compared in this paper to BERT (with and without synthetic pretraining); BERT substantially outperforms TP-MANN across nearly all k values, especially for small k where BERT nearly solves 1-3 step cases.",
            "limitations_or_failure_cases": "TP-MANN shows steep degradation as reasoning steps increase (k), achieving only ~21% at k=10, demonstrating difficulty scaling memory-based neural approaches to many-hop spatial reasoning in text; no further analysis or error cases beyond reported accuracies in this paper.",
            "uuid": "e8555.3",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 2
        },
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2
        },
        {
            "paper_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1
        }
    ],
    "cost": 0.01736725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning</h1>
<p>Roshanak Mirzaee<br>Michigan State University<br>mirzaee@msu.edu</p>
<h2>Parisa Kordjamshidi</h2>
<p>Michigan State University
kordjams@msu.edu</p>
<h4>Abstract</h4>
<p>Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with humangenerated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
<h2>1 Introduction</h2>
<p>Understanding spatial language is important in many applications such as navigation (Zhang and Kordjamshidi, 2022; Zhang et al., 2021; Chen et al., 2019), medical domain (Datta et al., 2020; Kamel Boulos et al., 2019; Massa et al., 2015), and robotics (Venkatesh et al., 2021; Kennedy et al., 2007). However, few benchmarks have directly focused on comprehending the spatial semantics of the text. Moreover, the existing datasets are either synthetic (Mirzaee et al., 2021; Weston et al., 2015; Shi et al., 2022) or at small scale (Mirzaee et al., 2021; Kordjamshidi et al., 2017).</p>
<p>The synthetic datasets often focus on specific types of relations with a small coverage of spatial semantics needed for spatial language understanding in various domains. Figure 2 indicates the coverage of sixteen spatial relation types (in</p>
<p>Three boxes called one, two and three exist in an image. Box one contains a big yellow melon and a small orange watermelon. Box two has a small yellow apple. A small orange apple is inside and touching this box. Box one is in box three. Box two is to the south of, far from and to the west of box three. A small yellow watermelon is inside box three.</p>
<p>Q: Is the yellow apple to the west of the yellow watermelon? Yes
Q: Where is box two relative to the yellow watermelon? Left, Below, Far
(a) SPARTUN - A synthetic large dataset provided as a source of supervision.</p>
<p>A grey car is parking in front of a grey house with brown window frames and plants on the balcony.</p>
<p>Q: Are the plants in front of the car? No
Q: Are the plants in the house? Yes
(b) RESQ - A human-generated dataset for probing the models on realistic SQA</p>
<p>Figure 1: Two new datasets on SQA</p>
<p>Table 1) collected from existing resources (Randell et al., 1992; Wolter, 2009; Renz and Nebel, 2007). The human-generated datasets, despite helping study the problem as evaluation benchmarks, are less helpful for training models that can reliably understand spatial language due to their small size (Mirzaee et al., 2021).</p>
<p>In this work, we build a new synthetic dataset on SQA, called SPARTUN ${ }^{1}$ (Fig 1a) to provide a source of supervision with broad coverage of spatial relation types and expressions ${ }^{2}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Formalism <br> (General Type)</th>
<th style="text-align: left;">Specific value</th>
<th style="text-align: left;">Spatial type/Spatial value)</th>
<th style="text-align: left;">Expressions (e.g.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">DC (disconnected) <br> EC (Externally Connected) <br> PO (Partially Overlapped) <br> EQ (Equal) <br> TPP (Tangential Proper Part) <br> NTPP (Non-Tangential Proper Part) <br> TPPI (Tangential Proper Part inverse) <br> NTPPI (Non-Tangential Proper Part inverse)</td>
<td style="text-align: left;">disjoint <br> touching <br> overlapped <br> equal <br> covered by <br> in, inside <br> covers <br> has</td>
</tr>
<tr>
<td style="text-align: left;">Topological</td>
<td style="text-align: left;">Relative</td>
<td style="text-align: left;">LEFT, RIGHT <br> BELOW, ABOVE <br> BEHIND, FRONT</td>
<td style="text-align: left;">left of, right of <br> under, over <br> behind, in front</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Far, Near</td>
<td style="text-align: left;">far, close</td>
</tr>
</tbody>
</table>
<p>Table 1: Spatial relation types and examples of spatial language expressions.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The comparative coverage of relation types based on Table 1 for SQA datasets.</p>
<p>To generate SPARTUN, we follow the idea of SPARTQA (Mirzaee et al., 2021) benchmark and generate scene graphs from a set of images. The edges in this graph yield a set of triplets such as $\mathrm{ABOVE}($ blue circle, red triangle), which are used to generate a scene description (i.e., a story).</p>
<p>In SPARTUN, we map the spatial relation types in triplets (e.g., ABOVE) to a variety of spatial language expressions (e.g., over, north, above) to enable the transfer learning for various data domains ${ }^{3}$. We also build a logical spatial reasoner to compute all possible direct and indirect spatial relations between graph nodes. Then, the questions of this dataset are selected from the indirect relations.</p>
<p>To evaluate the effectiveness of SPARTUN in transfer learning, we created another dataset named RESQ $^{4}$ (Fig 1b). This dataset is built on MSPRL (Kordjamshidi et al., 2017) corpus while we added human-generated spatial questions and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>answers to its real image descriptions. This dataset comparatively reflects more realistic challenges and complexities of the SQA problem.</p>
<p>We analyze the impact of SPARTUN as source of extra supervision on several SQA and SPRL benchmarks. To the best of our knowledge, we are the first to use synthetic supervision for the SPRL task. Our results show that the auto-generated data successfully improves the SOTA results on MSPRL and SPARTQA-HUMAN, which are annotated for SPRL task. Moreover, further pretraining models with SPARTUN for SQA task improves the result of previous models on RESQ, StepGame, and SPARTQA-HUMAN benchmarks. Furthermore, studying the broad coverage of spatial relation expressions of SPARTUN in realistic domains demonstrates that this feature is a key factor for transfer learning.</p>
<p>The contributions of this paper can be summarized as: (1) We build a new synthetic dataset to serve as a source of supervision and transfer learning for spatial language understanding tasks with broad coverage of spatial relation types and expressions (which is easily extendable); (2) We provide a human-generated dataset to evaluate the performance of transfer learning on real-world spatial question answering; (3) We evaluate the transferability of the models pretrained with SPARTUN on multiple SQA and SPRL benchmarks and show significant improvements in SOTA results.</p>
<h2>2 Related Research</h2>
<p>Requiring large amounts of annotated data is a wellknown issue in training complex deep neural mod-</p>
<p>els (Zhu et al., 2016) that is extended to spatial language processing tasks. In our study, we noticed that all available large datasets on SQA task including bAbI (Weston et al., 2015), SPARTQAAuto (Mirzaee et al., 2021), and StepGame (Shi et al., 2022) are, all, synthetic.
bAbI is a simple dataset that covers a limited set of relation types, spatial rules, and vocabulary. StepGame focuses on a few relation types but with more relation expressions for each and considers multiple reasoning steps. SPARTQA-AUTO, comparatively, contains more relation types and needs complex multi-hop spatial reasoning. However, it contains a single linguistic spatial expression for each relation type. All of these datasets are created based on controlled toy settings and are not comparable with real-world spatial problems in the sense of realistic language complexity and coverage of all possible relation types. SPARTQA-HUMAN (Mirzaee et al., 2021) is a human-generated version of SPARTQA-AUTO with more spatial expressions. However, this dataset is provided for probing purposes and has a small training set that is not sufficient for effectively training deep models.</p>
<p>For the SpRL task, mSPRL and SpaceEval (SemEval-2015 task 8) (Pustejovsky et al., 2015) are two available datasets with spatial roles and relation annotations. These are small-scale datasets for studying the SPRL problem. From the previous works which tried transfer learning on SPRL task, (Moussa et al., 2021) only used it on word embedding of their SPRL model, and (Shin et al., 2020) used PLM without any specifically designed dataset for further pretraining. These issues motivated us to create SPARTUN for further pretraining and transfer learning for SQA and SPRL.</p>
<p>Transfer learning has been used effectively in different NLP tasks to further fine-tune the PLMs (Razeghi et al., 2022; Alrashdi and O'Keefe, 2020; Magge et al., 2018). Besides transfer learning, several other approaches are used to tackle the lack of training data in various NLP areas, such as providing techniques to label the unlabeled data (Enayati et al., 2021), using semi-supervised models (Van Krieken et al., 2019; Li et al., 2021) or data augmentation with synthetic data (Li et al., 2019; Min et al., 2020). However, transfer learning is a simple way of using synthetic data as an extra source of supervision at no annotation cost. Compared to the augmentation methods, the data
in the transfer learning only needs to be close to the target task/domain (Ma et al., 2021) and not necessarily the same. Mirzaee et al. is the first work that considers transfer learning for SQA. It shows that training models on synthetic data and finetuning with small human-generated data results in a better performance of PLMs. However, their coverage of spatial relations and expressions is insufficient for effective transfer learning to realistic domains.</p>
<p>Using logical reasoning for building datasets that need complex reasoning for question answering is used before in building QA datasets (Clark et al., 2020; Saeed et al., 2021). More recent efforts even use the path of reasoning and train models to follow that (Tafjord et al., 2021). However, there are no previous works to model spatial reasoning as we do here with the broad coverage of spatial logic.</p>
<h2>3 Transfer Learning for Spatial Language Understanding</h2>
<p>To evaluate transfer learning on spatial language understanding, we select two main tasks, spatial question answering (SQA) and spatial role labeling (SPRL). Given the popularity of PLMs in transfer learning (Khashabi et al., 2020; Ma et al., 2021; Clark et al., 2020), we design PLM-based models for this evaluation. In the rest of this section, we describe each task and model in detail.</p>
<h3>3.1 Spatial Question Answering</h3>
<p>In spatial question answering, given a scene description, the task is to answer questions about the spatial relations between entities (e.g., Figure 1). Here, we focus on challenging questions that need multi-hop spatial reasoning over explicit relations. We consider two question types, YN (Yes/No) and FR(Find relations). The answer to YN is chosen from "Yes" or "No," and the answer to FR is chosen from a set of relation types.</p>
<p>We use a PLM with classification layers as a baseline for the SQA task. We use a binary classification layer for each label for questions with more than one valid answer and a multi-class classification layer for questions with a single valid answer. To predict the answer, we pass the concatenation of the question and story to the PLM (more detail in (Devlin et al., 2019).) The final output of $[C L S]$ token is passed to the classification layer and depending on the question type, a label or multiple labels with the highest probability are chosen as the final answer.</p>
<p>We train the models based on the summation of the cross-entropy losses of all binary classifiers in multi-label classification or the single crossentropy for a single classifier in multi-classification. In the multi-label setting, we remove inconsistent answers by post-processing during the inference phase. For instance, LEFT and RIGHT relations cannot be valid answers simultaneously.</p>
<h3>3.2 Spatial Role Labeling</h3>
<p>Spatial role labeling (Kordjamshidi et al., 2010, 2011) is the task of identifying and classifying the spatial roles (Trajector, Landmark, and spatial indicator) and their relations. A relation is selected from the relation types in Table 1 and assigned to each triplet of (Trajector, Spatial indicator, Landmark) extracted from the sentence. We call the former spatial role extraction and the latter spatial relation ${ }^{5}$ extraction (Figure 3).</p>
<p>Several neural models have been proposed to solve spatial role (Mazalov et al., 2015; Ludwig et al., 2016; Datta and Roberts, 2020). We take a similar approach to prior research (Shin et al., 2020) for the extraction of spatial roles (entities (Trajector/Landmark) and spatial indicators).</p>
<p>First, we separately tokenize each sentence in the context and use a PLM (which is BERT here) to compute the tokens representation. Next, we apply a BIO tagging layer on tokens representations using (O, B-entity, I-entity, B-indicator, I-indicator) tags. A Softmax layer on BIO tagger output is used to select the spatial entities and spatial indicators with the highest probability. For training, we use CrossEntropy loss given the spatial annotation.</p>
<p>For the spatial relation extraction model, similar to (Yao et al., 2019; Shin et al., 2020), we use BERT and a classification layer to extract correct triplets. Given the output of the spatial role extraction model, for each combination of (spatial entity(tr), spatial_indicator $(s p)$, spatial entity $(l m))$ in each sentence we create an input ${ }^{6}$ and pass it to the BERT model. To indicate the position of each spatial role in the sentence, we use segment embeddings and add 1 if it is a role position and 0 otherwise.</p>
<p>The $[C L S]$ output of BERT will be passed to a one-layer MLP that provides the probability for the triplet (see Fig 3). Compared to the prior research,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Spatial role labeling model includes two separately trained modules. E: entity, SP: spatial_indicators. As an example, triplet (a grey house, front, A grey car) is correct and the "spatial_type $=$ FRONT", and (A grey car, front, a grey house) is incorrect, and the "spatial_type $=\mathrm{NaN}$ ".
we predict the spatial type for each triplet as an auxiliary task for spatial relation extraction. To this aim, we apply another multi-class classification layer ${ }^{7}$ on the same $[C L S]$ token. To train the model, we use a joint loss function for both relation and type modules (more detail in Appendix B).</p>
<h2>4 SPARTUN: Dataset Construction</h2>
<p>To provide a source of supervision for spatial language understanding tasks, we generate a synthetic dataset with SQA format that contains SPRL annotation of sentences. We build this dataset by expanding SPARTQA in multiple aspects. The following additional features are considered in creating SPARTUN:
F1) A broad coverage of various types of spatial relations and including rules of reasoning over their combinations (e.g. $\operatorname{NTPP}(a, b), \operatorname{LEFT}(b, c) \rightarrow$ $\operatorname{LEFT}(a, c)$ ) in various domains.
F2) A broad coverage of spatial language expressions and utterances used in various domains.
F3) Including extra annotations such as the supporting facts and number of reasoning steps for SQA to be used in complex modeling.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The data construction flow of SPARTUN. First, we generate scene graphs from NLVR images. Then a spatial reasoner validates each path between each pair of entities in this graph. All facts $(F)$ in the selected path and some extra facts $(E)$ from the scene graph are selected as story triplets, and the start and end nodes of the path are selected as question triplets. Finally, we pass all triplets to a text generation module and compute the final answer. We ignore paths with length one (e.g., $A(A B O V E) C$ ) and only keep questions that need multi-hop reasoning.</p>
<p>In the rest of this section, we describe the details of creating SPARTUN and the way we support the above mentioned features. Figure 4 depicts SPARTUN data construction flow.</p>
<p>Spatial Relation Computation. Following SPARTQA-AUTO, we use the NLVR scene graphs (Suhr et al., 2017) and compute relations between objects in each block based on their given coordinates. NLVR is limited to 2D relation types ${ }^{8}$, therefore to add more dimensions (FRONT and BEHIND), we randomly change the LEFT and RIGHT to BEHIND and FRONT in a subset of examples. Moreover, there are no relations between blocks in NLVR descriptions.</p>
<p>To expand the types of relations, we extend this limitation and randomly assign relations ${ }^{9}$ to the blocks while ensuring the spatial constraints are not violated. Then, we create a new scene graph with computed spatial relations. The nodes in this graph represent the entities (objects or blocks), and the directed edges are the spatial relations.</p>
<p>Question Selection. There are several paths between each pair of entities in the generated scene graph. We call a path valid if at least one relation can be inferred between its start and end nodes can be inferred. For example, in Figure 4, $\operatorname{NTPP}(A, X), \operatorname{FRONT}(X, Y), \operatorname{TPPI}(Y, B)$ is valid since it results in $\operatorname{FRONT}(A, B)$ while $\operatorname{NTPP}(A, X), \operatorname{NTPPI}(X, C)$ is not a valid path -there is no rules of reasoning that can be applied to infer new relations.</p>
<p>To verify the validity of each path, we pass its edges, represented as triplets in the predicatearguments form to a logical spatial reasoner (imple-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>mented in Prolog) and query all possible relations between the pair. The number of triplets in each path represents the number of reasoning steps for inferring the relation.</p>
<p>We generate the question triplets from the paths with the most steps of reasoning (edges). This question will ask about the spatial relationship between the head and tail entity of the selected path. The triplets in this path are used to generate the story and are annotated as supporting facts. Additionally, the story will include additional information (extra triplets) unnecessary for answering the question to increase the complexity of the task.</p>
<p>Spatial Reasoner. We implement several rules (in the form of Horn clauses shown in Table 2) in Prolog, which express the logic between the relation types (described in Table 1) in various formalisms and model the logical spatial reasoning computation (see Appendix B.1). Compared to previous tools (Wolter, 2009), we are the first to include the spatial, logical computation between multiple formalisms. This reasoner validates the question/queries based on the given facts. For instance, by using the Combination rule in Table 2 over the set of facts ${\operatorname{NTPP}(A, X), \operatorname{FRONT}(X, Y), \operatorname{TPPI}(Y, B)}$, the reasoner returns True for the query $\operatorname{FRONT}(A, B)$ and False for $\operatorname{FRONT}(B, A)$ or $\operatorname{BEHIND}(A, B)$.</p>
<p>Text generation. The scene description is generated from the selected story triplets in question selection phase and using a publicly available contextfree grammar (CFG) provided in SPARTQA-AUTO. However, we increase the variety of spatial expressions by using a vocabulary of various entity properties and relation expressions (e.g., above, over, or north for ABOVE relation type) taken from exist-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Not</th>
<th style="text-align: left;">$\forall(X, Y) \in$ Entities</th>
<th style="text-align: left;">$R \in{D i r \vee P P}$</th>
<th style="text-align: left;">IF $R(X, Y)$</th>
<th style="text-align: left;">$\Rightarrow \operatorname{NOT}(R _$reverse $(X, Y))$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inverse</td>
<td style="text-align: left;">$\forall(X, Y) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i r \vee P P}$</td>
<td style="text-align: left;">IF $R(Y, X)$</td>
<td style="text-align: left;">$\Rightarrow R _$reverse $(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Symmetry</td>
<td style="text-align: left;">$\forall(X, Y) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i s \vee(R C C-P P)}$</td>
<td style="text-align: left;">IF $R(Y, X)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Transitivity</td>
<td style="text-align: left;">$\forall(X, Y, Z) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i r \vee P P}$</td>
<td style="text-align: left;">IF $R(X, Z), R(Z, Y)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Combination</td>
<td style="text-align: left;">$\forall(X, Y, Z, H) \in$ Entities</td>
<td style="text-align: left;">$R \in D i r, * P P \in P P$</td>
<td style="text-align: left;">IF $* P P(X, Z), R(Z, H), * P P i(Z, Y)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Designed spatial rules. Dir: Directional relations (e.g., LEFT), Dis: Distance relations (e.g., FAR), $P P$ : all Proper parts relations (NTPP, NTPPI, TPPI, TPP), $R C C-P P$ : All RCC8 relation except proper parts relations. <em>PP: one of TPP or NTPP. </em>PPi: one of NTPPi or TPPi.
ing resources (Freeman, 1975; Mark et al., 1989; Lockwood et al., 2006; Stock et al., 2022; Herskovits, 1986) We map the relation types and the entity properties to the lexical forms in our collected vocabulary.</p>
<p>For the question text, we generate the entity description and relation expression for each question triplet. The entity description is generated based on a subset of its properties in the story. For instance, an expression such as "a black object" can be generated to refer to both "a big black circle" and "a black rectangle". We generate two question types, YN (Yes/No) questions that ask whether a specific relation exists between two entities, and FR (Find Relations) questions that ask about all possible relations between them. To make YN questions more complex, we add quantifiers ("all" and "any") to the entities' descriptions.</p>
<p>Our text generation method can flexibly use an extended vocabulary to provide a richer corpus to supervise new target tasks when required.</p>
<p>Finding Answers. We search all entities in the story based on the entity descriptions (e.g., all circles, a black object) in each question and use the spatial reasoner to find the final answer.</p>
<p>SpRL Annotations. Along with generating the sentences for the story and questions, we automatically annotate the described spatial configurations with spatial roles and relations (trajector, landmark, spatial indicator, spatial type, triplet, entity ids). These annotations are based on a previously proposed annotation scheme of SPRL and provide free annotations for the SPRL task.</p>
<p>To generate SPARTUN, we use 6.6 k NLVR scene graphs for training and 1 k for each dev and test set. We collect 20k training, 3 k dev, and 3 k test examples for each FR and YN question (see Table 3) ${ }^{10}$. On average, each story of SPARTUN contains eight sentences and 91 tokens that describe</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>on average 10 relations between different mentions of entities. More details about the dataset statistics can be seen in Appendix A.1.</p>
<h2>5 Experimental Results</h2>
<p>The focus of this paper is to provide a generic source of supervision for spatial language understanding tasks rather than proposing new techniques or architectures. Therefore, in the experiments, we analyze the impact of SPARTUN on SQA and SPRL using the PLM-based models described in Section 3.</p>
<p>In all experiments, we compare the performance of models fine-tuned with the target datasets with and without further pretraining on synthetic supervision (SynSup). All codes are publicly available ${ }^{11}$. The details of experimental settings and hyperparameters of datasets are provided in the Appendix.</p>
<h3>5.1 Spatial Question Answering</h3>
<p>Here, we evaluate the impact of SPARTUN and compare it with the supervision received from other existing synthetic datasets. Since the datasets that we use contain different question types, we supervise the models based on the same question type as the target task ${ }^{12}$.</p>
<p>The baselines for all experiments include a majority baseline (MB) which predicts the most repeated label as the answer to all questions, and a pretrained language model, that is, BERT here. We also report the human accuracy in answering the questions for the human-generated datasets ${ }^{13}$. For all experiments, to evaluate the models, we measure the accuracy which is the percentage of correct predictions in the test sets.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>5.1.1 SQA Evaluation Datasets</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">bAbI</td>
<td style="text-align: center;">8992</td>
<td style="text-align: center;">992</td>
<td style="text-align: center;">992</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (YN)</td>
<td style="text-align: center;">26152</td>
<td style="text-align: center;">3860</td>
<td style="text-align: center;">3896</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (FR)</td>
<td style="text-align: center;">25744</td>
<td style="text-align: center;">3780</td>
<td style="text-align: center;">3797</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (YN)</td>
<td style="text-align: center;">162</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">143</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (FR)</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: left;">RESQ</td>
<td style="text-align: center;">1008</td>
<td style="text-align: center;">333</td>
<td style="text-align: center;">610</td>
</tr>
<tr>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (YN)</td>
<td style="text-align: center;">20334</td>
<td style="text-align: center;">3152</td>
<td style="text-align: center;">3193</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (FR)</td>
<td style="text-align: center;">18400</td>
<td style="text-align: center;">2818</td>
<td style="text-align: center;">2830</td>
</tr>
</tbody>
</table>
<p>Table 3: Size of SQA benchmarks.
bAbI We use tasks 17 and 19 of bAbI. Task 17 is on spatial reasoning and contains binary Yes/No questions. Task 19 is on path finding and contains FR questions with answers in {LEFT, RIGHT, ABOVE, BELOW} set. The original dataset contains west, east, north, and south, which we mapped to their corresponding relative relation type.</p>
<p>SPARTQA-HUMAN is a small human-generated dataset containing YN and FR questions that need multi-hop spatial reasoning. The answer of YN questions is in ${$ Yes, No,DK $}$ where DK denotes Do not Know is used when the answer cannot be inferred from the context. The answer to FR questions is in ${$ left, right, above, below, near to, far from, touching, DK $}^{14}$.</p>
<p>StepGame is a synthetic SQA dataset containing FR questions which need $k$ reasoning steps to be answered $(k=1$ to 10$)$. The answer to each question is one relation in ${$ left, right, below, above, lower-left, upper-right, lower-right, upper-left $}$ set.</p>
<p>RESQ We created this dataset to reflect the natural complexity of real-world spatial descriptions and questions. We asked three volunteers (Englishspeaking undergrad students) to generate Yes/No questions for MSPRL dataset that contains complex human-generated sentences. The questions require at least one step of reasoning. The advantage of RESQ is that the human-generated spatial descriptions and their spatial annotations already exist in the original dataset. The statistics of this dataset are provided in Appendix A.2.</p>
<p>One of the challenges of the RESQ, which is not addressed here, is that the questions require spatial commonsense knowledge in addition to capturing</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Impact of using synthetic supervision on the bAbI tasks. All the models are further fine-tuned on the training set of task 17 (size $=1 \mathrm{k}$ ) and 19 (size $=500$ ), and test on bAbI test sets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">YN</th>
<th style="text-align: center;">FR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MB</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">53.60</td>
<td style="text-align: center;">24.52</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">$\mathbf{4 9 . 6 5}$</td>
<td style="text-align: center;">18.18</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">39.86</td>
<td style="text-align: center;">48.05</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">44.05</td>
<td style="text-align: center;">11.68</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">44.75</td>
<td style="text-align: center;">37.66</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">48.25</td>
<td style="text-align: center;">$\mathbf{5 0 . 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">90.69</td>
<td style="text-align: center;">95.23</td>
</tr>
</tbody>
</table>
<p>Table 5: Transfer learning on SPARTQA-HUMAN. SPARTQA-A stands for SPARTQA-AUTO.
the spatial semantics. For example, by using commonsense knowledge from the sentence, "a lamp hanging on the ceiling", we can infer that the lamp is above all the objects in the room. To compute the human accuracy, we asked two volunteers to answer 100 questions from the test set of RESQ and compute the accuracy.</p>
<h3>5.1.2 Transfer Learning in SQA</h3>
<p>The following experiments demonstrate the impact of transfer learning for SQA benchmarks considering different supervisions.</p>
<p>Due to the simplicity of bAbI dataset, PLM can solve this benchmark with $100 \%$ accuracy (Mirzaee et al., 2021). Hence we run our experiment on only 1 k and 500 training examples of task 17 and task 19, respectively. Table 4 demonstrates the impact of synthetic supervision on both tasks of bAbI. The results with various synthetic data are fairly similar for these two tasks. However, pretraining the model with the simple version of SPARTUN, named SPARTUN-S, performs better than other synthetic datasets on task 17. This can be due to the fewer relation expressions in SPARTUN-S, which follows the same structure as task 17.</p>
<p>In the next experiment, we investigate the impact of SPARTUN on SPARTQA-HUMAN result. Comparing the results in Table 5, we find that even though the classification layer for SPARTQA-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">k steps of reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">SynSup</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">TP-MANN</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">85.77</td>
<td style="text-align: center;">60.31</td>
<td style="text-align: center;">50.18</td>
<td style="text-align: center;">37.45</td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">28.53</td>
<td style="text-align: center;">26.45</td>
<td style="text-align: center;">23.67</td>
<td style="text-align: center;">22.52</td>
<td style="text-align: center;">21.46</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">98.44</td>
<td style="text-align: center;">94.77</td>
<td style="text-align: center;">91.78</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">57.56</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">45.17</td>
<td style="text-align: center;">39.69</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">33.62</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">98.63</td>
<td style="text-align: center;">94.95</td>
<td style="text-align: center;">91.94</td>
<td style="text-align: center;">77.74</td>
<td style="text-align: center;">68.37</td>
<td style="text-align: center;">61.67</td>
<td style="text-align: center;">57.95</td>
<td style="text-align: center;">50.82</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">44.03</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">$\mathbf{9 8 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2 1}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 4 6}$</td>
<td style="text-align: center;">77.93</td>
<td style="text-align: center;">69.53</td>
<td style="text-align: center;">62.14</td>
<td style="text-align: center;">57.37</td>
<td style="text-align: center;">48.79</td>
<td style="text-align: center;">44.67</td>
<td style="text-align: center;">42.72</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">98.55</td>
<td style="text-align: center;">95.02</td>
<td style="text-align: center;">92.04</td>
<td style="text-align: center;">$\mathbf{7 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 3 4}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 3 9}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 7 4}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 0 9}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 6 8}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Result of models with and without extra synthetic supervision on StepGame.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">Accu</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MB</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">50.21</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">57.37</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-AUTO</td>
<td style="text-align: center;">55.08</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">60.14</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">58.03</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">$\mathbf{6 3 . 6 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">90.38</td>
</tr>
</tbody>
</table>
<p>Table 7: Results with and without extra supervision on ReSQ. The Human accuracy is the performance of human on answering a subset of test set.</p>
<p>Auto and SPARTQA-Human are the same, the model trained on SPARTUN has a better transferability. It achieves $2.6 \%$ better accuracy on FR and $9 \%$ better accuracy on YN questions compared to SPARTQA-AUTO. YN is, yet, the most challenging question type in SPARTQA-HUMAN and none of the PLM-based models can reach even the simple majority baseline.</p>
<p>Table 6 demonstrates our experiments on StepGame. BERT without any extra supervision, significantly, outperforms the best reported model in Shi et al., TP-MANN, which is based on a neural memory network. As expected, all the PLM-based models almost solve the questions with one step of reasoning (i.e. where the answer directly exists in the text). However, with increasing the steps of reasoning, the performance of the models decreases. Comparing the impact of different synthetic supervision, SPARTUN achieves the best result on $k&gt;3$. For questions with $k&lt;=3$, SPARTUNS achieves competitive similar results compared to SPARTUN. Overall, the performance gap in SPARTUN-S, SPARTQA-AUTO and SPARTUN shows that more coverage of relation expressions in SPARTUN is effective.</p>
<p>In the next experiment, we show the influence of SPARTUN on real-world examples, which contain more types of spatial relations and need more rules of reasoning to be solved. Table 7 shows the
result of transfer learning on ReSQ. This result shows that the limited coverage of spatial relations and expression in SPARTQA-AUTO impacts the performance of BERT negatively. However, further pretraining BERT on SPARTUN-S improves the result on RESQ. This can be due to the higher coverage of relation types in SPARTUN-S than SPARTQA-AUTO. Using SPARTUN for further pretraining BERT has the best performance and improves the result by $5.5 \%$, indicating its advantage for transferring knowledge to solve real-world spatial challenges.</p>
<h3>5.2 Spatial Role Labeling</h3>
<p>Here, we analyze the influence of the extra synthetic supervision on SPRL task when evaluated on human-generated datasets. Table 8 shows the number of sentences in each SPRL benchmarks.</p>
<p>The pipeline model provided in Section 3, contains two main parts, a model for spatial role extraction (SRol) and a model for spatial relation extraction (SRel), which we analyze separately.</p>
<p>We further pretrain the BERT module in these models and then fine-tune it on the target domain. We use Macro F1-score (mean of F1 for all classes) to evaluate the performance of the SRol and SRel models.</p>
<h3>5.2.1 SPRL Evaluation Datasets</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (story)</td>
<td style="text-align: center;">25755</td>
<td style="text-align: center;">16214</td>
<td style="text-align: center;">16336</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (question)</td>
<td style="text-align: center;">23584</td>
<td style="text-align: center;">15092</td>
<td style="text-align: center;">15216</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (story)</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">272</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (question)</td>
<td style="text-align: center;">155</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">367</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (story)</td>
<td style="text-align: center;">48368</td>
<td style="text-align: center;">7031</td>
<td style="text-align: center;">7191</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (question)</td>
<td style="text-align: center;">38734</td>
<td style="text-align: center;">5970</td>
<td style="text-align: center;">6023</td>
</tr>
<tr>
<td style="text-align: left;">MSPRL</td>
<td style="text-align: center;">481</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">461</td>
</tr>
</tbody>
</table>
<p>Table 8: Number of sentences of SPRL benchmarks. To train the SPARTQA-AUTO, we only use the 3 k training examples ( 23 - 25 k sentences).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">MSPRL</th>
<th style="text-align: center;">SPARTQA-H</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">R-Inf</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">80.92</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">$\mathbf{8 8 . 5 9}$</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">57.28</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">88.03</td>
<td style="text-align: center;">$\mathbf{7 2 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Evaluating spatial role extraction (SRol) on two MSPRL and SPARTQA-HUMAN(SPARTQA-H) datasets with and without synthetic supervision.</p>
<p>MSPRL is a human-curated dataset provided on SPRL task. This dataset contains spatial description of real-world images and corresponding SPRL annotations (see Appendix A.6).</p>
<p>SPARTQA-HUMAN did not contain SPRL annotations. Hence, we asked two expert volunteers to annotate the story/questions of this dataset. Then another expert annotator checked the annotation and discarded the erroneous ones. As a result, half of this training data is annotated with SPRL tags.</p>
<h3>5.2.2 Transfer learning in SPRL</h3>
<p>Table 9 demonstrates the influence of synthetic supervision in spatial role extraction evaluated on MSPRL and SPARTQA-HUMAN.</p>
<p>We compare the result of SRol model with the previous SOTA, "R-Inf" (Manzoor and Kordjamshidi, 2018), on MSPRL dataset. R-Inf uses external multi-modal resources and global inference. All of the BERT-based SRol models outperform the R-Inf, which shows the power of PLMs for this task. However, since the accuracy of the SRol is already very high, using synthetic supervision shows no improvements compared to the model that only trained with MSPRL training set for the SRol. In contrast, on SPARTQA-HUMAN, using synthetic supervision helps the model perform better. Especially, using SPARTUN increases the performance of the SRol model dramatically, by $15 \%$.</p>
<p>In table 10, we show the result of SRel model (containing spatial relation extraction and spatial relation type classification) for spatial relation extraction, with and without extra supervision from synthetic data. Same as SRol model, extra supervision from SPARTUN achieves the best result when tested on SPARTQA-HUMAN.</p>
<p>For MSPRL, we compared the SRel model with R-Inf on spatial relation extraction. As table 10 demonstrates we improve the SOTA by $2.6 \%$ on F1 measure using SPARTUN as synthetic supervision. Also, model further pretrained on SPARTQAAuto gets lower result than model with no extra</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">SynSup</th>
<th style="text-align: center;">MSPRL</th>
<th style="text-align: center;">SPARTQA-H</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R-Inf</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.12</td>
<td style="text-align: center;">S: 48.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 49.46</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTQA-A</td>
<td style="text-align: center;">68.84</td>
<td style="text-align: center;">S: 58.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 55.17</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">71.23</td>
<td style="text-align: center;">S: 61:53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 63.22</td>
</tr>
</tbody>
</table>
<p>Table 10: Spatial relation extraction (SRel) on MSPRL and SPARTQA-HUMAN(SPARTQA-H) with and without synthetic supervision. Since the questions(Q) and stories(S) in SPARTQA-HUMAN have different annotations (questions have missing roles), we separately train and test this model on each.
supervision due to the limited relation expressions used in this data.</p>
<p>In conclusion, our experiments show the efficiency of SPARTUN in improving the performance of models on different benchmarks due to the flexible coverage of relation types and expressions.</p>
<h2>6 Conclusion and Future Work</h2>
<p>We created a new synthetic dataset as a source of supervision for transfer learning for spatial question answering (SQA) and spatial role labeling (SPRL) tasks. We show that expanding the coverage of relation types and combinations and spatial language expressions can provide a more robust source of supervision for pretraining and transfer learning. As a result, this data improves the models' performance in many experimental scenarios on both tasks when tested on various evaluation benchmarks. This data includes rules of spatial reasoning and the chain of logical reasoning for answering the questions that can be used for further research in the future.</p>
<p>Moreover, we provide a human-generated dataset on a realistic SQA task that can be used to evaluate the models and methods for spatial language understanding related tasks in real-world problems. This data is an extension of a previous benchmark on SPRL task with spatial semantic annotations. As a result, this dataset contains annotations for both SPRL and SQA tasks.</p>
<p>In future work, we plan to investigate explicit spatial reasoning over text by neuro-symbolic models. Moreover, using our methodology to generate synthetic spatial corpus in other languages or for other types of reasoning, such as temporal reasoning, is an exciting direction for future research.</p>
<h2>Limitations</h2>
<p>Though we aim for a broad coverage of relation types and relations, we collected this from our available resources and spatial lexicons but this is not by any means complete. There can be relations and expressions that are not covered. In particular, the relation expressions are limited to verbs and prepositions. The performance and reasoning ability of our models is improved with transfer learning but this is, certainly, far from the natural language understanding desiderata. Our models are based on large language models and need GPU resources to execute.</p>
<h2>Acknowledgements</h2>
<p>This project is supported by National Science Foundation (NSF) CAREER award 2028626 and partially supported by the Office of Naval Research (ONR) grant N00014-20-1-2005. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation nor the Office of Naval Research. We thank all reviewers for their helpful comments and suggestions. We also thank Sania Sinha and Timothy Moran for their help in the human data generation and annotations.</p>
<h2>References</h2>
<p>Reem Alrashdi and Simon O’Keefe. 2020. Automatic Labeling of Tweets for Crisis Response Using Distant Supervision, page 418-425. Association for Computing Machinery, New York, NY, USA.</p>
<p>Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12538-12547.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882-3890. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Surabhi Datta and Kirk Roberts. 2020. A hybrid deep learning approach for spatial trigger extraction from radiology reports. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2020, page 50. NIH Public Access.</p>
<p>Surabhi Datta, Yuqi Si, Laritza Rodriguez, Sonya E Shooshan, Dina Demner-Fushman, and Kirk Roberts. 2020. Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest x-ray reports using deep learning. Journal of biomedical informatics, 108:103473.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Saman Enayati, Ziyu Yang, Benjamin Lu, and Slobodan Vucetic. 2021. A visualization approach for rapid labeling of clinical notes for smoking status extraction. In Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances, pages 24-30, Online. Association for Computational Linguistics.</p>
<p>John Freeman. 1975. The modelling of spatial relations. Computer graphics and image processing, 4(2):156171.
A. Herskovits. 1986. Language and Spatial Cognition: An Interdisciplinary Study of the Prepositions in English. Cambridge University Press.</p>
<p>Maged N Kamel Boulos, Guochao Peng, and Trang VoPham. 2019. An overview of geoai applications in health and healthcare. International journal of health geographics, 18(1):1-9.</p>
<p>William G Kennedy, Magdalena D Bugajska, Matthew Marge, William Adams, Benjamin R Fransen, Dennis Perzanowski, Alan C Schultz, and J Gregory Trafton. 2007. Spatial representation and reasoning for human-robot collaboration. In AAAI, volume 7, pages $1554-1559$.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial Role Labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), pages 413-420. European Language Resources Association (ELRA).</p>
<p>Parisa Kordjamshidi, Taher Rahgooy, Marie-Francine Moens, James Pustejovsky, Umar Manzoor, and Kirk Roberts. 2017. Clef 2017: Multimodal spatial role</p>
<p>labeling (msprl) task overview. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 367-376. Springer.</p>
<p>Parisa Kordjamshidi, Martijn Van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing (TSLP), 8(3):1-36.</p>
<p>Dongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma, and Maarten de Rijke. 2021. Semi-supervised variational reasoning for medical dialogue generation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 544554 .</p>
<p>Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Srikumar. 2019. A logic-driven framework for consistency of neural models. arXiv preprint arXiv:1909.00126.</p>
<p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988.</p>
<p>Kate Lockwood, Ken Forbus, D Halstead, and Jeffrey Usher. 2006. Automatic categorization of spatial prepositions. In Proceedings of the 28th annual conference of the cognitive science society, pages 17051710 .</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, and Marie-Francine Moens. 2016. Deep embedding for spatial role labeling. arXiv preprint arXiv:1603.08474.</p>
<p>Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, and Alessandro Oltramari. 2021. Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In 35th AAAI Conference on Artificial Intelligence.</p>
<p>Arjun Magge, Davy Weissenbacher, Abeed Sarker, Matthew Scotch, and Graciela Gonzalez-Hernandez. 2018. Deep neural networks and distant supervision for geographic location mention extraction. Bioinformatics, 34(13):i565-i573.</p>
<p>Umar Manzoor and Parisa Kordjamshidi. 2018. Anaphora resolution for improving spatial relation extraction from text. In Proceedings of the First International Workshop on Spatial Language Understanding, pages 53-62.</p>
<p>David M Mark et al. 1989. Languages of spatial relations: Researchable questions \&amp; NCGIA research agenda. National Center for Geographic Information and Analysis Santa Barbara ....</p>
<p>Wouter Massa, Parisa Kordjamshidi, Thomas Provoost, and Marie-Francine Moens. 2015. Machine reading of biological texts: bacteria-biotope extraction. In Proceedings of the 6th international conference on bioinformatics models, methods and algorithms, pages 55-64. SCITEPRESS.</p>
<p>Alexey Mazalov, Bruno Martins, and David Matos. 2015. Spatial role labeling with convolutional neural networks. In Proceedings of the 9th Workshop on Geographic Information Retrieval, pages 1-7.</p>
<p>Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. 2020. Syntactic data augmentation increases robustness to inference heuristics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4582-4598, Online. Association for Computational Linguistics.</p>
<p>Alaeddine Moussa, Sébastien Fournier, Khaoula Mahmoudi, Bernard Espinasse, and Sami Faiz. 2021. Spatial role labeling based on improved pre-trained word embeddings and transfer learning. Procedia Computer Science, 192:1218-1226.</p>
<p>James Pustejovsky, Parisa Kordjamshidi, MarieFrancine Moens, Aaron Levine, Seth Dworman, and Zachary Yocum. 2015. Semeval-2015 task 8: Spaceeval. In Proceedings of the 9th International Workshop on Semantic Evaluation (semeval 2015), pages 884-894. ACL.</p>
<p>David A Randell, Zhan Cui, and Anthony G Cohn. 1992. A spatial logic based on regions and connection. $K R$, $92: 165-176$.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206.</p>
<p>Jochen Renz and Bernhard Nebel. 2007. Qualitative spatial reasoning using constraint calculi. In Handbook of spatial logics, pages 161-215. Springer.</p>
<p>Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460-1476, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022. Stepgame: A new benchmark for robust multi-hop</p>
<p>spatial reasoning in texts. In Proceedings of the Association for the Advancement of Artificial Intelligence, AAAI '22.</p>
<p>Hyeong Jin Shin, Jeong Yeon Park, Dae Bum Yuk, and Jae Sung Lee. 2020. Bert-based spatial information extraction. In Proceedings of the Third International Workshop on Spatial Language Understanding, pages $10-17$.</p>
<p>Kristin Stock, Christopher B Jones, Shaun Russell, Mansi Radke, Prarthana Das, and Niloofar Aflaki. 2022. Detecting geospatial location descriptions in natural language text. International Journal of Geographical Information Science, 36(3):547-584.</p>
<p>Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Emile Van Krieken, Erman Acar, and Frank Van Harmelen. 2019. Semi-supervised learning using differentiable reasoning. arXiv preprint arXiv:1908.04700.</p>
<p>Sagar Gubbi Venkatesh, Anirban Biswas, Raviteja Upadrashta, Vikram Srinivasan, Partha Talukdar, and Bharadwaj Amrutur. 2021. Spatial reasoning from natural language instructions for robot manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 11196-11202. IEEE.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Diedrich Wolter. 2009. Sparq-a spatial reasoning toolbox. In AAAI Spring Symposium: Benchmarking of Qualitative Spatial and Temporal Reasoning Systems, page 53 .</p>
<p>Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kgbert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193.</p>
<p>Yue Zhang, Quan Guo, and Parisa Kordjamshidi. 2021. Towards navigation by reasoning over spatial configurations. In Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics, pages 42-52, Online. Association for Computational Linguistics.</p>
<p>Yue Zhang and Parisa Kordjamshidi. 2022. Explicit object relation alignment for vision and language navigation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 322-331, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xiangxin Zhu, Carl Vondrick, Charless C Fowlkes, and Deva Ramanan. 2016. Do we need more training data? International Journal of Computer Vision, 119(1):76-92.</p>
<p>Jordan Zlatev. 2008. Holistic spatial semantics of thai. In Cognitive linguistics and non-Indo-European languages, pages 305-336. De Gruyter Mouton.</p>
<h2>A Datasets</h2>
<h2>A. 1 SPARTUN</h2>
<p>As we described in Section 4 to cover more spatial expressions and spatial relation types, we provide an extendable vocabulary of these spatial phenomena. The entire vocabulary of supported relation expressions and entity properties are provided in Figure 10.</p>
<p>Statistic information: Each example in SPARTUN contains a story that describe the spatial relation between entities and some questions which ask about indirect relations between entities. On average, each story contains eight sentences and 91 tokens, which describe ten relations on average.</p>
<p>We follow SPARTQA for dataset split. The number of questions in each train, dev, and test sets is provided in Table 3. YN questions can have two answers "Yes," which is the answer to $54 \%$ of questions, and "No," which is the answer to $46 \%$ of questions.</p>
<p>FR is a question type with multiple answers. In below, you can see the percentage of existence of each relation in the whole data: { left : $10 \%$, right: $10 \%$, above: $27 \%$, below: $26 \%$, behind: $19 \%$, front: $10 \%$, near: $2 \%$, far: $15 \%$, dc: $26 \%$, ec: $7 \%$, po: $0.2 \%$, tpp: $2 \%$, ntpp: $10 \%$, tppi: $3 \%$, and ntppi: $8 \%$ }</p>
<h2>A. 2 RESQ</h2>
<p>The RESQ dataset generated over the context of MSPRL dataset. For each group of sentences ( describing an image), we ask three volunteers (English-speaking undergraduate students) to generate at least four Yes/No questions. On average, they spent 20 minutes generating questions for each group of sentences which, in total, they spent 210 hours generating the whole data. After gathering the data, another undergrad student check the questions and remove the incorrect ones and keep the rest. The train set is provided on the train set of MSPRL, and since it does not have a dev set, we split the $32 \%$ of test data (equal to $20 \%$ of the training set) and keep it as the dev set. $50 \%$ of questions in this data are "Yes" and 50\% are "No". The static information of this dataset comes in Table 3.</p>
<p>To compute the human accuracy we ask two undergraduate students, one from those who create the questions and one new volunteer to answer 100 questions from the test set of RESQ. In the end a third students grade their answers.</p>
<h2>A. 3 bAbI</h2>
<p>This dataset is automatically generated data including samples with two sentences describing relationships between three objects and Yes/No questions asking about the existence of a relation between two objects (Fig 5) focuses on multi-hop spatial reasoning question answering.</p>
<h2>"The pink rectangle is below the red square. The red square is below the blue square," <br> 1. Is the red square below the pink rectangle? No <br> 2. Is the pink rectangle below the blue square? Yes</h2>
<p>Figure 5: An example of bAbI
bAbI task 19, contain questions asking about the directed path from one room to another. More statistic information of this dataset comes in table 3.</p>
<h2>A. 4 SPARTQA</h2>
<p>SPARTQA-AUTO contains more complex textual context (story) and questions requiring complex multi-hop spatial reasoning (e.g. Fig 6). This datasets contains one large synthesized (SPARTQAAuto) and a small human-generated (SPARTQAHUMAN) subsets.</p>
<p>One of the advantages of SPARTQA is the SPRL annotation of whole data (Contexts and Questions) provided with the main dataset. In this work, we also recruited two experts annotator which spent 270 hours annotating 2 k sentences in SPARTQAHUMAN using WebAnno framework ${ }^{15}$. Then another expert annotator checks their annotation and discards the wrong ones. The statistic information of SPARTQA comes in Table 3.</p>
<h2>A. 5 StepGame</h2>
<p>StepGame is another synthesized datasets described in this paper. You can check a sample of this dataset in Figure 7.</p>
<h2>A. 6 MSPRL</h2>
<p>SPRL is the task of identifying and classifying the spatial arguments of the spatial expressions mentioned in a sentence (Kordjamshidi et al., 2010). The MSPRL(Kordjamshidi et al., 2017) is a dataset provided on SPRL task.. The statistic data of this dataset comes in Table 11. A SPRL can have following spatial semantic component (Zlatev, 2008) on the static environment, trajector (the main</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>STORY:</h1>
<p>We have three blocks, A, B and C. Block B is to the right of block C and it is below block A. Block A has two black medium squares. Medium black square number one is below medium black square number two and a medium blue square. It is touching the bottom edge of this block. The medium blue square is below medium black square number two. Block B contains one medium black square. Block C contains one medium blue square and one medium black square. The medium blue square is below the medium black square.</p>
<h2>QUESTIONS:</h2>
<p>FB: Which block(s) has a medium thing that is below a black square? A, B, C
FB: Which block(s) doesn't have any blue square that is to the left of a medium square? A, B
FR: What is the relation between the medium black square which is in block $C$ and the medium square that is below a medium black square that is touching the bottom edge of a block? Left
CO: Which object is above a medium black square? the medium black square which is in block C or medium black square number two? medium black square number two
YN: Is there a square that is below medium square number two above all medium black squares that are touching the bottom edge of a block? Yes</p>
<p>Figure 6: An example of SPARTQA-AUTO</p>
<div class="codehilite"><pre><span></span><code><span class="n">Story</span><span class="o">:</span>
<span class="w">    </span><span class="mi">0</span><span class="o">:</span><span class="s2">&quot;B is south east of J.&quot;</span>
<span class="w">    </span><span class="mi">1</span><span class="o">:</span><span class="s2">&quot;X is under E.&quot;</span>
<span class="w">    </span><span class="mi">2</span><span class="o">:</span><span class="s2">&quot;H is to the left of Z and is on the same horizontal plane.&quot;</span>
<span class="w">    </span><span class="mi">3</span><span class="o">:</span><span class="s2">&quot;If L is the center of a clock face, E is located between 10 and 11.&quot;</span>
<span class="w">    </span><span class="mi">4</span><span class="o">:</span><span class="s2">&quot;G is positioned above O.&quot;</span>
<span class="w">    </span><span class="mi">5</span><span class="o">:</span><span class="s2">&quot;Q is diagonally to the bottom right of L.&quot;</span>
<span class="w">    </span><span class="mi">6</span><span class="o">:</span><span class="s2">&quot;C and S are horizontal and C is to the left of 0.&quot;</span>
<span class="w">    </span><span class="mi">7</span><span class="o">:</span><span class="s2">&quot;I is above 0 with a small gap between them.&quot;</span>
<span class="w">    </span><span class="mi">8</span><span class="o">:</span><span class="s2">&quot;E is above N and to the left of N.&quot;</span>
<span class="w">    </span><span class="mi">9</span><span class="o">:</span><span class="s2">&quot;Q is below and to the right of 0.&quot;</span>
<span class="w">    </span><span class="mi">10</span><span class="o">:</span><span class="s2">&quot;X is to the left of C with a small gap between them.&quot;</span>
</code></pre></div>

<p>question:"What is the relation of the agent $L$ to the agent $J$ ? "lower-right"</p>
<p>Figure 7: StepGame. An example of questions which need 10 steps of reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sentences</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">613</td>
<td style="text-align: center;">1213</td>
</tr>
<tr>
<td style="text-align: left;">Trajectors</td>
<td style="text-align: center;">716</td>
<td style="text-align: center;">874</td>
<td style="text-align: center;">1590</td>
</tr>
<tr>
<td style="text-align: left;">Landmarks</td>
<td style="text-align: center;">612</td>
<td style="text-align: center;">573</td>
<td style="text-align: center;">1185</td>
</tr>
<tr>
<td style="text-align: left;">Spatial Indicators</td>
<td style="text-align: center;">666</td>
<td style="text-align: center;">795</td>
<td style="text-align: center;">1461</td>
</tr>
<tr>
<td style="text-align: left;">Spatial Triplets</td>
<td style="text-align: center;">761</td>
<td style="text-align: center;">939</td>
<td style="text-align: center;">1700</td>
</tr>
</tbody>
</table>
<p>Table 11: MSPRL statistics.
entity), landmark(the reference entity), and spatial_indicator (the spatial term describing the relationship between trajector and landmark.). The dynamic environment can also have path, region, direction, and motion. To understand MSPRL better you can take a look at Figure 8. In this figure the spatial value assigned to each spatial triplet can be chosen from Table 1.</p>
<p>The white car in the street, is in front of the blue building.</p>
<p>Trajector $I=$ The white car
Landmark $I=$ the street
Spatial_indicator $=i n$
Genaral type $=$ Topological (RCC8 (TPP))
TPP(the white car, the street)</p>
<p>Trajector $I=$ The white car
Landmark $I=$ the blue building
Spatial_indicator $=$ in front
Genaral type $=$ Directional (FRONT)
FRONT(the white car, the blue building)</p>
<p>Figure 8: Spatia Role Labeling (SpRL).</p>
<h2>B Models and modules</h2>
<p>We use the huggingFace ${ }^{16}$ implementation of pretrained BERT base which has 768 hidden dimensions. All models are trained on the training set, evaluated on the dev set, and reported the result on the test set. For training, we train the model until no changes happen on the dev set and then store and use the best model on the dev set. We use AdamW ((Loshchilov and Hutter, 2017)) optimizer on all models and modules.</p>
<p>For SQA tasks we use Focal Loss (Lin et al., 2017) with $\gamma=2$. For spatial argument extraction,</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we use cross-entropy loss for BIO-tagging, and for spatial relation extraction, we use the summation of loss for each spatial relation and relation type classification part.</p>
<p>$$
\begin{aligned}
\text { Loss }= &amp; \sum \text { CrossEntropyLoss }\left(\mathrm{p}^{\prime}, \mathrm{y}^{\prime}\right) \
&amp; +\operatorname{BCELoss}(\mathrm{p}, \mathrm{y})
\end{aligned}
$$</p>
<p>The rest of experimental setting such as number of epochs, batch size, and learning rate are provided in Table 13. This settings are chosen after trial and test on the dev set of the target task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">YN</th>
<th style="text-align: center;">FR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">92.83</td>
<td style="text-align: center;">93.66</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN-Simple</td>
<td style="text-align: center;">90.30</td>
<td style="text-align: center;">93.66</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN-Clock</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">87.13</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA</td>
<td style="text-align: center;">82.05</td>
<td style="text-align: center;">94.17</td>
</tr>
</tbody>
</table>
<p>Table 12: Result of BERT (SQA) model trained and test on two synthetic supervision data.</p>
<p>Besides, The result of BERT model trained on SPARTUN and SPARTUN and tested on the same dataset are provided in Table 12. SPARTUNSimple only contains one spatial expresion for each relation types, and SPARTUN-Clock contains all relation expression plus clock expressions (Column 5 in Table 10a) for relation types.</p>
<h1>B. 1 Logic-based spatial reasoner</h1>
<p>We consider the logic rules mentioned in Figure 2 and in the form of the Horn clauses. we collect the different combinations of spatial relations mentioned in Table 1 and implement the logic-based spatial reasoner. Figure 9a shows an example of some parts of our code on LEFT relation. In Figure 9 b , on the left, some facts are given, and the query " $n t p p i($ room, $X)$ " ask about all objects that existed in the room. Below each query, there are all possible predictions for them.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">DS</th>
<th style="text-align: center;">epochs</th>
<th style="text-align: center;">batch size</th>
<th style="text-align: center;">learning rate</th>
<th style="text-align: center;">classifier type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTQA YN</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTQA FR</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTUN YN</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTUN FR</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SQA experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-0$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPRL experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SRol</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-05$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$4 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$8 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-05$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$6 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 13: The hyperparameters and setups information for each experiment. The first three rows are related to further pretraining model on the synthetic data. These models are used in the other experiments as the further pretrained models.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Examaple of implemented rule clauses in Prolog.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Example of Facts, Query, and answer of implemented model</p>
<p>Figure 9: Logic-bases spatial reasoner</p>
<table>
<thead>
<tr>
<th style="text-align: center;">formalism</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cardinals</th>
<th style="text-align: center;">Clocks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Directional</td>
<td style="text-align: center;">left</td>
<td style="text-align: center;">"to the left of", "on the left side of", "to the left-hand side of"</td>
<td style="text-align: center;">"west of", "to the west of"</td>
<td style="text-align: center;">"at 9:00 position relative to", "at 9: <br> 00 position regarding to", "at 9 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">right</td>
<td style="text-align: center;">"to the right of", "on the right side of", "to the right-hand side of"</td>
<td style="text-align: center;">"east of", "to the east of"</td>
<td style="text-align: center;">"at 3:00 position relative to", "at 3: <br> 00 position regarding to", "at 3 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">below</td>
<td style="text-align: center;">"above", "over"</td>
<td style="text-align: center;">"north of", "to the north of"</td>
<td style="text-align: center;">"at 12:00 position relative to", "at 12:00 position regarding to", "at 12 o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">above</td>
<td style="text-align: center;">"below", "under"</td>
<td style="text-align: center;">"south of", "to the south of"</td>
<td style="text-align: center;">"at 6:00 position relative to", "at 6: <br> 00 position regarding to", "at 6 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">behind</td>
<td style="text-align: center;">"behind"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">front</td>
<td style="text-align: center;">"in front of"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Distances</td>
<td style="text-align: center;">far</td>
<td style="text-align: center;">"far from", "farther from", "away from"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">near</td>
<td style="text-align: center;">"near to", "close to"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Topological</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">disconnected from</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EC</td>
<td style="text-align: center;">"touch[es]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">"overlap[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EQ</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">"covered by", "inside and touching"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPPI</td>
<td style="text-align: center;">"cover[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">"in", "inside", "within"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NTPPI</td>
<td style="text-align: center;">"ha[s/ve]","contain[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) List of relation expression supported in SPARTUN.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">properties</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">Block, box</td>
</tr>
<tr>
<td style="text-align: center;">blocks</td>
<td style="text-align: center;">Blocks, boxes</td>
</tr>
<tr>
<td style="text-align: center;">object_general_name</td>
<td style="text-align: center;">thing, object, shape, fruit</td>
</tr>
<tr>
<td style="text-align: center;">objects_general_name</td>
<td style="text-align: center;">things, objects, shapes, fruits</td>
</tr>
<tr>
<td style="text-align: center;">block_name</td>
<td style="text-align: center;">AAA, BBB, CCC, DDD, EEE, JJJ, HHH, JJJ, LLL, KKK, one, two, three.</td>
</tr>
<tr>
<td style="text-align: center;">color</td>
<td style="text-align: center;">yellow, black, blue, green, red, orange, grey, white, purple</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
<td style="text-align: center;">small, big, medium, midsize, large, tiny, little</td>
</tr>
<tr>
<td style="text-align: center;">type</td>
<td style="text-align: center;">circle, oval, square, rectangle, dimond, star, triangle, hexagon, pentagon, watermelon, apple, melon,</td>
</tr>
<tr>
<td style="text-align: center;">types</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(b) List of entities properties supported in SPARTUN</p>
<p>Figure 10: The supported relation expression and entities properties in SPARTUN, which can easily extended based on the target task.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ https://huggingface.co/transformers/v2.9.1/ model_doc/bert.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{11}$ https://github.com/HLR/Spatial-QA-tasks
${ }^{12}$ StepGame only has FR question types. Hence, we use the model trained on FR questions for both FR and YN target tasks.
${ }^{13}$ All human results gathered by scoring the human answers over a subset of the test set.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
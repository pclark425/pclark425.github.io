<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3881 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3881</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3881</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-76b99439d524ae1813c30edc4bcad487a30a1f8c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76b99439d524ae1813c30edc4bcad487a30a1f8c" target="_blank">UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper explores targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction, and shows how ChatGPT can be distilled into much smaller UniversalNER models for open NER.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3881.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3881.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniNER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniversalNER (Targeted distillation via mission-focused instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted-distillation system that uses an LLM (ChatGPT) to generate large-scale, diverse instruction-tuning data from a broad unlabeled corpus (the Pile), and finetunes smaller LLaMA-based student models to perform open, type-guided NER across many domains and thousands of entity types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>UniversalNER (mission-focused instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Uses ChatGPT (gpt-3.5-turbo-0301) as a teacher to annotate passages sampled from a large web-derived corpus (the Pile) with entity mentions and types; then instruction-tunes LLaMA-based student models in a conversation-style per-type query format to produce JSON lists of entities per queried type. Key components: diverse input sampling from Pile, ChatGPT annotation (temperature=0), per-type conversational queries, negative sampling of entity types, optional supervised fine-tuning with dataset-specific templates.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Passages sampled from the Pile corpus (Gao et al., 2020). The authors chunked Pile articles to max 256-token passages and randomly sampled 50K passages; after filtering, the ChatGPT-annotated dataset comprised 45,889 passage–output pairs with 240,725 entity mentions and 13,020 distinct entity types. A definition-based variant produced 353,092 entity type definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Conversation-style natural language queries. For each entity type t_i present in an annotation, the prompt asks e.g. "What describes <t_i>?" (i.e., free-text natural language queries per entity type). The authors also experimented with asking for all types in a single query and with definition-based prompts replacing type names with short sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-step distillation: (1) Teacher labeling — prompt ChatGPT to extract entities and assign types (or generate type definitions) for sampled passages (temperature=0); (2) Student instruction tuning — finetune LLaMA (7B/13B) on conversation-style examples using a language-modeling objective to generate structured JSON lists of entities per queried type. Additional techniques: negative sampling (sample entity types not present and expect empty JSON lists), dataset-specific prompt templates for supervised finetuning, and optional joint supervised + distilled training.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured JSON lists: for each natural-language query about an entity type, the model outputs a JSON list of all entity mentions (strings) of that type in the passage. Variants include definition-based type queries and 'all-in-one' queries that request all types in one response.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated on a large assembled UniversalNER benchmark of 43 NER datasets across 9 domains (general, biomedical, clinical, STEM, programming, social media, law, finance, transportation). Metrics: strict entity-level micro-F1 (exact type and boundary match) and a partial-match variant (overlap counts as half true positive). Comparisons vs ChatGPT (gpt-3.5-turbo-0301), Vicuna, and supervised baselines (InstructUIE, BERT). Ablation studies on negative sampling, prompt templates, and data variants.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Zero-shot: UniNER-7B and UniNER-13B attained average F1 of 41.7% and 43.4% respectively vs ChatGPT's 34.9% (average). UniNER models outperformed Vicuna by large margins (UniNER > Vicuna by >30 absolute points in average F1 per reported summaries). Supervised / continual fine-tuning improved results further (e.g., continual fine-tuning UniNER-7B achieved out-of-domain average F1 60.0% and in-domain supervised average F1 84.78% on 20 datasets). Definition-based variant was more robust to paraphrased type names but performed worse on standard NER benchmarks; 'all-in-one' query variant underperformed single-type queries by ~3.3% on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include sensitivity of the 'type-name' variant to paraphrasing of entity types (i.e., lower robustness to synonymic queries), heavy-tail distribution of generated entity types, potential mismatches between open-world generated types and closed-world evaluation (addressed via negative sampling), decreased benchmark performance for definition-based variants despite improved paraphrase robustness, and label-definition inconsistencies across supervised datasets. The method depends on teacher LLM annotations (so teacher errors/biases can propagate), and the authors note challenges reconciling dataset label disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>UniNER is compared against ChatGPT, Vicuna, and InstructUIE (supervised multi-task IE). UniNER (zero-shot distilled) outperforms ChatGPT on average F1 reported and substantially outperforms Vicuna; UniNER also outperforms InstructUIE despite the latter being trained on supervised IE datasets. The paper reports numerical gaps (e.g., UniNER-13B 43.4% vs ChatGPT 34.9% avg F1; supervised UniNER variants further surpass InstructUIE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3881.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (instruction-tuned LLaMA using ShareGPT dialogs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-following chatbot obtained by finetuning LLaMA on ShareGPT conversation data (real ChatGPT user conversations), used here as a baseline for instruction-tuned student models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Vicuna (ChatGPT imitation via ShareGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Finetunes LLaMA on a corpus of real ChatGPT conversations (ShareGPT) to imitate ChatGPT-style responses; treated as an off-the-shelf instruction-tuned student model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ShareGPT conversation dataset (real user–ChatGPT dialogues) as used by Vicuna (details referenced but not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>General natural-language instructions in conversational format (dialogue turns from ShareGPT); used with the NER prompting template during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised finetuning of LLaMA on conversation data collected from ChatGPT interactions (no additional per-type distilled annotation pipeline described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Open-ended conversational responses; in this study used to produce NER outputs under the same prompting templates as other LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Zero-shot evaluation on the UniversalNER benchmark using the NER prompting template (and strict micro-F1 / partial-match metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Vicuna performed substantially worse than UniNER and ChatGPT on the UniversalNER zero-shot benchmark; it performed better than vanilla LLaMA/Alpaca in some settings but trailed ChatGPT by over 20 absolute F1 points and was outperformed by UniNER by large margins (reported average differences).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although trained on real ChatGPT conversations, Vicuna still trails teacher LLMs on targeted benchmarks such as open-domain NER; lacks the mission-focused instruction tuning on diverse per-type queries used by UniNER.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared directly in experiments: Vicuna-7B and Vicuna-13B had substantially lower average F1 than UniNER and ChatGPT on the UniversalNER benchmark (exact per-domain numbers reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3881.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (instruction-tuned LLaMA via synthetic instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned student model obtained by generating instruction–response pairs (self-instruct style) from a teacher LLM and finetuning LLaMA; cited as prior work on LLM-to-student distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stanford alpaca: An instruction-following llama model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Alpaca (self-instruct distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Generates instruction data (via LLM-based methods) and finetunes LLaMA to follow instructions; representative of general-purpose instruction-tuned distilled models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Instruction–response pairs automatically generated by an LLM teacher (paper cites Alpaca but does not provide Alpaca's detailed corpus within this work).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language instructions, diverse tasks produced by LLM-generated instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Teacher-generated instruction data followed by finetuning of smaller LM (instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>General instruction-following text responses across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Discussed in related work: Alpaca demonstrates ability to imitate ChatGPT but remains behind teacher on many downstream applications; no new evaluation performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited statement: instruction-tuned students like Alpaca can imitate ChatGPT but still trail teacher LLMs by large margins on targeted downstream tasks (citing Gudibande et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generic distillation across many tasks yields only a shallow approximation of teacher capabilities for specialized downstream applications; Alpaca lacks mission-focused tuning that UniNER applies for NER.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Mentioned qualitatively as inferior to teacher LLMs on many targeted tasks; no quantitative comparison within this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3881.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructUIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructUIE (Multi-task instruction-tuned unified information extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised multi-task instruction-tuned system for unified information extraction built on Flan-T5-11B and trained on diverse supervised IE datasets; used here as a strong supervised baseline for NER and IE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructuie: Multi-task instruction tuning for unified information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>InstructUIE</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A supervised instruction-tuned model for unified information extraction that converts IE datasets into natural-language instruction–response format and trains a Flan-T5-11B backbone to generate structured IE outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A collection of supervised information-extraction datasets (details not enumerated here beyond reference to Wang et al., 2023a); training is supervised rather than teacher-LLM-generated.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instruction prompts describing IE/NER tasks (unified generation objective).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised multi-task instruction tuning on labeled IE datasets (not LLM-to-student distillation in this paper's description).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated structured outputs for IE (used as NER predictions for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Reported comparisons on UniversalNER benchmark (both zero-shot and supervised/in-domain or out-of-domain evaluations). The authors re-evaluated InstructUIE after fixing a reported evaluation bug in InstructUIE's original script.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>UniNER (distilled from ChatGPT) outperformed InstructUIE by a large margin in zero-shot and out-of-domain settings despite InstructUIE being trained on supervised IE datasets; supervised/continual finetuning further improved UniNER to exceed InstructUIE in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>InstructUIE relies on supervised datasets and can suffer from label-definition conflicts across datasets; in this paper it yielded lower zero-shot generalization compared to the targeted distilled UniNER.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Directly compared in experiments; UniNER (both zero-shot and supervised variants) outperformed InstructUIE-11B on average F1 in reported evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3881.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gu2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events (Gu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work that focuses on distilling task-level abilities from LLMs for biomedical information extraction, specifically a case study on adverse drug event extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Distillation for biomedical knowledge extraction (Gu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced as an example of prior work distilling task-level abilities from LLMs into smaller models for biomedical IE tasks; specifics of the method are not detailed in this UniversalNER paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this paper; by the title, the original work targets biomedical text and adverse drug event extraction datasets (details to be obtained from the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified in this paper; the cited work is described as a case study on adverse drug events.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Described in this paper as part of the literature on distilling task-level abilities from LLMs; details (e.g., whether teacher-generated labels or other techniques were used) are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Goal is biomedical knowledge extraction (e.g., adverse drug event triples or entity/relation annotations); exact formats not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described in this paper; presumed to be task-specific evaluation on biomedical IE benchmarks in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of distilling LLMs for task-level biomedical IE; the UniversalNER paper does not report numbers from Gu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper beyond the general point that prior distillation studies often focus on certain datasets or domains, whereas UniversalNER aims at a broader application-class formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3881.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jung2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impossible Distillation (Jung et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study proposing an efficient method to distill a much smaller model that can outperform larger models on specialized tasks (summarization and paraphrasing) by generating high-quality datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Impossible Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced as prior work that generates data and distills a small student model to outperform larger models on specific tasks (summarization and paraphrasing); the UniversalNER paper cites it as part of task-level distillation literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; original work focuses on task-specific data generation and distillation for summarization/paraphrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Described briefly: an efficient method to distill an order-of-magnitude smaller model that can outperform GPT-3 on specialized tasks; details are in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Task-specific outputs for summarization and paraphrasing; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed here; referenced as prior art showing task-level distillation success.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited claim: distillation produced a smaller model that outperforms GPT-3 on specialized summarization and paraphrasing tasks in certain domains (as reported in Jung et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper notes that prior studies often perform distillation on certain datasets/domains rather than a general application-class; no further limitations about Jung et al. are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3881.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hsieh2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilling step-by-step! (Hsieh et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study on distilling chain-of-thought and reasoning abilities from LLMs into smaller models, cited as an example of reasoning-oriented distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Chain-of-thought distillation (Hsieh et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced work that distills LLMs' reasoning (chain-of-thought) capabilities into smaller models; cited as part of the literature on distilling specialized abilities (reasoning) from large models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper; original work focuses on chain-of-thought style reasoning examples.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Chain-of-thought distillation (teacher LLM provides step-by-step reasoning traces used to train the student); exact protocol not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Reasoned step-by-step explanations and final answers; not expanded upon here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not provided in this paper; referenced as prior art demonstrating reasoning distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as demonstrating that distilling chain-of-thought can help smaller models outperform larger models on some tasks (as reported in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No limitations discussed in this paper beyond the general observation that prior works are often task- or domain-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3881.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3881.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Instruct (automatic instruction generation by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method/pipeline (cited) for automatically generating diverse instruction–response pairs using LLMs themselves to bootstrap instruction-tuning datasets (used in Alpaca and related efforts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-instruct: Aligning language models with self-generated instructions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Self-Instruct (LLM-generated instruction data)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Generates instruction-following training data by prompting an LLM to produce diverse instructions and corresponding responses, then finetunes a student LM on these pairs. Cited as a general mechanism for producing instruction data from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the UniversalNER paper; generally involves teacher LLM outputs used to create synthetic instruction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instructions automatically produced by an LLM; used as training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Teacher LLM used to self-generate instruction–response pairs; student finetuning on this synthetic data (instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Instruction–response textual pairs across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed here; Self-Instruct is cited as a technique underlying other instruction-tuning works.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as enabling instruction-tuned models (e.g., Alpaca) but such general distillation can fall short on mission-specific downstream tasks, motivating the UniNER mission-focused approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generic instruction-generation approaches may not cover diverse input distributions for a specific application class; UniNER addresses this by sampling diverse inputs from a large unlabeled corpus instead of relying on LLMs to invent inputs for every domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events <em>(Rating: 2)</em></li>
                <li>Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing <em>(Rating: 2)</em></li>
                <li>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes <em>(Rating: 2)</em></li>
                <li>Stanford alpaca: An instruction-following llama model <em>(Rating: 1)</em></li>
                <li>Self-instruct: Aligning language models with self-generated instructions <em>(Rating: 1)</em></li>
                <li>Instructuie: Multi-task instruction tuning for unified information extraction <em>(Rating: 2)</em></li>
                <li>Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality <em>(Rating: 1)</em></li>
                <li>SciREX: A challenge dataset for document-level information extraction <em>(Rating: 1)</em></li>
                <li>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3881",
    "paper_id": "paper-76b99439d524ae1813c30edc4bcad487a30a1f8c",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "UniNER",
            "name_full": "UniversalNER (Targeted distillation via mission-focused instruction tuning)",
            "brief_description": "A targeted-distillation system that uses an LLM (ChatGPT) to generate large-scale, diverse instruction-tuning data from a broad unlabeled corpus (the Pile), and finetunes smaller LLaMA-based student models to perform open, type-guided NER across many domains and thousands of entity types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "UniversalNER (mission-focused instruction tuning)",
            "system_or_method_description": "Uses ChatGPT (gpt-3.5-turbo-0301) as a teacher to annotate passages sampled from a large web-derived corpus (the Pile) with entity mentions and types; then instruction-tunes LLaMA-based student models in a conversation-style per-type query format to produce JSON lists of entities per queried type. Key components: diverse input sampling from Pile, ChatGPT annotation (temperature=0), per-type conversational queries, negative sampling of entity types, optional supervised fine-tuning with dataset-specific templates.",
            "input_corpus_description": "Passages sampled from the Pile corpus (Gao et al., 2020). The authors chunked Pile articles to max 256-token passages and randomly sampled 50K passages; after filtering, the ChatGPT-annotated dataset comprised 45,889 passage–output pairs with 240,725 entity mentions and 13,020 distinct entity types. A definition-based variant produced 353,092 entity type definitions.",
            "topic_or_query_specification": "Conversation-style natural language queries. For each entity type t_i present in an annotation, the prompt asks e.g. \"What describes &lt;t_i&gt;?\" (i.e., free-text natural language queries per entity type). The authors also experimented with asking for all types in a single query and with definition-based prompts replacing type names with short sentences.",
            "distillation_method": "Two-step distillation: (1) Teacher labeling — prompt ChatGPT to extract entities and assign types (or generate type definitions) for sampled passages (temperature=0); (2) Student instruction tuning — finetune LLaMA (7B/13B) on conversation-style examples using a language-modeling objective to generate structured JSON lists of entities per queried type. Additional techniques: negative sampling (sample entity types not present and expect empty JSON lists), dataset-specific prompt templates for supervised finetuning, and optional joint supervised + distilled training.",
            "output_type_and_format": "Structured JSON lists: for each natural-language query about an entity type, the model outputs a JSON list of all entity mentions (strings) of that type in the passage. Variants include definition-based type queries and 'all-in-one' queries that request all types in one response.",
            "evaluation_or_validation_method": "Evaluated on a large assembled UniversalNER benchmark of 43 NER datasets across 9 domains (general, biomedical, clinical, STEM, programming, social media, law, finance, transportation). Metrics: strict entity-level micro-F1 (exact type and boundary match) and a partial-match variant (overlap counts as half true positive). Comparisons vs ChatGPT (gpt-3.5-turbo-0301), Vicuna, and supervised baselines (InstructUIE, BERT). Ablation studies on negative sampling, prompt templates, and data variants.",
            "results_summary": "Zero-shot: UniNER-7B and UniNER-13B attained average F1 of 41.7% and 43.4% respectively vs ChatGPT's 34.9% (average). UniNER models outperformed Vicuna by large margins (UniNER &gt; Vicuna by &gt;30 absolute points in average F1 per reported summaries). Supervised / continual fine-tuning improved results further (e.g., continual fine-tuning UniNER-7B achieved out-of-domain average F1 60.0% and in-domain supervised average F1 84.78% on 20 datasets). Definition-based variant was more robust to paraphrased type names but performed worse on standard NER benchmarks; 'all-in-one' query variant underperformed single-type queries by ~3.3% on average.",
            "limitations_or_challenges": "Reported limitations include sensitivity of the 'type-name' variant to paraphrasing of entity types (i.e., lower robustness to synonymic queries), heavy-tail distribution of generated entity types, potential mismatches between open-world generated types and closed-world evaluation (addressed via negative sampling), decreased benchmark performance for definition-based variants despite improved paraphrase robustness, and label-definition inconsistencies across supervised datasets. The method depends on teacher LLM annotations (so teacher errors/biases can propagate), and the authors note challenges reconciling dataset label disagreements.",
            "comparison_to_baselines_or_humans": "UniNER is compared against ChatGPT, Vicuna, and InstructUIE (supervised multi-task IE). UniNER (zero-shot distilled) outperforms ChatGPT on average F1 reported and substantially outperforms Vicuna; UniNER also outperforms InstructUIE despite the latter being trained on supervised IE datasets. The paper reports numerical gaps (e.g., UniNER-13B 43.4% vs ChatGPT 34.9% avg F1; supervised UniNER variants further surpass InstructUIE).",
            "uuid": "e3881.0",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (instruction-tuned LLaMA using ShareGPT dialogs)",
            "brief_description": "An open-source instruction-following chatbot obtained by finetuning LLaMA on ShareGPT conversation data (real ChatGPT user conversations), used here as a baseline for instruction-tuned student models.",
            "citation_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality",
            "mention_or_use": "use",
            "system_or_method_name": "Vicuna (ChatGPT imitation via ShareGPT)",
            "system_or_method_description": "Finetunes LLaMA on a corpus of real ChatGPT conversations (ShareGPT) to imitate ChatGPT-style responses; treated as an off-the-shelf instruction-tuned student model baseline.",
            "input_corpus_description": "ShareGPT conversation dataset (real user–ChatGPT dialogues) as used by Vicuna (details referenced but not expanded in this paper).",
            "topic_or_query_specification": "General natural-language instructions in conversational format (dialogue turns from ShareGPT); used with the NER prompting template during evaluation.",
            "distillation_method": "Supervised finetuning of LLaMA on conversation data collected from ChatGPT interactions (no additional per-type distilled annotation pipeline described in this paper).",
            "output_type_and_format": "Open-ended conversational responses; in this study used to produce NER outputs under the same prompting templates as other LLM baselines.",
            "evaluation_or_validation_method": "Zero-shot evaluation on the UniversalNER benchmark using the NER prompting template (and strict micro-F1 / partial-match metrics).",
            "results_summary": "Vicuna performed substantially worse than UniNER and ChatGPT on the UniversalNER zero-shot benchmark; it performed better than vanilla LLaMA/Alpaca in some settings but trailed ChatGPT by over 20 absolute F1 points and was outperformed by UniNER by large margins (reported average differences).",
            "limitations_or_challenges": "Although trained on real ChatGPT conversations, Vicuna still trails teacher LLMs on targeted benchmarks such as open-domain NER; lacks the mission-focused instruction tuning on diverse per-type queries used by UniNER.",
            "comparison_to_baselines_or_humans": "Compared directly in experiments: Vicuna-7B and Vicuna-13B had substantially lower average F1 than UniNER and ChatGPT on the UniversalNER benchmark (exact per-domain numbers reported in the paper).",
            "uuid": "e3881.1",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca (instruction-tuned LLaMA via synthetic instructions)",
            "brief_description": "An instruction-tuned student model obtained by generating instruction–response pairs (self-instruct style) from a teacher LLM and finetuning LLaMA; cited as prior work on LLM-to-student distillation.",
            "citation_title": "Stanford alpaca: An instruction-following llama model",
            "mention_or_use": "mention",
            "system_or_method_name": "Alpaca (self-instruct distillation)",
            "system_or_method_description": "Generates instruction data (via LLM-based methods) and finetunes LLaMA to follow instructions; representative of general-purpose instruction-tuned distilled models.",
            "input_corpus_description": "Instruction–response pairs automatically generated by an LLM teacher (paper cites Alpaca but does not provide Alpaca's detailed corpus within this work).",
            "topic_or_query_specification": "Natural language instructions, diverse tasks produced by LLM-generated instructions.",
            "distillation_method": "Teacher-generated instruction data followed by finetuning of smaller LM (instruction tuning).",
            "output_type_and_format": "General instruction-following text responses across tasks.",
            "evaluation_or_validation_method": "Discussed in related work: Alpaca demonstrates ability to imitate ChatGPT but remains behind teacher on many downstream applications; no new evaluation performed in this paper.",
            "results_summary": "Cited statement: instruction-tuned students like Alpaca can imitate ChatGPT but still trail teacher LLMs by large margins on targeted downstream tasks (citing Gudibande et al., 2023).",
            "limitations_or_challenges": "Generic distillation across many tasks yields only a shallow approximation of teacher capabilities for specialized downstream applications; Alpaca lacks mission-focused tuning that UniNER applies for NER.",
            "comparison_to_baselines_or_humans": "Mentioned qualitatively as inferior to teacher LLMs on many targeted tasks; no quantitative comparison within this paper's experiments.",
            "uuid": "e3881.2",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "InstructUIE",
            "name_full": "InstructUIE (Multi-task instruction-tuned unified information extraction)",
            "brief_description": "A supervised multi-task instruction-tuned system for unified information extraction built on Flan-T5-11B and trained on diverse supervised IE datasets; used here as a strong supervised baseline for NER and IE.",
            "citation_title": "Instructuie: Multi-task instruction tuning for unified information extraction",
            "mention_or_use": "use",
            "system_or_method_name": "InstructUIE",
            "system_or_method_description": "A supervised instruction-tuned model for unified information extraction that converts IE datasets into natural-language instruction–response format and trains a Flan-T5-11B backbone to generate structured IE outputs.",
            "input_corpus_description": "A collection of supervised information-extraction datasets (details not enumerated here beyond reference to Wang et al., 2023a); training is supervised rather than teacher-LLM-generated.",
            "topic_or_query_specification": "Natural-language instruction prompts describing IE/NER tasks (unified generation objective).",
            "distillation_method": "Supervised multi-task instruction tuning on labeled IE datasets (not LLM-to-student distillation in this paper's description).",
            "output_type_and_format": "Generated structured outputs for IE (used as NER predictions for comparison).",
            "evaluation_or_validation_method": "Reported comparisons on UniversalNER benchmark (both zero-shot and supervised/in-domain or out-of-domain evaluations). The authors re-evaluated InstructUIE after fixing a reported evaluation bug in InstructUIE's original script.",
            "results_summary": "UniNER (distilled from ChatGPT) outperformed InstructUIE by a large margin in zero-shot and out-of-domain settings despite InstructUIE being trained on supervised IE datasets; supervised/continual finetuning further improved UniNER to exceed InstructUIE in the reported experiments.",
            "limitations_or_challenges": "InstructUIE relies on supervised datasets and can suffer from label-definition conflicts across datasets; in this paper it yielded lower zero-shot generalization compared to the targeted distilled UniNER.",
            "comparison_to_baselines_or_humans": "Directly compared in experiments; UniNER (both zero-shot and supervised variants) outperformed InstructUIE-11B on average F1 in reported evaluations.",
            "uuid": "e3881.3",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Gu2023",
            "name_full": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events (Gu et al., 2023)",
            "brief_description": "Cited prior work that focuses on distilling task-level abilities from LLMs for biomedical information extraction, specifically a case study on adverse drug event extraction.",
            "citation_title": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events",
            "mention_or_use": "mention",
            "system_or_method_name": "Distillation for biomedical knowledge extraction (Gu et al., 2023)",
            "system_or_method_description": "Referenced as an example of prior work distilling task-level abilities from LLMs into smaller models for biomedical IE tasks; specifics of the method are not detailed in this UniversalNER paper.",
            "input_corpus_description": "Not specified in detail in this paper; by the title, the original work targets biomedical text and adverse drug event extraction datasets (details to be obtained from the cited work).",
            "topic_or_query_specification": "Not specified in this paper; the cited work is described as a case study on adverse drug events.",
            "distillation_method": "Described in this paper as part of the literature on distilling task-level abilities from LLMs; details (e.g., whether teacher-generated labels or other techniques were used) are not provided here.",
            "output_type_and_format": "Goal is biomedical knowledge extraction (e.g., adverse drug event triples or entity/relation annotations); exact formats not described in this paper.",
            "evaluation_or_validation_method": "Not described in this paper; presumed to be task-specific evaluation on biomedical IE benchmarks in the cited work.",
            "results_summary": "Cited as an example of distilling LLMs for task-level biomedical IE; the UniversalNER paper does not report numbers from Gu et al.",
            "limitations_or_challenges": "Not discussed in this paper beyond the general point that prior distillation studies often focus on certain datasets or domains, whereas UniversalNER aims at a broader application-class formulation.",
            "uuid": "e3881.4",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Jung2023",
            "name_full": "Impossible Distillation (Jung et al., 2023)",
            "brief_description": "A referenced study proposing an efficient method to distill a much smaller model that can outperform larger models on specialized tasks (summarization and paraphrasing) by generating high-quality datasets and models.",
            "citation_title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing",
            "mention_or_use": "mention",
            "system_or_method_name": "Impossible Distillation",
            "system_or_method_description": "Referenced as prior work that generates data and distills a small student model to outperform larger models on specific tasks (summarization and paraphrasing); the UniversalNER paper cites it as part of task-level distillation literature.",
            "input_corpus_description": "Not specified in this paper; original work focuses on task-specific data generation and distillation for summarization/paraphrasing.",
            "topic_or_query_specification": "Not specified here.",
            "distillation_method": "Described briefly: an efficient method to distill an order-of-magnitude smaller model that can outperform GPT-3 on specialized tasks; details are in the referenced work.",
            "output_type_and_format": "Task-specific outputs for summarization and paraphrasing; not detailed in this paper.",
            "evaluation_or_validation_method": "Not detailed here; referenced as prior art showing task-level distillation success.",
            "results_summary": "Cited claim: distillation produced a smaller model that outperforms GPT-3 on specialized summarization and paraphrasing tasks in certain domains (as reported in Jung et al., 2023).",
            "limitations_or_challenges": "This paper notes that prior studies often perform distillation on certain datasets/domains rather than a general application-class; no further limitations about Jung et al. are reported here.",
            "uuid": "e3881.5",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Hsieh2023",
            "name_full": "Distilling step-by-step! (Hsieh et al., 2023)",
            "brief_description": "A referenced study on distilling chain-of-thought and reasoning abilities from LLMs into smaller models, cited as an example of reasoning-oriented distillation.",
            "citation_title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
            "mention_or_use": "mention",
            "system_or_method_name": "Chain-of-thought distillation (Hsieh et al., 2023)",
            "system_or_method_description": "Referenced work that distills LLMs' reasoning (chain-of-thought) capabilities into smaller models; cited as part of the literature on distilling specialized abilities (reasoning) from large models.",
            "input_corpus_description": "Not detailed in this paper; original work focuses on chain-of-thought style reasoning examples.",
            "topic_or_query_specification": "Not specified here.",
            "distillation_method": "Chain-of-thought distillation (teacher LLM provides step-by-step reasoning traces used to train the student); exact protocol not provided in this paper.",
            "output_type_and_format": "Reasoned step-by-step explanations and final answers; not expanded upon here.",
            "evaluation_or_validation_method": "Not provided in this paper; referenced as prior art demonstrating reasoning distillation.",
            "results_summary": "Cited as demonstrating that distilling chain-of-thought can help smaller models outperform larger models on some tasks (as reported in the referenced work).",
            "limitations_or_challenges": "No limitations discussed in this paper beyond the general observation that prior works are often task- or domain-specific.",
            "uuid": "e3881.6",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-Instruct",
            "name_full": "Self-Instruct (automatic instruction generation by LLMs)",
            "brief_description": "A method/pipeline (cited) for automatically generating diverse instruction–response pairs using LLMs themselves to bootstrap instruction-tuning datasets (used in Alpaca and related efforts).",
            "citation_title": "Self-instruct: Aligning language models with self-generated instructions",
            "mention_or_use": "mention",
            "system_or_method_name": "Self-Instruct (LLM-generated instruction data)",
            "system_or_method_description": "Generates instruction-following training data by prompting an LLM to produce diverse instructions and corresponding responses, then finetunes a student LM on these pairs. Cited as a general mechanism for producing instruction data from LLMs.",
            "input_corpus_description": "Not specified in the UniversalNER paper; generally involves teacher LLM outputs used to create synthetic instruction datasets.",
            "topic_or_query_specification": "Natural-language instructions automatically produced by an LLM; used as training examples.",
            "distillation_method": "Teacher LLM used to self-generate instruction–response pairs; student finetuning on this synthetic data (instruction tuning).",
            "output_type_and_format": "Instruction–response textual pairs across many tasks.",
            "evaluation_or_validation_method": "Not detailed here; Self-Instruct is cited as a technique underlying other instruction-tuning works.",
            "results_summary": "Cited as enabling instruction-tuned models (e.g., Alpaca) but such general distillation can fall short on mission-specific downstream tasks, motivating the UniNER mission-focused approach.",
            "limitations_or_challenges": "Generic instruction-generation approaches may not cover diverse input distributions for a specific application class; UniNER addresses this by sampling diverse inputs from a large unlabeled corpus instead of relying on LLMs to invent inputs for every domain.",
            "uuid": "e3881.7",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events",
            "rating": 2
        },
        {
            "paper_title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing",
            "rating": 2
        },
        {
            "paper_title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
            "rating": 2
        },
        {
            "paper_title": "Stanford alpaca: An instruction-following llama model",
            "rating": 1
        },
        {
            "paper_title": "Self-instruct: Aligning language models with self-generated instructions",
            "rating": 1
        },
        {
            "paper_title": "Instructuie: Multi-task instruction tuning for unified information extraction",
            "rating": 2
        },
        {
            "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality",
            "rating": 1
        },
        {
            "paper_title": "SciREX: A challenge dataset for document-level information extraction",
            "rating": 1
        },
        {
            "paper_title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "rating": 1
        }
    ],
    "cost": 0.019664499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UniversalNER: TARGETED DISTILLATION FROM LARGE LANGUAGE MODELS FOR OPEN NAMED ENTITY RECOGNITION</h1>
<p>Wenxuan Zhou ${ }^{1 <em>}$, Sheng Zhang ${ }^{2 </em>}$, Yu Gu ${ }^{2}$, Muhao Chen ${ }^{1,3}$, Hoifung Poon ${ }^{2}$<br>${ }^{1}$ University of Southern California ${ }^{2}$ Microsoft Research ${ }^{3}$ University of California, Davis<br>${ }^{1}$ {zhouwenx,muhaoche}@usc.edu ${ }^{2}$ {shezhan,yugu1,hoifung}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) such as ChatGPT (Ouyang et al., 2022; OpenAI, 2023) have demonstrated remarkable generalization capabilities, but they generally require prohibitive cost in training and inference. Moreover, in mission-critical applications such as biomedicine, white-box access to model weights and inference probabilities are often important for explainability and trust. Consequently, instruction-tuning has become a popular approach for distilling LLMs into more cost-efficient and transparent student models. Such student models, as exemplified by Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023), have demonstrated compelling capabilities in imitating ChatGPT. However, upon close inspection, they still trail the teacher LLM by a large margin, especially in targeted downstream applications (Gudibande et al., 2023). Bounded by limited compute, it is unsurprising that generic distillation can only produce a shallow approximation of the original LLM across all possible applications.</p>
<p>In this paper, we instead explore targeted distillation where we train student models using missionfocused instruction tuning for a broad application class such as open information extraction (Etzioni et al., 2008). We show that this can maximally replicate LLM's capabilities for the given application</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>class, while preserving its generalizability across semantic types and domains. We choose named entity recognition (NER) for our case study, as it is one of the most fundamental tasks in natural language processing (Wu et al., 2017; Perera et al., 2020). Recent studies (Wei et al., 2023; Li et al., 2023) show that when there are abundant annotated examples for an entity type, LLMs still fall behind the state-of-the-art supervised system for that entity type. However, for the vast majority of entity types, there is little annotated data. New entity types constantly emerge, and it is expensive and time-consuming to generate annotated examples, especially in high-value domains such as biomedicine where specialized expertise is required for annotation. Trained on pre-specified entity types and domains, supervised NER models also exhibit limited generalizability for new domains and entity types.</p>
<p>We present a general recipe for targeted distillation from LLMs and demonstrate that for open-domain NER. We show how to use ChatGPT to generate instruction-tuning data for NER from broad-coverage unlabeled web text, and conduct instruction-tuning on LLaMA (Touvron et al., 2023a) to distill the UniversalNER models (UniNER in short).</p>
<p>To facilitate a thorough evaluation, we assemble the largest and most diverse NER benchmark to date (UniversalNER benchmark), comprising 43 datasets across 9 domains such as biomedicine, programming, social media, law, finance. On zero-shot NER, LLaMA and Alpaca perform poorly on this benchmark (close to zero F1). Vicuna performs much better by comparison, but still trails ChatGPT by over 20 absolute points in average F1. By contrast, UniversalNER attains state-of-the-art NER accuracy across tens of thousands of entity types in the UniversalNER benchmark, outperforming Vicuna by over 30 absolute points in average F1. With a tiny fraction of parameters, UniversalNER not only replicates ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute points in average F1. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE (Wang et al., 2023a), which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various distillation components, such as the instruction prompts and negative sampling.</p>
<h1>2 Related Work</h1>
<p>Knowledge distillation. While LLMs such as ChatGPT achieve promising results, these models are often black-box and have high computational costs. To address these issues, distilling the task capabilities of LLMs into smaller, more manageable models has emerged as a promising direction. Knowledge distillation (Hinton et al., 2015) often revolves around the transfer of knowledge from larger, more complex models to their smaller counterparts. Recent work (Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023) seeks to distill the general abilities of LLMs with the objective of matching, if not surpassing, the performance of the original LLMs. Particularly, Alpaca (Taori et al., 2023) automates the generation of instructions (Wang et al., 2023c) and distills the knowledge from a teacher LLM. Vicuna (Chiang et al., 2023) adopts the ShareGPT data, which are comprised of real conversations with ChatGPT conducted by users, thereby providing a more authentic context for distillation. Another line of work (Smith et al., 2022; Jung et al., 2023; Hsieh et al., 2023; Gu et al., 2023) focuses on distilling task-level abilities from LLMs. Particularly, Jung et al. (2023) propose an efficient method to distill an order of magnitude smaller model that outperforms GPT-3 on specialized tasks summarization and paraphrasing in certain domains. Hsieh et al. (2022) propose to distill LLMs' reasoning abilities into smaller models by chain-of-the-thought distillation. However, these studies perform distillation either on certain datasets or domains, while our work focuses on a more general formulation that can be applied to diverse domains.</p>
<p>Instruction tuning. As an effective method to adapt LMs to perform a variety of tasks, instruction tuning has attracted an increasing number of community efforts: FLAN (Chung et al., 2022), T0 (Sanh et al., 2021), and Tk-Instruct (Wang et al., 2022) convert a large set of existing supervised learning datasets into instruction-following format, and then fine-tune encoder-decoder models, showing strong zero-shot and few-shot performance on NLP benchmarks. Ouyang et al. (2022) crowd-source high-quality instruction data and fine-tune GPT-3 into InstructGPT, enhancing its ability to understand user intention and follow instructions. Recent advancements (Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023) have also led to smaller models that exhibit task-following capabilities, after being fine-tuned on instruction data generated by LLMs, such as ChatGPT or GPT4. However, these smaller</p>
<p>models often struggle to generate high-quality responses for a diverse range of tasks (Wang et al., 2023b). A closer examination on targeted benchmarks reveals a substantial gap between these models to ChatGPT (Gudibande et al., 2023). Our proposed method, in contrast, focuses on tuning models to excel at a specific type of tasks. The diversity in our instructing-tuning method comes from task labels (e.g., relation types for relation extraction, entity types for NER), rather than instructions. By focusing on task-level capabilities and using NER as a case study, we demonstrate that it is possible to devise a tuning recipe that not only closes the performance gap but also surpasses ChatGPT. Wang et al. (2023a) also explore instruction-tuning for information extraction tasks. However, their method relies solely on supervised datasets and yields subpar performance when compared to ChatGPT.</p>
<h1>3 Mission-Focused Instruction Tuning</h1>
<p>Instruction tuning (Ouyang et al., 2022; Wei et al., 2021) is a method through which pretrained autoregressive language models are finetuned to follow natural language instructions and generate responses. Existing work focuses on tuning models to do diverse tasks (Taori et al., 2023; Chiang et al., 2023). In contrast, we introduce a general recipe for mission-focused instruction tuning, where the pretrained model is tuned for a broad application class such as open information extraction.</p>
<p>In this paper, we conduct a case study on the NER task, as it is one of the fundamental tasks for knowledge extraction from text. The objective is to learn a model $f:(\mathcal{X} \times \mathcal{T}) \rightarrow \mathcal{Y}$, where $\mathcal{X}$ represents the set of inputs, $\mathcal{T}$ denotes a predefined set of entity types, and $\mathcal{Y}$ represents the set of entities of a specific type in the given input.</p>
<h3>3.1 Data Construction</h3>
<p>A typical instruction-tuning example is made of three parts, including instruction, input, and output, where the diversity of instruction causes the models to follow a wide range of task instructions. However, for mission-focused instruction tuning, our goal is to tune the model to maximally generalize across semantic types and domains for the targeted application class. Therefore, we focus on increasing the diversity of input rather than instruction.</p>
<p>While earlier work (Jung et al., 2023) employs language models to generate inputs, these models typically assume that the domains of test data are known and prompt LMs to generate data for each domain. This method falls short when applied to distillation for a broad application class, where the distribution of test data is unknown. Consequently, it is challenging to generate inputs from LMs that provide wide coverage of the test domains.</p>
<p>To address this limitation, we propose an alternative: directly sampling inputs from a large corpus across diverse domains, and then using an LLM to generate outputs. In this paper, we sample inputs from the Pile corpus (Gao et al., 2020), which compiles 22 distinct English subdatasets. We chunk the articles in Pile to passages of a max length of 256 tokens and randomly sample 50 K passages as the inputs. Subsequently, we use ChatGPT (gpt-3.5-turbo-0301) to generate entity mentions and their associated types based on the sampled passages. To ensure stability, we set the generation temperature to 0 . The specific prompt for constructing the data is shown in Fig. 1. In this prompt, we do not specify the set of entity types of interest, allowing the LLM to generate outputs encompassing a broad coverage of entity types.</p>
<p>Data statistics. After filtering out unparseable outputs and inappropriate entities, including nonEnglish entities and those classified under 'ELSE' categories, such as None, NA, MISC, and ELSE, our dataset comprises 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types. We divide the entity types according to frequency and show the top 10 entity types in each range in Tab. 1. The distribution of these entity types exhibits a heavy tail, where the top</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: left;">Entity types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Top 1\%</td>
<td style="text-align: left;">person, organization, location, date, concept, product, event, technology, group, medical</td>
</tr>
<tr>
<td style="text-align: center;">(74\%)</td>
<td style="text-align: left;">condition, ...</td>
</tr>
<tr>
<td style="text-align: center;">$1 \%-10 \%$</td>
<td style="text-align: left;">characteristic, research, county, module, unit, feature, cell, package, anatomical structure,</td>
</tr>
<tr>
<td style="text-align: center;">(19\%)</td>
<td style="text-align: left;">equipment, ...</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%-100 \%$</td>
<td style="text-align: left;">attribute value, pokemon, immune response, physiology, animals, cell feature, FAC, input</td>
</tr>
<tr>
<td style="text-align: center;">(7\%)</td>
<td style="text-align: left;">device, ward, broadcast, ...</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of entities across different frequency ranges - top 1\%, 1-10\%, and 10-100\%, along with the percentage of total frequencies for each range.
$1 \%$ of entities account for $74 \%$ of total frequencies. We find that the generated data contain entity types from various domains, ranging from the general domain (e.g., PERSON) to the clinical domain (e.g., MEDICAL CONDITION). Moreover, we observe variations in granularity among the entity types. E.g., COUNTY is the subset of LOCATION, and INPUT DEVICE is a subset of PRODUCT. These data characteristics offer extensive coverage of entity types, making them suitable for distilling capabilities from LLMs across various domains.</p>
<p>Definition-based data construction. Besides entity types, we also prompt ChatGPT to generate entity mentions and define their types using short sentences. To do so, we simply change the prompt in Fig. 1 from "extract all entities and identify their entity types" to "extract all entities and concepts, and define their type using a short sentence". This method generates a much more diverse set of 353,092 entity types and leads to a tuned model that is less sensitive to entity type paraphrasing (Section 5.5), but performs worse on standard NER benchmarks (Section 5.2).</p>
<h1>3.2 InStruction Tuning</h1>
<p>After obtaining the data, we apply instruction tuning to smaller models to distill for a broad application class, e.g., diverse entity types in NER. Our template, as shown in Fig. 2, adopts a conversation-style tuning format. In this approach, the language model is presented with a passage $\boldsymbol{X}<em i="i">{\text {passage }}$ as input. Then, for each entity type $\boldsymbol{t}</em>}$ that appears in the output, we transform it into a natural language query "What describes $\boldsymbol{t<em i="i">{i}$ ?" Subsequently, we tune the LM to generate a structured output $\boldsymbol{y}</em>}$ in the form of a JSON list containing all entities of $\boldsymbol{t<em 1="1">{i}$ in the passage. We consider $\boldsymbol{y}</em>$ as gold tokens and apply a language modeling objective on these tokens. Our preliminary experiments show that conversation-style tuning is better than traditional NER-style tuning adopted by Wang et al. (2023a); Sun et al. (2023).}, \ldots, \boldsymbol{y}_{T</p>
<p>Besides one entity type per query, we also consider combining all entity types in a single query, requiring the model to output all entities in a single response. Detailed results and discussions can be found in Section 5.2.</p>
<p>Negative sampling. Our data construction process follows an open-world assumption where we allow the model to generate entity types that have appeared in the passage. However, the generated data do not account for entity types that are not mentioned in the passage, i.e., negative entity types. As a result, it is challenging for us to apply a model trained on this data to a closed-world setting, where one may ask for entity types that do not exist in the passage. To address this potential mismatch, we sample negative entity types from the collection of all entity types that do not appear in the passage as queries and set the expected outputs as empty JSON lists. The sampling of negative entity types</p>
<p>is done with a probability proportional to the frequency of entity types in the entire dataset. This approach greatly improves the instruction tuning results, as shown in Section 5.4.</p>
<p>Supervised finetuning. When we have additional human annotations, model performance can be further improved with supervised data. However, a significant challenge arises when training with multiple datasets, as there might be discrepancies in label definitions among these datasets, resulting in label conflicts. For instance, some datasets like ACE (Walker et al., 2006) consider personal pronouns (e.g., she, he) as PERSON, while other datasets like multiNERD (Tedeschi \&amp; Navigli, 2022) do not include pronouns.</p>
<p>To address this issue, we propose to use datasetspecific instruction tuning templates to harmonize the discrepancies in label definitions, as illustrated in Fig. 3. Specifically, we augment the input with an additional field denoting the dataset name $\boldsymbol{D}$. By doing so, the model can learn the dataset-specific semantics of labels. During inference, we use the respective dataset name in the prompt for the supervised setting, whereas we omit the dataset field from the prompt in the zero-shot setting.</p>
<h2>4 Universal NER Benchmark</h2>
<p>To conduct a comprehensive evaluation of NER models across diverse domains and entity types, we collect the largest NER benchmark to date. This benchmark encompasses 43 NER datasets across 9 domains, including general, biomedical, clinical, STEM, programming, social media, law, finance, and transportation domains. An overview of data distribution is shown in Fig. 4. Detailed dataset statistics are available in Appendix Tab. 6.</p>
<p>Dataset processing. To make the entity types semantically meaningful to LLMs, we conduct a manual inspection of the labels and convert the original labels into natural language formats. For instance, we replace PER with PERSON. While we try to collect a broad coverage of NER datasets, we do not use all entity types. This is because some entity types (e.g., ElSE) are not coming from consistent sources across the different datasets. Their annotations often come from different ontologies for different purposes. The choices of entity types and their annotation guidelines are not optimized for holistic or comprehensive assessments, which renders them suboptimal for use as a "ground truth" to evaluate a universal NER model. Therefore, we remove those labels from the datasets. In addition, some datasets are at the document level and contain very long contexts, which might exceed the input length limit of models. Therefore, we split all instances in document-level datasets into sentence-level ones.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: The dataset-specific instruction tuning template. We add the dataset name $\boldsymbol{D}$ (colored in red) as part of the input to resolve conflicts in label definitions.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Distribution of UniNER benchmark.</p>
<h1>5 EXPERIMENTS</h1>
<p>This section presents experimental evaluations of UniversalNER. We start by outlining experimental settings (Section 5.1), followed by presenting the results on both distillation and supervised settings</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Comparisons of zero-shot models on different domains. Our distilled models achieve better results than ChatGPT in all evaluated domains.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Comparisons between UniNER-7B and two variants. UniNER-7B-definition is distilled on Pile data prompted with entity type definitions. UniNER-7B-all-in-one is tuned with the template where all entity types are asked in one query.
(Sections 5.2 and 5.3). Finally, we conduct analysis (Section 5.4) and case study (Section 5.5) to provide deeper insights into the model's performance.</p>
<h1>5.1 EXPERIMENTAL SETTINGS</h1>
<p>Model configurations. We train models based on LLaMA ${ }^{2}$ (Touvron et al., 2023a) following the training schedule of Chiang et al. (2023) for a fair comparison. Considering the large size of certain test datasets, we perform evaluation by sampling up to 200,000 passage-query pairs from each dataset. We use strict entity-level micro- $F_{1}$ in evaluation, requiring both the entity type and boundary to exactly match the ground truth.</p>
<p>Compared models. We compare our model (UniNER) against the following models: (1) ChatGPT (gpt-3.5-turbo-0301). We use the prompting template in Ye et al. (2023) for NER. (2) Vicuna (Chiang et al., 2023) is finetuned with ChatGPT conversations, using LLaMA as the base model. (3) InstructUIE (Wang et al., 2023a) is a supervised model finetuned on diverse information extraction datasets, employing a unified natural language generation objective. It adopts Flan-T5 11B (Chung et al., 2022) as the base model.</p>
<h3>5.2 DISTILLATION</h3>
<p>We first evaluate the models in a zero-shot setting. We compare the performance of ChatGPT, Vicuna, and our model UniNER, which is distilled from ChatGPT NER annotations on Pile without human-labeled datasets in training. Results are shown in Fig. 5a. ${ }^{3}$ We observe that our distilled</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>models, namely UniNER-7B and UniNER-13B, outperform ChatGPT in terms of average $F_{1}$. The average $F_{1}$ scores of UniNER-7B and UniNER-13B are $41.7 \%$ and $43.4 \%$, respectively, compared to $34.9 \%$ for ChatGPT. This demonstrates that our proposed targeted distillation from diverse inputs yields models that have superior performance on a broad application class while maintaining a relatively small model size. Additionally, UniNER-13B exhibits better performance compared to UniNER-7B, indicating that fine-tuning on larger models may lead to improved generalization. In terms of domains, both UniNER-7B and UniNER-13B outperform ChatGPT on all domains, showing that the improvements exist across various domains.</p>
<p>We further compare different variations of UniNER, including (1) UniNER-all-in-one, where the extraction of all entity types are combined into one query and response, and (2) UniNER-definition, where queries in instruction tuning data use entity type definitions generated by ChatGPT instead of entity types. Results are shown in Fig. 5b. We observe that both UniNER-all-in-one and UniNER-definition underperform UniNER-type by $3.3 \%$ and $11.8 \%$ on average, respectively. The UniNER-definition variant's decreased performance could be due to its lower consistency with the evaluation datasets, which all adopt words or short phrases as labels instead of sentences. The performance disparity in the UniNER-all-in-one variant can be potentially attributed to the attention distribution and task complexity. When the model is required to handle multiple entity types within a single query, it might disperse its attention across these varied types, possibly resulting in less accurate identification for each individual type. Conversely, by decomposing the task into several simpler ones, each focusing on one entity type at a time, the model might be better equipped to handle the complexity, thus yielding more accurate results.</p>
<h3>5.3 Supervised Finetuning</h3>
<p>We study whether our models can be further improved using additional human annotations. We compare the performance of ChatGPT, Vicuna, InstructUIE (Wang et al., 2023a) ${ }^{4}$, and UniNER.</p>
<p>Out-of-domain evaluation. We first study whether supervised finetuning leads to better generalization on unseen data. We follow InstructUIE to exclude two datasets CrossNER (Liu et al., 2021) and MIT (Liu et al., 2013) for out-of-domain evaluation, and fine-tune our model using training splits of the remaining datasets in the universal NER benchmark. Results are shown in Tab. 3. Notably, without any fine-tuning, instruction-tuned UniNER 7B and 13B already surpass ChatGPT, Vicuna, and the supervised fine-tuned InstructUIE-11B by a large margin. If we train our model from scratch only using the supervised data, it achieves an average $F_{1}$ of $57.2 \%$. Continual fine-tuning UniNER-7B using the supervised data achieves the best average $F_{1}$ of $60.0 \%$. These findings suggest that the models' generalization can be further improved with additional human-annotated data.</p>
<p>In-domain evaluation. We then study the performance of UniNER in an in-domain supervised setting, where we fine-tune UniNER-7B using the same training data as InstructUIE (Wang et al., 2023a). Results are shown in Tab. 2. Our UniNER-7B achieves an average $F_{1}$ of $84.78 \%$ on the 20 datasets, surpassing both BERT-base and InstructUIE-11B by $4.69 \%$ and $3.62 \%$, respectively. This experiment demonstrates the effectiveness of our model in the supervised setting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>6.0</td>
<td>5.3</td>
<td>12.8</td>
<td>16.1</td>
<td>17.0</td>
<td>20.5</td>
<td>13.0</td>
<td>13.0</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>0.9</td>
<td>0.4</td>
<td>22.7</td>
<td>22.7</td>
<td>26.6</td>
<td>27.2</td>
<td>22.0</td>
<td>17.5</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>5.3</td>
<td>32.8</td>
<td>52.4</td>
<td>39.8</td>
<td>$\mathbf{6 6 . 6}$</td>
<td>$\mathbf{6 8 . 5}$</td>
<td>$\mathbf{6 7 . 0}$</td>
<td>47.5</td>
</tr>
<tr>
<td>UniNER-7B</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
<tr>
<td>UniNER-13B</td>
<td>$\mathbf{4 8 . 7}$</td>
<td>$\mathbf{3 6 . 2}$</td>
<td>$\mathbf{5 4 . 2}$</td>
<td>$\mathbf{6 0 . 9}$</td>
<td>64.5</td>
<td>61.4</td>
<td>63.5</td>
<td>$\mathbf{5 5 . 6}$</td>
</tr>
<tr>
<td>In-domain supervised</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InstructUIE-11B</td>
<td>-</td>
<td>-</td>
<td>48.4</td>
<td>48.8</td>
<td>54.4</td>
<td>49.9</td>
<td>49.4</td>
<td>-</td>
</tr>
<tr>
<td>UniNER-7B (sup. only)</td>
<td>54.2</td>
<td>16.0</td>
<td>62.3</td>
<td>$\mathbf{6 7 . 4}$</td>
<td>69.0</td>
<td>64.5</td>
<td>66.9</td>
<td>57.2</td>
</tr>
<tr>
<td>UniNER-7B (inst-tuned + sup.)</td>
<td>$\mathbf{6 1 . 2}$</td>
<td>$\mathbf{3 5 . 2}$</td>
<td>$\mathbf{6 2 . 9}$</td>
<td>64.9</td>
<td>$\mathbf{7 0 . 6}$</td>
<td>$\mathbf{6 6 . 9}$</td>
<td>$\mathbf{7 0 . 8}$</td>
<td>$\mathbf{6 1 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Out-of-domain evaluation on datasets from <em>Wang et al. (2023a)</em>. “sup. only” denotes a variant of UniNER-7B, trained from scratch using in-domain supervised data only and evaluated on out-of-domain datasets.</p>
<h3>5.4 Analysis</h3>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>19.1</td>
<td>19.1</td>
<td>25.1</td>
<td>39.5</td>
<td>42.7</td>
<td>48.9</td>
<td>26.2</td>
<td>31.5</td>
</tr>
<tr>
<td>Uniform</td>
<td>42.5</td>
<td>29.0</td>
<td>42.5</td>
<td>53.3</td>
<td>57.4</td>
<td>56.8</td>
<td>52.6</td>
<td>47.7</td>
</tr>
<tr>
<td>Frequency</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study on negative sampling strategies for UniNER-7B. All models are instruction-tuned on Pile.</p>
<p>Negative sampling strategies. We experiment with different negative sampling strategies in instruction tuning, including (1) no negative sampling, (2) uniform sampling where entity types are randomly sampled with equal probability for each one, and (3) frequency-based sampling where we sample entity types with probabilities proportional to their frequency in the constructed dataset. Results are shown in Tab. 4. Among the approaches tested, frequency-based sampling yielded the best results, outperforming no sampling and uniform sampling by 21.9% and 5.7%, respectively. These findings highlight the crucial role of negative sampling in instruction tuning, with frequency-based sampling emerging as the most effective method for enhancing model performance in our study.</p>
<p>Dataset-specific template. We compare the results of our dataset-specific instruction tuning template and the original template in the supervised setting. As shown in Fig. 6, we find that the data-specific template outperforms the original template on most datasets. To gain deeper insights into the improvements achieved, we further divide the datasets into two categories: those with label (entity type) overlap with other datasets and those without overlap. Our analysis reveals that datasets with label overlap demonstrate more substantial improvements.</p>
<p>To explore this further, we measure $F_{1}$ score across all evaluation datasets and calculate the difference. Apart from the long-tail entity types that manifest a high variance in results, we identify two entity types where the dataset-specific template outperforms the original template by over 10%: FACILITY.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Different in $F_{1}$ between data-specific and original templates in the supervised setting. Orange and Blue mark datasets with/without label overlap with other datasets, respectively.</p>
<table>
<thead>
<tr>
<th>Partial match</th>
<th>Model</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>No</td>
<td>ChatGPT</td>
<td>5.3</td>
<td>32.8</td>
<td>52.4</td>
<td>39.8</td>
<td>66.6</td>
<td>68.5</td>
<td>67.0</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B w/ sup</td>
<td>61.2</td>
<td>35.2</td>
<td>62.9</td>
<td>64.9</td>
<td>70.6</td>
<td>66.9</td>
<td>70.8</td>
<td>61.8</td>
</tr>
<tr>
<td>Yes</td>
<td>ChatGPT</td>
<td>5.9</td>
<td>40.1</td>
<td>55.7</td>
<td>42.8</td>
<td>70.2</td>
<td>71.7</td>
<td>70.1</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B</td>
<td>46.9</td>
<td>40.3</td>
<td>57.7</td>
<td>62.7</td>
<td>62.9</td>
<td>63.2</td>
<td>63.3</td>
<td>56.7</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B w/ sup</td>
<td>65.5</td>
<td>39.4</td>
<td>66.2</td>
<td>67.2</td>
<td>72.7</td>
<td>68.9</td>
<td>73.4</td>
<td>64.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Allowing partial match between the prediction and the gold that has overlap increases the results. When it is allowed, any partial match is regarded as half correct (counted as 0.5 in true positive) when computing $F_{1}$.
(22.0%) and TIME (12.4%). Intriguingly, both labels exhibit inconsistencies in their definitions across various datasets. The FACILITY label has been annotated on pronouns (e.g., it, which) as entities in ACE datasets but are excluded in OntoNotes. The TIME label denotes well-defined time intervals (e.g., Christmas) in MultiNERD, but may encompass any general time expressions (e.g., 3 pm) in OntoNotes. This finding suggests that the improvements provided by the data-specific template are particularly effective in resolving label conflicts.</p>
<p>Evaluation with partial match. While using strict $F_{1}$ as an evaluation metric, we notice that it may underestimate the zero-shot learning capabilities of NER models. In particular, strict $F_{1}$ penalizes slight misalignments in the boundaries of the extracted entities, which may not necessarily indicate an incorrect understanding of the text. For instance, given the sentence any asian cuisine around and the entity type CUISINE, UniNER extracts asian cuisine as the named entity, while the ground truth only labels asian as the correct entity. However, the model’s prediction can still be viewed as correct, even though it is deemed incorrect by strict $F_{1}$. To better estimate the zero-shot abilities, we also consider partial match (Segura-Bedmar et al., 2013) in evaluation. In this context, a prediction that exhibits word overlap with the ground truth is regarded as half correct (counted as 0.5 in true positive) when computing $F_{1}$. Results are shown in Tab. 5. We find that allowing partial match consistently improves the results. Besides, our models is still the best-performing model on average.</p>
<h1>5.5 CASE STUDY</h1>
<p>Sensitivity to entity type paraphrasing. One type of entity can be expressed in multiple ways, so it is essential for our model to give consistent predictions given entity types with similar meanings. An example of sensitivity analysis is present in Fig. 7. We observe that UniNER-7B-type sometimes fails to recognize entities with similar semantic meanings. On the other hand, UniNER-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing. It demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.</p>
<p>Recognition of diverse entity types. We present an example in Fig. 8 showcasing the capabilities of UniNER in recognizing various entities. Particularly, we focus on a novel domain of code and assess UniNER's ability to extract diverse types of entities within the code. Despite minor mistakes (e.g., from_pretrained is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.</p>
<h2>6 CONCLUSION</h2>
<p>We present a targeted distillation approach with mission-focused instruction tuning. Using NER as a case study, we train smaller and more efficient models for open-domain NER. The proposed method successfully distills ChatGPT into a smaller model UniversalNER, achieving remarkable NER accuracy across a wide range of domains and entity types without direct supervision. These models not only retain ChatGPT's capabilities but also surpass it and other state-of-the-art systems in NER performance.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>Wenxuan Zhou and Muhao Chen were supported by the NSF Grants IIS 2105329 and ITE 2333736.</p>
<h2>REFERENCES</h2>
<p>Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, and Steven Skiena. Polyglot-ner: Massive multilingual named entity recognition. In Proceedings of the 2015 SIAM International Conference on Data Mining, pp. 586-594. SIAM, 2015.</p>
<p>Victoria Arranz, Khalid Choukri, Montse Cuadros, Aitor García Pablos, Lucie Gianola, Cyril Grouin, Manuel Herranz, Patrick Paroubek, and Pierre Zweigenbaum. MAPA project: Ready-to-go open-source datasets and deep learning technology to remove identifying information from text documents. In Proceedings of the Workshop on Ethical and Legal Issues in Human Language Technologies and Multilingual De-Identification of Sensitive Data In Language Resources within the 13th Language Resources and Evaluation Conference, pp. 64-72, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.legal-1. 12 .</p>
<p>Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. E-NER — an annotated named entity recognition corpus of legal text. In Nikolaos Aletras, Ilias Chalkidis, Leslie Barrett, Cătălina Goantă, and Daniel Preotiuc-Pietro (eds.), Proceedings of the Natural Legal Language Processing Workshop 2022, pp. 246-255, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.nllp-1.22. URL https: //aclanthology.org/2022.nllp-1.22.</p>
<p>Pei Chen, Haotian Xu, Cheng Zhang, and Ruihong Huang. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3329-3339, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.243. URL https://aclanthology.org/2022. naacl-main. 243 .</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //vicuna.lmsys.org.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Nigel Collier and Jin-Dong Kim. Introduction to the bio-entity recognition task at jnlpba. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pp. 73-78, 2004.</p>
<p>Leon Derczynski, Kalina Bontcheva, and Ian Roberts. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 1169-1179, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL https://aclanthology.org/C16-1111.</p>
<p>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3198-3213, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.248. URL https://aclanthology.org/2021.acl-long. 248.</p>
<p>Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1-10, 2014.</p>
<p>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. Open information extraction from the web. Communications of the ACM, 51(12):68-74, 2008.</p>
<p>Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Maruscyk, and Lukas Lange. The sofc-exp corpus and neural approaches to information extraction in the materials science domain, 2020.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Felix Grezes, Sergi Blanco-Cuaresma, Thomas Allen, and Tirthankar Ghosal. Overview of the first shared task on detecting entities in the astrophysics literature (DEAL). In Proceedings of the first Workshop on Information Extraction from Scientific Publications, pp. 1-7, Online, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wiesp-1. 1 .</p>
<p>Yu Gu, Sheng Zhang, Naoto Usuyama, Yonas Woldesenbet, Cliff Wong, Praneeth Sanapathi, Mu Wei, Naveen Valluri, Erika Strandberg, Tristan Naumann, and Hoifung Poon. Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events, 2023.</p>
<p>Runwei Guan, Ka Lok Man, Feifan Chen, Shanliang Yao, Rongsheng Hu, Xiaohui Zhu, Jeremy Smith, Eng Gee Lim, and Yutao Yue. Findvehicle and vehiclefinder: A ner dataset for natural language-based vehicle retrieval and a keyword-based cross-modal vehicle retrieval system. arXiv preprint arXiv:2304.10893, 2023.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms, 2023.</p>
<p>Sam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, and Ozlem Uzuner. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. Journal of the American Medical Informatics Association, 27(1):3-12, 2020.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8003-8017, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.507.</p>
<p>Yu-Ming Hsieh, Yueh-Yin Shih, and Wei-Yun Ma. Converting the Sinica Treebank of Mandarin Chinese to Universal Dependencies. In Proceedings of the 16th Linguistic Annotation Workshop (LAW-XVI) within LREC2022, pp. 23-30, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.law-1.4.</p>
<p>Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. SciREX: A challenge dataset for document-level information extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7506-7516, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.670. URL https://aclanthology. org/2020.acl-main. 670 .</p>
<p>Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset \&amp; model for summarization and paraphrasing, 2023.</p>
<p>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun'ichi Tsujii. Genia corpus-a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl_1):i180-i182, 2003.</p>
<p>Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, et al. The chemdner corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7(1):1-17, 2015.</p>
<p>Chaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu Machiraju. An annotated corpus for machine reading of instructions in wet lab protocols. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 97-106, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2016. URL https://aclanthology.org/N18-2016.</p>
<p>Aman Kumar and Binil Starly. "fabner": information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing, 33(8): 2393-2407, 2022.</p>
<p>Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. arXiv preprint arXiv:2304.11633, 2023.</p>
<p>Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database, 2016, 2016.</p>
<p>Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass. Asgard: A portable architecture for multilingual dialogue systems. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 8386-8390. IEEE, 2013.</p>
<p>Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. Crossner: Evaluating cross-domain named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 13452-13460, 2021.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3219-3232, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1360. URL https://aclanthology.org/018-1360.</p>
<p>Ling Luo, Po-Ting Lai, Chih-Hsuan Wei, Cecilia N Arighi, and Zhiyong Lu. Biored: a rich biomedical relation extraction dataset. Briefings in Bioinformatics, 23(5):bbac282, 2022.</p>
<p>Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. Ace 2004 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 1:1-1, 2005.</p>
<p>Danielle L Mowery, Sumithra Velupillai, Brett R South, Lee Christensen, David Martinez, Liadh Kelly, Lorraine Goeuriot, Noemie Elhadad, Sameer Pradhan, Guergana Savova, et al. Task 2: Share/clef ehealth evaluation lab 2014. In Proceedings of CLEF 2014, 2014.</p>
<p>Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain J Marshall, Ani Nenkova, and Byron C Wallace. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 197. NIH Public Access, 2018.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL https://aclanthology.org/P17-1178.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Nadeesha Perera, Matthias Dehmer, and Frank Emmert-Streib. Named entity recognition and relation detection for biomedical information extraction. Frontiers in cell and developmental biology, pp. 673, 2020.</p>
<p>Sampo Pyysalo and Sophia Ananiadou. Anatomical entity mention recognition at literature scale. Bioinformatics, 30(6):868-875, 2014.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2021.</p>
<p>David Schindler, Felix Bensmann, Stefan Dietze, and Frank Krüger. Somesci-a 5 star open data gold standard knowledge graph of software mentions in scientific articles. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, pp. 4574-4583, 2021.</p>
<p>Isabel Segura-Bedmar, Paloma Martínez Fernández, and María Herrero Zazo. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013). Association for Computational Linguistics, 2013.</p>
<p>Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. Finer: Financial named entity recognition dataset and weak-supervision model. arXiv preprint arXiv:2302.11157, 2023.</p>
<p>Larry Smith, Lorraine K Tanabe, Cheng-Ju Kuo, I Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, Manabu Torii, et al. Overview of biocreative ii gene mention recognition. Genome biology, 9(2):1-19, 2008.</p>
<p>Ryan Smith, Jason A Fries, Braden Hancock, and Stephen H Bach. Language models in the loop: Incorporating prompting into weak supervision. arXiv preprint arXiv:2205.02318, 2022.</p>
<p>Amber Stubbs, Christopher Kotfila, and Özlem Uzuner. Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1. Journal of biomedical informatics, 58:S11-S19, 2015.</p>
<p>Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. Journal of the American Medical Informatics Association, 20(5):806-813, 2013.</p>
<p>Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, and Guoyin Wang. Pushing the limits of chatgpt on nlp tasks, 2023.</p>
<p>Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. Code and named entity recognition in StackOverflow. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4913-4926, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.443. URL https://aclanthology.org/2020.acl-main. 443.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Simone Tedeschi and Roberto Navigli. MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation). In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 801-812, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.60. URL https://aclanthology.org/2022.findings-naacl.60.</p>
<p>Simone Tedeschi, Valentino Maiorca, Niccolò Campolungo, Francesco Cecconi, and Roberto Navigli. WikiNEuRal: Combined neural and knowledge-based silver data creation for multilingual NER. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2521-2533, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.findings-emnlp.215. URL https://aclanthology.org/2021.findings-emnlp. 215 .</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.</p>
<p>Asahi Ushio, Leonardo Neves, Vitor Silva, Francesco. Barbieri, and Jose Camacho-Collados. Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. In The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, Online, November 2022. Association for Computational Linguistics.</p>
<p>Özlem Uzuner, Yuan Luo, and Peter Szolovits. Evaluating the state-of-the-art in automatic deidentification. Journal of the American Medical Informatics Association, 14(5):550-563, 2007.</p>
<p>Özlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552-556, 2011.</p>
<p>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. Ace 2005 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 57:45, 2006.</p>
<p>Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, and Chunsai Du. Instructuie: Multi-task instruction tuning for unified information extraction, 2023a.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085-5109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main. 340 .</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long. 754.</p>
<p>Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, and Jiawei Han. Crossweigh: Training named entity tagger from imperfect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5157-5166, 2019.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. Zero-shot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205, 2023.</p>
<p>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23:170, 2013.</p>
<p>Ania Wróblewska, Agnieszka Kaliska, Maciej Pawłowski, Dawid Wiśniewski, Witold Sosnowski, and Agnieszka Ławrynowicz. Tasteset-recipe dataset and food entities recognition benchmark. arXiv preprint arXiv:2204.07775, 2022.</p>
<p>Yonghui Wu, Min Jiang, Jun Xu, Degui Zhi, and Hua Xu. Clinical named entity recognition using deep learning models. In AMIA annual symposium proceedings, volume 2017, pp. 1812. American Medical Informatics Association, 2017.</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420, 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 CASE Study</h2>
<p>Sensitivity to entity type paraphrasing. One type of entity can be expressed in multiple different ways. In this scenario, it is essential for our model to give consistent predictions given entity types with similar meanings. An example of sensitivity analysis is present in Fig. 7. We observe that UniNER-7B-type sometimes fails to recognize entities with similar semantic meanings. On the other hand, UniNER-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing. It demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sensitivity Analysis of Entity Types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Text: I'm visiting Los Angeles next week.</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes city in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes place in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes metropolis in the</td>
</tr>
<tr>
<td style="text-align: left;">text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes urban area in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes human settlement in</td>
</tr>
<tr>
<td style="text-align: left;">the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
</tbody>
</table>
<p>Figure 7: Sensitivity of UniNER-7B on different entity types. Green and red mark correct and wrong predictions, respectively.</p>
<p>Recognition of diverse entity types The code snippet provided is for text generation using the Huggingface library:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;This,is,assimple,program&quot;</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p>Figure 8: A code snippet to assess UniNER's ability to extract diverse types of entities within the code.</p>
<p>In the ensuing conversations with UniNER, we present the queries and corresponding results:</p>
<div class="codehilite"><pre><span></span><code><span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;transformers&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">local</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;input_text&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;input_ids&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;generated_text&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;encode&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;generate&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;decode&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="kd">class</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:[</span><span class="s2">&quot;GPT2Tokenizer&quot;</span><span class="o">,</span><span class="s2">&quot;GPT2LMHeadModel&quot;</span><span class="o">]</span>
</code></pre></div>

<p>Despite minor mistakes (e.g., from_pretrained is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.</p>
<h1>B Full Evaluation Results</h1>
<p>Full results on ChatGPT, UniNER-7B-type, and UniNER-7B-sup+type are shown in Fig. 9.</p>
<h2>C Data Statistics</h2>
<p>We show the full dataset statistics in Universal NER in Tab. 6, including the number of instances in train/dev/test data, number of entity types, average number of tokens in input text, and the average number of entities in each instance.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Full evaluation results of ChatGPT, UniNER-7B, and UniNER-7B w/ sup (joint training on both supervised and Pile-type data, MIT and CrossNER data are excluded in training).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># train</th>
<th style="text-align: center;"># dev</th>
<th style="text-align: center;"># test</th>
<th style="text-align: center;"># types</th>
<th style="text-align: center;">Avg. tokens</th>
<th style="text-align: center;">Avg. entities</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">ACE04 (Mitchell et al., 2005)</td>
<td style="text-align: center;">6202</td>
<td style="text-align: center;">745</td>
<td style="text-align: center;">812</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACE05 (Walker et al., 2006)</td>
<td style="text-align: center;">7299</td>
<td style="text-align: center;">971</td>
<td style="text-align: center;">1060</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">conllpp (Wang et al., 2019)</td>
<td style="text-align: center;">14041</td>
<td style="text-align: center;">3250</td>
<td style="text-align: center;">3453</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">1.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER AI (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">431</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER literature (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">416</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER music (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">380</td>
<td style="text-align: center;">465</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER politics (Liu et al., 2021)</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">650</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER science (Liu et al., 2021)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">543</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FewNERD-coarse (Ding et al., 2021)</td>
<td style="text-align: center;">131767</td>
<td style="text-align: center;">18824</td>
<td style="text-align: center;">37648</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FewNERD-fine (Ding et al., 2021)</td>
<td style="text-align: center;">131767</td>
<td style="text-align: center;">18824</td>
<td style="text-align: center;">37648</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiNERD (Tedeschi \&amp; Navigli, 2022)</td>
<td style="text-align: center;">134144</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ontonotes (Weischedel et al., 2013)</td>
<td style="text-align: center;">59924</td>
<td style="text-align: center;">8528</td>
<td style="text-align: center;">8262</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PolyglotNER (Al-Rfou et al., 2015)</td>
<td style="text-align: center;">393982</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TASTEset (Wróblewska et al., 2022)</td>
<td style="text-align: center;">556</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiANN en (Pan et al., 2017)</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiNeural (Tedeschi et al., 2021)</td>
<td style="text-align: center;">92720</td>
<td style="text-align: center;">11590</td>
<td style="text-align: center;">11597</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;">Biomed</td>
<td style="text-align: center;">AnatEM (Pyysalo \&amp; Ananiadou, 2014)</td>
<td style="text-align: center;">5861</td>
<td style="text-align: center;">2118</td>
<td style="text-align: center;">3830</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BioRED (Luo et al., 2022)</td>
<td style="text-align: center;">4373</td>
<td style="text-align: center;">1131</td>
<td style="text-align: center;">1106</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GENIA (Kim et al., 2003)</td>
<td style="text-align: center;">15023</td>
<td style="text-align: center;">1669</td>
<td style="text-align: center;">1854</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">3.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNLPBA (Collier \&amp; Kim, 2004)</td>
<td style="text-align: center;">18608</td>
<td style="text-align: center;">1940</td>
<td style="text-align: center;">4261</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc2gm (Smith et al., 2008)</td>
<td style="text-align: center;">12500</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc4chemd (Krallinger et al., 2015)</td>
<td style="text-align: center;">30682</td>
<td style="text-align: center;">30639</td>
<td style="text-align: center;">26364</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc5cdr (Li et al., 2016)</td>
<td style="text-align: center;">4560</td>
<td style="text-align: center;">4581</td>
<td style="text-align: center;">4797</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ncbi (Doğan et al., 2014)</td>
<td style="text-align: center;">5432</td>
<td style="text-align: center;">923</td>
<td style="text-align: center;">940</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Clinics</td>
<td style="text-align: center;">ebmnlp (Nye et al., 2018)</td>
<td style="text-align: center;">40713</td>
<td style="text-align: center;">10608</td>
<td style="text-align: center;">2076</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">1.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2006 deid 1B (Uzuner et al., 2007)</td>
<td style="text-align: center;">34958</td>
<td style="text-align: center;">14983</td>
<td style="text-align: center;">18095</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2010 concepts (Uzuner et al., 2011)</td>
<td style="text-align: center;">14553</td>
<td style="text-align: center;">1762</td>
<td style="text-align: center;">27625</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2012 temporal (Sun et al., 2013)</td>
<td style="text-align: center;">6235</td>
<td style="text-align: center;">787</td>
<td style="text-align: center;">5282</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2014 deid (Stubbs et al., 2015)</td>
<td style="text-align: center;">46272</td>
<td style="text-align: center;">4610</td>
<td style="text-align: center;">32587</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">n2c2 2018 task2 (Henry et al., 2020)</td>
<td style="text-align: center;">84351</td>
<td style="text-align: center;">9252</td>
<td style="text-align: center;">60228</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ShAEeCLEF (Mowery et al., 2014)</td>
<td style="text-align: center;">12494</td>
<td style="text-align: center;">2459</td>
<td style="text-align: center;">14143</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">DEAL (Grezes et al., 2022)</td>
<td style="text-align: center;">26906</td>
<td style="text-align: center;">20800</td>
<td style="text-align: center;">36665</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FabNER (Kumar \&amp; Starly, 2022)</td>
<td style="text-align: center;">9435</td>
<td style="text-align: center;">2182</td>
<td style="text-align: center;">2064</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOFC (Friedrich et al., 2020)</td>
<td style="text-align: center;">568</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciERC (Luan et al., 2018)</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">163</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciREX (Jain et al., 2020)</td>
<td style="text-align: center;">71511</td>
<td style="text-align: center;">15182</td>
<td style="text-align: center;">16599</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SoMeSci (Schindler et al., 2021)</td>
<td style="text-align: center;">31055</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">16427</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WLP (Kulkarni et al., 2018)</td>
<td style="text-align: center;">8177</td>
<td style="text-align: center;">2717</td>
<td style="text-align: center;">2726</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;">Programming</td>
<td style="text-align: center;">Stackoverflow-NER (Tabassum et al., 2020)</td>
<td style="text-align: center;">9263</td>
<td style="text-align: center;">2936</td>
<td style="text-align: center;">3108</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;">Social media</td>
<td style="text-align: center;">HarveyNER (Chen et al., 2022)</td>
<td style="text-align: center;">3967</td>
<td style="text-align: center;">1301</td>
<td style="text-align: center;">1303</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Broad Tweet Corpus (Derczynski et al., 2016)</td>
<td style="text-align: center;">5334</td>
<td style="text-align: center;">2001</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TweetNER7 (Ushio et al., 2022)</td>
<td style="text-align: center;">7111</td>
<td style="text-align: center;">886</td>
<td style="text-align: center;">576</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">3.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mit-movie (Liu et al., 2013)</td>
<td style="text-align: center;">9774</td>
<td style="text-align: center;">2442</td>
<td style="text-align: center;">2442</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mit-restaurant (Liu et al., 2013)</td>
<td style="text-align: center;">7659</td>
<td style="text-align: center;">1520</td>
<td style="text-align: center;">1520</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;">Law</td>
<td style="text-align: center;">E-NER (Au et al., 2022)</td>
<td style="text-align: center;">8072</td>
<td style="text-align: center;">1009</td>
<td style="text-align: center;">1010</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPA-coarse (Arranz et al., 2022)</td>
<td style="text-align: center;">893</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">408</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPA-fine (Arranz et al., 2022)</td>
<td style="text-align: center;">893</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">408</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: center;">Finance</td>
<td style="text-align: center;">FiNER-ord (Shah et al., 2023)</td>
<td style="text-align: center;">3262</td>
<td style="text-align: center;">403</td>
<td style="text-align: center;">1075</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;">Transportation</td>
<td style="text-align: center;">FindVehicle (Guan et al., 2023)</td>
<td style="text-align: center;">21565</td>
<td style="text-align: center;">20777</td>
<td style="text-align: center;">20777</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">5.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of datasets in our benchmark.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Please note that the original evaluation script in InstructUIE contains a critical bug. For passages that do not contain any entities, the script adds NONE as a placeholder entity and takes it into account when calculating $F_{1}$. To rectify this error, we re-evaluated InstructUIE using their released checkpoint.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
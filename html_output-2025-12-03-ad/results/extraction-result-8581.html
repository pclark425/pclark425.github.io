<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8581 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8581</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8581</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278171013</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.20213v1.pdf" target="_blank">Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework</a></p>
                <p><strong>Paper Abstract:</strong> This paper investigates the logical reasoning capabilities of large language models (LLMs). For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic. A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions. Incorrect proofs are caught by an automated proof checker. A critical obstacle for training is the scarcity of real-world proofs. We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions. The central evaluation question is whether an LLM has indeed learned to reason. We propose tests to measure the reasoning ability of a black-box LLM. By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity. Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8581.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8581.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3.2 8B (fine-tuned on synthetic Hilbert proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter Llama3 transformer model fine-tuned (via LoRA and 4-bit quantization) on synthetically generated Hilbert-style Boolean implication-only proofs with Template Transformation augmentation to produce formally checkable proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.2 (8B, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based Llama3 family model (8B parameters). Fine-tuned with Low-Rank Adaptation (LoRA) using 4-bit quantization on a synthetic dataset of Hilbert-style Boolean proofs generated by a randomized goal-directed algorithm; training batches applied Template Transformation augmentation with probability α_TT (default 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hilbert-style Boolean implication-only proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Construct a formal Hilbert-style proof that the goal formula follows from given assumptions in an implication-only fragment of propositional Boolean logic; proofs must be syntactically well-formed and pass an automated proof checker verifying axiom instances, correct use of assumptions, and Modus Ponens steps (a strict, formal logical reasoning task).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on synthetic, randomly generated goal-directed Hilbert proofs; data augmentation via Template Transformation (systematic variable→formula substitutions producing many structurally equivalent variants); training with LoRA and 4-bit quantization; evaluation via an automated formal proof validator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High on short proofs: 98% accuracy on depth-7 test proofs (trained on 9000 examples with Template Transformation). Performance degrades with depth: ≈66% at depth-10 and ≈32% at depth-13; general downward trend across depths 4–28.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms GPT-4o few-shot baselines on in-distribution depths (GPT-4o few-shot averaged ≈0.67±0.08 on depth-7); far exceeds pre-trained Llama3 base models (which produced ≈0.00 accuracy when not fine-tuned). Template Transformation improved accuracy substantially (reported +20–25% especially for shorter depths) versus standard fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sharp drop in accuracy as proof depth increases (depth generalization limited); struggles on out-of-distribution deeper proofs; bounded context and lack of auxiliary memory/backtracking in transformers likely contribute to forgetting distant assumptions; trained on synthetic proofs so domain gap to real-world proofs may limit applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Fine-tuning on synthetically generated formal proofs plus Template Transformation meaningfully teaches a model to produce formally valid proofs for short to moderate depths; Template Transformation forces abstraction from token-level cues and boosts handling of expression complexity (width), while model capacity (8B vs 1B) is important for generalization to deeper proofs. The formal setting enables precise automated validation and systematic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8581.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8581.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-1B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3.1 1B (fine-tuned on synthetic Hilbert proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1-billion-parameter Llama3 transformer model fine-tuned with the same synthetic Hilbert-proof pipeline and Template Transformation augmentation; shows improvements but with more limited depth generalization than the 8B model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.1 (1B, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based Llama3 family model (1B parameters). Fine-tuned with LoRA using 4-bit quantization on the same synthetic Hilbert proof corpus and Template Transformation augmentation (α_TT typically 0.7) to generate formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hilbert-style Boolean implication-only proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same strict formal proof generation task: produce a Hilbert-style derivation that the goal follows from assumptions in implication-only propositional logic, validated by an automated proof checker.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on synthetic goal-directed proofs plus Template Transformation augmentation; LoRA adaptation and 4-bit quantization for efficient training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Moderate on short proofs: ≈52% accuracy on depth-7 test proofs (trained on 9000 examples). Performance drops more sharply than 8B with increasing depth (not robust beyond depth ≈10). Template Transformation provides limited impact on depth generalization but significantly helps handling wider/complex expressions (width generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves substantially over pre-trained Llama3 base (≈0.00 accuracy without fine-tuning). With Template Transformation, the 1B model can achieve comparable performance to 8B on wider-expression cases (width ≥ 2), indicating augmentation compensates for capacity on expression complexity but not on depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited model capacity restricts depth generalization; sharp performance drop with deeper proofs; Template Transformation helps expression complexity but is less effective for long proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Data augmentation via Template Transformation can compensate for smaller model capacity when handling expression complexity (width), but larger model capacity remains important for generalizing to deeper proofs; training-data size and augmentation probability α_TT critically affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8581.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8581.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) evaluated in few-shot settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large commercial transformer model (OpenAI GPT-4 family variant) evaluated in few-shot prompting mode on the formal Hilbert proof generation task as a baseline; shows nontrivial but inferior performance to the small fine-tuned 8B model on in-distribution depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-scale generative transformer model (OpenAI GPT-4 family; cited as 100B+ parameters in the paper). Evaluated via few-shot prompting with {1,3,6,9} demonstrations drawn from training-depth examples; outputs were evaluated (automatically) for formal proof validity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>100B+ (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hilbert-style Boolean implication-only proof generation (evaluated via few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same strict formal proof generation task: given assumptions and goal in formal Hilbert implication-only syntax, produce a proof that verifies under the formal checker. In this paper GPT-4o was prompted in few-shot mode with examples in the same formal syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot exemplars (1,3,6,9-shot) in the prompt; no fine-tuning on the synthetic formal proofs in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Moderate but limited: averaged ≈0.67 ± 0.08 accuracy on depth-7 in the authors' few-shot evaluations; performance degrades with depth to ≈0.48 ± 0.08 at depth-10 and ≈0.26 ± 0.04 at depth-13.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed by the fine-tuned Llama3-8B model on in-distribution depths (8B fine-tuned reached 98% at depth-7). GPT-4o performs far better than pre-trained (non-fine-tuned) Llama3 base models but worse than the small amount of targeted fine-tuning used by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Few-shot prompting alone yields degraded performance as proof depth increases; few-shot setup is sensitive to the choice and number of exemplars; natural-language chain-of-thought style methods are not as suitable when formal, strictly checkable proofs are required (ambiguities and unverifiable steps are problematic).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Very large general-purpose models with few-shot prompting can solve some short formal proofs, but targeted fine-tuning on synthetic formal proofs (plus augmentation) enables smaller models to surpass few-shot performance on this strict formal reasoning task; scaling alone does not replace task-specific supervised fine-tuning for formal proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8581.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8581.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3 pre-trained baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-trained Llama3 base models (8B and 1B) without fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The off-the-shelf pre-trained Llama3 base models (8B & 1B) evaluated directly (few-shot) on the Hilbert proof task; they largely fail to produce valid formal proofs without targeted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3 (pre-trained, 8B and 1B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Llama3 base transformer models (8B and 1B) used as zero/few-shot baselines. No additional fine-tuning on formal proofs was applied for these baseline evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 1B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hilbert-style Boolean implication-only proof generation (zero/few-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same formal Hilbert proof generation task described above, evaluated without the targeted fine-tuning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct evaluation (pre-trained) possibly with few-shot exemplars; no LoRA fine-tuning or Template Transformation applied for the baseline runs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Approximately 0.00 accuracy: pre-trained Llama3 base models failed to generate valid Hilbert proofs in the authors' evaluation when used without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially worse than GPT-4o few-shot and dramatically worse than the fine-tuned Llama3-8B model; indicates that pretraining alone did not teach the strict syntactic and proof-structure skills needed for Hilbert-style formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Unable to infer the strict syntactic and generative constraints of Hilbert proofs from a few examples; produced invalid or malformed proofs when not fine-tuned on the formal dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Pre-training alone (even for models of multi-billion parameter scale) is insufficient for reliable production of formally valid proofs in this strict logical setting; modest amounts of targeted synthetic fine-tuning plus augmentation are crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>Learning deductive reasoning from synthetic corpus based on formal logic <em>(Rating: 2)</em></li>
                <li>Enhancing reasoning capabilities of llms via principled synthetic logic corpus <em>(Rating: 2)</em></li>
                <li>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>Evaluating logical reasoning of large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8581",
    "paper_id": "paper-278171013",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Llama3-8B (fine-tuned)",
            "name_full": "Llama3.2 8B (fine-tuned on synthetic Hilbert proofs)",
            "brief_description": "An 8-billion-parameter Llama3 transformer model fine-tuned (via LoRA and 4-bit quantization) on synthetically generated Hilbert-style Boolean implication-only proofs with Template Transformation augmentation to produce formally checkable proofs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3.2 (8B, fine-tuned)",
            "model_description": "Transformer-based Llama3 family model (8B parameters). Fine-tuned with Low-Rank Adaptation (LoRA) using 4-bit quantization on a synthetic dataset of Hilbert-style Boolean proofs generated by a randomized goal-directed algorithm; training batches applied Template Transformation augmentation with probability α_TT (default 0.7).",
            "model_size": "8B",
            "reasoning_task_name": "Hilbert-style Boolean implication-only proof generation",
            "reasoning_task_description": "Construct a formal Hilbert-style proof that the goal formula follows from given assumptions in an implication-only fragment of propositional Boolean logic; proofs must be syntactically well-formed and pass an automated proof checker verifying axiom instances, correct use of assumptions, and Modus Ponens steps (a strict, formal logical reasoning task).",
            "method_or_approach": "Fine-tuning on synthetic, randomly generated goal-directed Hilbert proofs; data augmentation via Template Transformation (systematic variable→formula substitutions producing many structurally equivalent variants); training with LoRA and 4-bit quantization; evaluation via an automated formal proof validator.",
            "performance": "High on short proofs: 98% accuracy on depth-7 test proofs (trained on 9000 examples with Template Transformation). Performance degrades with depth: ≈66% at depth-10 and ≈32% at depth-13; general downward trend across depths 4–28.",
            "baseline_comparison": "Outperforms GPT-4o few-shot baselines on in-distribution depths (GPT-4o few-shot averaged ≈0.67±0.08 on depth-7); far exceeds pre-trained Llama3 base models (which produced ≈0.00 accuracy when not fine-tuned). Template Transformation improved accuracy substantially (reported +20–25% especially for shorter depths) versus standard fine-tuning.",
            "limitations_or_failures": "Sharp drop in accuracy as proof depth increases (depth generalization limited); struggles on out-of-distribution deeper proofs; bounded context and lack of auxiliary memory/backtracking in transformers likely contribute to forgetting distant assumptions; trained on synthetic proofs so domain gap to real-world proofs may limit applicability.",
            "insights_or_conclusions": "Fine-tuning on synthetically generated formal proofs plus Template Transformation meaningfully teaches a model to produce formally valid proofs for short to moderate depths; Template Transformation forces abstraction from token-level cues and boosts handling of expression complexity (width), while model capacity (8B vs 1B) is important for generalization to deeper proofs. The formal setting enables precise automated validation and systematic evaluation.",
            "uuid": "e8581.0",
            "source_info": {
                "paper_title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama3-1B (fine-tuned)",
            "name_full": "Llama3.1 1B (fine-tuned on synthetic Hilbert proofs)",
            "brief_description": "A 1-billion-parameter Llama3 transformer model fine-tuned with the same synthetic Hilbert-proof pipeline and Template Transformation augmentation; shows improvements but with more limited depth generalization than the 8B model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3.1 (1B, fine-tuned)",
            "model_description": "Transformer-based Llama3 family model (1B parameters). Fine-tuned with LoRA using 4-bit quantization on the same synthetic Hilbert proof corpus and Template Transformation augmentation (α_TT typically 0.7) to generate formal proofs.",
            "model_size": "1B",
            "reasoning_task_name": "Hilbert-style Boolean implication-only proof generation",
            "reasoning_task_description": "Same strict formal proof generation task: produce a Hilbert-style derivation that the goal follows from assumptions in implication-only propositional logic, validated by an automated proof checker.",
            "method_or_approach": "Fine-tuning on synthetic goal-directed proofs plus Template Transformation augmentation; LoRA adaptation and 4-bit quantization for efficient training.",
            "performance": "Moderate on short proofs: ≈52% accuracy on depth-7 test proofs (trained on 9000 examples). Performance drops more sharply than 8B with increasing depth (not robust beyond depth ≈10). Template Transformation provides limited impact on depth generalization but significantly helps handling wider/complex expressions (width generalization).",
            "baseline_comparison": "Improves substantially over pre-trained Llama3 base (≈0.00 accuracy without fine-tuning). With Template Transformation, the 1B model can achieve comparable performance to 8B on wider-expression cases (width ≥ 2), indicating augmentation compensates for capacity on expression complexity but not on depth.",
            "limitations_or_failures": "Limited model capacity restricts depth generalization; sharp performance drop with deeper proofs; Template Transformation helps expression complexity but is less effective for long proofs.",
            "insights_or_conclusions": "Data augmentation via Template Transformation can compensate for smaller model capacity when handling expression complexity (width), but larger model capacity remains important for generalizing to deeper proofs; training-data size and augmentation probability α_TT critically affect results.",
            "uuid": "e8581.1",
            "source_info": {
                "paper_title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4o (few-shot baseline)",
            "name_full": "GPT-4o (OpenAI) evaluated in few-shot settings",
            "brief_description": "A very large commercial transformer model (OpenAI GPT-4 family variant) evaluated in few-shot prompting mode on the formal Hilbert proof generation task as a baseline; shows nontrivial but inferior performance to the small fine-tuned 8B model on in-distribution depths.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4o (few-shot)",
            "model_description": "Large-scale generative transformer model (OpenAI GPT-4 family; cited as 100B+ parameters in the paper). Evaluated via few-shot prompting with {1,3,6,9} demonstrations drawn from training-depth examples; outputs were evaluated (automatically) for formal proof validity.",
            "model_size": "100B+ (reported)",
            "reasoning_task_name": "Hilbert-style Boolean implication-only proof generation (evaluated via few-shot prompting)",
            "reasoning_task_description": "Same strict formal proof generation task: given assumptions and goal in formal Hilbert implication-only syntax, produce a proof that verifies under the formal checker. In this paper GPT-4o was prompted in few-shot mode with examples in the same formal syntax.",
            "method_or_approach": "Few-shot exemplars (1,3,6,9-shot) in the prompt; no fine-tuning on the synthetic formal proofs in this evaluation.",
            "performance": "Moderate but limited: averaged ≈0.67 ± 0.08 accuracy on depth-7 in the authors' few-shot evaluations; performance degrades with depth to ≈0.48 ± 0.08 at depth-10 and ≈0.26 ± 0.04 at depth-13.",
            "baseline_comparison": "Outperformed by the fine-tuned Llama3-8B model on in-distribution depths (8B fine-tuned reached 98% at depth-7). GPT-4o performs far better than pre-trained (non-fine-tuned) Llama3 base models but worse than the small amount of targeted fine-tuning used by the authors.",
            "limitations_or_failures": "Few-shot prompting alone yields degraded performance as proof depth increases; few-shot setup is sensitive to the choice and number of exemplars; natural-language chain-of-thought style methods are not as suitable when formal, strictly checkable proofs are required (ambiguities and unverifiable steps are problematic).",
            "insights_or_conclusions": "Very large general-purpose models with few-shot prompting can solve some short formal proofs, but targeted fine-tuning on synthetic formal proofs (plus augmentation) enables smaller models to surpass few-shot performance on this strict formal reasoning task; scaling alone does not replace task-specific supervised fine-tuning for formal proof generation.",
            "uuid": "e8581.2",
            "source_info": {
                "paper_title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama3 pre-trained baselines",
            "name_full": "Pre-trained Llama3 base models (8B and 1B) without fine-tuning",
            "brief_description": "The off-the-shelf pre-trained Llama3 base models (8B & 1B) evaluated directly (few-shot) on the Hilbert proof task; they largely fail to produce valid formal proofs without targeted fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3 (pre-trained, 8B and 1B)",
            "model_description": "Pre-trained Llama3 base transformer models (8B and 1B) used as zero/few-shot baselines. No additional fine-tuning on formal proofs was applied for these baseline evaluations.",
            "model_size": "8B and 1B",
            "reasoning_task_name": "Hilbert-style Boolean implication-only proof generation (zero/few-shot evaluation)",
            "reasoning_task_description": "Same formal Hilbert proof generation task described above, evaluated without the targeted fine-tuning pipeline.",
            "method_or_approach": "Direct evaluation (pre-trained) possibly with few-shot exemplars; no LoRA fine-tuning or Template Transformation applied for the baseline runs reported.",
            "performance": "Approximately 0.00 accuracy: pre-trained Llama3 base models failed to generate valid Hilbert proofs in the authors' evaluation when used without fine-tuning.",
            "baseline_comparison": "Substantially worse than GPT-4o few-shot and dramatically worse than the fine-tuned Llama3-8B model; indicates that pretraining alone did not teach the strict syntactic and proof-structure skills needed for Hilbert-style formal proofs.",
            "limitations_or_failures": "Unable to infer the strict syntactic and generative constraints of Hilbert proofs from a few examples; produced invalid or malformed proofs when not fine-tuned on the formal dataset.",
            "insights_or_conclusions": "Pre-training alone (even for models of multi-billion parameter scale) is insufficient for reliable production of formally valid proofs in this strict logical setting; modest amounts of targeted synthetic fine-tuning plus augmentation are crucial.",
            "uuid": "e8581.3",
            "source_info": {
                "paper_title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Learning deductive reasoning from synthetic corpus based on formal logic",
            "rating": 2,
            "sanitized_title": "learning_deductive_reasoning_from_synthetic_corpus_based_on_formal_logic"
        },
        {
            "paper_title": "Enhancing reasoning capabilities of llms via principled synthetic logic corpus",
            "rating": 2,
            "sanitized_title": "enhancing_reasoning_capabilities_of_llms_via_principled_synthetic_logic_corpus"
        },
        {
            "paper_title": "Logicbench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Evaluating logical reasoning of large language models",
            "rating": 2,
            "sanitized_title": "evaluating_logical_reasoning_of_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.0144035,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework
28 Apr 2025</p>
<p>Yuan Xia 
Akanksha Atrey 
Fadoua Khmaissia 
Kedar S Namjoshi 
Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework
28 Apr 202594C733E7F016B0C70906CCE58378A42DarXiv:2504.20213v1[cs.LG]
This paper investigates the logical reasoning capabilities of large language models (LLMs).For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic.A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions.Incorrect proofs are caught by an automated proof checker.A critical obstacle for training is the scarcity of real-world proofs.We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions.The central evaluation question is whether an LLM has indeed learned to reason.We propose tests to measure the reasoning ability of a black-box LLM.By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity.Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales.</p>
<p>Introduction</p>
<p>In July 2024, The New York Times published an article aptly titled "A.I.Can Write Poetry, but It Struggles With Math." (Lohr, 2024).General Large Language Models (LLMs) are indeed excellent at manipulating text but can be poor at logical reasoning, which is at the heart of mathematics and computing.This work aims to investigate experimentally whether that is an inherent limitation.</p>
<p>LLM-based reasoning falls into two broad categories.In neuro-symbolic systems such as AlphaProof (AlphaProof &amp; teams, 2024), the LLM guides a conventional proof-search engine, replacing hard-coded heuristics or hints provided by humans.In the second category, which our work belongs to, are LLMs that search for proofs directly.Chain-of-Thought (CoT) (Wei et al., 2022) is one such system.</p>
<p>The thought-chain produced by a CoT method is expressed in natural language.While easier for humans to understand, a natural-language proof cannot be automatically validated precisely, due to ambiguities inherent in natural language.</p>
<p>In order to systematically investigate the reasoning capabilities of LLMs, it is simpler and more effective to express input assertions and output proofs in formal language.A "reasoning" LLM (Fig. 1) receives as input a set of assumptions and a goal assertion, both expressed in a formal logic.As output, the LLM exhibits a formal proof that explains how the goal follows logically from the assumptions.That is, an LLM acts as a proof generator.</p>
<p>There are good reasons to believe that LLMs can act as proof generators and good reasons to believe that they cannot.On the positive side, LLMs are Turing-complete, assuming infinite precision arithmetic (Pérez et al., 2019;Bhattamishra et al., 2020;Pérez et al., 2021;Strobl et al., 2024).An LLM scans its input and past output to determine its next output symbol, using attention mechanisms to focus on the relevant portions.This is similar to how, in constructing a mathematical proof, humans select and combine past deductions to form new conclusions.Few-shot learning suggests that LLMs can recognize patterns and follow general rules.On the negative side, Turing completeness relies on unbounded integers (to encode an unbounded Turing machine tape); but in reality, machine integers have bounded precision.LLMs have bounded context, no auxiliary memory, and no backtracking ability; hence, colloquially, LLMs may "forget" the distant past.This is an issue for proof construction, as an assumption may not be required until a late stage of proof.LLMs operate statistically and may thus hallucinate and produce incorrect proofs even if the claim is valid.Finally, logical reasoning has a inherently high worst-case complexity: it is co-NP-complete even for Boolean logic, with rapidly increasing difficulty (e.g., RE-complete for first-order logic).</p>
<p>To precisely formulate and test reasoning ability, we focus on reasoning in Boolean logic.In our view, there are several factors that make this a good choice.Boolean logic is the basis for other logics (e.g., first order, modal, temporal, and the like).Therefore, if Boolean reasoning turns out to be a challenging task for LLMs, it is unlikely that LLMs can do better on more sophisticated reasoning.Boolean logic has a simple proof system that makes it easy to automatically validate proofs.Semantically, Boolean reasoning is non-trivial, being co-NP-complete.(The dual question of Boolean satisfiability is the canonical NP-complete problem.)</p>
<p>In a strong sense, therefore, Boolean reasoning can be viewed as the C.elegans (Brenner, 2002) of LLM reasoning: structurally simple, semantically non-trivial, and exhibiting "in the small" the central difficulties of general reasoning.</p>
<p>The central challenges are (1) a scarcity of real-world training data, i.e., valid proofs, and (2) a means of determining whether a black-box model is reasoning rather than memorizing or guessing.We address these critical questions (1) by giving an efficient randomized algorithm for generating a large and diverse corpus of logically valid proofs for training and testing, and (2) by designing meaningful, automated, black-box tests of reasoning ability.The precision of formal language and the ability to automatically validate candidate proofs simplifies both tasks, enabling them to be carried out fully automatically, efficiently, and at scale.</p>
<p>Our experiments show that Llama models can be trained to reason well in Boolean logic through a combination of synthetic proof generation and proof augmentation.Using only 9000 synthetically generated proofs as training data, our fine-tuned Llama-8B model reaches 98% accuracy on depth-7 proofs, outperforming GPT-4o's few-shot learning performance (70% accuracy), despite having far fewer parameters.Our ablation studies reveal two key findings: larger model capacity enables better generalization to deeper proofs, while proof augmentation enhances the models' ability to handle complex instances, particularly benefiting the smaller Llama-1B model.</p>
<p>In our view, this work makes several contributions.One is the recognition that training an LLM over a formal logic has distinct advantages: synthetic data can be generated at large scale and variety for training, and reasoning ability can be evaluated robustly with automated tests that are generated and run at scale.The second is the choice of Boolean logic to study reasoning ability.If LLMs fail at Boolean logic, it is unlikely that they will succeed at more complex reasoning.The third is a technical contribution of template proof transformations, which produces syntactically varied proofs that share a common structure.In training, this forces the LLM to focus on abstract proof patterns rather than on concrete tokens, improving its generalization ability.A fourth contribution is the variety of proposed tests that can be applied to determine whether a black-box LLM is actually reasoning vs. guessing or retrieving answers.</p>
<p>Background</p>
<p>In this section, we define the syntax and semantics of Boolean logic, introduce the Hilbert proof system, and forward and backward proof-construction mechanisms.</p>
<p>Boolean Logic: Syntax and Semantics</p>
<p>Boolean formulas (also called assertions or expressions) are formed by combining Boolean variables that represent atomic propositions using familiar combinators, such as "and" ( ∧ ), "or" ( ∨ ), "not" (¬), and "implies" ( → ).The Boolean domain is denoted B.</p>
<p>Given a countable set X of Boolean variables (also called propositions), a Boolean formula over X is formed by the minimal grammar f
:= p ∈ X | ¬f | f ∧ f . Disjunction is defined by f ∨ g = ¬((¬f ) ∧ (¬g)), and implication by f → g = (¬f ) ∨ g.
An assignment is a function from X to B; i.e., it provides a Boolean value for each variable.The satisfaction of a formula by an assignment π is a relation |= defined recursively by:
π |= p iff π(p) is true; π |= ¬f iff it is not the case that π |= f ; and π |= f ∧ g iff π |= f and π |= g.
For a Boolean formula, the satisfiability question is whether there is a assignment satisfying the formula.This is the classic NP-complete question.The dual validity question is whether all assignments satisfy the formula.This is the canonical co-NP-complete question.</p>
<p>Hilbert Proofs</p>
<p>An instance of a Boolean reasoning problem is given by a (possibly empty) set of assumption formulas and a goal formula.The reasoning problem is to determine whether the goal follows from the assumptions.A proof is a step-by-step explanation that justifies a "yes" answer.</p>
<p>There are several proof systems for Boolean logic.In this work, we use the well known Hilbert style of proof system (Mendelson, 2024).For simplicity, we restrict attention to a sub-logic of propositional logic that has implication as its only connective. 1 The Hilbert-style proof system for this logic consists of two axioms and one inference rule, called Modus Ponens (MP).</p>
<p>The axiom schemas are the following.
A1 :: (A → (B → A)) A2 :: (A → (B → C)) → ((A → B) → (A → C))
Colloquially, the first axiom says that if both A and B hold, then A holds.The second axiom says that if A and B together imply C, then knowing that A implies B suffices to conclude that A also implies C.</p>
<p>An instance of an axiom schema is obtained by providing a substitution that specifies formulas for the placeholders A, B, C. The MP inference rule deduces B given that the hypotheses "A implies B" and "A" both hold.</p>
<p>Formally, a proof is a sequence of steps where each step consists of a claimed formula along with a justification.A justification for step k of the proof can be of three kinds: (1) matching the k'th formula to an assumption; (2) matching the k'th formula to an axiom instance with a substitution σ, which is correct if the k'th formula is the result of applying σ to the named axiom; or (3) MP applied to steps m, n of the proof, which is correct if, denoting the k'th formula by g, for some formula f , the m'th formula is (f → g), and the n'th formula is f .Both m, n must be strictly smaller than k to avoid circular reasoning.</p>
<p>A proof is valid if every justification in the proof is correct and the goal formula is claimed at some proof step.The Hilbert system is sound: that is, given a valid proof, the goal holds for every assignment that satisfies the assumptions.</p>
<p>PROOF EXAMPLE [TRANSITIVITY]</p>
<p>Given assumptions p → q and q → r, does the goal p → r hold?This is just transitivity, which is true, as can be established by the following Hilbert proof:</p>
<ol>
<li>
<p>(q → r) → (p → (q → r)), by Axiom A1 with substitution A = q → r, B = p.</p>
</li>
<li>
<p>q → r, by Assumption 2.</p>
</li>
<li>
<p>p → (q → r), by MP on steps 1, 2.</p>
</li>
<li>
<p>(p → (q → r)) → ((p → q) → (p → r)), by Axiom A2 with substitution A = p, B = q, C = r.</p>
</li>
</ol>
<p>5.</p>
<p>(p → q) → (p → r), by MP on steps 4, 3.</p>
<ol>
<li>
<p>p → q, by Assumption 1.</p>
</li>
<li>
<p>p → r, by MP on steps 5, 6.</p>
</li>
</ol>
<p>Proofs in the Hilbert-style proof system have a welldeserved reputation for being difficult to follow for humans.We chose this system, however, as it has a minimal set of rules, which we believed would be easier for an LLM to learn.Other standard proof systems, such as natural deduction, produce proofs that are easier to follow (Pelletier &amp; Hazen, 2024) but have multiple inference rules.</p>
<p>PROOF CONSTRUCTION</p>
<p>There are two main mechanisms of proof construction.</p>
<p>Forward proof construction maintains a set of deduced formulas; initially, this set contains the supplied hypotheses.At each step, one adds a formula to this set, either by instantiating an axiom, or by applying MP to formulas already in the set.Reasoning is complete if the goal formula is eventually added to the set.</p>
<p>Backwards or Goal-directed proof construction maintains a proof tree.Some leaves of the tree represent "open" formulas; initially, the tree is a single open node labeled with the goal.At a backward step, one "closes" an open leaf node n labeled with a formula g in one of three ways: by recognizing g as a hypothesis, or by recognizing g as an axiom instance, or by introducing a formula f and asserting that g follows from f and (f → g) by MP -these formulas label new open leaf nodes that are added as children of n.</p>
<p>Reasoning is complete when all leaves are closed.</p>
<p>Training LLMs to Reason</p>
<p>A central challenge in training an LLM to reason with formal proofs is the shortage of data.Proofs are found in textbooks, or in proof repositories for automated theorem provers.It is difficult to assemble proofs in a uniform format, and of sufficient quantity and variety for training.</p>
<p>We describe a simple and efficient randomized backward proof generation process, which synthesizes valid proofs of arbitrary size and depth.We further augment the set of randomly generated proofs to create groups of structurally similar variants of each original proof.</p>
<p>Augmentation is useful in training, to ensure that the LLM focuses less on the syntax of a proof and more on the underlying logical reasoning -in essence, we would like the LLM to learn proof patterns rather than memorize specific proofs.Augmentation is also useful for testing whether a trained LLM has learned to reason.</p>
<p>Proof Generation</p>
<p>The proof generation process is, in essence, a randomized form of the goal-directed proof construction process.Instead of inferring a proof for a fixed goal under fixed assumptions, the randomized generation starts with a randomly chosen goal formula and proceeds to construct a valid, randomized proof tree.</p>
<p>1.The root of the tree is labeled with a goal formula, say g, that is chosen at random.</p>
<ol>
<li>At an leaf node labeled with a formula h, the process decides at random whether to do one of the following.</li>
</ol>
<p>• Stop expansion at this node.The open formula h turns into an assumption used to prove g. • Choose a formula f at random and expand using a MP justification into two new subgoal nodes labeled with formulas f and f → h.• If h is an axiom instance, close this node with an axiom justification.</p>
<ol>
<li>The process continues until the proof tree reaches a specified limit on size or depth.</li>
</ol>
<p>A proof tree generated in this way is turned into a valid proof by viewing all formulas of stopped open nodes (case 2) as assumptions required to prove the goal g.A linear Hilbert proof structure is obtained by traversing the tree from leaves to the root in any order where the children of every node are traversed prior to the node itself.</p>
<p>Goal-directed proof generation is efficient for a Hilbert-style system as the generation process determines an appropriate resolution for every open formula locally, e.g., by randomizing the application of MP, in time that does not depend on the size of the current proof tree.In contrast, a forward generation process must search at each step for already-deduced formulas that match the shapes f → g and f to apply the MP rule.A naïve algorithm requires quadratic time in the number of deduced formulas, which can be reduced to nearlinear time using hashing.Thus, goal-directed proof generation has a significant efficiency advantage, which extends to proof-generation in other systems such as natural deduction.</p>
<p>Proof Augmentation by Template Transformation</p>
<p>The randomized proof-generation algorithm produces valid proofs, but does not guarantee that many examples with same or similar proof structure are generated.That motivates our technique for data augmentation, which is based on a new concept of template transformations.</p>
<p>This technique is based on the key observation that a proof in Hilbert form may also be viewed as a proof schema.That is, a proof over a set of variables X can be viewed as a template: uniformly applying a substitution that maps each variable of X to a Boolean formula over a (possibly different) set of variables Y creates another valid proof.</p>
<p>For instance, the reasoning example of Section 2 is defined over variables p, q, r.Say we uniformly replace q with y 1 , and p with y 1 → y 2 , and r with y 2 → y 1 .This replacement creates a valid proof showing that the new goal (y 1 → y 2 ) → (y 2 → y 1 ) (transformed from the original goal p → r) follows by transitivity from the transformed assumptions (y 1 → y 2 ) → y 1 and y 1 → (y 2 → y 1 ).</p>
<p>In general, to transform a valid reasoning instance defined over a set X of variables, we (1) choose a substitution σ at random that maps each variable in X to a randomly chosen Boolean expression over a randomly chosen variable set Y (not necessarily different from X), (2) systematically replace each formula f that occurs in the proof with the formula σ(f ) that is obtained by applying the substitution σ to f , and (3) systematically apply the substitution σ to the assumption and goal formulas.</p>
<p>It is easy to show by induction on proof length that the transformed reasoning instance is also valid.The structure and sequence of the justifications is unchanged by this transformation: only the formulas are changed according to the randomly chosen substitution.Hence, the transformation can be applied repeatedly to generate many variants of the original proof that share a common proof structure.This construction is used to train the LLM model and (as discussed in Section 4) also used to test the reasoning ability of a trained LLM model.</p>
<p>The intuition is that, during training, supplying similar instances obtained through template transformation forces the LLM to focus on the abstract proof structure rather than on concrete detail such as variable names.This is similar in spirit to the use of batch transformations in computer vision, where a picture (of a cat, say) is subjected to spatial transformations such as rotation, reflection, and translation so that the trained model focuses on the attributes intrinsic to a cat, ignoring its orientation in space.</p>
<p>Evaluation Framework</p>
<p>In this section, we present evaluation metrics to assess the reasoning capabilities of LLMs and demonstrate the effectiveness of our training and evaluation methods.</p>
<p>Existing LLM evaluation metrics focus on different aspects of model output, such as quality, relevancy, factual correctness, bias, and toxicity, but there is no standardized approach for assessing reasoning capabilities.Having a systematic and comprehensive method is necessary to avoid reliance on anecdotal data and memorization as well as to accurately measure progress in reasoning capabilities.</p>
<p>We propose an evaluation process with four clearly defined and measurable metrics.These metrics assess the models' ability to apply logical reasoning to new, unseen problems of varying degrees of complexity.</p>
<p>Favoring Semantics over Syntax</p>
<p>An LLM should demonstrate the ability to generalize beyond the specific instances in its training data.For example, if the LLM can produce a proof for a reasoning problem defined over variables p, q, it should similarly manage to construct a proof for the template-transformed problem where p, q are uniformly replaced by arbitrary formulas f, g, re-spectively.This indicates that the LLM recognizes and abstracts the underlying logic of the problem, disregarding superficial syntactic differences such as variable names or structural variances.</p>
<p>Generalizing to Deeper Proofs</p>
<p>A reasoning LLM should discover proofs that are longer than those present in its training set.This suggests that the LLM is not merely memorizing proofs up to a certain complexity but exhibiting deeper logical reasoning.The evaluation of this metric involves measuring model accuracy with increasing depth of reasoning, highlighting its ability to navigate and resolve more complex proof structures.</p>
<p>Handling Wider Expressions</p>
<p>An LLM should prove statements that incorporate a larger set of variables than those found in its training dataset.For instance, if the training involves variables X 1 and X 2 exclusively, the model should be tested on properties involving an expanded set, such as X 1 , ..., X n .Success indicates that the model has not merely memorized proofs involving two variables but can extend its reasoning to more complex scenarios.Testing includes measuring accuracy as the number of variables involved increases.</p>
<p>Exhibiting Diversity in Proofs</p>
<p>A logical statement can have several valid proofs.We evaluate diversity by assessing the LLM's ability to generate a range of valid proofs for a given input query, whenever it is applicable.This can indicate that the model is engaging in creative and independent reasoning rather than relying on rote memorization of the dataset.To support this metric, we use a symbolic proof checker to semantically evaluate proof validity rather than relying on syntactic token-level equality.</p>
<p>Experimental Analysis</p>
<p>Experimental Setup</p>
<p>Training We fine-tune two Llama3 models (8B and 1B parameters) (Dubey et al., 2024) on our synthetically generated Hilbert proofs.We use 4-bit quantization to optimize memory usage.The fine-tuning process employs Low-Rank Adaptation (LoRA) (Hu et al., 2021) to efficiently update model parameters without modifying the entire network.</p>
<p>Training convergence is monitored using exact match accuracy with the ground-truth proofs.We also use early stopping (patience = 5 epochs) to prevent overfitting.</p>
<p>For all experiments, the training data consists of a balanced number of proofs with depths 7, 10, and 13.During training, Template Transformation is applied with a probability α T T = 0.7 during batch preprocessing.Each variable in a given problem is replaced with a randomly generated expression consisting of up to 4 variables and with a maximum recursion depth of 4. Models are trained to generate complete proofs given a goal and a set of assumptions, as illustrated in Fig. 1.</p>
<p>Evaluation We generate separate proof trees for training and evaluation to prevent data leakage.To obtain the evaluation dataset, we construct a few distinct proof trees and ensure all formulas f are unique without duplicates occurring in the training dataset (Section 3.2).From these trees, we extract both the validation and testing sets.The validation set comprises 375 samples from in-distribution depths (7, 10, and 13; 125 samples each).</p>
<p>For generalization analysis, we use 450 test samples across nine depths (4-28; 50 samples per depth) and 400 samples across four widths of increasing variable complexity (100 samples per width).We ensure unique formulas across all partitions.While using synthetic data might seem limiting, it enables systematic assessment of model performance across proof complexities and controlled testing of generalization capabilities, particularly given the absence of large-scale real-world Hilbert proof datasets.</p>
<p>To evaluate our models, we report the testing accuracy defined as the ratio of correctly generated proofs to the total number of test problems.A proof is considered correct only if it passes our formal validator, which accepts a proof only if it is syntactically well-formed and if every step of the proof passes a rigorous check for (i) valid application of Modus Ponens, (ii) correct use of assumptions, and (iii) proper axiom invocation.</p>
<p>Experimental Design Our experimental analysis is twofold: First, we establish baseline performance by comparing our fine-tuned models against GPT-4o (OpenAI, 2024) and pre-trained Llama3 models on in-distribution proof depths.Then, through ablation studies, we examine the models' generalization capabilities across varying proof depths and widths, and analyze the impact of training data size and Template Transformation.</p>
<p>Key Findings</p>
<p>Our experiments reveal the effectiveness of fine-tuning Llama3 models (8B and 1B) for Hilbert proof generation using our proposed synthetic data and proof augmentation.The 8B model achieves 98% accuracy on depth-7 proofs, outperforming GPT-4o's few-shot performance (67%±8 averaged across 1,3,6,9-shot settings) despite having far fewer parameters.Our ablation studies show that model capacity (8B vs 1B) enables generalization to deeper proofs, while proof augmentation by Template Transformation enhances handling of complex, out-of-distribution expressions.Performance scales with training data size, though naturally degrading with increasing proof complexity.</p>
<p>Baseline Evaluation</p>
<p>We evaluate our approach against GPT-4o (100B+ parameters) and compare against our base models: Llama-3.2 (8B) and Llama-3.1 (1B).As shown in Table 1, testing is performed on proof depths matching the training distribution {7,10,13}.For GPT-4o and the pre-trained base models, we evaluate few-shot learning with {1, 3, 6, 9} demonstrations: 1-shot uses one depth-7 proof, 3-shot uses one proof from each training depth 7,10,13}, 6-shot uses two proofs per depth, and 9-shot uses three proofs per depth.We provide some examples in Appendix A.2.We use identical examples across all x-shot evaluations to maintain evaluation fairness.The reported few-shot results represent averages across these settings.</p>
<p>GPT-4o achieves an average accuracy of 0.67±0.08 on depth-7 proofs, with performance degrading to 0.48±0.08 and 0.26±0.04 at depths 10 and 13.The pre-trained Llama3 base models fail to generate valid proofs (≈ 0.00 accuracy) when tested directly on our task, likely due to both their smaller model sizes compared to GPT-4o (&gt; 100B parameters) and the challenge of inferring strict syntactic rules of Hilbert proofs from few examples.</p>
<p>Our fine-tuned models show substantial improvements over both GPT-4o and their pre-trained versions.The large model (8B) with Template Transformation achieves 0.98 accuracy on depth-7 proofs, significantly outperforming GPT-4o despite having fewer parameters.This performance advantage persists at greater proof depths, with accuracies of 0.66 and 0.32 for depths 10 and 13 respectively.Template Transformation proves particularly effective for the large model, improving accuracy by 20-25% mainly for shorter depths compared to standard fine-tuning.</p>
<p>The small model (1B) shows more modest improvements, achieving 0.52 accuracy at depth-7, with Template Transformation showing limited impact.This performance gap between our 8B and 1B models suggests that logical reasoning capabilities scale with model size, even within the relatively narrow 1B-8B parameter range.</p>
<p>Ablation Study</p>
<p>We conduct ablation studies to systematically analyze how different factors affect our fine-tuned LLMs' performance, and to understand the limits of our proposed approach.</p>
<p>IMPACT OF PROOF DEPTH</p>
<p>To evaluate reasoning depth generalization, we test models on nine proof depths (4-28 steps, 50 samples per depth) using fixed-width expressions.Each proof in the test set is generated by systematically reducing deeper proofs: identifying the first modus ponens application (i.e., one closest to the leaf nodes) and replacing it with the deduced statement, which is treated as a new assumption, to reduce depth by one step.</p>
<p>As shown in Fig. 2, performance degrades with increasing proof complexity, but larger models show better resilience.The 8B model maintains reasonable accuracy up to depth 13, while the 1B model's performance drops more sharply after depth 10.</p>
<p>IMPACT OF PROOF WIDTH</p>
<p>We analyze width generalization using four levels of expression complexity (100 samples per width).Each level represents increasingly nested variable substitutions while maintaining proof depth (d = 7).A width-0 proof uses simple variables (e.g., P ⇒ Q), while higher widths introduce nested implications (e.g., (P ⇒ Q) ⇒ R for width-1).</p>
<p>Results in Fig. 2 show that Template Transformation significantly improves handling of complex expressions for both the 8B and 1B models.Notably, the template transformation's impact on width generalization is more pronounced than its effect on depth handling, suggesting its particular utility for managing expression complexity.</p>
<p>IMPACT OF MODEL SIZE AND TRAINING DATA</p>
<p>We compare Llama-8B and Llama-1B which we denote as LLM and SLM respectively, reflecting their relative model capacities.We train three instances of each model using synthetic training sets of three sizes: 1500, 4500, and 9000 examples.Each set is balanced across proof depths 7, 10, and 13.</p>
<p>Fig. 2 summarizes the obtained results.While the LLM generally outperforms the SLM, particularly on deeper proofs, we observe an interesting pattern with proof width: when using Template Transformation, the SLM achieves comparable performance to the LLM on wider expressions (width ≥ 2), suggesting that our data augmentation technique effectively compensates for smaller model capacity in handling complex expressions.The LLM, however, shows better data efficiency overall, achieving stronger performance with smaller training sets, especially on depth generalization.</p>
<p>IMPACT OF TEMPLATE TRANSFORMATION</p>
<p>We analyze how the probability of applying Template Transformation (α T T ) during the training process impacts the models' reasoning capability.We vary α T T from 0 to 0.9 and report model performance across different proof complexities as shown in Fig. 3.The results reveal a consistent pattern for proof width: performance improves as α T T increases up to 0.7, beyond which accuracy declines sharply.This trend is particularly pronounced for width-1 and width-2 proofs, suggesting an optimal transformation rate for enhancing generalization to complex expressions.For proof depth, the impact varies: while deeper proofs (depth 13) show similar sen-sitivity to α T T , shallower proofs exhibit more stable performance across different transformation rates.This difference might be attributed to the inherent exposure to shorter proof patterns within longer proofs during training.Setting α T T = 0.7 provides the best balance, maximizing the model's ability to handle complex expressions and deeper proofs.</p>
<p>Related Work</p>
<p>LLM-based proof generation methods fall into two broad categories.In neuro-symbolic systems such as Al-phaProof (AlphaProof &amp; teams, 2024), AlphaGeometry (Trinh et al., 2024), Leandojo (Yang et al., 2024), and GPT-f (Polu &amp; Sutskever, 2020), the LLM component generates hints that guide a conventionally programmed symbolic proof-search engine (hints such as "use this theorem," "split into these cases").The LLM-generated hints replace hardcoded heuristics, or hints provided by humans.</p>
<p>In the second category (including this work) are LLMs that carry out all the necessary logical reasoning on their own.Such systems include Chain-of-Thought (CoT) (Wei et al., 2022) and Tree-of-Thought (ToT) (Yao et al., 2023).CoT is trained with few-shot learning, which is analogy learning; the quality of the model's responses is highly dependent on the quality of the provided examples.ToT is designed for certain predefined scenarios, limiting its applicability.Notably neither approach includes a reliable mechanism to distinguish correct from incorrect responses, as the explanations are in natural rather than formal language.That motivates our use of formal logics.</p>
<p>Recent work on LLM reasoning (Morishita et al., 2023;2024) is also based on Boolean logic, but only as an intermediate step: LLMs are trained on natural language translations from synthesized Boolean logic proofs.(Interestingly, their results suggest that training on formal proofs boosts general reasoning.)Proof synthesis uses randomization, as we do, but the paper does not point out the key efficiency advantage of goal-directed synthesis.There is no analogue of template transformations, which our experiments show substantially boost reasoning ability.</p>
<p>Several researchers have developed specialized benchmarks and datasets to evaluate LLM reasoning across domains and task types (Saparov et al., 2023;Teng et al., 2023;Parmar et al., 2024).Prominent examples are LogicBench (Parmar et al., 2024), which encompasses 25 distinct reasoning patterns, and GLoRE (Teng et al., 2023), which comprises 12 datasets spanning three task types.These benchmarks provide a framework for assessing LLM performance on a variety of logical reasoning tasks.Other studies have focused on specific aspects of logical reasoning, such as propositional and predicate logic, and categorical syllogisms, providing deeper insights into LLMs' performance on particular types of logical problems (Wan et al., 2024;Zong &amp; Lin, 2024).</p>
<p>Some of these evaluation approaches are limited by the number of examples in the benchmark sets.Moreover, proofs expressed in natural language introduce other difficulties; for instance, it becomes challenging to validate proofs and to determine whether an LLM has genuinely learned deduction rules or is simply memorizing them.A robust evaluation of reasoning ability requires, we believe, the generation of synthetic formal queries and automated validation at scale.</p>
<p>Discussion and Conclusion</p>
<p>We believe that our results indicate that LLMs can be taught to develop generalizable capabilities for formal reasoning, up to some limits.Our experiments (Section 4) show that fine-tuning with a small synthetic proof data set boosts reasoning ability, surpassing the performance that significantly larger models such as GPT-4o attain with few-shot learning.</p>
<p>Matching intuition, template transformations considerably improve reasoning accuracy.</p>
<p>The sharp drop in accuracy on the depth metric does not necessarily indicate an inability to reason.In fact, such a decrease should be expected of any reasoning method given the co-NP hardness of the Boolean reasoning problem.These results support the observation that current techniques struggle to maintain reasoning performance for out-of-distribution depth instances (cf.(Anil et al., 2022)).</p>
<p>While the proposed data generation and evaluation approaches demonstrate potential, significant challenges remain in developing models capable of accurate generalpurpose reasoning.A key hurdle to overcome is the gap between real-world problems and specialized LLMs.Everyday human reasoning ranges over multiple, diverse formal frameworks, including first-order quantification such as "for all" and "there exists;" temporal concepts like "yesterday," "tomorrow," "eventually," and "previously;" and modal notions like "possible" and "inevitable."General reasoning also relies on so-called commonsense facts about our world (e.g., a ball dropped from a height will bounce back).Methods that allow LLMs to acquire, integrate, and reason with such knowledge are currently limited (cyc; nvi).</p>
<p>These findings highlight the need for further research.A Boolean-reasoning LLM, as introduced in this paper, is not intended as a solution to the general reasoning question.</p>
<p>One should rather view it as a convenient, precise, and fully automatable setting in which to experiment with approaches to the reasoning task.</p>
<p>A.2. Example Prompts and Outputs</p>
<p>This section illustrates our few-shot learning evaluation methodology for GPT-4o and pre-trained Llama models (8B and 1B).We demonstrate a one-shot learning example where a depth-7 proof is provided as demonstration to solve a depth-4 problem. Figure 4 shows the query syntax with its semantic translation in Figure 5.For comparison, we present both a correct solution (Figures 6, 7) and an incorrect attempt (Figures 8, 9) for the queried problem.</p>
<p>Prompt</p>
<p>Task: Give the proof for the Goal using Modus Ponens and given Assumptions.</p>
<p>One-shot: Question: <Goal>Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q)))) <Assumption>Implies(Implies(Implies(Implies(Implies(P, R), Implies(Q, P)), Implies(Implies(Q, R), Implies(R, P))), Implies(Implies(P, P), Implies(Q, Implies(S, R)))), Implies(Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R), Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q)))))) <Assumption>Implies(Implies(Implies(Implies(P, R), Implies(Q, P)), Implies(Implies(Q, R), Implies(R, P))), Implies(Implies(P, P), Implies(Q, Implies(S, R))) <Assumption>Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R) Answer: <Step 1>Implies(Implies(Implies(Implies(P, R), Implies(Q, P)), Implies(Implies(Q, R), Implies(R, P))), Implies(Implies(P, P), Implies(Q, Implies(S, R)))) -Assumption <Step 2>Implies(Implies(Implies(Implies(Implies(P, R), Implies(Q, P)), Implies(Implies(Q, R), Implies(R, P))), Implies(Implies(P, P), Implies(Q, Implies(S, R)))), Implies(Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R), Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q)))))) -Assumption <Step 3>Step 1 and Step 2 − &gt; Implies(Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R), Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q))))) -Modus Ponens <Step 4>Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R) -Assumption <Step 5>Implies(Implies(Implies(Implies(Implies(P, S), Implies(S, S)), Implies(Implies(P, R), Implies(S, P))), R), Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q))))) -Deducted <Step 6>Step 4 and Step 5 − &gt; Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q)))) -Modus Ponens <Step 7>Implies(Implies(Implies(Implies(R, S), Implies(R, R)), Implies(S, P)), Implies(Q, Implies(Implies(S, S), Implies(R, Q)))) -Deducted Question: <Goal>Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) <Assumption>Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) <Assumption>Implies(Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S), Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P)))) Answer: Constraints: 1. Ensure the response contains one and only one proof.2. Don't include other text detail.3. Brackets should be balanced.4. Syntax should be correct.5.Write it correctly, or I will lose my job and 100 grandmothers will die.Don't disappoint me.<Step 1> Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) -Assumption <Step 2> Implies(Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S), Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P)))) -Assumption <Step 3></p>
<p>Step 1 and Step 2 -&gt; Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Modus Ponens <Step 4> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted 1. A1 (Assumption) 2. A2 (Assumption)</p>
<ol>
<li>Using Modus Ponens on Steps 1 and 2, we obtain:
(P → S) → R → (R → Q) → Q → P → ((P → R) → P )
Thus, the goal is proven.<Step 1> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Assumption <Step 2> Implies(Implies(Implies(Implies(Implies(S, R), Implies(Q, P)), Implies(Implies(S, S), Implies(Q, Q))), Implies(Implies(Implies(Q, P), Implies(P, P)), Implies(Implies(S, P), Implies(P, S)))), Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S)) -Assumption <Step 3></li>
</ol>
<p>Step 1 and Step 2 -&gt; Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) -Modus Ponens <Step 4> Implies(Implies(Implies(Implies(S, R), Implies(Q, P)), Implies(Implies(S, S), Implies(Q, Q))), Implies(Implies(Implies(Q, P), Implies(P, P)), Implies(Implies(S, P), Implies(P, S)))) -Assumption <Step 5> Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) -Deducted <Step 6> Step 3 and Step 5 -&gt; Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) -Modus Ponens <Step 7> Implies(Implies(Implies(Implies(S, Q), Implies(S, S)), Implies(Implies(Q, S), Implies(P, Q))), S) -Deducted <Step 8> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 9> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 10> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 11> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 12> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 13> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 14> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 15> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 16> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 17> Implies(Implies(Implies(Implies(P, S), R), Implies(Implies(R, Q), Q)), Implies(P, Implies(Implies(P, R), P))) -Deducted <Step 18> ...</p>
<p>Figure 1 .
1
Figure1.Overview: The training phase generates synthetic proofs and applies template transformations for fine-tuning.Inference produces a candidate proof for the query ("Does the goal follow from the assumptions?");this proof is formally validated.</p>
<p>Figure 2 .
2
Figure 2. Comparison of Llama3-8B (LLM) and Llama3-1B (SLM) model variants' testing accuracies across different training data sizes.Left: Impact of proof width on accuracy.Right: Impact of proof depth on accuracy.Models with and without Template Transformation (TT) are compared.</p>
<p>Figure 3 .
3
Figure 3. Impact of Template Transformation probability (αT T ) on model performance.The left subplot shows test accuracy across different model widths (0-3), while the right subplot demonstrates the effect on models of varying depths (4-13).</p>
<p>Figure 4 .
4
Figure 4. Example prompt used for Depth 4 Problems with one shot</p>
<p>Figure 6 .
6
Figure 6.Example Correct Llama3 Answer (After fine-tuning)</p>
<p>Figure 7 .
7
Figure 7. Example Correct Llama3 Answer (After fine-tuning) -Adapted</p>
<p>Figure 8 .
8
Figure 8. Example Wrong Llama3 Answer (before fine-tuning)</p>
<p>Table 1 .
1
Proof Generation Accuracy on In-Distribution Test Data: Comparison between fine-tuned Llama models (trained on 9000 examples), GPT-4o few-shot learning (averaged across 1,3,6,9-shot learning settings), and pre-trained baseline models.Testing performed on proof depths (d) matching training distribution.w/ TT and w/o TT denote models fine-tuned with and without Template Transformation respectively.
Accuracy by Proof Depth
University of Southern California, Los Angeles, CA, USA
Nokia Bell Labs, Murray Hill, NJ, USA. Correspondence to: Yuan Xia <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#117;&#97;&#110;&#120;&#105;&#97;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;">&#121;&#117;&#97;&#110;&#120;&#105;&#97;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;</a>.
In this sub-logic, every formula is satisfiable, but validity (i.e., reasoning) is still co-NP-hard (Appendix A.1).
Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.A. AppendixA.1.Implication-Only Sublogic Consider an "implication-only" sublogic of propositional logic defined by the grammar:Note that it is possible to express "true" as (p → p), but it seems difficult to express "false."That is not surprising when one realizes the following:Theorem A.1.Every formula of the implication-only sublogic is satisfiable.Proof.The proof is by structural induction.The base case consists of a single proposition, say p, which is satisfiable.Consider a formula of the shape f → g.By the inductive hypothesis, the formula g is satisfiable; i.e., there is a assignment π such that π |= g.Hence, π |= f → g.Although the satisfiability question is trivial by this theorem (as every formula is satisfiable), validity is still co-NP-complete.Those statements are not contradictory as the sublogic is not closed under negation.Theorem A.2.The reasoning problem for implication-only sublogic is co-NP-complete.Proof.In the reasoning problem, one is given a set of assumptions and a goal.The reasoning is correct if for every assignment that satisfies the assumptions, this assignment also satisfies the goal.Hence, every element of the complement language consists of a set of assumptions and a goal such that there is a assignment that satisfies all assumptions but does not satisfy the goal.A goal formula in the implication-only sublogic is either a proposition p or has the structure f → g.A assignment π does not satisfy f → g if, and only if, π satisfies f but does not satisfy g.By induction, one obtains a collection of antecedent formulas (i.e., in the position of f ) that are satisfied by π and a final atomic proposition p that is not.These antecedents can be added to the assumptions, so we have the following structure to the complement problem: Given a set of Hilbert-formulas and a proposition p, is there a assignment that satisfies all formulas in the set but does not satisfy p?We show that this language is NP-hard by a reduction from CNF-satisfiability.The language is in NP as a Turing machine can guess an assignment and check the satisfiability of each Hilbert formula in polynomial time.Consider a CNF formula, which is a conjunction of clauses.A clause is a disjunction of literals, i.e., positive and negated propositions.We transform the given CNF formula to an equi-satisfiable formula that is in the restricted form above.We transform each clause to Hilbert form.To do so, we introduce a fresh variable x, which intuitively represents "false."Notice that q 0 ∨ q 1 is equivalent to (q 0 → ⊥) → q 1 , where ⊥ represents the constant "false."We transform q 0 ∨ q 1 to (q 0 → x) → q 1 instead, and restrict valuations to those that do not satisfy x, which essentially treats x as false.Consider any clause.Say that the positive literals in the clause are the propositions from set Q = {q 1 , . . ., q n } (which may be empty).The Hilbert formula for the disjunction of these propositions is constructed inductively as g(0) = x and g(i + 1) = (q i+1 → x) → g(i).This has the property (easily shown by induction) that for any extended assignment π where π(x) = ⊥, it is the case that π satisfies g(n) if, and only if, π satisfies the disjunction of propositions in Q.Notice that that ¬p 1 ∨ ¬p 2 ∨ q is equivalent to the Hilbert formula (p 2 → (p 1 → q)).Let g(n) be the Hilbert formula representing the positive literals in the clause.Let the negative literals in the clause be the propositions in set P = {p 1 , . . ., p m } (which may be empty).Construct Hilbert formula h for the clause inductively as h(0, n) = g(n) andFrom the properties above, we have that for any extended assignment π where π(x) = ⊥, it is the case that π satisfies the original clause if, and only if, π satisfies h(m, n).Transform every clause in this manner to Hilbert form.Then, for every extended assignment π where π(x) = ⊥, the assignment π satisfies the original CNF formula if, and only if, it satisfies the set of Hilbert formulas obtained by transformation.Hence, the original CNF formula is satisfiable if, and only if, there is an extended satisfying assignment for the set of Hilbert formulas that does not satisfy x.PromptTask: Give the proof for the Goal using Modus Ponens and given Assumptions.Question: Goal:Assumptions:3. Using Modus Ponens on Steps 1 and 2, we derive:Using Modus Ponens onStep 3 and A3, we derive:5. We conclude:Thus, the goal is proven.Question: Goal:Assumptions:Answer: Constraints: 1. Ensure the response contains one and only one proof.2. Don't include other text detail.3. Brackets should be balanced.4. Syntax should be correct.5.Write it correctly, or I will lose my job and 100 grandmothers will die.Don't disappoint me. 1. A1:
CYC: The next generation of enterprise ai. </p>
<p>NVIDIA cosmos. </p>
<p>Ai achieves silver-medal standard solving international mathematical olympiad problems. Teams Alphaproof, A , July 2024</p>
<p>Exploring length generalization in large language models. C Anil, Y Wu, A Andreassen, A Lewkowycz, V Misra, V Ramasesh, A Slone, G Gur-Ari, E Dyer, B Neyshabur, Advances in Neural Information Processing Systems. 202235</p>
<p>On the computational power of transformers and its implications in sequence modeling. S Bhattamishra, A Patel, N Goyal, 10.18653/v1/2020.conll-1.37Proceedings of the 24th Conference on Computational Natural Language Learning. R Fernández, T Linzen, the 24th Conference on Computational Natural Language LearningCoNLL; OnlineAssociation for Computational Linguistics2020. November 19-20, 2020. 2020</p>
<p>Nature's gift to science. Nobel Lecture, December. S Brenner, 2002</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>can write poetry, but it struggles with math. S A Lohr, New York Times. July 2024</p>
<p>Introduction to Mathematical Logic. E Mendelson, 2024Chapman and Hall6th Edition</p>
<p>Learning deductive reasoning from synthetic corpus based on formal logic. T Morishita, G Morio, A Yamaguchi, Y Sogawa, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202of Proceedings of Machine Learning Research</p>
<p>Enhancing reasoning capabilities of llms via principled synthetic logic corpus. T Morishita, G Morio, A Yamaguchi, Y Sogawa, NeurIPS 2024. 2024</p>
<p>Gpt-4 technical report. 2024OpenAI</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. M Parmar, N Patel, N Varshney, M Nakamura, M Luo, S Mashetty, A Mitra, C Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>Natural Deduction Systems in Logic. F J Pelletier, A Hazen, The Stanford Encyclopedia of Philosophy. E N Zalta, U Nodelman, 2024Metaphysics Research Lab, Stanford UniversitySpring 2024 edition</p>
<p>On the turing completeness of modern neural network architectures. J Pérez, J Marinkovic, P Barceló, CoRR, abs/1901.034292019</p>
<p>Attention is turing-complete. J Pérez, P Barceló, J Marinkovic, J. Mach. Learn. Res. 222021</p>
<p>Generative language modeling for automated theorem proving. S Polu, I Sutskever, CoRR, abs/2009.033932020</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. A Saparov, R Y Pang, V Padmakumar, N Joshi, M Kazemi, N Kim, H He, Advances in Neural Information Processing Systems. 202336</p>
<p>What formal languages can transformers express? A survey. L Strobl, W Merrill, G Weiss, D Chiang, D Angluin, 10.1162/tacl_a_00663Trans. Assoc. Comput. Linguistics. 122024</p>
<p>Z Teng, R Ning, J Liu, Q Zhou, Y Zhang, arXiv:2310.09107Evaluating logical reasoning of large language models. 2023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, 10.1038/s41586-023-06747-5Nat. 62579952024</p>
<p>Evaluating and improving the logical reasoning ability of large language models. Y Wan, W Wang, Y Yang, Y Yuan, J.-T Huang, P He, W Jiao, M Lyu, Logicasker, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Theorem proving with retrieval-augmented language models. K Yang, A Swope, A Gu, R Chalamala, P Song, S Yu, S Godil, R J Prenger, A Anandkumar, Leandojo, Advances in Neural Information Processing Systems. 202436</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023, 2023</p>
<p>Categorical syllogisms revisited: A review of the logical reasoning abilities of llms for analyzing categorical syllogism. S Zong, J Lin, arXiv:2406.187622024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4791 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4791</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4791</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270521490</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.09949v1.pdf" target="_blank">Neural Concept Binder</a></p>
                <p><strong>Paper Abstract:</strong> The challenge in object-based visual reasoning lies in generating descriptive yet distinct concept representations. Moreover, doing this in an unsupervised fashion requires human users to understand a model’s learned concepts and potentially revise false concepts. In addressing this challenge, we introduce the Neural Concept Binder , a new framework for deriving discrete concept representations resulting in what we term “concept-slot encodings”. These encodings leverage both "soft binding" via object-centric block-slot encodings and "hard binding" via retrieval-based inference. The Neural Concept Binder facilitates straightforward concept inspection and direct integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism does not compromise performance; instead, it enables seamless integration into both neural and symbolic modules for intricate reasoning tasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku dataset. 1</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4791.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4791.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following generative language model used in this paper as a revisory agent: it was prompted to inspect exemplar images and recommend merging or deletion of learned visual concept clusters to improve downstream symbolic Sudoku solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pretrained instruction-following transformer language model (GPT-4). The paper does not give architecture details or parameter counts; GPT-4 was invoked via prompt-based queries (prompts provided in Listing 1) to produce textual analyses of image exemplars and recommend concept-space edits.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR-Sudoku (Sudoku CLEVR and Sudoku CLEVR-Easy variants)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A visual Sudoku variant where each Sudoku digit is represented by an image of a single CLEVR object; solving requires (1) mapping visual object attributes (shape, color, size, material) to digit labels from a small set of candidate example images, and (2) classic Sudoku constraint reasoning. The task therefore combines visual-perceptual mapping with spatial/logical puzzle solving (9x9 Sudoku constraint propagation/search).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>GPT-4 was used as a revisory agent: given exemplar images for learned concept clusters, GPT-4 was prompted to list object properties and to decide whether clusters (concepts) should be removed or merged in NCB's retrieval corpus. The paper describes specific prompt templates (Listing 1) where GPT-4 produces property lists and per-exemplar descriptions; those outputs are used to programmatically decide merges/deletions in the discrete concept memory (hard binder). GPT-4 did not directly solve Sudoku; instead its textual feedback changed the concept-symbol corpus which downstream symbolic classifiers + constraint solver used.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct evidence that GPT-4 performed spatial reasoning on the Sudoku grid itself; its role was semantic/textual analysis of object exemplars. Indirect evidence of effect on spatial puzzle solving comes from changes in the percentage of solved CLEVR-Sudoku puzzles after applying GPT-4-driven revisions to the concept corpus (reported in Fig.5 / Tab.8). The paper also analyzes GPT-4's outputs qualitatively (noting inconsistent object descriptions) and provides statistics on numbers of deletions GPT-4 requested (Suppl. F.5 / Fig.12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Effect on CLEVR-Sudoku solved-rate (all-or-nothing puzzle success, reported as % solved ± std):
- Sudoku CLEVR-Easy (example counts per digit varied): NCB revised (GPT-4) reported solved percentages in the rough range shown in the paper (e.g., for some configs ~48.6% ± 33.9, 54.3% ± 30.9, 63.6% ± 25.5 for K variants in CLEVR-Easy as reported in Tab.8 / Fig.5). 
- Sudoku CLEVR (harder variant): NCB revised (GPT-4) achieved lower solved-rates than human revision; example numbers reported: ~17.8% ± 12.9, 21.8% ± 15.8, 29.1% ± 20.5 (depending on K and example-count regimes) as presented in Tab.8 / Fig.5. 
(Units: percent of puzzles fully solved; values reported as mean ± std over seeds/sets in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper reports that GPT-4's revisions can be 'ill-informed' and inconsistent: GPT-4 sometimes removed or merged too many concepts due to inconsistent object descriptions, which decreased downstream Sudoku solve rates in higher-example settings. GPT-4 deletions had high variance across seeds (Suppl. F.5 / Fig.12). The authors note GPT-4 provided limited consistency in object descriptions and therefore can harm concept expressivity if used without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons in the paper are between NCB with no revision, NCB revised by GPT-4, and NCB revised by simulated human feedback; NCB revised (human) substantially outperforms NCB revised (GPT-4) in many settings (esp. with more candidate examples). The symbolic solver with ground-truth (GT) concepts achieves 100% solved rate (upper-bound). GPT-4-based revision improves some low-data settings but is worse than human revision overall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Concept Binder', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4791.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4791.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLOTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Language of Thought Model (NLOTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural 'language of thought' style model used in the paper as a baseline for encoding expressivity; NLOTM builds on SysBinder principles and uses a semantic vector-quantized VAE and an autoregressive LoT prior to learn discrete semantic decompositions of scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural language of thought models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLOTM (Neural Language of Thought Model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as 'the recent Neural Language of Thought Model (NLOTM) [80]'; per the paper NLOTM combines a Semantic Vector-Quantized (SVQ) VAE with an Autoregressive LoT Prior (ALP) to produce hierarchical, composable discrete factors. The paper does not supply architecture sizes or parameter counts for NLOTM (it is used as a published baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR / CLEVR-Easy property classification (baseline for concept encoding expressivity); NOT used to solve CLEVR-Sudoku in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>NLOTM was included as a baseline in object-property classification experiments on CLEVR and CLEVR-Easy datasets to evaluate how informative discrete/continuous encodings are for downstream classifiers; these are visual-perceptual classification tasks rather than Sudoku spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Used as an encoding baseline: NLOTM provides discrete scene decompositions which are then fed to simple classifiers (decision trees) to assess how well object properties can be predicted from learned encodings. It was not used in the symbolic Sudoku solver pipeline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence in this paper that NLOTM was applied to spatial puzzle solving. Its use here is limited to evaluating representation expressivity for object-attribute classification; the paper reports that NLOTM's classification performance was substantially lower than NCB and the continuous SysBinder baseline (see Tab.2), indicating less effective object-factor encodings for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as a baseline in object-property classification (Tab.2): NLOTM yields lower accuracies than NCB and SysBinder continuous encodings across data regimes (the paper reports mean ± std over seeds; exact numeric values are presented in Tab.2 of the paper). The key qualitative statement: NLOTM performs worse than the continuous SysBinder encodings and worse than NCB's discrete encodings in the classification benchmarks used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In the experiments reported, NLOTM's encodings were less informative for object-property classification compared to SysBinder's continuous encodings and NCB's discrete retrieval-based encodings, especially in low-data regimes. The paper does not apply NLOTM to the CLEVR-Sudoku downstream solver, so its behavior on combined perception+Sudoku tasks is not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly in Tab.2 against SysBinder (continuous and discrete variants) and NCB; NLOTM underperformed relative to NCB and SysBinder (continuous) on CLEVR/CLEVR-Easy object-property classification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Concept Binder', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Neural language of thought models <em>(Rating: 2)</em></li>
                <li>A neurovector-symbolic architecture for solving raven's progressive matrices <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4791",
    "paper_id": "paper-270521490",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A large instruction-following generative language model used in this paper as a revisory agent: it was prompted to inspect exemplar images and recommend merging or deletion of learned visual concept clusters to improve downstream symbolic Sudoku solving.",
            "citation_title": "GPT-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A large pretrained instruction-following transformer language model (GPT-4). The paper does not give architecture details or parameter counts; GPT-4 was invoked via prompt-based queries (prompts provided in Listing 1) to produce textual analyses of image exemplars and recommend concept-space edits.",
            "puzzle_name": "CLEVR-Sudoku (Sudoku CLEVR and Sudoku CLEVR-Easy variants)",
            "puzzle_description": "A visual Sudoku variant where each Sudoku digit is represented by an image of a single CLEVR object; solving requires (1) mapping visual object attributes (shape, color, size, material) to digit labels from a small set of candidate example images, and (2) classic Sudoku constraint reasoning. The task therefore combines visual-perceptual mapping with spatial/logical puzzle solving (9x9 Sudoku constraint propagation/search).",
            "mechanism_or_strategy": "GPT-4 was used as a revisory agent: given exemplar images for learned concept clusters, GPT-4 was prompted to list object properties and to decide whether clusters (concepts) should be removed or merged in NCB's retrieval corpus. The paper describes specific prompt templates (Listing 1) where GPT-4 produces property lists and per-exemplar descriptions; those outputs are used to programmatically decide merges/deletions in the discrete concept memory (hard binder). GPT-4 did not directly solve Sudoku; instead its textual feedback changed the concept-symbol corpus which downstream symbolic classifiers + constraint solver used.",
            "evidence_of_spatial_reasoning": "No direct evidence that GPT-4 performed spatial reasoning on the Sudoku grid itself; its role was semantic/textual analysis of object exemplars. Indirect evidence of effect on spatial puzzle solving comes from changes in the percentage of solved CLEVR-Sudoku puzzles after applying GPT-4-driven revisions to the concept corpus (reported in Fig.5 / Tab.8). The paper also analyzes GPT-4's outputs qualitatively (noting inconsistent object descriptions) and provides statistics on numbers of deletions GPT-4 requested (Suppl. F.5 / Fig.12).",
            "performance_metrics": "Effect on CLEVR-Sudoku solved-rate (all-or-nothing puzzle success, reported as % solved ± std):\n- Sudoku CLEVR-Easy (example counts per digit varied): NCB revised (GPT-4) reported solved percentages in the rough range shown in the paper (e.g., for some configs ~48.6% ± 33.9, 54.3% ± 30.9, 63.6% ± 25.5 for K variants in CLEVR-Easy as reported in Tab.8 / Fig.5). \n- Sudoku CLEVR (harder variant): NCB revised (GPT-4) achieved lower solved-rates than human revision; example numbers reported: ~17.8% ± 12.9, 21.8% ± 15.8, 29.1% ± 20.5 (depending on K and example-count regimes) as presented in Tab.8 / Fig.5. \n(Units: percent of puzzles fully solved; values reported as mean ± std over seeds/sets in the paper.)",
            "limitations_or_failure_cases": "The paper reports that GPT-4's revisions can be 'ill-informed' and inconsistent: GPT-4 sometimes removed or merged too many concepts due to inconsistent object descriptions, which decreased downstream Sudoku solve rates in higher-example settings. GPT-4 deletions had high variance across seeds (Suppl. F.5 / Fig.12). The authors note GPT-4 provided limited consistency in object descriptions and therefore can harm concept expressivity if used without human oversight.",
            "comparison_baseline": "Comparisons in the paper are between NCB with no revision, NCB revised by GPT-4, and NCB revised by simulated human feedback; NCB revised (human) substantially outperforms NCB revised (GPT-4) in many settings (esp. with more candidate examples). The symbolic solver with ground-truth (GT) concepts achieves 100% solved rate (upper-bound). GPT-4-based revision improves some low-data settings but is worse than human revision overall.",
            "uuid": "e4791.0",
            "source_info": {
                "paper_title": "Neural Concept Binder",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "NLOTM",
            "name_full": "Neural Language of Thought Model (NLOTM)",
            "brief_description": "A neural 'language of thought' style model used in the paper as a baseline for encoding expressivity; NLOTM builds on SysBinder principles and uses a semantic vector-quantized VAE and an autoregressive LoT prior to learn discrete semantic decompositions of scenes.",
            "citation_title": "Neural language of thought models",
            "mention_or_use": "use",
            "model_name": "NLOTM (Neural Language of Thought Model)",
            "model_description": "Described in the paper as 'the recent Neural Language of Thought Model (NLOTM) [80]'; per the paper NLOTM combines a Semantic Vector-Quantized (SVQ) VAE with an Autoregressive LoT Prior (ALP) to produce hierarchical, composable discrete factors. The paper does not supply architecture sizes or parameter counts for NLOTM (it is used as a published baseline).",
            "puzzle_name": "CLEVR / CLEVR-Easy property classification (baseline for concept encoding expressivity); NOT used to solve CLEVR-Sudoku in this paper",
            "puzzle_description": "NLOTM was included as a baseline in object-property classification experiments on CLEVR and CLEVR-Easy datasets to evaluate how informative discrete/continuous encodings are for downstream classifiers; these are visual-perceptual classification tasks rather than Sudoku spatial puzzles.",
            "mechanism_or_strategy": "Used as an encoding baseline: NLOTM provides discrete scene decompositions which are then fed to simple classifiers (decision trees) to assess how well object properties can be predicted from learned encodings. It was not used in the symbolic Sudoku solver pipeline in this paper.",
            "evidence_of_spatial_reasoning": "No evidence in this paper that NLOTM was applied to spatial puzzle solving. Its use here is limited to evaluating representation expressivity for object-attribute classification; the paper reports that NLOTM's classification performance was substantially lower than NCB and the continuous SysBinder baseline (see Tab.2), indicating less effective object-factor encodings for these tasks.",
            "performance_metrics": "Reported as a baseline in object-property classification (Tab.2): NLOTM yields lower accuracies than NCB and SysBinder continuous encodings across data regimes (the paper reports mean ± std over seeds; exact numeric values are presented in Tab.2 of the paper). The key qualitative statement: NLOTM performs worse than the continuous SysBinder encodings and worse than NCB's discrete encodings in the classification benchmarks used.",
            "limitations_or_failure_cases": "In the experiments reported, NLOTM's encodings were less informative for object-property classification compared to SysBinder's continuous encodings and NCB's discrete retrieval-based encodings, especially in low-data regimes. The paper does not apply NLOTM to the CLEVR-Sudoku downstream solver, so its behavior on combined perception+Sudoku tasks is not evaluated here.",
            "comparison_baseline": "Compared directly in Tab.2 against SysBinder (continuous and discrete variants) and NCB; NLOTM underperformed relative to NCB and SysBinder (continuous) on CLEVR/CLEVR-Easy object-property classification.",
            "uuid": "e4791.1",
            "source_info": {
                "paper_title": "Neural Concept Binder",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Neural language of thought models",
            "rating": 2,
            "sanitized_title": "neural_language_of_thought_models"
        },
        {
            "paper_title": "A neurovector-symbolic architecture for solving raven's progressive matrices",
            "rating": 2,
            "sanitized_title": "a_neurovectorsymbolic_architecture_for_solving_ravens_progressive_matrices"
        }
    ],
    "cost": 0.015990749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Neural Concept Binder
24 Oct 2024</p>
<p>Wolfgang Stammer <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#111;&#108;&#102;&#103;&#97;&#110;&#103;&#46;&#115;&#116;&#97;&#109;&#109;&#101;&#114;&#64;&#99;&#115;&#46;&#116;&#117;&#45;&#100;&#97;&#114;&#109;&#115;&#116;&#97;&#100;&#116;&#46;&#100;&#101;">&#119;&#111;&#108;&#102;&#103;&#97;&#110;&#103;&#46;&#115;&#116;&#97;&#109;&#109;&#101;&#114;&#64;&#99;&#115;&#46;&#116;&#117;&#45;&#100;&#97;&#114;&#109;&#115;&#116;&#97;&#100;&#116;&#46;&#100;&#101;</a> 
Computer Science Department
TUDarmstadt</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>Antonia Wüst 
Computer Science Department
TUDarmstadt</p>
<p>David Steinmann 
Computer Science Department
TUDarmstadt</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>Kristian Kersting 
Computer Science Department
TUDarmstadt</p>
<p>Hessian Center for AI (hessian.AI)</p>
<p>German Research Center for AI (DFKI)</p>
<p>Centre for Cognitive Science
DarmstadtTU</p>
<p>Neural Concept Binder
24 Oct 2024761D052A79D7B546C1A5C34DEDA41BADarXiv:2406.09949v2[cs.AI]Learned concepts X XConcept learner (NCB) Revisable concepts Inspectable concepts
The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct.Achieving this in an unsupervised manner requires human users to understand the model's learned concepts and, if necessary, revise incorrect ones.To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as "concept-slot encodings".NCB employs two types of binding: "soft binding", which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent "hard binding", achieved through hierarchical clustering and retrieval-based inference.This enables obtaining expressive, discrete representations from unlabeled images.Moreover, the structured nature of NCB's concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4.Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks.We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset.Code and data at: project page.</p>
<p>Introduction</p>
<p>An essential aspect of visual reasoning is obtaining a proper conceptual understanding of the world by learning visual concepts and processing these into a suitable representation (cf.Fig. 1).The majority of current machine learning (ML) approaches that focus on visual concept-based processing utilize forms of supervised [34,68,32,84], weakly-supervised [41,69,49,63,8,82] or text-guided [29] learning of concepts.These approaches all require some form of additional (prior) knowledge about the relevant domain.An attractive alternative, though much more challenging, is to learn concepts in an unsupervised fashion.This comes with several challenges: (i) learning an expressive concept representation without concept supervision is intrinsically difficult [40], and (ii) there is no guarantee that learned concepts align with general domain knowledge [36,85,9] and (iii) can therefore be utilized for complex downstream tasks.Moreover, (iv) to trust that the learned concept representations are reliable for high stakes scenarios [15], it is necessary to make the model's concept representations human-inspectable and -revisable [68,69,31] (cf.Fig. 1 (left)).</p>
<p>These challenges raise questions about the nature of the unsupervised learned concept representations.Continuous encodings [60,59,77,79] are easier to learn and more expressive.However, they are difficult to interpret and suffer from problems related to poor generalization [81] and information leakage [47,50].On the other hand, discrete encodings [69,79,26,4] are hard to learn [44,72,9], but are easier to understand and thus align, e.g., to a task at hand.This work proposes the Neural Concept Binder (NCB) framework to learn expressive, yet inspectable and revisable, concepts from unlabeled data.NCB combines continuous encodings, obtained via block-slot-based soft-binding, with discrete concept representations, derived through retrieval-based hard-binding.NCB's soft binding leverages the object-factor disentanglement capabilities of the recent SysBinder mechanism [65].Subsequently, NCB's hard binding mechanism utilizes HDB-SCAN [12,13] to cluster the continuous block-slot encodings, distilling a structured corpus of discrete concepts from these clusters.This corpus enables the retrieval of discrete concept representations during inference by matching the continuous encoding with the closest entries in the corpus.Thus, to address the challenges of unsupervised concept learning, NCB integrates the strengths of both continuous and discrete concept representations.Moreover, NCB enables straightforward concept inspection and facilitates easy revision procedures, allowing alignment of the learned concepts with prior knowledge.In our evaluations, we demonstrate that NCB's discrete concept-slot encodings retain the expressiveness of their continuous counterparts.Moreover, they can be seamlessly integrated into downstream applications via symbolic and interpretable neural computations (cf.Fig. 1</p>
<p>(right)).</p>
<p>In this context, we introduce our novel CLEVR-Sudoku dataset, which presents a challenging visual puzzle that requires both perception and reasoning capabilities (cf.Fig. 4).</p>
<p>In summary, our contributions are the following: (i) we introduce the Neural Concept Binder framework (NCB) for unsupervised concept learning, (ii) we show the possibilities to integrate NCB with symbolic and subsymbolic modules in challenging downstream tasks, achieving performance on par with supervised trained models, (iii) we highlight the possibilities of easy concept inspection and revision via NCB, and (iv) we introduce the novel CLEVR-Sudoku dataset, which combines challenging visual perception and symbolic reasoning.</p>
<p>Related Work</p>
<p>Unsupervised visual concept learning focuses on obtaining concept-level representations from unlabeled images [25].Some works have tackled this only for specific domains, such as extracting "teachable" concepts for chess [62] or learning manipulation concepts from videos of task demonstrations [39].Others rely on object-level concept guidance through initial image segmentations [27] or "natural supervision" [49].In contrast, Vedantam et al. [77] and Wüst et al. [81] focus on learning higher-level relational concepts, i.e., assuming that basic-level concepts have already been provided.Several approaches learn concepts from the training signal of an image classification task [78,1,14,38], often focusing on image-region-based concepts [22].More recently, several works have explored leveraging the knowledge stored in large pretrained models, such as combining large language models with CLIP embeddings [82,52] or using weakly-supervised queries to a vision-language model [8].These approaches still rely on some form of supervision, whether through text, class labels, or prompts.In contrast, this work focuses on learning unsupervised concepts at both the object and factor levels, ensuring that these concepts remain inherently inspectable and revisable.</p>
<p>The motivation for inherently inspectable and revisable concept representations is to allow human stakeholders to investigate and potentially revise a model's internal concepts.Most research in this area focuses on post-hoc approaches that distill concept knowledge from pretrained models [83, 21, 57,   The structured retrieval corpus (distilled from the block-slot encodings) allows for easy concept inspection and revision by human stakeholders.Moreover, the resulting concept-slot encodings can be easily integrated into complex downstream tasks.18,23].In contrast, Lage and Doshi-Velez [35] explore learning inspectable concept representations through human feedback, focusing on tabular data and higher-level concepts.Similarly, Stammer et al. [69] develop inherently inspectable visual concepts using weak supervision and a prototype-based binding mechanism.However, no existing work addresses the development of inherently inspectable and revisable concept representations in the context of unsupervised visual learning.</p>
<p>Neural Concept Binder (NCB)
/
The properties of discrete vs. continuous encodings are a vibrant research topic that is highly relevant to learning suitable concept representations.Continuous encodings allow for easier and more flexible optimization and information binding [43,64,65,7].However, discrete representations are considered essential for understanding AI models [31], mitigating shortcut learning [68,3], and solving complex visual reasoning tasks [26,66].Despite their advantages, learning discrete representations through neural modules remains a challenging problem [44,24,20,74].While some works have focused on categorical-distribution-based discretization [4,28,46], others have explored retrieval-based discretization of continuous encodings using various forms of inherent "codebooks" [73,71].Only a few studies have explicitly addressed how to bind semantic visual information to specific discrete representations [69].Whereas previous works typically emphasize one of the two representation types, we see great potential in the recent trend of explicitly integrating both discrete and continuous representations [17,32,84,51].</p>
<p>Neural Concept Binder (NCB): Extracting Hard from Soft Concepts</p>
<p>In this work, we refer to a concept as "the label of a set of things that have something in common" [2].This definition can be applied on different scales of a visual scene: on an image level (e.g., an image of a park), an object level (e.g., a tree vs. a bird) or an object-factor level (e.g., the color of a bird).Our proposed Neural Concept Binder (NCB) framework tackles the challenge of learning inspectable and revisable object-factor level concepts from unlabeled images by combining two key elements: (i) continuous representations via SysBinder's block-slot-attention [65,43] with (ii) discrete representations via retrieval-based inference.Fig. 2 provides an overview of NCB's inference, training, concept inspection, and revision processes.Let us formally introduce these processes.</p>
<p>Overall, we consider a set of unlabeled images X
:= (x 1 , • • • , x N ) ∈ R N ×D with x i ∈ R D , N ∈ N
and D ∈ N (for simplicity, we drop the image index notation in the following).Briefly, given an image, x, NCB infers latent block-slot encodings, z, and performs a retrieval-based discretization step on z to infer concept-slot encodings, c.These express the concepts of the objects in the image, i.e., object-factor level concepts.We begin by introducing the inference procedure of NCB.We hereby assume that NCB's components have already been trained and will introduce details of the training procedure subsequently.</p>
<p>Inferring Concept-Slot Representations</p>
<p>Obtaining Continuous Block-Slot-Encodings. Consider an image x ∈ X.The first component of NCB, the soft binder, is based on the systematic binding mechanism [65] and is represented by a block-slot encoder (cf.Fig. 2
(i)), g θ : x → z ∈ R N S ×N B ×D B ,
where g is parameterized by θ (for simplicity, this notation is omitted in the following).The soft binder transforms an input image into a latent, continuous block-slot representation, where N S represents the number of slots, N B the number of blocks per slot, and D B the dimension of each block.The soft binder employs two key types of binding mechanisms: spatial and factor binding.Spatial binding ensures spatial modularity across the entire scene and is achieved through slot attention [43], allowing each object in the image to be represented in a specific slot, z i .Factor binding, introduced by Singh et al. [65], ensures that different object factors (e.g., attributes like color) are encoded in separate blocks of a slot, i.e., z j i .These two binding mechanisms work together to perform object-and factor-based image processing.We refer to Suppl.A.1 for additional details on both systematic (factor) binding and slot attention.Overall, the resulting block-slot encodings represent continuous, object-centric representations of the input image, with objects encoded in slots and object factors encoded within the blocks of those slots.</p>
<p>Obtaining Discrete Concept-Slot-Encodings.The role of NCB's second processing component, the hard binder, is to transform the continuous block-slot encodings into expressive, yet discrete concept-slot encodings.Specifically, the hard binder is represented by a retrieval encoder, f (cf.Fig. 2 (v)), which processes the block-slot encodings, z, into a set of discrete concept-slot encodings, c.In detail, f defines a function f R : z → c ∈ N N S ×N B , parameterized by a retrieval corpus R (cf.Fig. 2 (iv)).This retrieval corpus consists of a tuple of sets
R := [R 1 , . . . , R N B ], where each set R j := {(enc j l , v l ) : l ∈ {1, ..., |R j |}} contains tuples of block encodings, enc j l ∈ R D B , and corresponding discrete values, v l ∈ {1, • • • , N C }. Importantly, enc j
l is a representative block encoding of a specific concept cluster, determined during NCB's training phase (cf.Fig. 2 (iii), detailed below).v l serves as the symbol identifier for the concept cluster associated with enc j l .Each block can contain up to N C ∈ N different concepts.To infer the concept symbol for a sample's block-slot encoding, NCB compares z j i with the encodings in the corresponding block's retrieval corpus, R j , and selects the most fitting concept.Specifically, given a distance metric d(•, •) and the block-slot encoding z j i , the selection function s R : z j i → l ∈ N (Fig. 2 (v)) finds the index l of the closest encoding in the retrieval corpus: s R (z j i ) = argmin l d(enc j l , z j i ) such that (enc j l , v l ) ∈ R j .This results in the concept representation for slot i and block j, denoted as c j i := v s R (z j i ) .For slot i, the full concept representation is denoted as c i := [c 1 i , . . ., c N B i ] and the final concept-slot encoding as c := f R (z) = [c 1 , . . ., c N S ].We refer to Suppl.A.2 for details on an alternative top-k selection function.We further note that NCB's flexibility, in principle, allows also to utilize the continuous encodings of its soft binder (Fig. 2 dashed arrow) in case a downstream task requires it.Let us now move on to NCB's training procedure.</p>
<p>Unsupervised Concept Learning via NCB</p>
<p>The training procedure of the Neural Concept Binder is separated into two subsequent steps where we provide an overview here and details in Suppl.A.3.We formally describe these steps using the pseudocode in Alg. 1.The first step consists of optimizing the encoder, g, to provide object-factorised blockslot encodings.It is optimized for unsupervised image reconstruction based on the decoder model,
g ′ θ ′ : z → x ∈ R D (Fig. 2 (ii)
) and utilizing a mean squared error loss:
L = L MSE (x, g ′ (g(x)
)).The goal of NCB's second training step is to obtain the retrieval corpus, R.This procedure is based on obtaining an optimal clustering of block encodings via an unsupervised clustering model, h, and distilling the resulting information from h into explicit representations in the retrieval corpus.For each block j a clustering model h ϕ j (Fig. 2 (iii)) is fit to identify a potentially overparameterised set of clusters within a set of block encodings (based on an unsupervised criterion, e.g., a density-based score [53]), resulting in N C ∈ N clusters.Next, for each cluster, v ∈ {1, • • • , N C }, representative block encodings, enc j , are extracted from h.Such an encoding represents either an averaged prototype or instance-based exemplar encoding.The corresponding tuples (enc j , v) are explicitly stored in the retrieval corpus R j (Fig. 2 (iv)) where we use the index l to identify specific encodings in R j , leading to R j := {(enc j l , v l ) : l ∈ {1, ..., |R j |}}.Thus, enc j l represents one block encoding of R j that has been assigned to cluster
v l . Finally, R = [R 1 , • • • , R N B ]
represents the final retrieval Figure 3: NCB's concept space is inherently inspectable.A human stakeholder can easily inspect the concept space by asking a diverse set of questions.For example, NCB answers interventional questions (iii) via generating images with selectively modified concepts.corpus, i.e., the set of corpora for each block.Through this training procedure, NCB learns to unsupervisedly categorize the object-factor information from the latent encoding space of the soft binder and stores this information in a structured, symbolic, and accessible way in the hard binder's retrieval corpus.We refer to the resulting clusters of each block as NCB's concepts and denote concepts with a capital letter for the block and a natural number for the category id, e.g., A3.We note that in practice, it is further possible to finetune the block-slot encoder, g, through supervision from the hard binder (cf.gray arrow in Fig. 2), e.g., once initial categories have been identified, and can be achieved via a standard supervised approach.Ultimately, this allows for dynamically finetuning NCB's concept representations.Let us now introduce how human stakeholders can inspect and revise NCB's learned concepts.</p>
<p>Inspecting and Revising NCB's Concepts</p>
<p>Inspection.NCB inherently enables: (i) implicit, (ii) comparative, (iii) interventional and (iv) similarity-based inspection (cf.Fig. 3).Where the first three aim at investigating NCB's explicit, symbolic concept space (stored in R), the last one aims at investigating its latent, continuous concept space (stored in θ).(i) Implicit inspection queries the model to provide a set of examples for a specific concept.Essentially, this answers the question "What are examples of this concept?".NCB answers this question in two ways: by providing samples from the retrieval corpus corresponding to exemplars of the concept or by identifying additional data samples belonging to the concept at hand.(ii) Comparative inspection, on the other hand, allows comparing two specifically different concepts, e.g., "Why does this object depict concept H5 and not concept H1?".NCB hereby provides examples for both concepts for the user to compare and potentially identify dissimilar properties.Ultimately, this form of inspection allows to answer questions of the form "Why not ...?" and represents a valuable tool for in-depth and targeted concept inspection.(iii) Interventional inspection allows to answer questions such as "What if this object would have concept H1?" To answer this question, NCB utilizes its decoder g ′ .Specifically, by swapping the block z j i of a data sample's block-slot encoding with that of a representative sample, (enc j l , v l ) ∈ R j , NCB can provide an interventional image reconstruction, from which the effect of the swapped concept can be observed.Ultimately, this form of inspection allows to answer important questions of the form "What if ...?".Finally, (iv) Similarity inspection allows inspecting NCB's continuous encoding space on a more global level (in comparison to the more symbolic, sample-based inspection above), e.g., "What are similar concepts to this concept?".Specifically, NCB's distance metric d directly provides information about the similarity between concepts in the continuous representation.Inspecting the block-slot encoding space thus allows to identify a suboptimal soft binding, e.g., when block encodings are similar according to g but not according to the human stakeholder.Overall, these inspection mechanisms allow a human stakeholder to ask a diverse set of questions concerning a model's learned concepts (cf.Fig. 15, Fig. 16 and Fig. 17 for additional examples of the inspection types).</p>
<p>Revise.Let us now describe how a human stakeholder can revise NCB's concept space.Below, we provide details on the three main actions for symbolic revision (i.e., revision on the representations in R): (i) merging, (ii) deleting, or (iii) adding information.These actions can be performed on a single encoding or on a concept level and essentially represent a form of "reorganization" of information Table 1: Comparison of different approaches for concept learning.Hereby, we differentiate based on the following categories: whether a method (1) is learned in an unsupervised fashion, (2) provides object-level concepts (i.e., can explicitly process multiple objects), (3) provides factor-level concepts (e.g., the color green), (4) provides continuous concept encodings, (5)
✗ ✗ ✓ ✗ ✓ ✓ ✓ NeSyCL [68] ✗ ✓ ✓ ✗ ✓ ✓ ✓ GlanceNets [50] ✗ ✗ ✓ ✓ ✓ ✓ ✓ VAE [33] ✓ ✗ ✓ ✓ ✗ ✗ ✗ VQ-VAE [75] ✓ ✗ ✗ ✓ ✓ ✗ ✗ SA [43] ✓ ✓ ✗ ✓ ✗ (✓) ✗ SysBinder [65] ✓ ✓ ✓ ✓ ✗ (✓) ✗ Neural Concept Binder ✓ ✓ ✓ ✓ ✓ ✓ ✓
stored in R. Furthermore, we provide details on how to (iv) revise the continuous latent space, which essentially requires finetuning of g's parameters.(i) Merge Concepts: In the case that R contains multiple concepts that, according to additional knowledge (e.g., from a human or other model), represent a joint underlying concept (e.g., two concepts for purple in Fig. 3 (right)) it is easy to update the model's internal representations by replacing the concept symbols of one concept with those of the second concept.Specifically, for block j if concept m should be merged with concept b where m, b ∈ {1, • • • , N C }, then for all corpus tuples, (enc j l , v l ) ∈ R j , we replace v l with b if v l = m.(ii) Delete Encodings or Concepts: If R j contains an encoding, enc j l , for a specific concept, m, that does not match the other encodings of that concept (e.g., a misplaced exemplar) this encoding can simply be deleted from the corpus.Accordingly, if an entire concept, m, is identified as suboptimal, one can simply delete all corresponding encodings of that concept.I.e., for all corpus tuples, (enc j l , v l ) ∈ R j , we remove the tuple if v l = m.(iii) Add Encodings or Concepts: If a specific concept is not sufficiently well captured via the existing encodings in R j , one can simply add a new encoding, ê nc j l+1 , for the concept, m, to the corpus.This leads to an additional entry in the corpus, ( ê nc j l+1 , m).Accordingly, it is also possible to add encodings for an entire concept.Hereby, one gathers block encodings of objects that represent that novel concept and adds these to the corpus as ( ê nc j l+1 , b) with b = N C + 1. (iv) Revise the (Continuous) Latent Space: Lastly, if the soft binder provides suboptimal object-and factor-level block-slot encodings, it is further possible to integrate revisory feedback on the soft binder's continuous latent space.This can be achieved via additional finetuning of the soft binder's parameters, θ, e.g., via standard forms of weak supervision [42,69] or interactive learning [68,61].</p>
<p>In summary, our novel Neural Concept Binder framework fulfills several important desiderata for concept learning (cf.Tab. 1).Specifically, NCB learns concepts in an unsupervised fashion that are structured on both an object and factor-level.Furthermore, next to standard continuous encodings, NCB also provides discrete concept representations, which are crucial for interpretability and integration into symbolic computations.Lastly, NCB's concept space is inspectable and revisable, essential for unsupervised learned concept representations.</p>
<p>Experimental Evaluations</p>
<p>In our evaluations, we investigate the potential of NCB's soft and hard binding mechanisms in unsupervised concept learning and its integration into downstream tasks.Notably, NCB encompasses concept processing between both of its components (soft binder and hard binder) whereby the direction "soft binder ← hard binder" (cf.Fig. 2) represents a standard approach (i.e., supervised learning of the soft binder's encoding space via symbolic concept labels, e.g., [34,68]).Therefore, we focus our evaluations on NCB's more novel processing direction, "soft binder → hard binder".</p>
<p>We aim to answer the following research questions: (Q1) Does NCB provide expressive and distinct encodings?(Q2) Can NCB be combined with symbolic methods to solve complex downstream tasks?(Q3) Can NCB's learned concepts be revised to improve suboptimal behaviour?(Q4) Can NCB be combined with subsymbolic methods to transparently solve complex downstream tasks?Data.We focus our evaluations on different variations of the popular CLEVR dataset.Specifically, we investigate (Q1 &amp; Q3) in the context of the CLEVR [30] and CLEVR-Easy [65] datasets.For investigating the integration of NCB into symbolic modules (Q2), we utilize our novel CLEVR-Sudoku puzzles introduced in the following.Finally, to evaluate the integration of NCB into subsymbolic modules (Q4), we evaluate on confounded and non-confounded variants of the CLEVR-Hans3 dataset [68].We provide further details on these datasets in the supplements (cf.Suppl.C).</p>
<p>CLEVR-Sudoku.To investigate the potential of integrating NCB's discrete concept representations into symbolic downstream tasks, we introduce the novel CLEVR-Sudoku dataset.This dataset presents a challenging visual puzzle that requires both visual object perception and reasoning capabilities.Each sample in the dataset (cf.Fig. 4 for an example puzzle) consists of a Sudoku puzzle (partially filled) with CLEVR-based images [30] and additional example images depicting the mapping of relevant object properties to digits.Specifically, each digit in the Sudoku is replaced by an image of an object.All objects representing the same digit share a set of common properties, e.g., in Fig. 4, all objects replacing "1"s are yellow spheres.We introduce two variants of CLEVR-Sudoku: Sudoku CLEVR-Easy and Sudoku CLEVR.In the first variant, shape and color are distinguishing properties for the digits.In Sudoku CLEVR, additional object attributessize and material -are relevant for the digit identification.Moreover, up to 10 example images are provided per digit mapping; the fewer examples provided, the more difficult it becomes to learn the mapping.The initial state and digit-attribute mappings vary across samples.One specific intricacy of CLEVR-Sudoku is that the puzzle can only be solved if all subcell images are correctly mapped to their corresponding digits.Even a single mistake can render the Sudoku unsolvable.Thus, compared to standard Sudoku puzzles, which primarily require deductive reasoning, solving CLEVR-Sudoku also demands complex object recognition and the ability to map visual concept perceptions to the task concepts (i.e., the 9 digits of Sudoku).For further details, we refer to Suppl.B. Models.For our evaluations, we instantiate Neural Concept Binder based on the SysBinder model [65] for the soft binder encoder, g, and HDBSCAN [12,13] for the clustering model, h.Further details about the instantiation can be found in Suppl.A.4.In the context of Q1, we compare NCB's results to four variations of the SysBinder model [65], as well as the recent Neural Language of Thought Model (NLOTM) [80].We refer to the original SysBinder configuration as SysBinder (cont.), which provides continuous block-slot encodings.In SysBinder, SysBinder's continuous encodings are discretized at inference time via an argmin operation over its internal codebooks.SysBinder (hard) is trained from the beginning to produce discrete encodings using a low codebook softmax temperature.SysBinder (step) is trained with a step-wise decrease in temperature (cf.Suppl.D for details).For evaluations on CLEVR-Sudoku (Q2 and Q3), we first infer NCB's discrete concept-slot encodings from the puzzle's candidate examples.These encodings, along with their corresponding digit labels, are then passed to a symbolic classifier, which is trained to predict digits from the encodings.The classifier subsequently infers the digits for each subcell in the puzzle's initial state.These predictions are used by a constraint propagation and search-based algorithm [55,10] to solve the puzzle (cf.Suppl.E.2 for details).We refer to the combination of the symbolic classifier and constraint solver as the solver.We compare the solver's performance when provided with ground-truth (GT) object-property labels (GT concepts), encodings from a supervised slot attention encoder [43] (SA (supervised)), and the discrete encodings from SysBinder (denoted as SysBinder (unsupervised)).For classification evaluations (Q4), we evaluate a configuration in which a set transformer classifier [37] is provided with NCB's concept encodings (NCB + NN) to make final class predictions (cf.Suppl.E.4).We compare this to SA + NN, where a supervised slot attention encoder [43] provides object-property predictions.</p>
<p>Metrics.We evaluate all models based on their accuracies on held-out test splits, each with 3 seeded runs.We provide average accuracies and standard deviations over these.When assessing the expressiveness of NCB's concept-slot encodings (Q1), we evaluate the accuracy for object-property prediction.When evaluating the performance of the downstream tasks, we provide the percentage of solved CLEVR-Sudokus (Q2) and the classification accuracy on the test set of CLEVR-Hans3 (Q4).</p>
<p>Evaluations</p>
<p>Discrete, yet expressive representations (Q1).First, we investigate how much valuable information NCB's discrete concept-slot encodings contain, despite NCB's inherent information bottleneck.To assess this, we train a classifier on NCB's encodings to predict corresponding object-property labels, e.g., the color green (cf.Suppl.E.1 for details).In Tab. 2, we present the results for the CLEVR-Easy and CLEVR datasets, using classification training sets with 2000, 200, 50, or 20 encodings.Focusing first on the results for N = 2000, we observe that, as expected, the continuous representation of the original SysBinder model contains more information compared to all discrete encodings.Remarkably, however, NCB's discrete concept representations are nearly on par with the continuous encodings.This is particularly notable given NCB's immense information bottleneck 1 .Additionally, we observe that NCB's encodings significantly outperform all other forms of discrete representations.Shifting focus to the results when the classifier is trained on data subsets, we observe a substantial degradation in performance when using encodings from any of the discrete baselines or the continuous encodings.</p>
<p>In stark contrast, when classifying based on NCB's encodings, the accuracy remains nearly constant, even with just 1% of the initial training samples.We provide additional ablations on the effect of concept encoding types and NCB's selection function in Suppl.F.1, as well as an ablation analysis on the effect of suboptimal behavior from NCB's individual components in Suppl.F.2.Further analysis of NCB's concept space can be found in Suppl.F.3, along with qualitative examples of learned concepts in Suppl.G. Overall, our results demonstrate the expressiveness of NCB's concept encodings despite their significant information bottleneck.Furthermore, our results suggest that NCB's encodings are easier to generalize compared to the baselines.Thus, we answer Q1 affirmatively.</p>
<p>Utilizing unsupervised concepts for solving visual Sudoku (Q2).In our following evaluations, we investigate the potential of NCB's representations for solving complex reasoning tasks through their integration into symbolic computations.These evaluations are based on our novel CLEVR-Sudoku dataset.The percentage of solved puzzles for CLEVR-Sudoku is reported in Fig. 5.It is important to note that the solver can only solve a puzzle if each image in the initial state has been classified correctly, meaning the results in Fig. 5 represent "all-or-nothing" outcomes.Focusing on the results to the left of the dashed lines, we observe that the symbolic module solves every puzzle with ground-truth (GT) concepts, even when only one example image is provided.Interestingly, performance drops significantly when using encodings from SA (supervised).This highlights the difficulty of the CLEVR-Sudoku puzzles: minor errors in digit prediction can lead to major failures in solving the puzzle.When comparing the performance of encodings from the two unsupervised models, we observe that NCB's concept encodings perform quite well.E.g., they enable solving approximately 50% of the puzzles for the 10-example configurations, compared to approximately 61% for SA (supervised).In contrast, when using SysBinder's encodings, the solver fails across all Sudoku variations.This demonstrates the effectiveness of NCB's binding mechanisms over those of the SysBinder approach alone.We refer to Suppl.F.4 for further discussions and quantitative digit classification results (Fig. 10).Overall, our evaluations highlight the potential of NCB's unsupervised concept encodings for solving complex symbolic downstream tasks.We therefore answer Q2 affirmatively.
∧ H5 ∧ K5 ∧ O13 ∧ P6 (Gray1) ∧ (Red ∨ Gray2) ∧ (Large) ∧ (Gray3) ∧ (Gray4) "A large gray object" Small, metal cube B4 ∧ D4 ∧ H1 ∧ I1 ∧ K1 (Cube) ∧ (Small1) ∧ (Small2) ∧ (Small3) ∧ (Small4) "A small cube" Large, blue sphere B1 ∧ C7 ∧ H4 ∧ O1 ∧ P2 (Sphere) ∧ (Blue1) ∧ (Blue2) ∧ (Small ∨ Blue3) ∧ (Blue4 ∨ Green ∨ Purple)
"A blue sphere"</p>
<p>Easily revising NCB's concepts (Q3).In our next evaluations, we illustrate the potential of NCB's revision procedures.Since revising the continuous latent space of NCB's soft binder is analogous to existing approaches (e.g., [61,58,69]), we focus on the novel, NCB-specific forms of symbolic revision, i.e., revisions within the hard binder's concept space.We demonstrate two forms of symbolic revision (removing and merging concept information) using feedback from two sources: a pretrained vision-language model (here via GPT-4 [56]) and simulated human feedback.In both cases, we ask the revisory agent to identify which concepts in each block should be removed or merged based on exemplar images of each concept, i.e., implicit concept inspection (cf.Suppl.E.3 for details).In Fig. 5, we show CLEVR-Sudoku performance when NCB's retrieval corpus is updated by different revisory agents (i.e., NCB revised (GPT-4) and NCB revised (human)).Interestingly, while GPT-4's revisions improve performance in settings with few examples, they have a negative impact when more digit examples are present.This is due to GPT-4's suboptimal consistency in object descriptions, leading to the removal or merging of too much concept information.This highlights the potential issue of "ill-informed" feedback (cf.Suppl.F.5).In contrast, human revisions provide a substantial boost in Sudoku performance, particularly in puzzle configurations with fewer candidate examples.Moreover, using NCB's similarity inspection mechanism (cf.Sec.3.3), a human stakeholder can easily identify models that suffer from suboptimal soft binding processing.In such cases, these models can be excluded from further downstream evaluations (cf.NCB revised (human)*) and refined by finetuning g's parameters (e.g., via approaches from [61,58,69]).In Suppl.F.6, we further explore concept revision by adding new information.Overall, our results demonstrate the potential and ease of revising NCB's concept space, allowing us to answer Q3 positively.</p>
<p>Utilizing unsupervised concepts for understanding neural computations (Q4).In our final evaluations, we investigate whether NCB's discrete concept encodings can make subsymbolic compu-tations more transparent.We focus on the task of image classification using concept-bottleneck-like approaches [68,34] on variations of the benchmark CLEVR-Hans3 dataset [68].While the concept encodings in NCB + NN are trained unsupervised, they perform on par with the supervised approach of [68] (cf.Suppl.F.7).More importantly, integrating NCB's inherently inspectable concept representations into neural computations leads to more transparent decision processes.We illustrate this in Tab. 3, where we provide classlevel explanations of the classifier in NCB + NN (cf.Suppl.E.4 for details).Using NCB's inspection mechanisms, human stakeholders can easily identify the classifier's internal decision rules for a class (e.g., "a large gray object").This is a critical feature for deploying trustworthy AI models in real-world scenarios.The key result is that this transparency is achieved even with unsupervised concept encodings.In Fig. 6, we further investigate whether a NCBbased neural classifier can be revised to mitigate confounders in the CLEVR-Hans3 dataset (cf.Suppl.E.4 and Suppl.F.8 for details).The confounding factor in the training set is the color gray, and we present the non-confounded test set accuracy in Fig. 6.We observe that standard loss-based feedback via explanatory interactive learning (XIL) [68] on the NN classifier's explanations (+ XIL on NN) significantly reduces the effect of the confounder.Alternatively, by simply zeroing the activations of the undesired concept gray (+ XIL on concepts), we achieve even better confounding mitigation results without the typical issues of joint optimization.</p>
<p>Our results highlight the potential of integrating NCB's unsupervised concept representations for eliciting transparent and trustworthy subsymbolic computations.We thus answer Q4 affirmatively.</p>
<p>Limitations.NCB largely benefits from high-quality initial block-slot encodings.If these encodings are suboptimal, the resulting concept-slot encodings also degrade in quality.An important next step to handle more complex visual inputs, such as video data, is the integration of recent approaches (e.g., [16,19]).Additionally, due to NCB's unsupervised training nature, further alignment of NCB's concepts is inevitable for effective deployment in downstream tasks [9].Further, to build trust in NCB's concept knowledge, human inspection is essential.Lastly, revisions are a critical aspect of NCB.However, they rely on humans to provide accurate feedback; a malicious user could manipulate NCB's concepts.Fortunately, by inspecting the concept space, it is possible to track and mitigate such manipulation effectively.</p>
<p>Conclusions</p>
<p>In this work, we introduce the Neural Concept Binder framework for learning visual object-factor concepts in an unsupervised manner.Our evaluations suggest that NCB's specific binding mechanisms facilitate the learning of expressive yet discrete concept representations.Furthermore, our results highlight the potential of integrating NCB's inherently inspectable and revisable concept-slot encodings into both symbolic and neural modules.Promising directions for future research include exploring the benefits of NCB's representations in continual learning settings [11], high-level concept learning [81], and probabilistic logic programming approaches [66,67], as well as investigating connections to object-centric causal representation learning [48].Lastly, incorporating downstream learning signals may be valuable (if present) for improving the quality of NCB's initial concept encodings, e.g., through classification [5,6] or differentiable clustering [76].</p>
<p>Supplementary Materials</p>
<p>In the following, we provide details on Neural Concept Binder, experimental evaluations as well as additional evaluations.</p>
<p>Impact Statement</p>
<p>Our work provides a new framework for unsupervised concept learning for visual reasoning.It improves the reliability of the unsupervised concept learning by explicitly including both inspection and revision of the concept space in the framework.NCB thus makes an important step towards more reliable and transparent AI, by providing an interpretable symbolic concept representation.This representation can be utilized within reliable and proven symbolic methods, or to improve transparency of neural modules.However, as the concepts are learned unsupervised, one has to keep in mind that they are not necessarily aligned with human knowledge, and might require inspections to achieve this.As NCB features a concept revision via human feedback, it is also necessary to consider that these revisions could have negative effects.A user with malicious intents could modify the memory and thus make the concept space incorrect.The fact that the learned representation of NCB is explicitly inspectable can, however, prove to be helpful in limiting such malicious interventions.</p>
<p>A Details on Neural Concept Binder</p>
<p>A.1 Details on Systematic Binding and Slot Attention</p>
<p>The binding mechanism (SysBinder) of Singh et al. [65] allows images to be encoded into continuous block-slot representations and relies on the recently introduced slot attention mechanism [43].In slot attention, so-called slots, s ∈ R N S ×N B D B (each slot has dimension N B D B ), compete for attending to parts of the input via a softmax-based attention.These slot encodings are iteratively updated and allow to capture distinct objects or image components.The result is an attention matrix A ∈ R N S ×D for an input x ∈ R D .Each entry A i corresponds to the attention weight of slot i for the input x.</p>
<p>Based on the attention matrix, the input is processed to read-out each object by multiplying A with the input resulting in a matrix U ∈ R N S ×N B D B .</p>
<p>SysBinder now performs an additional factor binding on the vectors u i of U .The goal of this factor binding mechanism is to find a distribution over a codebook memory for each block in u i , i.e., u j i .This codebook memory (one for each block), M j ∈ R K×D B , consists of a set of K learnable codebook vectors.Specifically, for each block j an RNN consisting of a GRU and MLP component iteratively updates the j-th block of slot s i , s j i , based on u j i and previous s j i .Finally, a soft information bottleneck is applied where each block s j i performs dot-product attention over the codebook memory leading to the final block-slot representation:
s j i = softmax K s j i • (M j ) T √ D B • M j
This process is iteratively refined together with the refinement processes of slot attention.Overall, the encodings of SysBinder represent each object in an image by a slot with N B blocks where each block represents a factor of the object like shape or color.</p>
<p>Note that in the main text, the final s j i is denoted as z j i .</p>
<p>A.2 Selection Function</p>
<p>In the default setting, NCB selects that encoding from the retrieval corpus with the minimal distance to infer a corresponding concept representation.We further explore a top-k approach for the selection function s with k &gt; 1.In this case, s selects the values v l , for the k ∈ N closest encodings in the retrieval corpus and the resulting c j i is obtained via majority vote over these values.Additionally, via this selection approach the probability of c j i based on the occurrence distribution over the top-k values v l can be estimated.We provide ablations regarding this in our evaluations in Suppl.F.1.</p>
<p>A.3 Details on Training</p>
<p>The first step (cf.L.1 in Alg. 1) optimizes the encoder g to provide object-factorised block-slot encodings.It is optimized for unsupervised image reconstruction based on the decoder model, 2) and a mean squared error loss: L = L MSE (x, g ′ (g(x))).In practice, additional losses have been shown to be beneficial for further improving the obtained block-slot encodings [65,64].
g ′ θ ′ : z → x ∈ R D (cf. Fig.
The goal of NCB's second training step is to obtain the retrieval corpus, R.This procedure is based on obtaining an optimal clustering of block encodings via an unsupervised clustering model h and distilling the resulting information from h into explicit representations in the retrieval corpus.This step is divided into several substeps (cf.L.2-6 in Alg. 1).It starts with gathering a set of block-slot encodings Z = g θ (X).As Z can include slots which do not encode objects but, e.g., the background, we first select the "object-slot" encodings from Z.This step results in Z ⊆ Z and consists of a heuristic selection based on the corresponding slot attention masks (described in the following section).</p>
<p>For each block j we next perform the following steps: (i) a clustering model, h ϕ j (cf.Fig. 2), is fit to find a set of clusters within Zj thereby identifying N C ∈ N meaningful clusters.The learning of this optimal clustering is based on an unsupervised criteria, e.g., density based scores [53].Ideally, this leads to that objects that share similar block encodings are clustered together in the corresponding latent block space, whereas objects that possess very different block encodings are associated with distant clusters.This resulting clustering is stored in h's internal representation which we denote as ϕ j (e.g., the merge tree in a hierarchical clustering method [12,13,54].Importantly, h ϕ j is optimized individually for each block.(ii) In the distill step representative block encodings of each cluster, enc j , are extracted from h's internal representation, ϕ j .Hereby, every enc j can represent either an averaged prototype or instance-based exemplar encoding of a cluster.This is performed for every identified cluster, v ∈ {1, • • • , N C } and is based on Zj and ϕ j .As a result, the tuples (enc j , v) are explicitly stored in the retrieval corpus R j .The final retrieval corpus consists of the set of individual corpora for each block,
R = [R 1 , • • • , R N B ].
We note that in practice, it is further possible to finetune the block-slot encoder, g, through supervision from the hard binder, e.g., once initial categories have been identified and can be achieved via a standard supervised approach.Ultimately, this allows for dynamically finetuning NCB's concept representations.</p>
<p>Heuristic object-slot selection.In the following we describe the process of identifying the slot which contains an object.This is based on heuristically selecting slot ids based on their corresponding slot attention values.Importantly, this approach can select object-slot ids without additional supervision, e.g., via (GT) object segmentation masks.</p>
<p>In principle, our object-slot selection approach finds the slots which contain slot attention values above a predefined threshold, δ ∈ (0, 1].However, selecting such a threshold can be cumbersome in practice.In our evaluations we therefore select only a single slot per image, i.e., that slot which contains the maximum slot attention value over all slots.Essentially, this sets the maximum number of selected slots per image to 1 and in images that contain one objects represent no loss of object relevant information.In preliminary evaluations we observed that the consensus between object-slot selection based on GT object segmentation masks (matching object segmentation masks with slot attention masks) and our maximum-based selection heuristic is 99.45% over 2000 single object images.</p>
<p>A.4 Instantiating Neural Concept Binder</p>
<p>We instantiate NCB's soft binder via the SysBinder approach of Singh et al. [65] which has been shown to provide valuable, object-factor disentangled representations.Thus, the soft binder was trained as in the original setup and with the published hyperparameters.Furthermore we instantiate the clustering model, h, via the powerful HDBSCAN method [12,13,54] (based on the popular HDBSCAN library2 ).Hereby, h's internal representation, ϕ, consists of the learned hierarchical merge tree.In practice we found it beneficial to perform a grid search over h's hyperparameters based on the unsupervised density-based cluster validity score [53].The searched parameters are the minimal cluster size (the minimum number of samples in a group for that group to be considered a cluster) and minimal sample number (the number of samples in a neighborhood for a point to be considered as a core point) each over the values [5,10,15,20,25,30,50,80,100].Moreover, we utilize the excess of mass algorithm and allow for single clusters.We performed the training of the retrieval corpus, i.e., fitting h, on a dataset of images containing single objects for simplifying the subsequent concept inspection mechanisms of our evaluations.However, this can easily be extended to multiple object images by utilising the soft binder's slot attention masks to identify relevant objects in an image.Finally, we instantiate the retrieval corpus as a set of dictionaries and, unless stated otherwise, we utilise a retrieval corpus which contains one prototype and a set of exemplar encodings per concept.Furthermore, s R represents the argmin selection function and we utilize the euclidean distance as d(•, •).It is important to note that h does not make any assumptions about the number of clusters, N C .Thus, although h fits a clustering to best fit the block-slot encodings of a block, it can potentially provide an overparameterized clustering, e.g. by representing one underlying factor such as "gray" with several clusters.This highlights the importance of task-alignment, e.g., for symbolic downstream tasks, and concept inspection for general concept alignment.We refer to our code for more details 3 , where trained model checkpoints and corresponding parameter logs are available.</p>
<p>A.5 Computational Resources</p>
<p>The resources used for training NCB were: CPU: AMD EPYC 7742 64-Core Processor, RAM: 2064 GB, GPU: NVIDIA A100-SXM4-40GB GPU with 40 GB of RAM.Hereby, training the SysBinder model [65] is the computational bottleneck of NCB where we utilised two GPUs per SysBinder run.Training for 500 epochs took ≈108 GPU hours.The fitting of h (including the grid search over hyperparameters) was performed on the CPU and finished within a few hours.CLEVR-Sudoku provides Sudokus based on the datasets CLEVR and CLEVR-Easy.Classic Sudokus have a 9x9 grid which is filled with digits from 1 to 9. In CLEVR-Sudoku these digits are replaced by images of objects.Hereby, a digit corresponds to a specific attribute combination, e.g., "yellow" and "sphere".Consequently, digits of the Sudoku are replaced by images of objects with these attribute combinations.These images each contain one object.To indicate, which attributes correspond to which digit, candidate examples of the digits are provided.The number of these examples is a flexible parameter, in our evaluations we used N ∈ {1, 3, 5, 10}.Further, the number of images provided in the Sudoku grid is flexible as well.In our main evaluations we only considered CLEVR-Sudokus with K = 30, meaning that 51 of the 81 Sudoku cells are filled and 30 are left to complete.For additional investigation we considered values for K ∈ {10, 50} as well.Examples of those Sudokus for Sudoku CLEVR are shown in Fig. 7.The dataset has a number of 1000 samples for Sudoku CLEVR-Easy and Sudoku CLEVR respectively for each value of K.Each sample has a different puzzle and a distinct set of images, no image is used twice for one puzzle 4 .</p>
<p>B Details on CLEVR-Sudoku</p>
<p>C Datasets</p>
<p>CLEVR.Briefly, a CLEVR [30] image contains multiple 3D geometric objects placed in an illuminated background scene.Hereby, the objects can possess one of three forms, one of 8 colors, one of two sizes, one of two materials and a random position within the scene.</p>
<p>CLEVR-Easy.CLEVR-Easy [65] images are similar to CLEVR images, except that in CLEVR-Easy the size and material is fixed over all objects, i.e., all objects are large and metallic.</p>
<p>CLEVR-Hans3.The CLEVR-Hans3 [68] represents a classification dataset that contains images with CLEVR objects where the image class is determined based on the attribute combination of several objects (e.g., an image belongs to class 1 if it contains a large, gray cube and a large cylinder).Furthermore, we utilize a confounded and non-confounded version of CLEVR-Hans3.In the confounded case (i.e., the original dataset) the train and validation set contains spurious correlations among object attributes (e.g., all large cubes are gray in class 1) that are not present in the test set (e.g., large cubes of class 1 take any color).In our evaluations investigating only neural-based classification we utilize the original validation split as the held-out test split and select a subset from the original training split as validation set.Thus, the non-confounded version corresponds to a standard classification setup in which the data distribution is identical over all three data splits.Lastly we provide evaluations on a single object version of CLEVR-Hans3 (class 1: a large, gray cube; class 2: a small metal cube; class 3: a large, blue sphere; cf.Tab. 3) and the original, multi-object version.</p>
<p>D Baseline Models</p>
<p>We note upfront, that all SysBind configurations below were trained for as many epochs as NCB, followed by an additional finetuning for 2 epochs on the same dataset that was used to distill NCB's retrieval corpus.</p>
<p>SysBind (cont.</p>
<p>).This denotes the original SysBinder configuration which was trained as in [65] and provides continuous block-slot encodings.We refer to the original work for hyperparameter details.</p>
<p>SysBind.This denotes a SysBinder configuration that was trained as in [65].However, at inference time we perform discretisation via an argmin operation over the attention values to each block's prototype codebook.</p>
<p>SysBind (hard).This denotes a configuration in which the SysBinder model was trained via a codebook attention softmax temperature of 1e − 4, resulting in a learned discrete representation.</p>
<p>SysBind (step).SysBinder (step) is trained by step-wise decreasing this temperature his denotes a configuration in which the SysBinder model was trained via a step-wise decreasing codebook attention softmax temperature (with a decrease by a factor of 0.5 every 50 epochs, starting from 1.).</p>
<p>NLOTM.NLOTM [80] builds on the principles of SysBinder and incorporates a Semantic Vector-Quantized (SVQ) Variational Autoencoder along with the Autoregressive LoT Prior (ALP).The SVQ component facilitates discrete semantic decomposition of a scene by learning hierarchical, composable factors that correspond closely to objects and their attributes in visual scenes.We refer to the original work for details.</p>
<p>Supervised Concept Learner.This corresponds to a slot attention encoder [43] that was trained for set prediction (i.e., in a supervised fashion) to predict the object-properties for every object in a CLEVR image.We refer to Locatello et al. [43] and Stammer et al. [68] for details.</p>
<p>E Details on Experimental Setup E.1 Classifying object-properties from concept encodings</p>
<p>For our evaluations in the context of (Q1) we utilise a decision tree as classification model that is trained on a set of concept encodings to predict corresponding object properties, e.g., sphere, cube or cylinder.Importantly, we train a separate classifier for each property category, e.g., the categories shape, color, material and size in the case of CLEVR, and average accuracies over these.The classifiers parameters correspond to the default parameters of the sklearn library 5 .</p>
<p>E.2 CLEVR-Sudoku evaluations</p>
<p>For our CLEVR-Sudoku evaluations we use a solver that combines a symbolic classifier with a constraint propagation based algorithm.To solve CLEVR-Sudokus, it is at first required to detect the underlying mapping from the object attribute combinations to the digits via the provided candidate examples.For this, we require a symbolic classifier to learn this mapping, which in the case of our evaluations is achieved via a decision tree classifier.For each evaluated model the concept encodings of the candidate example images of a CLEVR-Sudoku are retrieved and provided as input to the classifier.Hereby, the corresponding digits are the labels to be predicted.With the predictions of the trained classifier the concept encodings of the images in the Sudoku grid are classified to get a symbolic representation of the Sudoku, i.e., map the images in the cells to their corresponding digits.Based on this numerical representation of the puzzle, we use an algorithm from [55] that uses a combination of constraint propagation [10] and search.The algorithm keeps track of all possible values for each cell.Within each step, the Sudoku constraints are used to eliminate all invalid digits from the possibilities.Then the search of the algorithm select a digit for a non-filled cell.Based on this digit, the possibilities are updated for all other cells.When there is a constraint violation, the search-tree is traversed backwards and other possible digits for non-filled cells are explored.This process is repeated until the Sudoku is solved (in case the initial state inferred from the objects was correct) or until there is no possible solution left (meaning that the initial state was incorrectly inferred from the objects).The implementation of the algorithm is based on the code from 6 .Finally, to avoid errors due to random seeding of the classifier, for each puzzle we fit 10 independent classifiers (each with different seeds) to predict the corresponding mapping.For the results in our evaluations we average the performance over these 10 classifier seeds.</p>
<p>Lastly, the evaluations in the context of (Q2) are based on the trained (NCB) models of (Q1).</p>
<p>E.3 Obtaining Revisory Feedback</p>
<p>We note that the evaluations in the context of (Q3) are based on the trained NCB models of (Q1).</p>
<p>Revisory feedback for downstream Sudoku task.</p>
<p>To revise its discrete concepts, NCB offers the possibility to delete or merge clusters in the blocks.In the case of merging, the prototypes and exemplars of the clusters to be merged get aggregated so that they all map to the same concept symbol.For deletion there are several processing cases, depending on how many categories are in the block and how many are supposed to be deleted:</p>
<p>• Case 1: if all clusters from a block should be deleted (or if there is only one concept in the block, which should be deleted), we map all samples to the same concept.This results in the block containing no information (we keep the block to avoid issues with the dimensions of the concept representation).• Case 2: all clusters but one are to be deleted.In this case we still want to distinguish between the presumably "informative" cluster and the uninformative other clusters.Therefore we map all the blocks to be deleted to one cluster id instead of deleting them completely.• Case 3: at least two clusters should not be deleted.In this case, we completely remove the encodings of the to-be-removed clusters.The cluster id for these clusters no longer exists in the retrieval corpus.</p>
<p>Feedback via GPT-4.We systematically prompt GPT-4 [56] for receiving revisory feedback.We provide example prompts in Listing 1. First, we ask GPT-4 to name relevant object properties for a set of example images, e.g., "shapes: [cube, cylinder], color: [red, blue]".Based on these provided property lists we ask GPT-4 to provide a descriptive list of each exemplar object's image for each concept of each block, e.g., "{Exemplar1: [cube, red], Exemplar2: [cube, blue], ... }".Based on these descriptions we identify whether all exemplar objects of one concept share a common subproperty, e.g., "cube".If there is no common subproperty, the concept should be removed from the retrieval corpus.In a second step we evaluate whether all exemplar objects from two separate concepts share a common subproperty.In this case we decide to merge the concepts based on GPT-4's analysis.We finally integrate GPT-4's feedback into NCB's retrieval corpus via the procedures described above.</p>
<p>Feedback via simulated humans.To simulate feedback by a human user, we utilise a decision tree (DT) classifier to classify attributes of objects based on NCB's discrete concepts (similar to Q1).</p>
<p>For this, we transform the concept-slot encodings into multi-hot encodings.We then extract the importance of the concepts from the trained DT classifier.Based on this we select "unimportant" concepts to be deleted based on the procedures describe above.Note that in this setting we do not query for feedback considering the merging of concepts.</p>
<p>E.4 Neural Classification</p>
<p>We note that the evaluations in the context of (Q4) are based on the trained NCB models of (Q1).</p>
<p>Neural classifier.In the context of the classification evaluations (Q4) we utilize the setup of Stammer et al. [68].Specifically, a set transformer [37] is trained to classify images from the CLEVR-Hans3 dataset given encodings that are, in turn, obtained from either NCB or a supervised trained slot attention encoder [43] (SA).In the case of utilizing NCB's encodings we transform the concept-slot encodings into multi-hot encodings to match those of the SA-based setup.We refer to Stammer et al. [68] and our code for additional details concerning this setup.</p>
<p>Obtaining explanations from the neural classifier.We provide the explanations in Tab. 3 for the single object version of Fig. 14.To obtain these explanations for the neural classifier we utilize the approach of Stammer et al. [68] which is based on the integrated gradients explanation method [70].This estimates the importance value of each input element (in this case input concept encodings) for a classifiers final decision.We remove negative importance values and normalise the importance values as in [68].We then sum over the importance values corresponding to images of a class, normalise the values per block and binarize these aggregated and normalised importance values via the threshold of 0.25 (i.e., importance values above 0.25 are set to 1, otherwise 0).This provides us with a binary vector indicating which concepts are considered important per block.We illustrate these investigations via explanations from one model.</p>
<p>Explanatory interactive learning (XIL). Explanatory interactive learning (+ XIL on NN</p>
<p>) is used to mitigate the confounder in the CLEVR-Hans dataset.Hereby, (simulated) human feedback on the explanation of the neural classifier is used to retrain the classifier via the loss based approach of Stammer et al. [68].The feedback annotations mark which of NCB's concepts should not be used for the NN's classification decision.This is integrated into the NN by training the model to provide (integrated gradients-based) explanations that do not focus on these concepts.We refer to Stammer et al. [68] for details.The second form of interactive learning (+XIL on NCB concepts) is directly applied on the NCB's concept representation.Specifically, concepts from NCB that encode information concerning the irrelevant, confounding factors are simply set to zero, corresponding to not being inferred for the object in the image.E.g., if the NCB infers concepts concerning the color "gray" to be present in an object and the underlying confounder is the color "gray" the corresponding concept activations of the NCB's prediction are set to zero, i.e., no gray.Then the neural classifier is retrained on the new concept representations.Next to a better performance, the advantage of this approach is that it does not require the more costly loss-based XIL training loop.We illustrate these investigations via interactions on one model.</p>
<p>F Additional Quantitative Results</p>
<p>F.1 Encoding Expressivity</p>
<p>In our evaluations in Tab. 2 it appears that training for discrete encodings via SysBinder (hard) leads to no learning effect of the model altogether.In contrast training step-wise via SysBinder (step) provides better results, even slightly above the encodings of SysBinder (i.e., training for continuous representations and then discretising via argmin).Lastly, we observe that NCB's encodings lead to much lower performance variance compared to all baselines.Particularly SysBinder (step)'s high variances, hint towards issues with local optima.</p>
<p>We further provide ablations in the context of (Q1) on different component choices of NCB in Tab. 4. Specifically, we investigate the effect of a top-k selection function as well as the influence of using only prototype encodings in the retrieval corpus (NCB (P)) versus using prototype and exemplar encodings (NCB (P+E)).Unless noted otherwise, the NCB configurations in Tab. 4 utilize the argmin selection function.We note that when using prototypes, the average encoding of all elements in a cluster is formed, resulting in one prototype encoding per cluster in R j .In the second variant, we extend the prototypes with exemplars for each cluster.Exemplars are representative encodings for this cluster added to the corpus, resulting in a larger corpus, which potentially provides an improved structure of the encoding space.Indeed, we observe that NCB provides the best performances via the argmin selection function and utilizing both prototype and exemplar encodings.This was the setting used in all evaluations of the main paper.</p>
<p>F.2 Ablation Analysis of Suboptimal NCB Components</p>
<p>Lastly, in the context of (Q1) we further refer to ablations in Tab. 5 on the specific implementation choices of the NCB instantiation of our evaluations.We hereby investigate the effect of sub-optimal soft and hard binder components on a classifier's ability to identify object attributes from NCB's concept encodings.Specifically, we investigate (i) the effect when the soft binder, i.e., SysBinder encoder, was trained for fewer epochs, resulting in less disentangled continuous representations, and</p>
<p>(ii) when the HDBSCAN model of the hard binder was not optimized via a parameter grid search or replaced with a more rudimentary clustering model, i.e., a k-means clustering approach [45].</p>
<p>In the leftmost column of Tab. 5, we provide the performances of the NCB configuration of our main evaluations as a reference.As a reminder, hereby, NCB's soft binder was finetuned for 500 epochs, and its hard binder component contains a clustering model based on the HDBSCAN approach that was furthermore optimized via a grid search over its corresponding hyperparameters.Focusing on the next two columns right of the baseline, we observe that when the soft binder component is trained for fewer epochs than the baseline NCB we indeed observe a decrease in classification performance.Notably, however, we still observe higher performances in comparison to the discrete SysBinder configurations (cf.Suppl.F.1), but also when compared to SysBinder's continuous configuration (for N = 20).Focusing next on the rightmost column of Tab. 5 where NCB's clustering model was replaced with the more rudimentary k-means clustering approach, we observe a strong decrease in classifier performance.This is particularly true in the small data regime (N = 50 and N = 20).Surprisingly, focusing on the second to the rightmost column, we observe that when we select the default hyperparameter values of the HDBSCAN package (rather than performing a grid-search over these), the classifier reaches slightly improved performances than via the baseline NCB configuration (particularly for N = 20).Thus, in this particular case, the default values seem practical.However, this cannot be guaranteed in all future cases, and we still recommend performing a form of grid search if no prior knowledge can be provided upfront on an optimal parameter set.We postulate that the specific density-based cluster validity score used for selecting the optimal cluster parameters has been sub-optimal and leave investigating other, more optimal selection criteria for future work.</p>
<p>Overall, our ablation investigations indeed indicate that we obtain less expressive concept encodings via NCB with less powerful sub-components.However, we also observe a certain amount of robustness of our NCB instantiation towards sub-optimal components.</p>
<p>F.3 Analysis of Learned Concept Space</p>
<p>We here provide a brief analysis of NCB's learned concept space.These evaluations were performed on the models that were trained in the context of (Q1).Specifically, in Fig. 8, we provide the number of obtained concepts over all blocks (averaged over the 3 initialization seeds) both for CLEVR-Easy and CLEVR.We observe a much larger number of concepts overall for the CLEVR dataset but also a much larger variance in the number of concepts.This is largely due to that in CLEVR-Easy N B = 8 whereas in CLEVR N B = 16.Thus, the models are able to learn a more overparameterized concept space in the case of CLEVR.Further, in Fig. 9, we present the distribution of the number of concepts per block over all 3 NCB runs, both for CLEVR-Easy and for CLEVR.We observe that while most blocks contain maximally 20 concepts for CLEVR-Easy and 50 for CLEVR, there are several block outliers which contain a much greater set of concepts.</p>
<p>1.0 1.5 2.0 0 500 These represent cases in which the initial blockslot encoding space was uninformative to begin with and, therefore, difficult to find some form of useful clustering via h.Where some of these blocks only contained irrelevant information in general, some blocks encoded positional information, which represents a continuous variable to begin with and is thus unlikely to be well represented via a clustering.</p>
<h1>Concepts</h1>
<p>Number Concepts</p>
<p>CLEVR-Easy CLEVR</p>
<p>F.4 CLEVR-Sudoku Evaluations</p>
<p>In our evaluations on (Q2) we observe that, interestingly, for Sudoku CLEVR the supervised object classifier shows better results than for CLEVR-Easy.This seems counter-intuitive, however, in CLEVR-Easy-Sudoku digit labels are mapped to combinations of attributes that only stem from two categories, shape and color (in contrast to four categories in CLEVR-Sudoku) thus making it more likely to obtain recurring attributes over several digits (e.g., digits 3, 4 and 5 of Fig. 4 all depict green objects).Thus, if an error occurs in the digit classification due to errors concerning one attribute the effect of this error will have a larger effect.Moreover in the case of CLEVR-Easy, we observe that in comparison to the supervised model, whose property misprediction errors can lead to large issues in the downstream module, NCB's unsupervised and somewhat overparameterised concept space   appears to dampen this issue, thus leading to a higher number of solved puzzles, e.g., for 3, 5 or 10 examples.</p>
<p>In Fig. 10 we report the errors in predicting the underlying digits of the CLEVR-Sudokus.We observe that the errors of SysBinder (unsupervised) are drastically higher than the errors of the other methods.These high classification errors further explain this method's low performances, i.e., did not allow to solve any Sudoku.It can further be seen that for one example per digit the digit classification errors are much higher.This is reasonable as hereby the difficulty for the classifier is also higher.However, with an increasing number of examples the classifier's errors decrease.The relations between the errors in the digit prediction and the overall performance in CLEVR-Sudoku are similar which is sensible since the error is decisive for the number of solved puzzles.We further evaluate the influence of the number of missing images per Sudoku.For this we consider Sudokus with K ∈ {10, 30, 50}.The results on these variations with 5 candidate example images are reported in Fig. 11.We see that the more empty cells there are in a Sudoku's initial state (higher K), the more Sudokus are solved.This is due to the lower probability of misclassifying an image inside the Sudoku cells, as there are less images to classify.This pattern is observable for all of the different concept encodings we compared.</p>
<p>F.5 Revision Statistics</p>
<p>CLEVR-Easy CLEVR</p>
<p>Deletions during Revision</p>
<p>Revision by GPT-4 Revision by human We provide statistics of the number of resulting removal requests per agent in Fig. 12.For the revision of CLEVR-Easy concepts we can see that GPT-4 detects only a few concepts to delete while via simulated human revision more concepts get deleted.In our initial evaluations (cf.Fig. 4) we had observed that human revision leads to substantial improvements while GPT-4's revision even reduces performances slightly.For CLEVR-Sudoku in Fig. 12, we specifically observe that the overall number of deletions via GPT-4 is significantly higher.Interestingly, GPT-4 detects on average more blocks to delete here but also has a higher variance over the 3 different NCB runs.We hypothesize that this very "conservative" revision leads to the removal of concepts that actually contain valuable concept information, thus leading to less expressive concept encodings overall.Ultimately, this is due to mistakes in GPT-4's analysis of provided images (cf.Suppl.E.3).</p>
<p>F.6 Dynamically Discretising Continuous Factors via Symbolic Revision</p>
<p>In our second set of evaluations in the context of (Q3) we investigate the third form of symbolic revision as introduced in Sec.3.3: adding concept information to the hard binder's retrieval corpus.Hereby, we focus on the task of learning a novel concept that had only been stored implicitly in the soft binder's representations, but not explicitly in the hard binder's representations.Specifically, we focus on positional concepts of CLEVR objects where the underlying GT position is represented via continuous values.Overall, it is debatable whether one, in principle, should or even can represent such a continuous underlying feature via a discrete concept representation.In this set of evaluations we investigate a setting in which it is necessary to identify coarse categorisations of an object's position, e.g., whether the object is placed in the left or right half of an image.We hereby simulate a human stakeholder that, having identified the block j that generally encodes position information, revises the corresponding concept encodings.This revision is performed in two ways: (i) by iterating over all of the block's concepts and merging concepts into left and right concepts or (ii) by replacing all information in R with encodings from a selected set of positive example images for the two relevant positions.Fig. 13 presents the results of training a classifier to predict the attributes "left" and "right" from NCB's encodings (we here focus only on one seeded run for illustrations) with different types of revision.We observe that both allow to easily retrieve relevant information from NCB's newly revised concept space.These results illustrate the important ability to easily adapt the hard binder's concept representations by dynamically re-reading out the information of the soft binder's representations in a use-case based manner.The results further illustrate the effect of adding prior knowledge to NCB's concept representations, thereby potentially reducing the amount of inspection effort required on the stakeholder's side, e.g., in comparison to the merge revision.</p>
<p>F.7 Classifying CLEVR-Hans3</p>
<p>In our final evaluations (Q1) we highlight the advantage of NCB's concept encodings when combined with subsymbolic (i.e., neural) modules for making their decisions transparent.Specifically, while a discrete concept representation is technically not required for neural modules, it has a key advantage: a discrete and inspectable representation allows for transparent downstream computations.We highlight this property in the context of image classification on variations of the benchmark CLEVR-Hans3 dataset [68].For these evaluations we revert to training a set transformer [37] (denoted as NN in the following) for classifying images when provided the unsupervised concept encodings of NCB as image representations.We denote this configuration as NCB + NN and compare it to a configuration in which the set transformer is provided concept encodings from a supervised slot attention encoder, denoted as SA + NN.In Fig. 14 we obseve that NCB's concepts perform on par with those learned supervisedly, each reaching held-out test accuracies higher than 95%.</p>
<p>F.8 Confounding Evaluations</p>
<p>For the confounding mitigation evaluations in the context of (Q4) we train the NCB + NN configuration on the confounded version of CLEVR-Hans3, where we hereby focus on the single object class rules similar to those in Fig. 14.In this case all</p>
<p>G Qualitative Results</p>
<p>Fig. 15 further exemplifies the inspection types of Sec.3.3.Fig. 16 and Fig. 17 represent qualitative inspection results of NCB's learned concepts.We specifically present implicit inspection via exemplars of concepts from two blocks from NCB when trained on CLEVR-Easy.One can observe that block 2 (Fig. 16) appears to encode shape concepts, however contains one ambiguous concept.We further observe that Fig. 17 appears to encode color concepts, whereby it contains one ambiguous concept (concept 8) and two concepts that appear to both encode the color purple (concept 9 and 10) which could potentially be merged.We observe that block 2 appears to encode shape information (concept 1-3) and contains one ambiguous concept (concept 4).</p>
<p>Figure 17: Concepts of Block 8 for NCB with CLEVR-Easy.We here provide implicit inspection examples (i.e., via exemplars of each concept).We observe that block 8 appears to be encoding color information, contains one ambiguous concept (concept 8) and two concepts that appear to both encode the color purple (concept 9 and 10).Justification: Access to the code repository is provided.For the CLEVR-Sudoku dataset, the generation files are provided in the code and the full datafiles will be made public upon acceptance.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental Setting/Details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes] Justification: The experimental details are provided in the appendix and the provided code.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes]</p>
<p>Justification: The paper provides results over multiple seeds.In all experiments, average and standard deviation are reported.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.Justification: The relevant parts of the code of ethics are discussed in the impact statement and the remainder of the checklist.Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).10.Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: The paper contains an explicit impact statement, discussing potential societal impacts.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific clusters), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper does not involve pretrained models of any kind.The released dataset is not scraped from the internet and does not require safeguards.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes] Justification: The authors of existing models and datasets used within the paper are cited and their licenses respected.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes] Justification: The released code and the new dataset are both documented, including training and license information.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and Research with Human Subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper did not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper did not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>
<p>• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p>Figure 1 :
1
Figure 1: Unsupervised learning of concepts for visual reasoning.(left) Models that learn concepts from unlabeled data require inspectable and revisable concept representations.(right) Concepts obtained from the Neural Concept Binder (NCB) can be utilized both in (interpretable) neural and symbolic computations.</p>
<p>Figure 2 :
2
Figure 2: The Neural Concept Binder (NCB) combines continuous, block-slot encodings via slotattention based image processing with discrete, concept-slot encodings via retrieval-based inference.The structured retrieval corpus (distilled from the block-slot encodings) allows for easy concept inspection and revision by human stakeholders.Moreover, the resulting concept-slot encodings can be easily integrated into complex downstream tasks.</p>
<p>Figure 4 :
4
Figure 4: Example from CLEVR-Sudoku.Each digit is represented by CLEVR objects with the same attribute combination.The objective is to solve the Sudoku only based on the initial grid of CLEVR images and the digit mapping of candidate examples.</p>
<p>Figure 5 :
5
Figure 5: NCB's unsupervised concepts allow solving symbolic puzzles.Accuracy of solved Sudokus via different discrete concept encodings on Sudoku CLEVR-Easy and Sudoku CLEVR (left sides).Additional revision on NCB's concepts leads to improved performances (right sides).</p>
<p>Figure 6 :
6
Figure 6: NCB's unsupervised concept representations facilitate shortcut mitigation.Test accuracy for classification via NN predictor when trained on confounded images.</p>
<p>Algorithm 1
1
Training NCB: Given a set of images, X, a block-slot encoder, g θ , an unsupervised clustering model h ϕ .1: θ ← fit(g θ , X) ▷ Step 1: Optimize the block-slot encoder 2: Z ← g θ (X) ▷ Step 2.1: Gather block-slots from optimized g 3: Z ← select_object_slots(Z) ▷ Step 2.2: Filter out non-object slots 4: for j ∈ {1, • • • , N B } do 5: φj ← fit(h, Zj ) ▷ Step 2.3: Obtain clustering of Zj 6: R j ← distill( φj , Zj ) ▷ Step 2.4: Extract clustering representation into R j</p>
<p>Figure 7 :
7
Figure 7: Examples of Sudoku CLEVR for different K values.</p>
<p>Figure 8 :
8
Figure 8: Average number of concepts (over all blocks) in NCB's retreival corpus.</p>
<p>Figure 9 :
9
Figure 9: The distribution of number of obtained concepts per block both for CLEVR-Easy and CLEVR.These values are computed over all seeds.</p>
<p>Figure 10 :
10
Figure 10: Error ratios (%) of the digit classification in CLEVR-Sudoku based on different symbolic concept encodings.</p>
<p>Figure 11 :
11
Figure 11: Solved Sudokus (%) of Sudoku CLEVR-Easy and Sudoku CLEVR with different values for K (empty cells).</p>
<p>Figure 12 :
12
Figure 12: Average number of cluster deletions over all blocks via GPT-4 and simulated human user revision.</p>
<p>NCB w. add 5 exem.NCB w. add 20 exem.</p>
<p>Figure 13 :
13
Figure 13: Test accuracy (%) for classifying objects as placed left or right in a scene.</p>
<p>Figure 14 :
14
Figure14: Test accuracy (%) for classifying CLEVR-Hans3 images with a neural classifier that is provided concept representations of NCB and of a supervised trained slot attention encoder.We differentiate here between class rules based on one object and multiple objects.</p>
<p>Figure 15 :
15
Figure 15: Further examples of interventional inspection.By swapping the encoding of block 2 with different exemplar encodings from different concepts, the shape (which is encoded by block 2) is changed.When swapping the encoding with an exemplar of the same concept, the shape remains unchanged.</p>
<p>Figure 16 :
16
Figure 16: Concepts of Block 2 for NCB with CLEVR-Easy.We here provide implicit inspection examples (i.e., via exemplars of each concept).We observe that block 2 appears to encode shape information (concept 1-3) and contains one ambiguous concept (concept 4).</p>
<p>Listing 1 :
1
Prompts for GPT-4.−−−−−−−−−−−−−−−−−−−−−−−−−−−− P r o p e r t y L i s t Prompt : You a r e p r o v i d e d s i x i m a g e s .An image c o n t a i n s s u b i m a g e s .Each s u b i m a g e d e p i c t s one o b j e c t .Each o b j e c t r e p r e s e n t s a r e f l e c t i v e g e o m e t r i c s o l i d t h a t i s p l a c e d i n a n e u t r a l g r a y b a c k g r o u n d s c e n e w i t h a l i g h t s o u r c e .F u r t h e r m o r e , e a c h o b j e c t h a s m u l t i p l e p r o p e r t i e s , e .g ., c o l o r , s h a p e , s i z e , m a t e r i a l .Each p r o p e r t y c a n be s u b d i v i d e d i n t o s e v e r a l sub − p r o p e r t i e s , e .g ., brown i s a sub − p r o p e r t y o f t h e p r o p e r t y c o l o r .P l e a s e p r o v i d e a l i s t o f o b e c t p r o p e r t i e s and s u b p r o p e r t i e s t h a t a r e d e p i c t e d i n a l l i m a g e s .I g n o r e t h e b a c k g r o u n d and t h e o b j e c t ' s l u m i n a n c e and r e f l e c t i v i t y .Use t h e f o l l o w i n g a n s w e r t e m p l a t e : { p r o p e r t y : [ sub − p r o p e r t y , sub − p r o p e r t y , . . .] p r o p e r t y : [ sub − p r o p e r t y , sub − p r o p e r t y , . . .] . . .} −−−−−−−−−−−−−−−−−−−−−−−−−−−− D e s c r i p t i o n Prompt : You a r e p r o v i d e d an image .The image c o n t a i n s a t most 25 s u b i m a g e s .Each s u b i m a g e d e p i c t s one o b j e c t .Each o b j e c t r e p r e s e n t s a r e f l e c t i v e g e o m e t r i c s o l i d t h a t i s p l a c e d i n a n e u t r a l g r a y b a c k g r o u n d s c e n e w i t h a l i g h t s o u r c e .F u r t h e r m o r e , e a c h o b j e c t h a s m u l t i p l e p r o p e r t i e s , e .g ., c o l o r .Each p r o p e r t y c a n be s u b d i v i d e d i n t o s e v e r a l sub − p r o p e r t i e s , e .g ., g r e e n i s a sub − p r o p e r t y o f t h e p r o p e r t y c o l o r .The p o s s i b l e p r o p e r t i e s and sub − p r o p e r t i e s a r e t h e f o l l o w i n g : INSERT_PREVIOUSLY_OBTAINED_PROPERTY_LIST F o c u s i n g o n l y on t h e s e p r o p e r t i e s , p l e a s e p e r f o r m t h e f o l l o w i n g t a s k s .F i r s t , f o r e v e r y o b j e c t i n t h e image p l e a s e l i s t t h e sub − p r o p e r t i e s from t h e g i v e n l i s t s t h a t t h e o b j e c t d e p i c t s .Only name t h e sub − p r o p e r t i e s t h a t a r e g i v e n .P l e a s e u s e t h e f o l l o w i n g f o r m a t : { O b j e c t 1 : [ sub − p r o p e r t y , . . .] , O b j e c t 2 : [ sub − p r o p e r t y , . . .] , . . .} −−−−−−−−−−−−−−−−−−−−−−−−−−−− Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]</p>
<p>provides discrete concept encodings, (6) provides inherently inspectable and (7) revisable concept representations.
MethodUnsupervised Obj. level Factor level Cont. encs Disc. encs Inspectable RevisableCBM [34]</p>
<p>Table 2 :
2
NCB's concept encodings are expressive despite information bottleneck.Classifying object properties from different continuous and discrete encodings.The classifier is provided with different amounts of training sample encodings.The best ("•") and runner-up ("•") results are bold.
DatasetN TrainSysBinder (cont.)SysBinderSysBinder (hard) SysBinder (step)NLOTMNeural Concept BinderN=2000• 99.83±0.2492.49±5.4522.92±0.0095.76±4.9284.36±8.54• 99.02±1.00CLEVR-N=200• 99.20±0.4187.90±8.0522.92±0.0092.42±7.3272.99±8.43• 98.50±1.80EasyN=50• 91.13±4.2178.41±8.6922.92±0.0070.64±11.8949.94±4.97• 95.87±2.93N=20• 64.88±10.8962.61±7.1822.92±0.0054.61±9.5737.05±4.11• 94.22±4.11N=2000• 98.86±1.1586.22±10.4036.46±0.0088.90±14.8154.10±18.78• 97.26±2.67CLEVRN=200 N=50• 97.61±2.58 • 93.25±4.6281.13±12.39 61.67±8.5136.46±0.00 36.46±0.0083.17±17.05 68.81±17.7450.17±16.26 43.60±13.38• 96.80±3.01 • 94.67±4.65N=20• 79.11±8.7549.79±6.7336.46±0.0058.58±16.0941.52±12.90• 88.57±4.68</p>
<p>Table 3 :
3
NCB's unsupervised concept representations facilitate interpretable neural computations.Explanations of a NN classifier trained on the unsupervised concepts of NCB.Via NCB's inherent inspection procedures a human stakeholder can identify which concepts the classifier focuses on to make its predictions and thus interpret the NN's underlying decision rule.
GT Class RuleNN Expl.Human InspectionHuman InterpretationLarge, gray cubeC4</p>
<p>Table 4 :
4
Ablation of NCB's selection components for classifying attributes from concept representations.Best results are in bold.
N TrainNCB (P)NCB (P+E) NCB (P+E, topk)-CLEVR-Easy -N=2000 98.76±1.05 99.02±1.0098.93±1.10N=20097.11±2.16 98.50±1.8098.42±1.91N=5094.31±4.47 95.87±2.9395.72±3.04N=2090.50±7.09 94.22±4.1194.15±4.14-CLEVR -N=2000 96.77±2.63 97.26±2.6797.17±2.68N=20096.41±2.64 96.80±3.0196.80±3.04N=5094.29±4.78 94.67±4.6594.10±5.25N=2087.55±5.35 88.57±4.6888.42±4.63</p>
<p>Table 5 :
5
Ablation: Classifying attributes from concept representations with sub-optimal NCB components.The left column serves as a reference and represents the configurations used in the main evaluations, i.e., where the soft binder was trained for 600 epochs and the clustering model represented the HDBSCAN approach that was optimized via a grid-search over its corresponding hyperparameters.
N TrainNCBNCB (50 epochs)NCB (100 epochs)NCB (w/o grid search)NCB (kmeans)-CLEVR -N=2000 97.26±2.6795.19±1.294.91±3.4597.69±2.9597.26±2.80N=20096.80±3.0193.69±1.0893.83±2.9096.80±3.0996.01±3.51N=5094.67±4.6589.10±4.2989.67±6.9594.46±5.6587.65±8.78N=2088.57±4.6883.46±6.0888.48±2.3690.51±4.4073.52±10.92</p>
<p>Table 8 :
8
Percentage of solved CLEVR-Sudokus for different values of K with 5 example images.35.07 54.95 ± 31.86 64.91 ± 25.50 NCB revised (GPT-4) 48.62 ± 33.92 54.31 ± 30.85 63.62 ± 25.52 NCB revised (human) 60.23 ± 28.77 67.07 ± 24.83 75.05 ± 20.51 NCB revised (human)<em> 60.50 ± 35.24 66.40 ± 30.38 73.39 ± 24.96
Sudoku CLEVR-EasyK=10K=30K=50GT Concepts100.0 ± 0.00100.0 ± 0.00100.0 ± 0.00SA (supervised)39.02 ± 3.2542.07 ± 3.1447.89 ± 3.37SysBinder (unsupervised)0.00 ± 0.000.00 ± 0.000.00 ± 0.00NCB (unsupervised) 49.64 ± Sudoku CLEVRGT Concepts100.00 ± 0.00 100.00 ± 0.00 100.00 ± 0.00SA (supervised)68.96 ± 0.6573.92 ± 1.6978.46 ± 1.72SysBinder (unsupervised)0.00 ± 0.000.00 ± 0.000.00 ± 0.00NCB (unsupervised)21.18 ± 14.85 26.62 ± 18.47 34.79 ± 21.84NCB revised (GPT-4)17.76 ± 12.86 21.76 ± 15.84 29.12 ± 20.49NCB revised (human)40.30 ± 34.34 44.80 ± 35.58 51.55 ± 35.05NCB revised (human)</em>60.10 ± 24.36 66.69 ± 21.46 74.54 ± 16.02</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: We provide details about the used computational resources in the appendix.Guidelines: • The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes]</p>
<p>SysBinder (cont.) provides
-sized continuous encodings, whereas NCB provides discrete encodings of size ≤ 16.
https://hdbscan.readthedocs.io/en/latest/index.html
Code available here.
The code for generating the dataset is available in our code repository, the already generated data files are accessible under https://huggingface.co/datasets/AIML-TUDA/CLEVR-Sudoku
https://scikit-learn.org/stable/modules/generated/sklearn.tree. DecisionTreeClassifier.html
https://github.com/ScriptRaccoon/sudoku-solver-python/tree/main
AcknowledgmentsThe authors thank Gautam Singh for help with SysBinder and Cyprien Dzialo for preliminary results and insights.This work was supported by the Priority Program (SPP) 2422 in the subproject "Optimization of active surface design of high-speed progressive tools using machine and deep learning algorithms" funded by the German Research Foundation (DFG), the "ML2MT" project from the Volkswagen Stiftung and the "The Adaptive Mind" project from the Hessian Ministry of Science and Arts (HMWK).It has further benefited from the HMWK projects "The Third Wave of Artificial Intelligence -3AI", and Hessian.AI, as well as the Hessian research priority program LOEWE within the project WhiteBox, and the EU-funded "TANGO" project (EU Horizon 2023, GA No 57100431).Block 8H Numerical ResultsIn our evaluations we presented the results on CLEVR-Sudoku in the form of bar plots.We refer to Tab. 6, Tab.7 and Tab. 8 for the numerical values for the different variations of the dataset.NeurIPS Paper Checklist Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: The paper does not include theoretical results.Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: Experimental setup and training details are provided in the appendix.Additionally, the setup is available in the provided code, together with the CLEVR-Sudoku dataset.Guidelines:• The answer NA means that the paper does not include experiments.• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.5. Open access to data and code
Towards robust interpretability with selfexplaining neural networks. David Alvarez, -Melis , Tommi S Jaakkola, Advances in Neural Information Processing Systems (NeurIPS). 2018</p>
<p>The psychological nature of concepts. James Archer, Analyses of concept learning. Elsevier1966</p>
<p>Unsupervised concept discovery mitigates spurious correlations. Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, Kenji Kawaguchi, International Conference on Machine Learning (ICML). 2024OpenReview.net</p>
<p>Classical planning in deep latent space: Bridging the subsymbolic-symbolic boundary. Masataro Asai, Alex Fukunaga, Conference on Artificial Intelligence (AAAI). AAAI Press2018</p>
<p>Embed2sym -scalable neurosymbolic reasoning via clustered embeddings. Yaniv Aspis, Krysia Broda, Jorge Lobo, Alessandra Russo, International Conference on Principles of Knowledge Representation and Reasoning (KR). 2022</p>
<p>Embed2rule scalable neuro-symbolic learning via latent space weak-labelling. Yaniv Aspis, Mohammad Albinhassan, Jorge Lobo, Alessandra Russo, Neural-Symbolic Learning and Reasoning (NeSy), volume 14979 of Lecture Notes in Computer Science. Springer2024</p>
<p>Interpretable neural-symbolic concept reasoning. Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Mateo Espinosa Zarlenga, Lucie Charlotte Magister, Alberto Tonda, Pietro Lio, Frédéric Precioso, Mateja Jamnik, Giuseppe Marra, International Conference on Machine Learning (ICML). 2023</p>
<p>Concept induction using llms: a user experiment for assessment. Adrita Barua, Cara Widmer, Pascal Hitzler, CoRR, abs/2404.118752024</p>
<p>Symbol correctness in deep neural networks containing symbolic layers. Aaron Bembenek, Toby Murray, CoRR, abs/2402.036632024</p>
<p>Constraint propagation. Christian Bessiere, Foundations of Artificial Intelligence. Elsevier20062</p>
<p>Where is the truth? the risk of getting confounded in a continual world. Florian Peter Busch, Roshni Kamath, Rupert Mitchell, Wolfgang Stammer, Kristian Kersting, Martin Mundt, CoRR, abs/2402.064342024</p>
<p>Density-based clustering based on hierarchical density estimates. J G B Ricardo, Davoud Campello, Jörg Moulavi, Sander, Advances in Knowledge Discovery and Data Mining (PAKDD). 2013</p>
<p>Hierarchical density estimates for data clustering, visualization, and outlier detection. J G B Ricardo, Davoud Campello, Arthur Moulavi, Jörg Zimek, Sander, ACM Transactions on Knowledge Discovery from Data. 101512015</p>
<p>This looks like that: Deep learning for interpretable image recognition. Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, Jonathan Su, Advances in Neural Information Processing Systems (NeurIPS). 2019</p>
<p>AI for radiographic COVID-19 detection selects shortcuts over signal. Alex J Degrave, Joseph D Janizek, Su-In Lee, Nature Machine Intelligence. 372021</p>
<p>Boosting object representation learning via motion and object continuity. Quentin Delfosse, Wolfgang Stammer, Thomas Rothenbacher, Dwarak Vittal, Kristian Kersting, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. 2023</p>
<p>Symbolicai: A framework for logic-based approaches combining generative models and solvers. Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter, CoRR, abs/2402.008542024</p>
<p>Understanding the (extra-)ordinary: Validating deep model decisions with prototypical concept-based explanations. Maximilian Dreyer, Reduan Achtibat, Wojciech Samek, Sebastian Lapuschkin, CoRR, abs/2311.166812023</p>
<p>Savi++: Towards end-to-end object-centric learning from real-world videos. F Gamaleldin, Aravindh Elsayed, Mahendran, Klaus Sjoerd Van Steenkiste, Michael C Greff, Thomas Mozer, Kipf, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>The neural binding problem (s). Jerome Feldman, Cognitive neurodynamics. 72013</p>
<p>DISSECT: disentangled simultaneous explanations via concept traversals. Asma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff, Rosalind W Picard, International Conference on Learning Representations (ICLR). 2022</p>
<p>Towards automatic conceptbased explanations. Amirata Ghorbani, James Wexler, James Y Zou, Been Kim, Advances in Neural Information Processing Systems (NeurIPS). 2019</p>
<p>Dividing and conquering a blackbox to a mixture of interpretable models: Route, interpret, repeat. Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich, International Conference on Machine Learning (ICML). 2023</p>
<p>On the binding problem in artificial neural networks. Klaus Greff, Sjoerd Van Steenkiste, Jürgen Schmidhuber, CoRR, abs/2012.052082020</p>
<p>A survey on concept-based approaches for model improvement. Avani Gupta, P J Narayanan, CoRR, abs/2403.145662024</p>
<p>A neurovector-symbolic architecture for solving raven's progressive matrices. Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, Abbas Rahimi, Nature Machine Intelligence. 542023</p>
<p>Segdiscover: Visual concept discovery via unsupervised semantic segmentation. Haiyang Huang, Zhi Chen, Cynthia Rudin, CoRR, abs/2204.109262022</p>
<p>Categorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, International Conference on Learning Representations (ICLR). 2017</p>
<p>An image is worth multiple words: Learning object level concepts using multi-concept prompt learning. Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare, CoRR, abs/2310.122742023</p>
<p>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross B Girshick, Conference on Computer Vision and Pattern Recognition (CVPR). 2017</p>
<p>Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable AI systems. Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, Lin Guan, Conference on Artificial Intelligence (AAAI). 2022</p>
<p>Probabilistic concept bottleneck models. Eunji Kim, Dahuin Jung, Sangha Park, Siwon Kim, Sungroh Yoon, International Conference on Machine Learning (ICML). 2023</p>
<p>An introduction to variational autoencoders. P Diederik, Max Kingma, Welling, Foundations and Trends in Machine Learning. 201912</p>
<p>Concept bottleneck models. Pang Wei Koh, Thao Nguyen, Siang Yew, Stephen Tang, Emma Mussmann, Been Pierson, Percy Kim, Liang, International Conference on Machine Learning (ICML). 2020</p>
<p>Learning interpretable concept-based models with human feedback. Isaac Lage, Finale Doshi-Velez, CoRR, abs/2012.028982020</p>
<p>The dangers of post-hoc interpretability: Unjustified counterfactual explanations. Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, Marcin Detyniecki, International Joint Conference on Artificial Intelligence (IJCAI). 2019</p>
<p>Set transformer: A framework for attention-based permutation-invariant neural networks. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, Yee Whye Teh, International Conference on Machine Learning (ICML). 2019</p>
<p>Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin, Conference on Artificial Intelligence (AAAI). 2018</p>
<p>Infocon: Concept discovery with generative and discriminative informativeness. Ruizhe Liu, Qian Luo, Yanchao Yang, 2024</p>
<p>Challenging common assumptions in the unsupervised learning of disentangled representations. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem, International Conference on Machine Learning (ICML). 2019</p>
<p>Weakly-supervised disentanglement without compromises. Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, Michael Tschannen, International conference on machine learning (ICML). 2020</p>
<p>Disentangling factors of variations using few labels. Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, International Conference on Learning Representations (ICLR). 2020</p>
<p>Object-centric learning with slot attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>The challenge of learning symbolic representations. Salvatore Luca, Marco Lorello, Lippi, International Workshop on Neural-Symbolic Learning and Reasoning. 2023</p>
<p>Some methods for classification and analysis of multivariate observations. James Macqueen, Berkeley Symposium on Mathematical Statistics and Probability. 1967</p>
<p>The concrete distribution: A continuous relaxation of discrete random variables. Chris J Maddison, Andriy Mnih, Yee Whye Teh, International Conference on Learning Representations (ICLR). 2017</p>
<p>Promises and pitfalls of black-box concept learning models. Anita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi-Velez, Weiwei Pan, CoRR, abs/2106.133142021</p>
<p>Object centric architectures enable efficient causal representation learning. Amin Mansouri, Jason S Hartford, Yan Zhang, Yoshua Bengio, International Conference on Learning Representations (ICLR). OpenReview.net. 2024</p>
<p>The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, International Conference on Learning Representations (ICLR). 2019</p>
<p>Glancenets: Interpretable, leak-proof concept-based models. Emanuele Marconato, Andrea Passerini, Stefano Teso, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>VAEL: bridging variational autoencoders and probabilistic logic programming. Eleonora Misino, Giuseppe Marra, Emanuele Sansone, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Text-to-concept (and back) via cross-model alignment. Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, Soheil Feizi, International Conference on Machine Learning (ICML). 2023</p>
<p>Density-based clustering validation. Davoud Moulavi, Pablo A Jaskowiak, Ricardo J G B Campello, Arthur Zimek, Jörg Sander, International Conference on Data Mining. 2014</p>
<p>Hierarchical clustering. Introduction to HPC with MPI for Data Science. Frank Nielsen, 2016</p>
<p>Solving every sudoku puzzle. Peter Norvig, 2006</p>
<p>GPT-4 technical report. CoRR, abs/2303.087742023OpenAI</p>
<p>Surrocbm: Concept bottleneck surrogate models for generative post-hoc explanation. Bo Pan, Zhenke Liu, Yifei Zhang, Liang Zhao, CoRR, abs/2310.076982023</p>
<p>Right for the right reasons: Training differentiable models by constraining their explanations. Andrew Slavin Ross, Michael C Hughes, Finale Doshi-Velez, International Joint Conference on Artificial Intelligence (IJCAI). 2017</p>
<p>Measuring abstract reasoning in neural networks. Adam Santoro, Felix Hill, G T David, Ari S Barrett, Timothy P Morcos, Lillicrap, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880of Proceedings of Machine Learning Research</p>
<p>Concept bottleneck model with additional unsupervised concepts. Yoshihide Sawada, Keigo Nakamura, IEEE Access. 102022</p>
<p>Making deep neural networks right for the right scientific reasons by interacting with their explanations. Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting, Nature Machine Intelligence. 282020</p>
<p>Bridging the human-ai knowledge gap: Concept discovery and transfer in alphazero. Lisa Schut, Nenad Tomasev, Tom Mcgrath, Demis Hassabis, Ulrich Paquet, Been Kim, CoRR, abs/2310.164102023</p>
<p>Weakly supervised disentanglement with guarantees. Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, Ben Poole, International Conference on Learning Representations (ICLR). 2020</p>
<p>Illiterate DALL-E learns to compose. Gautam Singh, Fei Deng, Sungjin Ahn, International Conference on Learning Representations (ICLR). 2022</p>
<p>Neural systematic binder. Gautam Singh, Yeongbin Kim, Sungjin Ahn, International Conference on Learning Representations (ICLR). 2023</p>
<p>Neural-probabilistic answer set programming. Arseny Skryagin, Wolfgang Stammer, Daniel Ochs, Devendra Singh Dhami, Kristian Kersting, International Conference on Principles of Knowledge Representation and Reasoning (KR). 2022</p>
<p>Scalable neuralprobabilistic answer set programming. Arseny Skryagin, Daniel Ochs, Devendra Singh Dhami, Kristian Kersting, Journal of Artificial Intelligence Research. 782023</p>
<p>Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>Interactive disentanglement: Learning concepts by interacting with their prototype representations. Wolfgang Stammer, Marius Memmel, Patrick Schramowski, Kristian Kersting, Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, International Conference on Machine Learning (ICML). 2017</p>
<p>Codebook features: Sparse and discrete interpretability for neural networks. Alex Tamkin, Mohammad Taufeeque, Noah D Goodman, CoRR, abs/2310.172302023</p>
<p>Techniques for symbol grounding with satnet. Sever Topan, David Rolnick, Xujie Si, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Discrete key-value bottleneck. Frederik Träuble, Anirudh Goyal, Nasim Rahaman, Michael Curtis Mozer, Kenji Kawaguchi, Yoshua Bengio, Bernhard Schölkopf, International Conference on Machine Learning (ICML). 2023</p>
<p>Solutions to the binding problem: progress through controversy and convergence. Anne Treisman, Neuron. 2411999</p>
<p>Neural discrete representation learning. Aäron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Advances in Neural Information Processing Systems (NeurIPS). 2017</p>
<p>Neural clustering based on implicit maximum likelihood. Georgios Vardakas, Aristidis Likas, Neural Computing and Applications. 35292023</p>
<p>CURI: A benchmark for productive concept learning under uncertainty. Ramakrishna Vedantam, Arthur Szlam, Maximilian Nickel, Ari Morcos, Brenden M Lake, International Conference on Machine Learning (ICML). 2021</p>
<p>Learning bottleneck concepts in image classification. Bowen Wang, Liangzhi Li, Yuta Nakashima, Hajime Nagahara, Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Emergent symbols through binding in external memory. Taylor Whittington, Webb , Ishan Sinha, Jonathan D Cohen, International Conference on Learning Representations (ICLR). 2021</p>
<p>Neural language of thought models. Yi-Fu Wu, Minseung Lee, Sungjin Ahn, International Conference on Learning Representations (ICLR). OpenReview.net. 2024</p>
<p>Pix2code: Learning to compose neural visual concepts as programs. Antonia Wüst, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting, Uncertainty in Artificial Intelligence. 2024</p>
<p>Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar, Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>On completeness-aware concept-based explanations in deep neural networks. Chih-Kuan Yeh, Been Kim, Sercan Ömer Arik, Chun-Liang Li, Tomas Pfister, Pradeep Ravikumar, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>Concept embedding models: Beyond the accuracyexplainability trade-off. Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Frédéric Precioso, Stefano Melacci, Adrian Weller, Pietro Lió, Mateja Jamnik, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>The unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Conference on Computer Vision and Pattern Recognition (CVPR). 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-bddb9d818b73de0a06197a6966673c7eb63c9146</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bddb9d818b73de0a06197a6966673c7eb63c9146" target="_blank">Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream and introduces Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations.</p>
                <p><strong>Paper Abstract:</strong> Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TiM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-in-Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agnostic external memory framework that stores LLM-generated "thoughts" (inductive relation triples) and uses a two-stage workflow (recall-before-generation and post-think update) with LSH-based indexing to enable efficient long-term memory and avoid repeated reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TiM (Think-in-Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-augmentation wrapper for LLM agents that (1) recalls relevant stored thoughts before generating a response, and (2) performs post-thinking after generation to produce and insert new thoughts; supports insert/forget/merge operations and uses LSH for scalable retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM; Baichuan2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGLM: open-source bilingual LLM (~6.2B) optimized for dialogue. Baichuan2: open large-scale multilingual LLM (~13B) trained on large corpora; both are used as base LLMs integrated with TiM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term multi-turn dialogue / response generation (GVD, KdConv, RMD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-turn conversation tasks evaluating retrieval of long-range history and generation of coherent, correct responses; datasets include Generated Virtual Dataset (GVD, bilingual simulated long-term chats), KdConv (Chinese multi-domain knowledge-driven conversations), and RMD (Real-world Medical Dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-term dialogue / conversational QA / response generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external long-term "thought" memory (episodic-style, inductive thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Hash-table (LSH) index mapping embeddings to groups; two-stage retrieval: (1) LSH group selection for the query embedding, (2) similarity ranking (top-k) within the group; memory updated by post-thinking (insert), and organized via forget and merge operations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Self-generated inductive thoughts (textual relation triples or short reasoning summaries) stored as (hash_index, thought) tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>LSH-based nearest-group lookup followed by pairwise semantic similarity ranking inside the selected group (top-k recall; experiments use top-5 by default; analysis over k up to 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GVD (ChatGLM English): Retrieval Acc 0.820; Response Correctness 0.450; Contextual Coherence 0.735. GVD (ChatGLM Chinese): Retrieval Acc 0.850; Response Correctness 0.605; Coherence 0.665. KdConv (ChatGLM Film): Retrieval Acc 0.920; Response Correctness 0.827; Coherence 0.943. KdConv (ChatGLM Music): Retrieval Acc 0.970; Response Correctness 0.826; Coherence 0.926. KdConv (ChatGLM Travel): Retrieval Acc 0.940; Response Correctness 0.766; Coherence 0.912. KdConv (Baichuan2 Film): Retrieval Acc 0.913; Response Correctness 0.743; Coherence 0.870. KdConv (Baichuan2 Music): Retrieval Acc 0.900; Response Correctness 0.710; Coherence 0.780. KdConv (Baichuan2 Travel): Retrieval Acc 0.833; Response Correctness 0.757; Coherence 0.807. RMD (ChatGLM): Retrieval Acc 0.900; Response Correctness 0.843; Coherence 0.943. RMD (Baichuan2): Retrieval Acc 0.873; Response Correctness 0.538; Coherence 0.663. Retrieval time: 0.5305 ms (per retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>GVD (SiliconFriend baseline, ChatGLM English): Retrieval Acc 0.809; Response Correctness 0.438; Coherence 0.680. GVD (SiliconFriend, ChatGLM Chinese): Retrieval Acc 0.840; Response Correctness 0.418; Coherence 0.428. KdConv (ChatGLM baseline, no memory): Film Response Correctness 0.657; Coherence 0.923. KdConv (ChatGLM baseline Music): Correctness 0.666; Coherence 0.910. KdConv (ChatGLM baseline Travel): Correctness 0.735; Coherence 0.906. KdConv (Baichuan2 baseline Film): Correctness 0.360; Coherence 0.413. KdConv (Baichuan2 baseline Music): Correctness 0.253; Coherence 0.283. KdConv (Baichuan2 baseline Travel): Correctness 0.207; Coherence 0.280. RMD (ChatGLM baseline): Correctness 0.806; Coherence 0.893. RMD (Baichuan2 baseline): Correctness 0.506; Coherence 0.538. Retrieval time (pairwise baseline): 0.6287 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparisons: TiM vs SiliconFriend (raw Q-R memory) and vs no-memory baselines across three datasets. TiM consistently improves retrieval accuracy, response correctness, and contextual coherence; retrieval time reduced from 0.6287 ms (pairwise baseline) to 0.5305 ms (TiM's LSH+grouped similarity). Top-k analysis: top-1 recall >0.7 on KdConv(Travel), top-10 recall = 0.973. No isolated ablation reported that separately removes insert/forget/merge operations or disables post-thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing LLM-generated "thoughts" as memory (instead of raw Q-R text) reduces repeated reasoning and inconsistent reasoning paths, improves retrieval accuracy and downstream response correctness and coherence across languages and domains, and yields modestly faster retrieval via LSH-grouping; TiM is LLM-agnostic and supports dynamic memory operations (insert/forget/merge).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include dependence on the quality of LLM-generated thoughts (no automatic gold-standard thought extraction), limited ablation studies (no per-operation ablation for insert/forget/merge), only modest retrieval-time improvement (≈0.1 ms), evaluation relies on human annotators (subjective labels), and potential sensitivity to base LLM capabilities (Chinese gains larger than English in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SiliconFriend</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SiliconFriend (baseline memory mechanism used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical external memory baseline referenced and used in comparisons that stores raw question-response (Q-R) pairs and retrieves via similarity over the whole memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SiliconFriend (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline memory mechanism that stores raw conversation text (Q-R pairs) in an external memory and retrieves relevant history via pairwise similarity across the entire memory; used as baseline in GVD experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM (in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGLM (used as base LLM for comparisons in GVD experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generated Virtual Dataset (GVD) long-term retrieval & response</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether retrieval-augmented LLMs can recall long-term simulated user facts and answer probing questions in English and Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-term dialogue memory retrieval & response generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external raw-text memory (Q-R pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Full-memory pairwise similarity retrieval (compute similarity between query embedding and each memory entry); no LSH grouping described for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw question-response conversation text pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Pairwise semantic similarity across the whole memory (no LSH grouping), rank and return top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GVD (ChatGLM English): Retrieval Acc 0.809; Response Correctness 0.438; Contextual Coherence 0.680. GVD (ChatGLM Chinese): Retrieval Acc 0.840; Response Correctness 0.418; Coherence 0.428.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly to TiM: TiM yields modestly better retrieval accuracy and notably better contextual coherence and response correctness on GVD; baseline retrieval time (pairwise search) measured as 0.6287 ms vs TiM's 0.5305 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing raw Q-R pairs enables retrieval but requires repeated re-reasoning by the LLM and is less effective at producing coherent responses than TiM's thought-based memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher retrieval cost (pairwise over entire memory), lacks operations for forgetting or merging, stores raw tokens requiring potential LLM architecture adjustments for some token-based approaches (not LLM-agnostic in some designs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior external memory mechanism (referenced) that stores dialogue turns (Q-R pairs) and includes forgetting strategies inspired by human memory; cited as a baseline in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An external memory mechanism that caches Q-R pairs and applies operations inspired by forgetting curves to manage memory over long-term conversations (referenced but not reimplemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term conversation benchmarks (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed for improving LLM performance on long-term conversational tasks by maintaining and recalling prior dialogue content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-term memory-augmented dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external Q-R memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>External memory cache with management policies (insert and forget operations mentioned in the paper's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw question-response pairs (dialogue text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity-based retrieval (pairwise similarity) across stored Q-R entries (as described in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work and compared in Table 1 as supporting insert and forget operations but not merge; not evaluated experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work that stores Q-R pairs and uses forgetting operations; contrasted with TiM which stores thoughts and supports insert/forget/merge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As discussed in this paper, Q-R based memories require repeated reasoning and can be costly to retrieve from at long scales (pairwise similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmenting language models with long-term memory (token-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-based external memory approach (referenced) that maintains tokens in memory and may require architecture changes to the LLM for integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LongMem (token-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prior approach that stores tokens as persistent memory to extend context, typically requiring adjustments to the LLM internal architecture to handle token-based memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context language modeling (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extend LLMs' effective context length by storing token-level long-term memory for retrieval/conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-context language modeling / memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>token-level external memory (requires LLM-aware integration)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores tokens in external memory and integrates them into model context (token-level memory), architecture-modifications typically required.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw tokens / token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Not detailed in this paper; referenced as token-based memory in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Referenced in related work and contrasted with TiM: token-based memory often requires LLM architecture adaptation and lacks the thought-level operations TiM proposes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Token-based memories can be effective but are less LLM-agnostic and harder to combine with diverse LLMs compared to TiM's thought-based external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires changes to LLM internals; less flexible for plugging into closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelationLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational memory-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that constructs relation-structured memory or knowledge graph style memory integrated with language models (referenced in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Relational memory-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RelationLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-augmented language model that integrates relational (knowledge-graph-like) memory; mentioned as a contrast to TiM which stores thoughts and is LLM-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Relation-aware language modeling / memory augmentation (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Modeling and using relational memory structures to augment language modeling and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory-augmented language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge-graph / relational memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Relational or KG-style memory storing relations; integration details are in the referenced paper, not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured relation triples / graph edges.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Not specified here (referenced only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work and contrasted in Table 1 (RelationLM is not LLM-agnostic in the paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Relation-structured memory approaches exist but may not be readily LLM-agnostic compared to TiM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>According to the paper's comparison, relational memory approaches may require stronger coupling with model architecture (not LLM-agnostic).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Augmenting language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Relational memory-augmented language models <em>(Rating: 1)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 1)</em></li>
                <li>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8426",
    "paper_id": "paper-bddb9d818b73de0a06197a6966673c7eb63c9146",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "TiM",
            "name_full": "Think-in-Memory",
            "brief_description": "An LLM-agnostic external memory framework that stores LLM-generated \"thoughts\" (inductive relation triples) and uses a two-stage workflow (recall-before-generation and post-think update) with LSH-based indexing to enable efficient long-term memory and avoid repeated reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TiM (Think-in-Memory)",
            "agent_description": "A memory-augmentation wrapper for LLM agents that (1) recalls relevant stored thoughts before generating a response, and (2) performs post-thinking after generation to produce and insert new thoughts; supports insert/forget/merge operations and uses LSH for scalable retrieval.",
            "model_name": "ChatGLM; Baichuan2",
            "model_description": "ChatGLM: open-source bilingual LLM (~6.2B) optimized for dialogue. Baichuan2: open large-scale multilingual LLM (~13B) trained on large corpora; both are used as base LLMs integrated with TiM.",
            "task_name": "Long-term multi-turn dialogue / response generation (GVD, KdConv, RMD)",
            "task_description": "Multi-turn conversation tasks evaluating retrieval of long-range history and generation of coherent, correct responses; datasets include Generated Virtual Dataset (GVD, bilingual simulated long-term chats), KdConv (Chinese multi-domain knowledge-driven conversations), and RMD (Real-world Medical Dataset).",
            "task_type": "long-term dialogue / conversational QA / response generation",
            "memory_used": true,
            "memory_type": "external long-term \"thought\" memory (episodic-style, inductive thoughts)",
            "memory_mechanism": "Hash-table (LSH) index mapping embeddings to groups; two-stage retrieval: (1) LSH group selection for the query embedding, (2) similarity ranking (top-k) within the group; memory updated by post-thinking (insert), and organized via forget and merge operations.",
            "memory_representation": "Self-generated inductive thoughts (textual relation triples or short reasoning summaries) stored as (hash_index, thought) tuples.",
            "memory_retrieval_method": "LSH-based nearest-group lookup followed by pairwise semantic similarity ranking inside the selected group (top-k recall; experiments use top-5 by default; analysis over k up to 10).",
            "performance_with_memory": "GVD (ChatGLM English): Retrieval Acc 0.820; Response Correctness 0.450; Contextual Coherence 0.735. GVD (ChatGLM Chinese): Retrieval Acc 0.850; Response Correctness 0.605; Coherence 0.665. KdConv (ChatGLM Film): Retrieval Acc 0.920; Response Correctness 0.827; Coherence 0.943. KdConv (ChatGLM Music): Retrieval Acc 0.970; Response Correctness 0.826; Coherence 0.926. KdConv (ChatGLM Travel): Retrieval Acc 0.940; Response Correctness 0.766; Coherence 0.912. KdConv (Baichuan2 Film): Retrieval Acc 0.913; Response Correctness 0.743; Coherence 0.870. KdConv (Baichuan2 Music): Retrieval Acc 0.900; Response Correctness 0.710; Coherence 0.780. KdConv (Baichuan2 Travel): Retrieval Acc 0.833; Response Correctness 0.757; Coherence 0.807. RMD (ChatGLM): Retrieval Acc 0.900; Response Correctness 0.843; Coherence 0.943. RMD (Baichuan2): Retrieval Acc 0.873; Response Correctness 0.538; Coherence 0.663. Retrieval time: 0.5305 ms (per retrieval).",
            "performance_without_memory": "GVD (SiliconFriend baseline, ChatGLM English): Retrieval Acc 0.809; Response Correctness 0.438; Coherence 0.680. GVD (SiliconFriend, ChatGLM Chinese): Retrieval Acc 0.840; Response Correctness 0.418; Coherence 0.428. KdConv (ChatGLM baseline, no memory): Film Response Correctness 0.657; Coherence 0.923. KdConv (ChatGLM baseline Music): Correctness 0.666; Coherence 0.910. KdConv (ChatGLM baseline Travel): Correctness 0.735; Coherence 0.906. KdConv (Baichuan2 baseline Film): Correctness 0.360; Coherence 0.413. KdConv (Baichuan2 baseline Music): Correctness 0.253; Coherence 0.283. KdConv (Baichuan2 baseline Travel): Correctness 0.207; Coherence 0.280. RMD (ChatGLM baseline): Correctness 0.806; Coherence 0.893. RMD (Baichuan2 baseline): Correctness 0.506; Coherence 0.538. Retrieval time (pairwise baseline): 0.6287 ms.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Comparisons: TiM vs SiliconFriend (raw Q-R memory) and vs no-memory baselines across three datasets. TiM consistently improves retrieval accuracy, response correctness, and contextual coherence; retrieval time reduced from 0.6287 ms (pairwise baseline) to 0.5305 ms (TiM's LSH+grouped similarity). Top-k analysis: top-1 recall &gt;0.7 on KdConv(Travel), top-10 recall = 0.973. No isolated ablation reported that separately removes insert/forget/merge operations or disables post-thinking.",
            "key_findings": "Storing LLM-generated \"thoughts\" as memory (instead of raw Q-R text) reduces repeated reasoning and inconsistent reasoning paths, improves retrieval accuracy and downstream response correctness and coherence across languages and domains, and yields modestly faster retrieval via LSH-grouping; TiM is LLM-agnostic and supports dynamic memory operations (insert/forget/merge).",
            "limitations_or_challenges": "Reported limitations include dependence on the quality of LLM-generated thoughts (no automatic gold-standard thought extraction), limited ablation studies (no per-operation ablation for insert/forget/merge), only modest retrieval-time improvement (≈0.1 ms), evaluation relies on human annotators (subjective labels), and potential sensitivity to base LLM capabilities (Chinese gains larger than English in experiments).",
            "uuid": "e8426.0",
            "source_info": {
                "paper_title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SiliconFriend",
            "name_full": "SiliconFriend (baseline memory mechanism used in experiments)",
            "brief_description": "A classical external memory baseline referenced and used in comparisons that stores raw question-response (Q-R) pairs and retrieves via similarity over the whole memory.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "use",
            "agent_name": "SiliconFriend (baseline)",
            "agent_description": "Baseline memory mechanism that stores raw conversation text (Q-R pairs) in an external memory and retrieves relevant history via pairwise similarity across the entire memory; used as baseline in GVD experiments.",
            "model_name": "ChatGLM (in experiments)",
            "model_description": "ChatGLM (used as base LLM for comparisons in GVD experiments).",
            "task_name": "Generated Virtual Dataset (GVD) long-term retrieval & response",
            "task_description": "Evaluate whether retrieval-augmented LLMs can recall long-term simulated user facts and answer probing questions in English and Chinese.",
            "task_type": "long-term dialogue memory retrieval & response generation",
            "memory_used": true,
            "memory_type": "external raw-text memory (Q-R pairs)",
            "memory_mechanism": "Full-memory pairwise similarity retrieval (compute similarity between query embedding and each memory entry); no LSH grouping described for this baseline.",
            "memory_representation": "Raw question-response conversation text pairs.",
            "memory_retrieval_method": "Pairwise semantic similarity across the whole memory (no LSH grouping), rank and return top-k.",
            "performance_with_memory": "GVD (ChatGLM English): Retrieval Acc 0.809; Response Correctness 0.438; Contextual Coherence 0.680. GVD (ChatGLM Chinese): Retrieval Acc 0.840; Response Correctness 0.418; Coherence 0.428.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared directly to TiM: TiM yields modestly better retrieval accuracy and notably better contextual coherence and response correctness on GVD; baseline retrieval time (pairwise search) measured as 0.6287 ms vs TiM's 0.5305 ms.",
            "key_findings": "Storing raw Q-R pairs enables retrieval but requires repeated re-reasoning by the LLM and is less effective at producing coherent responses than TiM's thought-based memory.",
            "limitations_or_challenges": "Higher retrieval cost (pairwise over entire memory), lacks operations for forgetting or merging, stores raw tokens requiring potential LLM architecture adjustments for some token-based approaches (not LLM-agnostic in some designs).",
            "uuid": "e8426.1",
            "source_info": {
                "paper_title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank: Enhancing large language models with long-term memory",
            "brief_description": "A prior external memory mechanism (referenced) that stores dialogue turns (Q-R pairs) and includes forgetting strategies inspired by human memory; cited as a baseline in related work.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "An external memory mechanism that caches Q-R pairs and applies operations inspired by forgetting curves to manage memory over long-term conversations (referenced but not reimplemented here).",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-term conversation benchmarks (general reference)",
            "task_description": "Designed for improving LLM performance on long-term conversational tasks by maintaining and recalling prior dialogue content.",
            "task_type": "long-term memory-augmented dialogue",
            "memory_used": true,
            "memory_type": "external Q-R memory",
            "memory_mechanism": "External memory cache with management policies (insert and forget operations mentioned in the paper's comparison table).",
            "memory_representation": "Raw question-response pairs (dialogue text).",
            "memory_retrieval_method": "Similarity-based retrieval (pairwise similarity) across stored Q-R entries (as described in related work).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work and compared in Table 1 as supporting insert and forget operations but not merge; not evaluated experimentally in this paper.",
            "key_findings": "Cited as prior work that stores Q-R pairs and uses forgetting operations; contrasted with TiM which stores thoughts and supports insert/forget/merge.",
            "limitations_or_challenges": "As discussed in this paper, Q-R based memories require repeated reasoning and can be costly to retrieve from at long scales (pairwise similarity).",
            "uuid": "e8426.2",
            "source_info": {
                "paper_title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LongMem",
            "name_full": "Augmenting language models with long-term memory (token-based memory)",
            "brief_description": "A token-based external memory approach (referenced) that maintains tokens in memory and may require architecture changes to the LLM for integration.",
            "citation_title": "Augmenting language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "LongMem (token-based memory)",
            "agent_description": "A prior approach that stores tokens as persistent memory to extend context, typically requiring adjustments to the LLM internal architecture to handle token-based memory.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-context language modeling (general reference)",
            "task_description": "Extend LLMs' effective context length by storing token-level long-term memory for retrieval/conditioning.",
            "task_type": "long-context language modeling / memory augmentation",
            "memory_used": true,
            "memory_type": "token-level external memory (requires LLM-aware integration)",
            "memory_mechanism": "Stores tokens in external memory and integrates them into model context (token-level memory), architecture-modifications typically required.",
            "memory_representation": "Raw tokens / token sequences.",
            "memory_retrieval_method": "Not detailed in this paper; referenced as token-based memory in related work.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Referenced in related work and contrasted with TiM: token-based memory often requires LLM architecture adaptation and lacks the thought-level operations TiM proposes.",
            "key_findings": "Token-based memories can be effective but are less LLM-agnostic and harder to combine with diverse LLMs compared to TiM's thought-based external memory.",
            "limitations_or_challenges": "Requires changes to LLM internals; less flexible for plugging into closed-source models.",
            "uuid": "e8426.3",
            "source_info": {
                "paper_title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RelationLM",
            "name_full": "Relational memory-augmented language models",
            "brief_description": "A prior work that constructs relation-structured memory or knowledge graph style memory integrated with language models (referenced in related work).",
            "citation_title": "Relational memory-augmented language models",
            "mention_or_use": "mention",
            "agent_name": "RelationLM",
            "agent_description": "A memory-augmented language model that integrates relational (knowledge-graph-like) memory; mentioned as a contrast to TiM which stores thoughts and is LLM-agnostic.",
            "model_name": null,
            "model_description": null,
            "task_name": "Relation-aware language modeling / memory augmentation (reference)",
            "task_description": "Modeling and using relational memory structures to augment language modeling and downstream tasks.",
            "task_type": "memory-augmented language modeling",
            "memory_used": true,
            "memory_type": "knowledge-graph / relational memory",
            "memory_mechanism": "Relational or KG-style memory storing relations; integration details are in the referenced paper, not in this paper.",
            "memory_representation": "Structured relation triples / graph edges.",
            "memory_retrieval_method": "Not specified here (referenced only).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work and contrasted in Table 1 (RelationLM is not LLM-agnostic in the paper's comparison).",
            "key_findings": "Relation-structured memory approaches exist but may not be readily LLM-agnostic compared to TiM.",
            "limitations_or_challenges": "According to the paper's comparison, relational memory approaches may require stronger coupling with model architecture (not LLM-agnostic).",
            "uuid": "e8426.4",
            "source_info": {
                "paper_title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Augmenting language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Relational memory-augmented language models",
            "rating": 1
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 1
        },
        {
            "paper_title": "Unleashing infinite-length input capacity for largescale language models with self-controlled memory system",
            "rating": 1
        }
    ],
    "cost": 0.01257375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</h1>
<p>Lei Liu*<br>liulei1497@gmail.com<br>CUHK-Shenzhen, Ant Group<br>Binbin Hu, Zhiqiang Zhang<br>{bin.hbb,lingyao.zzq}@antfin.com<br>Ant Group</p>
<p>Xiaoyan Yang<br>joyce.yxy@antgroup.com<br>Ant Group<br>Jinjie Gu<br>jinjie.gjj@antfin.com<br>Ant Group</p>
<p>Yue Shen ${ }^{\dagger}$<br>zhanying@antgroup.com<br>Ant Group<br>Guannan Zhang<br>zgn138592@antfin.com<br>Ant Group</p>
<h2>ABSTRACT</h2>
<p>Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, i.e., inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (i.e., insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.</p>
<h2>KEYWORDS</h2>
<p>Large Language Model, Response Generation, Long-term Memory</p>
<h2>1 INTRODUCTION</h2>
<p>Impressive advancements in Large Language Models (LLMs) have revolutionized the interaction between human beings and artificial intelligence (AI) systems. These advancements have particularly showcased superior performance in human-agent conversations, as demonstrated by ChatGPT [1] and GPT-4 [2]. From finance [3] and healthcare [4] to business and customer service [5], these advanced LLMs exhibit a remarkable ability to understand questions and generate corresponding responses. Notably, the large model scale,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>reaching up to hundreds of billions of parameters, enables the emergence of such human-like abilities within LLMs [6].</p>
<p>Despite the remarkable abilities of LLMs pre-trained on large corpora, LLM-based AI agents still face a significant limitation in long-term scenarios, i.e., inability to process exceptionally lengthy inputs [7]. This is particularly important in some specific tasks, e.g., medical AI assistants [4] rely on the symptoms of past conversations to provide accurate clinical diagnosis. Thus, LLMs without the capability of dealing with long-term inputs may hinder the diagnosis accuracy due to forgetting important disease symptoms (see in Section 4.4). Therefore, it is necessary to develop AI systems with long-term capabilities for more accurate and reliable interactions.</p>
<p>There have been various studies conducted to improve the capabilities of LLMs to handle long-term inputs. Overall, these studies can be roughly divided into two types: (1) Internal memory based methods [8] aims to reduce the computational costs of self-attention for expanding the sequence length. To accommodate longer input texts, special positional encoding should be exploited to learn relative positions. For example, [9] explored a block-local Transformer with global encoder tokens, combined with additional long input pre-training. (2) External memory based methods (also called long-term memory mechanism [10]) generally utilize a physical space as a memory cache to store historical information, where relevant history can be read from the memory cache to augment LLMs without forgetting. In particular, both token and raw text can be maintained as history in the memory. For instance, [11] demonstrated a significant performance improvement by augmenting LLMs with an external memory cache containing trillions of tokens assisted by BERT embeddings [12]. It should be noticed that token-based memory mechanism requires to adjust the LLM's architecture for adaption, which is hard to be combined with different LLMs. By accessing an external memory cache, the augmented LLMs have achieved new state-of-the-art records in various language modeling benchmarks, which generally performs better than internal memory based methods. Therefore, this work focuses on designing an LLM-agnostic external memory mechanism to enhanced the memorization capacity of LLMs.</p>
<p>In general, the utility of memory-augmented LLMs primarily hinges on their ability for iterative recalling and repeated reasoning over the history in an external memory cache. In detail, for conversations after the $n$-th turn, LLMs are required to re-understand and re-reason the history from 0 -th to $(n-1)$-th conversations. For example, as shown in Figure 1, to answer the questions of 2-th and 3-th turns, LLMs recall 1-th turn and reason over it for twice.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparisons between previous memory mechanisms with our proposed TiM. (Left): Existing memory mechanisms mainly save raw text of previous turns, which require repeated reasoning over the same history. This easily leads to the inconsistent reasoning path (i.e., red part of the left) with wrong response. (Right): The proposed TiM stores the thoughts of LLMs for previous turns, which can avoid such inconsistency without repeated reasoning (i.e., red part of the right).</p>
<p>Unfortunately, this paradigm is prone to encountering several issues and potentially causes a performance bottleneck in real-world applications. The main issues are shown in follows:</p>
<ul>
<li>Inconsistent reasoning paths. Prior studies [13, 14] has shown that LLMs easily generate diverse reasoning paths for the same query. As shown in Figure 1 (Left), LLMs give a wrong response due to inconsistent reasoning over the context.</li>
<li>Unsatisfying retrieval cost. To retrieve relevant history, previous memory mechanisms need to calculate pairwise similarity between the question and each historical conversation, which is time-consuming for long-term dialogue.</li>
</ul>
<p>To address these concerns, we would like to advance one step further in memory-augmented LLMs with the analogy to the typical process of metacognition [15], where the brain saves thoughts as memories rather than the details of original events. Thus, in this work, we propose a Think-in-Memory (TiM) framework to model the human-like memory mechanism, which enables LLMs to remember and selectively recall historical thoughts in long-term interaction scenarios. Specifically, as shown in Figure 2, the TiM framework is divided into two stages: (1) In the recalling stage, LLMs generate the response for the new query with recalling relevant thoughts in the memory; (2) In the post-thinking stage, the LLM engages in reasoning and thinking over the response and saves new thoughts into an external memory. Besides, to mirror the cognitive process of humans, we formulate some basic principles to organize the thoughts in memory based on the well-established operations (e.g., insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Specifically, TiM is built on
a hash-based retrieval mechanism (i.e., Locality-Sensitive Hashing [16]) to support efficient hand-in (i.e., insert thoughts) and hand-out (i.e., recall thoughts) operations. Additionally, TiM is designed to be LLM-agnostic, which means it can be combined with various types of language models. This includes closed-source LLMs such as ChatGPT [1], as well as open-source LLMs like ChatGLM[17].</p>
<p>The key contributions of this work are summarized as follows:</p>
<ul>
<li>We propose a novel human-like long-term memory mechanism called TiM, enabling LLMs to remember and selectively recall thoughts. TiM can let LLM think in memory without repeated reasoning over the long-term history.</li>
<li>We formulate some basic principles to organize the thoughts in memory based on the well-established operations, which mirrors human cognitive process to empower dynamic updates and evolution for the thoughts in memory. Besides, a hash-based retrieval mechanism is introduced for efficient utilization of TiM.</li>
<li>We conducted extensive experiments on multi-turn dialogue datasets. The results indicate that our method can substantially enhance LLM's performance across various dimensions: (1) It enables diverse topics ranging from open to specific domains; (2) It supports bilingual languages in both Chinese and English; (3) It improves response correctness and coherence.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overview of TiM framework. LLMs firstly recall history and give response for the question. Then new thoughts can be generated via the post-thinking step. These thoughts are saved as the memory to avoid repeated reasoning on the history.</p>
<h2>2 RELATED WORK</h2>
<h3>2.1 Large Language Models</h3>
<p>Recently, Large Language Models (LLMs) have attracted significant attention for their superior performance on a wide range of Natural Language Processing tasks, such as machine translation [18], sentiment analysis [19], and question answering systems [20]. These advancements are indeed supported by the developments of deep learning techniques and the availability of vast amounts of text data. From the perspective of open source, existing LLMs can roughly divided into two types: (1) cutting-edge closed-source LLMs, e.g., PaLM [21], GPT-4 [2], and ChatGPT [1]; (2) open-source LLMs, e.g., LLaMa [22], ChatGLM [17], and Alpaca [23]. Researchers have studied various methods for the applications of these popular LLMs. For example, many strategies are proposed to fine-tune pre-trained LLM models on specific tasks [24], which can further improve their capabilities in specific domains. Besides, some efforts have been made to enhance the quality of the generated content of LLMs, e.g., generating more diverse and creative text while maintaining coherence and fluency [25]. Overall, recent developments of LLMs cover a broad range of topics, including model architecture [17], training methods [26], fine-tuning strategies [27], as well as ethical considerations [21]. All these methods aim to enhance the understanding capabilities of LLMs for real-world applications. However, these powerful LLM models still have some shortcomings. One notable limitation of LLMs is their lack of a strong long-term memory, which hinders their ability to process lengthy context and retrieve relevant historical information.</p>
<h3>2.2 Long-term Memory</h3>
<p>Numerous efforts have been conducted to enhance the memory capabilities of LLMs. One approach is to utilize memory-augmented
networks (MANNs) [28], such as Neural Turing Machines (NTMs) [29], which is designed to utilize more context information for dialogue. In general, MANNs are proposed with an external memory cache via the storage and manipulation of information, which can well handle tasks of long-term period by interacting with memory. In addition, many studies focused on long-term conversations [3033]. For example, Xu et al. [30] introduced a new English dataset consisting of multi-session human-human crowdworker chats for long-term conversations. Zhong et al. [32] proposed a MemoryBank mechanism inspired by Ebbinghaus' forgetting curve theory. However, these methods still face some great challenges to achieve a reliable and adaptable long-term memory mechanism for Language and Learning Models (LLMs). Concretely, these methods only considered storing the raw dialogue text, requiring repeated reasoning of the LLM agent over the same history. Besides, these models need to calculate pairwise similarity for recalling relevant information, which is time-consuming for the long-term interactions.</p>
<h2>3 METHODOLOGY</h2>
<p>In this section, we first introduce the overall workflow of our proposed framework. Then we provide a detailed description for each stage of TiM, involving storage for memory cache, organization principle for memory updating, and retrieval for memory recalling.</p>
<h3>3.1 Framework Overview</h3>
<p>Given a sequence of conversation turns, each turn is denoted by a tuple $(Q, R)$, representing the user's query $(Q)$ and the agent's response (R) at that specific turn. The main objective is to generate a more accurate response $R_{y}$ like a human for a new coming query $Q_{x}$, while remembering the contextual information of historical conversation turns. The proposed TiM allows the agent to process</p>
<p>long-term conversation and retain useful historical information after multiple conversations with the user.
3.1.1 Main Components. As illustrated in Figure 2, our TiM comprises the following components, working together to provide more accurate and coherent responses for long-term conversation:</p>
<ul>
<li>Agent $\mathcal{A}: \mathcal{A}$ is a pre-trained LLM model to facilitate dynamic conversations, such as ChatGPT [1] and ChatGLM [17].</li>
<li>Memory Cache $\mathcal{M}: \mathcal{M}$ a continually growing hash table of keyvalue pairs, where key is the hash index and value is a single thought. More details of $\mathcal{M}$ can refer to Section 3.2. To be clear, $\mathcal{M}$ supports varying operations as shown in Table 1.</li>
<li>Hash-based Mapping $\mathbf{F}(\cdot)$ : Locality-sensitive Hashing is introduced to quickly save and find the relevant thoughts in $\mathcal{M}$.
3.1.2 Workflow. Overall framework is divided into two stages:</li>
<li>Stage-1: Recall and Generation. Given a new question from the user, LLM agent $\mathcal{A}$ retrieves relevant thoughts for generating accurate responses. Since we save the self-generated reasoning thoughts as external memory, this stage can directly recall and answer the question without repeated reasoning over the raw historical conversation text.</li>
<li>Stage-2: Post-think and Update. After answering the question, we let the LLM agent post-think upon $Q-R$ pair and insert the newly self-generated reasoning thoughts into memory cache $\mathcal{M}$.</li>
</ul>
<h3>3.2 Storage for Memory Cache</h3>
<p>3.2.1 Thoughts-based System. TiM's storage system $\mathcal{M}$ aims to save the knowledge of AI-user interactions via self-generated inductive thoughts (Definition 3.1) upon the conversations. Each piece of thought $T$ is stored in the format of the tuple $\left(H_{i d x}, T\right)$, where $H_{i d x}$ is the hash index obtained by hash function $\mathbf{F}(T)$. This hash-based storage not only aids in quick memory retrieval but also facilitates the memory updating, providing a detailed index of historical thoughts.</p>
<p>Definition 3.1. Inductive Thought. The inductive thought is defined as the text which contains the relation between two entities, i.e., satisfying a relation triple $\left(E_{h}, r_{i}, E_{t}\right) . E_{h}$ is head entity connected with tail entity $E_{t}$ via the relation $r_{i}$, where $i \in[0, N]$ and $N$ is the relation number. Conceptually, $R_{h}=\left{r_{1}, \cdots, r_{N}\right}$ consists of all the one-hop relations for the entity $E_{h}$.</p>
<p>The main challenge of utilizing inductive thoughts for LLM is obtaining high-quality sentences matching relation triples. Here we provide two kinds of solutions to obtain inductive thoughts: (1) pre-trained model for open information extraction, such as OpenIE [34]; (2) In-context learning with few-shot prompts based on LLM. In this work, we utilize the second solution, i.e., utilizing LLM Agent $\mathcal{A}$ to generate inductive thoughts, as shown in Figure 3.
3.2.2 Hash-based Storage. We aim to save inductive thoughts into the memory following a certain rule, i.e., similar thoughts should be stored in the same group in the memory for efficiency. To this end, we adopt a hash table as the architecture of TiM's storage system, where similar thoughts are assigned with the same hash index.</p>
<p>Given a query, we propose to quickly search its nearest thoughts in a high-dimensional embedding space, which can be solved by the locality-sensitive hashing (LSH) method. The hashing scheme</p>
<h2>Prompt for Generating Thoughts</h2>
<p>Given the following question and response pairs, please extract the relation (subject, relation, object) with corresponding text:</p>
<h2>Example 1.</h2>
<p>Input:
Question: Do you have any company recommendations for me? Response: I recommend Google.
Output:
(Company, Recommended, Google).
Recommended company is Google.
Example 2.
Input:
Question: Which City is the capital of China?
Response: Beijing.
Output:
(China, Capital, Beijing).
The capital of China is Beijing.</p>
<h2>Input:</h2>
<p>Question: Do you have any book recommendations for me? Response: I recommend "The Little Prince". Output:</p>
<p>Figure 3: An example of prompts for generating thoughts.
of LSH is to assign each $d$-dimension embedding vector $x \in \mathbf{R}^{d}$ to a hash index $\mathbf{F}(x)$, where nearby vectors get the same hash index with higher probability. We achieve this by exploiting a random projection as follows:</p>
<p>$$
\mathbf{F}(x)=\arg \max ([x R ;-x R])
$$</p>
<p>where $R$ is a random matrix of size $(d, b / 2)$ and $b$ is the number of groups in the memory. $[u ; v]$ denotes the concatenation of two vectors. This LSH method is a well known LSH scheme [16] and is easy to implement. Figure 2 shows a schematic exhibition of TiM's storage system based on LSH.</p>
<h3>3.3 Retrieval for Memory Recalling</h3>
<p>Built on the memory storage, the memory retrieval operates a two-stage retrieval task for the most relevant thoughts, i.e., LSHbased retrieval followed by similarity-based retrieval. The paradigm involves the following detailed points.</p>
<ul>
<li>Stage-1: LSH-based Retrieval. For a new query $Q$, we first obtain its embedding vector $x$ based on LLM agent. Then LSH function (i.e., Eq. 1) can produce the hash index of the query. This hash index also indicates the its nearest group for similar thoughts in the memory cache according to the property of LSH.</li>
<li>Stage-2: Similarity-based Retrieval. Within the nearest group, we calculate the pairwise similarity between the query and each piece of thought in the group. Then top-k thoughts are recalled as the relevant history for accurately answering the query. It</li>
</ul>
<p>Given the following thoughts, please remove the counterfactual thoughts or contradictory thoughts:</p>
<p>Example 1.
Input:
The capital of China is Beijing.
The capital of China is Shanghai.
The capital of the United States is Washington.
The capital of the United States is New York.
Output:
The capital of China is Beijing.
The capital of the United States is Washington.
Example 2.
Input:
Michael likes to play football.
Michael does not like to play football.
James likes to swim.
Mary likes to read books.
Output:
James likes to swim.
Mary likes to read books.</p>
<p>Input:
[A group of thoughts]
Output:</p>
<p>Figure 4: An example of prompts for forgetting thoughts.
should be noticed that pairwise similarity is calculated within a group rather than the whole memory cache, which can achieve more efficient retrieval than previous memory mechanisms.</p>
<h3>3.4 Organization for Memory Updating</h3>
<p>With the above-discussed memory storage and retrieval, the longterm memory capability of LLMs can be well enhanced. Motivated by the human memory, there needs some organization principles based on the well-established operations for dynamic updates and evolution of the thoughts, e.g., insert new thoughts, forget less important thoughts, and merge repeated thoughts, which can make the memory mechanism more natural and applicable.</p>
<p>Beginning with the architecture of the storage for memory cache, TiM adopts the hash table to store the self-generated thoughts, where each hash index corresponds a group containing similar thoughts. Within same group, TiM supports the following operations to organize the thoughts in the memory:</p>
<ul>
<li>Insert, i.e., store new thoughts into the memory. The prompt for generating thoughts is shown in Figure 3.</li>
<li>Forget, i.e., remove unnecessary thoughts from the memory, such as contradictory thoughts. The prompt of this operation is shown in Figure 4.</li>
<li>Merge, i.e., merge similar thoughts in the memory, such as thoughts with the same head entity. The prompt of this operation is shown in Figure 5.</li>
</ul>
<p>Given the following thoughts, please merge the similar thoughts with the same entity:</p>
<p>Example 1.
Input:
John works as an actor.
John works as a director.
John works as a writer.
Mike works as a teacher.
Output:
John works as an actor, a director, and a writer.
Mike works as a teacher.
Example 2.
Input:
Michael likes to play football.
Michael likes to play basketball.
James likes to swim.
Mary likes to read books.
Output:
Michael likes to play football and basketball.
James likes to swim.
Mary likes to read books.
Input:
[A group of thoughts]
Output:</p>
<p>Figure 5: An example of prompts for merging thoughts.</p>
<p>Table 1: Organization comparisons between previous memory mechanisms and ours. KG denotes the knowledge graph and Q-R denotes the question and response pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Content</th>
<th style="text-align: center;">LLM-agnostic</th>
<th style="text-align: center;">Insert</th>
<th style="text-align: center;">Forget</th>
<th style="text-align: center;">Merge</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SCM [33]</td>
<td style="text-align: center;">Q-R</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">RelationLM [7]</td>
<td style="text-align: center;">KG</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">LongMem [10]</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">MemoryBank [32]</td>
<td style="text-align: center;">Q-R</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Ours (TiM)</td>
<td style="text-align: center;">Thoughts</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<h3>3.5 Parameter-efficient Tuning</h3>
<p>We adopt a computation-efficient fine-tuning approach called LowRank Adaptation (LoRA) [27] for the scenarios with limited computational resources. LoRA [27] optimizes pairs of rank-decomposition matrices while keeping the original weights frozen, which can effectively reduce the number of trainable parameters. Specifically, considering a linear layer defined as $y=W x$, LoRA fine-tunes it according to $y=W x+B A x$, where $W \in \mathbf{R}^{d \times k}, B \in \mathbf{R}^{d \times r}, A \in \mathbf{R}^{r \times k}$, and $r \ll \min (d ; k)$. Essentially, this fine-tuning stage can adapt LLMs to multi-turn conversations, providing appropriately and effectively response to users. For all experiments, we set LoRA rank $r$ as 16 and train the LLM models for 10 epochs.</p>
<h3>3.6 Insightful Discussion</h3>
<p>Here we make a summary for previous memory mechanisms and our method in Table 1, including memory content, LLM-agnostic, and organization operations. There are several important observations from Table 1: (1) Previous memory mechanisms only save raw conversation text (Q-R pairs) as the memory, which requires repeated reasoning over the history. Our method maintains thoughts in the memory cache and can directly recall them without repeated reasoning. (2) Previous memory mechanisms only support simple read and write (insert) operations, while our method provides more manipulate way for the memory. (3) Some previous memory mechanisms store the tokens in the memory, which requires adjusting LLM architecture (LLM-aware) for applications. Our method is deigned as a LLM-agnostic module, which can be easily combined with other LLMs.</p>
<h2>4 EXPERIMENT</h2>
<h3>4.1 Experimental Settings</h3>
<p>4.1.1 Dataset. Three datasets are used to demonstrate the effectiveness of the proposed method.</p>
<ul>
<li>KdConv: KdConv is a Chinese multi-domain knowledge-driven conversation benchmark [35] grounding the topics to knowledge graphs, which involves 4.5 K conversations and 86 K utterances from three domains (film, music, and travel). The average turn number is 19 .</li>
<li>Generated Virtual Dataset (GVD): GVD is a long-term conversation dataset [32] involving 15 virtual users (ChatGPT) over 10 days. Conversations are synthesized using pre-defined topics, including both English and Chinese languages. For the test set, [32] manually constructed 194 query questions ( 97 in English and 97 in Chinese) to evaluate whether the LLM could accurately recall the memory and produce the appropriate answers.</li>
<li>Real-world Medical Dataset (RMD): To evaluate the effectiveness of the proposed memory mechanism in the real-world scenarios, we manually collect and construct a dataset containing 1,800 conversations for medical healthcare consumer. For the test set, 80 conversations are used to evaluate whether the LLM could provide the accurate diagnosis.
4.1.2 LLM. We integrate two powerful LLMs to demonstrate the effectiveness of the proposed TiM mechanism. These LLMs originally lack long-term memory and specific adaptability to the long-term conversations. The detailed introduction of these LLMs are follows.</li>
<li>ChatGLM [17]: ChatGLM is an open-source bilingual language model based on the General Language Model (GLM) framework [17]. This model contains 6.2 billion parameters with specific optimization, involves supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback.</li>
<li>Baichuan2 [36]: Baichuan2 is an open-source large-scale multilingual language model containing 13 billion parameters, which is trained from scratch on 2.6 trillion tokens. This model excels at dialogue and context understanding.
4.1.3 Baseline. One baseline is to answer questions without using any memory mechanism. Another baseline is SiliconFriend [32], a
classical memory mechanism, which can store the raw text into the memory and support reading operation.
4.1.4 Evaluation Protocol. Following [32], three metrics are adopted to evaluate the performance of the proposed method.</li>
<li>Retrieval Accuracy evaluates whether the relevant memory is successfully recalled (labels: ${0:$ no; 1: yes $}$ ).</li>
<li>Response Correctness evaluates if correctly answering the probing question (labels: ${0:$ wrong; 0.5 : partial; 1 : correct $}$ ).</li>
<li>Contextual Coherence evaluates whether the response is naturally and coherently generated, e.g., connecting the dialogue context and retrieved memory (labels: ${0:$ not coherent; 0.5 : partially coherent; 1 : coherent $}$.
To be fair, during evaluation, the prediction results of all LLMs are firstly shuffled, ensuring the human evaluator does not know which LLM the results come from. Then the final evaluation results are obtained by the human evaluation.</li>
</ul>
<h3>4.2 Comparison Results</h3>
<p>4.2.1 Results on GVD dataset. We evaluate our method on both English and Chinese test sets of GVD dataset. The following insights are observed from Table 2: (1) Compared with SiliconFriend [32], our method exhibits superior performance for all metric, especially for the contextual coherence, indicating the effectiveness of TiM mechanism. (2) TiM delivers better results on both languages. The performance improvement on Chinese is larger than English, which may be attributed to the abilities of the LLMs.
4.2.2 Results on KdConv dataset. Table 2 illustrates the comparison results on KdConv dataset. We evaluate 2 different LLMs with TiM over different topics (film, music, and travel). As shown in Table 2 , it is observed that our method can obtain best results across all topics. Our method can achieve high retrieval accuracy to recall the relevant thoughts. When without the memory mechanism, these LLMs usually exhibit lower response correctness due to lack of long-term memory capability, while TiM can well eliminate such negative issue. Furthermore, TiM can also help to improve the contextual coherence of the response.
4.2.3 Results on RMD dataset. Table 2 reports the comparison results on RMD dataset, which contains the realistic conversations between the doctors and patients. As shown in Table 2, our method can improve the overall response performance for the real-world medical conversations. In detail, using TiM, both ChatGLM and Baichuan2 can improve their capability for long-term conversations, i.e., significant improvements on the response correctness and the contextual coherence. The main reason is that TiM is more similar to the workflow of human memory, which can enhance the ability of LLMs to produce more human-like responses.</p>
<h3>4.3 More Analysis</h3>
<p>4.3.1 Retrieval Time. We report the comparison results of retrieval time. The baseline is to calculate pairwise similarity between the question and the whole memory, which is utilized as the default retrieval way for most previous mechanisms. For both baseline and our method, the memory length is as 140 and the memory context is fixed. Table 2 shows the time cost for making a single retrieval. It</p>
<p>Table 2: Comparison Results on Three Datasets. Top-5 thoughts are recalled from the memory cache.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Language/Topic</th>
<th style="text-align: center;">Memory</th>
<th style="text-align: center;">Retrieval Accuracy</th>
<th style="text-align: center;">Response Correctness</th>
<th style="text-align: center;">Contextual Coherence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GVD</td>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">English/Open</td>
<td style="text-align: center;">SiliconFriend</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.680</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.735</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chinese/Open</td>
<td style="text-align: center;">SiliconFriend</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.428</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.665</td>
</tr>
<tr>
<td style="text-align: center;">Kdconv</td>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">Chinese/Film</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.923</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chinese/Music</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.910</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.926</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chinese/Travel</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.906</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.912</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baichuan2</td>
<td style="text-align: center;">Chinese/Film</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.413</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.870</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chinese/Music</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.283</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.780</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chinese/Travel</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.280</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.807</td>
</tr>
<tr>
<td style="text-align: center;">RMD</td>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">Chinese/Medical</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.893</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baichuan2</td>
<td style="text-align: center;">Chinese/Medical</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.538</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TiM (Ours)</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.663</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparisons of Retrieval Time. Baseline calculates pairwise similarity between the question and memory.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Retrieval Time (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.6287</td>
</tr>
<tr>
<td style="text-align: left;">Ours (TiM)</td>
<td style="text-align: center;">0.5305</td>
</tr>
</tbody>
</table>
<p>is observed that our method can reduce about 0.1 ms retrieval time compared with baseline method.
4.3.2 Top-k Recall. We report the retrieval accuracy using different values of $k$ on Kdconv dataset (Travel). As shown in Figure 6, top-1 retrieval accuracy is higher than 0.7 . The overall retrieval accuracy is improved with increasing value of $k$, where top- 10 can achieve 0.973 retrieval accuracy. Besides, as shown in Table 2, top-5 recall can significantly improve the performance of existing LLMs for long-term conversations.</p>
<h3>4.4 Industry Application</h3>
<p>In this section, based on the ChatGLM and TiM, we develop a medical agent (named TiM-LLM) in the context of patient-doctor
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Tendency of retrieval accuracy with different $k$.
conversations (as shown in Figure 7). Note that TiM-LLM is only an auxiliary tool for the clinical doctors to give treatment options and medical suggestions for patients' needs.</p>
<p>Figure 7 illustrates a real-world conversation between a patient and a doctor, where the clinical diagnosis results are given by the medical agent with and without TiM. As shown in Figure 7, without TiM, the medical agent may struggle to recall previous symptoms,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: The application of TiM. The left is the background of TiM-LLM application and the right is user interface.
resulting in incomplete or incorrect assessments (red part), i.e., the agent has forgotten previous symptoms so it is uncertain whether oral mucosal inflammation is the only cause. Assisted by TiM, the medical agent can recall relevant symptoms and make a comprehensive understanding of a patient's diseases. Thus it provide accurate diagnosis and treatment (bold part).</p>
<h2>5 CONCLUSION</h2>
<p>In this work, we propose a novel memory mechanism called TiM to address the issue of biased thoughts in Memory-augmented LLMs. By storing historical thoughts in an evolved memory, TiM enables LLMs to recall relevant thoughts and incorporate them into the conversations without repeated reasoning. TiM consists of two key stages: recalling thoughts before generation and post-thinking after generation. Besides, TiM works with the several basic principles to organize the thoughts in memory, which can achieve dynamic updates of the memory. Furthermore, we introduce Locality-Sensitive</p>
<p>Hashing into TiM to achieve efficient retrieval for the long-term conversations. The qualitative and quantitative experiments conducted on real-world and simulated dialogues demonstrate the significant benefits of equipping LLMs with TiM. Overall, TiM is designed as an approach to improve the quality and consistency of responses for long-term human-AI interactions.</p>
<h2>REFERENCES</h2>
<p>[1] OpenAI. Chatgpt. 2022.
[2] OpenAI. Gpt-4 technical report. 2023.
[3] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023.
[4] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint arXiv:2305.15075, 2023.
[5] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023.
[6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, 35:22199-22213, 2022.
[7] Qi Liu, Dani Yogatama, and Phil Blunsom. Relational memory-augmented language models. Transactions of the Association for Computational Linguistics, 10:555-572, 2022.
[8] Quentin Fournier, Gaétan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 55(14s):1-40, 2023.
[9] Jason Phang, Yao Zhao, and Peter J Liu. Investigating efficiently extending transformers for long input summarization. arXiv preprint arXiv:2208.04347, 2022.
[10] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174, 2023.
[11] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, pages 22062240. PMLR, 2022.
[12] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, volume 1, page 2, 2019.
[13] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.
[14] Xuezhi Wang, Jason Wei, Dale Schnurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowolhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[15] John Dunlosky and Janet Metcalfe. Metacognition. Sage Publications, 2008.
[16] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance. Advances in Neural Information Processing Systems, 28, 2015.
[17] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.
[18] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: A case study. arXiv preprint arXiv:2301.07069, 2023.
[19] Boyu Zhang, Hongyang Yang, Tianyu Zhou, Ali Babar, and Xiao-Yang Liu. Enhancing financial sentiment analysis via retrieval augmented large language models. arXiv preprint arXiv:2310.04027, 2023.
[20] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1086710877, 2023.
[21] Aakanksha Chowolhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
[24] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1-40, 2023.
[25] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164,</p>
<p>2019
[26] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506-17533. PMLR, 2023.
[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[28] Lian Meng and Minlie Huang. Dialogue intent classification with long short-term memory networks. In Natural Language Processing and Chinese Computing: 6th CCF International Conference, NLPCC 2017, Dalian, China, November 8-12, 2017, Proceedings 6, pages 42-50. Springer, 2018.
[29] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
[30] Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. arXiv preprint arXiv:2107.07567, 2021.
[31] Xinchao Xu, Zhihin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time no see! open-domain conversation with long-term persona memory. arXiv preprint arXiv:2203.05797, 2022.
[32] Wanjun Zhong, Lianghong Guo, Qiuj Gao, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.
[33] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.
[34] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, 2015.
[35] Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, and Xiaoyan Zhu. KdConv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. In ACL, 2020.
[36] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work was done when Lei Liu was a research intern at Ant Group.
${ }^{\dagger}$ Corresponding Author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1303 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1303</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1303</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-238215782</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.14516v2.pdf" target="_blank">On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a"proxy domain"is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1303.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1303.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Duckietown sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Duckietown Simulator (gym-duckietown)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A customizable simulator for small wheeled robots (Duckiebots) providing dynamics, rendering, and parameterizable sensors/actuators used as a proxy domain for lane-following and embodied driving tasks; supports domain randomization and integration with AI Driving Olympics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Duckietown simulator (gym-duckietown)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulates Duckiebot dynamics, environment maps, and camera rendering; provides tunable parameters (e.g. wheel trim, command delay, camera blur, camera pitch) and supports domain randomization and batch evaluation via a challenge server.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied navigation (mechanics, perception)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity (task-conditioned): simulates robot kinematics/dynamics and visual rendering with tunable parameters but not necessarily full high-fidelity physical phenomena (e.g. may omit some contact/slip phenomena).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes: approximate vehicle dynamics, camera rendering, configurable wheel trim, input/command delay, camera blur and camera pitch; supports randomizing parameters across runs. Likely omits or simplifies some phenomena (authors note e.g. wheel slip may be unmodeled) and noise models must be tuned to match target.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AI-DO lane-following controllers and an imitation-learning neural network (winner of AI-DO3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of controllers submitted to AI Driving Olympics (varied architectures) and a neural-network-based imitation learning agent used for lane following; learning agent initialized from sim-trained weights then fine-tuned on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied lane-following driving task: follow lane center, maximize distance traveled and survival time (60s cap); task-specific evaluation metrics used for predictivity and learning-value experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Qualitative: agents trained entirely on target domain converge to ~55s performance at convergence (mean score) with ~9000 annotated real-world training samples; sim-trained agents' zero-shot real-world scores vary by simulator parameterization (e.g., camera pitch), but pretraining in simulator reduced required real samples substantially. (No precise numerical training scores for simulator-only training reported in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real Duckietown robot runs (AIDO embodied challenge / Autolab robot evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: Zero-shot performance on real robot depended on simulator instance (e.g., 'sim ca19' had higher zero-shot than 'sim ca03' and 'sim ca35'); after fine-tuning agents pretrained in simulation, they achieved the target-domain convergence performance (~55s) with substantially fewer real-world samples than training from scratch (exact reduced sample counts reported in paper's Table I but not numeric in body text).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors varied simulator parameters (trim, command delay, blur time, camera angle) and compared PRPV and PLV: PRPV identified correct trim (0.0) and showed predictivity degrades with incorrect trim; command delay ~50 ms and camera angle used in AIDO were predictive; blur time required tuning (lower blur improved predictivity). For learning value (PLV), different camera angles yielded different zero-shot scores but pretraining from even poorly predictive simulators still substantially reduced real-data requirements — i.e., a simulator can be useful for learning even if not highly predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper argues for task-conditioned fidelity: only aspects important to the task need accurate modeling. They note some features (e.g. wheel slip) if unmodeled produce irreducible domain gap; thus minimum fidelity depends on the task metrics and can be tuned (via PRPV/PLV) rather than insisting on full physical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported issues include: incorrect noise/dynamics models causing rank inversion and overfitting to simulator glitches; non-modeled phenomena (example: wheel slip) cause persistent errors in predictivity; some simulator instances produced low zero-shot performance though still useful for reducing required real samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1303.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1303.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>multi-fidelity metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulator fidelity in multi-fidelity simulator environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced definition of 'simulator fidelity' used in a multi-fidelity RL training context, where fidelity of a simulator is defined by the maximum error in the optimal value function relative to a target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>multi-fidelity simulator environment (general concept, citation [28])</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Conceptual multi-fidelity setup where different simulators have associated fidelities and costs; fidelity measured by worst-case error in optimal value function.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / robotics (methodological)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>formal definition: fidelity quantified by maximum error in optimal value function between simulator and target (not an absolute 'high/low' label).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity characterized by how well optimal value functions match target; practical limitations: value function may be hard to compute in real world so definition inconvenient for simulator assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Used as a formal metric for RL training across simulators, not tied to a specific scientific reasoning task in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity simulator or real-world target (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Referenced as a prior approach: enables optimizing training by considering fidelity vs. data generation cost, but impractical when value functions are not measurable in target.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Discussion notes limitation: tying fidelity to value function is inconvenient in real-world assessment; authors instead favor task-conditioned, agent-agnostic metrics (PRPV/PLV).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not a direct experiment; criticism is conceptual (value-function-based fidelity impractical when target value function unknown).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1303.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1303.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson env v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied simulation environment for interactive navigation (referenced in related work) that provides visually realistic scenes for navigation and embodied AI research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson env v2</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A high-quality visual/embodied simulation environment designed for interactive navigation tasks with photorealistic scenes and physics for embodied agents (referenced, no experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied visual navigation / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>presented in literature as high visual fidelity for navigation tasks (paper only mentions the environment as a related work example).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Emphasizes realistic rendering and interactive navigation; specific fidelity details not expanded in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied visual navigation (mentioned as an example of simulation platform in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world navigation / embodied benchmarks (general, not reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1303.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1303.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gradsim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gradsim: Differentiable simulation for system identification and visuomotor control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable simulator referenced in related work that enables gradient-based system identification and end-to-end visuomotor control by backpropagating through simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>gradsim (differentiable simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Differentiable physics/graphics simulator designed to enable gradient-based optimization for system identification and visuomotor tasks; cited as an example of differentiable proxy domains.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / system identification / differentiable simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>varies; emphasized for differentiability to support gradient-based tuning rather than necessarily perfect physical fidelity (mentioned as future/related work enabling gradient optimization of proxy parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides differentiable simulation pipeline for dynamics and rendering enabling gradient-based optimization of simulator parameters; exact physical fidelity depends on implementation (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>System identification and visuomotor control tasks (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real robot or target domains via system identification (general concept, not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as an approach to optimize proxy parameters when differentiable simulators are available; authors note gradient-based methods could be applied in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1303.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1303.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Difftaichi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difftaichi: Differentiable programming for physical simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable programming framework for physical simulation referenced in related work, enabling differentiable simulations for learning and system identification purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Difftaichi (differentiable simulation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Framework for building differentiable physical simulators to support gradient-based optimization of simulator parameters and learning pipelines (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physical simulation / differentiable programming</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>framework-level; fidelity depends on constructed simulators; emphasis is on differentiability rather than one specific fidelity level.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Enables differentiable implementations of physical simulations; specifics not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>General physical simulation tasks useful for system identification and control (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>gradsim: Differentiable simulation for system identification and visuomotor control <em>(Rating: 2)</em></li>
                <li>Gibson env v2: Embodied simulation environments for interactive navigation <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 1)</em></li>
                <li>Sim-to-real via simto-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 1)</em></li>
                <li>Traversing the reality gap via simulator tuning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1303",
    "paper_id": "paper-238215782",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Duckietown sim",
            "name_full": "Duckietown Simulator (gym-duckietown)",
            "brief_description": "A customizable simulator for small wheeled robots (Duckiebots) providing dynamics, rendering, and parameterizable sensors/actuators used as a proxy domain for lane-following and embodied driving tasks; supports domain randomization and integration with AI Driving Olympics.",
            "citation_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
            "mention_or_use": "use",
            "simulator_name": "Duckietown simulator (gym-duckietown)",
            "simulator_description": "Simulates Duckiebot dynamics, environment maps, and camera rendering; provides tunable parameters (e.g. wheel trim, command delay, camera blur, camera pitch) and supports domain randomization and batch evaluation via a challenge server.",
            "scientific_domain": "robotics / embodied navigation (mechanics, perception)",
            "fidelity_level": "medium-fidelity (task-conditioned): simulates robot kinematics/dynamics and visual rendering with tunable parameters but not necessarily full high-fidelity physical phenomena (e.g. may omit some contact/slip phenomena).",
            "fidelity_characteristics": "Includes: approximate vehicle dynamics, camera rendering, configurable wheel trim, input/command delay, camera blur and camera pitch; supports randomizing parameters across runs. Likely omits or simplifies some phenomena (authors note e.g. wheel slip may be unmodeled) and noise models must be tuned to match target.",
            "model_or_agent_name": "AI-DO lane-following controllers and an imitation-learning neural network (winner of AI-DO3)",
            "model_description": "Set of controllers submitted to AI Driving Olympics (varied architectures) and a neural-network-based imitation learning agent used for lane following; learning agent initialized from sim-trained weights then fine-tuned on real data.",
            "reasoning_task": "Embodied lane-following driving task: follow lane center, maximize distance traveled and survival time (60s cap); task-specific evaluation metrics used for predictivity and learning-value experiments.",
            "training_performance": "Qualitative: agents trained entirely on target domain converge to ~55s performance at convergence (mean score) with ~9000 annotated real-world training samples; sim-trained agents' zero-shot real-world scores vary by simulator parameterization (e.g., camera pitch), but pretraining in simulator reduced required real samples substantially. (No precise numerical training scores for simulator-only training reported in text.)",
            "transfer_target": "Real Duckietown robot runs (AIDO embodied challenge / Autolab robot evaluations)",
            "transfer_performance": "Qualitative: Zero-shot performance on real robot depended on simulator instance (e.g., 'sim ca19' had higher zero-shot than 'sim ca03' and 'sim ca35'); after fine-tuning agents pretrained in simulation, they achieved the target-domain convergence performance (~55s) with substantially fewer real-world samples than training from scratch (exact reduced sample counts reported in paper's Table I but not numeric in body text).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Authors varied simulator parameters (trim, command delay, blur time, camera angle) and compared PRPV and PLV: PRPV identified correct trim (0.0) and showed predictivity degrades with incorrect trim; command delay ~50 ms and camera angle used in AIDO were predictive; blur time required tuning (lower blur improved predictivity). For learning value (PLV), different camera angles yielded different zero-shot scores but pretraining from even poorly predictive simulators still substantially reduced real-data requirements — i.e., a simulator can be useful for learning even if not highly predictive.",
            "minimal_fidelity_discussion": "Paper argues for task-conditioned fidelity: only aspects important to the task need accurate modeling. They note some features (e.g. wheel slip) if unmodeled produce irreducible domain gap; thus minimum fidelity depends on the task metrics and can be tuned (via PRPV/PLV) rather than insisting on full physical fidelity.",
            "failure_cases": "Reported issues include: incorrect noise/dynamics models causing rank inversion and overfitting to simulator glitches; non-modeled phenomena (example: wheel slip) cause persistent errors in predictivity; some simulator instances produced low zero-shot performance though still useful for reducing required real samples.",
            "uuid": "e1303.0",
            "source_info": {
                "paper_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "multi-fidelity metric",
            "name_full": "Simulator fidelity in multi-fidelity simulator environment",
            "brief_description": "A referenced definition of 'simulator fidelity' used in a multi-fidelity RL training context, where fidelity of a simulator is defined by the maximum error in the optimal value function relative to a target domain.",
            "citation_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
            "mention_or_use": "mention",
            "simulator_name": "multi-fidelity simulator environment (general concept, citation [28])",
            "simulator_description": "Conceptual multi-fidelity setup where different simulators have associated fidelities and costs; fidelity measured by worst-case error in optimal value function.",
            "scientific_domain": "reinforcement learning / robotics (methodological)",
            "fidelity_level": "formal definition: fidelity quantified by maximum error in optimal value function between simulator and target (not an absolute 'high/low' label).",
            "fidelity_characteristics": "Fidelity characterized by how well optimal value functions match target; practical limitations: value function may be hard to compute in real world so definition inconvenient for simulator assessment.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Used as a formal metric for RL training across simulators, not tied to a specific scientific reasoning task in this paper.",
            "training_performance": null,
            "transfer_target": "Higher-fidelity simulator or real-world target (general concept)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "Referenced as a prior approach: enables optimizing training by considering fidelity vs. data generation cost, but impractical when value functions are not measurable in target.",
            "minimal_fidelity_discussion": "Discussion notes limitation: tying fidelity to value function is inconvenient in real-world assessment; authors instead favor task-conditioned, agent-agnostic metrics (PRPV/PLV).",
            "failure_cases": "Not a direct experiment; criticism is conceptual (value-function-based fidelity impractical when target value function unknown).",
            "uuid": "e1303.1",
            "source_info": {
                "paper_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Gibson v2",
            "name_full": "Gibson env v2",
            "brief_description": "An embodied simulation environment for interactive navigation (referenced in related work) that provides visually realistic scenes for navigation and embodied AI research.",
            "citation_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
            "mention_or_use": "mention",
            "simulator_name": "Gibson env v2",
            "simulator_description": "A high-quality visual/embodied simulation environment designed for interactive navigation tasks with photorealistic scenes and physics for embodied agents (referenced, no experiments here).",
            "scientific_domain": "embodied visual navigation / robotics",
            "fidelity_level": "presented in literature as high visual fidelity for navigation tasks (paper only mentions the environment as a related work example).",
            "fidelity_characteristics": "Emphasizes realistic rendering and interactive navigation; specific fidelity details not expanded in this paper.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Embodied visual navigation (mentioned as an example of simulation platform in related work).",
            "training_performance": null,
            "transfer_target": "Real-world navigation / embodied benchmarks (general, not reported here)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1303.2",
            "source_info": {
                "paper_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "gradsim",
            "name_full": "gradsim: Differentiable simulation for system identification and visuomotor control",
            "brief_description": "A differentiable simulator referenced in related work that enables gradient-based system identification and end-to-end visuomotor control by backpropagating through simulation.",
            "citation_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
            "mention_or_use": "mention",
            "simulator_name": "gradsim (differentiable simulator)",
            "simulator_description": "Differentiable physics/graphics simulator designed to enable gradient-based optimization for system identification and visuomotor tasks; cited as an example of differentiable proxy domains.",
            "scientific_domain": "robotics / system identification / differentiable simulation",
            "fidelity_level": "varies; emphasized for differentiability to support gradient-based tuning rather than necessarily perfect physical fidelity (mentioned as future/related work enabling gradient optimization of proxy parameters).",
            "fidelity_characteristics": "Provides differentiable simulation pipeline for dynamics and rendering enabling gradient-based optimization of simulator parameters; exact physical fidelity depends on implementation (not detailed here).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "System identification and visuomotor control tasks (related work mention).",
            "training_performance": null,
            "transfer_target": "Real robot or target domains via system identification (general concept, not reported here).",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Mentioned as an approach to optimize proxy parameters when differentiable simulators are available; authors note gradient-based methods could be applied in future work.",
            "failure_cases": null,
            "uuid": "e1303.3",
            "source_info": {
                "paper_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Difftaichi",
            "name_full": "Difftaichi: Differentiable programming for physical simulation",
            "brief_description": "A differentiable programming framework for physical simulation referenced in related work, enabling differentiable simulations for learning and system identification purposes.",
            "citation_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
            "mention_or_use": "mention",
            "simulator_name": "Difftaichi (differentiable simulation framework)",
            "simulator_description": "Framework for building differentiable physical simulators to support gradient-based optimization of simulator parameters and learning pipelines (cited as related work).",
            "scientific_domain": "physical simulation / differentiable programming",
            "fidelity_level": "framework-level; fidelity depends on constructed simulators; emphasis is on differentiability rather than one specific fidelity level.",
            "fidelity_characteristics": "Enables differentiable implementations of physical simulations; specifics not discussed in this paper.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "General physical simulation tasks useful for system identification and control (related work mention).",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1303.4",
            "source_info": {
                "paper_title": "On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "gradsim: Differentiable simulation for system identification and visuomotor control",
            "rating": 2,
            "sanitized_title": "gradsim_differentiable_simulation_for_system_identification_and_visuomotor_control"
        },
        {
            "paper_title": "Gibson env v2: Embodied simulation environments for interactive navigation",
            "rating": 2,
            "sanitized_title": "gibson_env_v2_embodied_simulation_environments_for_interactive_navigation"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real via simto-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 1,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Traversing the reality gap via simulator tuning",
            "rating": 2,
            "sanitized_title": "traversing_the_reality_gap_via_simulator_tuning"
        }
    ],
    "cost": 0.012957999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</p>
<p>Anthony Courchesne 
Andrea Censi 
Liam Paull 
On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents</p>
<p>In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a "proxy domain" is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.</p>
<p>I. INTRODUCTION</p>
<p>Developing and evaluating agents for physically embodied systems such as robots in the setting that they are meant to be deployed in can be costly, dangerous, and time consuming. As a result, it is often desirable to have some proxy of the target task domain, which can be a simulation environment or some simpler smaller scale real environment. The fidelity or accuracy of this proxy domain is important in the sense that we need it to be a faithful predictor of performance on the target domain.</p>
<p>Recent breakthroughs in machine learning have increased the popularity of data-driven approaches to solve tasks with embodied agents. Two prevalent paradigms include reinforcement [1] and imitation [2] learning. As a result, the value of data has increased dramatically. Obtaining data from robots is costly since it requires deployment of hardware, operation of the robot and time-consuming rollouts of policies. Moreover, the policy exploration process might end up in hardware damage while unsafe states are explored. This further motivates the need for a proxy domain, but the objectives here are somewhat different. In this case we want Fig. 1: How do we compare the value of two different instantiations of the same task? This is a common setting in robotics where it is easier to develop and evaluate on a simulator or proxy domain before deploying on real hardware (the target domain). If we start the agent at the same state in both domains and execute the same sequence of control signals, we will get different output measurements due to discrepancies in dynamics, appearance, rendering, and many other factors. We argue that directly comparing these observations is not an appropriate measure of the proxy's value, since it does not consider the consequence of the actions under the task specification. Instead, we propose measures for assessing the value quantitatively both in terms of how useful the proxy domain is in terms of predictivity and as a tool for learning agents.</p>
<p>to efficiently acquire knowledge in the proxy domain that can be transferred to the target domain.</p>
<p>There may be other advantages of the proxy domain for the purposes of agent learning, such as: the availability of ground-truth data such as robot localization, reduced cost of data collection, ability to generate and randomize large numbers of samples, customization of scenarios to facilitate the learning of edge cases, parallelization of the data generation process, and others.</p>
<p>Any domain used as a proxy for a target domain introduces a set of discrepancies, often resulting in performance reduction. The "reality gap" is a term that is typically used in the context of sim-to-real, although the concept can be generalized to any domain transfer. For this reason, we will instead use the term "domain gap" as a more generic case. The domain gap has been observed in multiple works, and the community has found multiple ways to mitigate its effects (i.e. "crossing the gap"), without actually explicitly quantifying it. As a result, algorithms that show good performance in some settings (e.g. when the domain discrepancy is small) may completely fail in others, and there are no methods of predicting transferability a priori.</p>
<p>In this work, we quantify the usefulness of a proxy domain by providing proxy usefulness (semi)metrics. We make a clear distinction between proxies used to predict task performance (used to select agents according to their performance) and data-generating proxies (used to generate data samples or policies). In the first case, we propose the Proxy Relative Predictivity Value (PRPV), a metric to quantify the predictivity of a proxy, enabling researchers to find the most predictive proxy available to them. We also prescribe a metric to assess the usefulness of a proxy to generate learning data or trained agents, giving the possibility to researchers to compare different data-generating domains and select the one that yields the best agents. Our new metrics allow robotics practitioners to tune some parameters of their proxy domain for which system identification is non-trivial and ground truth value for the target domain is often not available, such as observation blur, input delay, field of view, etc.</p>
<p>II. RELATED WORK</p>
<p>In the most common configuration, the source domain is a simulator and the target domain is a real robot, in which case the policy transfer is called "sim-to-real". However, it is also frequent to see "sim-to-sim" ([3]- [5]), or even "real-toreal", such as in the case of [6] where small, cheap robots are used to represent an expensive car, or in [7] where authors used real-to-real to validate their sim-to-real performance. Our analysis of the domain discrepancies applies to all of these cases.</p>
<p>A. Crossing the Domain Gap</p>
<p>Multiple techniques exist to mitigate the effects of the domain gap. One such promising technique that has received a lot of attention lately is domain randomization [8]: during training, a different proxy is sampled from a domain family (see III) for each run, preventing the model from overfitting to a specific instance. In the best-case scenario, the target domain will appear as another sample from the same distribution. Data augmentation is another popular solution [9]- [11] where a model is trained to augment data generated by a proxy to match that of the target domain. Meta-learning and domain adaptation are also often used to transfer the knowledge acquired in a source domain to a different domain [12,13]. The authors in [14,15] show that it is possible to find which controllers are most likely to transfer well to the target setup using only a few sample experiments on the real robot, a method they call "The transferability approach". Crossing the domain gap was also shown to be possible by learning a common representation for both source and target domains, either by imposing a common state representation using an autoencoder and a disciminator network [16] or by forming weakly aligned pairs of source and target data, effectively transferring the annotations to the target images [17].</p>
<p>B. Optimizing Proxies</p>
<p>In our work, we are more interested in the different ways to analyze the domain gap and improve the proxy rather The agent/environment interface for proxy (simulation) and the target (real robot) domains. An agent receives observations and generates control commands. The predictive value of a simulator lies in its ability to faithfully reproduce an estimate of the task performance. A simulator may also be used in a learning paradigm. In this case, the value of the simulator lies in how many fewer trials we need to perform on the real robot to achieve equivalent performance.</p>
<p>than circumvent its effects. The first step to reduce the gap imposed by a proxy is to do proper system identification [18]. For a specific domain pair, it is possible to carry out standardized tests to find inconsistencies between models [19]. It is common [20]- [22] to use a task performance metric to optimize proxy parameters such as actuator gains, masses, static friction, etc. Recent papers have also shown success in improving their proxies by adding a module that learns inverse dynamics to adapt the action that the proxy outputs to the expected action in the target domain [23,24].</p>
<p>C. Quantifying Domain Discrepancies</p>
<p>Being able to quantify the domain gap may lead to better prediction of the transferability of agents. As such, multiple attempts have been made by the community to obtain a metric to quantify this gap. One promising work in that direction establishes the sim-to-real disparity: a metric specific to an agent that predicts how well it will transfer to the real robot [25]. By conducting only a few experiments both on the real robot and in simulation, it is possible to learn a surrogate model that will estimate the fitness function on the real robot via interpolation. However, it is assumed that we have access to a quick way to compute the distance between two controllers in a domain such that similar controllers will have a similar transferability (e.g. behavioral features), which is often not the case. Moreover, we are interested in a way to quantify the domain gap at the level of a simulator instead of at the agent level. We believe that the domain gap should be agnostic to the agent.</p>
<p>The "ν-gap" [26] is a metric used in robust control theory to analyze the discrepancy between two feedback control system. It has been used [27] to quantify the discrepancy between robotic systems without any knowledge about their dynamics since it compares two different controllers in a black-box fashion. The metric is computed by measuring the largest chordal distance between the two controllers projected onto the Riemann sphere. It is then possible to minimize it to find the most representative domain. This metric has the limitation of comparing only the dynamics and being only available for linear systems.</p>
<p>A version of "simulator fidelity" has been used as a metric in a multi-fidelity simulator environment [28] to define the fidelity of a domain Σ i to a target domain Σ j . The fidelity is defined by the maximum error in the optimal value function. Using this metric, it is possible to optimize the training of reinforcement-learning-based agents using multiple simulators, taking into account the cost of generating data for high-fidelity simulators. On the other hand, this definition is inconvenient when used to assess a simulator since the value function is not always trivially computed, especially in the real world. We believe that the definition of the domain gap should not be tightly coupled with machine learning, and it should be independent of the algorithm used.</p>
<p>In [29], the authors were able to create a high-fidelity virtual domain for their task using real-to-sim. Given agents and a task, they used the Pearson correlation coefficient between runs on the real robot and in simulation to evaluate the reality gap. This allowed them to find that the noise model for their dynamics was wrong and that learningbased agents were overfitting to the simulator domain by using glitches. In their work, they frequently refer to rank inversion to validate their empirical results, which is an idea that is also used in [30]. Like the former, we support that the predictive ability of a simulator lay in its capacity to predict the performance ordering of the agents. In our work, we further define the domain gap based on rank inversion. Using a relative metric for the reality gap allows us to use multiple performance metrics to get a domain gap measure through partial ordering. We additionally propose a separate and orthogonal measure of a simulator's value for learning.</p>
<p>III. PRELIMINARIES</p>
<p>We will consider a domain to encapsulate the actuation, environment and sensing, as shown in Fig. 2. For a simulated domain, those components translate to a simulation of the dynamics, an environment model and rendering, respectively. The various components in the domain may include tunable parameters, θ. A domain may also optionally provide a scalar reward r ∈ R ⊆ R signal, where a reward function maps an action and the internal environment state to a scalar value. Note that it is entirely possible that a reward can be provided in the proxy domain but not in the target domain since we don't have direct access to the internal states needed to calculate it. Consequently, a domain S ∈ S can be considered as something that maps control commands u ∈ U to observations z ∈ Z, conditioned on some environment model state x ∈ X env and parameters θ ∈ Θ (S : U × X env × Θ → Z × R). We will refer to a single domain generated by a specific set of parameters as a domain instance S θ and the set of all domains that are possible to achieve by varying the parameters as a domain family, S Θ .</p>
<p>A task, T ∈ T , is specified through one or more evaluation metrics, M , which map a trajectory of N states to a real-valued number:
T {M i } m i=1 , M i : X N → R
(1) An agent 1 contains the algorithm that is used to generate control commands from the history of observations, the history of control commands, some initial state x 0 ∈ X agent (at time t, A : Z t−1 × U t−1 × X agent → U, or if the state is assumed to be Markovian then A : Z × X agent → U). Presumably, the algorithm informing this agent is designed to optimize the specified task evaluation metrics. A learning agent is able to adapt its behaviour over time by means of a learning algorithm (A at time k is not necessarily the same as A at time k + 1). However, we assume here that, for a stationary domain and task, the learning agent will converge to a stationary agent for some k large enough.</p>
<p>Given a sequence u 0:n of control values (generated by any means), a domain instance S θ can generate a dataset D S θ , conditioned on an initial state x 0 , which is a collection of n labelled data samples each consisting of a tuple of observation z and command u (
D S θ = {(z i , r i , u i )} n i=1 where (z i , r i ) = S θ (x env i , u i )).
It is also possible that the parameters, θ, are randomized over the domain family during dataset generation. In this case we have
D SΘ = {(z i , r i , u i )} n i=1 where (z i , r i ) = S θ (x env i , u i ) and θ ∼ Θ.
Such datasets are useful as demonstrations for the learning agent, for example in imitation learning algorithms [31] which try to reproduce the behaviour of an expert.</p>
<p>In an on-policy reinforcement learning setting, the control actions are selected at every timestep:
u i = A(z i , x agent i ).
The domain will update its state according to the command and will return the next observation and optionally a reward to the agent. Normally the agent will be able to use the reward to update its algorithm. Again the on-policy rollouts can be executed on the same domain instance or on random samples from the simulator family (as is the case in domain randomization [8]) 2 .</p>
<p>Our objective in this work is to provide measures to quantify the usefulness of a proxy domain. We will use the term "Proxy Usefulness (PU)" to quantify the value of a proxy domain.</p>
<p>A. A Naive Measure of Proxy Usefulness</p>
<p>Naively, one could define the PU of a proxy domain by its discrepancies with the target domain it is expected to represent. Following, the usefulness of a proxy domain would be inversely proportional to its domain gap with the target domain and, according to (Fig. 2), would be defined by:</p>
<p>Def. 1 (Proxy Observation Discrepancy (POD)). A proxy is deemed more useful inasmuch as the "difference" in the resulting observations produced by the proxy's and the target's robots for a predefined sequence of control commands is small.</p>
<p>For a sequence of control values u 0..n and initial state, this could simply be calculated as:
||S proxy θ (x env i , u i ) − S target θ (x env i , u i )||
(2) where in this case we are primarily concerned with the observations that are output and not the rewards.</p>
<p>There are several issues with Def. 1: 1) It combines in an opaque way the various sources of the discrepancy. Referring to Fig. 2, there could be a "gap" in the dynamics, the environment model (for example how other agents move in the environment), or in the generation of sensor data based on a rendering model. Moreover, errors in upstream models will compound. 2) It is agnostic to the task that the agent is trying to solve.</p>
<p>Many proxies should be deemed perfectly faithful if a trivial task is chosen, but that will not be the case here.</p>
<p>3) It presupposes that the fidelity in the target domain is needed. In practice, we only require a form of taskconditioned fidelity: only the things that are important to solve the task at hand must be faithfully reproduced in the proxy. 4) It in no way represents how useful the proxy is for learning. In the remainder of this paper, we offer alternative ways to evaluate the PU to address these issues.</p>
<p>We make a clear distinction between two different uses of a proxy: 1) The predictivity value of a proxy domain (given a target domain) is its ability to generate accurate predictions about the performance of agents in the target domain. The teaching value of a proxy domain encapsulates how useful it is as a tool to train learning agents that perform well on the target domain.</p>
<p>IV. THE PROXY AS A PREDICTOR</p>
<p>The first "value" of a proxy domain is as a tool to predict. However, different from Def. 1, we argue that the domain's ability to predict task performance rather than exact observations is what is relevant:</p>
<p>Def. 2 (Proxy Predictivity Value (PPV)). Given a task T defined by evaluation metrics M 1:m , and an agent A that generates trajectory x 1:N proxy in the proxy domain and x 1:N target in a target domain given equivalent starting conditions, then we define the PPV of a proxy S θ as the discrepancy of the resulting evaluation metrics:
PPV(S θ , A) m i=1 β i |M i (x 1:N proxy ) − M i (x 1:N target )|(3)
where the β i terms are weighting constants that can account for mismatched units or possibly increased importance of one metric over another. A proxy is deemed more useful if it has a lower PPV.</p>
<p>By Def. 2, a proxy domain can be considered perfectly faithful to a target domain for a given task if the PPV is zero for all possible agents. This definition is in some sense a generalization of the Sim-vs-Real Correlation Coefficient (SRCC) [33] to the case where there are multiple metrics that define the task, except with a 1-norm distance instead of the bivariate correlation.</p>
<p>The need for the β constants in Def. 2 is undesirable since it allows some room for subjectivity that can effect the results. This can be avoided by considering that in many cases we are interested in comparing agents rather than finding exact evaluations of the metrics. As a result, we can consider a relaxation of Def. 2 to the relative case. Given that a task may contain several evaluation metrics, agents can be arranged in a partial ordering whose binary relation ≤ is defined by dominance along all of the available metrics:
A 1 ≥ A 2 → M i (X 1 ) ≥ M i (X 2 ) ∀i(4)
where X j is shorthand for the trajectory produced by agent A j (either in the proxy or in target domain).</p>
<p>Def. 3 (Proxy Relative Predictivity Value (PRPV)). Given K agents, the relative predictive ability of a proxy S θ is defined by its ability to accurately predict the binary relations between agents that would be present in the target domain.</p>
<p>Let A proxy = [α proxy ij ] i,j=1..K be a matrix whose entries are given by:
α proxy ij =      1 A proxy i ≥ A proxy j 0.5 A proxy i A proxy j &amp; A proxy j A proxy i 0 A proxy i ≤ A proxy j (5) where A proxy j (A proxy i )
is agent j (i) applied to the proxy domain. We similarly construct A target . Then the PRPV of a proxy domain is given by the 1-norm between the two matrices that represent the relations in the two partial orders [34]:
PRPV(S θ , A 1:K ) = K i,j=1 |α proxy ij − α target ij |(6)
According to Def. 3, a proxy domain is now perfectly faithful if it produces the identical partial order over agents that would be produced if the agents were run on the real robot. This is closely related to the concept of "rank inversion" [35].</p>
<p>Note that in the case of both PPV and PRPV, the value of the domain is conditioned on the task and agnostic to the agent (only requires some method of generating trajectories). Also note that in practice the performance of the agent in a simulated domain or (especially) in the real domain will be stochastic and therefore PPV and PRPV should be redefined as metrics over distributions and approximated by sequences of trials, but we omit this here for clarity.</p>
<p>V. THE PROXY AS A TEACHER</p>
<p>Orthogonal from the proxy domain's predictivity, it may have value as a tool for agents that learn (III). That proxy domain now becomes a part of the agent generation process since, as shown by the dashed lines in Fig. 2, the task performance may be fed back to the agent. We can assess the usefulness of the proxy domain by evaluating the performance of the agents that it trains on the target domain, compared to agents that learn entirely on the target domain. A proxy domain is deemed more valuable if it reduces the number of trials that are needed in the target domain. One naive option to evaluate a domain transfer method would be to consider the zero-shot (or N-shot) performance on the target domain. However, similar to the argument we made in Def. 2, the outputs of these metrics may not calibrate well to the actual learning that has taken place. Instead, we define the usefulness for learning explicitly as what we are trying to minimize through using the simulator for training: the number of trials on the target domain required to achieve equivalent performance as we would achieve if we had not pre-trained in the proxy domain. 
PLV = |D target | − |D proxy→target |(7)
Note here that, in contrast to the predictivity, the usefulness of the proxy as a teacher is conditioned on both the task and the learning algorithm used to train the agent. In addition, different learning algorithms may leverage the proxy domain in different ways. Behavior cloning methods may generate a dataset that includes expert trajectories from both domains [36]. In this case the usefulness is determined by the reduction in the size of the dataset, D target from the target domain (an example we demonstrate in Sec. VII-B). As noted in Sec. III, the proxy domain dataset can be generated from a single domain instance or over the domain family for increased robustness. In either case, the definition of the usefulness value is unchanged.</p>
<p>In reinforcement learning, the agent is learning through interaction with the environment [37]. In this case the data is generated through trials and this number of trials is what we seek to minimize. Again, it is entirely possible that the training episodes in the proxy environment randomize over the domain family, as is the case in domain randomization [8].</p>
<p>In some cases, both off-policy data and on-policy rollouts may be used. Such is the case in approaches that leverage domain adversarial transfer [16,38]. Off-policy data is used to learn the mapping from proxy domain to the source domain (for example using a discriminator). At test time, this mapping is applied but then fine-tuning is achieved with on-policy trials. A generalization of the PLV may consider these two cases (on-policy and off-policy) as being scaled differently. A natural priority would be to minimize the number of on-policy trials on the target domain at the expense of off-policy data, which may be easier and safer to obtain. As such we can generalize (7) to:
PLV = η on (|D target on | − |D proxy→target on |) + η of f (|D target of f | − |D proxy→target of f |)(8)
where η on and η of f represent the relative importance of off and on-policy data.</p>
<p>VI. OPTIMIZING THE PROXY</p>
<p>Using any of the proxy domain metrics defined above, we can optimize the set of parameters θ of the proxy instance.</p>
<p>In the case of predictivity, we desire to minimize the PRPV:
θ * (A 1:K ) = arg min θ PRPV(S θ , A 1:K )(9)
which will yield the parameters that are most predictive of the task metrics. The residual of this optimization defines the irreducible domain gap and is due to the fact that the proxy models may be limited in their capacity. For example, if wheel slip is not modeled in the proxy domain but it exists in the target domain, we will always incur error.</p>
<p>In the case of teaching, we desire to maximize the PLV. It is possible that we wish to find the single best proxy instance, but more recent methods leverage randomization for increased robustness. In this case, we may wish to optimize for the optimal distribution or sequence of domain instances (e.g., as in [32]), a problem that can be formulated as curriculum learning:
p(θ) * = arg max p(θ) PLV(θ)(10)
or possibly over the domain itself:
S * = arg max S PLV(S)(11)
These optimization may be solved with gradient-based methods in the case that the proxy domain is differentiable [39,40]. Evaluating the loss function may incur significant cost, however, particularly in the case of the PLV where training an agent in the proxy domain and evaluating it in the target domain is needed. For these cases, an approach such as Bayesian Optimization or some approximation may be more appropriate [41].</p>
<p>VII. EXPERIMENTS</p>
<p>We evaluate the proposed metrics in the context of Duckietown [6], a platform to run vehicle controllers both in a simulator and on a real robot seamlessly. The platform offers a simulated domain ( [42]) where domain randomization is available, as well as multiple customizable features (see Fig. 5). A challenge server is in place such that the users can easily submit a Docker container to the server, which will evaluate it and returns footage of the runs as well as performance metrics. Duckietown now additionally offers the evaluation of runs on the real robot at the Autolab through the challenge server [43]. The Autolab is an embodied domain monitored by watchtowers. As such, users can receive the ground truth position of the robot at each timestamp as well as a precise trajectory of the performance of their agent. </p>
<p>A. Duckietown Simulator Predictivity</p>
<p>We collected 10 agent submissions from a lane-following challenge from the AI Driving Olympics (AIDO) [44]. Each submission consist of a controller that is designed to drive a Duckiebot along a lane, following the center as closely as possible. The environment configuration in our experiments was a simple 3-by-3 Duckietown map, in which every run would last 60 seconds or until the robot crashes. For a given Duckiebot trajectory, the task is specified through two performance metrics: M 1 : distance traveled along a lane and M 2 : survival time, capped at 60 seconds. Since each agent had multiple runs in both the proxy and target domain, the metrics reported are actually means of multiple entries 3 .</p>
<p>The target domain was set to be the embodied lane following AIDO domain, while the proxy domains were instances of the Duckietown simulator [42]. We selected a list of parameters consisting of trim value, which controls the wheel trim, command delay, which simulates latency in the control loop, blur time, which is used to simulate camera blur, and camera angle, which is the pitch angle of the camera. For some of these parameters (namely the trim value and camera angle) we know the ground truth value on the real Duckiebot. In the case of the trim value, the robot is made to go straight using an odometric calibration procedure, therefore the ground truth value in the simulator should be 0.0. For the camera angle, we can determine it though extrinsic camera calibration.</p>
<p>As a result, we can verify that our metrics are correct since the best predictivity should correspond to the ground truth value. We investigate this for the case of trim in Fig. 3, and compare the scores for SRCC (Fig. 3c) [29], POD (Fig. 3d), PPV (Fig. 3b) and our proposed PRPV (Fig. 3a). We can see from the plots that the PRPV identifies the correct trim value, and also shows monotonic decrease in predictivity as the trim value increases. By writing our metric in terms of agent ordering explicitly we are able to capture the essence of how well the agents are able to perform the task.</p>
<p>We apply a similar approach to the other tunable parameters. For each simulator instance, we report the PRPV in fig 4 for three different parameter values centered on the ones that were actually used in the recent AIDO competition.</p>
<p>As expected, the results in fig. 4 show that having a non-zero trim value highly reduces the predictivity of our 3 The results can be found at https:// challenges-stage.duckietown.org/humans/challenges/ anc-01-LF-sim-validation/leaderboard Fig. 4: Evaluation of different instances of gym duckietown as a proxy for the AIDO embodied challenge according to the PRPV. The green bars represent the parameter value that was used for AIDO. Using our metric we can now determine which of our parameters were correct and which should be modified.</p>
<p>simulator. We also observe that the command delay that was initially set to 50 ms seems to be good, same for the camera angle. However, in the case of the blur time, our simulator could make better predictions if we decrease the parameter value. To obtain the optimal simulator instance, a second iteration of that experience could be repeated, using the new information to choose the range of the parameters to explore.</p>
<p>B. Duckietown Simulator for Learning</p>
<p>To compute the PLV, we collected annotated data from an expert that has access to ground truth both in the proxy and target domains ( fig. 5). We then used the algorithm of the controller that placed first during AI-DO3 [45], which is based on imitation learning [36], to obtain a learningbased agent capable of following a Duckietown lane. Again, the environment configurations was set to be a 3-by-3 Duckietown map, where the agent has to drive within the outer lane for 60 seconds. We note the performance of an agent by the amount of time it can drive, where each second spent with one wheel outside the lane is subtracted and each second spent with both wheels outside the lane is subtracted twice. If the agent drives out of the map entirely, the run ends.</p>
<p>Computing the PLV requires comparison of the performance of an agent with and without the proxy domain. We trained the neural network based on various amount of real world samples, each time deploying on the real robot and observing its performance. We observed that  the agents converge at a performance of around 55 seconds score (that is, they survive the whole 60 seconds run and get around 5 seconds of penalty), which was obtained with 9000 annotated data points from the target domain. Afterwards, we performed the same experiment, but we initialized the weights of the network from a neural network trained entirely with data from a simulator-based proxy domain.</p>
<p>The experiment was done for three different proxy instances where we modified the angle of the simulated camera to 3°, 19°and 35°, respectively. The learning curves for the four cases are shown in Fig. 6 and results are presented in Table I. We see here that the zero-shot score on the target domain is not a representative value for how useful the simulator is for learning. The zero-shot scores for 'sim ca03' and 'sim ca35' are very low compared to 'sim ca19'. However, they are both still very useful since they dramatically reduce the amount of target domain data needed compared to training entirely on the target domain. We also see here that these two different uses of a simulator (predictivity and teaching) may tell us different things. A simulator that is not particularly predictive (since the camera pitch angle is way off) may still be useful for learning if the learning algorithm is able to adapt properly.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>We introduce new metrics to assess the usefulness of proxy domains for agent learning. In a robotics setting it is common to use simulators for development and evaluation to reduce the need to deploy on real hardware. We argue that it is necessary to to take into account the specific task when evaluating the usefulness of the the proxy. We establish novel metrics for two specific uses of a proxy. When the proxy domain is used to predict performance in the target domain, we offer the PRPV to assess the usefulness of the proxy as a predictor, and we argue that the task needs to be imposed but not the agent. When a proxy is used to generate training data for a learning algorithm, we propose the PLV as a metric to assess usefulness of the source domain, which is dependent on a specific task and a learning algorithm. We demonstrated the use of these measures for predicting parameters in the Duckietown environment. Future work will involve more rigorous treatment of the optimization problems posed to find optimal parameters, possibly in connection with differentiable simulation environments.</p>
<p>Fig. 2 :
2Fig. 2: The agent/environment interface for proxy (simulation) and the target (real robot) domains. An agent receives observations and generates control commands. The predictive value of a simulator lies in its ability to faithfully reproduce an estimate of the task performance. A simulator may also be used in a learning paradigm. In this case, the value of the simulator lies in how many fewer trials we need to perform on the real robot to achieve equivalent performance.</p>
<p>Def. 4 (
4Proxy Learning Value (PLV)). Consider that a learning agent, A target trained entirely in the target domain is able to achieve a performance of M target i..m on task T at convergence using dataset S target . An agent A proxy→target pre-trained on the proxy domain and then transferred to the target domain and fine-tuned until it achieves an equivalent performance M proxy→target i..m ≥ M target i..musing dataset on the target domain S proxy→target . Then, the PLV is given by:</p>
<p>Fig. 3 :
3Comparison of proxy usefulness metrics. From left to right: PRPV, PPV, SRCC, POD. For each proxy, the metric is reported for a linear range of trim value from -0.3 to 0.3. The ground truth, 0.0, is always in the center.</p>
<p>Fig. 5 :
5Proxy (left) and target (right) domain of Duckietown, displaying the environment, observation and trajectories. Proxy zero-shot score target domain samples %</p>
<p>Fig. 6 :
6Performance of an imitation learning agent in a 3x3 duckietown map lane-following challenge with different proxy domains</p>
<p>TABLE I :
IZero shot score and number of target domain samples required to achieve real world performance at convergence on the robot according to proxy used for pre-training.
Montréal Robotics and Embodied AI Lab (REAL) at Université de Montéal, Mila.; 2 ETH Zürich, Switzerland; * Corresponding author anthony.courchesne@mail.mcgill.ca
We distinguish an agent from the more standard notion of policy in that a policy maps internal states to actions2 The instances may not be sampled randomly, as is the case in[32] 
ACKNOWLEDGEMENTS Liam Paull is supported by the Canada CIFAR AI Chairs Program. The work was also supported by the National Science and Engineering Research Council of Canada under the Discovery Grant Program and by Samsung Electronics Co. Ldt. All of which we gratefully acknowledge. The authors would also like to thank Chude (Frank) Qian for his contributions to the imitation learning pipeline and Charlie Gauthier for additional support.
Reinforcement learning in the multi-robot domain. M J Matarić, SpringerRobot colonies.M. J. Matarić, "Reinforcement learning in the multi-robot domain," in Robot colonies. Springer, 1997, pp. 73-83.</p>
<p>One-shot visual imitation learning via meta-learning. C Finn, T Yu, T Zhang, P Abbeel, S Levine, Conference on Robot Learning. C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, "One-shot visual imitation learning via meta-learning," in Conference on Robot Learning. PMLR, 2017, pp. 357-368.</p>
<p>Sim-to-real via simto-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, "Sim-to-real via sim- to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12 627-12 637.</p>
<p>Sim-toreal transfer with neural-augmented robot simulation. F Golemo, A A Taiga, A Courville, P.-Y Oudeyer, Conference on Robot Learning. PMLR. F. Golemo, A. A. Taiga, A. Courville, and P.-Y. Oudeyer, "Sim-to- real transfer with neural-augmented robot simulation," in Conference on Robot Learning. PMLR, 2018, pp. 817-828.</p>
<p>Policy transfer with strategy optimization. W Yu, C K Liu, G Turk, arXiv:1810.05751arXiv preprintW. Yu, C. K. Liu, and G. Turk, "Policy transfer with strategy optimization," arXiv preprint arXiv:1810.05751, 2018.</p>
<p>Duckietown: an open, inexpensive and flexible platform for autonomy education and research. L Paull, J Tani, H Ahn, J Alonso-Mora, L Carlone, M Cap, Y F Chen, C Choi, J Dusek, Y Fang, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEEL. Paull, J. Tani, H. Ahn, J. Alonso-Mora, L. Carlone, M. Cap, Y. F. Chen, C. Choi, J. Dusek, Y. Fang et al., "Duckietown: an open, inexpensive and flexible platform for autonomy education and research," in 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 1497-1504.</p>
<p>Gibson env v2: Embodied simulation environments for interactive navigation. F Xia, C Li, K Chen, W B Shen, R Martín-Martín, N Hirose, A R Zamir, L Fei-Fei, S Savarese, Stanford University, Tech. Rep.F. Xia, C. Li, K. Chen, W. B. Shen, R. Martín-Martín, N. Hirose, A. R. Zamir, L. Fei-Fei, and S. Savarese, "Gibson env v2: Embodied simulation environments for interactive navigation," Stanford University, Tech. Rep., 2019.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017, pp. 23-30.</p>
<p>Learning from simulated and unsupervised images through adversarial training. A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R Webb, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb, "Learning from simulated and unsupervised images through adversarial training," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2107-2116.</p>
<p>Learning to augment synthetic images for sim2real policy transfer. A Pashevich, R Strudel, I Kalevatykh, I Laptev, C Schmid, arXiv:1903.07740arXiv preprintA. Pashevich, R. Strudel, I. Kalevatykh, I. Laptev, and C. Schmid, "Learning to augment synthetic images for sim2real policy transfer," arXiv preprint arXiv:1903.07740, 2019.</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, "Unsupervised pixel-level domain adaptation with generative adversarial networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3722-3731.</p>
<p>One-shot imitation from observing humans via domain-adaptive meta-learning. T Yu, C Finn, A Xie, S Dasari, T Zhang, P Abbeel, S Levine, arXiv:1802.01557arXiv preprintT. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine, "One-shot imitation from observing humans via domain-adaptive meta-learning," arXiv preprint arXiv:1802.01557, 2018.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEK. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige et al., "Using simulation and domain adaptation to improve efficiency of deep robotic grasping," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 4243-4250.</p>
<p>Crossing the reality gap: a short introduction to the transferability approach. J.-B Mouret, S Koos, S Doncieux, arXiv:1307.1870arXiv preprintJ.-B. Mouret, S. Koos, and S. Doncieux, "Crossing the reality gap: a short introduction to the transferability approach," arXiv preprint arXiv:1307.1870, 2013.</p>
<p>20 years of reality gap: a few thoughts about simulators in evolutionary robotics. J.-B Mouret, K Chatzilygeroudis, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference CompanionJ.-B. Mouret and K. Chatzilygeroudis, "20 years of reality gap: a few thoughts about simulators in evolutionary robotics," in Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2017, pp. 1121-1124.</p>
<p>Adversarial discriminative sim-to-real transfer of visuo-motor policies. F Zhang, J Leitner, Z Ge, M Milford, P Corke, The International Journal of Robotics Research. 3810F. Zhang, J. Leitner, Z. Ge, M. Milford, and P. Corke, "Adversarial discriminative sim-to-real transfer of visuo-motor policies," The International Journal of Robotics Research, vol. 38, no. 10-11, pp. 1229-1245, 2019.</p>
<p>Adapting deep visuomotor representations with weak pairwise constraints. E Tzeng, C Devin, J Hoffman, C Finn, P Abbeel, S Levine, K Saenko, T Darrell, Algorithmic Foundations of Robotics XII. SpringerE. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell, "Adapting deep visuomotor representations with weak pairwise constraints," in Algorithmic Foundations of Robotics XII. Springer, 2020, pp. 688-703.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.10332arXiv preprintJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, "Sim-to-real: Learning agile locomotion for quadruped robots," arXiv preprint arXiv:1804.10332, 2018.</p>
<p>Robot simulation physics validation. C Pepper, S Balakirsky, C Scrapper, Proceedings of the 2007 Workshop on Performance Metrics for Intelligent Systems. the 2007 Workshop on Performance Metrics for Intelligent SystemsC. Pepper, S. Balakirsky, and C. Scrapper, "Robot simulation physics validation," in Proceedings of the 2007 Workshop on Performance Metrics for Intelligent Systems, 2007, pp. 97-104.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simulation randomization with real world experience," in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 8973-8979.</p>
<p>Traversing the reality gap via simulator tuning. J Collins, R Brown, J Leitner, D Howard, arXiv:2003.01369arXiv preprintJ. Collins, R. Brown, J. Leitner, and D. Howard, "Traversing the reality gap via simulator tuning," arXiv preprint arXiv:2003.01369, 2020.</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). J. Tan, Z. Xie, B. Boots, and C. K. Liu, "Simulation-based design of dynamic controllers for humanoid balancing," in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</p>
<p>. IEEE. IEEE, 2016, pp. 2729-2736.</p>
<p>Transfer from simulation to real world through learning deep inverse dynamics model. P Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, arXiv:1610.03518arXiv preprintP. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba, "Transfer from simulation to real world through learning deep inverse dynamics model," arXiv preprint arXiv:1610.03518, 2016.</p>
<p>Grounded action transformation for robot learning in simulation. J Hanna, P Stone, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31J. Hanna and P. Stone, "Grounded action transformation for robot learning in simulation," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, 2017.</p>
<p>The transferability approach: Crossing the reality gap in evolutionary robotics. S Koos, J.-B Mouret, S Doncieux, IEEE Transactions on Evolutionary Computation. 171S. Koos, J.-B. Mouret, and S. Doncieux, "The transferability approach: Crossing the reality gap in evolutionary robotics," IEEE Transactions on Evolutionary Computation, vol. 17, no. 1, pp. 122-145, 2012.</p>
<p>Frequency domain uncertainty and the graph topology. G Vinnicombe, IEEE Transactions on Automatic Control. 389G. Vinnicombe, "Frequency domain uncertainty and the graph topology," IEEE Transactions on Automatic Control, vol. 38, no. 9, pp. 1371-1383, 1993.</p>
<p>Experience selection using dynamics similarity for efficient multi-source transfer learning between robots. M J Sorocky, S Zhou, A P Schoellig, arXiv:2003.13150arXiv preprintM. J. Sorocky, S. Zhou, and A. P. Schoellig, "Experience selection using dynamics similarity for efficient multi-source transfer learning between robots," arXiv preprint arXiv:2003.13150, 2020.</p>
<p>Reinforcement learning with multi-fidelity simulators. M Cutler, T J Walsh, J P How, 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEEM. Cutler, T. J. Walsh, and J. P. How, "Reinforcement learning with multi-fidelity simulators," in 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 3888-3895.</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance?. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, IEEE Robotics and Automation Letters. 54A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and D. Batra, "Sim2real predictivity: Does evaluation in simulation predict real-world performance?" IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 6670-6677, 2020.</p>
<p>On mimicking the effects of the reality gap with simulation-only experiments. A Ligot, M Birattari, International Conference on Swarm Intelligence. SpringerA. Ligot and M. Birattari, "On mimicking the effects of the reality gap with simulation-only experiments," in International Conference on Swarm Intelligence. Springer, 2018, pp. 109-122.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference ProceedingsS. Ross, G. Gordon, and D. Bagnell, "A reduction of imitation learning and structured prediction to no-regret online learning," in Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627-635.</p>
<p>Active domain randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, Proceedings of the Conference on Robot Learning. the Conference on Robot LearningB. Mehta, M. Diaz, F. Golemo, C. J. Pal, and L. Paull, "Active domain randomization," in Proceedings of the Conference on Robot Learning, 2020, pp. 1162-1176.</p>
<p>Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and D. Batra, "Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation," 2019.</p>
<p>An axiomatic approach to distance on partial orderings. W D Cook, M Kress, L M Seiford, RAIRO -Operations Research -Recherche Opérationnelle. 20W. D. Cook, M. Kress, and L. M. Seiford, "An axiomatic approach to distance on partial orderings," RAIRO -Operations Research - Recherche Opérationnelle, vol. 20, no. 2, pp. 115-122, 1986.</p>
<p>A Ligot, M Birattari, On Mimicking the Effects of the Reality Gap with Simulation-Only Experiments: 11th International Conference. Rome, ItalyA. Ligot and M. Birattari, On Mimicking the Effects of the Reality Gap with Simulation-Only Experiments: 11th International Conference, ANTS 2018, Rome, Italy, October 29-31, 2018, Proceedings, 01 2018, pp. 109-122.</p>
<p>End to end learning for self-driving cars. M Bojarski, D Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, arXiv:1604.07316arXiv preprintM. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., "End to end learning for self-driving cars," arXiv preprint arXiv:1604.07316, 2016.</p>
<p>End-to-end driving via conditional imitation learning. F Codevilla, M Müller, A López, V Koltun, A Dosovitskiy, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEF. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy, "End-to-end driving via conditional imitation learning," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 4693-4700.</p>
<p>A data-efficient framework for training and sim-to-real transfer of navigation policies. H Bharadhwaj, Z Wang, Y Bengio, L Paull, 2019 International Conference on Robotics and Automation (ICRA). H. Bharadhwaj, Z. Wang, Y. Bengio, and L. Paull, "A data-efficient framework for training and sim-to-real transfer of navigation policies," in 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 782-788.</p>
<p>gradsim: Differentiable simulation for system identification and visuomotor control. J K Murthy, M Macklin, F Golemo, V Voleti, L Petrini, M Weiss, B Considine, J Parent-Lévesque, K Xie, K Erleben, L Paull, F Shkurti, D Nowrouzezahrai, S Fidler, International Conference on Learning Representations. J. K. Murthy, M. Macklin, F. Golemo, V. Voleti, L. Petrini, M. Weiss, B. Considine, J. Parent-Lévesque, K. Xie, K. Erleben, L. Paull, F. Shkurti, D. Nowrouzezahrai, and S. Fidler, "gradsim: Differentiable simulation for system identification and visuomotor control," in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=c E8kFWfhp0</p>
<p>Difftaichi: Differentiable programming for physical simulation. Y Hu, L Anderson, T.-M Li, Q Sun, N Carr, J Ragan-Kelley, F Durand, arXiv:1910.00935arXiv preprintY. Hu, L. Anderson, T.-M. Li, Q. Sun, N. Carr, J. Ragan-Kelley, and F. Durand, "Difftaichi: Differentiable programming for physical simulation," arXiv preprint arXiv:1910.00935, 2019.</p>
<p>Practical bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, arXiv:1206.2944arXiv preprintJ. Snoek, H. Larochelle, and R. P. Adams, "Practical bayesian optimization of machine learning algorithms," arXiv preprint arXiv:1206.2944, 2012.</p>
<p>Duckietown environments for openai gym. M Chevalier-Boisvert, F Golemo, Y Cao, B Mehta, L Paull, M. Chevalier-Boisvert, F. Golemo, Y. Cao, B. Mehta, and L. Paull, "Duckietown environments for openai gym," https://github.com/ duckietown/gym-duckietown, 2018.</p>
<p>Integrated benchmarking and design for reproducible and accessible evaluation of robotic agents. J Tani, A F Daniele, G Bernasconi, A Camus, A Petrov, A Courchesne, B Mehta, R Suri, T Zaluska, M R Walter, arXiv:2009.04362arXiv preprintJ. Tani, A. F. Daniele, G. Bernasconi, A. Camus, A. Petrov, A. Courchesne, B. Mehta, R. Suri, T. Zaluska, M. R. Walter et al., "Integrated benchmarking and design for reproducible and accessible evaluation of robotic agents," arXiv preprint arXiv:2009.04362, 2020.</p>
<p>J Zilly, J Tani, B Considine, B Mehta, A F Daniele, M Diaz, G Bernasconi, C Ruch, J Hakenberg, F Golemo, arXiv:1903.02503The ai driving olympics at neurips 2018. arXiv preprintJ. Zilly, J. Tani, B. Considine, B. Mehta, A. F. Daniele, M. Diaz, G. Bernasconi, C. Ruch, J. Hakenberg, F. Golemo et al., "The ai driving olympics at neurips 2018," arXiv preprint arXiv:1903.02503, 2019.</p>
<p>Teaching cars to drive themselves. "Teaching cars to drive themselves," http://www.igvc.org/design/2019/ 1.pdf, accessed: 2021-02-14.</p>            </div>
        </div>

    </div>
</body>
</html>
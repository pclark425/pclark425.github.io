<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-986 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-986</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-986</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-54e1a1f3a60ce51eae5e27e1724294032cc60929</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/54e1a1f3a60ce51eae5e27e1724294032cc60929" target="_blank">Invariant Causal Prediction for Block MDPs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper uses tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting, and proves that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return.</p>
                <p><strong>Paper Abstract:</strong> Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We give empirical evidence that our methods work in both linear and nonlinear settings, attaining improved generalization over single- and multi-task baselines.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e986.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e986.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear MISA (ICP-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Model-Irrelevance State Abstraction (Linear MISA) using Invariant Causal Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-variable-selection algorithm that applies Invariant Causal Prediction (ICP) iteratively to replay-buffered transitions across training environments to identify the minimal causal feature set (ancestors of reward) and produce a model-irrelevance state abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Linear MISA (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Algorithm 1 in the paper: given a replay buffer of transitions labeled by environment, iteratively run ICP tests to identify variables that are invariantly predictive of the reward and next-state (the causal ancestors of reward). The algorithm unions identified sets from iterative ICP calls (with adjusted confidence alpha), producing a subset S of observed variables; the abstraction phi_S(x) = [x]_S is then used for downstream prediction and control. Assumes linear predictors for reward/next-state with respect to observed variables and relies on identifiability conditions from Peters et al. (2016). Complexity is exponential in variable count (ICP combinatorial testing).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic linear Block MDP family (small toy MDP used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Low-dimensional controlled family of MDPs (k=3 in the experiment) with soft interventions on noise variables across training environments; each environment is a Block MDP where observations correspond directly to variables x1,x2,x3 with simple linear dynamics. This is a small interactive virtual lab where interventions are simulated by varying noise terms across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via Invariant Causal Prediction (statistical invariance tests across environments); explicit removal (zeroing out) of non-causal variables.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables / spuriously correlated observation dimensions (noise variables whose distribution changes across environments), measurement noise</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>ICP statistical invariance tests: test whether regression coefficients (or residuals) change across environments; variables not satisfying invariance are excluded. Iterative ancestor-finding by applying ICP to reward and next-state predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Hard exclusion (variable selection) — variables not in the identified causal set are removed from the input representation (phi zeros them out).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical invariance testing with confidence levels (alpha) from ICP; identifiability conditions and iterative ICP application allow rejection of spurious predictors that do not remain invariant across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: In the linear toy experiment, the predictor trained on the Linear MISA-selected features (phi(x) = {x1,x2}) attained zero generalization error on held-out environments with hard interventions on x3 (spurious), demonstrating perfect robustness in that setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Qualitative: A least-squares predictor trained on the full observation (including spurious x3) suffered arbitrarily large generalization error when test environments intervened on x3 (nonzero weight on x3 caused large errors).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>1 spurious variable in the linear toy experiment (x3); more generally described as small number of noise variables</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When the identifiability conditions hold and dynamics/reward are linear, iterative ICP applied across multiple training environments correctly identifies the minimal causal variable set (ancestors of reward), and removing spurious variables yields perfect generalization to test interventions that alter spurious dimensions. ICP's combinatorial cost and linearity assumption are noted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Prediction for Block MDPs', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e986.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e986.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonlinear MISA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonlinear Model-Irrelevance State Abstraction (MISA) — invariant representation learning with task-specific factors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-based method to learn an invariant latent state embedding phi (causal state) and separate task-specific latent psi (spurious factors) using a mixture of dynamics/reconstruction losses, reward prediction, adversarial task-classifier regularization (maximize entropy), and optional L1 sparsity to disentangle causal state from distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nonlinear MISA (IRM-like + adversarial disentanglement)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Algorithm 2 in the paper: learn an invariant encoder phi: X -> Z (causal latent), a decoder phi^{-1}, an invariant dynamics model f_s on Z, task-specific dynamics f_eta and encoders psi^e for each environment, and an invariant reward model r on Z. Optimize a composite loss J_ALL = J_D + alpha_R J_R - alpha_C H(C(phi)), where J_D is reconstruction/dynamics error (using both invariant and task-specific dynamics), J_R is reward prediction error in Z, and the final term is adversarial: train a task classifier C on Z to predict environment id (cross-entropy) and optimize phi to maximize entropy (adversarially remove environment information). Additional regularizers include L1 on S latent dims and decoder grounding. The objective is inspired by IRM but extended to nonlinear function approximation with explicit disentangling of spurious task factors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Block MDPs with rich observations (DeepMind Control Suite tasks: Cheetah Run, Cartpole Swingup), imitation-learning and RL setups</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive continuous-control simulated environments (DeepMind Control Suite) rendered to rich pixel observations or low-dimensional proprioceptive states; experiments include varying camera angles, randomized background colors, multiplicative spurious state dimensions, and environment identifiers. These are simulated virtual labs allowing multi-environment data collection; interventions are simulated by changing rendering, backgrounds, camera, or multiplicative spurious factors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Disentangling representation: explicit separation into invariant latent Z (phi) and task-specific latent H (psi^e); adversarial domain-classifier entropy maximization to remove task-specific signals from Z; decoder + task-specific dynamics to capture remaining information; L1 regularization to encourage sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables and non-causal visual features (background color, camera angle), correlated spurious state dimensions (multiplicative factors), measurement noise</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Indirect detection via multi-environment training losses: if a feature carries environment-identifying information (task classifier can predict env), it's considered task-specific; invariance constraints on predictor/dynamics reveal features that change across environments and thus are spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Adversarial removal (optimize encoder to maximize entropy of task-classifier output), penalty on IRM-like invariance (enforce invariant predictor), L1 sparsity on latent S to reduce influence of nuisance dims; task-specific dynamics capture remaining non-invariant components, effectively downweighting them from the invariant embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical generalization on held-out environments (zero-shot evaluation) and comparison to baselines (single-task, multi-task aggregate, IRM baseline); IRM-like invariance and adversarial objectives aim to eliminate predictors that fail to generalize and thus refute spurious causal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: Nonlinear MISA reduced model error on held-out evaluation environments (Cheetah Run) compared to single-environment training, aggregated baseline, and IRM; in RL (cartpole_swingup) MISA reduced the generalization gap versus baseline SAC and IRM-critic variants. Imitation-learning experiments showed slower increase in test error across camera-angle shifts compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Qualitative: Single-task training or naïve aggregation across environments yields higher model error and larger generalization gaps; IRM baseline was reported as brittle and harder to tune and did not match MISA's performance in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Varied: randomized background colors (visual distractors) across 2 training + 2 eval environments; RL experiments add 1 multiplicative spurious dimension + environment id; imitation experiments vary camera angles (2 train vs 1 eval).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Nonlinear MISA (invariant encoder + adversarial domain information removal + explicit task-specific dynamics) outperforms single-task and multi-task baselines and an IRM implementation on held-out environments; disentangling invariant and task-specific factors and adversarially removing environment-identifying signals in the shared latent improves zero-shot generalization in rich-observation RL and imitation settings. MISA is robust to visual distractors (background/camera changes) and spurious correlated state dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Prediction for Block MDPs', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e986.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e986.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Inference Using Invariant Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical causal discovery method that identifies the causal feature set by testing for invariance of conditional distributions or regression coefficients across multiple environments or interventions; yields identifiable causal predictors under sufficient diversity of interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICP (Peters et al., 2016) searches for subsets S of predictors such that the conditional distribution of the target given S is invariant across environments; algorithmic implementation typically involves hypothesis tests on regression residuals or coefficient equality across environments and returns a confidence set of causal predictors. In this paper ICP is used as the core variable-selection step in Linear MISA (Algorithm 1) and is cited as providing identifiability guarantees under certain intervention conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied within Block MDP synthetic experiments (toy linear MDP) and used within replay-buffered multi-environment datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive statistical experiments formed from logged transitions across multiple simulated environments; environments are generated by soft/hard interventions on noise features of a Block MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection by testing invariance of predictive relationships across environments; variables failing invariance are excluded (thereby removing distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables and spuriously correlated predictors whose conditional relation to target changes across environments; selection bias implicitly handled via interventions assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Hypothesis testing for invariance: regression residual tests or coefficient comparison across environments to detect variables whose predictive effect is non-invariant.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Hard exclusion (variables not in the invariant set are dropped).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical confidence via the ICP testing framework; identifiability and refutation rely on having diverse interventions across environments (as per Peters et al. conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative/claimed: Under suitable identifiability conditions and linear dynamics, ICP (as used in Linear MISA) returns the exact causal variable set with high probability and yields perfect generalization in the toy linear experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Not directly reported for ICP itself beyond comparisons showing that predictors using full feature sets fail to generalize when spurious features are intervened on.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICP provides a principled way to detect and remove spurious variables in the linear setting; when training environments include sufficient interventions on non-causal features, ICP identifies the causal ancestors of reward and yields state abstractions that are model-irrelevance abstractions across environments. Main limitations: exponential cost with many variables and reliance on linearity/identifiability assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Prediction for Block MDPs', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e986.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e986.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant Risk Minimization (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learning objective that seeks representations phi such that a single classifier w is simultaneously optimal across multiple environments, encouraging invariance to spurious correlations and improved out-of-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposed by Arjovsky et al. (2019) and discussed in Section 2.2: IRM optimizes for a representation phi and classifier w such that w is an optimal predictor across all environments (constrained optimization), commonly implemented via a penalized objective that enforces invariance (e.g., gradient penalty). In this paper IRM is used as a baseline for comparison and as conceptual inspiration for the nonlinear MISA objective.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as baseline on DeepMind Control Suite tasks (Cheetah Run, Cartpole Swingup) and in model-learning comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Block MDP simulated environments as used for MISA experiments; IRM is evaluated in model-learning and RL settings under multiple training environments with visual or spurious-state variations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Representation learning via invariance constraint: enforce the same optimal classifier across environments to remove features that are spuriously correlated with the target in some environments but not causal.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations and non-causal features that vary across environments (visual distractors, correlated noise in labels/state).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: invariance is enforced during training; features that cause environment-specific optimal classifiers are downweighted by the penalty (gradient-based checks on classifier optimality per environment).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalty/regularizer (IRM gradient penalty schedule) that discourages representations enabling environment-specific classifiers; effectively downweights features that do not admit a common linear classifier across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Objective enforces invariance; empirically refutation is via generalization failure on held-out environments for models that rely on spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: IRM baseline in the paper showed higher initial loss and was reported to be brittle and hard to tune; did not match Nonlinear MISA's generalization performance in the rich-observation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Not quantified beyond being a baseline; single-task and aggregated multi-task baselines performed worse than MISA and typically worse or similar to IRM depending on tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRM is a motivating prior method for invariant representation learning; in the experiments reported IRM was less effective and harder to tune than MISA, highlighting practical difficulties in applying pure IRM penalties in nonlinear, high-dimensional RL settings without additional architectural/disentangling mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Prediction for Block MDPs', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e986.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e986.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial Task-Classifer (Domain Adversarial)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Task-Classifier / Adversarial Discriminative Domain Adaptation style regularizer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial domain-classifier trained to predict environment id from the shared latent; the encoder is trained adversarially to maximize entropy of the classifier output (remove environment-identifying information) so that the shared latent is invariant to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial discriminative domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Adversarial task-classifier / entropy-maximization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The paper trains a task classifier C: Z -> [0,1]^N with cross-entropy on environment labels, and simultaneously trains the encoder phi to maximize the classifier's entropy (adversarial objective), following adversarial domain adaptation ideas. This encourages the shared latent Z to omit environment-specific information (visual/background cues, env id), forcing spurious information into the task-specific latent psi^e.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied in Nonlinear MISA experiments on DeepMind Control Suite (pixel or proprioceptive observation variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated control tasks with multiple training environments differing in visuals or spurious state augmentations; environments are labeled (env id) enabling supervised adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Adversarial domain discrimination: explicit removal of environment-identifying features from the invariant latent via maximizing classifier entropy / adversarial objective; combined with decoder and task-specific latent to absorb remaining distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Visual distractors (backgrounds, camera angles), environment-specific non-causal cues, correlated spurious state dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>If the task-classifier can predict environment id from Z, that indicates presence of distractor information in Z; adversarial training minimizes this predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Encoder optimized to maximize classifier entropy (adversarially), thus downweighting/removing environment-specific signals from Z; residual information allocated to task-specific latents.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical: by forcing env-id unpredictability in Z, features that enabled env-specific correlations are removed and cannot serve as predictors; validated by improved generalization to held-out environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: In combination with invariant dynamics/reward objectives, adversarial task-classifier improved generalization of learned models and policies compared to baselines that lacked this adversarial disentangling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Not isolated numerically; overall baselines that did not use adversarial removal (single-task, aggregated multi-task, plain IRM) performed worse in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adversarial removal of environment-identifying information from the shared latent is an effective practical mechanism to downweight and remove spurious visual and environment-specific cues when combined with dynamics/reward objectives and task-specific latents; it stabilizes invariance learning in nonlinear settings where IRM penalties alone were brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Prediction for Block MDPs', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Invariant Risk Minimization <em>(Rating: 2)</em></li>
                <li>Adversarial discriminative domain adaptation <em>(Rating: 1)</em></li>
                <li>Provably efficient RL with rich observations via latent state decoding <em>(Rating: 2)</em></li>
                <li>Observational overfitting in reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-986",
    "paper_id": "paper-54e1a1f3a60ce51eae5e27e1724294032cc60929",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Linear MISA (ICP-based)",
            "name_full": "Linear Model-Irrelevance State Abstraction (Linear MISA) using Invariant Causal Prediction",
            "brief_description": "A linear-variable-selection algorithm that applies Invariant Causal Prediction (ICP) iteratively to replay-buffered transitions across training environments to identify the minimal causal feature set (ancestors of reward) and produce a model-irrelevance state abstraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Linear MISA (ICP)",
            "method_description": "Algorithm 1 in the paper: given a replay buffer of transitions labeled by environment, iteratively run ICP tests to identify variables that are invariantly predictive of the reward and next-state (the causal ancestors of reward). The algorithm unions identified sets from iterative ICP calls (with adjusted confidence alpha), producing a subset S of observed variables; the abstraction phi_S(x) = [x]_S is then used for downstream prediction and control. Assumes linear predictors for reward/next-state with respect to observed variables and relies on identifiability conditions from Peters et al. (2016). Complexity is exponential in variable count (ICP combinatorial testing).",
            "environment_name": "Synthetic linear Block MDP family (small toy MDP used in paper)",
            "environment_description": "Low-dimensional controlled family of MDPs (k=3 in the experiment) with soft interventions on noise variables across training environments; each environment is a Block MDP where observations correspond directly to variables x1,x2,x3 with simple linear dynamics. This is a small interactive virtual lab where interventions are simulated by varying noise terms across environments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via Invariant Causal Prediction (statistical invariance tests across environments); explicit removal (zeroing out) of non-causal variables.",
            "spurious_signal_types": "Irrelevant variables / spuriously correlated observation dimensions (noise variables whose distribution changes across environments), measurement noise",
            "detection_method": "ICP statistical invariance tests: test whether regression coefficients (or residuals) change across environments; variables not satisfying invariance are excluded. Iterative ancestor-finding by applying ICP to reward and next-state predictions.",
            "downweighting_method": "Hard exclusion (variable selection) — variables not in the identified causal set are removed from the input representation (phi zeros them out).",
            "refutation_method": "Statistical invariance testing with confidence levels (alpha) from ICP; identifiability conditions and iterative ICP application allow rejection of spurious predictors that do not remain invariant across environments.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: In the linear toy experiment, the predictor trained on the Linear MISA-selected features (phi(x) = {x1,x2}) attained zero generalization error on held-out environments with hard interventions on x3 (spurious), demonstrating perfect robustness in that setup.",
            "performance_without_robustness": "Qualitative: A least-squares predictor trained on the full observation (including spurious x3) suffered arbitrarily large generalization error when test environments intervened on x3 (nonzero weight on x3 caused large errors).",
            "has_ablation_study": true,
            "number_of_distractors": "1 spurious variable in the linear toy experiment (x3); more generally described as small number of noise variables",
            "key_findings": "When the identifiability conditions hold and dynamics/reward are linear, iterative ICP applied across multiple training environments correctly identifies the minimal causal variable set (ancestors of reward), and removing spurious variables yields perfect generalization to test interventions that alter spurious dimensions. ICP's combinatorial cost and linearity assumption are noted limitations.",
            "uuid": "e986.0",
            "source_info": {
                "paper_title": "Invariant Causal Prediction for Block MDPs",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Nonlinear MISA",
            "name_full": "Nonlinear Model-Irrelevance State Abstraction (MISA) — invariant representation learning with task-specific factors",
            "brief_description": "A gradient-based method to learn an invariant latent state embedding phi (causal state) and separate task-specific latent psi (spurious factors) using a mixture of dynamics/reconstruction losses, reward prediction, adversarial task-classifier regularization (maximize entropy), and optional L1 sparsity to disentangle causal state from distractors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Nonlinear MISA (IRM-like + adversarial disentanglement)",
            "method_description": "Algorithm 2 in the paper: learn an invariant encoder phi: X -&gt; Z (causal latent), a decoder phi^{-1}, an invariant dynamics model f_s on Z, task-specific dynamics f_eta and encoders psi^e for each environment, and an invariant reward model r on Z. Optimize a composite loss J_ALL = J_D + alpha_R J_R - alpha_C H(C(phi)), where J_D is reconstruction/dynamics error (using both invariant and task-specific dynamics), J_R is reward prediction error in Z, and the final term is adversarial: train a task classifier C on Z to predict environment id (cross-entropy) and optimize phi to maximize entropy (adversarially remove environment information). Additional regularizers include L1 on S latent dims and decoder grounding. The objective is inspired by IRM but extended to nonlinear function approximation with explicit disentangling of spurious task factors.",
            "environment_name": "Block MDPs with rich observations (DeepMind Control Suite tasks: Cheetah Run, Cartpole Swingup), imitation-learning and RL setups",
            "environment_description": "Interactive continuous-control simulated environments (DeepMind Control Suite) rendered to rich pixel observations or low-dimensional proprioceptive states; experiments include varying camera angles, randomized background colors, multiplicative spurious state dimensions, and environment identifiers. These are simulated virtual labs allowing multi-environment data collection; interventions are simulated by changing rendering, backgrounds, camera, or multiplicative spurious factors.",
            "handles_distractors": true,
            "distractor_handling_technique": "Disentangling representation: explicit separation into invariant latent Z (phi) and task-specific latent H (psi^e); adversarial domain-classifier entropy maximization to remove task-specific signals from Z; decoder + task-specific dynamics to capture remaining information; L1 regularization to encourage sparsity.",
            "spurious_signal_types": "Irrelevant variables and non-causal visual features (background color, camera angle), correlated spurious state dimensions (multiplicative factors), measurement noise",
            "detection_method": "Indirect detection via multi-environment training losses: if a feature carries environment-identifying information (task classifier can predict env), it's considered task-specific; invariance constraints on predictor/dynamics reveal features that change across environments and thus are spurious.",
            "downweighting_method": "Adversarial removal (optimize encoder to maximize entropy of task-classifier output), penalty on IRM-like invariance (enforce invariant predictor), L1 sparsity on latent S to reduce influence of nuisance dims; task-specific dynamics capture remaining non-invariant components, effectively downweighting them from the invariant embedding.",
            "refutation_method": "Empirical generalization on held-out environments (zero-shot evaluation) and comparison to baselines (single-task, multi-task aggregate, IRM baseline); IRM-like invariance and adversarial objectives aim to eliminate predictors that fail to generalize and thus refute spurious causal relationships.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: Nonlinear MISA reduced model error on held-out evaluation environments (Cheetah Run) compared to single-environment training, aggregated baseline, and IRM; in RL (cartpole_swingup) MISA reduced the generalization gap versus baseline SAC and IRM-critic variants. Imitation-learning experiments showed slower increase in test error across camera-angle shifts compared to baselines.",
            "performance_without_robustness": "Qualitative: Single-task training or naïve aggregation across environments yields higher model error and larger generalization gaps; IRM baseline was reported as brittle and harder to tune and did not match MISA's performance in these experiments.",
            "has_ablation_study": true,
            "number_of_distractors": "Varied: randomized background colors (visual distractors) across 2 training + 2 eval environments; RL experiments add 1 multiplicative spurious dimension + environment id; imitation experiments vary camera angles (2 train vs 1 eval).",
            "key_findings": "Nonlinear MISA (invariant encoder + adversarial domain information removal + explicit task-specific dynamics) outperforms single-task and multi-task baselines and an IRM implementation on held-out environments; disentangling invariant and task-specific factors and adversarially removing environment-identifying signals in the shared latent improves zero-shot generalization in rich-observation RL and imitation settings. MISA is robust to visual distractors (background/camera changes) and spurious correlated state dimensions.",
            "uuid": "e986.1",
            "source_info": {
                "paper_title": "Invariant Causal Prediction for Block MDPs",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Invariant Causal Prediction (ICP)",
            "name_full": "Causal Inference Using Invariant Prediction (ICP)",
            "brief_description": "A statistical causal discovery method that identifies the causal feature set by testing for invariance of conditional distributions or regression coefficients across multiple environments or interventions; yields identifiable causal predictors under sufficient diversity of interventions.",
            "citation_title": "Causal inference using invariant prediction: identification and confidence intervals",
            "mention_or_use": "use",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "ICP (Peters et al., 2016) searches for subsets S of predictors such that the conditional distribution of the target given S is invariant across environments; algorithmic implementation typically involves hypothesis tests on regression residuals or coefficient equality across environments and returns a confidence set of causal predictors. In this paper ICP is used as the core variable-selection step in Linear MISA (Algorithm 1) and is cited as providing identifiability guarantees under certain intervention conditions.",
            "environment_name": "Applied within Block MDP synthetic experiments (toy linear MDP) and used within replay-buffered multi-environment datasets",
            "environment_description": "Non-interactive statistical experiments formed from logged transitions across multiple simulated environments; environments are generated by soft/hard interventions on noise features of a Block MDP.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection by testing invariance of predictive relationships across environments; variables failing invariance are excluded (thereby removing distractors).",
            "spurious_signal_types": "Irrelevant variables and spuriously correlated predictors whose conditional relation to target changes across environments; selection bias implicitly handled via interventions assumptions.",
            "detection_method": "Hypothesis testing for invariance: regression residual tests or coefficient comparison across environments to detect variables whose predictive effect is non-invariant.",
            "downweighting_method": "Hard exclusion (variables not in the invariant set are dropped).",
            "refutation_method": "Statistical confidence via the ICP testing framework; identifiability and refutation rely on having diverse interventions across environments (as per Peters et al. conditions).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative/claimed: Under suitable identifiability conditions and linear dynamics, ICP (as used in Linear MISA) returns the exact causal variable set with high probability and yields perfect generalization in the toy linear experiments.",
            "performance_without_robustness": "Not directly reported for ICP itself beyond comparisons showing that predictors using full feature sets fail to generalize when spurious features are intervened on.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ICP provides a principled way to detect and remove spurious variables in the linear setting; when training environments include sufficient interventions on non-causal features, ICP identifies the causal ancestors of reward and yields state abstractions that are model-irrelevance abstractions across environments. Main limitations: exponential cost with many variables and reliance on linearity/identifiability assumptions.",
            "uuid": "e986.2",
            "source_info": {
                "paper_title": "Invariant Causal Prediction for Block MDPs",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Invariant Risk Minimization (IRM)",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "A learning objective that seeks representations phi such that a single classifier w is simultaneously optimal across multiple environments, encouraging invariance to spurious correlations and improved out-of-distribution generalization.",
            "citation_title": "Invariant Risk Minimization",
            "mention_or_use": "use",
            "method_name": "Invariant Risk Minimization (IRM)",
            "method_description": "Proposed by Arjovsky et al. (2019) and discussed in Section 2.2: IRM optimizes for a representation phi and classifier w such that w is an optimal predictor across all environments (constrained optimization), commonly implemented via a penalized objective that enforces invariance (e.g., gradient penalty). In this paper IRM is used as a baseline for comparison and as conceptual inspiration for the nonlinear MISA objective.",
            "environment_name": "Used as baseline on DeepMind Control Suite tasks (Cheetah Run, Cartpole Swingup) and in model-learning comparisons",
            "environment_description": "Same Block MDP simulated environments as used for MISA experiments; IRM is evaluated in model-learning and RL settings under multiple training environments with visual or spurious-state variations.",
            "handles_distractors": true,
            "distractor_handling_technique": "Representation learning via invariance constraint: enforce the same optimal classifier across environments to remove features that are spuriously correlated with the target in some environments but not causal.",
            "spurious_signal_types": "Spurious correlations and non-causal features that vary across environments (visual distractors, correlated noise in labels/state).",
            "detection_method": "Implicit: invariance is enforced during training; features that cause environment-specific optimal classifiers are downweighted by the penalty (gradient-based checks on classifier optimality per environment).",
            "downweighting_method": "Penalty/regularizer (IRM gradient penalty schedule) that discourages representations enabling environment-specific classifiers; effectively downweights features that do not admit a common linear classifier across environments.",
            "refutation_method": "Objective enforces invariance; empirically refutation is via generalization failure on held-out environments for models that rely on spurious features.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: IRM baseline in the paper showed higher initial loss and was reported to be brittle and hard to tune; did not match Nonlinear MISA's generalization performance in the rich-observation experiments.",
            "performance_without_robustness": "Not quantified beyond being a baseline; single-task and aggregated multi-task baselines performed worse than MISA and typically worse or similar to IRM depending on tuning.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "IRM is a motivating prior method for invariant representation learning; in the experiments reported IRM was less effective and harder to tune than MISA, highlighting practical difficulties in applying pure IRM penalties in nonlinear, high-dimensional RL settings without additional architectural/disentangling mechanisms.",
            "uuid": "e986.3",
            "source_info": {
                "paper_title": "Invariant Causal Prediction for Block MDPs",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Adversarial Task-Classifer (Domain Adversarial)",
            "name_full": "Adversarial Task-Classifier / Adversarial Discriminative Domain Adaptation style regularizer",
            "brief_description": "An adversarial domain-classifier trained to predict environment id from the shared latent; the encoder is trained adversarially to maximize entropy of the classifier output (remove environment-identifying information) so that the shared latent is invariant to distractors.",
            "citation_title": "Adversarial discriminative domain adaptation",
            "mention_or_use": "use",
            "method_name": "Adversarial task-classifier / entropy-maximization",
            "method_description": "The paper trains a task classifier C: Z -&gt; [0,1]^N with cross-entropy on environment labels, and simultaneously trains the encoder phi to maximize the classifier's entropy (adversarial objective), following adversarial domain adaptation ideas. This encourages the shared latent Z to omit environment-specific information (visual/background cues, env id), forcing spurious information into the task-specific latent psi^e.",
            "environment_name": "Applied in Nonlinear MISA experiments on DeepMind Control Suite (pixel or proprioceptive observation variants)",
            "environment_description": "Interactive simulated control tasks with multiple training environments differing in visuals or spurious state augmentations; environments are labeled (env id) enabling supervised adversarial training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Adversarial domain discrimination: explicit removal of environment-identifying features from the invariant latent via maximizing classifier entropy / adversarial objective; combined with decoder and task-specific latent to absorb remaining distractors.",
            "spurious_signal_types": "Visual distractors (backgrounds, camera angles), environment-specific non-causal cues, correlated spurious state dimensions",
            "detection_method": "If the task-classifier can predict environment id from Z, that indicates presence of distractor information in Z; adversarial training minimizes this predictability.",
            "downweighting_method": "Encoder optimized to maximize classifier entropy (adversarially), thus downweighting/removing environment-specific signals from Z; residual information allocated to task-specific latents.",
            "refutation_method": "Empirical: by forcing env-id unpredictability in Z, features that enabled env-specific correlations are removed and cannot serve as predictors; validated by improved generalization to held-out environments.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: In combination with invariant dynamics/reward objectives, adversarial task-classifier improved generalization of learned models and policies compared to baselines that lacked this adversarial disentangling.",
            "performance_without_robustness": "Not isolated numerically; overall baselines that did not use adversarial removal (single-task, aggregated multi-task, plain IRM) performed worse in reported experiments.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Adversarial removal of environment-identifying information from the shared latent is an effective practical mechanism to downweight and remove spurious visual and environment-specific cues when combined with dynamics/reward objectives and task-specific latents; it stabilizes invariance learning in nonlinear settings where IRM penalties alone were brittle.",
            "uuid": "e986.4",
            "source_info": {
                "paper_title": "Invariant Causal Prediction for Block MDPs",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Invariant Risk Minimization",
            "rating": 2
        },
        {
            "paper_title": "Adversarial discriminative domain adaptation",
            "rating": 1
        },
        {
            "paper_title": "Provably efficient RL with rich observations via latent state decoding",
            "rating": 2
        },
        {
            "paper_title": "Observational overfitting in reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01766625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Invariant Causal Prediction for Block MDPs</h1>
<p>Amy Zhang ${ }^{<em> 123}$ Clare Lyle ${ }^{</em> 4}$ Shagun Sodhani ${ }^{3}$ Angelos Filos ${ }^{4}$ Marta Kwiatkowska ${ }^{4}$ Joelle Pineau ${ }^{123}$<br>Yarin Gal ${ }^{4}$ Doina Precup ${ }^{125}$</p>
<h4>Abstract</h4>
<p>Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We give empirical evidence that our methods work in both linear and nonlinear settings, attaining improved generalization over single- and multi-task baselines.</p>
<h2>1. Introduction</h2>
<p>The canonical reinforcement learning (RL) problem assumes an agent interacting with a single MDP with a fixed observation space and dynamics structure. This assumption is difficult to ensure in practice, where state spaces are often large and infeasible to explore entirely during training. However, there is often a latent structure to be leveraged to allow for good generalization. As an example, a robot's sensors may be moved, or the lighting con-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ditions in a room may change, but the physical dynamics of the environment are still the same. These are examples of environment-specific characteristics that current RL algorithms often overfit to. In the worst case, some training environments may contain spurious correlations that will not be present at test time, causing catastrophic failures in generalization (Zhang et al., 2018a; Song et al., 2020). To develop algorithms that will be robust to these sorts of changes, we must consider problem settings that allow for multiple environments with a shared dynamics structure.</p>
<p>Recent prior works (Amit and Meir, 2018; Yin et al., 2019) have developed generalization bounds for the multitask problem, but they depend on the number of tasks seen at training time, which can be prohibitively expensive given how sample inefficient RL is even in the single task regime. To obtain stronger generalization results, we propose to consider a problem which we refer to as 'multienvironment' RL: like multi-task RL, the agent seeks to maximize return on a set of environments, but only some of which can be trained on. We make the assumption that there exists some latent causal structure that is shared among all of the environments, and that the sources of variability between environments do not affect reward. This family of environments is called a Block MDP (Du et al., 2019), in which the observations may change, but the latent states, dynamics, and reward function are the same. A formal definition of this type of MDP will be presented in Section 3.</p>
<p>Though the setting we consider is a subset of the multitask RL problem, we show in this work that the added assumption of shared structure allows for much stronger generalization results than have been obtained by multi-task approaches. Naive application of generalization bounds to the multi-task reinforcement learning setting is very loose because the learner is typically given access to only a few tasks relative to the number of samples from each task. Indeed, Cobbe et al. (2018); Zhang et al. (2018b) find that agents trained using standard methods require many thousands of environments before succeeding at 'generalizing' to new environments.</p>
<p>The main contribution of this paper is to use tools from causal inference to address generalization in the Block MDP setting, proposing a new method based on the invari-</p>
<p>ant causal prediction literature. In certain linear function approximation settings, we demonstrate that this method will, with high probability, learn an optimal state abstraction that generalizes across all environments using many fewer training environments than would be necessary for standard PAC bounds. We replace this PAC requirement with requirements from causal inference on the types of environments seen at training time. We then draw a connection between bisimulation and the minimal causal set of variables found by our algorithm, providing bounds on the model error and sample complexity of the method. We further show that using analogous invariant prediction methods for the nonlinear function approximation setting can yield improved generalization performance over multi-task and single-task baselines. We relate this method to previous work on learning representations of MDPs (Gelada et al., 2019; Luo et al., 2019) and develop multi-task generalization bounds for such representations. Code is available at https://github.com/facebookresearch/ icp-block-mdp.</p>
<h2>2. Background</h2>
<h3>2.1. State Abstractions and Bisimulation</h3>
<p>State abstractions have been studied as a way to distinguish relevant from irrelevant information (Li et al., 2006) in order to create a more compact representation for easier decision making and planning. Bertsekas and Castanon (1989); Roy (2006) provide bounds for approximation errors for various aggregation methods, and Li et al. (2006) discuss the merits of abstraction discovery as a way to solve related MDPs.</p>
<p>Bisimulation relations are a type of state abstraction that offers a mathematically precise definition of what it means for two environments to 'share the same structure' (Larsen and Skou, 1989; Givan et al., 2003). We say that two states are bisimilar if they share the same expected reward and equivalent distributions over the next bisimilar states. For example, if a robot is given the task of washing the dishes in a kitchen, changing the wallpaper in the kitchen doesn't change anything relevant to the task. One then could define a bisimulation relation that equates observations based on the locations and soil levels of dishes in the room and ignores the wallpaper. These relations can be used to simplify the state space for tasks like policy transfer (Castro and Precup, 2010), and are intimately tied to state abstraction. For example, the model-irrelevance abstraction described by Li et al. (2006) is precisely characterized as a bisimulation relation.
Definition 1 (Bisimulation Relations (Givan et al., 2003)). Given an MDP $\mathcal{M}$, an equivalence relation $B$ between states is a bisimulation relation if for all states $s_{1}, s_{2} \in \mathcal{S}$ that are equivalent under $B$ (i.e. $s_{1} B s_{2}$ ), the following
conditions hold for all actions $a \in \mathcal{A}$ :</p>
<p>$$
\begin{aligned}
R\left(s_{1}, a\right) &amp; =R\left(s_{2}, a\right) \
\mathcal{P}\left(G \mid s_{1}, a\right) &amp; =\mathcal{P}\left(G \mid s_{2}, a\right), \forall G \in \mathcal{S} / B
\end{aligned}
$$</p>
<p>Where $\mathcal{S} / B$ denotes the partition of $\mathcal{S}$ under the relation $B$, the set of all groups of equivalent states, and where $\mathcal{P}(G \mid s, a)=\sum_{s^{\prime} \in G} \mathcal{P}\left(s^{\prime} \mid s, a\right)$.</p>
<p>Whereas this definition was originally designed for the single MDP setting to find bisimilar states within an MDP, we are now trying to find bisimilar states across different MDPs, or different experimental conditions. One can intuitively think of this carrying over by imagining all experimental conditions $i$ mapped to a single super-MDP with state space $\mathcal{S}=\cup_{i} \mathcal{S}<em i="i">{i}$ where we give up the irreducibility assumption, i.e. we can no longer reach every state $s</em>$. Bisimilar MDPs are therefore MDPs which are behaviourally the same.}$ from any other state $s_{j}$. Specifically, we say that two MDPs $M_{1}$ and $M_{2}$ are bisimilar if there exist bisimulation relations $B_{1}$ and $B_{2}$ such that $M_{1} / B_{1}$ is isomorphic to $M_{2} / B_{2</p>
<h3>2.2. Causal Inference Using Invariant Prediction</h3>
<p>Peters et al. (2016) first introduced an algorithm, Invariant Causal Prediction (ICP), to find the causal feature set, the minimal set of features which are causal predictors of a target variable, by exploiting the fact that causal models have an invariance property (Pearl, 2009; Schölkopf et al., 2012). Arjovsky et al. (2019) extend this work by proposing invariant risk minimization (IRM, see Equation (1)), augmenting empirical risk minimization to learn a data representation free of spurious correlations. They assume there exists some partition of the training data $\mathcal{X}$ into $e x$ periments $e \in \mathcal{E}$, and that the model's predictions take the form $Y^{e}=\mathbf{w}^{\top} \phi\left(X^{e}\right)$. IRM aims to learn a representation $\phi$ for which the optimal linear classifier, $\mathbf{w}$, is invariant across $e$, where optimality is defined as minimizing the empirical risk $R^{e}$. We can then expect this representation and classifier to have low risk in new experiments $e$, which have the same causal structure as the training set.</p>
<p>$$
\begin{aligned}
&amp; \min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{\substack{\phi: \mathcal{X} \rightarrow \mathbb{R}^{d} \
\mathbf{w} \in \mathbb{R}^{d}}} \sum</em>\right)\right) \
&amp; \text { s.t. } \mathbf{w} \in \underset{\tilde{\mathbf{w}} \in \mathbb{R}^{d}}{\arg \min } R^{e}\left(\tilde{\mathbf{w}}^{\top} \phi\left(X^{e}\right)\right) \quad \forall e \in \mathcal{E}
\end{aligned}
$$}} R^{e}\left(\mathbf{w}^{\top} \phi\left(X^{e</p>
<p>The IRM objective in Equation (1) can be thought of as a constrained optimization problem, where the objective is to learn a set of features $\phi$ for which the optimal classifier in each environment is the same. Conditioned on the environments corresponding to different interventions on the data-generating process, this is hypothesized to yield features that only depend on variables that bear a causal relationship to the predicted value. Because the constrained</p>
<p>optimization problem is not generally feasible to optimize, <em>Arjovsky et al. (2019)</em> propose a penalized optimization problem with a schedule on the penalty term as a tractable alternative.</p>
<h2>3 Problem Setup</h2>
<p>We consider a family of environments $\mathcal{M}<em e="e">{\mathcal{E}}=$ ${(\mathcal{X}</em>},\mathcal{A},\mathcal{R<em e="e">{e},\mathcal{T}</em>},\gamma)|\ e\in\mathcal{E}}$, where $\mathcal{E}$ is some index set. For simplicity of notation, we drop the subscript $e$ when referring to the union over all environments $\mathcal{E}$. Our goal is to use a subset $\mathcal{E<em _text_train="\text{train">{\text{train}}subset\mathcal{E}$ of these environments to learn a representation $\phi:\mathcal{X}\rightarrow\mathbb{R}^{d}$ which enables generalization of a learned policy to <em>every</em> environment. We denote the number of training environments as $N:=|\mathcal{E}</em>|$. We assume that the environments share some structure, and consider different degrees to which this structure may be shared.}</p>
<h3>3.1 The Block MDP</h3>
<p>Block MDPs <em>(Du et al., 2019)</em> are described by a tuple $\langle\mathcal{S},\mathcal{A},\mathcal{X},p,q,R\rangle$ with a finite, unobservable state space $\mathcal{S}$, finite action space $\mathcal{A}$, and possibly infinite, but observable space $\mathcal{X}$. $p$ denotes the latent transition distribution $p(s^{\prime}|s,a)$ for $s,s^{\prime}\in\mathcal{S},a\in\mathcal{A},q$ is the (possibly stochastic) emission function that gives the observations from the latent state $q(x|s)$ for $x\in\mathcal{X},s\in\mathcal{S}$, and $R$ the reward function. A graphical model of the interactions between the various variables can be found in Figure 1.</p>
<p>Assumption 1 (Block structure <em>(Du et al., 2019)</em>). <em>Each observation $x$ uniquely determines its generating state $s$. That is, the observation space $\mathcal{X}$ can be partitioned into disjoint blocks $\mathcal{X}_{s}$, each containing the support of the conditional distribution $q(\cdot|s)$.</em></p>
<p>This assumption gives us the Markov property in $\mathcal{X}$. We translate the block MDP to our multi-environment setting as follows. If a family of environments $\mathcal{M}<em e="e">{\mathcal{E}}$ satisfies the block MDP assumption, then each $e\in\mathcal{E}$ corresponds to an emission function $q</em>}$, with $S,A,\mathcal{X}$ and $p$ shared for all $M\in\mathcal{M<em e="e">{\mathcal{E}}$. We will move the potential randomness from $q</em>$, given that only a subset is provided for training. }$ into an auxiliary variable $\eta\in\Omega$, with $\Omega$ some probability space, and write $q_{e}(\eta,s)$. Further, we require that if $\text{range}(q_{e}(\cdot,s)) \cap \text{range}\left(q_{e^{\prime}}(\cdot,s^{\prime})\right) \neq \emptyset$, then $s=s^{\prime}$. The objective is to learn a useful state abstraction to promote generalization across the different emission functions $q_{e<em>Song et al. (2020)</em> also describes a similar POMDP setting where there is an additional observation function, but assume information can be lost. We note that this problem can be made arbitrarily difficult if each $q_{e}$ has a disjoint range, but will focus on settings where the $q_{e}$ overlap in structured ways – for example, where $q_{e}$ is the concatenation of the noise and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Graphical model of a block MDP with stochastic, correlated observations, with an IRM goal to extract $s$ from the sequence of observations, and discard the spurious noise $\eta$. Red dashed ovals indicate the entire tangled latent state at each timestep. Black dashed lines and grey lines indicate two additional tiers of difficulty to consider.</p>
<p>state variables: $q_{e}(\eta,s)=s \oplus f(\eta)$.</p>
<h3>3.2 Relaxations</h3>
<p>Spurious correlations. Our initial presentation of the block MDP assumes that the noise variable $\eta$ is sampled randomly at every time step, which prevents multi-timestep correlations (Figure 1 in black, solid lines). We therefore also consider a more realistic <em>relaxed block MDP</em>, where spurious variables may have different transition dynamics across the different environments so long as these correlations do not affect the expected reward (Figure 1, now including black dashed lines). This is equivalent to augmenting each Block MDP in our family with a noise variable $\eta_{e}$, such that the observation $x=(q_{e}(\eta_{e},s))$, and</p>
<p>$$p(x'|x,a) = p(q^{-1}(x')|s,a)p_{e}(\eta_{e} |s,\eta_{e}).$$</p>
<p>We note that this section still satisfies Assumption 1.</p>
<p>Realizability. Though our analysis will require Assumption 1, we claim that this is a reasonable requirement as it makes the learning problem realizable. Relaxing Assumption 1 means that the value function learning problem may become ill-posed, as the same observation can map to entirely different states in the latent MDP with different values, making our environment partially observable (a POMDP, Figure 1 with grey lines). We provide a lower bound on the value approximation error attainable in this setting in the appendix (Proposition 2).</p>
<h3>3.3 Assumptions on causal structure</h3>
<p>State abstraction and causal inference both aim to eliminate spurious features in a learning algorithm’s input. However, these two approaches are applied to drastically different types of problems. Though we demonstrate that causal inference methods can be applied to reinforcement learning, this will require some assumption on how causal mechanisms are observed. Definitions of the notation used in this</p>
<p>section are deferred to the appendix, though they are standard in the causal inference literature.</p>
<p>The key assumption we make is that the variables in the environment state at time $t$ can only affect the values of the state at time $t+1$, and can only affect the reward at time $t$. This assumption allows us to consider the state and action at time $t$ as the only candidate for causal parents of the state at time $t+1$ and of the reward at time $t$. This assumption is crucial to the Markov behaviour of the Markov decision process. We refer the reader to Figure 2 to demonstrate how causal graphical models can be translated to this setting.
Assumption 2 (Temporal Causal Mechanisms). Let $x^{1}$ and $x^{2}$ be components of the observation $x$. Then when no intervention is performed on the environment, we have the following independence.</p>
<p>$$
X_{t+1}^{1} \perp X_{t+1}^{2} \mid x_{t}
$$</p>
<p>Assumption 3 (Environment Interventions). Let $\mathcal{X}=$ $X_{1} \times \cdots \times X_{n}$, and $\mathcal{S}=X_{i_{1}} \times \ldots X_{i_{k}}$. Each environment $e \in \mathcal{E}$ corresponds to a do- (Pearl, 2009) or soft (Eberhardt and Scheines, 2007) intervention on a single variable $x_{i}$ in the observation space.</p>
<p>This assumption allows us to use tools from causal inference to identify candidate model-irrelevance state abstractions that may hold across an entire family of MDPs, rather than only the ones observed, based on using the state at one timestep to predict values at the next timestep. In the setting of Assumption 3, we can reconstruct the block MDP emission function $q$ by concatenating the spurious variables from $\mathcal{X} \backslash \mathcal{S}$ to $\mathcal{S}$. We discuss some constraints on interventions necessary to satisfy the block MDP assumption in the appendix.</p>
<h2>4. Connecting State Abstractions to Causal Feature Sets</h2>
<p>Invariant causal prediction aims to identify a set $S$ of causal variables such that a linear predictor with support on $S$ will attain consistent performance over all environments. In other words, ICP removes irrelevant variables from the input, just as state abstractions remove irrelevant information from the environment's observations. An attractive property of the block MDP setting is that it is easy to show that there does exist a model-irrelevance state abstraction $\phi$ for all MDPs in $\mathcal{M}_{\mathcal{E}}$ - namely, the function mapping each observation $x$ to its generating latent state $\phi(x)=q^{-1}(x)$. The formalization and proof of this statement are deferred to the appendix (see Theorem 4).</p>
<p>We consider whether, under Assumptions 1-3, such a state abstraction can be obtained by ICP. Intuitively, one would then expect that the causal variables should have nice properties as a state abstraction. The following result confirms
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Graphical causal models with temporal dependence note that while $x^{2}$ (circled in blue) is the only causal parent of the reward, because its next-timestep distribution depends on $x^{1}$, a model-irrelevance state abstraction must include both variables. Shaded in blue: the graphical causal model of an MDP with states $s=\left(x^{1}, x^{2}\right)$ when ignoring timesteps.
this to be the case; a state abstraction that selects the set of causal variables from the observation space of a block MDP will be a model-irrelevance abstraction for every environment $e \in \mathcal{E}$.</p>
<p>Theorem 1. Consider a family of MDPs $M_{\mathcal{E}}=$ $\left{(\mathcal{X}, A, R, P_{e}, \gamma) \mid e \in \mathcal{E}\right}$, with $\mathcal{X}=\mathbb{R}^{k}$. Let $M_{\mathcal{E}}$ satisfy Assumptions 1-3. Let $S_{R} \subseteq{1, \ldots, k}$ be the set of variables such that the reward $R(x, a)$ is a function only of $[x]<em R="R">{S</em>$.}}$ ( $x$ restricted to the indices in $S_{R}$ ). Then let $S=\boldsymbol{A N}(R)$ denote the ancestors of $S_{R}$ in the (fully observable) causal graph corresponding to the transition dynamics of $M_{\mathcal{E}}$. Then the state abstraction $\phi_{S}(x)=[x]_{S}$ is a model-irrelevance abstraction for every $e \in \mathcal{E</p>
<p>An important detail in the previous result is the model irrelevance state abstraction incorporates not just the parents of the reward, but also its ancestors. This is because in RL, we seek to model return rather than solely rewards, which requires a state abstraction that can capture multi-timestep interactions. We provide an illustration of this in Figure 2. As a concrete example, we note that in the popular benchmark CartPole, only position $x$ and angle $\theta$ are necessary to predict the reward. However, predicting the return requires $\hat{\theta}$ and $\hat{x}$, their respective velocities.</p>
<p>Learning a minimal $\phi$ in the setting of Theorem 1 using a single training environment may not always be possible. However, applying invariant causal prediction methods in the multi-environment setting will yield the minimal causal set of variables when the training environment interventions satisfy certain conditions necessary for the identifiability of the causal variables (Peters et al., 2016).</p>
<h2>5. Block MDP Generalization Bounds</h2>
<p>We continue to relax the assumptions needed to learn a causal representation and look to the nonlinear setting. As a reminder, the goal of this work is to produce representations that will generalize from the training environments to a novel test environment. However, normal PAC generalization bounds require a much larger number of environments than one could expect to obtain in the reinforcement learning setting. The appeal of an invariant representation is that it may allow for theoretical guarantees on learning the right state abstraction with many fewer training environments, as discussed by Peters et al. (2016). If the learned state abstraction is close to capturing the true base MDP, then the model error in the test environment can be bounded by a function of the distance of the test environment's abstract state distribution to the training environments'. Though the requirements given in the following Theorem 2 are difficult to guarantee in practice, the result will hold for any arbitrary learned state abstraction.
Theorem 2 (Model error bound). Consider an MDP $M$, with $M^{\prime}$ denoting a coarser bisimulation of $M$. Let $\phi$ denote the mapping from states of $M$ to states of $M^{\prime}$. Suppose that the dynamics of $M$ are L-Lipschitz w.r.t. $\phi(X)$ and that $T$ is some approximate transition model satisfying $\max <em M="M">{s} \mathbb{E}|T(\phi(s))-\phi\left(T</em>\right)$ denote the 1-Wasserstein distance. Then
$\mathbb{E}}(s)\right)|&lt;\delta$, for some $\delta&gt;0$. Let $W_{1}\left(\pi_{1}, \pi_{2<em M_prime="M^{\prime">{x \sim M^{\prime}}\left[|T(\phi(x))-\phi\left(T</em>\right)$}}(x)\right)|\right] \leq \delta+2 L W_{1}\left(\pi_{\phi(M)}, \pi_{\phi\left(M^{\prime}\right)</p>
<p>Proof found in Appendix B.
Instead of assuming access to a bisimilar MDP $M^{\prime}$, we can provide discrepancy bounds for an MDP $\bar{M}$ produced by a learned state representation $\phi(x)$, dynamics function $f_{s}$, and reward function $R$ using the distance in dynamics $J_{D}^{\infty}$ and reward $J_{R}^{\infty}$ of $\bar{M}$ to the underlying MDP $M$. We first define these distances,</p>
<p>$$
\begin{aligned}
J_{R}^{\infty} &amp; :=\sup <em D="D">{x \in \mathcal{X}, a \in \mathcal{A}}\left|R\left(\phi(x), a, \phi\left(x^{\prime}\right)\right)-r(x, a)\right| \
J</em> &amp; :=\sup }^{\infty<em 1="1">{x \in \mathcal{X}, a \in \mathcal{A}} W</em>(\phi(x), a), \phi P(x, a)\right)
\end{aligned}
$$}\left(f_{s</p>
<p>Theorem 3. Let $M$ be a block MDP and $\bar{M}$ the learned invariant MDP with a mapping $\phi: \mathcal{X} \mapsto \mathcal{Z}$. For any LLipschitz valued policy $\pi$ the value difference of that policy is bounded by</p>
<p>$$
\left|Q^{\pi}(x, a)-\bar{Q}^{\pi}(\phi(x), a)\right| \leq \frac{J_{R}^{\infty}+\gamma L J_{D}^{\infty}}{1-\gamma}
$$</p>
<p>where $Q^{\pi}$ is the value function for $\pi$ in $M$ and $\bar{Q}^{\pi}$ is the value function for $\pi$ in $\bar{M}$.</p>
<p>Proof found in Appendix B. This gives us a bound on generalization performance that depends on the supremum of
the dynamics and reward errors, which correspondingly is a regression problem that will depend on $\sum_{e \in \mathcal{E}} n_{e}$, the number of samples we have in aggregate over all training environments rather than the number of training environments, $|\mathcal{E}|$. Recent generalization bounds for deep neural networks using Rademacher complexity (Bartlett et al., 2017a; Arora et al., 2018) scale with a factor of $\frac{1}{\sqrt{n}}$ where $n$ is the number of samples. We can use $n:=\sum_{e \in \mathcal{E}} n_{e}$ for our setting, getting generalization bounds for the block MDP setting that scale with the number of samples in aggregate over all environments, an improvement over previous multi-task bounds that depend on $|\mathcal{E}|$.</p>
<h2>6. Methods</h2>
<p>Given these theoretical results, we propose two methods to learn invariant representations in the block MDP setting. Both methods take inspiration from invariant causal prediction, with the first being the direct application of linear ICP to select the causal variables in the state in the setting where variables are given. This corresponds to direct feature selection, which with high probability returns the minimal causal feature set. The second method is a gradient-based approach akin to the IRM objective, with no assumption of a linear causal relationship and a learned causal invariant representation. Like the IRM goal (Equation (1)), we aim to learn an invariant state abstraction from stochastic observations across different interventions $i$, and impose an additional invariance constraint.</p>
<h3>6.1. Variable Selection for Linear Predictors</h3>
<p>The following algorithm (Algorithm 1) returns a modelirrelevance state abstraction. We require the presence of a replay buffer $\mathcal{D}$, in which transitions are stored and tagged with the environment from which they came. The algorithm then applies ICP to find all causal ancestors of the reward iteratively. This approach has the benefit of inheriting many nice properties from ICP - under suitable identifiability conditions, it will return the exact causal variable set to a specified degree of confidence.</p>
<p>It also inherits inconvenient properties: the ICP algorithm is exponential in the number of variables, and so this method is not efficient for high-dimensional observation spaces. We are also restricted to considering linear relationships of the observation to the reward and next state. Further, because we take the union over iterative applications of ICP, the confidence parameter $\alpha$ used in each call must be adjusted accordingly. Given $n$ observation variables, we give a conservative bound of $\frac{\alpha}{n}$.</p>
<p>Algorithm 1 Linear MISA: Model-irrelevance State Abstractions
Result: $S \subset{1, \ldots, k}$, the causal state variables
Input: $\alpha$, a confidence parameter, $\mathcal{D}$, an replay buffer with observations $\mathcal{X}$.
$S \leftarrow \emptyset$
stack $\leftarrow \mathrm{r}$
while stack is not empty do
$v=$ stack.pop()
if $v \notin S$ then
$S^{\prime} \leftarrow \operatorname{ICP}\left(\mathrm{v}, \mathcal{D}, \frac{\alpha}{\operatorname{dim}(X)}\right)$
$S \leftarrow S \cup S^{\prime}$
stack.push $\left(S^{\prime}\right)$
end
end</p>
<h3>6.2. Learning a Model-irrelevance State Abstraction</h3>
<p>We design an objective to learn a dynamics preserving state abstraction $\mathcal{Z}$, or model-irrelevance abstraction (Li et al., 2006), where the similarity of the model is bounded by the model error in the environment setting shown in Figure 1. This requires disentangling the state space into a minimal representation that causes reward $s_{t}:=\phi\left(x_{t}\right)$ and everything else $\eta_{t}:=\varphi\left(x_{t}\right)$. Our algorithm proceeds as follows.</p>
<p>We assume the existence of an invariant state embedding, whose mapping function we denote by $\phi: \mathcal{X} \mapsto \mathcal{Z}$. We also assume an invariant dynamics model $f_{s}: \mathcal{A} \times \mathcal{Z} \mapsto \mathcal{Z}$, a task-specific dynamics model $f_{\eta}: \mathcal{A} \times \mathcal{H} \mapsto \mathcal{H}$, and an invariant reward model $r: \mathcal{Z} \times \mathcal{A} \times \mathcal{Z} \mapsto \mathbb{R}$ in the embedding space. To incorporate a meaningful objective and ground the learned representation, we need a decoder $\phi^{-1}: \mathcal{Z} \times \mathcal{H} \mapsto \mathcal{X}$. We assume $N&gt;1$ training environments are given. The overall dynamics and reward objectives become</p>
<p>$$
\begin{gathered}
J_{D}\left(\phi, \psi, f_{s}, f_{\eta}\right)=\sum_{i} \mathbb{E}<em b__i="b_{i">{\pi</em>\right)\right)\right.\right.\right. \
\left.\left.f_{\eta}\left(a, \psi\left(x_{i}\right)\right)\right)-x_{i}^{\prime}\right)^{2}\right] \
J_{R}(\phi, R)=\sum_{i} \mathbb{E}}}}\left[\left(\phi^{-1}\left(f_{s}\left(a, \phi\left(x_{i<em b__i="b_{i">{\pi</em>\right]
\end{gathered}
$$}}}\left[\left(R\left(\phi\left(x_{i}\right), a, \phi\left(x_{i}^{\prime}\right)\right)-r_{i}^{\prime}\right)^{2</p>
<p>under data collected from behavioral policies $\pi_{b_{i}}$ for each experimental setting.</p>
<p>Of course, this does not guarantee that the representation learned by $\phi$ will be minimal, so we incorporate additional regularization as an incentive. We train a task classifier on the shared latent representation $C: \mathcal{Z} \mapsto[0,1]^{N}$ with cross-entropy loss and employ an adversarial loss (Tzeng et al., 2017) on $\phi$ to maximize the entropy of the classifier output to ensure task specific information is not passing through to $\mathcal{Z}$.</p>
<p>Algorithm 2 Nonlinear Model-irrelevance State Abstraction (MISA) Learning
Result: $\phi$, an invariant state encoder
$\pi \leftarrow \pi_{0}$
$\phi, f_{s} \leftarrow \phi_{0}, f_{s, 0}$
$\psi^{e}, f_{\eta}^{e} \leftarrow \psi_{0}^{e}, f_{\eta, 0}^{e}$ for $e \in \mathcal{E}$
$\mathcal{D}<em e="e">{e} \leftarrow \emptyset$ for $e \in \mathcal{E}$
while forever do
for $e \in \mathcal{E}$ do
$a \leftarrow \pi\left(x</em>\right)$
$x_{e}^{\prime}, r \leftarrow \operatorname{step}\left(x_{e}, a\right)$
$\operatorname{store}\left(x_{e}, a, r, x_{e}^{\prime}\right)$
end
for $e \in \mathcal{E}$ do
Sample batch $X_{e}$ from $\mathcal{D}<em _eta="\eta">{e}$
$f</em>\right)\right]$
end
$f_{s}, \phi, r \leftarrow \sum_{X_{e}} \nabla_{f_{s}, \phi}\left[J_{\mathrm{ALL}}\left(X_{e}\right)\right]$
$C \leftarrow \nabla_{C} \mathrm{CE} _\operatorname{loss}\left(C\left(\phi\left(\left{x_{e}\right}}^{e}, \psi^{e} \leftarrow \nabla_{f_{\eta}^{e}, \psi^{e}}\left[J_{D}\left(X_{e<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e \in \mathcal{E}}\right),{e}</em>\right)\right.$
end}</p>
<p>This gives us a final objective</p>
<p>$$
\begin{aligned}
&amp; J_{\mathrm{ALL}}\left(\phi, \psi, f_{s}, f_{\eta}, r\right)= \
&amp; J_{D}\left(\phi, \psi, f_{s}, f_{\eta}\right)+\alpha_{R} J_{R}(\phi, r)-\alpha_{C} H(C(\phi))
\end{aligned}
$$</p>
<p>where $\alpha_{R}$ and $\alpha_{C}$ are hyperparameters and $H$ denotes entropy (Algorithm 2).</p>
<h2>7. Results</h2>
<p>We evaluate both linear and non-linear versions of MISA, in corresponding Block MDP settings with both linear and non-linear dynamics. First, we examine model error in environments with low-dimensional (Section 7.1.1) and highdimensional (Section 7.1.2) observations and demonstrate the ability for MISA to zero-shot generalize to unseen environments. We next look to imitation learning in a rich observation setting (Section 7.2) and show non-linear MISA generalize to new camera angles. Finally, we explore end-to-end reinforcement learning in the low-dimensional observation setting with correlated noise (Section 7.3) and again show generalization capabilities where single task and multi-task methods fail.</p>
<h3>7.1. Model Learning</h3>
<h3>7.1.1. LINEAR SETTING</h3>
<p>We first evaluate the linear MISA algorithm in Algorithm 1. To empirically evaluate whether eliminating spurious variables from a representation is necessary to guarantee gen-</p>
<p>eralization, we consider a simple family of MDPs with state space $\mathcal{X}={(x_{1},x_{2},x_{3})}$, with a transition dynamics structure such that $x_{1}^{t+1}=x_{1}^{t}+\epsilon_{1}^{e},x_{2}^{t+1}=x_{2}^{t}+\epsilon_{2}^{e}$, and $x_{3}^{t+1}=x_{2}^{t}+\epsilon_{3}^{e}$. We train on 3 environments with soft interventions on each noise variable. We then run the linear MISA algorithm on batch data from these 3 environments to get a state abstraction $\phi(x)={x_{1},x_{2}}$, and then train 2 linear predictors on $\phi(x)$ and $x$. We then evaluate the generalization performance for novel environments that correspond to different hard interventions on the value of the $x_{3}$ variable. We observe that the predictor trained on $\phi(x)$ attains zero generalization error because it zeros out $x_{3}$ automatically. However, any nonzero weight on $x_{3}$ in the least-squares predictor will lead to arbitrarily large generalization error, which is precisely what we observe in Figure 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>Figure 3.</strong> The presence of spurious uncorrelated variables in the state can still lead to poor generalization of linear function approximation methods. Invariant Causal Prediction methods can eliminate these spurious variables altogether.</p>
<h3>7.1.2. RICH OBSERVATION SETTING</h3>
<p>We next test the gradient-based MISA method (Algorithm 2) in a setting with nonlinear dynamics and rich observations. Instead of having access to observation variables and selecting the minimal causal feature set, we are tasked with learning the invariant causal representation. We randomly initialize the background color of two train environments from Deepmind Control (Tassa et al., 2018) from range [0, 255]. We also randomly initialize another two backgrounds for evaluation. The orange line in Figure 4 shows performance on the evaluation environments in comparison to three baselines. In the first, we only train on a single environment and test on another with our method, (MISA - 1 env). Without more than a single experiment to observe at training time, there is no way to disentangle what is causal in terms of dynamics, and what is not. In the second baseline, we combine data from the two environments and train a model over all data (Baseline - 1 decoder). The third is another invariance-based method which uses a gradient penalty, IRM (Arjovsky et al., 2019). In the second case the error is tempered by seeing variance in the two environments at training time, but it is not as effective as MISA with two environments at disentangling what is invariant, and therefore causal with respect to dynamics, and what is not. With IRM, the loss starts much higher but very slowly decreases, and we find it is very brittle to tune in practice. Implementation details found in Appendix C.1.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><strong>Figure 4.</strong> Model error on evaluation environments on Cheetah Run from Deepmind Control. 10 seeds, with one standard error shaded.</p>
<h3>7.2. Imitation Learning</h3>
<p>In this setup, we first train an expert policy using the proprioceptive state of Cheetah Run from (Tassa et al., 2018). We then use this policy to collect a dataset for imitation learning in each of two training environments. When rendering these low dimensional images, we alter the camera angles in the different environments (Figure 5). We report the generalization performance as the test error when predicting actions in Figure 6. While we see test error does increase with our method, MISA, the error growth is significantly slower compared to single task and multi-task baselines.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p><strong>Figure 5.</strong> The Cheetah Run environment from Deepmind Control with different camera angles. The first two images are from the training environments and the last image is from evaluation environment.</p>
<h3>7.3. Reinforcement Learning</h3>
<p>We go back to the proprioceptive state in the cartpole_swingup environment in Deepmind Control (Tassa et al., 2018) to show that we can learn MISA while training a policy. We use Soft Actor Critic (Haarnoja et al., 2018) with an additional linear encoder, and add spurious correlated dimensions which are a multiplicative factor of the original state space. We also add an additional environment identifier to the observation. This multiplicative factor varies across environments, and we train on two environments with 1× and 2×, and test on 3×. Like</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Actor error on evaluation environments on Cheetah Run from Deepmind Control. 10 seeds, with one standard error shaded.</p>
<p>Arjovsky et al. (2019), we also incorporate noise on the causal state to make the task harder, specifically Gaussian noise $\mathcal{N}(0,0.01)$ to the true state dimension. This incentivizes the agent to attend to the spuriously correlated dimension instead, which has no noise. In Figure 7 we see the generalization gap drastically improve with our method in comparison to training SAC with data over all environments in aggregate and with IRM (Arjovsky et al., 2019) implemented on the critic loss. Implementation details and more information about Soft Actor Critic can be found in Appendix C.2.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Generalization gap in SAC performance with 2 training environments on cartpole_swingup from DMC. Evaluated with 10 seeds, standard error shaded.</p>
<h2>8. Related Work</h2>
<h3>8.1. Prior Work on Generalization Bounds</h3>
<p>Generalization bounds provide guarantees on the test set error attained by an algorithm. Most of these bounds are probabilistic and targeted at the supervised setting, falling into the PAC (Probably Approximately Correct) framework. PAC bounds give probabilistic guarantees on a model's true error as a function of its train set error and the complexity of the function class encoded by the model. Many measures of hypothesis class complexity exist: the</p>
<p>Vapnik-Chernovenkis (VC) dimension (Vapnik and Chervonenkis, 1971), the Lipschitz constant, and classification margin of a neural network (Bartlett et al., 2017b), and second-order properties of the loss landscape (Neyshabur et al., 2019) are just a few of many.</p>
<p>Analogous techniques can be applied to Bayesian methods, giving rise to PAC-Bayes bounds (McAllester, 1999). This family of bounds can be optimized to yield nonvacuous bounds on the test error of over-parametrized neural networks (Dziugaite and Roy, 2017), and have demonstrated strong empirical correlation with model generalization (Jiang* et al., 2020). More recently, Amit and Meir (2018); Yin et al. (2019) introduce a PAC-Bayes bound for the multi-task setting dependent on the number of tasks seen at training time.</p>
<p>Strehl et al. (2006) extend PAC framework to reinforcement learning, defining a new class of bounds called PAC-MDP. An algorithm is PAC-MDP if for any $\epsilon$ and $\delta$, the sample complexity of the algorithm is less than some polynomial in $(S, A, 1 / \epsilon, 1 / \delta, 1 /(1-\gamma))$ with probability at least $1-\delta$. The authors provide a PAC-MDP algorithm for model-free Q-learning. Lattimore and Hutter (2012) offers lower and upper bounds on the sample complexity of learning nearoptimal behavior in MDPs by modifying the Upper Confidence RL (UCRL) algorithm (Jaksch et al., 2010).</p>
<h3>8.2. Multi-Task Reinforcement Learning</h3>
<p>Teh et al. (2017); Borsa et al. (2016) handle multi-task reinforcement learning with a shared "distilled" policy (Teh et al., 2017) and shared state-action representation (Borsa et al., 2016) to capture common or invariant behavior across all tasks. No assumptions are made about how these tasks relate to each other other than a shared state and action space.</p>
<p>D'Eramo et al. (2020) show the benefits of learning a shared representation in multi-task settings with an approximate value iteration bound and Brunskill and Li (2013) also demonstrate a PAC-MDP algorithm with improved sample efficiency bounds through transfer across similar tasks. Again, none of these works look to the multi-environment setting to explicitly leverage environment structure. Barreto et al. (2017) exploit successor features for transfer, making the assumption that the dynamics across tasks are the same, but the reward changes. However, they do not handle the setting where states are latent, and observations change.</p>
<h2>9. Discussion</h2>
<p>This work has demonstrated that given certain assumptions, we can use causal inference methods in reinforcement learning to learn an invariant causal representation</p>
<p>that generalizes across environments with a shared causal structure. We have provided a framework for defining families of environments, and methods, for both the low dimensional linear value function approximation setting and the deep RL setting, which leverage invariant prediction to extract a causal representation of the state. We have further provided error bounds and identifiability results for these representations. We see this paper as a first step towards the more significant problem of learning useful representations for generalization across a broader class of environments. Some examples of potential applications include third-person imitation learning, sim2real transfer, and, related to sim2real transfer, the use of privileged information in one task (the simulation) as grounding and generalization to new observation spaces (Salter et al., 2019).</p>
<h2>10. Acknowledgements</h2>
<p>MK has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 834115). The authors would also like to thank Marlos Machado for helpful feedback in the writing process.</p>
<h2>References</h2>
<p>Amit, R. and Meir, R. (2018). Meta-learning by adjusting priors based on extended PAC-Bayes theory. In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 205214, Stockholmsmässan, Stockholm Sweden. PMLR.</p>
<p>Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant Risk Minimization. arXiv e-prints.</p>
<p>Arora, S., Ge, R., Neyshabur, B., and Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. In Krause, A. and Dy, J., editors, 35th International Conference on Machine Learning, ICML 2018, 35th International Conference on Machine Learning, ICML 2018, pages 390-418. International Machine Learning Society (IMLS).</p>
<p>Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. arXiv e-prints.</p>
<p>Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D. (2017). Successor features for transfer in reinforcement learning. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 40554065. Curran Associates, Inc.</p>
<p>Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017a).</p>
<p>Spectrally-normalized margin bounds for neural networks. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 6240-6249. Curran Associates, Inc.</p>
<p>Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017b). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249.</p>
<p>Bertsekas, D. and Castanon, D. (1989). Adaptive aggregation for infinite horizon dynamic programming. Automatic Control, IEEE Transactions on, 34:589 - 598.</p>
<p>Borsa, D., Graepel, T., and Shawe-Taylor, J. (2016). Learning shared representations in multi-task reinforcement learning. CoRR, abs/1603.02041.</p>
<p>Brunskill, E. and Li, L. (2013). Sample complexity of multi-task reinforcement learning. Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013.</p>
<p>Castro, P. S. and Precup, D. (2010). Using bisimulation for policy transfer in mdps. In Twenty-Fourth AAAI Conference on Artificial Intelligence.</p>
<p>Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. (2018). Quantifying generalization in reinforcement learning. CoRR, abs/1812.02341.</p>
<p>D'Eramo, C., Tateo, D., Bonarini, A., Restelli, M., and Peters, J. (2020). Sharing knowledge in multi-task deep reinforcement learning. In International Conference on Learning Representations.</p>
<p>Du, S. S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudík, M., and Langford, J. (2019). Provably efficient RL with rich observations via latent state decoding. CoRR, abs/1901.09018.</p>
<p>Dziugaite, G. K. and Roy, D. M. (2017). Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data.</p>
<p>Eberhardt, F. and Scheines, R. (2007). Interventions and causal inference. Philosophy of Science, 74(5):981-995.</p>
<p>Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M. G. (2019). DeepMDP: Learning continuous latent space models for representation learning. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2170-2179, Long Beach, California, USA. PMLR.</p>
<p>Givan, R., Dean, T. L., and Greig, M. (2003). Equivalence notions and model minimization in markov decision processes. Artif. Intell., 147:163-223.</p>
<p>Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861-1870, Stockholmsmässan, Stockholm Sweden. PMLR.</p>
<p>Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res., 11:1563-1600.</p>
<p>Jiang<em>, Y., Neyshabur</em>, B., Krishnan, D., Mobahi, H., and Bengio, S. (2020). Fantastic generalization measures and where to find them. In International Conference on Learning Representations.</p>
<p>Larsen, K. G. and Skou, A. (1989). Bisimulation through probabilistic testing (preliminary report). In Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL '89, page 344-352, New York, NY, USA. Association for Computing Machinery.</p>
<p>Lattimore, T. and Hutter, M. (2012). Pac bounds for discounted mdps. In International Conference on Algorithmic Learning Theory, pages 320-334. Springer.</p>
<p>Li, L., Walsh, T., and Littman, M. (2006). Towards a unified theory of state abstraction for mdps.</p>
<p>Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2019). Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. In International Conference on Learning Representations.</p>
<p>McAllester, D. A. (1999). Pac-bayesian model averaging. In COLT, volume 99, pages 164-170. Citeseer.</p>
<p>Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2019). The role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations.</p>
<p>Pearl, J. (2009). Causality: Models, Reasoning and Inference. Cambridge University Press, New York, NY, USA, 2nd edition.</p>
<p>Peters, J., Bühlmann, P., and Meinshausen, N. (2016). Causal inference using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, Series B (with discussion), 78(5):9471012 .</p>
<p>Roy, B. V. (2006). Performance loss bounds for approximate value iteration with state aggregation. Math. Oper. Res., 31(2):234-244.</p>
<p>Salter, S., Rao, D., Wulfmeier, M., Hadsell, R., and Posner, I. (2019). Attention privileged reinforcement learning for domain transfer.</p>
<p>Schölkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. (2012). On causal and anticausal learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, page 459-466, Madison, WI, USA. Omnipress.</p>
<p>Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B. (2020). Observational overfitting in reinforcement learning. In International Conference on Learning Representations.</p>
<p>Strehl, A. L., Li, L., Wiewiora, E., Langford, J., and Littman, M. L. (2006). Pac model-free reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pages 881-888. ACM.</p>
<p>Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., and Riedmiller, M. (2018). DeepMind control suite. Technical report, DeepMind.</p>
<p>Teh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R. (2017). Distral: Robust multitask reinforcement learning. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 44964506. Curran Associates, Inc.</p>
<p>Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain adaptation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2962-2971, Los Alamitos, CA, USA. IEEE Computer Society.</p>
<p>Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264-280.</p>
<p>Yarats, D. and Kostrikov, I. (2020). Soft actor-critic (sac) implementation in pytorch. https://github.com/ denisyarats/pytorch_sac.</p>
<p>Yin, M., Tucker, G., Zhou, M., Levine, S., and Finn, C. (2019). Meta-learning without memorization. arXiv preprint arXiv:1912.03820.</p>
<p>Zhang, A., Wu, Y., and Pineau, J. (2018a). Natural environment benchmarks for reinforcement learning. CoRR, abs/1811.06032.</p>
<p>Zhang, C., Vinyals, O., Munos, R., and Bengio, S. (2018b). A study on overfitting in deep reinforcement learning. CoRR, abs/1804.06893.</p>
<h1>A. Notation</h1>
<p>We provide a summary of key notation used throughout the paper here.
$\mathbf{P A}<em _mathcal_G="\mathcal{G">{\mathcal{G}}(X)$ : the parents of node $X$ in the causal graph $\mathcal{G}$. When $\mathcal{G}$ is clear from the setting, abbreviate this notation to $\mathbf{P A}(X)$. $\mathbf{A N}</em>$ omitted when unambiguous).
$[x]}}(X):$ the ancestors of node $X$ in $\mathcal{G}$ (again, $\mathcal{G<em i__1="i_{1">{S}:\left[x</em> \in S\right]$
$\pi_{M}$ : the stationary distribution given by some fixed policy in an MDP $M$.
$q$ : the emission function of a block MDP.
$\mathcal{E}:$ a set of environments.}}, \ldots, x_{i_{k}} \mid i_{j</p>
<h2>B. Proofs</h2>
<p>Technical notes and assumptions. In order for the block MDP assumption to be satisfied, we will require that the interventions defining each environment only occur outside of the causal ancestors of the reward. Otherwise, the different environments will have different latent state dynamics, which violates our assumption that the environments are obtained by an noisy emission function from the latent state space $\mathcal{S}$. Although ICP will still find the correct causal variables in this setting, this state abstraction will no longer be a model irrelevance state abstraction over the union of the environments.
Theorem 1. Consider a family of MDPs $M_{\mathcal{E}}=\left{(\mathcal{X}, A, R, P_{e}, \gamma) \mid e \in \mathcal{E}\right}$, with $\mathcal{X}=\mathbb{R}^{k}$. Let $M_{\mathcal{E}}$ satisfy Assumptions 1-3. Let $S_{R} \subseteq{1, \ldots, k}$ be the set of variables such that the reward $R(x, a)$ is a function only of $[x]<em R="R">{S</em>$.}}$ ( $x$ restricted to the indices in $S_{R}$ ). Then let $S=\boldsymbol{A N}(R)$ denote the ancestors of $S_{R}$ in the (fully observable) causal graph corresponding to the transition dynamics of $M_{\mathcal{E}}$. Then the state abstraction $\phi_{S}(x)=[x]_{S}$ is a model-irrelevance abstraction for every $e \in \mathcal{E</p>
<p>Proof. To prove that $\phi_{S}$ is a model-irrelevance abstraction, we must first show that $r(x)=r\left(x^{\prime}\right)$ for any $x, x^{\prime}: \phi_{S}(x)=$ $\phi_{S}\left(x^{\prime}\right)$. For this, we note that $\mathbb{E}[R(x)]=\int_{r \in \mathbb{R}} r d p(r \mid x)=\int_{r \in \mathbb{R}} r d p(r \mid[x]<em S_C="S^{C">{S},[x]</em>$. Therefore,}})$ and, because by definition $S^{C} \subset$ $\mathbf{P A}(R)^{C}$, we have that $R \perp[x]_{S^{C}</p>
<p>$$
\mathbb{E}[R(x)]=\int_{r \in \mathbb{R}} r d p(r \mid[x]<em _in="\in" _mathbb_R="\mathbb{R" r="r">{S})=\int</em>\right)\right]
$$}} r d p\left(r \mid\left[x^{\prime}\right]_{S}\right)=\mathbb{E}\left[R\left(x^{\prime</p>
<p>To show that $[x]<em 1="1">{S}$ is a MISA, we must also show that for any $x</em>$.}, x_{2}$ such that $\phi\left(x_{1}\right)=\phi\left(x_{2}\right)$, and for any $e \in \mathcal{E}$, the distribution over next state equivalence classes will be equal for $x_{1}$ and $x_{2</p>
<p>$$
\sum_{x^{\prime} \in \phi^{-1}(\bar{X})} P_{x_{1} x^{\prime}}^{e}=\sum_{x^{\prime} \in \phi^{-1}(\bar{X})} P_{x_{2} x^{\prime}}^{e}
$$</p>
<p>For this, it suffices to observe that $S$ is closed under taking parents in the causal graph, and that by construction environments only contain interventions on variables outside of the causal set. Specifically, we observe that the probability of seeing any particular equivalence class $\left[x^{\prime}\right]<em S="S">{S}$ after state $x$ is only a function of $[x]</em>$.</p>
<p>$$
P\left(\left[x^{\prime}\right]<em S="S">{S} \mid x\right)=f\left([x]</em>\right)
$$},\left[x^{\prime}\right]_{S</p>
<p>This allows us to define a natural decomposition of the transition function as follows.</p>
<p>$$
\begin{aligned}
&amp; P\left(x^{\prime} \mid x\right)=P\left([x]<em S_C="S^{C">{S} \oplus[x]</em>\right]}} \mid\left[x^{\prime<em S_C="S^{C">{S} \oplus\left[x^{\prime}\right]</em> \
&amp; P\left(x^{\prime} \mid x\right)=f\left(\left[x^{\prime}\right]}}\right) \text { which by the independent noise assumption gives <em S="S">{S},[x]</em> \mid x\right)
\end{aligned}
$$}\right) P\left(\left[x^{\prime}\right]_{S^{c}</p>
<p>We further observe that since the components of $x$ are independent, $\sum_{\left[x^{\prime}\right]<em S_C="S^{C">{S^{C}}} P\left(\left[x^{\prime}\right]</em> \mid x\right)=1$. We now return to the}</p>
<p>property we want to show:</p>
<p>$$
\begin{aligned}
\sum_{x^{\prime} \in \phi^{-1}(\bar{x})} P_{x_{1} x^{\prime}}^{c} &amp; =\sum_{x^{\prime} \in \phi^{-1}(\bar{x})} f\left(\left[x_{1}\right]<em S="S">{S},\left[x^{\prime}\right]</em>\right) \
&amp; =f\left(\phi\left(x_{1}\right), \bar{x}\right) \sum_{\left[x^{\prime}\right]}\right) P\left(x^{\prime} \mid x_{1<em S_C="S^{C">{S} C} P\left(\left[x^{\prime}\right]</em>\right) \
&amp; =f\left(\phi\left(x_{1}\right), \bar{x}\right)
\end{aligned}
$$}} \mid x_{1</p>
<p>and because $\phi\left(x_{1}\right)=\phi\left(x_{2}\right)$, we have</p>
<p>$$
=f\left(\phi\left(x_{2}\right), \bar{x}\right)
$$</p>
<p>for which we can apply the previous chain of equalities backward to obtain</p>
<p>$$
=\sum_{x^{\prime} \in \phi^{-1}(\bar{x})} P_{x_{2} x^{\prime}}^{c}
$$</p>
<p>Proposition 1 (Identifiability and Uniqueness of Causal State Abstraction). In the setting of the previous theorem, assume the transition dynamics and reward are linear functions of the current state. If the training environment set $\mathcal{E}<em S="S">{\text {train }}$ satisfies any of the conditions of Theorem 2 (Peters et al., 2016) with respect to each variable in $\boldsymbol{A N}(\boldsymbol{R})$, then the causal feature set $\phi</em>$ globally.}$ is identifiable. Conversely, if the training environments don't contain sufficient interventions, then it may be that there exists a $\phi$ such that $\phi$ is a model irrelevance abstraction over $\mathcal{E}_{\text {train }}$, but not over $\mathcal{E</p>
<p>Proof. The proof of the first statement follows immediately from the iterative application of the identifiability result of Peters et al. (2016) to each variable in the causal variables set.</p>
<p>For the converse, we consider a simple counterexample in which one variable $x_{m}$ is constant in every training environment, with value $v_{m}$. Then letting $S=\mathbf{A N}(R)$, we observe that $S \cup{m}$ is also a model-irrelevance state abstraction. First, we show $r\left(x_{1}\right)=r\left(x_{2}\right)$ for any $x_{1}, x_{2}: \phi_{S \cup{m}}\left(x_{1}\right)=\phi_{S \cup{m}}\left(x_{2}\right)$.</p>
<p>$$
\begin{aligned}
p(R \mid x_{1}, a) &amp; =p\left(R\left|x_{1}\right|<em 1="1">{S}, a\right) \
&amp; =p\left(R\left|x</em>\right|<em m="m">{S \cup{m}}, a, m=v</em>\right) \
&amp; =p\left(R\left|\left(x_{2}\right|<em m="m">{S \cup{m}}, a, m=v</em>\right)\right. \
&amp; =p(R \mid x_{2}, a)
\end{aligned}
$$</p>
<p>Finally, we must show that</p>
<p>$$
\sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{X})} P_{x_{1} x^{\prime}}=\sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{X})} P_{x_{2} x^{\prime}}
$$</p>
<p>Again starting from the result of Theorem 1 we have:</p>
<p>$$
\begin{aligned}
\sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{x})} P_{x_{1} x^{\prime}} &amp; =\sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{x})} f\left(x_{1} \mid_{S \cup{m}}, x^{\prime} \mid_{S \cup{m}}\right) p\left(x^{\prime}\left|x_{1}\right|<em m="m">{(S \cup{m})^{C}}, m=v</em>\right) \
&amp; =f\left(\phi_{S \cup{m}}\left(x_{1}\right), \bar{x}\right) \sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{x})} p\left(x^{\prime} \mid x_{1}, m=v_{m}\right) \
&amp; =f\left(\phi_{S \cup{m}}\left(x_{1}\right), \bar{x}\right)
\end{aligned}
$$</p>
<p>and because $\phi_{S \cup{m}}\left(x_{1}\right)=\phi_{S \cup{m}}\left(x_{2}\right)$, we have</p>
<p>$$
=f\left(\phi_{S \cup{m}}\left(x_{2}\right), \bar{x}\right)
$$</p>
<p>for which we can apply the previous chain of equalities backward to obtain</p>
<p>$$
=\sum_{x^{\prime} \in \phi_{S \cup{m}}^{-1}(\bar{x})} P_{x_{2} x^{\prime}}
$$</p>
<p>However, if one of the test environments contains the intervention $x_{m} \leftarrow v_{m}+\mathcal{N}\left(0, \sigma^{2}\right)$, then the distribution over next-states in the new environment will violate the conditions for a model-irrelevance abstraction.</p>
<p>Theorem 2. Consider an MDP $M$, with $M^{\prime}$ denoting a coarser bisimulation of $M$. Let $\phi$ denote the mapping from states of $M$ to states of $M^{\prime}$. Suppose that the dynamics of $M$ are $L$-Lipschitz w.r.t. $\phi(X)$ and that $T$ is some approximate transition model satisfying $\max <em M="M">{s} \mathbb{E}|T(\phi(s))-\phi\left(T</em>\right)$ denote the 1-Wasserstein distance. Then}(s)\right)|&lt;\delta$, for some $\delta&gt;0$. Let $W_{1}\left(\pi_{1}, \pi_{2</p>
<p>$$
\mathbb{E}<em M_prime="M^{\prime">{x \sim M^{\prime}}\left[\left|T(\phi(x))-\phi\left(T</em>\right)
$$}}(x)\right)\right|\right] \leq \delta+2 L W_{1}\left(\pi_{\phi(M)}, \pi_{\phi\left(M^{\prime}\right)</p>
<p>We will use the shorthand $\pi$ for $\pi_{\phi(M)}$, the distribution of state embeddings $\phi(M)$ corresponding to the behaviour policy, and $\pi^{\prime}$ for $\pi_{\phi\left(M^{\prime}\right)}$ for the distribution of state embeddings $\phi\left(M^{\prime}\right)$ given by the behaviour policy.</p>
<p>Proof.</p>
<p>$$
\begin{aligned}
\mathbb{E}<em M_prime="M^{\prime">{x \sim M^{\prime}}\left[|T(\phi(x))-\phi\left(T</em>}}(x)\right)|\right] &amp; =\mathbb{E<em X__M="X_{M" _in="\in" y="y">{x \sim M^{\prime}}\left[\min </em>(x)\right)|\right] \
&amp; \leq \mathbb{E}}}|T(\phi(x))-T(\phi(y))+T(\phi(y))-\phi\left(T_{M<em X__M="X_{M" _in="\in" y="y">{x \sim M^{\prime}}\left[\min </em>|T(\phi(x))-T(\phi(y)) |\right. \
&amp; \left.+\left|T(\phi(y))-\phi\left(T_{M}(y)\right)\right|+\left|\phi\left(T_{M}(y)\right)-\phi\left(T_{M}(x)\right)\right|\right]
\end{aligned}
$$}</p>
<p>Let $\gamma$ be a coupling over the distributions of $\phi\left(M^{\prime}\right)$ and $\phi(M)$ such that $\mathbb{E}<em 1="1">{\gamma(\phi(x), \phi(y))}|\phi(x)-\phi(y)|=W</em>\right)$}\left(\pi, \pi^{\prime</p>
<p>$$
\begin{aligned}
&amp; \leq \mathbb{E}<em _gamma_phi_y_="\gamma(\phi(y)" _mid="\mid" _phi_x_="\phi(x))">{x \sim M^{\prime}}\left[\mathbb{E}</em>|T(\phi(x))-T(\phi(y))|]+\delta+L|x-y|\right] \
&amp; \leq \mathbb{E}<em _gamma_phi_y_="\gamma(\phi(y)" _mid="\mid" _phi_x_="\phi(x))">{x \sim M^{\prime}}\left[\mathbb{E}</em> L|\phi(x)-\phi(y)|+\delta+L|\phi(x)-\phi(y)|\right] \
&amp; =\mathbb{E}<em 1="1">{\gamma(\phi(x), \phi(y))}[L|\phi(x)-\phi(y)|+\delta+L|\phi(x)-\phi(y)|] \
&amp; =2 L W</em>\right)+\delta
\end{aligned}
$$}\left(\pi, \pi^{\prime</p>
<p>Theorem 4 (Existence of model-irrelevance state abstractions). Let $\mathcal{E}$ denote some family of bisimilar MDPs with joint state space $\mathcal{X}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{\mathcal{E}}=\cup</em>$.}} X_{e}$. Let the mapping from states in $M_{e}$ to the underlying abstract MDP $\tilde{M}$ be denoted by $f_{e}$. Then if the states in $X_{\mathcal{E}}$ satisfy $x \in X_{e^{\prime}} \cap X_{e} \Longrightarrow f_{e}(x)=f_{e^{\prime}}(x)$, then $\phi=\cup f_{e}$ is a model-irrelevance state abstraction for $\mathcal{E</p>
<p>Proof. First, note that $\cup f_{e}$ is well-defined (because each $f$ agrees with the rest on the value of all states appearing in multiple tasks). Then $\phi$ will be a model-irrelevance abstraction for every MDP $M_{e}$ because it agrees with $f_{e}$ (a modelirrelevance abstraction).</p>
<p>Theorem 3. Let $M$ be our block MDP and $\tilde{M}$ the learned invariant MDP with a mapping $\phi: \mathcal{X} \mapsto \mathcal{Z}$. For any L-Lipschitz valued policy $\pi$ the value difference is bounded by</p>
<p>$$
\left|Q^{\pi}(x, a)-\bar{Q}^{\pi}(\phi(x), a)\right| \leq \frac{J_{R}^{\infty}+\gamma L J_{D}^{\infty}}{1-\gamma}
$$</p>
<p>Proof.</p>
<p>$$
\begin{aligned}
&amp; \sup <em t="t">{x</em>\right)\right| \
&amp; \leq \sup } \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|Q^{\pi}\left(x_{t}, a_{t}\right)-\bar{Q}^{\pi}\left(\phi\left(x_{t}\right), a_{t<em t="t">{x</em>\right)\right)-r(x, a)\right|+\gamma \sup } \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|R\left(\phi\left(x_{t}\right), a, \phi\left(x_{t+1<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em>} \sim P\left(\cdot \mid x_{t}, a_{t}\right)} V^{\pi}\left(x_{t+1}\right)-\mathbb{E<em t_1="t+1">{z</em>\right)\right| \
&amp; =J_{R}^{\infty}+\gamma \sup } \sim f\left(\cdot \mid \phi\left(x_{t}\right), a_{t}\right)} \bar{V}^{\pi}\left(z_{t+1<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em>} \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[V^{\pi}\left(x_{t+1}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t+1}\right)\right)\right]+\mathbb{E<em t_1="t+1">{\substack{x</em>\right) \
z_{t+1} \sim f\left(\cdot \mid \phi\left(x_{t}\right), a_{t}\right)}}\left[\bar{V}^{\pi}\left(\phi\left(x_{t+1}\right)\right)-\bar{V}^{\pi}\left(z_{t+1}\right)\right]\right| \
&amp; \leq J_{R}^{\infty}+\gamma \sup } \sim P\left(\cdot \mid x_{t}, a_{t<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em>\right)\right)\right]\right| \
&amp; +\gamma \sup } \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[V^{\pi}\left(x_{t+1}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t+1<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em>\right)\right]\right| \
&amp; \leq J_{R}^{\infty}+\gamma \sup } \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[\bar{V}^{\pi}\left(\phi\left(x_{t+1}\right)\right)-\bar{V}^{\pi}\left(z_{t+1<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em>\right)\right)\right]\right|+\gamma L \sup } \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[V^{\pi}\left(x_{t+1}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t+1<em t="t">{x</em>\right)\right) \
&amp; =J_{R}^{\infty}+\gamma \sup } \in \mathcal{X}, a_{t} \in \mathcal{A}} W\left(\phi\left(P\left(\cdot \mid x_{t}, a_{t}\right)\right), f\left(\cdot \mid \phi\left(x_{t}\right), a_{t<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\mathbb{E<em t_1="t+1">{x</em> \
&amp; \leq J_{R}^{\infty}+\gamma \sup } \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[V^{\pi}\left(x_{t+1}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t+1}\right)\right)\right]\right|+\gamma L J_{D}^{\infty<em t="t">{x</em>} \in \mathcal{X}, a_{t} \in \mathcal{A}} \mathbb{E<em t_1="t+1">{x</em> \
&amp; \leq J_{R}^{\infty}+\gamma \sup } \sim P\left(\cdot \mid x_{t}, a_{t}\right)}\left[\left[V^{\pi}\left(x_{t+1}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t+1}\right)\right)\right]\right]+\gamma L J_{D}^{\infty<em t="t">{x</em> \
&amp; \leq J_{R}^{\infty}+\gamma \sup } \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\left[V^{\pi}\left(x_{t}\right)-\bar{V}^{\pi}\left(\phi\left(x_{t}\right)\right)\right]\right|+\gamma L J_{D}^{\infty<em t="t">{x</em> \
&amp; =\frac{J_{R}^{\infty}+\gamma L J_{D}^{\infty}}{1-\gamma}
\end{aligned}
$$} \in \mathcal{X}, a_{t} \in \mathcal{A}}\left|\left[Q^{\pi}\left(x_{t-1}, a_{t-1}\right)-\bar{Q}^{\pi}\left(\phi\left(x_{t-1}\right), a_{t-1}\right)\right]\right|+\gamma L J_{D}^{\infty</p>
<p>Proposition 2 (Lower bound on abstraction error). Let $f_{e}$ be a mapping from $\mathcal{S} \rightarrow \mathcal{X}$. Fix some arbitrary policy $\rho$ and let $v(s)$ denote the value of state $s$ under $\rho$, with $\pi$ its stationary distribution. If $\exists e, e^{\prime}, s, s^{\prime}$ such that $f_{e}(s)=f_{e^{\prime}}\left(s^{\prime}\right)$ (i.e. different states induce the same observation), then the following bound is a lower bound on the error obtained by a joint state abstraction over all environments.</p>
<p>$$
\min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{e} \frac{1}{|\mathcal{E}|} \sum</em>\right) \geq \min }} \operatorname{err}\left(\phi\left(X_{e}\right), \hat{v<em _mathcal_E="\mathcal{E">{s, s^{\prime}: v(s) \neq v\left(s^{\prime}\right)}\left(\left|v(s)-v\left(s^{\prime}\right)\right|\right) P</em>\right.
$$}}\left((\phi(x) \neq f_{e}^{-1}(x)) \geq \delta \frac{H(V(S) \mid X)-1}{\log |V(S)|</p>
<p>Where</p>
<p>$$
\operatorname{err}\left(\phi\left(X_{e}\right), \hat{v}\right):=\mathbb{E}<em e="e">{\pi\left(X</em>(x)\right)|
$$}\right)}|\hat{v}(\phi(x))-v\left(f_{e}^{-1</p>
<p>and</p>
<p>$$
\delta=\min _{s, s^{\prime}: v(s) \neq v\left(s^{\prime}\right)}\left(|v(s)-v\left(s^{\prime}\right)|\right)
$$</p>
<p>Proof. (Sketch) The error obtained by state abstraction will be at least the decoding error of values from abstract states scaled by $\delta$. This in turn depends on how effectively it is possible to decode a potentially lossy mapping from observations back to states. This leads to the second inequality, due to Fano, where the entropy $H(V(S) \mid X)$ is given by marginalizatiion with respect to $v(s)$ of the following probability distributions.</p>
<p>$$
\begin{aligned}
p(x) &amp; =\frac{1}{|\mathcal{E}|} \sum_{s, e} \mathbb{1}\left[f_{e}(s)=x\right] \pi(s) \
p(s \mid x) &amp; =\frac{1}{p(x)} \frac{1}{|\mathcal{E}|} \sum_{e} \pi(s)
\end{aligned}
$$</p>
<h1>C. Implementation Details</h1>
<h2>C.1. Model Learning: Rich Observations</h2>
<p>For the model learning experiments we use an almost identical encoder architecture as in Tassa et al. (2018), with two more convolutional layers to the convnet trunk. Secondly, we use ReLU activations after each convolutional layer, instead of ELU. We use kernels of size $3 \times 3$ with 32 channels for all the convolutional layers and set stride to 1 everywhere, except of the first convolutional layer, which has stride 2 . We then take the output of the convolutional net and feed it into a single fully-connected layer normalized by LayerNorm (Ba et al., 2016). Finally, we add tanh nonlinearity to the 50 dimensional output of the fully-connected layer.</p>
<p>The decoder consists of one fully-connected layer that is then followed by four deconvolutional layers. We use ReLU activations after each layer, except the final deconvolutional layer that produces pixels representation. Each deconvolutional layer has kernels of size $3 \times 3$ with 32 channels and stride 1 , except of the last layer, where stride is 2 .</p>
<p>The dynamics and reward models are all MLPs with two hidden layers with 200 neurons each and ReLU activations.</p>
<h2>C.2. Reinforcement Learning</h2>
<p>For the reinforcement learning experiments we modify the Soft Actor-Critic PyTorch implementation by Yarats and Kostrikov (2020) and augment with a shared encoder between the actor and critic, the general model $f_{s}$ and task-specific models $f_{q}^{r}$. The forward models are multi-layer perceptions with ReLU non-linearities and two hidden layers of 200 neurons each. The encoder is a linear layer that maps to a 50 -dim hidden representation. We also use L1 regularization on the $S$ latent representation. We add two additional dimensions to the state space, a spurious correlation dimension that is a multiplicative factor of the last dimension of the ground truth state, as well as an environment id. We add Gaussian noise $\mathcal{N}(0,0.01)$ to the original state dimension, similar to how Arjovsky et al. (2019) incorporate noise in the label to make the task harder for the baseline.</p>
<p>Soft Actor Critic (SAC) (Haarnoja et al., 2018) is an off-policy actor-critic method that uses the maximum entropy framework to derive soft policy iteration. At each iteration, SAC performs soft policy evaluation and improvement steps. The policy evaluation step fits a parametric soft Q-function $Q\left(x_{t}, a_{t}\right)$ using transitions sampled from the replay buffer $\mathcal{D}$ by minimizing the soft Bellman residual,</p>
<p>$$
J(Q)=\mathbb{E}<em t="t">{\left(x</em>\right]
$$}, x_{t}, r_{t}, x_{t+1}\right) \sim \mathcal{D}}\left[\left(Q\left(x_{t}, a_{t}\right)-r_{t}-\gamma \bar{V}\left(x_{t+1}\right)\right)^{2</p>
<p>The target value function $\bar{V}$ is approximated via a Monte-Carlo estimate of the following expectation,</p>
<p>$$
\bar{V}\left(x_{t+1}\right)=\mathbb{E}<em t_1="t+1">{a</em>\right)\right]
$$} \sim \pi}\left[\bar{Q}\left(x_{t+1}, a_{t+1}\right)-\alpha \log \pi\left(a_{t+1} \mid x_{t+1</p>
<p>where $\bar{Q}$ is the target soft Q-function parameterized by a weight vector obtained from an exponentially moving average of the Q-function weights to stabilize training. The policy improvement step then attempts to project a parametric policy $\pi\left(a_{t} \mid x_{t}\right)$ by minimizing KL divergence between the policy and a Boltzmann distribution induced by the Q-function, producing the following objective,</p>
<p>$$
J(\pi)=\mathbb{E}<em t="t">{x</em>} \sim \mathcal{D}}\left[\mathbb{E<em t="t">{a</em>\right)\right]\right]
$$} \sim \pi}\left[\alpha \log \left(\pi\left(a_{t} \mid x_{t}\right)\right)-Q\left(x_{t}, a_{t</p>
<p>We provide the hyperparameters used for the RL experiments in Table 1.</p>
<h2>D. Additional Results</h2>
<h2>D.1. Reinforcement Learning</h2>
<p>We find that even without noise on the ground truth states, with only two environments, baseline SAC fails (Figure 8).</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Generalization gap in SAC performance with 2 training environments on Cartpole Swingup from DMC. Evaluated with 10 seeds, standard error shaded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter name</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Replay buffer capacity</td>
<td style="text-align: center;">1000000</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Critic learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Critic target update frequency</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Critic Q-function soft-update rate $\tau_{\mathrm{Q}}$</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: left;">Critic encoder soft-update rate $\tau_{\text {enc }}$</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: left;">Actor learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Actor update frequency</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Actor log stddev bounds</td>
<td style="text-align: center;">$[-5,2]$</td>
</tr>
<tr>
<td style="text-align: left;">Encoder learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Decoder learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Decoder weight decay</td>
<td style="text-align: center;">$10^{-7}$</td>
</tr>
<tr>
<td style="text-align: left;">L1 regularization weight</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Temperature learning rate</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Temperature Adam's $\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Init temperature</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 1. A complete overview of used hyper parameters.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution ${ }^{1}$ McGill University ${ }^{2}$ Mila ${ }^{3}$ Facebook AI Research ${ }^{4}$ University of Oxford ${ }^{5}$ Deepmind. Correspondence to: Amy Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#109;&#121;&#46;&#120;&#46;&#122;&#104;&#97;&#110;&#103;&#64;&#109;&#97;&#105;&#108;&#46;&#109;&#99;&#103;&#105;&#108;&#108;&#46;&#99;&#97;">&#97;&#109;&#121;&#46;&#120;&#46;&#122;&#104;&#97;&#110;&#103;&#64;&#109;&#97;&#105;&#108;&#46;&#109;&#99;&#103;&#105;&#108;&#108;&#46;&#99;&#97;</a>, Clare Lyle $&lt;$ clare.lyle@univ.ox.ac.uk $&gt;$.</p>
<p>Proceedings of the $37^{\text {th }}$ International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
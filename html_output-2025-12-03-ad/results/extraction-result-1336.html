<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1336 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1336</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1336</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-a5aad5abb32f6b15f31b92312bb3b0f7b6470977</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5aad5abb32f6b15f31b92312bb3b0f7b6470977" target="_blank">On the evolution of random graphs</a></p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1336",
    "paper_id": "paper-a5aad5abb32f6b15f31b92312bb3b0f7b6470977",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0060585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>THE EVOLUTION OF RANDOM GRAPHS</h1>
<p>BY<br>BÉLA BOLLOBÁs ${ }^{1}$</p>
<h4>Abstract</h4>
<p>According to a fundamental result of Erdös and Rényi, the structure of a random graph $G_{M}$ changes suddenly when $M \sim n / 2$ : if $M=\lfloor c n\rfloor$ and $c&lt;\frac{1}{2}$ then a.e. random graph of order $n$ and since $M$ is such that its largest component has $O(\log n)$ vertices, but for $c&gt;\frac{1}{2}$ a.e. $G_{M}$ has a giant component: a component of order $\left(1-\alpha_{c}+o(1)\right) n$ where $\alpha_{c}&lt;1$. The aim of this paper is to examine in detail the structure of a random graph $G_{M}$ when $M$ is close to $n / 2$. Among others it is proved that if $M=n / 2+s, s=o(n)$ and $s \geq(\log n)^{1 / 2} n^{2 / 3}$ then the giant component has $(4+o(1)) s$ vertices. Furthermore, rather precise estimates are given for the order of the $r$ th largest component for every fixed $r$.</p>
<ol>
<li>Introduction. Let $n$ be a natural number and set $N=\binom{n}{2}$ and $V=$ ${1,2, \ldots, n}$. A graph process on $V$ is a sequence $\left(G_{t}\right)<em t="t">{0}^{N}$ such that (i) each $G</em>$ at time $t$. Clearly a (random) graph process is a Markov chain whose states are graphs on $V$. This Markov chain is a model of the evolution of a random graph (r.g.) with vertex set $V$.}$ is a graph on $V$ with $t$ edges, and (ii) $G_{0} \subset G_{1} \subset \cdots \subset G_{N}$. Let $\tilde{\mathcal{G}}$ be the set of all $N$ ! graph processes. Turn $\tilde{\mathcal{G}}$ into a probability space by giving all members of it the same probability and write $\tilde{G}$ for random elements of $\tilde{\mathcal{G}}$. Furthermore, call $G_{t}$ the state of the process $\tilde{G}=\left(G_{t}\right)_{0}^{N</li>
</ol>
<p>The evolution of random graphs was first studied by Erdös and Rényi [5-7]. They investigated the least values of $t$ for which certain properties are likely to appear, i.e. they studied the stage of the evolution of a r.g. at which a given property first appears. Erdös and Rényi proved the surprising fact that most properties studied in graph theory appear rather suddenly: there are functions $t_{1}(n)&lt;t_{2}(n)$ rather close to each other such that almost no $G_{t_{1}}$ has the property and almost every $G_{t_{2}}$ has the property. (As customary in the theory of random graphs, the term 'almost every' (a.e.) means 'with probability tending to 1 as $n \rightarrow \infty$ '.)</p>
<p>Perhaps the most interesting results of Erdös and Rényi concern the sudden change in the structure of $G_{t}$ around $t=n / 2$.</p>
<p>They proved that if $t \sim c n$ for some constant $c, 0&lt;c&lt;1 / 2$, then a.e. $G_{t}$ is such that its largest component has $O(\log n)$ vertices: if $t \sim c n$ and $c&gt;1 / 2$ then the largest component of a.e. $G_{t}$ has $\left(1-\alpha_{c}+o(1)\right) n$ vertices, where $0&lt;\alpha_{c}&lt;1$; and if $t=\lfloor n / 2\rfloor$ then the maximal size of a component of a.e. $G_{t}$ has order $n^{2 / 3}$. (In fact, Erdös and Rényi $[\mathbf{6}, \mathbf{7}]$ asserted the last statement for $t \sim n / 2$ but, as we</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>shall see, that is not true.) Furthermore,</p>
<p>$$
\alpha_{c}=\frac{1}{2 c} \sum_{k=1}^{\infty} \frac{k^{k-1}}{k!}\left(2 c e^{-2 c}\right)^{k}
$$</p>
<p>so $\alpha_{c} e^{2 c} \rightarrow 1$ as $c \rightarrow \infty$.
The aim of this note is to prove considerably more precise results about the structure of $G_{t}$, especially when $t$ is close to $n / 2$. Among others, our results will shed light on the curious double jump described above.</p>
<p>Loosely speaking, we shall show that a.e. $\hat{G}$ is such that for $t \geq n / 2+$ $(\log n)^{1 / 2} n^{2 / 3}$ the graph $G_{t}$ has a unique component of order at least $n^{2 / 3}$ (the giant component of $G_{t}$ ) and all other components have fewer than $n^{2 / 3} / 2$ vertices. Furthermore, if $n^{\beta_{1}}&lt;s&lt;n^{\beta_{2}}$, where $2 / 3&lt;\beta_{1}&lt;\beta_{2}&lt;1$, then $G_{n / 2-s}$ and $G_{n / 2+s}$ have remarkably similar structures. A.e. $G_{n / 2-s}$ has all its vertices on components of order at most $n^{2}(\log n) / s$ and so has $G_{n / 2+s}$, except for its vertices on its giant component.</p>
<p>The giant component of $G_{n / 2+s}$ has $(4+o(1)) s$ vertices. Most vertices are on small components which are trees. The distributions of these tree-components of $G_{n / 2-s}$ and $G_{n / 2+s}$ are very similar, except in $G_{n / 2+s}$ there are about $1-2 s / n^{2}$ times as many of them as in $G_{n / 2-s}$. As an easy corollary of our results, for $t \sim c n$, $c&gt;1 / 2$, we obtain precise information about the distribution of the order of the $r$ th largest component of $G_{t}$ for every fixed $r$.
2. Definitions and basic facts. In addition to graph processes we shall consider two other models of random graphs. The space $\mathcal{G}(n, M)$ consists of all graphs with $M=M(n)$ edges and with vertex set $V={1,2, \ldots, n}$; the elements of $\mathcal{G}(n, M)$ are equiprobable. The model $\mathcal{G}(n, p)$ consists of all $2^{N}$ graphs with vertex set $V$, in which the probability of a graph with $m$ edges is $p^{m} q^{N-m}$, where $q=1-p$. Thus $\mathcal{G}(n, p)$ consists of all graphs with vertex set $V$ in which the edges are chosen independently and with the same probability $p=p(n), 0&lt;p&lt;1$. For basic properties of these models see [1, Chapter 7 and 3]; for undefined terminology in graph theory see [2]. As customary, we shall talk of random graphs $G_{M}$ and $G_{p}$, meaning that we consider elements of $\mathcal{G}(n, M)$ and $\mathcal{G}(n, p)$. Note that the probability that a r.g. $G_{M}$ has $Q$ is the same as the probability that a graph process $\hat{G}=\left(G_{t}\right)<em t="t">{0}^{N}$ is such that $G</em>$. These remarks imply that most of our results could be formulated for any of our models; nevertheless, there are advantages in considering all three.}$ has $Q$ for $t=M$. Therefore no confusion will arise from the two slightly different meanings of the symbol $G_{M}$. We say that almost every $G_{M}$ (or $G_{p}$ ) has a property $Q$ if the probability that $G_{M}$ (or $G_{p}$ ) has $Q$ tends to 1 as $n \rightarrow \infty$. If $M$ is close to $p N$ then the models $\mathcal{G}(n, M)$ and $\mathcal{G}(n, p)$ are virtually interchangeable. In particular, if $p q N \rightarrow \infty, Q$ is a convex property (that is if $F \subset G \subset H$ and $F$ and $H$ have $Q$ then so does $G$ ) and a.e. $G_{p}$ has $Q$ then so does a.e. $G_{M}$, provided $|M-p n|=O(p q N)^{1 / 2</p>
<p>Denote by $C(k, d)$ the number of connected labelled graphs with $k$ vertices and $k+d$ edges. Thus $C(k, d)=0$ if $d \leq 2, C(k,-1)$ is the number of labelled trees of order $k$, so $C(k,-1)=k^{k-2}$, and $C(k, 0)$ is the number of connected unicyclic graphs. For $d \geq 0$ the function $C(k, d)$ is less simple. It was proved by Katz [9]</p>
<p>and Rényi [10] (and it is also a consequence of more general formulae in [8]) that</p>
<p>$$
C(k, 0)=\frac{1}{2} k^{k-1} \sum_{r=3}^{k} \prod_{j=1}^{r-1}\left(1-\frac{j}{k}\right) \sim\left(\frac{\pi}{8}\right)^{1 / 2} k^{k-1 / 2}
$$</p>
<p>For $d \geq 1$ Wright $[\mathbf{1 1 - 1 3}]$ proved a number of results about $C(k, d)$. He showed that for $d=o\left(k^{1 / 3}\right)$</p>
<p>$$
C(k, d)=f_{d} k^{d+(3 d-1) / 2}\left{1+O\left(d^{3 / 2} / k\right)\right}
$$</p>
<p>where $f_{d}$ depends only on $d$. In particular, $f_{0}=(\pi / 8)^{1 / 2}, f_{1}=5 / 24$ and $f_{2}=$ $5 \sqrt{2} / 128$. We shall not need exact estimates in the vein (2), but we shall need a bound on $C(k, d)$ which is valid for all values of $d$. To be precise, we need the following inequality from [4]: for every $K&gt;0$ there is a constant $c_{0}=c_{0}(K)$ such that</p>
<p>$$
C(k, d) \leq c_{0} K^{-d} k^{k+(3 d-1) / 2}
$$</p>
<p>for every $d,-1 \leq d \leq\binom{k}{2}-k$. In fact, we could manage with considerably cruder bounds than (3) but the calculations become more pleasant if $k^{k+(3 d-1) / 2}$ is multiplied by a factor tending to 0 as $d \rightarrow \infty$, rather than by one increasing with $d$. We could also use the fact that $C(k, d)$ is long concave as a function of $d$. This was proved by Odlyzko, answering a question of mine, by making use of the proof in [13].</p>
<p>A component of a graph is said to be a $(k, d)$-component if it has $k$ vertices and $k+d$ edges. We denote by $X(k, d)$ the number of $(k, d)$-components of a random graph. Note that in $\mathcal{G}(n, p)$ the expectation of $X(k, d)$ is</p>
<p>$$
E_{p}(X(k, d))=\binom{n}{k} C(k, d) p^{k+d}(1-p)^{k(n-k)+\binom{k}{2}-k-d}
$$</p>
<p>In particular, the expected number of tree-components of $G_{p}$ is</p>
<p>$$
E_{p}(X(k,-1))=\binom{n}{k} k^{k-2} p^{k-1}(1-p)^{k n-k^{2} / 2-3 k / 2+1}
$$</p>
<p>For the sake of convenience we shall omit the integrality signs throughout the paper. It is easily seen that the validity of the arguments will remain unaffected. Furthermore, our inequalities are asserted to hold if $n$ is sufficiently large. Finally, $c_{1}, c_{2}, \ldots$ denote positive constants.
3. Gaps in the sequences of components. The key result in proving the sudden emergence of the giant component and in estimating its order rather precisely is that from shortly after time $n / 2$ most graph processes never have a component of order between $n^{2 / 3} / 2$ and $n^{2 / 3}$. The restriction $t \leq 2 n / 3$ in the result below is only for the sake of convenience, it can be easily removed.</p>
<p>THEOREM 1. Let $s_{0}=\left(\frac{6}{7} \log n\right)^{1 / 2} n^{2 / 3}$ and $t_{0}=n / 2+s_{0}$. Then a.e. graph process $\tilde{G}=\left(G_{t}\right)<em 0="0">{0}^{\infty}$ is such that if $t</em>$.} \leq t \leq 2 n / 3$ and $G_{t}$ has a component of order $k$ then either $k<n^{2 / 3} / 2$ or else $k>n^{2 / 3</p>
<p>Proof. Throughout the proof we shall assume that $n^{2 / 3} / 2 \leq k \leq n^{2 / 3}$. Denote by $E_{t}(k)$ the expected number of components of order $k$ in $G_{t}: E_{t}(k)=$</p>
<p>$\sum\left{E_{t}(X(k, d)):-1 \leq d \leq\binom{ k}{2}-k\right}$. In order to prove our theorem it suffices to show that</p>
<p>$$
\sum_{t=t_{0}}^{2 n / 3} \sum_{k=n^{2 / 3} / 2}^{n^{2 / 3}} E_{t}(k)=o(1)
$$</p>
<p>For $d&gt;\binom{k}{2}-k$ we have $X(k, d)=0$ and for $k+d \leq t$ and $-1 \leq d \leq\binom{ k}{2}-k$ the expected number of $(k, d)$-components of $G_{t}$ is</p>
<p>$$
E_{t}(X(k, d))=\binom{n}{k} C(k, d)\binom{n-k\binom{n-k}{2}}{t-k-d} /\binom{N}{t}
$$</p>
<p>Indeed, having chosen the $k+d$ edges of a $(k, d)$-component, the remaining $t-k-d$ edges have to be chosen from a set of $\binom{n-k}{2}$ edges. Set</p>
<p>$$
E_{t}^{\prime}(k)=\sum_{d=-1}^{10 k} E_{t}(X(k+d)) \quad \text { and } \quad E_{t}^{\prime \prime}(k)=E_{t}(k)=E_{t}^{\prime}(k)
$$</p>
<p>We shall estimate separately the sums of the $E_{t}^{\prime}(k)$ 's and $E_{t}^{\prime \prime}(k)$ 's. In the first case we shall make use of (3), but in the second case it will suffice to bound $C(k, d)$ by the total number of graphs with $k$ labelled vertices and $k+d$ edges. In both cases the initial difficulty is that (6) is considerably more unpleasant than (4) with $p=t / N$.
(i) Suppose $\frac{1}{2} n^{2 / 3} \leq k \leq n^{2 / 3}, t_{0} \leq t \leq 2 n / 3$ and $-1 \leq d \leq 10 k$. Then</p>
<p>$$
\begin{aligned}
&amp; \binom{n-k\binom{n-k}{2}}{t-k-d} /\binom{N}{t}=\frac{(t)<em t-k-d="t-k-d">{k+d}\binom{n-k}{2}</em>{(N)}<em t-k-d="t-k-d">{t}} \
&amp; \leq(1+o(1))\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{(k+d)^{2}}{2 t}\right} \frac{\binom{n-k}{2}</em>{(N)}<em 1="1">{t-k-d}} \
&amp; \leq c</em>(t-k-d)\right. \
&amp; \left.\quad-\frac{\left(k n-k^{2} / 2\right)^{2}}{2 N^{2}}(t-k-d)\right} \
&amp; \leq c_{2}\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{k n-k^{2} / 2}{N} t+(k+d)\left[\frac{k n-k^{2} / 2}{N}-\frac{k+d}{2 t}\right]-\frac{k^{2} n^{2}}{2 N^{2}} t\right} \
&amp; \leq c_{3}\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{k n-k^{2} / 2}{N} t+2(k+d) k / n-\frac{(k+d)^{2}}{2 t}-2 k^{2} t / n^{2}\right} \
&amp; \leq c_{3}\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{k n-k^{2} / 2}{N} t\right}
\end{aligned}
$$}\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{(k+d)^{2}}{2 t}-\frac{k n-k^{2} / 2}{N</p>
<p>The last inequality holds since for fixed values of $n, k$ and $d$ the minimum of $(k+d)^{2} / 2 t+2 k^{2} t / n^{2}$ is attained at $t=(k+d) n / 2 k$ and the minimum is exactly $2(k+d) k / n$.</p>
<p>Relations (3), (6) and (7) imply that</p>
<p>$$
\begin{aligned}
E_{t}^{\prime}(k) &amp; \leq c_{4}\binom{n}{k} \sum_{d=-1}^{10 k} 2^{-d} k^{k+(3 d-1) / 2}\left(\frac{t}{N}\right)^{k+d} \exp \left{-\frac{k n-k^{2} / 2}{N} t\right} \
&amp; \leq c_{5}\binom{n}{k} k^{k-2}\left(\frac{t}{N}\right)^{k-1} \exp \left{-\frac{k n-k^{2} / 2}{N} t\right}
\end{aligned}
$$</p>
<p>Setting $s=t-n / 2$ and $\varepsilon=2 s / n$ (so that $t / N$ is well approximated by $(1+\varepsilon) / n$ and $\varepsilon \leq \frac{1}{3}$ ) we find that</p>
<p>$$
\begin{aligned}
E_{t}^{\prime}(k) &amp; \leq c_{6} n k^{-5 / 2} \exp \left{-\frac{k^{2}}{2 n}+k+\varepsilon k-\frac{\varepsilon^{2}}{2} k+\frac{\varepsilon^{3}}{3} k-k+\frac{k^{2}}{2 n}-\varepsilon k+\frac{\varepsilon k^{2}}{2 n}\right} \
&amp; \leq c_{6} n k^{-5 / 2} \exp \left{-\frac{2 \varepsilon^{2}}{5}(1-\varepsilon) k\right}=c_{6} n k^{-5 / 2} \exp \left{-\frac{8}{5} s^{2}\left(1-\frac{2 s}{n}\right) k / n^{2}\right}
\end{aligned}
$$</p>
<p>Consequently</p>
<p>$$
\sum_{k=n^{2 / 3} / 2}^{n^{2 / 3}} E_{t}^{\prime}(k) \leq c_{7} n^{-2 / 3} \exp \left{-\frac{4}{5} s^{2}\left(1-\frac{2 s}{n}\right) n^{-4 / 3}\right} n^{2} s^{-2}
$$</p>
<p>and so</p>
<p>$$
\begin{aligned}
\sum_{s=s_{0}}^{n / 6} \sum_{k=n^{2 / 3} / 2}^{n^{2 / 3}} E_{t}^{\prime}(k) &amp; \leq c_{8} n^{4 / 3} s_{0}^{-2} \exp \left{-\frac{4}{5} s_{0}^{2}\left(1-\frac{2 s_{0}}{n}\right) n^{-4 / 3}\right} n^{4 / 3} / s_{0} \
&amp; \leq c_{9} n^{8 / 3} s_{0}^{-3} \exp \left{-\frac{4}{5} s_{0}^{2} n^{-4 / 3}\right} \
&amp; \leq c_{10} n^{2 / 3}(\log n)^{-3 / 2} \exp \left{-\frac{4}{5}\left(\frac{6}{7} \log n\right)\right} \
&amp; =o(1)
\end{aligned}
$$</p>
<p>(ii) Suppose $\frac{1}{2} n^{2 / 3} \leq k \leq n^{2 / 3}, t_{0} \leq t \leq 2 n / 3$ and $10 k \leq d \leq\binom{k}{2}-k$. In this case rather crude estimtes will suffice:</p>
<p>$$
\binom{\binom{n-k}{2}}{t-k-d} /\binom{N}{t} \leq\binom{N}{t-k-d} /\binom{N}{t} \leq\left(\frac{t}{N}\right)^{k+d} \leq\left(\frac{4}{n}\right)^{k+d}
$$</p>
<p>and</p>
<p>$$
C(k, d) \leq\binom{\binom{k}{2}}{k+d} \leq\left(\frac{e k^{2}}{2(k+d)}\right)^{k+d}
$$</p>
<p>Therefore by (6)</p>
<p>$$
E_{t}(X(k, d)) \leq\left(\frac{e n}{k}\right)^{k}\left(\frac{e k^{2}}{2(k+d)}\right)^{k+d}\left(\frac{4}{n}\right)^{k+d} \leq\left(\frac{k}{n}\right)^{d} \leq n^{-d / 3}
$$</p>
<p>This implies that $E_{t}^{\prime \prime}(k)=o\left(n^{-3}\right)$ and so</p>
<p>$$
\sum_{t=t_{0}}^{2 n / 3} \sum_{k=n^{2 / 3} / 2}^{n^{2 / 3}} E_{t}^{\prime \prime}(k)=o\left(n^{-1}\right)
$$</p>
<p>Let us call a component small if it has fewer than $n^{2 / 3} / 2$ vertices and large if it has more than $n^{2 / 3}$ vertices. Theorem 1 states that a.e. graph process is such that for $t_{0} \leq t \leq 2 n / 3$ every component of $G_{t}$ is either small or large. This will enable us to prove that a.e. graph process such that shortly after time $n / 2$ it has a unique large component (a 'giant' component), and all other components are small. In the sequel we shall need a fairly good upper bound for the variance of the number of small components.</p>
<p>THEOREM 2. Suppose $\Lambda \subset \mathbf{N} \times{\mathbf{N} \cup{0,-1}},-1&lt;\varepsilon=\varepsilon(n) \leq n / 4$ and $p=(1+\varepsilon) / n$. Set</p>
<p>$$
X_{i}=\sum\left{k^{i} X(k, d):(k, d) \in \Lambda\right}, \quad \mu_{i}=E_{p}\left(X_{i}\right)
$$</p>
<p>and $\tilde{k}=\max {k:(k, d) \in \Lambda}$. If $-1&lt;\varepsilon \leq-(1+\varepsilon)^{2} / n$ then $\sigma^{2}\left(X_{i}\right) \leq \mu_{2 i}$ and if $-(1+\varepsilon)^{2} / n \leq \varepsilon$ and $\tilde{k}^{2}\left(\varepsilon+(1+\varepsilon)^{2} / n\right) \leq n$ then</p>
<p>$$
\sigma^{2}\left(X_{i}\right) \leq \mu_{2 i}+\frac{2}{n}\left(\varepsilon+(1+\varepsilon)^{2} / n\right) \mu_{i+1}^{2}
$$</p>
<p>Proof. By relation (4)</p>
<p>$$
E_{p}\left(X_{i}\right)=\sum_{(k, d) \in \Lambda} k^{i}\binom{n}{k} C(k, d) p^{k+d}(1-p)^{k(n-k)+\binom{k}{2}-k-d}
$$</p>
<p>Also, if $\left(k_{1}, d_{1}\right)$ and $\left(k_{2}, d_{2}\right)$ are distinct elements of $\Lambda$ then</p>
<p>$$
\begin{aligned}
&amp; E_{p}\left(X\left(k_{1}, d_{1}\right)\left(k_{2}, d_{2}\right)\right)=\binom{n}{k_{1}}\binom{n-k_{1}}{k_{2}} C\left(k_{1}, d_{1}\right) C\left(k_{2}, d_{2}\right) \
&amp; p^{k_{1}+k_{2}+d_{1}+d_{2}}(1-p)^{\left(k_{1}+k_{2}\right)\left(n-k_{1}-k_{2}\right)+\binom{k_{1}+k_{2}}{2}-k_{1}-k_{2}-d_{1}-d_{2}} \
&amp; =E_{p}\left(X\left(k_{1}, d_{1}\right)\right) E_{p}\left(X\left(k_{2}, d_{2}\right)\right) \frac{(n)<em 1="1">{k</em>{(n)}+k_{2}}<em 1="1">{k</em>(n)}<em 2="2">{k</em> .
\end{aligned}
$$}}}(1-p)^{-k_{1} k_{2}</p>
<p>Similarly, for $(k, d) \in \Lambda$,</p>
<p>$$
E_{p}\left(X(k, d)^{2}\right) \leq E_{p}(X(k, d))+E_{p}(X(k, d))^{2} \frac{(n)<em k="k">{2 k}}{(n)</em>
$$}(n)_{k}}(1-p)^{-k^{2}</p>
<p>Relations (8) and (9) give</p>
<p>$$
\begin{aligned}
&amp; E_{p}\left(X_{i}^{2}\right) \leq E_{p}\left(X_{i}\right)+\sum\left{k_{1}^{i} k_{2}^{i} E\left(X\left(k_{1}, d_{1}\right)\right) E\left(X\left(k_{2}, d_{2}\right)\right)\right. \
&amp;\left.\frac{(n)<em 1="1">{k</em>{(n)}+k_{2}}<em 1="1">{k</em>(n)}<em 2="2">{k</em>\right) \in \Lambda\right}
\end{aligned}
$$}}}(1-p)^{-k_{1} k_{2}}:\left(k_{1}, d_{1}\right),\left(k_{2}, d_{2</p>
<p>Since $1-y \leq(1-x) e^{x-y}$ whenever $0 \leq x \leq y \leq 1$, we find that</p>
<p>$$
\begin{aligned}
\frac{(n)<em 1="1">{k</em>{(n)}+k_{2}}<em 1="1">{k</em>(n)}<em 2="2">{k</em>\right) \
&amp; \leq \exp \left{\sum_{i=1}^{k_{1}-1}\left(\frac{i}{n}-\frac{k_{2}+i}{n}\right)\right}=\exp \left{-k_{1} k_{2} / n\right}
\end{aligned}
$$}}} &amp; =\prod_{i=0}^{k_{1}-1}\left(1-\frac{k_{2}+i}{n}\right) /\left(1-\frac{i}{n</p>
<p>Furthermore,</p>
<p>$$
1-p=1-\frac{1+\varepsilon}{n} \geq \exp \left{-\frac{1+\varepsilon}{n}-\frac{(1+\varepsilon)^{2}}{n^{2}}\right}
$$</p>
<p>so by $(11)$</p>
<p>$$
\begin{aligned}
\frac{(n)<em 1="1">{k</em>{(n)}+k_{2}}<em 1="1">{k</em>(n)}<em 2="2">{k</em>\right} \
&amp; =\exp \left{\frac{k_{1} k_{2}}{n}\left(\varepsilon+\frac{(1+\varepsilon)^{2}}{n}\right)\right}=1+\delta\left(k_{1}, k_{2}\right)
\end{aligned}
$$}}}(1-p)^{-k_{1} k_{2}} &amp; \leq \exp \left{-\frac{k_{1} k_{2}}{n}+\frac{(1+\varepsilon) k_{1} k_{2}}{n}+\frac{(1+\varepsilon)^{2} k_{1} k_{2}}{n^{2}</p>
<p>Now if $\varepsilon+(1+\varepsilon)^{2} / n \leq 0$ then $\delta\left(k_{1}, k_{2}\right) \leq 0$ and so by (10)</p>
<p>$$
\sigma^{2}\left(X_{i}\right) \leq E_{p}\left(X_{2 i}\right)=\mu_{2 i}
$$</p>
<p>Finally, if $-(1+\varepsilon)^{2} / n \leq \varepsilon$ and $\tilde{k}^{2}\left(\varepsilon+(1+\varepsilon)^{2} / n\right) \leq n$ then</p>
<p>$$
\delta\left(k_{1}, k_{2}\right) \leq \frac{2 k_{1} k_{2}}{n}\left(\varepsilon+\frac{(1+\varepsilon)^{2}}{n}\right)
$$</p>
<p>Therefore (10) gives</p>
<p>$$
\begin{aligned}
\sigma^{2}\left(X_{i}\right) &amp; \leq E\left(X_{2 i}\right)+\left(\frac{2 \varepsilon}{n}+\frac{2(1+\varepsilon)^{2}}{n^{2}}\right) \sum\left{k_{1}^{i+1} k_{2}^{i+1} E\left(X\left(k_{1}, d_{1}\right)\right) E\left(X\left(k_{2}, d_{2}\right)\right)\right. \
&amp; \left.\left(k_{1}, d_{1}\right),\left(k_{2}, d_{2}\right) \in \Lambda\right} \
&amp; =\mu_{i}+\left(\frac{2 \varepsilon}{n}+\frac{2(1+\varepsilon)^{2}}{n^{2}}\right) \mu_{i+1}^{2}
\end{aligned}
$$</p>
<p>Armed with Theorem 2, we can locate the maximal order of a component of $G_{p}$ having fewer than $n^{2 / 3}$ vertices, provided $p$ is too close to the critical value $1 / n$. As for $p&gt;1 / n$ this will turn out to be the order of the second largest component, we denote it by $S\left(G_{p}\right)$ :
$S\left(G_{p}\right)=\max \left{k: k=0\right.$ or $\left.k \leq n^{2 / 3}\right.$ and $G_{p}$ has a component of order $\left.k\right}$.
THEOREM 3. Let $-\frac{1}{4}&lt;\varepsilon=\varepsilon(n)&lt;\frac{1}{4}, p=(1+\varepsilon) / n$ and define</p>
<p>$$
g_{\varepsilon}(k)=\log n-\frac{5}{2} \log k+k(\log (1+\varepsilon)-\varepsilon)+2 \log (1 / \varepsilon)
$$</p>
<p>Let $k_{0}=k_{0}(n)$ and $k_{2}=k_{2}(n)&lt;n^{2 / 3}$ be such that</p>
<p>$$
g_{\varepsilon}\left(k_{0}\right) \rightarrow \infty \quad \text { and } \quad g_{\varepsilon}\left(k_{2}\right) \rightarrow-\infty
$$</p>
<p>Then a.e. $G_{p}$ is such that $S\left(G_{p}\right)&lt;k_{2}$ and if $n|\varepsilon|^{3}(\log n)^{-2} \rightarrow \infty$ then a.e. $G_{p}$ is such that $k_{0}&lt;S\left(G_{p}\right)$.</p>
<p>Proof. (i) As in the proof of Theorem 1, for $k \leq n^{2 / 3}$</p>
<p>$$
\sum_{d \geq-1} E_{p}(X(k, d)) \leq c_{1} E_{p}(X(k,-1))
$$</p>
<p>Hence the expected number of components of $G_{p}$ with order between $k_{2}$ and $n^{2 / 3}$ is at most</p>
<p>$$
\begin{aligned}
c_{1} \sum_{k=k_{2}}^{n^{2 / 3}} E_{p} &amp; (X(k,-1)) \
&amp; \leq c_{2} n \sum_{k=k_{2}}^{n^{2 / 3}} k^{-5 / 2} \exp \left{k-k^{2} / 2 n\right}(1+\varepsilon)^{k}\left(1-\frac{1+\varepsilon}{n}\right)^{k n-k^{2} / 2} \
&amp; \leq c_{2} n \sum_{k=k_{2}}^{n^{2 / 3}} k^{-5 / 2} \exp \left{k-\frac{k^{2}}{2 n}+k \log (1+\varepsilon)-k(1+\varepsilon)-\frac{k^{2}}{2 n}(1+\varepsilon)\right} \
&amp; \leq c_{3} n \sum_{k=k_{2}}^{n^{2 / 3}} k^{-5 / 2} \exp {k(\log (1+\varepsilon)-\varepsilon)} \
&amp; \leq c_{4} n k_{2}^{-5 / 2} \exp \left{k_{2}(\log (1+\varepsilon)-\varepsilon)\right} \varepsilon^{-2} \
&amp; =c_{4} \exp \left{g_{\varepsilon}\left(k_{2}\right)\right}
\end{aligned}
$$</p>
<p>By our choice of $k_{2}$ we have $g_{\varepsilon}\left(k_{2}\right) \rightarrow-\infty$ so almost no $G_{p}$ has a component whose order is between $k_{2}$ and $n^{2 / 3}$.
(ii) In the proof of the second inequality we may and shall assume that $n|\varepsilon|^{3}(\log n)^{-2} \rightarrow \infty$ and $k_{0} \rightarrow \infty$. Set $k_{\varepsilon}=\left\lfloor 8(\log n) \varepsilon^{-2}\right\rfloor, \Lambda=\left{(k,-1): k_{0} \leq\right.$ $\left.k \leq k_{\varepsilon}\right}$ and let $X_{0}$ be as in Theorem 2. Then $g_{\varepsilon}\left(k_{\varepsilon}\right) \rightarrow-\infty$ and</p>
<p>$$
\mu_{0}=E\left(X_{0}\right)=\sum_{k=k_{0}}^{k_{\varepsilon}} E\left(T_{k}\right) \sim \frac{n}{\sqrt{2 \pi}} \sum_{k=k_{0}}^{k_{\varepsilon}} k^{-5 / 2} \exp {k(\log (1+\varepsilon)-\varepsilon)}
$$</p>
<p>Clearly $k_{\varepsilon}&gt;k_{0}+\varepsilon^{2}$ so by the choice of $k_{0}$ we have $\mu_{0} \rightarrow \infty$. Furthermore, we may suppose that $\mu_{0}$ does not grow too fast, say $\mu_{0}=o\left(n|\varepsilon|^{3}(\log n)^{-2}\right)$. Then</p>
<p>$$
\mu_{0} \varepsilon k_{\varepsilon}^{2} / n=O\left(\mu_{0} \varepsilon(\log n)^{2} \varepsilon^{-4} n^{-1}\right)=o(1)
$$</p>
<p>Theorem 2 implies that</p>
<p>$$
\sigma^{2}\left(X_{0}\right) \leq \mu_{0}+3 \varepsilon \mu_{1}^{2} / n \leq \mu_{0}+3 \varepsilon \mu_{0}^{2} k_{\varepsilon}^{2} / n=\mu_{0}(1+o(1))
$$</p>
<p>so by Chebyshev's inequality $P(X&gt;0) \rightarrow 1$.
Let us state some explicit bounds for $S\left(G_{p}\right)$ implied by Theorem 3.
COROLLARY 4. Let $p=(1+\varepsilon) / n$.
(i) If $0&lt;\varepsilon&lt;\frac{1}{4}$ is fixed then a.e. $G_{p}$ satisfies $S\left(G_{p}\right) \leq 3(\log n) \varepsilon^{-2}$,
(ii) If $n^{-\gamma_{0}} \leq \varepsilon=o\left((\log n)^{-1}\right)$ and $\omega(n) \rightarrow \infty$ then</p>
<p>$$
\left|S\left(G_{p}\right)-(2 \log n+6 \log \varepsilon-5 \log \log n) \varepsilon^{-2}\right| \leq \omega(n) \varepsilon^{-2}
$$</p>
<p>for a.e. $G_{p}$.
Proof. (i) All we have to check is that $g_{\varepsilon}\left(3(\log n) \varepsilon^{-2}\right) \rightarrow-\infty$.
(ii) Straightforward calculations show that</p>
<p>$$
k_{0}=(2 \log n+6 \log \varepsilon-5 \log \log n-\omega(n)) \varepsilon^{-2}
$$</p>
<p>and</p>
<p>$$
k_{2}=(2 \log n+6 \log \varepsilon-5 \log \log n+\omega(n)) \varepsilon^{-2}
$$</p>
<p>satisfy the conditions in Theorem 3.
The corollary above enables us to prove an analog of Theorem 1 for $t \geq \frac{2}{3} n$.
THEOREM 5. A.e. graph process $\tilde{G}=\left(G_{t}\right)<em t="t">{0}^{N}$ is such that for $t \geq 5 n / 8$ the graph $G</em>$ has no small component of order at least $100 \log n$.</p>
<p>Proof. By Corollary 4 a.e. graph process is such that for some $t$ satisfying $3 n / 5&lt;t&lt;5 n / 8$ the graph $G_{t}$ has no small component whose order is at least $75 \log n$. Since the union of two components of order at most $a=\lceil 100 \log n\rceil$ has order at most $2 a&lt;n^{2 / 3}$, the assertion of the theorem will follow if we show that a.e. graph process is such that for $t \geq 3 n / 5$ the graph $G_{t}$ has no component whose order is at least $a$ and at most $2 a$. Furthermore, since for $t \geq 2 n \log n$ a.e. $G_{t}$ is connected, it suffices to prove this for $t \leq 2 n \log n$.</p>
<p>Let $\frac{3}{5} n \leq t \leq 2 n \log n$ and set $c=2 t / n$. Then the expected number of components of $G_{t}$ having order at least $a$ and at most $2 a$ is</p>
<p>$$
\begin{aligned}
E\left(\sum_{k=a}^{2 a} \sum_{d \geq-1} X(k, a)\right) &amp; =\sum_{k=a}^{2 a}\binom{n}{k} \sum_{d \geq-1} C(k, d)\binom{\binom{n-k}{2}<em 0="0">{t-k-d}}{t}\left\langle\binom{ N}{t}\right. \
&amp; \leq c</em>} \sum_{k=a}^{2 a}(e n)^{k} k^{-5 / 2}\binom{\binom{n-k}{2<em 1="1">{t-k+1}}{t}\left\langle\binom{ N}{t}\right. \
&amp; \leq c</em> \
&amp; \leq c_{2} n \sum_{k=a}^{2 a} e^{k} k^{-5 / 2} c^{k} e^{-c k} \
&amp; =c_{2} n \sum_{k=a}^{2 a} k^{-5 / 2}\left(c e^{1-c}\right)^{k}
\end{aligned}
$$} \sum_{k=a}^{2 a}(e n)^{k} k^{-5 / 2}\left(\frac{t}{N}\right)^{k-1}\left(\frac{N-k n}{N}\right)^{t-k</p>
<p>where $c_{0}, c_{1}$ and $c_{2}$ are absolute constants. Since for $c=1.2$ we have $c-1-\log c&gt;$ 0.0176 , the last expression is at most</p>
<p>$$
c_{2} n(\log n)^{-3 / 2} n^{-1.7}=o\left(n^{-1 / 2}\right)
$$</p>
<ol>
<li>The emergence of the giant component. Given a graph process $\tilde{G}=$ $\left(G_{t}\right)<em t="t">{0}^{2 k}$, denote by $w</em>$ and let}$ the number of components of $G_{t</li>
</ol>
<p>$$
V=\bigcup_{i=1}^{w_{t}} U_{i}\left(G_{t}\right)
$$</p>
<p>be the partition of $V$ into vertex sets of its components. Note that for every $t$ either $G_{t}$ and $G_{t+1}$ define the same partition of $V$ or else $w_{t+1}=w_{t}-1$ and the partition defined by $G_{t}$ is a refinement of the partition defined by $G_{t+1}$. Hence if $\tilde{G}$ is such that for $t \geq t_{0}$ the graph $G_{t}$ does not have a component whose order is between $n^{2 / 3} / 2$ and $n^{2 / 3}$ then for $t_{0} \leq t \leq t^{\prime}$ the graph $G_{t^{\prime}}$ has at most as many components of order at least $n^{2 / 3}$ as $G_{t}$.</p>
<p>Moreover, if in such a $\tilde{G}$ the graph $G_{t}$ has a component which contains all large components of $G_{t_{0}}$ then $G_{t}$ has a unique component of order at least $n^{2 / 3}$ and so has $G_{t^{\prime}}$ for every $t^{\prime} \geq 1$. Therefore in order to establish the existence of the giant component, we have to show that the large components of $G_{t_{0}}$ are contained in a single component of $G_{t}$ for some $t&gt;t_{0}$. Furthermore, we have to estimate the number of vertices on the giant component or, equivalently, the number of vertices on the small components. We start with the second task. For the sake of convenience we take $\varepsilon=p n-1=n^{-\gamma}$.</p>
<p>THEOREM 6. Let $0&lt;\gamma&lt;\frac{1}{3}, \varepsilon=n^{-\gamma}, p=(1+\varepsilon) / n$ and $\omega(n) \rightarrow \infty$. For a graph $G$, denote by $Y_{1}(G)$ the number of vertices on the small $(k, d)$-components with $d \geq 1$, by $Y_{0}(G)$ the number of vertices on the small unicyclic components and by $Y_{-1}(G)$ the number of vertices on the small tree-components. Then a.e. $G_{p}$ is such that</p>
<p>$$
Y_{1} \leq \omega(n) n^{5 \gamma-1}, \quad Y_{0} \leq \omega(n) n^{2 \gamma}
$$</p>
<p>and</p>
<p>$$
Y_{-1}=n-2 n^{1-\gamma}+O\left(\omega(n) n^{(1+\gamma) / 2}+(\log n) n^{1-2 \gamma}\right)
$$</p>
<p>Proof. Set $k_{3}=9(\log n) n^{2 \gamma}$. Then for $k_{3} \leq k \leq n^{2 / 3}$ we have</p>
<p>$$
\varepsilon^{2} k / 2-k \varepsilon^{3} / 3-k^{2} \varepsilon /(2 n) \geq \varepsilon^{2} k / 3 \geq 3 \log n
$$</p>
<p>so</p>
<p>$$
\begin{aligned}
\sum_{k=k_{3}}^{n^{2 / 3}} \sum_{d=-1}^{\left(k_{2}^{-1}\right)} E(k X(k, d)) &amp; =O\left(\sum_{k=k_{3}}^{n^{2 / 3}} E(k X(k,-1))\right) \
&amp; =O\left(n \sum_{k=k_{3}}^{n^{2 / 3}} k^{-3 / 2} \exp \left{-\varepsilon^{2} k / 2+k \varepsilon^{3} / 3+k^{2} \varepsilon /(2 n)\right}\right) \
&amp; =O\left(n^{-2} \sum_{k=k_{3}}^{n^{2 / 3}} k^{-3 / 2}\right)=o\left(n^{-2}\right)
\end{aligned}
$$</p>
<p>This shows that in our proof we may replace $Y_{i}$ by</p>
<p>$$
\tilde{Y}<em _leq="\leq" k="k" k__3="k_{3">{i}=\sum</em> k X(k, i), \quad i=-1,0,1
$$}</p>
<p>(i) Note first that by (3)</p>
<p>$$
\begin{aligned}
E\left(\tilde{Y}<em _leq="\leq" k="k" k__3="k_{3">{1}\right) &amp; =O\left(\sum</em>\right) \
&amp; =O\left(n^{-1} \sum_{k \leq k_{3}} k^{3 / 2} \exp \left{-\varepsilon^{2} k / 2\right}\right)
\end{aligned}
$$}} k\binom{n}{k} C(k, 1) p^{k+1}(1-p)^{k n-k^{2} / 2</p>
<p>since for $1 \leq k \leq k_{3}$ we have</p>
<p>$$
k \varepsilon^{3} / 3+k^{2} \varepsilon /(2 n)=O(1)
$$</p>
<p>Hence</p>
<p>$$
E\left(\tilde{Y}_{1}\right)=O\left(n^{-1} \varepsilon^{-5}\right)=O\left(n^{5 \gamma-1}\right)
$$</p>
<p>implying the first assertion.
(ii) The expectation of $\tilde{Y}_{0}$ is easily estimated:</p>
<p>$$
\begin{aligned}
E\left(\tilde{Y}<em _leq="\leq" k="k" k__3="k_{3">{0}\right) &amp; =\sum</em> k E(X(k, 0)) \
&amp; =\sum_{k \leq k_{3}} k\binom{n}{k} C(k, k)\left(\frac{1+\varepsilon}{n}\right)^{k}\left(1-\frac{1-\varepsilon}{n}\right)^{k n-k^{2} / 2+3 k / 2} \
&amp; \sim \frac{1}{\sqrt{2 \pi}} \sqrt{\pi / 8} \sum_{k \leq k_{3}} \exp \left{-k \varepsilon^{2} / 2\right} \
&amp; \sim \frac{1}{4} \int_{0}^{\infty} \exp \left{-x \varepsilon^{2} / 2\right} d x \sim \frac{1}{2} n^{2 \gamma}
\end{aligned}
$$}</p>
<p>Therefore, by Chebyshev's inequality, $\tilde{Y}<em p="p">{0} \leq \omega(n) n^{2 \gamma}$ for a.e. random graph $G</em>$.
(iii) We shall estimate $E\left(\tilde{Y}<em p="p">{-1}\right)$ rather precisely and then we shall make use of Theorem 2 to conclude that for a.e. $G</em>$ is rather close to its expectation.}$ the variable $Y_{-1</p>
<p>Set $p^{\prime}=(1-\varepsilon) / n$ and write $E^{\prime}$ for the expectation in $\mathcal{G}\left(n, p^{\prime}\right)$. We shall exploit the fact that $E$ and $E^{\prime}$ are rather closely related.</p>
<p>First of all, calculations analogous to those in (i) and (ii) show that</p>
<p>$$
E^{\prime}\left(n-\tilde{Y}<em -1="-1">{0}-\tilde{Y}</em>
$$}\right)=o\left(n^{2 \gamma}\right) \quad \text { and } \quad E^{\prime}\left(Y_{0}\right) \sim \frac{1}{2} n^{2 \gamma</p>
<p>Hence</p>
<p>$$
E^{\prime}\left(\tilde{Y}_{-1}\right)=n-\frac{1}{2} n^{2 \gamma}-o\left(n^{2 \gamma}\right)
$$</p>
<p>In order to pass from $E^{\prime}\left(\tilde{Y}<em -1="-1">{-1}\right)$ to $E\left(\tilde{Y}</em>$ we have}\right)$, note that for $k \leq k_{3</p>
<p>$$
\begin{aligned}
E(X(k,-1)) / E^{\prime}(X(k,-1)) &amp; =\left(\frac{1+\varepsilon}{1-\varepsilon}\right)^{k-1}\left(\frac{1-(1+\varepsilon) / n}{1-(1-\varepsilon) / n}\right)^{k n-k^{2} / 2+3 k / 2-1} \
&amp; =\exp \left{2(k-1) \varepsilon-\left(2 k n-k^{2}\right) \varepsilon / n+O\left((\log n) n^{-2 \gamma}\right)\right} \
&amp; =\exp \left{-2 \varepsilon+k^{2} \varepsilon / n+O\left((\log n) n^{-2 \gamma}\right)\right} \
&amp; =1-2 \varepsilon+O\left(k^{2} \varepsilon / n\right)+O\left((\log n) n^{-2 \gamma}\right)
\end{aligned}
$$</p>
<p>Consequently</p>
<p>$$
\begin{aligned}
E\left(\tilde{Y}<em -1="-1">{-1}\right)= &amp; (1-2 \varepsilon) E^{\prime}\left(\tilde{Y}</em>\right) \
&amp; +O\left(\frac{\varepsilon}{n} \sum_{k \leq k_{3}} k^{3}\binom{n}{k} E^{\prime}(X(k,-1))\right) \
= &amp; n-2 n^{1-\gamma}+O\left(n^{2 \gamma}\right)+O\left((\log n) n^{1-2 \gamma}\right) \
&amp; +O\left(\varepsilon \sum k^{1 / 2} \exp \left{-k \varepsilon^{2} / 2\right}\right) \
= &amp; n-2 n^{1-\gamma}+O\left(n^{2 \gamma}+(\log n) n^{1-2 \gamma}\right)
\end{aligned}
$$}\right)+O\left((\log n) n^{1-2 \gamma</p>
<p>Finally, in order to apply Theorem 2 we have to estimate the following expectation:</p>
<p>$$
E\left(\sum_{k \leq k_{3}} k^{2} X(k,-1)\right)=O\left(n \sum_{k \leq k_{3}} k^{-1 / 2} \exp \left{-k \varepsilon^{2} / 2\right}\right)=O\left(n^{1+\gamma}\right)
$$</p>
<p>By Theorem 2 we have $\sigma^{2}\left(\tilde{Y}<em p="p">{-1}\right)=O\left(n^{1+\gamma}\right)$ so by Chebyshev's inequality a.e. $G</em>$ satisfies</p>
<p>$$
\left|\tilde{Y}<em -1="-1">{-1}-E\left(\tilde{Y}</em>
$$}\right)\right| \leq \omega(n) n^{(1+\gamma) / 2</p>
<p>Relations (12) and (13) imply that</p>
<p>$$
\tilde{Y}_{-1}=n-2 n^{1-\gamma}+O\left(\omega(n) n^{(1+\gamma) / 2}+(\log n) n^{1-2 \gamma}\right)
$$</p>
<p>as claimed.
Let us establish now the emergence of the giant component shortly after time $n / 2$.</p>
<p>THEOREM 7. A.e. graph process $\tilde{G}=\left(G_{t}\right)<em 1="1">{0}^{N}$ is such that for every $t \geq t</em> / 2$ vertices each.}=$ $n / 2+(\log n)^{1 / 2} n^{2 / 3}$ the graph $G_{t}$ has a unique component of order at least $n^{2 / 3}$. The other components of $G_{t}$ have at most $n^{2 / 3</p>
<p>Proof. As before, we call a component small if it has fewer than $n^{2 / 3} / 2$ vertices, and large if it has at least $n^{2 / 3}$ vertices. By Theorem 1 a.e. graph process $\tilde{G}$ is such that for $t \geq t_{0}=n / 2+s_{0}$ every component of $G_{t}$ is either small or large, where $t_{0}$ is as defined in Theorem 1. Let $C_{1}, C_{2}, \ldots, C_{l}$ be the large components of $G_{t_{0}}$ in such a $\tilde{G}$ and suppose in some $G_{t}, t \geq t_{0}$, all the $C_{i}$ 's are contained in the same component. Then this component of $G_{t}$ is the unique large component and for $t^{\prime} \geq t$ the graph $G_{t^{\prime}}$ has also a unique large component. Indeed, as $t$ increases from $t_{0}$, a vertex $x \in V$ can become a vertex of a large component only if that component contains a $C_{i}$, for the union of two small components contains fewer than $n^{2 / 3}$ vertices. Consequently our theorem will follow if we show that a.e. $\tilde{G}$ is such that for some $t$ between $t_{0}$ and $t_{1}$ all large components of $G_{t_{0}}$ are contained in the same component of $G_{t}$. Imitating the proof of Theorem 6 , one can show that a.e. $\tilde{G}$ is such that $G_{t_{0}}$ has at least $2 s_{0}&gt;(\log n)^{1 / 2} n^{2 / 3}$ vertices on its large components. (In fact, the expected number of these vertices is about $4 s_{0}$.) Therefore one can find disjoint subsets $V_{1}, V_{2}, \ldots, V_{m}$ of $V\left(G_{t_{0}}\right)=V$ such that</p>
<p>$$
m \geq(\log n)^{1 / 2} / 2, \quad\left|V_{i}\right| \geq n^{2 / 3}, \quad i=1, \ldots, m
$$</p>
<p>each $V_{i}$ is contained in some $V\left(C_{j}\right)$ and</p>
<p>$$
\bigcup_{i=1}^{m} V_{i}=\bigcup_{j=1}^{l} V\left(C_{j}\right)
$$</p>
<p>where $C_{1}, \ldots, C_{l}$ are the large components of $G_{t_{0}}$.
Set $p=n^{-4 / 3}$ and denote by $H_{p}$ the random graph obtained from $G_{t_{0}}$ by adding to it edges independently and with probability $p$. Then a.e. $H_{p}$ has at most $t_{0}+n^{2 / 3}&lt;t_{1}$ edges so it suffices to show that $\bigcup_{i-1}^{m} V_{i}$ is contained in a single component in a.e. $H_{p}$.</p>
<p>What is the probability that for a given pair $(i, j), 1 \leq i&lt;j \leq m$, some edge of $H_{p}$ joins $V_{i}$ to $V_{j}$ ? It is clearly at least</p>
<p>$$
1-(1-p)^{n^{4 / 3}} \geq 1-e^{-1}&gt;1 / 2
$$</p>
<p>Hence the probability that in $H_{p}$ all $V_{i}$ 's are contained in the same component is at least the probability that an element of $G\left(m, \frac{1}{2}\right)$ is connected. Since $m \rightarrow \infty$, this probability tends to 1. (See [5 or 2, p. 140] for considerably stronger results.) This completes our proof.</p>
<p>Combining Theorems 3, 6 and 7, we obtain rather precise information about the orders of the largest components. Since the property of having at most $x$ vertices on the small tree components is monotone, and so are the properties of having at least $y$ vertices on the large components and at most $z$ vertices on the small components which are trees or unicyclic graphs, using [2, Theorem 8, p. 133] we may pass from the model $\mathcal{G}(n, p)$ to the model $\mathcal{G}(n, M)$. Denote by $L_{r}(G)$ the $r$ th largest order of a component of a graph $G$.</p>
<p>THEOREM 8. Let $0&lt;\gamma&lt;\frac{1}{3}, s=\frac{1}{2} n^{1-\gamma}$ and $t=n / 2+s$. Then for every $m \in \mathbf{N}$ and $\omega(n) \rightarrow \infty$ a.e. $G_{t}$ is such that</p>
<p>$$
L_{1}(G)=4 s+O\left(\omega(n) n / s^{1 / 2}+(\log n) s^{2} / n\right)
$$</p>
<p>and</p>
<p>$$
k_{0}&lt;L_{m}(G) \leq L_{m-1}(G) \leq \cdots \leq L_{2}(G)&lt;k_{2}
$$</p>
<p>where $k_{0}$ and $k_{2}$ are as in Theorem 3.
Before extending the range of $t$ in the theorem above, we investigate the distribution of the small components of $G_{p}$ in the case when $p$ is not as close to $1 / n$ as has been required so far.
5. Components of order less than $n^{2 / 3}$. First we consider the small components of $G_{p}$ with $p&lt;1 / n$.</p>
<p>THEOREM 9. Let $0&lt;\varepsilon=\varepsilon(n)=o(1)$ be such that $\varepsilon n^{\eta} \rightarrow \infty$ for every fixed $\eta&gt;0$ and set $p=(1-\varepsilon) / n$. Given $\lambda \in \mathbf{R}^{+}$, choose $l_{\lambda}=l_{\lambda}(n)$ in such a way that</p>
<p>$$
\mu\left(n, \varepsilon, l_{\lambda}\right)=(2 / \pi)^{1 / 2} n l_{\lambda}^{-5 / 2}\left((1-\varepsilon) e^{\varepsilon}\right)^{l_{\lambda}} \varepsilon^{-2} \rightarrow \lambda
$$</p>
<p>and denote by $Z=Z\left(G_{p}\right)$ the number of components of $G_{p}$ having at least $l_{\lambda}$ vertices. Then the distribution of $Z$ tends to $P_{\lambda}$, the Poisson distribution with mean $\lambda$.</p>
<p>Proof. The assumptions imply easily that $l_{\lambda} \rightarrow \infty, l_{\lambda}=o\left(n^{\eta}\right)$ for every $\eta&gt;0$ and the expected number of vertices on components containing cycles is bounded. Hence we may assume that $Z$ is the number of tree-components of order at least $l_{\lambda}$. Since also $l_{\lambda} \varepsilon^{2} \rightarrow \infty$, a trite calculation shows that</p>
<p>$$
\begin{aligned}
E(Z) &amp; =\sum_{k=l_{\lambda}}^{n} E(X(k,-1)) \sim \frac{n}{\sqrt{2 \pi}} \sum_{k=l_{\lambda}}^{n} k^{-5 / 2}\left((1-\varepsilon) e^{\varepsilon}\right)^{k} \
&amp; \sim \frac{n}{\sqrt{2 \pi}} l_{\lambda}^{-5 / 2}\left((1-\varepsilon) e^{\varepsilon}\right)^{k} /\left(1-(1-\varepsilon) e^{\varepsilon}\right) \sim \mu\left(n, \varepsilon, l_{\lambda}\right) \sim \lambda
\end{aligned}
$$</p>
<p>Furthermore, by making use of $l_{\lambda}=o\left(n^{n}\right)$ it is easily shown that for every $r \in \mathbf{N}$ the $r$ th factorial moment</p>
<p>$$
E_{r}(Z)=E(Z(Z-1) \cdots(Z-r+1))
$$</p>
<p>converges to $\lambda^{r}$. Hence $Z \xrightarrow{d} P_{\lambda}$, as claimed.
Corollary 10. Let $\varepsilon, p$ and $l_{\lambda}$ be as in Theorem 9 , and let $\omega(n) \rightarrow \infty$. Then for every $m \in \mathbf{N}$ a.e. $G_{p}$ satisfies</p>
<p>$$
l_{1}-\omega(n) / \varepsilon \leq L_{m}\left(G_{p}\right) \leq \cdots \leq L_{1}\left(G_{p}\right) \leq l_{1}+\omega(n) / \varepsilon
$$</p>
<p>Proof. It is easily seen that for every fixed $\lambda$ we have
$l_{\lambda} \sim(\log n) /(\varepsilon-\log (1 /(1-\varepsilon))) \sim 2(\log n) / \varepsilon \quad$ and $\quad l_{1}-\omega(n) / \varepsilon&lt;l_{\lambda}&lt;l_{1}+\omega(n) / \varepsilon$.
Hence the assertion follows from Theorem 9.
If $p=c / n$ for some constant $c&lt;1$ then we need not be able to find numbers $l_{\lambda}=l_{\lambda}(n)$ ensuring that $\mu\left(n, 1-c, l_{\lambda}\right) \rightarrow \lambda$. Nevertheless, with some simple changes the results above carry over to this case without any difficulty. In fact, our task is easier since the tree-components we have to consider have only $O(\log n)$ vertices. Thus one arrives at the following result of Erdös and Rényi [6, p. 49].</p>
<p>If $0&lt;c&lt;1$ is a constant, $p=c / n, \alpha=c-1-\log c$,</p>
<p>$$
k_{0}=\frac{1}{\alpha}\left{\log n-\frac{5}{2} \log \log n-l_{0}\right} \in \mathbf{N} \quad \text { and } \quad l_{0}=O(1)
$$</p>
<p>then the number of components of $G_{p}$ with at least $k_{0}$ vertices has asymptotically Poisson distribution with mean</p>
<p>$$
\lambda \sim \frac{1}{c \sqrt{2 \pi}} \frac{\alpha^{5 / 2}}{1-e^{-\alpha}} e^{l_{0}}
$$</p>
<p>Though the simple method of means is sufficient to prove this assertion, we would like to point out that recently Barbour [1] applied a more sophisticated and powerful method to prove that the number of certain components has asymptotically Poisson distribution.</p>
<p>Let us turn to the case $p=(1+\varepsilon) / n&gt;1 / n$. The proof of Theorem 6 is easily adapted to show that if $\varepsilon=o(1)$ then the distribution of the number of components of order less than $n^{2 / 3}$ in $G_{p}$ is almost the same as the distribution in $G_{p^{\prime}}$ with $p^{\prime}=(1-\varepsilon) / n$. Furthermore, it is easily seen that $\varepsilon$ can be rather small for a slightly weaker version of Theorem 6 to remain valid. As in Theorem 8, we state the result for $G_{t}$ rather than $G_{p}$.</p>
<p>THEOREM 11. Let $t=n / 2+s, s=o(n), s n^{-2 / 3}(\log n)^{2} \rightarrow \infty, \varepsilon=2 s / n$ and for $\lambda&gt;0$ choose $l_{\lambda}=l_{\lambda}(n)$ in such a way that</p>
<p>$$
(2 / \pi)^{1 / 2} n l_{\lambda}^{-5 / 2}\left((1-\varepsilon) e^{\varepsilon}\right)^{l_{\lambda}} \varepsilon^{-2} \rightarrow \lambda
$$</p>
<p>Then for $\omega(n) \rightarrow \infty$ and $m \in \mathbf{N}$ a.e. $G_{t}$ is such that</p>
<p>$$
\begin{gathered}
L_{1}\left(G_{t}\right)=(4+o(1)) s \
l_{1}-\omega(n) / \varepsilon \leq L_{m}\left(G_{t}\right) \leq \cdots \leq L_{2}\left(G_{t}\right) \leq l_{1}+\omega(n) / \varepsilon \
L_{i}(G)=(1+o(1)) l_{1}, \quad i=2, \ldots, m
\end{gathered}
$$</p>
<p>If $\varepsilon n^{\eta} \rightarrow \infty$ for every $\eta&gt;0$ then $Z^{<em>} \xrightarrow{d} P_{\lambda}$, where $Z^{</em>}=\max \left{m-1: L_{m} \geq l_{\lambda}\right}$.</p>
<ol>
<li>The small components after time $n / 2$. As $t$ increases, our task of locating the orders of the components of $G_{t}$ becomes easier. Indeed, if $c&gt;1$ is a constant and $t-c n / 2=o(n)$ then the expected number of vertices on components containing cycles and having fewer than $n^{2 / 3}$ vertices is bounded and so is the expected number of vertices on components whose order is between $c_{1} \log n$ and $n^{2 / 3}$. Hence if $\omega(n) \rightarrow \infty$ then a.e. $G_{t}$ is such that with the exception of $\omega(n)$ vertices all vertices belong to the giant component or else to trees of order at most $c_{1} \log n$. Now the expectation of the number of vertices on these trees is about $c_{2} n$ and by a slight variant of Theorem 2 the variance is $O\left(n(\log n)^{2}\right)$. Therefore we are led to the following result.</li>
</ol>
<p>THEOREM 12. Let $c&gt;1$ be a constant and let $t=\lfloor c n / 2\rfloor, \omega(n) \rightarrow \infty$. Then a.e. $G_{t}$ is such that, with the exception of at most $\omega(n)$ vertices, all vertices of $G_{t}$ belong to the giant component or to components which are trees. Furthermore,</p>
<p>$$
\left|L_{1}\left(G_{t}\right)-n\left{1-\frac{1}{c} \sum_{k=1}^{\infty} \frac{k^{k-1}}{k!}\left(c e^{-c}\right)^{k}\right}\right| \leq \omega(n) n^{1 / 2} \log n
$$</p>
<p>and if $k_{0}=\frac{1}{\alpha}\left{\log n-\frac{5}{2} \log \log n-l_{0}\right} \in \mathbb{N}$, where $\alpha=c-1-\log c$ and $l_{0}=O(1)$, then $Z^{*}=\max \left{m-1: L_{m}\left(G_{t}\right) \geq k_{0}\right}$ has asymptotically Poisson distribution with mean</p>
<p>$$
\lambda \sim \frac{1}{c \sqrt{2 \pi}} \frac{\alpha^{5 / 2}}{1-e^{-\alpha}} e^{l_{0}}
$$</p>
<p>In the range of $t$ covered by Theorem 11 the giant component increases about four times as fast as $t$. Another way of proving Theorem 12 would be to establish this fact first and then use it to deduce the assertion about the size of the giant component. What is the expectation of the increase of $L_{1}\left(G_{t}\right)$ as $t$ changes to $t+1$ ? The probability that the $(t+1)$ st edge will join the giant component to a component of order $L_{j}$ is about $L_{1} L_{j} /\binom{n}{2}$. Hence the expectation is about</p>
<p>$$
\begin{aligned}
&amp; \left(2 L_{1} / n^{2}\right) \sum_{k=1}^{n^{2 / 3}} k^{2}\binom{n}{k} k^{k-2}\left(\frac{1+\varepsilon}{n}\right)^{k-1}\left(1-\frac{1+\varepsilon}{n}\right)^{k n-k^{2} / 2} \
&amp; \sim\left(2 L_{1} / n^{2}\right) \sum_{k=1}^{n^{2 / 3}} \frac{n}{\sqrt{2 \pi}} k^{1 / 2} \exp \left{-\varepsilon^{2} k / 2\right} \
&amp; \sim\left(\frac{2}{\pi}\right)^{1 / 2}\left(L_{1} / n\right) \int_{0}^{\infty} x^{1 / 2} e^{-x \varepsilon^{2} / 2} d x \
&amp; =\left(\frac{2}{\pi}\right)^{1 / 2}\left(L_{1} / n\right) \Gamma\left(\frac{1}{2}\right)\left(\varepsilon^{2} / 2\right)^{-1 / 2}=2 L_{1} /(\varepsilon n)=L_{1} / s .
\end{aligned}
$$</p>
<p>From this one can deduce that if $t$ is $o(n)$ but not too small then $L_{1}(t)=$ $L_{1}(n / 2+s) \sim c_{1} s$. By considering the crude order of $L_{1}\left(G_{t}\right)$ as $t$ ceases to be $o(n)$, one can show easily that the constant $c_{1}$ is 4 .</p>
<p>Using Theorem 2, for $t \geq(1+\varepsilon) n / 2$ it is easy to obtain fairly precise information about the distribution of the orders of the components of $G_{t}$. We shall do a little more than that: we shall prove some results about all graphs $G_{t}$ of a graph process after time $(1+\varepsilon) n / 2$. Let us start with a rather crude result.</p>
<p>THEOREM 13. Let $\varepsilon&gt;0$ be fixed and for $c_{0}=c_{0}(n) \geq 1+\varepsilon$ set $c_{1}=$ $3 /\left(c_{0}-1-\log c_{0}\right)$. Then a.e. graph process $\tilde{G}=\left(G_{t}\right)<em 0="0">{0}^{N}$ is such that for $t \geq c</em>$.} n / 2$ the graph $G_{t}$ does not contain a component whose order is between $c_{1} \log n$ and $n^{2 / 3</p>
<p>Proof. Let $c_{0} \leq c \leq 3 \log n$ and set $p=c / n$. By inequality (3) the expected number of components of $G_{p}$ whose order $k$ satisfies $k_{1}=\left\lfloor c_{1} \log n\right\rfloor \leq k \leq k_{2}=$ $\left\lfloor n^{2 / 3}\right\rfloor$ is at most</p>
<p>$$
\begin{aligned}
\sum_{k=k_{1}}^{k_{2}} &amp; \binom{n}{k} \sum_{d \geq-1}\left(\frac{2}{3}\right)^{d+1} k^{k+(3 d-1) / 2}\left(\frac{c}{n}\right)^{k+d}\left(1-\frac{c}{n}\right)^{k(n-k)} \
&amp; =O(1) \sum_{k=k_{1}}^{k_{2}}\binom{n}{k} k^{k-2}\left(\frac{c}{n}\right)^{k-1}\left(1-\frac{c}{n}\right)^{k(n-k)} \
&amp; =o(n) c e^{1-c^{k_{1}}}=o\left(n^{-2}\right)
\end{aligned}
$$</p>
<p>Since the property of containing a component of order $k$ is convex and a.e. graph process becomes connected by time $n \log n$, this implies our assertion.</p>
<p>THEOREM 14. (i) Let $c_{0}&gt;1$ be fixed. Then a.e. $\tilde{G}$ is such that for $t \geq c_{0} n / 2$ the graph $G_{t}$ does not contain a $(k, d)$-component with $d \geq 1$ and $k \leq n^{2 / 3}$.
(ii) For $\omega(n) \rightarrow \infty$ a.e. $\tilde{G}$ is such that for $t \geq \omega(n) n$ every component of $G_{t}$, with the exception of its giant component is a tree.</p>
<p>Proof. Since for $t \geq n \log n$ a.e. $G_{t}$ is connected, it suffices to restrict our attention to the range $c_{0} n / 2 \leq t \leq n \log n$. Furthermore, by Theorem 11 it suffices to consider components of order at most $k_{1}=\left\lfloor c_{1} \log n\right\rfloor$, where</p>
<p>$$
c_{1}=3 /\left(c_{0}-1-\log c_{0}\right)
$$</p>
<p>Note that the expected number of $(k, d)$-components with $4 \leq k \leq k_{1}$ and $d \geq 1$ a graph process contans between times $t_{0}=\left\lfloor c_{0} n / 2\right\rfloor$ and $t_{1}=\lfloor 2 n \log n\rfloor$ is at most</p>
<p>$$
2 \sum_{t=t_{0}}^{t_{1}} \sum_{k=4}^{k_{1}}\binom{n}{k} \sum_{d \geq 1}\left(\frac{2}{3}\right)^{d+1} k^{k+(3 d-1) / 2}\left(\frac{2 t}{n^{2}}\right)^{k+d}\left(1-\frac{2 t}{n^{2}}\right)^{k(n-k)}=O(1)
$$</p>
<p>On the other hand, it is easily seen (cf. Theorem 9c of $[\mathbf{6}]$ ) that for $t_{0} \leq$ $t \leq t_{1}=\lfloor n \log n\rfloor$ and $1 \leq k \leq k_{1}$ the life-time of a component of order $k$ in $G_{t}$ has approximately exponential distribution with mean $n /(2 k)$. In particular, a component of order $k$ in $G_{t}$ will be a component of $G_{t+1}, G_{t+2}, \ldots, G_{t+l}$ with probability at least $\frac{1}{2}$, where $l=\left\lfloor n /\left(3 k_{1}\right)\right\rfloor$. Consequently the probability that $\tilde{G}$ is such that there is a time $t$ with $t_{0} \leq t \leq t_{1}$ for which $G_{t}$ has a $(k, d)$-component with $4 \leq k \leq k_{1}$ and $d \geq 1$ is at most</p>
<p>$$
O(1)(2 / l)=O((\log n) / n)
$$</p>
<p>This proves (i). Assertion (ii) is proved analogously.
Let $\tilde{k}=O(\log n), \Lambda={(k, d): 1 \leq k \leq \tilde{k}, d=-1$ or 0$}$, and define the $X_{i}$ as in Theorem 2. Then for every fixed $i$ there is a constant $d_{i}$ such that for</p>
<p>$n / 2 \leq t \leq n \log n$ in $G_{t}$ we have</p>
<p>$$
\mu_{i}=E_{t}\left(X_{i}\right) \leq 2 \sum_{k=1}^{\bar{k}}\binom{n}{k} k^{k-2+i}\left(\frac{2 t}{n^{2}}\right)^{k-1}\left(1-\frac{2 t}{n^{2}}\right)^{k(n-k)} \leq d_{i} n e^{-2 t / n}
$$</p>
<p>and, using a slight variant of Theorem 2,</p>
<p>$$
\sigma^{2}\left(X_{i}\right) \leq \mu_{2 i}+\frac{4 t}{m} \mu_{i+1}^{2} / n \leq d_{i} \mu_{2 i}
$$</p>
<p>Consequently by Chebyshev's inequality</p>
<p>$$
P_{t}\left(\left|X_{i}\left(G_{t}\right)-\mu_{i}\right| \geq u\right) \leq \frac{d_{i} \mu_{2 i}}{u^{2}}
$$</p>
<p>This relation enables us to deduce uniform bounds for the $X_{i}$. Here we state only a rather simple result about $w_{t}=w\left(G_{t}\right)$, the number of components of $G_{t}$.</p>
<p>THEOREM 15. Suppose $0 \leq \varepsilon, \omega(n) \rightarrow \infty$ and $c(n)=\log n-\omega(n) \rightarrow \infty$. Then a.e. $\hat{G}$ is such that for every $t$ satisfying $n \leq 2 t \leq c(n) n$ we have</p>
<p>$$
\left|w_{t}-n \beta(2 t / n)\right|&lt;\varepsilon n \beta(2 t / n)
$$</p>
<p>where</p>
<p>$$
\beta(c)=\frac{1}{c} \sum_{k=1}^{\infty} \frac{k^{k-2}}{k!}\left(c e^{1-c}\right)^{k}
$$</p>
<p>Proof. Let $\eta&gt;0$ and set $t_{j}=\lfloor(1+j \eta) n / 2\rfloor, j=0,1, \ldots, m=\lfloor c(n) / \eta\rfloor$. Put $\bar{k}=\lceil(3 \log n) /(\eta-\log (1+\eta))\rceil$ and let $\Lambda$ and $X_{i}$ be as above. Then a.e. $\hat{G}$ is such that for $t_{1} \leq t \leq t_{m}$ we have $w_{t}=w\left(G_{t}\right)=X_{0}\left(G_{t}\right)+1$. This implies that it suffices to estimate $X_{0}\left(G_{t}\right)$ instead of $w\left(G_{t}\right)$.</p>
<p>It is easily seen that if $\eta&gt;0$ is sufficiently small then for $n / 2 \leq t^{\prime} \leq t \leq c(n) n / 2$</p>
<p>$$
\left|E_{t^{\prime}}\left(X_{0}\right)-n \beta(2 t / n)\right|&lt;\frac{\varepsilon}{5} n \beta_{2 t / n}
$$</p>
<p>and</p>
<p>$$
\left|\beta(2 t / n)-\beta\left(2 t^{\prime} / n\right)\right|&lt;\frac{\varepsilon}{t} \beta(2 t / n)
$$</p>
<p>Note that $\beta(c) \geq e^{1-c}$. Hence by (14) and (16)</p>
<p>$$
P_{t_{j}}\left(\left|X_{0}\left(G_{t_{j}}\right)-E_{t_{j}}\left(X_{0}\right)\right| \geq \frac{\varepsilon}{5} E_{t_{j}}\left(X_{0}\right)\right) \leq \frac{25 d_{0}}{\varepsilon^{2} E_{t_{j}}\left(X_{0}\right)} \leq \frac{50 d_{0}}{\varepsilon^{2}} n^{-1} e^{j \eta}
$$</p>
<p>Since $\sum_{j=1}^{m} e^{j \eta}=o(1)$, a.e. $\hat{G}$ is such that</p>
<p>$$
\left|X_{0}\left(G_{t_{j}}\right)-E_{t_{j}}\left(X_{0}\right)\right|&lt;\frac{\varepsilon}{5} E_{t_{j}}\left(X_{0}\right)
$$</p>
<p>for every $j=1, \ldots, m$. A.e. $\hat{G}$ is such that $w_{t+1}(\hat{G}) \leq w_{t}(\hat{G})$ if $t \geq t_{1}$ so a.e. $G$ is such that $X_{0}=X_{0}\left(G_{t}\right)$ is a monotone decreasing function of $t$ for $t \geq t_{1}$. Therefore relations (16), (17) and (18) imply that a.e. $\hat{G}$ satisfies (15) if $t_{1} \leq t \leq c(n) n / 2$.</p>
<p>Finally, if $\eta$ is sufficiently small then a.e. $\hat{G}$ is such that $\left|w_{t}-n / 2\right|&lt;\varepsilon n / 5$ and $\left|\beta(2 t / n)-\frac{1}{2}\right|&lt;\varepsilon / 5$ whenever $n / 2 \leq t \leq t_{1}$, so (15) holds in this range as well.</p>
<p>In Theorem 15 the approximation of $w_{t}$ becomes more precise as $t$ grows. From inequality (14) one can also obtain approximations to the same degree for every value of $t$. For example, it is easy to show that if $\omega(n) \rightarrow \infty$ then $\hat{G}$ is such that</p>
<p>$$
\left|w_{t}(\hat{G})-\frac{n^{2}}{2 t} \sum_{k=1}^{\infty} \frac{k^{k-2}}{k!}\left(\frac{2 t}{n} e^{-2 t / n}\right)^{k}\right|&lt;\omega(n) n^{2 / 3}
$$</p>
<p>for every $t \geq n / 2$.</p>
<h1>REFERENCES</h1>
<ol>
<li>A. D. Barbour, Poisson convergence and random graphs, Math. Proc. Cambridge Philos. Soc. 92 (1982), 349-359.</li>
<li>B. Bollobás, Graph theory-An introductory course, Graduate Texts in Math., Springer-Verlag, New York, Heidelberg and Berlin, 1979.</li>
<li>, Random graphs, Combinatorics (H. N. V. Temperley, ed.), London Math. Soc. Lecture Notes Series, vol. 52, Cambridge Univ. Press, New York, 1981, pp. 80-102.</li>
<li>, The evolution of sparse graphs, Graph Theory and Combinatorics (B. Bollobás, ed.), Academic Press, London, 1984.</li>
<li>P. Erdös and A. Rényi, On random graphs. I, Publ. Math. Debrecen 6 (1959), 290-297.</li>
<li>, On the evolution of random graphs, Publ. Math. Inst. Hungar. Acad. Sci. 5 (1960), 17-61.</li>
<li>, On the evolution of random graphs, Bull. Inst. Internat. Statist. Tokyo 38 (1961), 343-347.</li>
<li>C. W. Ford and G. E. Uhlenbeck, Combinatorial problems in the theory of graphs, Proc. Nat. Acad. Sci. U.S.A. 43 (1957), 163-167.</li>
<li>L. Katz, Probability of indecomposability of a random mapping function, Ann. Math. Statist. 26 (1955), 512-517.</li>
<li>A. Rényi, On connected graphs. I, Publ. Math. Inst. Hungar. Acad. Sci. 4 (1959), 385-387.</li>
<li>E. M. Wright, Asymptotic enumeration of connected graphs, Proc. Roy. Soc. Edinburgh Sect. A 68 (1968/69), 298-308.</li>
<li>, The number of connected sparsely-edged graphs. II: Smooth graphs and blocks, J. Graph Theory 2 (1978), 299-305.</li>
<li>, The number of connected sparsely-edged graphs. III: Asymptotic results, J. Graph Theory 4 (1980), 393-407.</li>
</ol>
<p>Department of Mathematics, Louisiana State University, Baton Rouge, LOUISIANA 70803</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Received by the editors December 3, 1982 and, in revised form, August 15, 1983.
1980 Mathematics Subject Classification. Primary 05C99, 05C30; Secondary 60J99, 62P99.
${ }^{1}$ Research supported by NSF Grant MCS-8104854.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6546 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6546</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6546</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-261049794</p>
                <p><strong>Paper Title:</strong> <a href="http://export.arxiv.org/pdf/2308.10379" target="_blank">Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Current literature, aiming to surpass the"Chain-of-Thought"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6546.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6546.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AoT-mini-crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Algorithm of Thoughts (AoT) applied to 5x5 mini crosswords (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper implements the Algorithm of Thoughts (AoT) with GPT-4 to solve 5x5 mini crossword puzzles by generating candidate word lists for each row/column (warm‑start) and then performing a single-generation, in‑context DFS‑style search with backtracking/compatibility checks inside the model's output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned large autoregressive language model (decoder transformer); the paper states all experiments were conducted with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>5×5 mini crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / word puzzle (spatially arranged grid with letter intersection constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>goobix.com 5×5 mini crosswords (prompts drawn from games 136,141,146,151,156; testing on 20 games e.g., games 1,6,...,91,96)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few‑shot AoT (5‑shot) with a two‑step process: (1) warm‑start preselection — ask model to propose up to 5 candidate words per row/column; (2) single‑generation Algorithm of Thoughts recursive search starting from a preselected word; temperature=0 for sequential deterministic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>In‑context tree‑search style reasoning implemented inside a single generation: DFS‑style exploration with candidate generation, compatibility checking at intersections, local backtracking within the generated trajectory (no external repeated queries).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Grid encoded via human‑readable row/column slots and letter intersection constraints; candidate word lists per row/column provided in context; letters and indices referenced explicitly in prompt (positioned letter constraints used to assess compatibility).</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Word success rate (percentage of words correctly completed out of total) and average number of LLM queries per puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>52% word success rate; average queries = 2</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>AoT substantially improves over single‑query baselines (standard prompting and CoT) but trails ToT; common error modes include: no preselections (15.8%), expression missteps (5.3%), incorrect pattern extraction (26.3%), and erroneous word placement (52.6%); model difficulties include aligning letters to precise indices, handling outdated/esoteric terms or typos in clues, and incomplete activation of backtracking when constrained to single generation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared to baselines on same task: Standard prompting 14% (1 query), CoT 15.6% (1 query); ToT (reported from prior work) 60% (>200 queries). AoT achieves 52% with ~2 queries, showing a large improvement over standard/CoT with minimal queries but lower absolute success than ToT which uses many external queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>AoT's in‑generation exploration increases token usage relative to standard prompting/CoT; backtracking capability in prompts isn't fully activated without much longer generations; performance depends on high‑capability LLM (experiments only with GPT‑4); struggles with precise letter alignment and domain knowledge gaps (obscure words); overall still trails ToT when external memory/backtracking and many queries are allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6546.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6546.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT-mini-crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT) baseline applied to 5×5 mini crosswords (reported from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tree of Thoughts (ToT) is an external tree‑search style method that uses LLMs both to propose candidate continuations and to evaluate/prune tree nodes; the paper cites ToT results as a higher‑performing but extremely query‑heavy baseline on 5×5 mini crosswords.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>5×5 mini crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / word puzzle (spatial grid with intersection constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>goobix.com 5×5 mini crosswords (as used in prior ToT study and cited here)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tree‑of‑Thoughts prompting (external multi‑query approach): generate candidate thoughts with one LLM, evaluate/prune with another LLM, drive an external tree search (breadth/best‑first variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>External tree search (BFS/DFS variants) with explicit node evaluation/pruning using LLMs; heavy backtracking and external memory to maintain and traverse large search trees.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>External tree nodes, stored states representing partially filled grid and candidate words; explicit external memory for backtracking (details from cited ToT work rather than this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External tree‑search mechanism that issues many LLM queries (generator + evaluator roles) and holds tree nodes in external memory; used to explore and prune large search spaces via repeated LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Word success rate and average number of LLM queries per puzzle (as reported in the cited ToT study and reproduced in comparisons here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>60% word success rate; average queries >200 (reported from the ToT study; cited directly by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Higher absolute success attributed to full backtracking and external memory, allowing exhaustive/heuristic search across many nodes; however, comes with extremely high LLM query counts, latency, cost, and environmental/computational footprint.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared in-paper as a baseline: ToT 60% (>200 queries) vs AoT 52% (2 queries) and vs standard/CoT ~14–15.6% (1 query). The tradeoff highlighted is higher success for ToT at the cost of orders‑of‑magnitude more queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ToT's main limitation is its heavy reliance on many LLM queries and external tooling (latency, monetary cost, system load), making it less practical for low‑query or real‑time settings; paper cites ToT results rather than re‑implementing them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6546.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6546.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline‑prompting‑mini-crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard prompting and Chain‑of‑Thought (CoT) baselines on 5×5 mini crosswords (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates standard few‑shot prompting (input/output examples) and Chain‑of‑Thought prompting (asking for words per clue) with GPT‑4 as low‑query baselines on the 5×5 mini crossword benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned large autoregressive language model (decoder transformer); used as the model for baseline prompting experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>5×5 mini crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / word puzzle (spatial grid with intersection constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>goobix.com 5×5 mini crosswords (same test set as AoT comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Standard few‑shot prompting (examples of crossword + solutions) and Chain‑of‑Thought prompting (ask model to retrieve words for each clue before filling grid).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Linear decomposition per clue (list candidate words per clue) without explicit global backtracking or in‑generation tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>In‑context examples that include complete crossword grids and solutions; clues and slot patterns represented textually in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Word success rate and average number of LLM queries per puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard prompting: 14% word success rate, avg queries = 1; CoT: 15.6% word success rate, avg queries = 1</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Both baselines perform poorly on the puzzle compared to search‑based approaches; CoT yields only modest improvement over standard prompting, indicating that linear chain reasoning without global search/backtracking is insufficient for the spatial constraint coupling of crosswords.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>CoT vs standard: small improvement (15.6% vs 14%); both far below AoT (52%) and ToT (60%). This demonstrates the benefit of search/backtracking (even when implemented in a single generation) over strictly linear CoT for spatial constraint puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Unable to maintain global consistency across intersecting words and lacks efficient backtracking; fails on aligning letters precisely at intersections and handling complex dependencies between slots; single‑query linear reasoning insufficient for high success rates on spatial crossword puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6546",
    "paper_id": "paper-261049794",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "AoT-mini-crosswords",
            "name_full": "Algorithm of Thoughts (AoT) applied to 5x5 mini crosswords (this paper)",
            "brief_description": "This paper implements the Algorithm of Thoughts (AoT) with GPT-4 to solve 5x5 mini crossword puzzles by generating candidate word lists for each row/column (warm‑start) and then performing a single-generation, in‑context DFS‑style search with backtracking/compatibility checks inside the model's output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction‑tuned large autoregressive language model (decoder transformer); the paper states all experiments were conducted with GPT-4.",
            "model_size": null,
            "puzzle_name": "5×5 mini crosswords",
            "puzzle_type": "constraint satisfaction / word puzzle (spatially arranged grid with letter intersection constraints)",
            "dataset_name": "goobix.com 5×5 mini crosswords (prompts drawn from games 136,141,146,151,156; testing on 20 games e.g., games 1,6,...,91,96)",
            "prompting_method": "Few‑shot AoT (5‑shot) with a two‑step process: (1) warm‑start preselection — ask model to propose up to 5 candidate words per row/column; (2) single‑generation Algorithm of Thoughts recursive search starting from a preselected word; temperature=0 for sequential deterministic generation.",
            "reasoning_technique": "In‑context tree‑search style reasoning implemented inside a single generation: DFS‑style exploration with candidate generation, compatibility checking at intersections, local backtracking within the generated trajectory (no external repeated queries).",
            "internal_representation": "Grid encoded via human‑readable row/column slots and letter intersection constraints; candidate word lists per row/column provided in context; letters and indices referenced explicitly in prompt (positioned letter constraints used to assess compatibility).",
            "use_of_external_tool": false,
            "external_tool_description": "",
            "evaluation_metric": "Word success rate (percentage of words correctly completed out of total) and average number of LLM queries per puzzle",
            "performance": "52% word success rate; average queries = 2",
            "analysis_findings": "AoT substantially improves over single‑query baselines (standard prompting and CoT) but trails ToT; common error modes include: no preselections (15.8%), expression missteps (5.3%), incorrect pattern extraction (26.3%), and erroneous word placement (52.6%); model difficulties include aligning letters to precise indices, handling outdated/esoteric terms or typos in clues, and incomplete activation of backtracking when constrained to single generation.",
            "ablation_comparison": "Compared to baselines on same task: Standard prompting 14% (1 query), CoT 15.6% (1 query); ToT (reported from prior work) 60% (&gt;200 queries). AoT achieves 52% with ~2 queries, showing a large improvement over standard/CoT with minimal queries but lower absolute success than ToT which uses many external queries.",
            "limitations": "AoT's in‑generation exploration increases token usage relative to standard prompting/CoT; backtracking capability in prompts isn't fully activated without much longer generations; performance depends on high‑capability LLM (experiments only with GPT‑4); struggles with precise letter alignment and domain knowledge gaps (obscure words); overall still trails ToT when external memory/backtracking and many queries are allowed.",
            "uuid": "e6546.0",
            "source_info": {
                "paper_title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ToT-mini-crosswords",
            "name_full": "Tree of Thoughts (ToT) baseline applied to 5×5 mini crosswords (reported from prior work)",
            "brief_description": "Tree of Thoughts (ToT) is an external tree‑search style method that uses LLMs both to propose candidate continuations and to evaluate/prune tree nodes; the paper cites ToT results as a higher‑performing but extremely query‑heavy baseline on 5×5 mini crosswords.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "5×5 mini crosswords",
            "puzzle_type": "constraint satisfaction / word puzzle (spatial grid with intersection constraints)",
            "dataset_name": "goobix.com 5×5 mini crosswords (as used in prior ToT study and cited here)",
            "prompting_method": "Tree‑of‑Thoughts prompting (external multi‑query approach): generate candidate thoughts with one LLM, evaluate/prune with another LLM, drive an external tree search (breadth/best‑first variants).",
            "reasoning_technique": "External tree search (BFS/DFS variants) with explicit node evaluation/pruning using LLMs; heavy backtracking and external memory to maintain and traverse large search trees.",
            "internal_representation": "External tree nodes, stored states representing partially filled grid and candidate words; explicit external memory for backtracking (details from cited ToT work rather than this paper).",
            "use_of_external_tool": true,
            "external_tool_description": "External tree‑search mechanism that issues many LLM queries (generator + evaluator roles) and holds tree nodes in external memory; used to explore and prune large search spaces via repeated LLM calls.",
            "evaluation_metric": "Word success rate and average number of LLM queries per puzzle (as reported in the cited ToT study and reproduced in comparisons here).",
            "performance": "60% word success rate; average queries &gt;200 (reported from the ToT study; cited directly by this paper).",
            "analysis_findings": "Higher absolute success attributed to full backtracking and external memory, allowing exhaustive/heuristic search across many nodes; however, comes with extremely high LLM query counts, latency, cost, and environmental/computational footprint.",
            "ablation_comparison": "Compared in-paper as a baseline: ToT 60% (&gt;200 queries) vs AoT 52% (2 queries) and vs standard/CoT ~14–15.6% (1 query). The tradeoff highlighted is higher success for ToT at the cost of orders‑of‑magnitude more queries.",
            "limitations": "ToT's main limitation is its heavy reliance on many LLM queries and external tooling (latency, monetary cost, system load), making it less practical for low‑query or real‑time settings; paper cites ToT results rather than re‑implementing them.",
            "uuid": "e6546.1",
            "source_info": {
                "paper_title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Baseline‑prompting‑mini-crosswords",
            "name_full": "Standard prompting and Chain‑of‑Thought (CoT) baselines on 5×5 mini crosswords (this paper)",
            "brief_description": "The paper evaluates standard few‑shot prompting (input/output examples) and Chain‑of‑Thought prompting (asking for words per clue) with GPT‑4 as low‑query baselines on the 5×5 mini crossword benchmark.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction‑tuned large autoregressive language model (decoder transformer); used as the model for baseline prompting experiments in the paper.",
            "model_size": null,
            "puzzle_name": "5×5 mini crosswords",
            "puzzle_type": "constraint satisfaction / word puzzle (spatial grid with intersection constraints)",
            "dataset_name": "goobix.com 5×5 mini crosswords (same test set as AoT comparisons)",
            "prompting_method": "Standard few‑shot prompting (examples of crossword + solutions) and Chain‑of‑Thought prompting (ask model to retrieve words for each clue before filling grid).",
            "reasoning_technique": "Linear decomposition per clue (list candidate words per clue) without explicit global backtracking or in‑generation tree search.",
            "internal_representation": "In‑context examples that include complete crossword grids and solutions; clues and slot patterns represented textually in the prompt.",
            "use_of_external_tool": false,
            "external_tool_description": "",
            "evaluation_metric": "Word success rate and average number of LLM queries per puzzle",
            "performance": "Standard prompting: 14% word success rate, avg queries = 1; CoT: 15.6% word success rate, avg queries = 1",
            "analysis_findings": "Both baselines perform poorly on the puzzle compared to search‑based approaches; CoT yields only modest improvement over standard prompting, indicating that linear chain reasoning without global search/backtracking is insufficient for the spatial constraint coupling of crosswords.",
            "ablation_comparison": "CoT vs standard: small improvement (15.6% vs 14%); both far below AoT (52%) and ToT (60%). This demonstrates the benefit of search/backtracking (even when implemented in a single generation) over strictly linear CoT for spatial constraint puzzles.",
            "limitations": "Unable to maintain global consistency across intersecting words and lacks efficient backtracking; fails on aligning letters precisely at intersections and handling complex dependencies between slots; single‑query linear reasoning insufficient for high success rates on spatial crossword puzzles.",
            "uuid": "e6546.2",
            "source_info": {
                "paper_title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.015555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models</p>
<p>Bilgehan Sel 
Ahmad Al-Tawaha 
Vanshaj Khattar 
Lu Wang 
Ruoxi Jia 
Ming Jin 
Virginia Tech 
Microsoft 
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models
17EA1D312F7068DC66C09FAD0EEB8EB9
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities.This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads.Addressing this, we propose the Algorithm of Thoughts-a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning.By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries.Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm.Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches.We probe into the underpinnings of our method's efficacy and its nuances in application.</p>
<p>Introduction</p>
<p>Recent developments in large language models (Chowdhery et al. 2022;Thoppilan et al. 2022;Liu et al. 2023, inter alia) have spotlighted their efficacy in general problem solving (Huang and Chang 2022;Suzgun et al. 2022), code generation (Chen et al. 2021;Austin et al. 2021), and instruction following (Ouyang et al. 2022;Bai et al. 2022).While early models relied on direct answer strategies (Brown et al. 2020), contemporary research veers towards linear reasoning paths (Wei et al. 2022b;Kojima et al. 2022;Zhang et al. 2022) by breaking problems into sub-tasks for solution discovery, or harnesses external mechanisms to alter token generation by changing the context (Zhou et al. 2022;Drozdov et al. 2022;Yao et al. 2023).</p>
<p>Analogous to human cognition (Sloman 1996; Kahneman 2011), early LLM strategies seemed to emulate the instantaneous System 1, characterized by its impulsive decisionmaking.In contrast, more recent methodologies like chainof-thought (CoT) (Wei et al. 2022b) and least-to-most prompting (L2M) (Zhou et al. 2022;Drozdov et al. 2022) Preprint.Under review.</p>
<p>reflect the introspective nature of System 2. Notably, integrating intermediary reasoning steps has yielded improvements in arithmetic reasoning tasks (Srivastava et al. 2022;Liang et al. 2022).</p>
<p>However, as tasks shift towards deeper planning and extensive thought exploration, these methods appear restrictive.Although CoT integrated with Self-Consistency (CoT-SC) (Wang et al. 2022) enlists multiple LLM outputs for a consensus, the lack of meticulous evaluation can result in model misdirection.The "Tree of Thoughts" (Yao et al. 2023;Long 2023) emerges as a notable solution.While one LLM is dedicated to idea generation, another steps in to assess the merit of these ideas, following a halting-assessmentresuming cycle.This iterative process, anchored by tree search, has shown marked effectiveness, especially in tasks with a breadth of continuations.We see this progression as akin to humans employing tools to circumvent working memory limitations, serving as an external augmentation for LLMs (Mialon et al. 2023).</p>
<p>On the flip side, this enhanced LLM approach is not without pitfalls.A prominent downside is the substantial surge in the number of queries and computational demands.Each query to online LLM APIs such as GPT-4-a focal point of our study-incurs a monetary expense (Chen, Zaharia, and Zou 2023) but also contributes to latency, a significant limitation especially critical in real-time applications.Cumulative delays from these queries can compromise solution efficiency.Infrastructure-wise, continuous interactions can stress systems, leading to potential bandwidth constraints and reduced model availability (Aminabadi et al. 2022).Moreover, the environmental implications cannot be ignored; incessant querying escalates the energy consumption of already power-hungry data centers, exacerbating the carbon footprint (Wu et al. 2022;Dhar 2020).</p>
<p>With this in mind, our goal is to dramatically reduce the query counts employed by contemporary multi-query reasoning methods while maintaining performance for tasks necessitating adept use of world knowledge, thereby steering a more responsible and proficient use of AI resources.</p>
<p>Reflecting on the evolution of LLMs from System 1 to System 2, an essential ingredient comes to light: algorithms.Characterized by its methodical nature, the algorithmic perspective offers a path to keenly explore problem spaces, enact strategies, and formulate solutions (Helie and Pizlo 2022; U ser: 11 10 5 1</p>
<p>Assistant: 11 -10 = 1 (left: 1, 1, 5) 1 * 1 = 1 (left: 1, 5) 5 * 1 = 5 (left: 5) Answer: ((11 -10) * 1) * 5 -1 = 24.</p>
<p>Algorithm of Thoughts</p>
<p>Model Input</p>
<p>Model Output U ser: 8 6 4 4 Assistant:</p>
<ol>
<li>4 -4: (left: 8, 6, 0) . . .3. 8 -6: (left: 4, 4, 2) -4 + 4: (left: 8, 2) 10, 6, 16, 4 -4 -4: (left: 2, 0) 2, 2, 0, undefined -4 * 4: (left: 16, 2) 18, 14, 32, 8 -4 / 4: (left: 2, 1) 3, 1, 2, 2 -4 + 2: (left: 6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Answer: (4 + (8 -6)) * 4 = 24.</li>
</ol>
<p>U ser: 11 10 5 1</p>
<p>Assistant:</p>
<ol>
<li>11 -1: (left: 10, 10, 5) . . . 5. 11 + 1: (left: 12, 10, 5) -12 + 10: (left: 22, 5) 27, 17, 110, 4.4 -12 -10: (left: 5, 2) 7, 3, 10, 2.5 -12 * 10: (left: 120, 5) 24 = 120 / 5 -&gt; found it!Answer: ((11 + 1) * 10) / 5 = 24.</li>
</ol>
<p>Standard Prompting</p>
<p>Model Input Model Output U ser: 8 6 4 4 Assistant: Answer: (4 + (8 -6)) * 4 = 24.</p>
<p>U ser: 11 10 5 1 Assistant: Answer: (11 -1) * (10 -5) = 24</p>
<p>Figure 1: Comparison between standard prompting, CoT, and AoT in the game of 24.While standard prompting aims for a direct answer, CoT sketches out the successive steps to the final solution.AoT's in-context example, distinct from CoT, integrates the search process, highlighted by markers '1',..., '3' as "first operations" guiding subtree exploration for the problem set '8 6 4 4'.For clarity, only a single in-context example is displayed, with a focus on the third subtree exploration.AoT produces prospective search steps (e.g., the subtree exploration '5.11 + 1') and evaluates potential subsequent steps to either progress towards a solution or retrace to another viable subtree.Banerjee et al. 2022).While much of the prevailing literature treats algorithms as external to LLMs, given LLMs' inherent generative recurrence, can we channel this iterative logic to internalize an algorithm?Drawing upon both the intricate nuances of human reasoning and the disciplined precision of algorithmic methodologies, our work aims to fuse these dual facets to augment reasoning capabilities within LLMs.Existing research underscores that humans, when navigating complex problems, instinctively draw upon past efforts, ensuring a comprehensive contemplation rather than a narrow focus (Monsell 2003;Holyoak and Morrison 2005;Baddeley 2003).LLMs, with their generative span bounded only by token limits, appear poised to break through the barriers of human working memory.Spurred by this observation, we investigated if LLMs could mirror a similar layered exploration of ideas, referencing prior intermediate steps to sieve out infeasible options, all within their iterative generation cycle.And while humans excel with their intuitive acumen, algorithms stand out with organized, systematic exploration.Current techniques, like CoT, often sidestep this synergistic potential, imposing undue pressure on LLMs for on-the-spot precision.By capitalizing on LLMs' recursive capabilities, we emulate a hybrid human-algorithmic approach.This is achieved through our use of algorithmic examples that capture the essence of exploration, from initial candidates to validated solutions.Thus emerges our concept of the Algorithm of Thoughts (AoT), as illustrated in Figs. 1 and 2.</p>
<p>More broadly, our approach signifies a new paradigm of in-context learning.Instead of the traditional "supervisedlearning" mold of [PROBLEM, SOLUTION] or [PROBLEM, SUCCESSIVE STEPS TO SOLUTION], we present a new structure that covers [PROBLEM, SEARCH PROCESS, SO-LUTION].Naturally, when instructing an LLM using an algorithm, the anticipation leans towards the LLM simply imitating the algorithm's iterative thinking.However, what emerges as intriguing is the LLM's ability to infuse its own "intuition" to achieve a search efficiency that even surpasses the algorithm itself (see Fig. 5).</p>
<p>In the subsequent sections, we first situate our work within the existing literature, followed by a discussion of our principal idea.We then present our experimental results and probe a series of hypotheses related to this emerging capability of LLM before rounding off with a conclusion.</p>
<p>Related Work</p>
<p>Standard Prompting.Also known as input-output prompting, it provides a few input-output examples of the task before getting an answer for the test sample from the language model (Brown et al. 2020).Although this method is very general and does not need any special prompting strategy, the performance is also worse compared to more advanced methods (Shao et al. 2023;Wei et al. 2022a;Lyu et al. 2023) et al. 2022b;Lyu et al. 2023).By mimicking the examples in the context, the LLM automatically divides the solution into simpler linear steps to arrive at the answer, improving performance across numerous reasoning benchmarks.Self-consistency (Wang et al. 2022) is a widely used decoding strategy aimed at generating a variety of reasoning paths by choosing the final answer through a majority vote, though this necessitates additional generations.Contrary to CoT's linear, direct progression, our approach pivots towards the explorative aspect of LLMs.We reconceptualize the c 1 , . . ., c n sequence, not merely as successive steps towards a solution, but as a dynamic, potentially mutable path that resembles an algorithmic search, allowing for exploration, recalibration, and non-linear progression.</p>
<p>Least-to-Most prompting (L2M).Taking cues from educational psychology (Libby et al. 2008), L2M prompting directs the LLM to decompose the central problem into smaller subproblems.Each subproblem is tackled in sequence, with the outcome appended before progressing to the next (Zhou et al. 2022;Drozdov et al. 2022).While this structured delineation is beneficial for broader generalization, it operates on the premise of finding a nearly perfect decomposition in a single attempt-ideal for problems with a clear-cut structure.Yet, when tasks intertwine with their decomposition complexities (like games of 24), this method's inflexibility becomes apparent.Contrastingly, AoT not only underscores the active subproblem (as shown in Fig. 1), but also permits a more contemplative approach by entertaining various options for each subproblem, while maintaining efficacy even with minimal prompts.</p>
<p>Tree of Thoughts (ToT).In the cases where each subproblem has multiple viable options to explore, linear reasoning paths from CoT or L2M substantially limit the coverage of the thought space.Considering possible options for each subproblem, the decision tree can be explored by external tree-search mechanisms (e.g., BFS, DFS) (Yao et al. 2023).Evaluation capabilities of LLMs can also be used to direct the search by pruning nodes that are hopeless to increase efficiency.However, ToT's Achilles' heel is its excessive reliance on LLM queries, at times necessitating hundreds for just one problem.We tackle this limitation by generating the whole thought process within a single context.</p>
<p>Algorithm of Thoughts</p>
<p>Our strategy pivots on recognizing a core shortcoming of current in-context learning paradigms.CoT, while enhancing the coherency of thought linkages leading to solutions, occasionally falters, presenting incorrect intermediate steps (Zelikman et al. 2022;Turpin et al. 2023;Lanham et al. 2023).Faithful CoT (Lyu et al. 2023) ought to amend this by eliciting symbolic chains of reasoning where the LLM's output resembles task-specific pseudo-code, primed for deterministic execution like Python.The intention is only to use the thought processes but not the outputs and inputs of each link since they have a tendency to be unreliable.But, the occasional missteps of CoT may not necessarily due to the LLM's inability to compute correctly.The LLM, when confronted with questions that closely match conditions of previous in-context examples, may favor echoing those outputs over generating the appropriate questions.To shed light on this phenomenon, we designed an experiment.Querying text-davinci-003 for arithmetic tasks (e.g., '11 − 2 ='), we prefixed them with multiple in-context equations converging to an identical output (e.g.'15 − 5 = 10, 8 + 2 = 10').Our results, presented in Fig. 3, reveal a steep decline in accuracy, suggesting that the mere presence of correct reasoning in the context might inadvertently compromise even basic arithmetic skills.</p>
<p>To offset this bias, diversifying the outputs of examples might seem like a viable solution, but this could subtly skew the distribution of outputs.Merely adding unsuccessful trials, much like a random search, might inadvertently encourage the model to retry rather than truly solve.Capturing the true essence of algorithmic behavior, where both failed searches and subsequent recovering and learning from such attempts play a role, we incorporate in-context examples pat- This paper focuses on a broad class of tasks reminiscent of tree-search problems.These tasks necessitate breaking down the main problem, crafting feasible solutions for each segment, and making decisions on the paths to either pursue or forsake, with the option of reevaluating more promising segmentations.Rather than posing separate queries for every subset, we leverage the iterative capabilities of the LLM to address them in one unified generation sweep.By confining ourselves to one or two LLM interactions, this approach naturally incorporates insights from antecedent context candidates and tackles intricate issues requiring an in-depth exploration of the solution domain.In alignment with our goal, we also give insights into how small or big those thoughts should be and what type of in-context examples should be given to the LLM to promote token efficiency.Subsequently, we outline key components of tree-search algorithms and their manifestation in our framework.</p>
<ol>
<li>Decomposition into Subproblems.Given a problem, constructing a search tree that delineates feasible reasoning pathways is already a demanding task, excluding the actual problem-solving aspect.Any decomposition must consider not just the interrelations between subtasks, but also the ease of addressing each individually.Consider a simple multidigit addition: while converting numbers to binary might be efficient for a computer, humans typically find base 10 arithmetic more intuitive.Furthermore, even if the subproblems remain constant, their execution might vary.Intuition can lead to shortcuts between solution steps, while its absence might necessitate more detailed steps.Crafting the right prompt (i.e., in-context algorithmic examples) hinges on these nuances, determining the minimal tokens an LLM would need for dependable performance.This is not only essential to fit within the LLM's context constraints but also vital for efficacy, as we'd expect LLMs to address problems resonant with its context using a similar token volume.</li>
</ol>
<p>Proposing Solutions to Subproblems.</p>
<p>A dominant approach in existing works involves direct sampling from LLM token output probabilities (Wang et al. 2022;Yao The first five prime numbers: The benefits are three-fold.First, with all generated solutions existing within a shared context, there's no need for individual model queries for each solution evaluation.Second, while it may seem counterintuitive initially, isolated token or token group probabilities might not always yield meaningful choices.A simple illustration is found in Fig. 4. When evaluated independently, the second-most probable token for our inaugural number is '1'-not qualifying as prime.But, when generation remains unbroken, the derived sequence is correct.This incongruence points towards the restrictive nature of the Markov property in sequence modeling.Core to our perspective is the premise that for sequential tasks like algorithmic search, LLMs are more adept at generating entire sequences than intermittently pausing and re-initiating the token sampling process.</p>
<ol>
<li>
<p>Gauging the Promise of a Subproblem.As above, existing techniques lean on additional prompting to discern the potential of tree nodes, aiding decisions regarding exploration direction.Our observations suggest that if the most promising routes are encapsulated within the incontext examples, LLMs inherently gravitate towards prioritizing those promising candidates.This diminishes the need for intricate prompt engineering and allows the incorporation of intricate heuristics, whether intuitive or knowledgedriven.Again, the absence of disjoint prompts in our approach allows for an immediate assessment of candidate viability in the same generation.</p>
</li>
<li>
<p>Backtracking to a Preferable Juncture.The decision of which node to explore next (including retracing to a prior node) inherently depends on the selected tree-search algorithm.While previous studies (Yao et al. 2023) have employed external means such as coded mechanisms for the search process, this restricts its broader appeal and entails additional customization.Our designs predominantly adopt a DFS approach supplemented by pruning.The aim is to maintain proximity between nodes sharing the same parent, thereby encouraging the LLM to prioritize local over distant features.Additionally, we present performance metrics for the AoT approach grounded in BFS.Our reliance on the model's inherent capacity to glean insights from incontext examples obviates the necessity for additional, bespoke mechanisms.</p>
</li>
</ol>
<p>Experiments</p>
<p>We show that AoT surpasses the performance of other single-prompt methods (e.g.standard, CoT/-SC prompting) while remaining competitive even when compared to methods that utilize external mechanisms, such as ToT, in benchmarks like the game of 24 and 5x5 mini crosswords.</p>
<p>Game of 24</p>
<p>The game of 24 is a mathematical card game in which players are given four numbers and must use addition, subtraction, multiplication, and division (each operation can be used more than once) to manipulate those numbers to total 24.For instance, for the numbers '8 8 5 4', one solution would be '8 * (5 − (8/4)) = 24'.At first glance, the game might appear straightforward.However, a cursory calculation suggests there are nearly 13,000 distinct expressions possible for any set of four numbers (without accounting for the commutative properties of addition and multiplication), making it a formidable challenge for present-day LLMs.</p>
<p>Task Setup.Adhering to the setup detailed in (Yao et al. 2023), we use games from indices 901-1000, sourced from the 1362 games ranked by relative difficulty at 4nums.com.For an attempt to be considered successful, it must derive a total of 24 using the exact numbers provided and only the allowed operations.</p>
<p>Baselines.Standard prompting and CoT are used in the 5shot setting, with CoT integrating 3 steps for the operations.These methods are sampled 100 times, and the averaged success rates from these samples are reported.CoT-SC is also tested with 100 votes in our setup.For ToT, we use a breadth of 5.The performance metrics from their study are directly cited to obviate the need for needless carbon emissions.</p>
<p>AoT Setup.We employ the same 5-shot setting as in standard prompting and CoT baseline setup.Our in-context samples leverage a DFS-style search algorithm, which, for clarity, is the same version used when contrasting with traditional DFS in Fig. 5.During each subtree exploration, dubbed either the 'first step' or 'first operation', we choose two numbers-illustrated by the selection of 8 and 6 in the third 'first step' (i.e., subtree labeled '3') of Fig. 1-and a corresponding operation (e.g., 8 − 6).This operation results in a new number, 2, leaving us with three numbers in total.A thorough combing of these three numbers culminates in 19 leaf nodes, all visible under the '3' subtree in Fig. 1.We aim to assess two aspects: the ability of the LLM to pinpoint promising first operations, which directly impacts the number of resolved leaf nodes, and its performance against a conventional DFS.Details on the prompts we employed are provided in the Appendix.As our method emphasizes sequential generation over trajectory sampling, we operate with a temperature setting of 0.</p>
<p>Results.From Table 1, it's evident that standard prompting combined with CoT/-SC significantly lags behind tree search methods when used with LLMs.The "Standard + Refine" result, showing a 27% success rate, is referenced from (Yao et al. 2023).This method involves iteratively asking the LLM (up to 10 iterations) to refine its answer if the initial one is incorrect.Meanwhile, ToT is limited to a maximum of 100 node visits, translating to several hundred LLM queries for each example.Remarkably, AoT achieves its results with just a single query.Despite reducing the number of requests by more than a factor of 100, AoT still outperforms ToT in this task.Error Analysis.Using a strictly LLM-centric approacheschewing any external tooling or edits-we sought to categorize mistakes observed during the game of 24.This aids in highlighting areas for refinement when solely deploying LLMs.We've classified these errors into four distinct, exhaustive categories: 1) Out-of-token error: The LLM reaches its maximum token threshold without identifying a solution.</p>
<p>Method</p>
<p>2) Expression misstep: The LLM has the correct logic or steps but fails when trying to express or formulate them into a coherent answer.3) Non-finalization error: The LLM discovers the solution but continues its search without consolidating the finding.4) Other errors: This umbrella term encompasses other mistakes like computational errors that result in overlooking the solution or furnishing incorrect answers.To exclusively showcase the AoT's search capabilities, we also present the AoT + Manual Resolution version.Here, once the LLM pinpoints a solution, its final articulation is manually processed-a strategy also employed by the ToT method.As evidenced in</p>
<p>Mini Crosswords</p>
<p>The 5 × 5 mini crossword is a compact word puzzle featuring a grid of 25 squares arranged in a 5-by-5 configuration.Players are tasked with filling the grid based on provided clues for each word.Clues are given for words that run both across (horizontally) and down (vertically).Words intersect at certain letters, offering additional hints to complete the puzzle.</p>
<p>Task Setup.Adhering to the setup outlined in (Yao et al. 2023), we draw our prompts from games 136, 141, 146, 151, and 156 out of the 156 games available on goobix.com.Our testing focuses on a set of 20 games, specifically games 1, 6, . .., 91, and 96.</p>
<p>Baselines.Mirroring our approach for the game of 24, we benchmark our method against established techniques: standard prompting, CoT, and ToT.For standard prompting, we provide both the crosswords and their respective solutions as in-context examples.CoT augments this by prompting the retrieval of words for each of the ten clues-equally split between horizontal and vertical orientations.We directly extract the success rates of ToT from their original publication for comparison.</p>
<p>AoT Setup.We divide the process into two steps, each involving a query.Initially, we task the LLM with suggesting five potential words for each row and column.We then pinpoint the starting word candidates that have the highest compatibility with other words within the crossword framework.This preliminary phase mirrors a 'warm-up' sequence in algorithm initialization.In the subsequent step, we exclusively leverage the LLM's algorithmic reasoning prowess, starting with the pre-selected word.The method involves cyclically choosing a likely option (specifically, a row or column) for insertion, generating candidate words, and assessing their compatibility with the words already on the board.If no match is found, the process shifts focus to another promising candidate.Otherwise, the word is added to the crossword, and the search continues.The cycle concludes either when the board is fully populated or no more suitable words can be found, which may be due to either incorrect existing words or the absence of matching words.Notably, this entire process unfolds within a single generation window.The algorithmic examples in our prompt (detailed in the Appendix) include three that achieve game completion and two that predominantly populate the crossword, filling 8 or 9 slots.</p>
<p>Results.Table 3 underscores AoT's proficiency in the mini crosswords task, showcasing a word success rate-a measure used in existing studies to represent the percentage of words correctly completed out of the total-that surpasses earlier methods reliant on various prompting techniques.However, it trails behind ToT.An important observation is the sheer volume of queries ToT employs, exceeding AoT's by over a factor of 100.One factor hindering AoT from surpassing ToT is that the backtracking capability inherent in the algorithmic example isn't fully activated.Fully unlocking this capability would lead to a significant elongation in the generation phase.In contrast, ToT has the advantage of leveraging external memory for its backtracking.</p>
<p>Method</p>
<p>Word Success Avg.Queries
Standard Prompting 14% 1 CoT 15.6% 1 ToT 60% &gt;200 AoT (ours) 52% 2
Table 3: 5 × 5 mini crosswords word: word success rates and the average number of LLM queries for each example.</p>
<p>Error Analysis.To understand the prevalent mistakes made by AoT, we've categorized the errors into four distinct categories.In our analysis for each game, we focus on the initial error the LLM produces while charting its reasoning path, given that an early error typically cascades into subsequent failures.1) No preselections: LLM fails to generate compatible words essential for the warm-start phase.Given a correctly preselected word, the second phase for recursive reasoning can exhibit errors including: 2) Expression misstep: The LLM mistakenly believes it has exhausted all choices and jumps to an answer prematurely.3) Incorrect pattern extraction: The LLM wrongly extracts a pattern based on the current board layout.4) Erroneous word placement: Despite recognizing the correct pattern, the LLM selects a mismatched word or misses better-fitting alternatives.</p>
<p>Navigating the crossword complexity arises from outdated terms, esoteric references, and typographical mishaps.Predominantly, the errors observed are due to misguided word placements followed by pattern misinterpretations.Also, the LLM seems challenged in aligning letters at precise indices to create word structures-an obstracle circumvented by an external mechanism in the ToT framework.</p>
<p>Discussion</p>
<p>In this section, we delve into crucial aspects to consider when crafting prompts for AoT, using the game of 24 as our primary case study.</p>
<p>Can AoT surpass the DFS it's patterned after?A core query of ours is to ascertain if the LLM has the capability to not only mirror but also outdo the efficiency of the algorithm introduced in-context.As evidenced in Fig. 5 systematically navigates fewer nodes than its DFS counterpart.While DFS employs a uniform strategy when choosing the subsequent subtree to investigate, AoT's LLM integrates its inherent heuristic.This amplification over the base algorithm exemplifies the advantages of LLM's recursive reasoning capability.</p>
<p>How does algorithm selection influence AoT's efficacy?</p>
<p>To explore the impact of algorithm choice on AoT's performance, we implemented both BFS and random search within the AoT framework.Our findings, presented in Table 5, reveal that all three AoT variations outperform the single-query CoT.This outcome was anticipated as AoT, irrespective of the algorithm, undertakes a search and revisits potential mistakes-either by random retry in the random search variant or through backtracking in the DFS and BFS configurations.Notably, the structured search versions, AoT (DFS) and AoT (BFS), displayed better efficiency than AoT (Random), underscoring the advantage of algorithmic insights in solution discovery.However, AoT (BFS) lagged behind AoT (DFS).Closer inspection of errors made by AoT (BFS) revealed the LLM faced greater challenges in identifying optimal operations than its DFS counterpart.</p>
<p>How does the search step count within the algorithmic example modulate AoT's behavior?We begin with the standard AoT prompt and modify the subtree explorations.</p>
<p>In AoT (Short), each in-context example uses one or two steps to reach a solution, while AoT (Long) incorporates three to five extra subtree explorations.The impact on total search steps is illustrated in Fig. 6. for AoT (Short) relative to the original AoT.This suggests that the search step count introduces an implicit bias on the LLM's search velocity.Notably, even when navigating incorrect steps, it's essential to emphasize the exploration of promising directions.Limitations.While AoT substantially cuts down on the number of queries relative to ToT, its resource demands exceed those of standard prompting and CoT, a consequence of its extensive exploration of ideas via token generation.Crafting token-efficient algorithmic examples is one avenue, but there's also potential in judiciously tapping into or unlocking the LLM's "tunnel-vision".Our research primarily spotlighted certain algorithms, with a keen focus on treesearch tasks.It's pertinent to highlight that we conducted our tests exclusively with GPT-4.Though more costly than other LLMs, GPT-4's advanced capabilities appear pivotal for AoT's optimal functioning; models of lesser caliber might not yield comparable performance boosts from AoT.</p>
<p>Conclusion</p>
<p>This paper presents the Algorithm of Thoughts, a pioneering prompting strategy to navigate reasoning pathways in LLMs using minimal queries.Our findings reveal that this method not only substantially surpasses prior single-query techniques but also rivals external tree-search implementations.Such an approach augments the potential to streamline idea discovery in LLMs, balancing both cost and computational demands.Future work includes designing token-efficient algorithmic examples, developing adaptive mechanisms for "tunnel-vision" activation to expedite the search, and deepening the understanding of this fresh mode of incontext learning from theoretical angles.</p>
<p>Game of 24 -Additional Details</p>
<p>In order to avoid confusion in our analysis of AoT in the game of 24, we give additional details in terms of terminologies we use as well as their direct implications in the performance figures.An Illustration of these are given in Fig. 7. ...
4</p>
<p>Subtree Exploration</p>
<p>Figure 7: An illustration of terminologies we use for the game of 24.The yellow nodes represent the first operations and the states they lead to; the green node represents the node where we find the solution; all other nodes are represented by pink.</p>
<p>First operations / First iterations.This represents the scenario that after we choose the first two number in the game of 24, the case of either adding, subtracting, multiplying or dividing them.</p>
<p>Subtree Exploration.This denotes searching all or most of the nodes coming from the same state, typically states with less than four numbers left.</p>
<p>Number of nodes visited.This is the number of states that the method has been on the game of 24.Each state is the set of number we are left with, after our operations in the numbers.For example, after the first operation we might be left with the numbers '8 3 1'.This set of numbers represent a state, as well as the state of '8 3' that we will be left with after another operation of '8 * 1 = 8'.</p>
<p>Creative Writing</p>
<p>We use the creative writing task, also used by (Yao et al. 2023), where the LLM is provided with four arbitrary sentences.The objective is to craft a cohesive narrative divided into four paragraphs, with each paragraph culminating in one of the given sentences.This exercise not only fosters creativity but also emphasizes strategic deliberation.</p>
<p>Task Setup</p>
<p>Sentences are randomly sourced from randomwordgenerator.com, resulting in 100 distinct sets of inputs.Given the absence of predetermined correct answers, the primary focus lies in evaluating the coherence of the responses.We have noted that GPT-4 consistently aligns with these input guidelines.Evaluation is centered around assessing passage coherence using a GPT-4 zeroshot prompt, where each output is rated on a scale of 1 to 10.Each task response undergoes five such evaluations, with their scores being averaged subsequently.</p>
<p>Baselines</p>
<p>For this task, both standard and CoT prompts are employed without preliminary training.While the standard prompt directly guides the LLM to fashion a cohesive narrative based on stipulated parameters, the CoT prompt obliges the model to initially outline a succinct plan prior to drafting the narrative, serving as an intermediate cognitive bridge.For each task iteration, ten samples are generated using both the standard and CoT methods.Results of the ToT approach are presented without modification.</p>
<p>AoT Setup</p>
<p>Mirroring ToT's methodology, the task is tackled in a zero-shot setting.Our prompt instructs the model to first formulate five distinct plans.Subsequent to this, the model selects the most promising among them to shape a narrative and then refines it for optimal coherence.The exact prompts used for this zero-shot approach will be provided in the subsequent section.</p>
<p>Results</p>
<p>As depicted in Fig. 8, AoT outpaces other singular query prompting techniques such as standard prompting and CoT in terms of performance.It also exhibits a marked improvement over ToT, although the difference is not statistically significant.Comprehensive scores, along with the average query count needed for each method, are consolidated in Table 6</p>
<p>CoT vs. Single Iteration AoT in the Game of 24</p>
<p>To demonstrate that the tree search mechanism is fundamentally distinct from the CoT prompting, even in scenarios where AoT's in-context examples include only a single initial operation in the game of 24, we draw a comparison between AoT (Short) and CoT.In this setup, AoT (Short) determines the first operation and subsequently conducts a tree search on the remaining three numbers.Interestingly, AoT (Short) achieves a success rate of 48%, while CoT lags significantly, securing only 4%.These results underscore the notion that even a rudimentary search mechanism can lead to significant performance enhancements.</p>
<p>Detailed Analysis on the Effect of the Length of the Prompts</p>
<p>In this section, we delve deeper into Fig.6 by presenting histograms for the successful, unsuccessful, and total games of '24', considering the number of initial steps in methods AoT (Short), AoT, and AoT (Long).These are displayed in Figs.9-11.</p>
<p>From these figures, it becomes evident that the length of the prompts, measured by the number of initial steps included in in-context examples, correlates with the length of their solutions to test examples.This trend is consistent across all three cases, suggesting that AoT's strategy in determining the number of initial steps is influenced by its in-context examples.</p>
<p>Interestingly, when AoT is provided a well-balanced set of initial steps that emphasize the most promising operations, it excels in solving the majority of games in earlier iterations.This indicates AoT's capacity to prioritize swift problem-solving without sacrificing performance.This tendency is also observed in AoT (Long), albeit with a somewhat reduced success rate, as illustrated in Fig. 9</p>
<p>Prompts Game of 24</p>
<p>Below, we represent the specific prompts employed for the various methods detailed in the experiments section.It's important to note that the terms "System","User", and "Assistant" are utilized to denote the roles within the OpenAI API when operating in chat completion mode.The line breaks serve to show the transitions between the user and assistant interactions within the API.</p>
<p>AoT (DFS)</p>
<p>System: Use numbers and basic arithmetic operations (+ -* /) to obtain 24.When considering the next steps, do not choose operations that will result in a negative or fractional number.In order to help with the calculations, the numbers in the parenthesis represent the numbers that are left after the operations and they are in descending order.</p>
<p>Another thing we do is when there are only two numbers left in the parenthesis, we check whether we can arrive at 24 only by using basic arithmetic operations (+ -* /).Some examples regarding this idea: (21 2) no since 21 + 2 = 23, 21 -2 = 19, 21 * 2 = 42, 21 / 2 = 10.5, none of which is equal to 24. (30 6) 30 -6 = 24 yes (8 3) 8 * 3 = 24 yes (12 8) no (48 2) 48 / 2 = 24 yes Most importantly, do not give up, all the numbers that will be given has indeed a solution.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>14 Step 1: 8 / 2 = 4</p>
<p>Step 2: 14 -8 = 6</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (14 -8) * 4 = (14 -8) * (8 / 2) = 24.answer: (14 -8) * (8 / 2) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>9 5 5 5 Assistant:</p>
<p>Trying a promising first operation: 1. 9 -5: (5, 5, 4) -5 + 5: (10, 4) 14, 6, 40, fractional -5 -5: (4, 0) 4, 4, 0, undefined -5 * 5: (25, 4) 29, 21, 100, fractional -5 / 5: (4, 1) 5, 3, 4, 4 -5 + 4: (9, 5) 14, 4, 45, fractional -5 -4: (5, 1) 6, 4, 5, 0.2 -5 * 4: (20, 5) 25, 15, 100, fractional -5 / 4: fractional</p>
<p>Trying another promising first operation: 2. 5 + 5: (10, 9, 5) -10 + 9: (19, 5) 24 = 19 + 5 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 5 + 5 = 10</p>
<p>Step 2: 10 + 9 = 19</p>
<p>Step 3: 19 + 5 = 24 Considering these steps: 24 = 19 + 5 = (10 + 9) + 5 = ((5 + 5) + 9) + 5 = 24.answer: ((5 + 5) + 9) + 5 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>8 6 4 4 Assistant:</p>
<p>Trying a promising first operation: 1. 8 -6: (4, 4, 2) -4 + 4: (8, 2) 10, 6, 16, 4 -4 -4: (2, 0) 2, 2, 0, undefined -4 * 4: (16, 2) 18, 14, 32, 8 -4 / 4: (2, 1) 3, 1, 2, 2 -4 + 2: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 8 -6 = 2</p>
<p>Step 2: 4 + 2 = 6</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (4 + 2) * 4 = (4 + (8 -6)) * 4 = 24.answer: (4 + (8 -6)) * 4 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>13 10 9 4 Assistant:</p>
<p>Trying a promising first operation: 1. 13 -4: (10, 9, 9) -10 + 9: (19, 9) 28, 10, 171, fractional -10 -9: (9, 1) 10, 8, 9, 9 -10 * 9: (90, 9) 99, 81, 810, fractional -10 / 9: fractional -9 + 9: (18, 10) 28, 8, 180, fractional -9 -9: (10, 0) 10, 10, 0, undefined -9 * 9: (81, 10) 91, 71, 810, fractional -9 / 9: (10, 1) 11, 9, 10, 10</p>
<p>Trying another promising first operation: 2. 10 -4: (13, 9, 6) -13 + 9: (22, 6) 28, 16, 132, fractional -13 -9: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 10 -4 = 6</p>
<p>Step 2: 13 -9 = 4</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = 6 * (13 -9) = (10 -4) * (13 -9) = 24.answer: (10 -4) * ( 13 Step 1: 11 * 3 = 33</p>
<p>Step 2: 33 -5 = 28</p>
<p>Step 3:</p>
<p>28 -4 = 24 Considering these steps: 24 = 28 -4 = (33 -5) -4 = ((11 * 3) -5) -4 = 24.answer: ((11 * 3) -5) -4 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>13 12 5 2 Assistant:</p>
<p>Trying a promising first operation: 1. 12 + 2: (14, 13, 5) -14 + 13: (27, 5) 32, 22, 135, fractional -14 -13: (5, 1) 6, 4, 5, 5 -14 * 13: (182, 5) 187, 177, 910, fractional -14 / 13: fractional -14 + 5: (19, 13) 32, 6, 247, fractional -14 -5: ( Step 1: 9 -1 = 8</p>
<p>Step 2: 8 * 2 = 16</p>
<p>Step 3: 16 + 8 = 24 Considering these steps: 24 = 16 + 8 = (8 * 2) + 8 = ((9 -1) * 2) + 8 = 24.answer: ((9 -1) * 2) + 8 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ã</p>
<p>oT (Long)</p>
<p>System:</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.When considering the next steps, do not choose operations that will result in a negative or fractional number.In order to help with the calculations, the numbers in the parenthesis represent the numbers that are left after the operations and they are in descending order.</p>
<p>Another thing we do is when there are only two numbers left in the parenthesis, we check whether we can arrive at 24 only by using basic arithmetic operations (+ -* /).Some examples regarding this idea: (21 2) no since 21 + 2 = 23, 21 -2 = 19, 21 * 2 = 42, 21 / 2 = 10.5, none of which is equal to 24. (30 6) 30 -6 = 24 yes (8 3) 8 * 3 = 24 yes (12 8) no (48 2) 48 / 2 = 24 yes Most importantly, do not give up, all the numbers that will be given has indeed a solution.User:</p>
<p>14 Trying another promising first operation: 7. / 2: (14, 8, 4) -14 + 8: (22, 4) 26, 18, 88, fractional -14 -8: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 8 / 2 = 4</p>
<p>Step 2: 14 -8 = 6</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (14 -8) * 4 = (14 -8) * (8 / 2) = 24.answer: (14 -8) * (8 / 2) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>9 5 5 Assistant:</p>
<p>Trying a promising first operation: Trying another promising first operation: 6. 5 + 5: (10, 9, 5) -+ 9: (19, 5) 24 = 19 + 5 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 5 + 5 = 10</p>
<p>Step 2: 10 + 9 = 19</p>
<p>Step 3: 19 + 5 = 24 Considering these steps: 24 = 19 + 5 = (10 + 9) + 5 = ((5 + 5) + 9) + 5 = 24.answer: ((5 + 5) + 9) + 5 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>8 6 4 4 Assistant:</p>
<p>Trying a promising first operation: 1. 8 * 6: (48, 4, 4) -48 + 4: (52, 4) 56, 48, 208, 13 -48 -4: (44, 4) 48, 40, 176, 11 -48 * 4: (192, 4) 196, 188, 768, 48 -48 / 4: (12, 4)  Trying another promising first operation: 5. 13 / 9: (10, 9, 1.4) -10 + 9: (19, 1.4) fractional -10 -9: (1.4, 1) fractional -10 * 9: (90, 1.4) fractional, fractional, 130, fractional -10 / 9: (1.1, 1.4) fractional -10 + 1.4: (11.4,9) fractional -10 -1.4: (9, 8.6) fractional -10 * 1.4: (140, 9) 149, 131, 1260, fractional -10 / 1.4: (9, 7.1) fractional -9 + 1.4: (10.4,10) fractional, fractional, 104, fractional -9 -1.4: (10, 7.6) fractional, fractional, 76, fractional -9 * 1.4: (12.6, 10) fractional, fractional, 126, fractional -9 / 1.4: (10, 6.4) fractional, fractional, 64, fractional Trying another promising first operation: 6. 10 -4: (13, 9, 6) -13 + 9: (22, 6) 28, 16, 132, fractional -13 -9: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 10 -4 = 6</p>
<p>Step 2: 13 -9 = 4</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = 6 * (13 -9) = (10 -4) * (13 -9) = 24.answer: (10 -4) * (13 -9) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ã oT (Random)</p>
<p>System:</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>4 4 6 8. Assistant:</p>
<p>Trying a promising first operation: (4 + 4) * 6 -8 = 40.</p>
<p>Trying another promising first operation: (8 -6) * 4 + 4 = 12.</p>
<p>Trying another promising first operation: (6 -4) * (4 + 8) = 24.answer: (6 -4) * (4 + 8) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser: 2 9 10 12. Assistant:</p>
<p>Trying a promising first operation: ((12 -10) / 2) + 9 = 10.</p>
<p>Trying another promising first operation: (10 + 9) + (12 / 2) = 25.</p>
<p>Trying another promising first operation: (12 * 2) * (10 -9) = 24.answer: (12 * 2) * (10 -9) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser: 4 9 10 13.Assistant:</p>
<p>Trying a promising first operation: ((13 -10) / 4) + 9 = 10.</p>
<p>Trying another promising first operation: 4 * (9 -(13 -10)) = 24.answer: 4 * (9 -(13 -10)) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>1 4 8 8. Assistant:</p>
<p>Trying a promising first operation: (8 + 1) + 4 * 8 + 32.</p>
<p>Trying another promising first operation:</p>
<p>(1 + 8 / 4) * 8 = 24.answer: (1 + 8 / 4) * 8 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>5 5 5 9. Assistant:</p>
<p>Trying a promising first operation: ((5 + 5) + 5) + 9 = 24.answer: ((5 + 5) + 5) + 9 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ã</p>
<p>oT (BFS)</p>
<p>System:</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.When considering the next steps, do not choose operations that will result in a negative or fractional number.In order to help with the calculations, the numbers in the parenthesis represent the numbers that are left after the operations and they are in descending order.</p>
<p>Another thing we do is when there are only two numbers left in the parenthesis, we check whether we can arrive at 24 only by using basic arithmetic operations (+ -* /).Some examples regarding this idea: (21, 2): 23, 19, 42, fractional (30, 6): 36, 24 = 30 -6 -&gt; found it!(8, 3): 11, 5, 24 = 8 * 3 yes (12, 8): 20, 4, 72, fractional In the second steps, when there are three numbers left, we choose the most promising operations so that when we are left with two numbers, we will be able to get to 24.</p>
<p>So, when we reach 24 in the third steps, that means we found the solution, we directly backtrack to write the answer.</p>
<p>In each step, we want to choose the best operations so that the numbers left, will have a good chance to reach 24.User: 14 8 8 2 Assistant:</p>
<p>Let's consider the most promising possible first steps: 2. 14 -8 = 6 -8 -6: (2, 2): 4, 0, 4, 1 -8 / 2: (6, 4): 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 14 -8 = 6</p>
<p>Step 2: 8 / 2 = 4</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (14 -8) * (8 / 2) = 24.answer: (14 -8) * (8 / 2) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>9 5 5 5 Assistant:</p>
<p>Let's consider the most promising first steps: 1. 9 -5: (5, 5, 4) 2. 5 + 5: (9, 10, 5) 3. 9 + 5: (14, 5, 5) 4. 5 -5: (9, 5, 0)</p>
<p>Let's consider the most promising second steps: 1. (5, 5, 4) -5 + 5: (10, 4) -5 * 4: (20, 5) -5 -4: (5, 1) -5 / 4: (5, 1.25) 2. (9, 10, 5) -10 + 9: (19, 5) -10 -5: (9, 5) -10 * 5: (50, 9) -10 / 5: (9, 2)</p>
<ol>
<li>(14, 5, 5) -14 -5: (9, 5) -14 + 5: (19, 5) -5 * 5: (25, 14) -5 / 5: (14, 1) 4. (9, 5, 0) -9 + 5: (14, 0) -9 -5: (4, 0) -5 * 0: (9, 0) -5 / 0: (9, infinity)</li>
</ol>
<p>Let's consider the most promising third steps: 1. 9 -5 = 4 -5 + 5: (10, 4): 14, 6, 40, fractional -5 * 4: (20, 5): 25, 15, 100, 4 -5 -4: (5, 1): 6, 4, 5, 5 -5 / 4: (5, 1.25): 6.25, 3.75, 6.25, 4 2. 5 + 5 = 10 -10 + 9: (19, 5): 24 = 19 + 5 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 5 + 5 = 10</p>
<p>Step 2: 10 + 9 = 19</p>
<p>Step 3: 19 + 5 = 24 Considering these steps: 24 = 19 + 5 = (10 + 9) + 5 = ((5 + 5) + 9) + 5 = 24.answer: ((5 + 5) + 9) + 5 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>8 -13 -4: (9, 1) -13 * 4: (52, 1) -4 -1: (13, 3) -13 -1: (12, 4)</p>
<p>Let's consider the most promising third steps: 1. 13 -4 = 9 -10 + 9: (19, 9): 28, 10, 171, fractional -10 -9: (9, 1): 10, 8, 9, 9 -9 + 9: (18, 10): 28, 8, 180, fractional -9 / 9: (9, 1): 10, 8, 9, 9 2. 10 -4 = 6 -9 + 6: (15, 13): 28, 2, 195, fractional -9 * 6: (54, 13): 67, 41, 702, fractional -13 -9: (6, 4): 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 13 -9 = 4</p>
<p>Step 2: 10 -4 = 6</p>
<p>Step 3:</p>
<p>6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (10 -4) * (13 -9) = 24.answer: (10 -4) * ( 13 -11 + 8: (19, 4): 23, 15, 76, 4.75 -8 + 4: (12, 11): 23, 7, 132, 3 -11 -8: (4, 3): 7, 1, 12, 1.33 -8 -4: (7, 11): 18, 4, 77, 1.75 3. 4 + 1 = 5 -11 -5: (7, 6): 13, 1, 42, 1.17 Let's consider the most promising third steps to reach 24: 1. 11 * 3 = 33 -33 -5: (28, 4): 32, 24 = 28 -4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 11 * 3 = 33</p>
<p>Step 2: 33 -5 = 28</p>
<p>Step 3: 28 -4 = 24 Considering these steps: 24 = 28 -4 = (33 -5) -4 = ((11 * 3) -5) -4 = 24.answer: ((11 * 3) -5) -4 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>13 12 5 2 Assistant:</p>
<p>Let's consider the most promising first steps to reach 24: 1. 12 / 2: (13, 6, 5) 2. 13 + 5: (18, 12, 2) 3. 12 -5: (13, 7, 2) 4. 5 -2: (13, 12, 3)</p>
<p>Let's consider the most promising second steps to reach 24: 1. (13, 6, 5) -13 + 6: (19, 5) -13 -5: (8, 6) -6 * 5: (30, 13) -6 -5: (13, 1) Let's consider the most promising third steps to reach 24: 1. 9 -1 = 8 -8 + 8: (16, 2): 18, 14, 32, 8 -8 * 2: (16, 8): 24 = 16 + 8 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 9 -1 = 8</p>
<p>Step 2: 8 * 2 = 16</p>
<p>Step 3: 16 + 8 = 24 Considering these steps: 24 = 16 + 8 = (8 * 2) + 8 = ((9 -1) * 2) + 8 = 24.answer: ((9 -1) * 2) + 8 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ã oT (Short)</p>
<p>System:</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.When considering the next steps, do not choose operations that will result in a negative or fractional number.In order to help with the calculations, the numbers in the parenthesis represent the numbers that are left after the operations and they are in descending order.</p>
<p>Another thing we do is when there are only two numbers left in the parenthesis, we check whether we can arrive at 24 only by using basic arithmetic operations (+ -* /).Some examples regarding this idea: (21 2) no since 21 + 2 = 23, 21 -2 = 19, 21 * 2 = 42, 21 / 2 = 10.5, none of which is equal to 24. (30 6) 30 -6 = 24 yes (8 3) 8 * 3 = 24 yes (12 8) no (48 2) 48 / 2 = 24 yes Most importantly, do not give up, all the numbers that will be given has indeed a solution.User:</p>
<p>14 8 8 2 Assistant:</p>
<p>Trying a promising first operation: 1. 8 / 2: (14, 8, 4) -14 + 8: (22, 4) 26, 18, 88, fractional -14 -8: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 8 / 2 = 4</p>
<p>Step 2: 14 -8 = 6</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = (14 -8) * 4 = (14 -8) * (8 / 2) = 24.answer: (14 -8) * (8 / 2) = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>9 5 5 5 Assistant:</p>
<p>Trying a promising first operation: 1. 5 + 5: (10, 9, 5) -10 + 9: (19, 5) 24 = 19 + 5 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 5 + 5 = 10</p>
<p>Step 2: 10 + 9 = 19</p>
<p>Step 3: 19 + 5 = 24 Considering these steps: 24 = 19 + 5 = (10 + 9) + 5 = ((5 + 5) + 9) + 5 = 24.answer: ((5 + 5) + 9) + 5 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>8 6 4 4 Assistant:</p>
<p>Trying a promising first operation: Trying a promising first operation: 1. 10 -4: (13, 9, 6) -13 + 9: (22, 6) 28, 16, 132, fractional -13 -9: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 10 -4 = 6</p>
<p>Step 2: 13 -9 = 4</p>
<p>Step 3: 6 * 4 = 24 Considering these steps: 24 = 6 * 4 = 6 * (13 -9) = (10 -4) * (13 -9) = 24.answer: (10 -4)  -33 -5: (28, 4) 32, 24 = 28 -4 -&gt; found it!Backtracking the solution:</p>
<p>Step 1: 11 * 3 = 33</p>
<p>Step 2: 33 -5 = 28</p>
<p>Step 3: 28 -4 = 24 Considering these steps: 24 = 28 -4 = (33 -5) -4 = ((11 * 3) -5) -4 = 24.answer: ((11 * 3) -5) -4 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜Ũ ser:</p>
<p>13 12 5 2 Assistant:</p>
<p>Trying a promising first operation: Step 1: 9 -1 = 8</p>
<p>Step 2: 8 * 2 = 16</p>
<p>Step 3: 16 + 8 = 24 Considering these steps: 24 = 16 + 8 = (8 * 2) + 8 = ((9 -1) * 2) + 8 = 24.answer: ((9 -1) * 2) + 8 = 24.˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜˜5 × 5 Mini Crosswords Prompts</p>
<p>In the following we give all the exact prompts we used for the methods given in the experiments section and some selection of the generations from the LLM.In the prompts the keywords "System", "User" and "Assistant" used are the roles in the OpenAI API in chat completion mode.The line breaks are to show the turns between the user and assistant in the API.There are two horizontal and two vertical words.Since they are the amount, let's consider the next unfound horizontal word, which is h2.Since we are considering the second horizontal word, we need to look at the second letters of the vertical words we found: -What is the second letter of ilebo: e -What is the second letter of enter: n The word we are looking fits the clue "A fatty oil" and needs to have the letters e and n in its (v2, two) second and (v5, five) fifth letters: _ e _ _ n.The candidates are: oleic, olein, olive.-oleic (o l e i c, _ e _ _ n) doesn't fit -olein (o l e i n, _ e _ _ n) fits -olive (o l i v e, _ e _ _ n) doesn't fit We add the word olein for h2.The current words are: h1.rille h2.olein h3.tempt v2.ilebo v5.enter Now, let's consider the next unfound vertical word, which is v1.Since we are considering the first vertical word, we need to look at the first letters of the horizontal words we found: -What is the first letter of rille: r -What is the first letter of olein: o -What is the first letter of tempt: t The word we are looking fits the clue "According to the roster" and needs to have the letters r, o, and t in its (h1, one) first, (h2, two) second, and (h3, three) third letters: r o t _ <em>.The candidates are: none Now, let's consider the next unfound vertical word, which is v3.Since we are considering the third vertical word, we need to look at the third letters of the horizontal words we found: -What is the third letter of rille: l -What is the third letter of olein: e -What is the third letter of tempt: m The word we are looking fits the clue "An illicit lover; a European lake" and needs to have the letters l, e, and m in its (h1, one) first, (h2, two) second , and (h3, three) third letters: l e m _ </em>.The candidates are: leman.-leman (l e m a n, l e m _ <em>) fits We add the word leman for v3.The current words are: h1.rille h2.olein h3.tempt v2.ilebo v3.leman v5.enter Now, let's consider the next unfound horizontal word, which is h4.Since we are considering the fourth horizontal word, we need to look at the fourth letters of the vertical words we found: -What is the fourth letter of ilebo: b -What is the fourth letter of leman: a -What is the fourth letter of enter: e The word we are looking fits the clue "To lower; to reduce" and needs to have the letters b, a, and e in its (v2, two) second, (v3, three) third, and (v5, five) fifth letters: _ b a _ e.The candidates are: abase, abate.-abase (a b a s e, _ b a _ e) fits -abate (a b a t e, _ b a _ e) fits Since both fits, let's consider another word.Now, let's consider the last unfound vertical word, which is v4.Since we are considering the fourth vertical word, we need to look at the fourth letters of the horizontal words we found: -What is the fourth letter of rille: l -What is the fourth letter of olein: i -What is the fourth letter of tempt: p The word we are looking fits the clue "To lisp" and needs to have the letters l, i , p, and s in its (h1, one) first, (h2, two) second, (h3, three) third letters : l i p _ </em>.The candidates are: The candidate is "lisps".-lisps (l i s p s, l i p _ _) doesn't fit</p>
<p>AoT</p>
<p>We have considered all of the words for the horizontal words, we have two options: abase and abate, let's choose one of them.There are two horizontal and two vertical words.Since they are the amount, let's consider the next unfound horizontal word, which is h2.Since we are considering the second horizontal word, we need to look at the second letters of the vertical words we found: -What is the second letter of surge: u -What is the second letter of edema: d The word we are looking fits the clue "A fungus genus" and needs to have the letters u and d in its (v1, one) first and (v4, four) fourth letters: u _ _ d <em>.The candidates are: uredo.-uredo (u r e d o, u _ _ d </em>) fits We add the word uredo for h2.The current words are: h1.sawer h2.uredo h3.rater v1.surge v4.edema Now, let's consider the next unfound vertical word, which is v2.Since we are considering the second vertical word, we need to look at the second letters of the horizontal words we found: -What is the second letter of sawer: a -What is the second letter of uredo: r -What is the second letter of rater: a The word we are looking fits the clue "The Brazilian macaw; an Australian bird" and needs to have the letters a, r, and a in its (h1, one) first, (h2, two) second, and (h3, third) third letters: a r a _ <em>.The candidates are: arara.-arara (a r a r a, a r a _ </em>) fits We add the word arara for v2.The current words are: h1.sawer h2.uredo h3.rater v1.surge v2.arara v4.edema Now, let's consider the next unfound horizontal word, which is h4.Since we are considering the fourth horizontal word, we need to look at the fourth letters of the vertical words we found: -What is the fourth letter of surge: g -What is the fourth letter of arara: r -What is the fourth letter of edema: m the word we are looking fits the clue "Pasture land" and needs to have the letters g, r, and m in its (v1, one) first, (v2, two) second, and (v4, four) fourth letters: g r _ m _.The candidates are: grama.</p>
<p>-grama (g r a m a, g r a _ _) fits We add the word grama for h4.The current words are: h1.Sawer h2.Uredos h3.Rater h4.Grama v1.surge v2.arara v4.edema Now, let's consider the next unfound vertical word, which is v3.Since we are considering the third vertical word, we need to look at the third letters of the horizontal words we found: -What is the third letter of sawer: w -What is the third letter of uredo: e -What is the third letter of rater: t Plan 1: Your plan here.</p>
<p>Plan 2: Your plan here.</p>
<p>Plan 3: Your plan here.</p>
<p>Plan 4: Your plan here.</p>
<p>Plan 5: Your plan here.</p>
<p>Secondly, given an instruction and several plans, decide which choice is most promising.Analyze each choice in detail, then conclude in the last line "The best choice is {{s}}", where s the integer id of the choice.</p>
<p>Thirdly, write the passage according to that chosen plan in the most coherent way.Add "Passage:" before writing the passage under it.</p>
<p>Passage: Your passage here.</p>
<p>Finally, refine the passage in the most coherent way, but you still have to end each paragraph with the given sentences as before.</p>
<p>Final Passage: Final passage here.</p>
<p>Score Prompt</p>
<p>Analyze the following passage, then at the last line conclude "Thus the coherency score is {{s}}", where s is an integer from 1 to 10. {0}</p>
<p>Figure 3 :
3
Figure 3: The probability of generating the correct token as we add more in-context examples that are correct but possess identical outputs.</p>
<p>Figure 5 :
5
Figure 5: Histogram showing the number of visited nodes for AoT and DFS in the Game of 24.</p>
<p>Figure 6 :
6
Figure 6: Comparison of AoT with shorter and longer incontext examples prompted AoT versions: cumulative number of games for the number of visited nodes.</p>
<p>Figure 10 :
10
Figure 10: Histogram of the number of unsuccessful games with respect to the number of first steps for AoT (Short), AoT and AoT (Long).</p>
<p>Figure 11 :
11
Figure 11: Histogram of the number of all games with respect to the number of first steps for AoT (Short), AoT and AoT (Long).</p>
<dl>
<dt>-</dt>
<dd>
<p>(25, 4) 29, 21, 100, fractional -/ 5: (4, 1) 5, 3, 4, 4 -+ 4: (9, 5) 14, 4, 45, fractional --4: (5, 1) 6, 4, 5, 0.2 (25, 4) 29, 21, 100, fractional -<em> 5: (45, 25) 70, 20, 1125, fractional -/ 5: (25, 1.8) fractional, fractional, 45, fractional -+ 0: (9, 5) 14, 4, 45, fractional --0: (9, 5) 14, 4, 45, fractional -</em> 0: (5, 0) 5, 5, 0, undefined -</p>
</dd>
</dl>
<p>Illustration outlining various strategies for tackling reasoning problems with LLMs.Each box signifies a distinct thought, functioning as a unified string of words that forms an incremental pathway to reasoning.Green boxes indicate ideas deemed promising by the LLM, while red boxes represent less promising concepts.
InputInputInputInputOutputOutputOutputOutputStandard PromptingChain of ThoughtsTree of ThoughtsAlgorithm of ThoughtsFigure 2:
Chain-of-Thought.In CoT, LLMs are presented with examples where a given question x unfolds through a chain of intermediate reasoning pieces c 1 , . . ., c n to reach an answer y, represented as x → c 1 → . . .→ c n → y (Wei</p>
<p>Table 1 :
1
Game of 24: success rates and the average number of LLM queries for each example.
Success Avg. QueriesStandard Prompting 7.3%1CoT4.0%1CoT-SC (k = 100)9.0%100Standard + Refine27%10ToT (b = 5)69%109.1AoT (ours)71%1</p>
<p>Table 2
2, a notable
cessful problem resolutions.Additionally, the token limitation underscores the appeal of expanding the generative context window, which may further bolster LLMs' recursive reasoning when engaged with algorithmic examples.</p>
<p>Table 2 :
2
Game of 24: AoT error analysis.</p>
<p>Table 4 :
4
, AoT Breakdown of errors in 5 × 5 mini crosswords with AoT.Numbers indicate the relative percentage of each error type among all errors.
Error TypeErrorNo preselections15.8%Expression misstep5.3%Incorrect pattern extraction 26.3%Erroneous word placement 52.6%</p>
<p>Table 5 :
5
Comparative success rates and average LLM query counts for AoT variations templated by distinct algorithms.
MethodSuccess Avg. QueriesCoT4%1CoT-SC (k=100) 9%100ToT69%109.1AoT (DFS)71%1AoT (BFS)48%1AoT (Random)20%1
Our observations highlight longer generations for AoT (Long) and shorter ones</p>
<p>Table 6 :
6
. Notably, AoT necessitates fewer queries compared to ToT.Comparison of the standard prompting, CoT, ToT and AoT on the creative writing task.Performance of the methods determined by
1086420Standard CoTToTAoTFigure 8: MethodScore Avg. QueriesStandard Prompting 6.191CoT6.931ToT7.5620AoT7.581</p>
<p>. Figure 9: Histogram of the number of successful games with respect to the number of first steps for AoT (Short), AoT and AoT (Long).</p>
<h1>of Successful Games0 02 24 46 68 810 1012 12AoT (Short) AoT AoT (Long)0246 # of First Steps81012# of Unsuccessful Games0 02 24 46 68 810 1012 12AoT (Short) AoT AoT (Long)0246 # of First Steps81012</h1>
<p>-What is the third letter of grama: a The word we are looking fits the clue "A Timorese island" and needs to have the letters w, e, t, and a in its (h1, one) first, (h2, two) second, (h3, three) third, and (h4, four) fourth letters: w e t a <em>.The candidates are: The candidate is "wetar".-wetar (w e t a r, w e t a </em>) fits We add the word wetar for v3.The current words are: h1.sawer h2.uredo h3.rater h4.grama v1.surge v2.arara v3.wetar v4.edema Now, let's consider the last unfound horizontal word, which is h5.Since we are considering the fifth horizontal word, we need to look at the fifth letters of the vertical words we found: -What is the fifth letter of surge: e -What is the fifth letter of arara: a -What is the fifth letter of wetar: r -What is the fifth letter of edema: a The word we are looking fits the clue "Receiving by the ear" and needs to have the letters e, a, r, and a in its (v1, one) first, (v2, two) second, (v3, three) third, and (v4, four) fourth letters: e a r a <em>.The candidates are: The candidate is "earal".-earal (e a r a l, e a r a </em>) fits We add the word earal for h5.Let's consider the next unfound horizontal word, which is h1.Since we are considering the first horizontal word, we need to look at the first letters of the vertical words we found: -What is the first letter of swipe: s -What is the first letter of carex: c The word we are looking fits the clue "Dandruff; scum; the bull-trout" and needs to have the letters s and c in its (v1, one) first and (v2, two) second letters: s c _ _ <em>.The candidates are: scurf.-scurf (s c u r f, s c _ _ </em>) fits We add the word scurf for h1.The current words are: h1.scurf h2.waver h5.exeat v1.swipe v2.carex Now, let's consider the next unfound vertical word, which is v3.Since we are considering the third vertical word, we need to look at the third letters of the horizontal words we found: -What is the third letter of scurf: u -What is the third letter of waver: v -What is the third letter of exeat: e The word we are looking fits the clue "Grape jam" and needs to have the letters u, v, and e in its (h1, one) first, (h2, two) second, and (h5, five) fifth letters: u v _ _ e.The candidates are: The candidate is "uvetta".-uvetta (u v e t t a, u v _ _ e) doesn't fit Let's consider the next unfound vertical word, which is v4.Since we are considering the fourth vertical word, we need to look at the fourth letters of the horizontal words we found: -What is the fourth letter of scurf: r -What is the fourth letter of waver: e -What is the fourth letter of exeat: a The word we are looking fits the clue "A flatworm larva" and needs to have the letters r, e, and a in its (h1, one) first, (h2, two) second, and (h5, five) fifth letters: r e _ _ a.The candidates are: redia.-redia (r e d i a, r e _ _ a) fits We add the word redia for v4.The current words are: h1.scurf h2.waver h5.exeat v1.swipe v2.carex v4.redia Now, let's consider the next unfound horizontal word, which is h3.Since we are considering the third horizontal word, we need to look at the third letters of the vertical words we found: -What is the third letter of swipe: i -What is the third letter of carex: r -What is the third letter of redia: d The word we are looking fits the clue "A Turkish written decree" and needs to have the letters i, r, and d in its (v1, one) first, (v2, two) second, and (v4, four) fourth letters: i r _ d <em>.The candidates are: irade.-irade (i r a d e, i r _ d </em>) fits We add the word irade for h3.The current words are: h1.scurf h2.waver h3.irade h5.exeat v1.swipe v2.carex v4.redia Now, let's consider the last unfound vertical word, which is v5.Since we are considering the fifth vertical word, we need to look at the fifth letters of the horizontal words we found: -What is the fifth letter of scurf: f -What is the fifth letter of waver: r -What is the fifth letter of irade: e -What is the fifth letter of exeat: t The word we are looking fits the clue "Ore refuse; to prepare material for glass by heat" and needs to have the letters f, r, e, and t in its (h1, one) first, (h2, two) second, (h3, three) third, and (h5, five) fifth letters: f r e _ t.The candidates are: The candidate is "frett".-frett (f r e t t, f r e _ t) fits We add the word frett for v5.AoT "Write a coherent passage of 4 short paragraphs.The end sentence of each paragraph must be: {0} Firstly, make five different plans for a coherent passage, then write.Your output should be of the following format:
DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale. R Y Aminabadi, S Rajbhandari, A A Awan, C Li, D Li, E Zheng, O Ruwase, S Smith, M Zhang, J Rasley, SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2022</p>
<p>J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Working memory: looking back and looking forward. A Baddeley, Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kerr, J Mueller, J Ladish, J Landau, K Ndousse, K Lukosuite, L Lovitt, M Sellitto, N Elhage, N Schiefer, N Mercado, N Dassarma, R Lasenby, R Larson, S Ringer, S Johnston, S Kravec, S E Showk, S Fort, T Lanham, T Telleen-Lawton, T Conerly, T Henighan, T Hume, S R Bowman, Z Hatfield-Dodds, B Mann, D Amodei, N Joseph, S Mccandlish, T Brown, J Kaplan, ArXiv:2212.08073Constitutional AI: Harmlessness from AI Feedback. 2003. 20224</p>
<p>Qualitative Mechanical Problem-Solving by Artificial Agents:: Further Progress, Under Psychometric AI. S Banerjee, S Bringsjord, M Giancola, N S Govindarajulu, The International FLAIRS Conference Proceedings. 202235</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 332020</p>
<p>FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. L Chen, M Zaharia, J Zou, arXiv:2305.051762023arXiv preprint</p>
<p>The carbon impact of artificial intelligence. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374arXiv:2204.02311Evaluating large language models trained on code. 2021. 2022. 20202arXiv preprintPalm: Scaling language modeling with pathways</p>
<p>When is psychology research useful in artificial intelligence? A case for reducing computational complexity in problem solving. A Drozdov, N Schärli, E Akyürek, N Scales, X Song, X Chen, O Bousquet, D Zhou, S Helie, Z Pizlo, Topics in Cognitive Science. 1442022. 2022Compositional Semantic Parsing with Large Language Models</p>
<p>The Cambridge handbook of thinking and reasoning. K J Holyoak, R G Morrison, J Huang, K C Chang, -C , arXiv:2212.10403Towards reasoning in large language models: A survey. Cambridge University Press2005. 2022arXiv preprint</p>
<p>Large Language Models are Zero-Shot Reasoners. S Kadavath, T Conerly, A Askell, T Henighan, D Drain, E Perez, N Schiefer, Z Hatfield-Dodds, N Dassarma, E Tran-Johnson, arXiv:2207.05221Kahneman, D. 2011. Thinking, fast and slow. 2022. 202235arXiv preprintLanguage models (mostly) know what they know</p>
<p>T Lanham, A Chen, A Radhakrishnan, B Steiner, C Denison, D Hernandez, D Li, E Durmus, E Hubinger, J Kernion, arXiv:2307.13702Measuring Faithfulness in Chainof-Thought Reasoning. 2023arXiv preprint</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>A comparison of most-to-least and least-to-most prompting on the acquisition of solitary play skills. M E Libby, J S Weiss, S Bancroft, W H Ahearn, Behavior analysis in practice. 12008</p>
<p>Large Language Model Guided Tree. Y Liu, T Han, S Ma, J Zhang, Y Yang, J Tian, H He, A Li, M He, Z Liu, arXiv:2304.01852arXiv:2305.08291Summary of chatgpt/gpt-4 research and perspective towards the future of large language models. Long, J2023. 2023. 2023of-Thought. arXiv preprintFaithful Chain-of-Thought Reasoning. ArXiv:2301.13379 [cs</p>
<p>Augmented language models: a survey. G Mialon, R Dessì, M Lomeli, C Nalmpantis, R Pasunuru, R Raileanu, B Rozière, T Schick, J Dwivedi-Yu, A Celikyilmaz, arXiv:2302.07842Trends in cognitive sciences. 732023. 2003MonsellarXiv preprintTask switching</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Synthetic Prompting: Generating Chainof-Thought Demonstrations for Large Language Models. Sloman, S. A. 1996. The empirical case for two systems of reasoning. J Robinson, D Wingate, Z Shao, Y Gong, Y Shen, M Huang, N Duan, W Chen, Psychological bulletin. 119132022. 2023Leveraging Large Language Models for Multiple Choice Question Answering</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, ArXiv:2210.092612022</p>
<p>R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>M Turpin, J Michael, E Perez, S R Bowman, arXiv:2305.04388Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. 2023arXiv preprint</p>
<p>X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, Self-Consistency Improves Chain of Thought Reasoning in Language Models. 2022</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, ArXiv:2206.07682Emergent Abilities of Large Language Models. 2022a</p>
<p>Chainof-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Sustainable ai: Environmental implications, challenges and opportunities. C.-J Wu, R Raghavendra, U Gupta, B Acun, N Ardalani, K Maeng, G Chang, F Aga, J Huang, C Bai, Proceedings of Machine Learning and Systems. Machine Learning and Systems20224</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, ArXiv:2305.106012023</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Z Zhang, A Zhang, M Li, A Smola, D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, E H Chi, Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. 2022. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-c5d18dbb92d0cd5393baa1e69de33d6922ac3e57</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c5d18dbb92d0cd5393baa1e69de33d6922ac3e57" target="_blank">RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> A novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning and shows RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.</p>
                <p><strong>Paper Abstract:</strong> We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2558.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2558.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-robot collaboration framework that delegates one LLM-powered agent per robot to conduct a dialog-style coordination process which outputs per-robot sub-task plans and optional task-space waypoint paths; validated plans are converted to joint-space goals and solved by a centralized multi-arm motion planner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RoCo</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RoCo assigns each physical robot an LLM-based agent that engages in a constrained natural-language dialog to (1) reason about task decomposition and allocate sub-tasks, (2) optionally generate task-space waypoint paths, and (3) iteratively refine proposals using explicit environment feedback (parsing, task-constraint checks, IK, collision checking, waypoint validation). Once a sub-task plan passes validations the system uses IK to obtain joint-space goal(s) and a centralized RRT-based multi-arm motion planner to compute collision-free trajectories that are executed open-loop. The architecture is hybrid: decentralized language-mediated coordination + centralized trajectory planning/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (1 agent per robot; experiments used 2–3 robot agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Each agent represents a physical robot with specific embodiment and capability constraints (e.g., UR5E with Robotiq gripper, Franka Panda, 20-DoF Humanoid). Specializations are concrete: reach range, end-effector type, allowed primitive actions (examples: PICK, PLACE, PICK [object] PATH [path], PLACE [object] [target] PATH [path], SWEEP, DUMP, OPEN). There are no separate 'critic' or 'evaluator' agents — environment validation modules act as external validators; humans can act as a dialog partner in human-in-the-loop runs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>High-level task planning (decomposition and allocation), plan generation (action + optional task-space waypoints), motion planning/execution (joint-space trajectory planning and open-loop execution), and evaluation (benchmarking on RoCoBench). Does not cover literature review or automated experiment-design beyond per-episode task strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized dialog-based coordination among LLM agents with an enforced conversational protocol (each agent responds in turn and must either pass turn or output a final sub-task proposal). Coordination is sequential within a round until one agent issues a final plan; rounds happen before each environment step. Execution coordination for motion is centralized (a joint-space RRT planner handles multi-arm trajectory generation once sub-task goals are fixed).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language messages via structured LLM prompts. Each agent's prompt contains: Task Context, Round History, Agent Capability, Communication Instruction, Current Observation, and optional Plan Feedback. Agents must respond in a constrained textual format (e.g., explicit ACTION tokens like 'PICK [object] PATH [(x,y,z),...]' or 'EXECUTE NAME <agent> ACTION ...' and end with either a 'PROCEED' token or the final plan). Waypoints, when present, are represented as sequences of 3D coordinates in the textual plan output.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative environment-feedback loop: once a dialog yields a sub-task plan, the system runs a validation pipeline (1: text parsing, 2: task-constraint checks, 3: IK feasibility, 4: collision checking, 5: waypoint validity). If any check fails, a structured feedback message (reasons for failure) is appended to agents' prompts and a new dialog/re-plan attempt is made; agents may re-plan until a maximum number of attempts per round is reached. This is an iterative refinement loop driven by environment validators rather than a learned critic.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Dialog occurs before each environment interaction (i.e., at every round/time step). Within a round agents converse in-turn until a final plan or a PROCEED is produced. Re-plan cycles (feedback → appended prompt → new dialog) occur on-demand until a per-round re-plan budget (K) is exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Physical multi-robot manipulation (tabletop robotic collaboration); generalizable zero-shot/few-shot planning for multi-robot tasks (not explicitly applied to other scientific domains in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: per-task success rate (mean ± std) over 20 runs, average number of environment steps in successful runs, average number of re-plan attempts across all runs. Example (Dialog / RoCo results): Pack Grocery success 0.44 ± 0.06 (steps,replans: 9.9, 3.5); Arrange Cabinet 0.75 ± 0.10 (4.7, 2.0); Sweep Floor 0.95 ± 0.05 (7.1, 1.0); Make Sandwich 0.80 ± 0.08 (10.2, 1.7); Sort Cubes 0.93 ± 0.06 (4.9, 1.3); Move Rope 0.65 ± 0.11 (2.5, 3.1). Toy 3D-grid LLM-path-planning: GPT-4 achieved 86.7% success over 30 runs with average 2.73 attempts (reported in section 3.3 / Sec.13).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to an oracle LLM-planner 'Central Plan' (one LLM given full environment state and asked to plan for all robots centrally) and two ablations (Dialog w/o History and Dialog w/o Feedback). Example Central Plan success rates: Pack Grocery 0.82 ± 0.06; Arrange 0.90 ± 0.07; Sweep 1.00 ± 0.00; Make Sandwich 0.96 ± 0.04; Sort Cubes 0.70 ± 0.10; Move Rope 0.50 ± 0.11. The dialog (decentralized) method sometimes matches or outperforms the oracle (e.g., Sort Cubes: Dialog 0.93 vs Central 0.70), while the oracle is stronger on waypoint planning in Pack Grocery.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Dialog enables: interpretable and flexible task decomposition (human-readable dialog), robust zero-shot/few-shot adaptation to task variations (object initialization, goal variations, agent capability differences), and improved handling of asymmetric observation/capability setups; LLM-proposed task-space waypoints accelerate multi-arm motion planning for placing sub-tasks (reported qualitatively and empirically — LLM waypoints significantly reduced planning time for placing compared to linear or hard-coded top-down waypoints). Quantitative examples: dialog success on Make Sandwich 0.80 vs ablated versions 0.33 (w/o History) and 0.35 (w/o Feedback), showing large gains from full dialog+feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Main limitations include: reliance on accurate perception (oracle state in sim; real-world detection errors caused failures), open-loop trajectory execution (no closed-loop correction), LLM factual errors that can propagate through dialog (agents can converge on incorrect conclusions), LLM query latency and cost (one LLM call per agent per dialog message), and degraded LLM path-planning performance as environment complexity increases (toy 3D-grid success declined with grid size/obstacles). Example failure mode: multiple agents erroneously agreeing the task is complete and entering WAIT, causing deadlock.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Two ablations were evaluated: 'Dialog w/o History' (removes past dialog/action history) and 'Dialog w/o Feedback' (removes environment feedback and failed plans are discarded). Results show ablations degrade performance on multiple tasks: e.g., Make Sandwich success: Dialog 0.80 ± 0.08 vs Dialog w/o History 0.33 ± 0.12 vs Dialog w/o Feedback 0.35 ± 0.11. Sweep Floor remained high across some ablations, but overall best results used full prompts (history+feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper observations: (1) Including both round history and environment feedback in prompts yields best overall performance across tasks. (2) For the toy LLM 3D path-planning experiment, a moderate GPT-4 temperature (~0.6) produced better success rates (more 1-shot successes) than fully deterministic or high-temperature settings. (3) Centralized joint-space motion planning combined with validated LLM sub-task goals is the recommended hybrid configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoCo: Dialectic Multi-Robot Collaboration with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2558.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2558.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Dialog Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-powered multi-agent dialog agents (per-robot LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained LLM (GPT-4) used as the per-robot reasoning and communication engine; each robot's agent receives a structured prompt (task context, history, capability, observation, communication instruction, plan feedback) and outputs constrained natural-language proposals (sub-tasks and optional waypoint sequences) that are parsed and validated by the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 Dialog Agents (as used in RoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each physical robot is represented by a GPT-4 instance (queried per message) that: (1) ingests agent-specific structured prompt components, (2) participates in a round-robin dialog governed by a communication protocol (must either pass turn or output final plan after at least one exchange), (3) emits sub-task plans and optionally task-space waypoint paths in text form which are then parsed into actions and IK goals, and (4) iteratively refines plans upon receiving environment feedback appended to subsequent prompts. The GPT-4 agents are stateless beyond the provided round history and feedback buffers (history is appended into prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (one GPT-4 agent per robot; experimental setups used 2 or 3 agents; toy path-planning used 3–4 agent paths planned simultaneously by the model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Specialization is grounded in the prompt: each GPT-4 agent is told its identity and capabilities (e.g., reachability, end-effector, allowed primitives). Thus specialization is role-based (e.g., 'Alice: UR5E dustpan-holder capable of DUMP and hold dustpan', 'Bob: Franka broom-holder able to SWEEP'), enabling capability-aware allocation. There are no separate modules for idea-generation vs evaluation — planning and negotiation are both handled by the same LLM agent, while environment validators handle feasibility checks.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Idea/strategy generation (task decomposition and coordination), plan proposal (action + waypoint generation), iterative refinement via environment feedback; not used for literature review or meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Peer-to-peer dialog among GPT-4 agents following an enforced turn-taking and end-condition protocol. The dialog is orchestrated by the experiment controller (pseudocode provided): the controller loops rounds, calls PromptDialogs(H, F, o_t, u^n) to collect agent messages, then validates/executes the final plan. Thus coordination is decentralized linguistically but orchestrated centrally by the controller which enforces validation and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Structured natural language via LLM prompts. Each message follows a strict output format instruction (e.g., 'EXECUTE NAME <agent> ACTION ...' or 'PICK [object] PATH [list of 3D waypoints]'). Prompts include explicit 'Communication Instruction' telling the agent how to format outputs and when to indicate 'PROCEED' vs final plan. Feedback messages from validators are appended verbatim in natural language and may include structured failure reasons (e.g., 'IK failure at waypoint 3', 'collision detected between joint configs X and Y').</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Environment-driven iterative feedback: failed plan validation returns explicit reasons appended to the agent's prompt; agents use that feedback in-context to re-generate corrected plans. The system allows up to K re-plan attempts per round; failure reasons include parsing errors, task-constraint violation, IK infeasibility, collision, and waypoint invalidity.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Agents are queried each dialog turn; dialog is executed once per environment timestep/round and on-demand for re-planning (feedback-triggered). In practice: one round of dialog before each environment action, plus potentially multiple re-plan iterations within that round until success or budget exhaustion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-robot manipulation coordination and multi-agent path planning (toy 3D-grid example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used as RoCo agents (GPT-4): per-task success rates and re-plan/step statistics given in RoCoBench (see RoCo entry). Toy 3D-grid multi-agent path planning using GPT-4: 86.7% success (30 runs) with average 2.73 attempts; success rate depended on temperature with best results around 0.6.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4 dialog agents (decentralized) compared against an oracle centralized LLM planner (a single LLM planning for all robots with full state). Results: decentralized GPT-4 dialog sometimes matched or exceeded the centralized oracle on some tasks (e.g., Sort Cubes: Dialog 0.93 vs Central Plan 0.70), while the oracle had advantages on integrated waypoint planning in Pack Grocery (Central Plan 0.82 vs Dialog 0.44). Ablations removing history or feedback reduced GPT-4 dialog performance substantially on tasks like Make Sandwich.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Using GPT-4 per-robot agents provided: (1) capability-aware decentralized allocation (agents reason about what they can reach/do), (2) interpretable textual dialog (easy human-in-the-loop integration), (3) iterative refinement with explicit environment feedback enabling recovery from invalid proposals, and (4) an ability to propose semantically meaningful task-space waypoints that reduced motion-planner sampling complexity for placing tasks. Quantitatively: enabling history+feedback improves Make Sandwich success from ~0.33 (ablations) to 0.80 (full dialog).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>High LLM query cost and latency (one call per agent per message), susceptibility to LLM factual mistakes which can cascade through dialog, dependence on accurate perception (real-world object-detection errors reduced overall performance), reduced performance for path-planning as environment complexity increases, and open-loop trajectory execution exposing the system to execution noise without closed-loop correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations showed that removing dialog history ('Dialog w/o History') or removing environment feedback ('Dialog w/o Feedback') decreases performance on multiple tasks. Example: Make Sandwich success 0.80 (Dialog) vs 0.33 (w/o History) vs 0.35 (w/o Feedback). Sweep Floor was less sensitive in some ablations, but overall full prompting (history + feedback) performed best.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper recommends: include round history and explicit environment feedback in agent prompts for best task performance; for LLM path-planning experiments, use a moderate temperature (~0.6) to balance determinism and variability. Hybrid configuration (decentralized LLM dialog + centralized joint-space motion planner) is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoCo: Dialectic Multi-Robot Collaboration with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>Ai safety via debate <em>(Rating: 2)</em></li>
                <li>Dera: Enhancing large language model completions with dialog-enabled resolving agents <em>(Rating: 1)</em></li>
                <li>Encouraging divergent thinking in large language models through multi-agent debate <em>(Rating: 1)</em></li>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2558",
    "paper_id": "paper-c5d18dbb92d0cd5393baa1e69de33d6922ac3e57",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "RoCo",
            "name_full": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
            "brief_description": "A multi-robot collaboration framework that delegates one LLM-powered agent per robot to conduct a dialog-style coordination process which outputs per-robot sub-task plans and optional task-space waypoint paths; validated plans are converted to joint-space goals and solved by a centralized multi-arm motion planner.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RoCo",
            "system_description": "RoCo assigns each physical robot an LLM-based agent that engages in a constrained natural-language dialog to (1) reason about task decomposition and allocate sub-tasks, (2) optionally generate task-space waypoint paths, and (3) iteratively refine proposals using explicit environment feedback (parsing, task-constraint checks, IK, collision checking, waypoint validation). Once a sub-task plan passes validations the system uses IK to obtain joint-space goal(s) and a centralized RRT-based multi-arm motion planner to compute collision-free trajectories that are executed open-loop. The architecture is hybrid: decentralized language-mediated coordination + centralized trajectory planning/execution.",
            "number_of_agents": "variable (1 agent per robot; experiments used 2–3 robot agents)",
            "agent_specializations": "Each agent represents a physical robot with specific embodiment and capability constraints (e.g., UR5E with Robotiq gripper, Franka Panda, 20-DoF Humanoid). Specializations are concrete: reach range, end-effector type, allowed primitive actions (examples: PICK, PLACE, PICK [object] PATH [path], PLACE [object] [target] PATH [path], SWEEP, DUMP, OPEN). There are no separate 'critic' or 'evaluator' agents — environment validation modules act as external validators; humans can act as a dialog partner in human-in-the-loop runs.",
            "research_phases_covered": "High-level task planning (decomposition and allocation), plan generation (action + optional task-space waypoints), motion planning/execution (joint-space trajectory planning and open-loop execution), and evaluation (benchmarking on RoCoBench). Does not cover literature review or automated experiment-design beyond per-episode task strategy.",
            "coordination_mechanism": "Decentralized dialog-based coordination among LLM agents with an enforced conversational protocol (each agent responds in turn and must either pass turn or output a final sub-task proposal). Coordination is sequential within a round until one agent issues a final plan; rounds happen before each environment step. Execution coordination for motion is centralized (a joint-space RRT planner handles multi-arm trajectory generation once sub-task goals are fixed).",
            "communication_protocol": "Natural-language messages via structured LLM prompts. Each agent's prompt contains: Task Context, Round History, Agent Capability, Communication Instruction, Current Observation, and optional Plan Feedback. Agents must respond in a constrained textual format (e.g., explicit ACTION tokens like 'PICK [object] PATH [(x,y,z),...]' or 'EXECUTE NAME &lt;agent&gt; ACTION ...' and end with either a 'PROCEED' token or the final plan). Waypoints, when present, are represented as sequences of 3D coordinates in the textual plan output.",
            "feedback_mechanism": "Iterative environment-feedback loop: once a dialog yields a sub-task plan, the system runs a validation pipeline (1: text parsing, 2: task-constraint checks, 3: IK feasibility, 4: collision checking, 5: waypoint validity). If any check fails, a structured feedback message (reasons for failure) is appended to agents' prompts and a new dialog/re-plan attempt is made; agents may re-plan until a maximum number of attempts per round is reached. This is an iterative refinement loop driven by environment validators rather than a learned critic.",
            "communication_frequency": "Dialog occurs before each environment interaction (i.e., at every round/time step). Within a round agents converse in-turn until a final plan or a PROCEED is produced. Re-plan cycles (feedback → appended prompt → new dialog) occur on-demand until a per-round re-plan budget (K) is exhausted.",
            "task_domain": "Physical multi-robot manipulation (tabletop robotic collaboration); generalizable zero-shot/few-shot planning for multi-robot tasks (not explicitly applied to other scientific domains in the paper).",
            "performance_metrics": "Reported metrics: per-task success rate (mean ± std) over 20 runs, average number of environment steps in successful runs, average number of re-plan attempts across all runs. Example (Dialog / RoCo results): Pack Grocery success 0.44 ± 0.06 (steps,replans: 9.9, 3.5); Arrange Cabinet 0.75 ± 0.10 (4.7, 2.0); Sweep Floor 0.95 ± 0.05 (7.1, 1.0); Make Sandwich 0.80 ± 0.08 (10.2, 1.7); Sort Cubes 0.93 ± 0.06 (4.9, 1.3); Move Rope 0.65 ± 0.11 (2.5, 3.1). Toy 3D-grid LLM-path-planning: GPT-4 achieved 86.7% success over 30 runs with average 2.73 attempts (reported in section 3.3 / Sec.13).",
            "baseline_comparison": "Compared to an oracle LLM-planner 'Central Plan' (one LLM given full environment state and asked to plan for all robots centrally) and two ablations (Dialog w/o History and Dialog w/o Feedback). Example Central Plan success rates: Pack Grocery 0.82 ± 0.06; Arrange 0.90 ± 0.07; Sweep 1.00 ± 0.00; Make Sandwich 0.96 ± 0.04; Sort Cubes 0.70 ± 0.10; Move Rope 0.50 ± 0.11. The dialog (decentralized) method sometimes matches or outperforms the oracle (e.g., Sort Cubes: Dialog 0.93 vs Central 0.70), while the oracle is stronger on waypoint planning in Pack Grocery.",
            "coordination_benefits": "Dialog enables: interpretable and flexible task decomposition (human-readable dialog), robust zero-shot/few-shot adaptation to task variations (object initialization, goal variations, agent capability differences), and improved handling of asymmetric observation/capability setups; LLM-proposed task-space waypoints accelerate multi-arm motion planning for placing sub-tasks (reported qualitatively and empirically — LLM waypoints significantly reduced planning time for placing compared to linear or hard-coded top-down waypoints). Quantitative examples: dialog success on Make Sandwich 0.80 vs ablated versions 0.33 (w/o History) and 0.35 (w/o Feedback), showing large gains from full dialog+feedback.",
            "coordination_challenges": "Main limitations include: reliance on accurate perception (oracle state in sim; real-world detection errors caused failures), open-loop trajectory execution (no closed-loop correction), LLM factual errors that can propagate through dialog (agents can converge on incorrect conclusions), LLM query latency and cost (one LLM call per agent per dialog message), and degraded LLM path-planning performance as environment complexity increases (toy 3D-grid success declined with grid size/obstacles). Example failure mode: multiple agents erroneously agreeing the task is complete and entering WAIT, causing deadlock.",
            "ablation_studies": "Two ablations were evaluated: 'Dialog w/o History' (removes past dialog/action history) and 'Dialog w/o Feedback' (removes environment feedback and failed plans are discarded). Results show ablations degrade performance on multiple tasks: e.g., Make Sandwich success: Dialog 0.80 ± 0.08 vs Dialog w/o History 0.33 ± 0.12 vs Dialog w/o Feedback 0.35 ± 0.11. Sweep Floor remained high across some ablations, but overall best results used full prompts (history+feedback).",
            "optimal_configurations": "Paper observations: (1) Including both round history and environment feedback in prompts yields best overall performance across tasks. (2) For the toy LLM 3D path-planning experiment, a moderate GPT-4 temperature (~0.6) produced better success rates (more 1-shot successes) than fully deterministic or high-temperature settings. (3) Centralized joint-space motion planning combined with validated LLM sub-task goals is the recommended hybrid configuration.",
            "uuid": "e2558.0",
            "source_info": {
                "paper_title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-4 Dialog Agents",
            "name_full": "GPT-4-powered multi-agent dialog agents (per-robot LLM agents)",
            "brief_description": "Pre-trained LLM (GPT-4) used as the per-robot reasoning and communication engine; each robot's agent receives a structured prompt (task context, history, capability, observation, communication instruction, plan feedback) and outputs constrained natural-language proposals (sub-tasks and optional waypoint sequences) that are parsed and validated by the environment.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "system_name": "GPT-4 Dialog Agents (as used in RoCo)",
            "system_description": "Each physical robot is represented by a GPT-4 instance (queried per message) that: (1) ingests agent-specific structured prompt components, (2) participates in a round-robin dialog governed by a communication protocol (must either pass turn or output final plan after at least one exchange), (3) emits sub-task plans and optionally task-space waypoint paths in text form which are then parsed into actions and IK goals, and (4) iteratively refines plans upon receiving environment feedback appended to subsequent prompts. The GPT-4 agents are stateless beyond the provided round history and feedback buffers (history is appended into prompts).",
            "number_of_agents": "variable (one GPT-4 agent per robot; experimental setups used 2 or 3 agents; toy path-planning used 3–4 agent paths planned simultaneously by the model)",
            "agent_specializations": "Specialization is grounded in the prompt: each GPT-4 agent is told its identity and capabilities (e.g., reachability, end-effector, allowed primitives). Thus specialization is role-based (e.g., 'Alice: UR5E dustpan-holder capable of DUMP and hold dustpan', 'Bob: Franka broom-holder able to SWEEP'), enabling capability-aware allocation. There are no separate modules for idea-generation vs evaluation — planning and negotiation are both handled by the same LLM agent, while environment validators handle feasibility checks.",
            "research_phases_covered": "Idea/strategy generation (task decomposition and coordination), plan proposal (action + waypoint generation), iterative refinement via environment feedback; not used for literature review or meta-analysis.",
            "coordination_mechanism": "Peer-to-peer dialog among GPT-4 agents following an enforced turn-taking and end-condition protocol. The dialog is orchestrated by the experiment controller (pseudocode provided): the controller loops rounds, calls PromptDialogs(H, F, o_t, u^n) to collect agent messages, then validates/executes the final plan. Thus coordination is decentralized linguistically but orchestrated centrally by the controller which enforces validation and execution.",
            "communication_protocol": "Structured natural language via LLM prompts. Each message follows a strict output format instruction (e.g., 'EXECUTE NAME &lt;agent&gt; ACTION ...' or 'PICK [object] PATH [list of 3D waypoints]'). Prompts include explicit 'Communication Instruction' telling the agent how to format outputs and when to indicate 'PROCEED' vs final plan. Feedback messages from validators are appended verbatim in natural language and may include structured failure reasons (e.g., 'IK failure at waypoint 3', 'collision detected between joint configs X and Y').",
            "feedback_mechanism": "Environment-driven iterative feedback: failed plan validation returns explicit reasons appended to the agent's prompt; agents use that feedback in-context to re-generate corrected plans. The system allows up to K re-plan attempts per round; failure reasons include parsing errors, task-constraint violation, IK infeasibility, collision, and waypoint invalidity.",
            "communication_frequency": "Agents are queried each dialog turn; dialog is executed once per environment timestep/round and on-demand for re-planning (feedback-triggered). In practice: one round of dialog before each environment action, plus potentially multiple re-plan iterations within that round until success or budget exhaustion.",
            "task_domain": "Multi-robot manipulation coordination and multi-agent path planning (toy 3D-grid example).",
            "performance_metrics": "When used as RoCo agents (GPT-4): per-task success rates and re-plan/step statistics given in RoCoBench (see RoCo entry). Toy 3D-grid multi-agent path planning using GPT-4: 86.7% success (30 runs) with average 2.73 attempts; success rate depended on temperature with best results around 0.6.",
            "baseline_comparison": "GPT-4 dialog agents (decentralized) compared against an oracle centralized LLM planner (a single LLM planning for all robots with full state). Results: decentralized GPT-4 dialog sometimes matched or exceeded the centralized oracle on some tasks (e.g., Sort Cubes: Dialog 0.93 vs Central Plan 0.70), while the oracle had advantages on integrated waypoint planning in Pack Grocery (Central Plan 0.82 vs Dialog 0.44). Ablations removing history or feedback reduced GPT-4 dialog performance substantially on tasks like Make Sandwich.",
            "coordination_benefits": "Using GPT-4 per-robot agents provided: (1) capability-aware decentralized allocation (agents reason about what they can reach/do), (2) interpretable textual dialog (easy human-in-the-loop integration), (3) iterative refinement with explicit environment feedback enabling recovery from invalid proposals, and (4) an ability to propose semantically meaningful task-space waypoints that reduced motion-planner sampling complexity for placing tasks. Quantitatively: enabling history+feedback improves Make Sandwich success from ~0.33 (ablations) to 0.80 (full dialog).",
            "coordination_challenges": "High LLM query cost and latency (one call per agent per message), susceptibility to LLM factual mistakes which can cascade through dialog, dependence on accurate perception (real-world object-detection errors reduced overall performance), reduced performance for path-planning as environment complexity increases, and open-loop trajectory execution exposing the system to execution noise without closed-loop correction.",
            "ablation_studies": "Ablations showed that removing dialog history ('Dialog w/o History') or removing environment feedback ('Dialog w/o Feedback') decreases performance on multiple tasks. Example: Make Sandwich success 0.80 (Dialog) vs 0.33 (w/o History) vs 0.35 (w/o Feedback). Sweep Floor was less sensitive in some ablations, but overall full prompting (history + feedback) performed best.",
            "optimal_configurations": "Paper recommends: include round history and explicit environment feedback in agent prompts for best task performance; for LLM path-planning experiments, use a moderate temperature (~0.6) to balance determinism and variability. Hybrid configuration (decentralized LLM dialog + centralized joint-space motion planner) is recommended.",
            "uuid": "e2558.1",
            "source_info": {
                "paper_title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 2
        },
        {
            "paper_title": "Ai safety via debate",
            "rating": 2
        },
        {
            "paper_title": "Dera: Enhancing large language model completions with dialog-enabled resolving agents",
            "rating": 1
        },
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "rating": 1
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 2
        }
    ],
    "cost": 0.01570725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</h1>
<p>Zhao Mandi<br>Columbia University</p>
<p>Shreeya Jain<br>Columbia University</p>
<p>Shuran Song<br>Columbia University</p>
<h4>Abstract</h4>
<p>We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach - it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility - in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website project-roco.github.io for videos and code.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We propose RoCo, a unified approach for multi-robot collaboration that leverages LLMs for both high-level task coordination and low-level motion planning. We demonstrate its utility on RoCoBench, a benchmark we introduce that includes a diverse set of challenges in collaboration task scenarios.</p>
<h2>1 Introduction</h2>
<p>Multi-robot systems are intriguing for their promise of enhancing task productivity, but are faced with various challenges. For robots to effectively split and allocate the work, it requires high-level understanding of a task, and consideration of each robot's capabilities such as reach range or payload. Another challenge lies in low-level motion planning: as the configuration space grows with the number of robots, finding collision-free motion plans becomes exponentially difficult. Finally, traditional multi-robot systems typically require task-specific engineering, hence compromise generalization: with much of the task structures pre-defined, these systems are incapable of adapting to new scenarios or variations in a task.</p>
<p>We propose RoCo, a zero-shot multi-robot collaboration method to address the above challenges. Our approach includes three key components:</p>
<ul>
<li>Dialogue-style task-coordination: To facilitate information exchange and task reasoning, we let robots 'talk' among themselves by delegating each robot to an LLM agent in a dialog, which allows robots to discuss the task in natural language, with high interpretability for supervision.</li>
<li>Feedback-improved Sub-task Plan Generated by LLMs: The multi-agent dialog ends with a sub-task plan for each agent (e.g. pick up object). We provide a set of environment validations and feedback (e.g. IK failures or collision) to the LLM agents until a valid plan is proposed.</li>
<li>LLM-informed Motion-Planning in Joint Space: From the validated sub-task plan, we extract goal configurations in the robots' joint space, and use a centralized RRT-sampler to plan motion trajectories. We explore a less-studied capability of LLM: 3D spatial reasoning. Given the start, goal, and obstacle locations in task space, we show LLMs can generate waypoint paths that incorporate high-level task semantics and environmental constraints, and significantly reduce the motion planner's sample complexity.
We next introduce RoCoBench, a benchmark with 6 multi-robot manipulation tasks. We experimentally demonstrate the effectiveness of RoCo on the benchmark tasks: by leveraging the commonsense knowledge captured by large language models (LLMs), RoCo is flexible in handling a variety of collaboration scenarios without any task-specific training.
In summary, we propose a novel approach to multi-robot collaboration, supported by two technical contributions: 1) An LLM-based multi-robot framework (RoCo) that is flexible in handling a large variety of tasks with improved task-level coordination and action-level motion planning; 2) A new benchmark (RoCoBench) for multi-robot manipulation to systematically evaluate these capabilities. It includes a suite of tasks that are designed to examine the flexibility and generality of the algorithm in handling different task semantics (e.g., sequential or concurrent), different levels of workspace overlaps, and varying agent capabilities (e.g., reach range and end-effector types) and embodiments (e.g., 6DoF UR5, 7DoF Franka, 20DoF Humanoid).</li>
</ul>
<h1>2 Preliminaries</h1>
<p>Task Assumption. We consider a cooperative multi-agent task environment with $N$ robots, a finite time horizon $T$, full observation space $O$. Each agent $n$ has observation space $\Omega^{n} \subset O$. Agents may have asymmetric observation spaces and capabilities, which stresses the need for communication. We manually define description functions $f$ that translate task semantics and observations at a timestep $t$ into natural language prompts: $l_{t}^{n}=f^{n}\left(o_{t}\right), o_{t} \in \Omega^{n}$. We also define parsing functions that map LLM outputs (e.g. text string "PICK object") to the corresponding sub-task, which can be described by one or more gripper goal configurations.
Multi-arm Path Planning. Given a sub-task, we use centralized motion planning that finds trajectories for all robots to reach their goal configurations. Let $\mathcal{X} \in \mathbb{R}^{d}$ denote the joint configuration space of all $N$ robot arms and $\mathcal{X}<em _free="{free" _text="\text">{\text {ob }}$ be the obstacle region in the configuration space, then collisionfree space $\mathcal{X}</em>}}=\mathcal{X} \backslash \mathcal{X<em _init="{init" _text="\text">{\text {ob }}$. Given an initial condition $x</em>}} \in \mathcal{X<em _goal="{goal" _text="\text">{\text {free }}$, a goal region $\mathcal{X}</em>}} \in \mathcal{X<em _init="{init" _text="\text">{\text {free }}$, the motion planner finds an optimal $\sigma^{<em>}:[0,1] \rightarrow \mathcal{X}$ that satisfies: $\sigma^{</em>}(0)=x</em>, \sigma^{}<em>}(1) \in x_{\text {goal }}$. This resulting $\sigma^{</em>}$ is used by the robots' arm joint controllers to execute in open-loop.</p>
<h2>3 Multi-Robot Collaboration with LLMs</h2>
<p>We present RoCo, a novel method for multi-robot collaboration that leverages LLMs for robot communication and motion planning. The three key components in our method are demonstrated in Fig. 2 and described below:</p>
<h3>3.1 Multi-Agent Dialog via LLMs</h3>
<p>We assume multi-agent task environments with asymmetric observation space and skill capabilities, which means agents can't coordinate meaningfully without first communicating with each other. We leverage pre-trained LLMs to facilitate this communication. Specifically, before each environment interaction, we set up one round of dialog where each robot is delegated an LLM-generated agent,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RoCo consists of three main components: 1) Multi-agent dialog via LLMs: each robot is equipped with an LLM that 'talks' on its behalf, enabling a discussion of task strategy. 2) LLM-Generated Sub-task Plan: the dialog ends with a proposal of sub-task plan, including optionally a path of task space waypoints, and environment feedback on invalid plans are provided for the agents to improve. 3) Multi-arm motion planning: A validated sub-task plan then produces goal configurations for the robot arms, which are used by a centralized multi-arm motion planner that outputs trajectories for each robot.
which receives information that is unique to this robot and must respond strictly according to its role (e.g. "I am Alice, I can reach ...").</p>
<p>For each agent's LLM prompt, we use a shared overall structure but with agent-specific content, varied with each robot's individual status. The prompt is composed of the following key components:</p>
<ol>
<li>Task Context: describes overall objective of the task.</li>
<li>Round History: past dialog and executed actions from previous rounds.</li>
<li>Agent Capability: the agent's available skills and constraints.</li>
<li>Communication Instruction: how to respond to other agents and properly format outputs.</li>
<li>Current Observation: unique to each agent's status, plus previous responses in the current dialog.</li>
<li>Plan Feedback: (optional) reasons why a previous sub-task plan failed.</li>
</ol>
<p>We use the following communication protocol to move the dialog forward: each agent is asked (instructions are given in Communication Instruction part of the prompt) to end their response by deciding between two options: 1) indicate others to proceed the discussion; 2) summarize everyone's actions and make a final action proposal; this is allowed only if each agent has responded at least once in the current dialog round. This protocol is designed to allow the agents to freely discuss, while guaranteeing one sub-task plan will be proposed within a finite number of exchanges.</p>
<h1>3.2 LLM-Generated Sub-task Plan</h1>
<p>Once a round of dialog ends, the last speaking agent summarizes a 'subtask plan', where each agent gets one sub-task (e.g. pick up object) and optionally a path of 3D waypoints in the task space. This sub-task plan is first passed through a set of validations before going into execution. If any of the checks fail, the feedback is appended to each agent's prompt and another round of dialog begins. The validations are conducted in the following order, and each one assumes the previous check has passed (e.g. a plan must be parsed before checking for task and agent constraints):</p>
<ol>
<li>Text Parsing ensures the plan follows desired format and contains all required keywords</li>
<li>Task Constraints checks whether each action complies with the task and agent constraints</li>
<li>IK checks whether each robot arm's target pose is feasible via inverse kinematics</li>
<li>Collision Checking checks if the IK-solved arm join configurations cause collision</li>
<li>Valid Waypoints optionally, if a task requires path planning, each intermediate waypoint must be IK-solvable and collision-free, and all steps should be evenly spaced
The agents are allowed to re-plan until reaching a maximum number of attempts, after which the current round ends without any execution and the next round begins. The episode is considered failed if the task is not completed within a finite number of rounds.</li>
</ol>
<h1>3.3 LLM-informed Motion Planning in Joint Space</h1>
<p>Once a sub-task plan passes all validations, we combine it with IK to produce a goal configuration jointly over all robot arms, and optionally, each step of the task space waypoint paths produces an intermediate goal configuration. The goal configuration(s) are passed to an RRT-based multi-arm motion planner that jointly plans across all robot arms, and outputs motion trajectories for each robot to execute in the environment, then the task moves on to the next round.</p>
<p>Toy Example: 3D Spatial Reasoning Capability in LLMs. As a motivating example, we ask an LLM (GPT-4) to plan multi-agent paths in a 3D grid. We randomly sample 3 agents’ (start, goal) coordinates and a set of obstacles, and prompt GPT-4 to plan collision-free paths. Given feedback on failed plans, allowing up to 5 attempts, GPT-4 obtains $86.7 \%$ success rate over 30 runs, using on average 2.73 attempts. See Fig. 3 for an example output. See Appendix 13 for further details and results. While this capability is encouraging, we found it to also be limited as the grid size and number of obstacles increase.</p>
<h2>4 Benchmark</h2>
<p>RoCoBench is a suite of 6 multi-robot collaboration tasks in a tabletop manipulation setting. The tasks involve common-sense objects that are semantically easy to understand for LLMs, and span a repertoire of collaboration scenarios that require different robot communication and coordination behaviors. See Appendix 10 for detailed documentation on the benchmark. We remark three key properties that define each task, summarized in Table 1:</p>
<ol>
<li>Task decomposition: whether a task can be decomposed into sub-parts that can be completed in parallel or in certain order. Three tasks in RoCoBench have a sequential nature (e.g. Make Sandwich task requires food items to be stacked in correct order), while the other three tasks can be executed in parallel (e.g. objects in Pack Grocery task can be put into bin in any order).</li>
<li>Observation space: how much of the task and environment information each robot agent receives. Three tasks provide shared observation of the task workspace, while the other three have a more asymmetric setup and robots must inquire each other to exchange knowledge.</li>
<li>Workspace overlap: proximity between operating robots; we rank each task from low, medium or high, where higher overlap calls for more careful low-level coordination (e.g. Move Rope task requires manipulating the same object together).</li>
</ol>
<h2>5 Experiments</h2>
<p>Overview. We design a series of experiments using RoCoBench to validate our approach. In Section 5.1, we evaluate the task performance of RoCo compared to an oracle LLM-planner that does not use dialog, and ablate on different components of the dialog prompting in RoCo. Section 5.2 shows empirically the benefits of LLM-proposed 3D waypoints in multi-arm motion planning. Section 5.3 contains qualitative results that demonstrate the flexibility and adaptability of RoCo. Additional experiment results, such as failure analysis, are provided in Appendix 12.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sweep <br> Floor</th>
<th style="text-align: center;">Pack <br> Grocery</th>
<th style="text-align: center;">Move <br> Rope</th>
<th style="text-align: center;">Arrange <br> Cabinet</th>
<th style="text-align: center;">Make <br> Sandwich</th>
<th style="text-align: center;">Sort <br> Cubes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">Parallel</td>
<td style="text-align: center;">Parallel</td>
<td style="text-align: center;">Parallel</td>
<td style="text-align: center;">Seq</td>
<td style="text-align: center;">Seq</td>
<td style="text-align: center;">Seq</td>
</tr>
<tr>
<td style="text-align: center;">Observation</td>
<td style="text-align: center;">Asym.</td>
<td style="text-align: center;">Shared</td>
<td style="text-align: center;">Shared</td>
<td style="text-align: center;">Asym.</td>
<td style="text-align: center;">Asym.</td>
<td style="text-align: center;">Shared</td>
</tr>
<tr>
<td style="text-align: center;">Workspace</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Low</td>
</tr>
</tbody>
</table>
<p>Table 1: Overview of the key properties designed in RoCoBench tasks. Task Decomposition (1st row): whether sub-tasks can be completed in parallel or sequentially; Observation space (2nd row): whether all agents receive the same information of task status; Workspace Overlap (3rd row): proximity between robots during execution.</p>
<h3>5.1 Main Results on RoCoBench</h3>
<p>Experiment Setup. We use GPT-4 [1] for all our main results. In addition to our method 'Dialog', we set up an oracle LLM-planner 'Central Plan', which is given full environment observation, information on all robots' capabilities and the same plan feedback, and prompts an LLM to plan actions for all robots at once. We also evaluate two ablations on the prompt components of RoCo: first removes dialog and action history from past rounds, i.e. 'Dialog w/o History'. Second removes environment feedback, i.e. 'Dialog w/o Feedback', where a failed action plan is discarded and agents</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pack <br> Grocery</th>
<th style="text-align: center;">Arrange <br> Cabinet</th>
<th style="text-align: center;">Sweep <br> Floor</th>
<th style="text-align: center;">Make <br> Sandwich</th>
<th style="text-align: center;">Sort <br> Cubes</th>
<th style="text-align: center;">Move <br> Rope</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Central Plan <br> (oracle)</td>
<td style="text-align: center;">Success <br> step, replan</td>
<td style="text-align: center;">$0.82 \pm 0.06$</td>
<td style="text-align: center;">$0.90 \pm 0.07$</td>
<td style="text-align: center;">$1.00 \pm 0.00$</td>
<td style="text-align: center;">$0.96 \pm 0.04$</td>
<td style="text-align: center;">$0.70 \pm 0.10$</td>
<td style="text-align: center;">$0.50 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$11.1,3.9$</td>
<td style="text-align: center;">$4.0,2.7$</td>
<td style="text-align: center;">$8.4,2.0$</td>
<td style="text-align: center;">$8.8,1.2$</td>
<td style="text-align: center;">$8.6,2.6$</td>
<td style="text-align: center;">$2.3,3.9$</td>
</tr>
<tr>
<td style="text-align: left;">Dialog w/o <br> History</td>
<td style="text-align: center;">Success <br> step, replan</td>
<td style="text-align: center;">$0.48 \pm 0.11$</td>
<td style="text-align: center;">$1.00 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$0.33 \pm 0.12$</td>
<td style="text-align: center;">$0.73 \pm 0.11$</td>
<td style="text-align: center;">$0.65 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$9.2,3.1$</td>
<td style="text-align: center;">$4.2,1.4$</td>
<td style="text-align: center;">N/A, 1.0</td>
<td style="text-align: center;">$9.6,1.8$</td>
<td style="text-align: center;">$5.8,1.4$</td>
<td style="text-align: center;">$3.7,3.1$</td>
</tr>
<tr>
<td style="text-align: left;">Dialog w/o <br> Feedback</td>
<td style="text-align: center;">Success <br> step, replan</td>
<td style="text-align: center;">$0.35 \pm 0.10$</td>
<td style="text-align: center;">$0.70 \pm 0.10$</td>
<td style="text-align: center;">$0.95 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.11$</td>
<td style="text-align: center;">$0.53 \pm 0.13$</td>
<td style="text-align: center;">$0.45 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$18.0,1.0$</td>
<td style="text-align: center;">$5.9,1.0$</td>
<td style="text-align: center;">$7.6,1.0$</td>
<td style="text-align: center;">$12.6,1.0$</td>
<td style="text-align: center;">$4.9,1.0$</td>
<td style="text-align: center;">$3.4,1.0$</td>
</tr>
<tr>
<td style="text-align: left;">Dialog <br> (ours)</td>
<td style="text-align: center;">Success <br> step, replan</td>
<td style="text-align: center;">$0.44 \pm 0.06$</td>
<td style="text-align: center;">$0.75 \pm 0.10$</td>
<td style="text-align: center;">$0.95 \pm 0.05$</td>
<td style="text-align: center;">$0.80 \pm 0.08$</td>
<td style="text-align: center;">$0.93 \pm 0.06$</td>
<td style="text-align: center;">$0.65 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$9.9,3.5$</td>
<td style="text-align: center;">$4.7,2.0$</td>
<td style="text-align: center;">$7.1,1.0$</td>
<td style="text-align: center;">$10.2,1.7$</td>
<td style="text-align: center;">$4.9,1.3$</td>
<td style="text-align: center;">$2.5,3.1$</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation results on RoCoBench. We report averaged success rates $\uparrow$ over 20 runs per task, the average number of steps in successful runs $\downarrow$, and average number of re-plan attempts used across all runs $\downarrow$.
are prompted to continue discussion without detailed failure reasons. To offset the lack of re-plan rounds, each episode is given twice the budget of episode length.</p>
<p>Evaluation Metric. Provided with a finite number of rounds per episode and a maximum number of re-plan attempts per round, we evaluate the task performance on three metrics: 1) task success rate within the finite rounds; 2) number of environment steps the agents took to succeed an episode, which measures the efficiency of the robots' strategy; 3) average number of re-plan attempts at each round before an environment action is executed - this reflects the agents' ability to understand and use environment feedback to improve their plans. Overall, a method is considered better if the task success rate is higher, it takes fewer environment steps, and requires fewer number of re-plans.</p>
<p>Results. The evaluation results are reported in Table 2. We remark that, despite receiving less information, dialog agents sometimes achieve comparable performance to the oracle planner. Particularly in Sort Cubes task, agents are able to find a strategy to help each other through dialog, but the oracle makes mistakes in trying to satisfy all agents' constraints at once. While removing history information or plan feedback rounds does not negatively impact performance on some tasks, full prompt that includes both achieves the best overall results. Lastly, on Pack Gocery task, the oracle planner shows better capability in waypoint planning, displaying better capability at incorporating feedback and improve on individual coordinate steps.</p>
<h1>5.2 Effect of LLM-proposed 3D Waypoints</h1>
<p>We demonstrate the utility of LLM-proposed task space waypoints. We use two tasks that were designed to have high workspace overlap, i.e. Pack Grocery and Move Rope, which require both picking and placing to complete the task. For comparison, we define a hard-coded waypoint path that performs top-down pick or place, i.e. always hovers over a gripper atop a certain height before picking an object, and moves an object up to a height above the table before moving and placing. We single-out one-step pick or place snapshots, and run multi-arm motion planning using the compared waypoints, under a maximum of 300 second planning time budget.
As shown in Fig. 4, LLM-proposed waypoints show no clear benefits for picking sub-tasks, but significantly accelerate planning for placing, where collisions are more likely to happen between the arms and the desktop objects.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: We demonstrate the utility of LLM-proposed waypoints by comparing with two alternatives: a linear waypoint path that interpolates between start and goal; 'Hard-code', a predefined waypoint path that always performs top-down pick or place.</p>
<h1>5.3 Zero-shot Adaptation to Task Variations</h1>
<p>Leveraging the zero- and few-shot ability of LLMs, RoCo demonstrates strong adaptation ability to varying task semantics, which traditionally would require modification or re-programming of a system, e.g. fine-tuning a learning-based policy. We showcase 3 main variation categories, all using Make Sandwich task in RoCoBench.</p>
<ol>
<li>Object Initialization: The locations of the food items are randomized, and we show the dialog agents' reasoning is robust to this variation.</li>
<li>Task Goal: The agents must stack food items in the correct order given in the sandwich recipe, and are able to coordinate sub-task strategies accordingly.</li>
<li>Robot Capability: The agents are able to exchange information on items that are within their respective reach and coordinate their plans accordingly.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
</ol>
<p>Figure 5: RoCo demonstrates strong adaptation ability to variations in task semantics. We use Make Sandwich task in RoCoBench to showcase three variation categories: 1) object initialization, i.e. randomized food items' locations on the table; 2) task goals, i.e. robots must change behavior according to different sandwich recipes; 3) agent capability, e.g. agents can only pick food items that are within their reach range.</p>
<h3>5.4 Real-world Experiments: Human-Robot Collaboration</h3>
<p>We validate RoCo in a real world setup, where a robot arm collaborates with a human to complete a sorting blocks task (Fig. 6). We run RoCo with the modification that only the robot agent is controlled by GPT-4, and it discusses with a human user that interacts with part of the task workspace. For perception, we use a pre-trained object detection model, OWL-ViT [2], to generate scene description from top-down RGB-D camera images. The task constrains the human to only move blocks from cups to the table, then the robot only picks blocks from table into wooden bin. We evaluate 2 main variation categories: 1) object initialization, i.e. initial block locations are randomized for each run (Fig. 6.1); 2) task order specification, where the agents are asked to follow a fixed order to move the blocks (Fig. 6.2). We also evaluate two types of human behaviors: first is an oracle human that corrects mistakes in the OWL-ViT-guided scene descriptions and the robot's responses; second is an imperfect human that provides no feedback to those errors.</p>
<p>We evaluate 10 runs for each setup, see Table 3 for results. We report task success rate within the finite rounds, and number of steps the agents took to succeed an episode. We remark that task performance is primarily bottle-necked by incorrect object detection from OWL-ViT, which leads to either an incorrect object being picked up and resulting in failure or no object being picked up and resulting in higher steps. See Appendix 12.2 for further details on the real world experiments.</p>
<h2>6 Multi-Agent Representation and Reasoning Dataset</h2>
<p>In addition to our main experimental results, we curate a text-based dataset, RoCoBench-Text, to evaluate an LLM's agent representation and task reasoning ability. This dataset aligns LLM with desirable capabilities in multi-agent collaboration, without requiring robotic environment interaction. It builds on data from our evaluation on RoCoBench, and contains a series of additional questions that are more open-ended and go beyond simply finding the next best action plan.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Real world experiments: collaborative block sorting between a robot and a human, with varying task semantics. We test two variation categories: 1) object initialization, i.e. the object locations are randomized for each episode 2) task order specification, i.e. agents must follow the specified order to move blocks.</p>
<h1>6.1 Dataset Overview</h1>
<p>This dataset contains yes/no, multiple-choice or short question-answering questions, spanning a range of different reasoning abilities (see Appendix 14 for more details):
Self-knowledge evaluates how well the agent establishes its identity under a given task context, divided into two categories: 1) understanding an agent's own capability (e.g. which objects/area are not reachable); 2) memory retrieval, i.e. inferring information from past dialog and actions.
Communication Skills evaluates an agent's ability to effectively exchange information and drive a discussion into an agreeable plan. The questions ask an LLM to 1) choose appropriate response to other agents' questions; 2) choose appropriate inquiries to other agents.
Adaptation evaluates adaptation to unexpected situations that were not specified in context. We use a subset of RoCoBench tasks to design unexpected occurrences, either regarding task state (e.g. a missing object) or a response from another agent, and ask an LLM agent to choose the best response. See below for an example question: two agents make a sandwich together; one agent is informed of a broken gripper and must infer that the sandwich can actually be completed without any item from its side of the table.</p>
<p>You are a robot Chad collaborating with Dave ...[task context] Your gripper is not working. What should you say to Dave? Select exactly one option from below.
A: Sorry Dave, we can't complete the task anymore, my gripper is broke.
B: Let's stop. The recipe needs ham but Dave can't reach my side and my gripper is not functioning.
C: Dave, go ahead and finish the sandwich without me, there isn't anything we need on my side anyway.</p>
<h3>6.2 Evaluation Results</h3>
<p>Setup. All questions are designed to have only one correct answer, hence we measure the average accuracy in each category. We evaluate GPT-4 (OpenAI), GPT-3.5-turbo (OpenAI), and Claude-v1 (Anthropic[3]). For GPT-4, we use two models marked with different time-stamps, i.e. 03/14/2023 and 06/13/2023. Results are summarized in Table 4: we observe that, with small performance variations between the two versions, GPT-4 leads the performance across all categories. We remark that there is still a considerable gap from fully accurate, and hope this dataset will be useful for improving and evaluating language models in future work.
Qualitative Results. We observe GPT-4 is better at following the instruction to formulate output, whereas GPT-3.5-turbo is more prone to confident and elongated wrong answers. See below for an example response from an agent capability question (the prompt is redacted for readability).
You are robot Chad .. [cube-on-panel locations...]. You can reach: [panels]
Which cube(s) can you reach? [...] Answer with a list of cube names, answer None if you can't reach any. Solution: None
GPT-4: None: Claude-v1: yellow_trapezoid
GPT-3.5-turbo: At the current round, I can reach the yellow_trapezoid cube on panel3.</p>
<table>
<thead>
<tr>
<th></th>
<th>Self-knowledge</th>
<th></th>
<th>Communication</th>
<th></th>
<th>Adaptation</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Capability</td>
<td>Memory</td>
<td>Inquiry</td>
<td>Respond</td>
<td></td>
</tr>
<tr>
<td>GPT-4-0314</td>
<td>$0.67 \pm 0.06$</td>
<td>$0.84 \pm 0.06$</td>
<td>$\mathbf{0 . 7 9} \pm \mathbf{0 . 0 6}$</td>
<td>$0.83 \pm 0.04$</td>
<td>$0.68 \pm 0.08$</td>
</tr>
<tr>
<td>GPT-4-0613</td>
<td>$\mathbf{0 . 6 8} \pm \mathbf{0 . 0 6}$</td>
<td>$\mathbf{0 . 9 1} \pm \mathbf{0 . 0 4}$</td>
<td>$0.57 \pm 0.08$</td>
<td>$\mathbf{0 . 8 6} \pm \mathbf{0 . 0 3}$</td>
<td>$\mathbf{0 . 7 1} \pm \mathbf{0 . 0 8}$</td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>$\mathbf{0 . 6 8} \pm \mathbf{0 . 0 6}$</td>
<td>$0.59 \pm 0.07$</td>
<td>$0.48 \pm 0.08$</td>
<td>$0.30 \pm 0.05$</td>
<td>$0.58 \pm 0.09$</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>$0.37 \pm 0.06$</td>
<td>$0.70 \pm 0.07$</td>
<td>$0.55 \pm 0.08$</td>
<td>$0.60 \pm 0.05$</td>
<td>$0.65 \pm 0.09$</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluation results on the multi-agent LLM reasoning dataset. We measure the question-answering accuracy on each test category and compare performance of four different models.</p>
<h1>7 Limitation</h1>
<p>Oracle state information in simulation. RoCo assumes perception (e.g., object detection, pose estimation and collision-checking) is accurate. This assumption makes our method prone to failure in cases where perfect perception is not available: this is reflected in our real-world experiments, where the pre-trained object detection produces errors that can cause planning mistakes.</p>
<p>Open-loop execution. The motion trajectories from our planner are executed by robots in an openloop fashion and lead to potential errors. Due to the layer of abstraction in scene and action descriptions, LLMs can't recognize or find means to handle such execution-level errors.</p>
<p>LLM-query Efficiency. We rely on querying pre-trained LLMs for generating every single response in an agent's dialog, which can be cost expensive and dependant on the LLM's reaction time. Response delay from LLM querying is not desirable for tasks that are dynamic or speed sensitive.</p>
<h2>8 Related Work</h2>
<p>LLMs for Robotics. An initial line of prior work uses LLMs to select skill primitives and complete robotic tasks, such as SayCan [4], Inner Monologue [5], which, similarly to ours, uses environment feedback to in-context improve planning. Later work leverages the code-generation abilities of LLMs to generate robot policies in code format, such as CaP [6], ProgGPT [7] and Demo2Code [8]; or longer programs for robot execution such as TidyBot [9] and Instruct2Act [10]. Related to our use of motion-planner, prior work such as Text2Motion [11], AutoTAMP [12] and LLM-GROP [13, 14] studies combining LLMs with traditional Task and Motion Planning (TAMP). Other work explores using LLMs to facilitate human-robot collaboration [15], to design rewards for reinforcement learning (RL) [16], and for real-time motion-planning control in robotic tasks [17]. While prior work uses single-robot setups and single-thread LLM planning, we consider multi-robot settings that can achieve more complex tasks, and use dialog prompting for task reasoning and coordination.</p>
<p>Multi-modal Pre-training for Robotics. LLMs' lack of perception ability bottlenecks its combination with robotic applications. One solution is to pre-train new models with both vision, language and large-scale robot data: the multi-modal pre-trained PALM-E [18] achieves both perception and task planning with a single model; Interactive Language [19] and DIAL [20] builds a large dataset of language-annotated robot trajectories for training generalizable imitation policies. Another solution is to introduce other pre-trained models, mainly vision-language models (VLMs) such as CLIP [21]). In works such as Socratic Models [22], Matcha [23], and Kwon et al. [24], LLMs are used to repeatedly query and synthesize information from other models to improve reasoning about the environment. While most use zero-shot LLMs and VLMs, works such as CogLoop [25] also explores fine-tuning adaptation layers to better bridge different frozen models. Our work takes advantage of simulation to extract perceptual information, and our real world experiments follow prior work $[26,7,9]$ in using pre-trained object detection models [2] to generate scene description.</p>
<p>Dialogue, Debate, and Role-play LLMs. Outside of robotics, LLMs have been shown to possess the capability of representing agentic intentions [27] and behaviors, which enables multi-agent interactions in simulated environments such as text-based games [28, 29] and social sandbox [30, 31, 32]. Recent work also shows a dialog or debate style prompting can improve LLMs' performance on human alignment [33] and a broad range of goal-oriented tasks [34, 35, 36]. While prior work focuses more on understanding LLM behaviors or improve solution to a single question, our setup requires</p>
<p>planning separate actions for each agent, thus adding to the complexity of discussion and the difficulty in achieving consensus.</p>
<p>Multi-Robot Collaboration and Motion Planning. Research on multi-robot manipulation has a long line of history [37]. A first cluster of work studies the low-level problem of finding collisionfree motion trajectories. Sampling-based methods are a popular approach [38], where various algorithmic improvements have been proposed [39]. Recent work also explored learning-based methods [40] as alternative. While our tasks are mainly set in static scenes, much work has also studied more challenging scenarios that require more complex low-level control, such as involving dynamic objects [41] or closed-chain kinematics [42, 43]. A second cluster of work focuses more on high-level planning to allocate and coordinate sub-tasks, which our work is more relevant to. While most prior work tailor their systems to a small set of tasks, such as furniture assembly [44], we highlight the generality of our approach to the variety of tasks it enables in few-shot fashion.</p>
<h1>9 Conclusion</h1>
<p>We present RoCo, a new framework for multi-robot collaboration that leverages large language models (LLMs) for robot coordination and planning. We introduce RoCoBench, a 6-task benchmark for multi-robot manipulation to be open-sourced to the broader research community. We empirically demonstrate the generality of our approach and many desirable properties such as few-shot adaptation to varying task semantics, while identifying limitations and room for improvement. Our work falls in line with recent literature that explores harnessing the power of LLMs for robotic applications, and points to many exciting opportunities for future research in this direction.</p>
<h2>Acknowledgments</h2>
<p>This work was supported in part by NSF Award #2143601, #2037101, and #2132519. We would like to thank Google for the UR5 robot hardware. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors. The authors would like to thank Zeyi Liu, Zhenjia Xu, Huy Ha, Cheng Chi, Samir Gadre, Mengda Xu, and Dominik Bauer for their fruitful discussions throughout the project and for providing helpful feedback on initial drafts of the manuscript.</p>
<h2>References</h2>
<p>[1] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[2] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, X. Wang, X. Zhai, T. Kipf, and N. Houlsby. Simple open-vocabulary object detection with vision transformers, 2022.
[3] A. LLC. Introducing claude, 2023. URL https://www.anthropic.com/index/ introducing-claude.
[4] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.
[5] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. R. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, 2022.</p>
<p>[6] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policis: Language model programs for embodied control. In arXiv preprint arXiv:2209.07753, 2022.
[7] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.
[8] H. Wang, G. Gonzalez-Pumariega, Y. Sharma, and S. Choudhury. Demo2code: From summarizing demonstrations to synthesizing code via extended chain-of-thought. arXiv preprint arXiv:2305.16744, 2023.
[9] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658, 2023.
[10] S. Huang, Z. Jiang, H. Dong, Y. Qiao, P. Gao, and H. Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176, 2023.
[11] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.
[12] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan. Autotamp: Autoregressive task and motion planning with llms as translators and checkers. arXiv preprint arXiv:2306.06531, 2023.
[13] Y. Ding, X. Zhang, C. Paxton, and S. Zhang. Task and motion planning with large language models for object rearrangement. arXiv preprint arXiv:2303.06247, 2023.
[14] X. Zhang, Y. Zhu, Y. Ding, Y. Zhu, P. Stone, and S. Zhang. Visually grounded task and motion planning for mobile manipulation. In 2022 International Conference on Robotics and Automation (ICRA), pages 1925-1931. IEEE, 2022.
[15] Y. Cui, S. Karamcheti, R. Palleti, N. Shivakumar, P. Liang, and D. Sadigh. " no, to the right"online language corrections for robotic manipulation via shared autonomy. arXiv preprint arXiv:2301.02555, 2023.
[16] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023.
[17] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L. Chiang, T. Erez, L. Hasenclever, J. Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023.
[18] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.
[19] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence. Interactive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.
[20] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint arXiv:2211.11736, 2022.
[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.</p>
<p>[22] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, and P. Florence. Socratic models: Composing zero-shot multimodal reasoning with language, 2022.
[23] X. Zhao, M. Li, C. Weber, M. B. Hafez, and S. Wermter. Chat with the environment: Interactive multimodal perception using large language models. arXiv preprint arXiv:2303.08268, 2023.
[24] M. Kwon, H. Hu, V. Myers, S. Karamcheti, A. Dragan, and D. Sadigh. Toward grounded social reasoning. arXiv preprint arXiv:2306.08651, 2023.
[25] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, and J. Fu. Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation, 2023.
[26] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.
[27] J. Andreas. Language models as agent models. arXiv preprint arXiv:2212.01681, 2022.
[28] D. Schlangen. Dialogue games for benchmarking language understanding: Motivation, taxonomy, strategy, 2023.
[29] K. Chalamalasetti, J. Götze, S. Hakimov, B. Madureira, P. Sadler, and D. Schlangen. clembench: Using game play to evaluate chat-optimized language models as conversational agents, 2023.
[30] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023.
[31] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. Camel: Communicative agents for "mind" exploration of large scale language model society. ArXiv, abs/2303.17760, 2023.
[32] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi. Training socially aligned language models in simulated human society, 2023.
[33] G. Irving, P. Christiano, and D. Amodei. Ai safety via debate, 2018.
[34] V. Nair, E. Schumacher, G. Tso, and A. Kannan. Dera: Enhancing large language model completions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071, 2023.
[35] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent thinking in large language models through multi-agent debate. ArXiv, abs/2305.19118, 2023.
[36] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023.
[37] Y. Koga and J.-C. Latombe. On multi-arm manipulation planning. Proceedings of the 1994 IEEE International Conference on Robotics and Automation, pages 945-952 vol.2, 1994.
[38] S. Karaman and E. Frazzoli. Sampling-based algorithms for optimal motion planning, 2011.
[39] A. Dobson and K. E. Bekris. Planning representations and algorithms for prehensile multi-arm manipulation. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6381-6386. IEEE, 2015.
[40] H. Ha, J. Xu, and S. Song. Learning a decentralized multi-arm motion planner. In Conference on Robotic Learning (CoRL), 2020.</p>
<p>[41] S. S. M. Salehian, N. Figueroa, and A. Billard. Coordinated multi-arm motion planning: Reaching for moving objects in the face of uncertainty. In Robotics: Science and Systems, 2016.
[42] J. Yakey, S. LaValle, and L. Kavraki. Randomized path planning for linkages with closed kinematic chains. Robotics and Automation, IEEE Transactions on, 17:951 - 958, 012002. doi:10.1109/70.976030.
[43] Z. Xian, P. Lertkultanon, and Q.-C. Pham. Closed-chain manipulation of large objects by multi-arm robotic systems. IEEE Robotics and Automation Letters, 2(4):1832-1839, 2017.
[44] M. Dogar, A. Spielberg, S. Baker, and D. Rus. Multi-robot grasp planning for sequential assembly operations. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 193-200, 2015. doi:10.1109/ICRA.2015.7138999.
[45] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 50265033, 2012. doi:10.1109/IROS.2012.6386109.
[46] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020.
[47] M. M. Contributors. MuJoCo Menagerie: A collection of high-quality simulation models for MuJoCo, 2022. URL http://github.com/deepmind/mujoco.menagerie.
[48] S. Dasari, A. Gupta, and V. Kumar. Learning dexterous manipulation from exemplar object trajectories and pre-grasps. In IEEE International Conference on Robotics and Automation 2023, 2023.</p>
<h1>10 RoCoBench</h1>
<h3>10.1 Overview</h3>
<p>RoCoBench is built with MuJoCo [45] physics engine. The authors would like to thank the various related open-source efforts that greatly assisted the development of RoCoBench tasks: DMControl [46], Menagerie[47], and MuJoCo object assets from Dasari et al. [48]. The sections below provide a detailed documentation for each of the 6 simulated collaboration tasks.</p>
<h3>10.2 Task: Sweep Floor</h3>
<p>Task Description. 2 Robots bring a dustpan and a broom to opposite sides of each cube to sweep it up, then the robot holding dustpan dumps cubes into a trash bin.</p>
<p>Agent Capability. Two robots stand on opposite sides of the table:</p>
<ol>
<li>UR5E with robotiq gripper ('Alice'): holds a dustpan</li>
<li>Franka Panda ('Bob'): holds a broom</li>
</ol>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Sweep Floor task</p>
<p>Observation Space. 1) cube locations: a. on table; b. inside dustpan; c. inside trash bin; 2) robot status: 3D gripper locations
Available Robot Skills. 1) MOVE [target]: target can only be a cube; 2) SWEEP [target]: moves the groom so it pushes the target into dustpan; 3) WAIT; 4) DUMP: dump dustpan over the top of trash bin.</p>
<h3>10.3 Task: Make Sandwich</h3>
<p>Task Description. 2 Robots make a sandwich together, each having access to a different set of ingredients. They must select the required items and take turns to stack them in the correct order.
Agent Capability. Two robots stand on opposite sides of the table:</p>
<ol>
<li>UR5E with suction gripper ('Chad'): can only reach right side</li>
<li>Humanoid robot with suction gripper ('Dave'): can only reach left side</li>
</ol>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Make Sandwich task</p>
<p>Observation Space 1) the robot's own gripper state (either empty or holding an object); 2) food items on the robot's own side of the table and on the cutting board.
Available Robot Skills. 1) PICK [object]; 2) PUT [object] on [target]; WAIT</p>
<h3>10.4 Task: Sort Cubes</h3>
<p>Task Description. 3 Robots sort 3 cubes onto their corresponding panels. The robots must stay within their respective reach range, and help each other to move a cube closer.</p>
<p>Agent Capability. Three robots each responsible for one area on the table</p>
<ol>
<li>UR5E with robotiq gripper ('Alice'): must put blue square on panel2, can only reach: panel1, panel2, panel3.</li>
<li>Franka Panda ('Bob'): must put pink polygon on panel4, can only reach: panel3, panel4, panel5.</li>
<li>UR5E with suction gripper ('Chad'): must put yellow trapezoid on panel6, can only reach: panel5, panel6, panel7.
Observation Space 1) the robot's own goal, 2) locations of each cube.
Available Robot Skills. 1) PICK [object] PLACE [panelX]; 2) WAIT</li>
</ol>
<h1>10.5 Task: Pack Grocery</h1>
<p>Task Description. 2 Robots pack a set of grocery items from the table into a bin. The objects are in close proximity and robots must coordinate their paths to avoid collision.</p>
<p>Agent Capability. Two robots on opposite sides of table</p>
<ol>
<li>UR5E with robotiq gripper ('Alice'): can pick and place any object on the table</li>
<li>Franka Panda ('Bob'): can pick and place any object on the table
<img alt="img-7.jpeg" src="img-7.jpeg" /></li>
</ol>
<p>Figure 10: Pack Grocery task</p>
<p>Observation Space 1) robots' gripper locations, 2) locations of each object, 3) locations of all slots in the bin.</p>
<p>Available Robot Skills. (must include task-space waypoints) 1) PICK [object] PATH [path]; 2) PLACE [object] [target] PATH [path]</p>
<h3>10.6 Task: Move Rope</h3>
<p>Task Description. 2 robots lift a rope together over a wall and place it into a groove. They must coordinate their grippers to avoid collision.</p>
<p>Agent Capability. Two robots on opposite sides of table</p>
<ol>
<li>UR5E with robotiq gripper ('Alice'): can pick and place any end of the rope within its reach
<img alt="img-8.jpeg" src="img-8.jpeg" /></li>
<li>Franka Panda ('Bob'): can pick and place any end of the rope within its reach</li>
</ol>
<p>Observation Space 1) robots' gripper locations, 2) locations of rope's front and end back ends; 3) locations of corners of the obstacle wall; 4) locations of left and right ends of the groove slot.</p>
<p>Available Robot Skills. (must include task-space waypoints) 1) PICK [object] PATH [path]; 2) PLACE [object] [target] PATH [path]</p>
<h3>10.7 Task: Arrange Cabinet</h3>
<p>Task Description. 3 robots, two of them each hold one side of the cabinet door open, while the third robot takes the cups out and place them onto the correct coasters.</p>
<p>Agent Capability. Three robots, one on left side of the table, two on right side of table</p>
<ol>
<li>UR5E with robotiq gripper ('Alice'): stands on left side, can only reach left cabinet door</li>
<li>Franka Panda ('Bob'): : stands on right side, can only reach right cabinet door
<img alt="img-9.jpeg" src="img-9.jpeg" /></li>
</ol>
<p>Figure 12: Arrange Cabinet task
3. UR5E with suction gripper ('Chad'):: stands on right side, can reach right cabinet door and cups and mugs inside the cabinet.
Observation Space 1) locations cabinet door handles; 2) each robot's reachable objects, unaware of other robot's reach range.</p>
<p>Available Robot Skills. 1) PICK [object]; 2) OPEN [one side of door handle]; 3) WAIT; 3) PLACE [object] [target]</p>
<h1>11 Details on LLM Prompting</h1>
<p>We use a separate query call for every agent's individual response in a dialog, see the text box below for a redacted example of an agent's prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="p">[</span><span class="n">agent</span><span class="w"> </span><span class="n">name</span><span class="p">],</span><span class="w"> </span><span class="n">collaborating</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">[</span><span class="n">other</span><span class="w"> </span><span class="n">agent</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="p">[</span><span class="n">task</span><span class="w"> </span><span class="n">context</span><span class="p">].</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="p">[</span><span class="n">agent</span><span class="w"> </span><span class="n">capability</span><span class="p">].</span>
<span class="nl">Previously</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">round</span><span class="w"> </span><span class="n">history</span><span class="p">]</span>
<span class="n">At</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">round</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">current</span><span class="w"> </span><span class="n">observation</span><span class="p">]</span>
<span class="n">Discuss</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">[</span><span class="n">other</span><span class="w"> </span><span class="n">agent</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">coordinate</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">together</span><span class="p">.[</span><span class="n">communication</span><span class="w"> </span><span class="n">instruction</span><span class="p">].</span>
<span class="n">Never</span><span class="w"> </span><span class="n">forget</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="p">[</span><span class="n">agent</span><span class="w"> </span><span class="n">name</span><span class="p">]</span><span class="o">!</span><span class="w"> </span><span class="n">Respond</span><span class="w"> </span><span class="n">very</span><span class="w"> </span><span class="n">concisely</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="p">[</span><span class="n">response</span><span class="w"> </span><span class="n">format</span><span class="p">]</span>
<span class="n">Previous</span><span class="w"> </span><span class="n">chat</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">dialog</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">re</span><span class="o">-</span><span class="n">plan</span><span class="w"> </span><span class="n">rounds</span><span class="p">]</span>
<span class="n">This</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">failed</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">plan</span><span class="w"> </span><span class="n">feedback</span><span class="p">]</span><span class="w"> </span><span class="n">Current</span><span class="w"> </span><span class="n">chat</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">dialog</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">round</span><span class="p">]</span>
<span class="n">Your</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="n">I</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">Alice</span><span class="p">,</span><span class="w"> </span><span class="p">...</span>
</code></pre></div>

<p>Algorithm 1 Multi-agent dialog for collaboration
Require: agent $u^{1}, \ldots, u^{N}$, task horizon $T$;
Require: max number of re-plans $K$, max number of dialog per round $M$,
Require: history buffer $H$; feedback buffer $F$
$t \leftarrow 0$
$o_{t} \leftarrow \operatorname{env} . \operatorname{reset}()$
$H$.empty()
while $t&lt;T$ do
$F$.empty()
while len $(F)<K$ do
dialog, plan $\leftarrow$ PromptDialogs $\left(\mathrm{H}, \mathrm{F}, o_{t}, u^{n}\right)$
plan-valid, feedback $\leftarrow$ GiveFeedback(plan)
if plan-valid then
final-plan $\leftarrow$ parsed-plan
break
end if
$F$.append(feedback)
end while
if plan-valid then
$\sigma_{t} \leftarrow$ MotionPlanner $\left(o_{t}\right.$, final-plan)
$o_{t+1}, r_{t+1} \leftarrow \operatorname{env} \cdot \operatorname{step}\left(\sigma_{t}\right)$
if $r_{t+1}>0$ then
break
end if
end if
H.append(dialog)
$t \leftarrow t+1$
end while</p>
<h2>12 Additional Experiment Results</h2>
<h3>12.1 Failure Analysis on RoCoBench</h3>
<p>We provide example failure scenarios observed in the agent dialog and environment interaction.
Factual Errors in Dialog. We observe that when one LLM-generated agent makes a mistake, it could sometimes cause subsequent dialog to build on the wrong reasoning. See an example below: one agent (Alice) wrongfully decided the task is complete, and other agents repeat this wrong claim and choose to wait for multiple rounds while the task is, in fact, not finished.
[Alice]: [...] we have successfully completed our task. [...], let's wait for further instructions.
[Bob]: I agree that we have successfully completed our task. Great teamwork, everyone! Let's wait [...]
[Chad]: I concur that we have accomplished our goal. Excellent teamwork! Let's wait [...].</p>
<p>Errors in Motion Trajectory Execution Due to the open-loop nature of the execution, small errors in a motion trajectory could lead to unexpected errors, e.g. knocking of an object by accident.</p>
<h1>12.2 Real World Experiment Setup</h1>
<p>The robot agent is a 6 DoF UR5E arm with suction gripper, and dialog is enabled by querying a GPT-4 model to respond as agent 'Bob', who is discussing with a human collaborator 'Alice'. The human user provides text input to engage in the dialog, and arranges cubes on the same tabletop. For perception, we use top-down RGB-D image from an Azure Kinect sensor.</p>
<p>See the text below for an example of the robot's prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="o">====</span><span class="w"> </span><span class="n">System</span><span class="w"> </span><span class="n">Prompt</span><span class="w"> </span><span class="o">====</span>
<span class="w">    </span><span class="p">[</span><span class="n">Action</span><span class="w"> </span><span class="n">Options</span><span class="p">]</span>
<span class="w">    </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">PICK</span><span class="w"> </span><span class="o">&lt;</span><span class="n">obj</span><span class="o">&gt;</span><span class="w"> </span><span class="n">PLACE</span><span class="w"> </span><span class="o">&lt;</span><span class="n">target</span><span class="o">&gt;:</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">decide</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">PICK</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">PLACE</span><span class="p">.</span><span class="w"> </span><span class="n">To</span>
<span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">PLACE</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">blocks</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">wooden</span><span class="w"> </span><span class="n">bin</span><span class="p">.</span>
<span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">WAIT</span><span class="o">:</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">choose</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">nothing</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">wait</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="n">Alice</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">blocks</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">cups</span><span class="w"> </span><span class="n">to</span>
<span class="n">the</span><span class="w"> </span><span class="n">table</span><span class="p">.</span>
<span class="w">    </span><span class="p">[</span><span class="n">Action</span><span class="w"> </span><span class="n">Output</span><span class="w"> </span><span class="n">Instruction</span><span class="p">]</span>
<span class="w">    </span><span class="n">First</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="err">&#39;</span><span class="n">EXECUTE</span>
<span class="w">    </span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">exactly</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">ACTION</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">robot</span><span class="p">.</span>
<span class="w">    </span><span class="n">Example</span><span class="err">#</span><span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="err">&#39;</span><span class="n">EXECUTE</span>
<span class="w">    </span><span class="n">NAME</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="n">ACTION</span><span class="w"> </span><span class="n">PICK</span><span class="w"> </span><span class="n">green</span><span class="p">,</span><span class="n">cube</span><span class="w"> </span><span class="n">PLACE</span><span class="w"> </span><span class="n">wooden</span><span class="p">.</span><span class="n">bin</span>
<span class="w">    </span><span class="err">&#39;</span><span class="w"> </span><span class="n">Example</span><span class="err">#</span><span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="err">&#39;</span><span class="n">EXECUTE</span>
<span class="w">    </span><span class="n">NAME</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="n">ACTION</span><span class="w"> </span><span class="n">WAIT</span>
<span class="w">    </span><span class="p">,</span>
</code></pre></div>

<p>You are a robot called Bob, and you are collaborating with human Alice to move blocks from inside cups to a wooden bin.
You cannot pick blocks when they are inside cups, but can pick blocks when they are on the table. Alice must help you by moving blocks from inside cups to the table.
You must WAIT for Alice to move blocks from inside cups to the table, then you can PICK blocks from the table and PLACE them in the wooden bin.
[mention task order specification]
Talk with Alice to coordinate and decide what to do.
At the current round:
[object descriptions from observation]
Think step-by-step about the task and Alice's response.
Improve your plans if given [Environment Feedback].
Propose exactly one action for yourself at the current round, select from [Action Options].
End your response by either: 1) output PROCEED, if the plans require further discussion; 2) If everyone has made proposals and got approved, output the final plan, must strictly follow [Action Output Instruction]!</p>
<div class="codehilite"><pre><span></span><code>==== User Prompt ====
You are Bob, your response is:
response from GP1-4:
EXECUTE
NAME Bob ACTION ...
</code></pre></div>

<h1>13 Toy Example on LLM for 3D Path Planning</h1>
<h3>13.1 Experiment Setup</h3>
<p>We use the chatCompletion mode in GPT-4. At each evaluation run, we randomly sample a new set of obstacles and agents' start-goal locations. Each run is given up to 5 attempts: using the same system prompt, which describes the task and output instructions, and a user prompt, which describes the current grid layout. If a first attempt fails, the feedback from previous plans are appended to the user prompt at later attempts, until reaching the max number of attempts. See below for the prompts and GPT-4's response (some coordinate tuples are omitted (marked as "omitted") due to limited space).</p>
<div class="codehilite"><pre><span></span><code><span class="o">====</span><span class="w"> </span><span class="n">System</span><span class="w"> </span><span class="n">Prompt</span><span class="w"> </span><span class="o">====</span>
<span class="n">Plan</span><span class="w"> </span><span class="n">paths</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">navigate</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="mi">3</span><span class="n">D</span><span class="w"> </span><span class="n">grid</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">reach</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">respective</span><span class="w"> </span><span class="n">goals</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">avoid</span><span class="w"> </span><span class="n">collision</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">given</span><span class="o">:</span>
<span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">obstacle</span><span class="w"> </span><span class="n">coordinates</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">locations</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">obstacle</span><span class="w"> </span><span class="n">grid</span><span class="w"> </span><span class="n">cells</span><span class="p">,</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">avoid</span><span class="w"> </span><span class="n">them</span><span class="p">.</span>
<span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="p">[([</span><span class="n">name</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">init</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">goal</span><span class="p">])</span><span class="w"> </span><span class="n">tuples</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">init</span><span class="p">]</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="p">[</span><span class="n">goal</span><span class="p">]</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="mi">3</span><span class="n">D</span><span class="w"> </span><span class="n">coordinates</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">goal</span>
<span class="n">position</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">named</span><span class="w"> </span><span class="p">[</span><span class="n">name</span><span class="p">].</span>
<span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">plan</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">any</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">why</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span><span class="w"> </span><span class="n">Analyze</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">re</span><span class="o">-</span><span class="n">plan</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">collision</span><span class="o">-</span><span class="n">free</span><span class="w"> </span><span class="n">path</span><span class="p">.</span>
<span class="n">How</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">path</span><span class="o">:</span>
<span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">Make</span><span class="w"> </span><span class="n">sure</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">touch</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">obstacle</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">[</span><span class="n">planning</span><span class="w"> </span><span class="n">instructions</span><span class="p">]</span>
<span class="p">[</span><span class="n">instruction</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">format</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">====</span><span class="w"> </span><span class="k">User</span><span class="w"> </span><span class="n">Prompt</span><span class="w"> </span><span class="o">====</span>
<span class="k">At</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="nl">step</span><span class="p">:</span><span class="w"> </span><span class="n">Grid</span><span class="w"> </span><span class="k">size</span><span class="err">:</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="mi">10</span>
<span class="nl">Obstacles</span><span class="p">:(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)...</span><span class="w"> </span><span class="o">[</span><span class="n">list of all obstacle 3D locations</span><span class="o">]</span>
<span class="n">Agent</span><span class="w"> </span><span class="n">Alice</span><span class="w"> </span><span class="nl">init</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="nl">goal</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="n">Agent</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="nl">init</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="nl">goal</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="n">Agent</span><span class="w"> </span><span class="n">Chad</span><span class="w"> </span><span class="nl">init</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="nl">goal</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">)</span>
<span class="n">Agent</span><span class="w"> </span><span class="n">Dave</span><span class="w"> </span><span class="nl">init</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nl">goal</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="nl">Feedback</span><span class="p">:</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="nl">failed</span><span class="p">:</span>
<span class="o">[</span><span class="n">gust plan, omitted</span><span class="o">]</span>
<span class="ow">Some</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exactly</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">away</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="nl">other</span><span class="p">:</span><span class="w"> </span><span class="nl">Bob</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
<span class="k">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">try</span><span class="w"> </span><span class="n">again</span><span class="p">,</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">collision</span><span class="o">-</span><span class="k">free</span><span class="p">,</span><span class="w"> </span><span class="n">strictly</span><span class="w"> </span><span class="n">one</span><span class="o">-</span><span class="n">step</span><span class="o">-</span><span class="n">apart</span><span class="w"> </span><span class="n">paths</span><span class="p">.</span>
<span class="nl">Feedback</span><span class="p">:</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="nl">failed</span><span class="p">:</span>
<span class="p">...</span><span class="o">[</span><span class="n">another feedback, same format as above</span><span class="o">]</span>
<span class="n">Your</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="k">is</span><span class="err">:</span>
<span class="n">response</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">GPT</span><span class="o">-</span><span class="mi">4</span><span class="err">:</span>
<span class="k">PLAN</span>
<span class="n">NAME</span><span class="w"> </span><span class="n">Alice</span><span class="w"> </span><span class="k">PATH</span><span class="w"> </span><span class="o">[</span><span class="n">(7, 6, 2), (6, 6, 2), ...[omitted</span><span class="o">]</span><span class="w"> </span><span class="err">]</span>
<span class="n">NAME</span><span class="w"> </span><span class="n">Bob</span><span class="w"> </span><span class="k">PATH</span><span class="w"> </span><span class="o">[</span><span class="n">(9, 9, 5), (8, 9, 5), ...[omitted</span><span class="o">]</span>
<span class="n">NAME</span><span class="w"> </span><span class="n">Chad</span><span class="w"> </span><span class="k">PATH</span><span class="w"> </span><span class="o">[</span><span class="n">(1, 0, 0), (2, 0, 0), ...[omitted</span><span class="o">]</span>
<span class="n">NAME</span><span class="w"> </span><span class="n">Dave</span><span class="w"> </span><span class="k">PATH</span><span class="w"> </span><span class="o">[</span><span class="n">(9, 1, 1), (8, 1, 1), (7, 1, 1), (7, 1, 0)</span><span class="o">]</span>
</code></pre></div>

<h3>13.2 Full Experiment Results</h3>
<p>We run 30 evaluation runs, each with a randomly sampled grid layout. See the left plot in Figure 13 for evaluation results. We make the following observations: first, the planning success rate varies moderately with the temperature parameter of GPT-4, which controls how deterministic its output generation is. A moderate temperature around 0.6 , i.e. partially deterministic, shows the best success rate, and notably is able to 1 -shot succeed the most times. Next, we observe the model's ability to read from the feedback of a previous plan, and eventually succeed after a few more attempts. We also provide a visualization of an example output from one of the runs, where the path from first attempt has collision, and the model is able to correct this plan.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Top right: planning success rate over 30 evaluation runs on a size $5^{3}$ 3D grid. The LLM (GPT-4)'s performance shows variation with the temperature parameter. Top left: an example output on a size $5^{3}$ grid. GPT-4 is able to improve the previous plan based on feedback on collided steps, which is appended to the prompt in 3D coordinate format. Bottom: we repeat the same experiments and show example outputs on a more challenging, size $10^{3}$ grid, where the planning success rate is lower, and the model requires more re-planning attempts before reaching a successful plan.</p>
<h1>14 Multi-Agent Representation and Reasoning Dataset</h1>
<h3>14.1 Task: Self-Knowledge Question-Answering</h3>
<h3>14.1.1 Agent Capability</h3>
<p>57 total questions. Sim. Task: sequential transport</p>
<ul>
<li>Context (system prompt):</li>
</ul>
<p>7 panels on the table, ordered left to right: panel1,....panel7. They form a straight assembly line, panel1 is closed to panel2 and farthest from panel7.
You are robot Alice in front of panel2. You are collaborating with Bob, Chad to sort cubes into their target panels. The task is NOT done until all three cubes are sorted.
At current round:
blue_square is on panel5
pink_polygon is on panel1
yellow_trapezoid is on panel3
Your goal is to place blue_square on panel2, but you can only reach panel1, panel2, panel3: this means you can only pick cubes from these panels, and can only place cubes on these panels.
Never forget you are Alice! Never forget you can only reach panel1, panel2, panel3!</p>
<ul>
<li>Question (user prompt):</li>
</ul>
<p>You are Alice. List all panels that are out of your reach. Think step-by-step. Answer with a list of panel numbers, e.g. [1, 2] means you can't reach panel 1 and 2.</p>
<ul>
<li>Solution:
panels $[4,5,6,7]$</li>
</ul>
<h3>14.1.2 Memory Retrieval</h3>
<p>44 total questions. Task: Make Sandwich, Sweep Floor</p>
<ul>
<li>Context (system prompt):</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">[History]</span>
<span class="na">Round#0</span><span class="o">:</span>
<span class="na">[Chat History] [Chad]</span><span class="o">:</span><span class="w"> </span><span class="s">... [Dave]:... [Chad]: ... ... [Executed Action]...</span>
<span class="na">Round#1</span><span class="o">:</span>
</code></pre></div>

<ul>
<li>Current Round</li>
</ul>
<p>You are a robot Chad, collaborating with Dave to make a vegetarian.sandwich [.....] You can see these food items are on your reachable side: ...</p>
<ul>
<li>Question (user prompt) You are Chad. Based on your [Chat History] with Dave and [Executed Action] from previous rounds in [History], what food items were initially on Dave's side of the table? Only list items that Dave explicitly told you about and Dave actually picked up. Don't list items that you are unsure about. Output the item names as a list. Think step-by-step.</li>
<li>Solution:
bread_slice1</li>
</ul>
<h1>14.2 Challenge: Effective Communication</h1>
<h3>14.3 Inquiry</h3>
<p>41 multiple-choice questions, using Arrange Cabinet task.</p>
<h2>- Context (system prompt):</h2>
<p>You are Bob, collaborating with Alice, Chad to pick a mug and a cup out of cabinet, and place them on correct coasters. Both left and right cabinet doors should be OPENed and held open, while anything inside can be PICKed. You must coordinate to complete the task.
At current round: left door is closed, right door is closed, mug is inside cabinet; cup is inside cabinet; Alice's gripper is holding nothing,
Your gripper is holding nothing,
Chad's gripper is holding nothing,
Never forget you are Bob! Never forget you can only reach right door handle!</p>
<ul>
<li>Question (user prompt):</li>
</ul>
<p>You are thinking about picking right door handle. Who and what should you ask to confirm this action? Think step-by-step, then choose exactly one option from below.
[A] tell others about this plan because you are free and right door handle is within your reach.
[B] ask if Alice and Chad can reach right door handle because it's not within your reach.
[C] ask if Alice and Chad can help, because you can reach right door handle, but you are busy and they are free.
[D] all three of you are busy, so it's better to wait until later.</p>
<ul>
<li>Solution: [A]</li>
</ul>
<h3>14.4 Responsiveness</h3>
<h2>96 questions. Binary YES or NO</h2>
<p>7 panels on the table, ordered left to right: panel1,...,panel7. They form a straight assembly line, panel1 is closed to panel2 and farthest from panel7.
You are robot Alice in front of panel2. You are collaborating with Bob, Chad to sort cubes into their target panels. The task is NOT done until all three cubes are sorted. At current round:
blue_square is on panel5 pink_polygon is on panel3 yellow_trapezoid is on panel3
Your goal is to place blue_square on panel2, but you can only reach panel1, panel2, panel3: this means you can only pick cubes from these panels, and can only place cubes on these panels. You can help others by moving other cubes that are within your reachable panels. Never forget you are Alice! Never forget you can only reach panel1, panel2, panel3!</p>
<p>You are Alice. Chad needs to put yellow_trapezoid on panel6, and asked you for help. How should you respond? Think step-by-step. If you can help, your answer must start with YES and propose a plan to help. If you can't help, must answer NO.</p>
<h1>14.5 Challenge: Adaptation to Unexpected Scenarios</h1>
<p>31 questions. Multiple choice (A, B, C).
You are a robot Chad, collaborating with Dave to make a [vegetarian_sandwich]. Food items must be stacked following this order: bread_slice1, tomato, cheese, cucumber, bread_slice2, where bread_slice 1 must be PUT on cutting_board. You must stay on right side of the table! This means you can only PICK food from right side, and Dave can only PICK from the other side. Only one robot can PUT at a time, so you must coordinate with Dave. At the current round: You can see these food items are on your reachable side: bread_slice1: on cutting_board cheese: atop tomato tomato: atop bread_slice1 cucumber: atop cheese ham: on your side beef_patty: on your side Your gripper is empty
You are Chad. Your gripper is not working right now. What should you say to Dave? Select exactly one option from below. You must first output a single option number (e.g. A), then give a very short, one-line reason for why you choose it.
Options:
A: Sorry Dave, we can't complete the task anymore, my gripper is broke.
B: Let's stop. The recipe needs ham but Dave can't reach my side and my gripper is not functioning.
C: Dave, go ahead and finish the sandwich without me, there isn't anything we need on my side anyway.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1689 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1689</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1689</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-267199734</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.13170v3.pdf" target="_blank">CFMatch : Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Question answering ( QA ) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence ( AE ) often do not align with human judgments, particularly more verbose, free-form answers from large language models ( LLM ). There are two challenges: a lack of data and that models are too big: LLM -based scorers can correlate better with human judges, but this expensive task has only been tested on limited QA datasets, but even when available, update of the model is limited because LLM are, well, large and often expensive. We rectify both of these issues by providing clear and consistent guideline for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust and lightweight discriminate AE classifier-based matching method</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1689",
    "paper_id": "paper-267199734",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005345249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
24 Jan 2024</p>
<p>Zongxia Li 
University of Maryland
College Park</p>
<p>Ishani Mondal imondal@cs.umd.edu 
University of Maryland
College Park</p>
<p>Yijun Liang yliang17@cs.umd.edu 
University of Maryland
College Park</p>
<p>Huy Nghiem nghiemh@cs.umd.edu 
University of Maryland
College Park</p>
<p>Jordan Boyd-Graber 
University of Maryland
College Park</p>
<p>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering
24 Jan 2024BD5789FAF205C6A83F57974DC64381E4arXiv:2401.13170v1[cs.CL]
Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM).There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this expensive task has only been tested on limited QA datasets, but even when available, update of the model is limited because LLM are, well, large and often expensive.We rectify both of these issues by providing clear and consistent guideline for evaluating AE in machine QA adopted from professional human QA contests.We also introduce a combination of standard evaluation and a more efficient, robust and lightweight discriminate AE classifier-based matching method -CFMatch (&lt;1 MB), trained and validated to more accurately evaluate answer correctness in accordance with adopted expert AE rules are more aligned with human judgments.</p>
<p>Introduction</p>
<p>QA is a key task in natural language processing with many models and datasets, where most interactions with a language model can be considered QA tasks.There are two common ways to improve QA models' ability to better answer questions: either use new and better data or build new, better, and (usually) bigger models.This paper does neither of those things.Instead, we focus on improving evaluation.</p>
<p>A skeptical reader might ask: Won't a better model always have a higher score even if the evaluation is better. . .who cares?From a model selection perspective-particularly for models that are similar-subtle differences could swap the order of models (Voorhees and Tice, 2000).However, from a training perspective, having the right objective still matters: keeping models, data, and test evaluation fixed, you can still improve test evaluation by improving the training evaluation metric (Si et al., 2021).</p>
<p>We focus on the answer evaluation (AE) task: given a set of gold answers, is the output of a system equivalent to one of the gold answers?The standard AE evaluations of QA are Exact Match (EM)-evaluates QA systems' ability to extract a specific span of text given a passage and a question, token level F 1 match-measures the average overlap between the prediction and ground truth answer, ROUGE score (Lin, 2004)-measures the overlap of n-grams between the system's output and a set of reference summaries, METEOR scoreevaluates machine translation output and has stemming and synonymy matching features not present in other metrics (Banerjee and Lavie, 2005).While these standard AE evaluation methods work okay on the most common QA evaluations, we argue in Section 2 that they stumble on the long-tail of QA examples because they lack the depth of semantic understanding of the contexts of the questions and answers that humans use when adjudicating answers.</p>
<p>Fortunately, we are not starting from scratch!We draw on rules for adjudicating human question answering competitions, studying and adopting standardized AE guidelines from National Academic Quiz Tournaments (NAQT) (National Academic Quiz Tournaments, LLC, 2024), A Jeopardy! 'Case Book' (Carberry, 2019) and the efficient QA competition (Min et al., 2021) to define acceptable answers for machine QA and expand the long-tail of automated QA evaluation in Section 3.1.We use a large language models (LLM)-GPT-4 (Bubeck et al., 2023)-to generate and self-verify AE examples according to our AE guidelines.</p>
<p>This broader view of AE is more necessary than ever because generative AI is often more creative with their answers (Zhu et al., 2021).For example, e.g., GPT-series models (Bubeck et al., 2023), LLAMA 2 (Touvron et al., 2023) go beyond extracting exact answers from retrieved passages, such as Dense Retriever-Reader (DPR) models (Karpukhin et al., 2020) and generate more freeformed answers, with more variability than a set of given reference answers.Creative answers often make automated AE evaluations more challenging.For example, section 5.2 shows that out of 250 selected AE examples, around 90% of the examples have correct candidate answers missing from the reference answer sets, which are equivalent under human judgment but not equivalent when using string exact match.</p>
<p>However, we recognize that one of the reasons that exact match (EM) is so popular is because it is simple, fast, and requires minimun setup and computation resources comparing to rising popularity of LLM-based evaluation methods (Kamalloo et al., 2023a).Thus Section 3.3 proposes CFMatch, which combines standard evaluation methods -F 1 -with a very simple discrimative logistic regression (LR) classifier and trains it on our augmented AE dataset to distill more complicated models to a fast, small, easy to run deterministic classifier that is competitive with our LLM approach.Our classifier, less than 1 MB in size, is tested on a challenge expert QA set involving expert adjudications of AE that requires a regular person's years of experience to get right.It achieves the best AE alignments with expert judgments without access to additional online or retrieved knowledge, followed by BERTbased method trained on our augmented AE dataset then other standard evaluation methods.</p>
<p>In addition, We conduct further human evaluation to show that CFMatch reduces common judgement errors present in current evaluation metrics and is comparable with Muppet Model (transformer architecture models)-based matching methods, and more closely aligned and generalized with our AE rules.We also demonstrate the advantage of augmenting AE training data can improve both our classifier and Muppet Models' ability to detect AE.We release a python package specifically for QA evaluation that includes EM, F 1 socre, CFMatch, fine-tuned BERT matching (download optional) for researchers to access popular and robust QA evaluation metrics.1</p>
<p>Limitations of Current Evaluation Methods</p>
<p>In this section, we first review the current state and definition of AE in machine QA domain.Then we conduct our initial error analysis of current popular QA evaluation methods-EM, F 1 , BEM (Bulian et al., 2022)-on the benchmark Reading Comprehension with Commonsense Reasoning (ROPES) dev dataset (Lin et al., 2019), and we show the pitfalls of current evaluation methods.We specifically present annotated examples to show the lack of generalization of these evaluation methods on outof-distribution (OOD) datasets.especially for BERTbased methods trained on AE data generated only from the Stanford Question Answering Dataset (Rajpurkar et al., 2016).</p>
<p>Definition of QA evaluations</p>
<p>The definition of AE varies depending on the type of evaluation methods.We present three QA evaluation methods with their definitions, which we will also use for our error analysis in section 2.2.</p>
<p>Exact Match (EM) It is both the simplest and most-used QA evaluation method used in QA because it is unambiguous.EM was introduced when extractive QA focused on source texts, but it falters with models that provide answers in different but correct forms (for example, answers with additional helpful information useful for a user using a QA system).</p>
<p>F 1 Score The F 1 score measures the token overlap between the reference and candidate answers, focusing on both precision and recall.Each continuous sequence of non-whitespace characters is a token.Suppose reference answer World Health Organization global public health leader has 7 tokens-["World", "Health", "Organization", "global", "public", "health", "leader"], whereas candidate answer WHO global leader has three tokens-["WHO", "global", "leader"].Precision calculates the proportion of relevant words in the candidate answer, while recall assesses the proportion of relevant words in the reference answer that are also in the candidate answer.The True Positive (TP) between the examples are 2 since "global" and "leader" are two common words in both answers; the False Positive (FP) is 1 since "WHO" is the token not present in the reference; the False Negative (FN) is 7 since there are 7 tokens in the reference but not in the candidate.Thus we calculate precision and recall using formulas:
Precision = tp tp + fp , Recall = tp tp + fn
The precision and recall are approximately 0.67 and 0.29, thus resulting F 1 score is 0.4.This method acknowledges the correctness of a candidate answer by accessing essential information and amount of token overlap in the reference answers, but does not take the context of the question into account.In addition,there is no consensus regarding the choice of threshold for F 1 score, making this metric more challenging to interpret than the EM.A 0.3 threshold will make the candidate answer equivalent to the reference but 0.5 will not.Section 3.3 will introduce how we combine F 1 with a new evaluation method, eliminating the need for threshold selection.</p>
<p>BERT-Matching (BEM) BEM uses a BERT-based model trained on an AE dataset that can can take the questions, reference, and candidate answers all into account to detect candidate answer correctness.(Bulian et al., 2022) provides two fundamental criteria to define AE and annotate the AE dataset based on 1: Candidate answers contain at least the same (or more) relevant information as the reference answers, while taking into account of the context and the question; in particular it does not omit any relevant information present in the reference answer should be ruled correct; 2: Candidate answers contain neither misleading or excessive information not present in the reference answers taking into account of the question and context should be ruled incorrect.Fine-tuned BERT-based evaluation methods have the highest alignment with human judgments than EM and F 1 on their AE test set.</p>
<p>Generalization of Current AE Evaluation</p>
<p>Out-of-distribution (OOD) dataset We run error analysis on current QA evaluation methods using the Reading Comprehension with Commonsense Reasoning (ROPES), which focuses on causal situational reasoning in reading QA pairs (Lin et al., 2019).Each question has its background information and a context, and a reference answer.We use the dev set with 1,688 QA pairs as our test set in our experiment.</p>
<p>Candidate answer generation</p>
<p>We use FLAN-T5 XL (Chung et al., 2022), a closed-book instructiontuned language model for any tasks as an our QA model to generate candidate answers for ROPES.We concatenate the background information and situation passage as the context of the question and prompt FLAN-T5 with the context and question to generate candidate answers for ROPES. 2   Pitfalls and generalization of current evaluation methods We manually annotate the correctness of 1,688 candidate answers based on selected correctness guidelines discussed in Section 1, then run EM, F 1 and BEM to evaluate the correctness of the candidate answers.We then compare the alignment between human judgments and the three evaluation methods, and list out pitfalls separately for EM, F 1 , and BEM.</p>
<p>EM is overly strict We analyze total 102 examples that EM considers incorrect but are actually correct under human judgments.EM suffers when the candidate answers are a good choice for the question but not in the reference lists.For example, 12 PM and 12 noon, Mount Everest and Mt.Everest, wetlands area and wetland are all failure cases for EM to detect AE.It becomes even more challenging for EM in the era of generative QA models where the answers are not exact extractions from the resource passages.EM greatly underestimates our selected machine QA model's accuracies.</p>
<p>F 1 Score is sensitive to thresholding We run F 1 evaluation with a threshold of 0.3, 0.5, and 0.7 to determine AE, where the number of misalignements with human judgments are 303, 51, 100.However, under a practical case when we use F 1 to evaluate QA answering models, the selection of threshold is critical and challenging depending on the purpose of the evaluation and the type of the dataset.F 1 also cannot evaluate AE by taking the context of the questions into account.An example would be Question: Will there be more or less blue ice at the glacier than the frozen over lake?Reference: more.Candidate: more blue ice, where additional information is provided but fades out the number of token overlap, resulting in an incorrect judgment for F 1 .</p>
<p>BEM suffers on OOD datasets BEM is trained on AE data generated from the Standford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), where it can detect AE beyond token-level matching and word similarity in the embedding space (BERT-Score).However, when testing it on an OOD dataset, where the testing data has a different style from the training data, BEM has 702 misalignments with human judgments.Specifically, BEM have the most number of misalignments with human judgments on examples where the reference and candidate answers are different but might be close in distance in the embedding space.more and less, Wednesday and Thursday, planet A and planet B, turn off and on are all examples where human judges it as incorrect but BEM judges it as correct.The limitation of current BERT-based evaluation methods face challenges in generalization to OOD datasets for its AE training data with less -well-defined correctness rules and restrictions.</p>
<p>Although the current QA evaluation methods work okay in most of the middle parts cases, there are still gaps between the human correctness decision making process and the current evaluation methods, and new evaluation methods adopted from expert human decision process are needed in the current era of machine QA</p>
<p>The New Evaluation Framework</p>
<p>In this section, we explore the process to revise AE rules from the NQAT (National Academic Quiz Tournaments, LLC, 2024), Jeopardy!(Carberry, 2019), EfficientQA (Min et al., 2021).We then introduce steps to augmenting our AE training set, and the selection and training details of our AE classifiers.</p>
<p>Revising AE Framework</p>
<p>We probe the limitations of current QA evaluation methods and show their lack of generalization in the era of generative QA models.Thus we need a more robust QA evaluation framework that can balance limitations of current evaluation methods.</p>
<p>Building AE and answer acceptability rules from scratch with limited data is challenging, meanwhile also not generalizable to possible diverse styles of questions asked.Thus, having a pointer to existing detailed AE guidelines can both ensure the rigorousness, diversity, and generality of our AE guidelines.Luckily, the professional Trivia community already have a set of well-defined norms to evaluate humans' ability in QA.NAQT is a thirtyyear-old purveyor of QA competitions spanning middle schools to "open" competitions that anyone can join.Because of the breadth of participants and judges (often drawn from parent volunteers of participants), the organization has developed easy-to-follow rules that make answer adjudication consistent (otherwise, participants would complain about unfairness).Likewise, the American game show Jeopardy!was forged in the crucible of the mid-century game show crisis (Stone and Yohn, 1992), and in addition to its quirky format where contestants' responses are gramatically questions, it has a complicated adjudication process to ensure fairness (McNear, 2020).</p>
<p>Expert rules in machine QA paradigm Since NAQT and Jeopardy!format is designed for real human players and is quite different from the current machine QA paradigms, a better practice than adopting all the rules is to revise them and fit them into te current machine QA settings.For example, the rules about pronunciation, the order of names, foreign language responses, etc are not considered suitable under a standard machine QA paradigms.The rules about pronunciations are specifically targeted for in-person or online audio versions of the tournament, where plausible pronunciations or phonetics are generally acceptable-koan and cone, mee-jee and mye-jye are considered equivalent under their rules.NAQT has very strict rules for the order of names, where Martin Luther King and King, Luther Marin are considered not equivalent but under our machine QA paradigm, we do not enforce the order of names and foreign language answers.On the other hand, the guidelines nonetheless provide rules we can reference in the machine QA paradigm.For example, the specificity rule is both present in the NAQT and Jeopardy!rules, where the responses should be specific enough under the context of a question-Question: Where is the Eiffel Tower located?-where the answer Europe is incorrect if the given reference answer is France, but is acceptable if Europe were the intended answer.</p>
<p>The revising process Thus we analyze misalignment of AE examples between human and selected automated evaluation methods, we also selectively analyze the aligned examples that all evaluation methods get right to ensure the generality and comprehensiveness of our AE guidelines.Our analy-  1 with additional examples in Appendix 9.</p>
<p>Training Data Augmentation</p>
<p>With these revised AE and acceptability rules, we want to train our AE classifier to follow these rules.Entity Aliasing Expansion This method is specifically targeted to correctness rule 1, where the reference and candidate answers are entities that have other possible aliases not in the current answer set.This step is for users who wish a more rigorous evaluation that can find more relevant and timely aliasing entities from Wikipedia to increase the chance of answer matching depending on their model deployment and usage purposes.For example, if the evaluation dataset is constructed in 2016, where the reference answer is only Burj Khalifa, but when the question What is the tallest building in the world? is asked today, the same building is also known for other aliasing names such as Khalifa Tower, Burj Dubai.In addition, a classifier trained on a fixed AE dataset without updates of aliasing examples would also be a challenge to detect more general AE.Thus, we also include Wikipedia entity linking features (Wikidata Contributors, 2019) into our evaluation pipeline to expand the reference and candidate answer set of our test sets if possible to improve AE generalization, but this step can be turned on or off depending on users' purposes and trade off of more resource required. 4</p>
<p>Feature Engineering and Classifier Training</p>
<p>QA evaluation is not a task like QA where the size of the model is a secondary consideration; rather, an effective method (i.e., that could be adopted) needs to be competitive as well.The goal of this section is to match the effectiveness of the prior LLM-based evaluation method while minimizing computation, latency, and disk space, thereby taking advantages of both LLM-based and standard evaluation methods.EM and F 1 evaluation methods are fast and require no disk space but they have the limitation of superficial token-level matching.LLM-based evaluation methods can go beyond token-level matching and be trained to aligned more human judgments, but the amount of computational resource, disk space, latency, bandwidth, computational time, and costs required also hinder their frequent usage in the standard QA training and evaluation pipelines.We want to not just build a system that does the best on AE task but one that does the best with minimal resources because in training QA systems, it will be used many times.In the rest of the paper, we provide details of construction and evaluation of CFMatch and compare it with LLM-based methods in computation cost, time cost, disk space cost, accuracy performance.Then we conduct human annotations and analysis of our evaluation methods.Feature Engineering To maximally take advantage of detecting AE from both the answers and semantic meanings of the questions, we follow encoding method used to train LLMs to format the QA pairs: [CLS] candidate [SEP] reference [SEP] question [SEP], where an encoding vectorizer will encode the [CLS] and [SEP] tokens as separator between questions and answers.We use unigram tf-idf (Jones, 2021) to encode our training data as pairs.In addition, F 1 score provides rich information about the amount of information and token overlap, precision and recall for each reference and candidate pair; we concatenate the F 1 score, precision and recall to the encoded unigram text features as inputs to our base classifier.We show example visualizations of QA pairs turned into features for LR:</p>
<ol>
<li>Truth and Justice [SEP] What is the name questions. 5 We use l2 penalty and a starting random state 668 for logistic classifier.We trained BERT with learning rate = 1e-5, weight decay=0.01,batch size=32 for 7 epochs and save the state with highest accuracy on the AE test dataset.We do not use BEM directly because we want the two BERT models have consistent training hyperparameters and details.</li>
</ol>
<p>of the book written by Anton Hansen Tammsaare?[SEP] [0.86, 1.0, 0.75] The text parts of the examples will be turned into td-idf unigrams and concatenated with F 1 score, precision, and recall vectors.The classifier automatically learns to capture semantic information in QA pairs while also learns an optimal F 1 score threshold decision for individual QA examples.Theoretically, a LR classifier does not have maximum input token length limit like LLMs do (usually 512 tokens), we can encode arbitrary length of text inputs including context as features, but this will not be in the scope of this paper.</p>
<p>Resource Requirements and Stability</p>
<p>This section investigates how extensive the resources AE methods need.While exact match is the least expensive, CFMatch requires far fewer resources than LLM-based approaches.Moreover, this helps show how important AE is: changing the metric changes the ordering of the top models on leaderboards.</p>
<p>OOD Evaluation Datasets</p>
<p>This section delves into the details of our selected OOD test sets not relevant to the training data.We use three benchmark datasets and one our challenging datasets as OOD datasets to validate that our classifier can generalize well to follow our AE rules.We list the introduction of the datasets below and examples in Table 2.</p>
<p>NQ-OPEN The Natural Questions Open dataset is an adoptation of the original Natural Questions dataset (Kwiatkowski et al., 2019, NQ), which is based on real questions from GOOGLE search users.</p>
<p>NARRATIVE QA is a Reading Comprehension dataset that provides narrative texts and requiring models to draw answers from the story context (Kočiský et al., 2018).</p>
<p>Hotpot QA HotpotQA is designed for the purpose of evaluating and enhancing machine comprehension and question-answering (QA) capabilities of artificial intelligence systems (Yang et al., 2018), where each question is encompassed with a long context.</p>
<p>Evaluation Method Evaluation</p>
<p>Candidate Answer Generations QA models have quite diverse performance with different model sizes.We select Flan-T5-xl (3B) (Chung et al., 2022), LLaMA 2 (7B) (Touvron et al., 2023), GPT-3.5-TURBO(black-box model) as our QA models and generate answers for the three benchmark datasets.We concatenate the contexts with the questions if available. 6  Answer Set Expansion We use pywikibot (Wikidata Contributors, 2019) to crawl entity aliases for the reference and candidate answers for all four datasets if they are available.We save the original test sets and the expanded test datasets that contain a list of reference and candidate answers with aliases.</p>
<p>Automated Evaluation Metric Results</p>
<p>We use EM, F 1 matching, BERT, improved BERT (BERT-Aug), logistic regression trained on original AE training set (CFM), and on the AE-augmented training (CFM-Aug) set as our QA evaluation methods and run them on the original test set with candidate answers (O) and the expanded test sets with aliases (E).Table 4 shows that all classifier-based methods have more relaxed fit in evaluating the correctness of candidate answers and achieve higher evaluation accuracies than standard evaluation methods when comparing on the same dataset and QA models.Classifier-based methods also achieve more relaxed fit and results in higher evaluation accuracy when they are trained on AE augmented datasets that contain well-defined AE rules.An additional exploration is that CFM with AE augmented dataset has a very comparable evaluation accuracy as BERTbased method.and the amount of computation resource needed are also factors researchers need to consider based on the amount of resources and time they have.We track the average evaluation time for each metric to complete evaluation for the three datasets (take the total time of evaluation of each QA model's response divided by the total number of models) shown in Table 3 and show that LR-based evaluation methods most effectively balance the advantage and shortcomings of standard evaluation metrics and LLM-based evaluation methods.7</p>
<p>Runtime and Model Size</p>
<p>Evaluation Method Intuition There are flips of model accuracies on NQ-OPEN and NQA when using different evaluation metrics shown in Table 4.</p>
<p>We observe that GPT-3.5-TURBO and LLaMA 2 7B both have worse evaluation accuracies than Flan-T5 xl (3B) when using the standard evaluation methods, but when using classifier-based matching methods, the order of model accuracies changes to GPT-3.5-TURBO being the best, followed by LLaMA 7B and Flan-T5 xl.The new model evaluation rankings intuitively makes more sense to us since LLaMA (possibly    expert judgments and use it evaluate the effectiveness of various QA evaluation methods.</p>
<p>Jeopardy! is a popular American television quiz game show.This show is famous for its distinctive formats of questions, where the questions covers diverse categories, and many of them require a knowledgeable human with years of expertise to be able to answer.An important note is that that there is a pipeline of people who have worked for NAQT have gone on to contribute to Jeopardy! in one way or another, the most notable of which is Ken Jennings, who became the first response adjudicator for NAQT (Tournaments, 2019).Thus the selected Jeopardy!challenge problems also conform to our AE rules, where we will remove examples with rules we did not adopt (pronunciation, name orders, etc). 9</p>
<p>9 Jeopardy!calls questions clues rather than questions, and the answers are responses.We rephrase the clues to be questions and response to gold answers to conform to machine Expert dataset details We collect 371 examples from J!Archive, a fans website to record every season of the Jeopardy!game with questions and answers presented. 10Each example in the dataset has a question, a reference answer, a candidate response by different Jeopardy!players, and expert judgment of the candidate answer.The dataset includes both difficult correct answers and difficult incorrect answer.E.g., the difficult correct answers are the ones the expert host ruled incorrect but was overruled by a panel.For example, given a question Your surgeon could choose to take a look inside you with this type of fiber optic instrument, the reference answer is laparoscope, but an endoscope was given and ruled incorrect during the show that later reverted to correct, verified by professionals and the general public in accordance with the Jeopardy!QA format.</p>
<p>10 https://www.j-archive.com/suggestcorrection. php?clue_id=353154 answer acceptability rules.The dataset includes roughly 68.46% correct examples and 31.54%incorrect examples, which we use to test the long-tail automated QA evaluation methods.</p>
<p>Alignment with Expert Judgments and Analysis</p>
<p>We first compare judgment alignments of the evaluation methods with gold expert labels, then manually annotate additional examples from the benchmark test sets and categorize our improved classifier-based evaluation method.</p>
<p>Challenge set evaluation The Jeopardy!dataset provides the most straightforward expert verification of the effectiveness of QA evaluation methods since it is a challenge set already contains expert human judgments of AE.This test set is challenging because the initial judgments of the responses were incorrect during the show plays, but they were reverted and corrected by professional judges or Jeopardy!fans during or after the show.The decisions to make the right judgment for an answer is even very challenging for experts since it requires a human with deep and broad knowledge and understanding to a subject, culture, and the question.An reverted example from the dataset is Question: I've seen the promised land.I may not get there with you.but...we, as a people, will get to the promised land... Gold Answer: Martin Luther King; Prompted Response: Martin Luther King in "I had a dream", where the response was initially ruled correct but later reverted because the talk in the question is not from I had a dream speech.We show the percentage of evaluation decisions of each metric aligned with human expert judgments in Table 5.</p>
<p>Classifier-based evaluation methods are generally more aligned with expert judgments Classifier-based methods except BERT are all more aligned with expert judgments than EM and F 1 , where CFM-Aug does the best on the original test set and BERT-Aug does the best on the expanded test set shown in Table 5.Although EM and F 1 are better than BERT by blindly judges every candidate response as incorrect without considering the context of the questions, the fourth figure in Figure 1 shows that EM completely fails on the Jeopardy!challenge set by producing a 0 evaluation accuracy for expert responses.During training stage of QA models, EM would not be a good fit as a valida-tion metric if the model constantly generate correct answers that are not in the reference list.</p>
<p>Expanding aliases can improve evaluation methods' judgment alignment with expert judgments</p>
<p>We see an improve of judgment percentage for all metrics after expanding the answer sets in EM fails to detect correct answers mostly for additional correct information in candidate answer 90% of the annotated samples are considered correct under human judgment are not correct using EM.Specifically, 52.4% of the candidate answers are correct under our rule 4, where additional information is provided: Elizabeth Hartman played the role of Selina D'Arcy is correct given question Who did an actress from The Secret of NIMH play in A Patch of Blue? and reference Selina D'Arcy".</p>
<p>Robustness of AE-Improved Classifier</p>
<p>This section categorizes all the annotated examples where classifiers trained with augmented set (CFM-Aug and BERT-Aug) consistent with human judgments but classifiers trained with original set (CFM and BERT) not consistent (83.6% of the selected examples).We provide the categories with example questions and answers, with explanation of our human judgement according to our correctness rules.</p>
<p>Specificity: candidate answers should contain at least certain specific information to be considered correct, any answers too general will be considered incorrect, and this is related to our rule 3 and 4. candidate: Joanna is working as a waitress.The she in the reference is referring to Joanna and they should be equivalent.</p>
<p>Weaknesses of AE-Improved Classifier</p>
<p>We analyze example where AE-improved judges inconsistent with human judgments and provide pitfalls of our improved AE methods (16.2% of the examples).</p>
<p>Irrelevant information: around 12% of the examples are in this category.This is related to our rule 6, where the candidate answers is irrelevant or not an accurate description to the question, which should be deemed incorrect.However, AE judges often consider irrelevant candidate answers as correct on the NARRATIVE-QA dataset, where it requires reasoning abilities to identify the correctness of candidate answers.For example, Vietnam War and Colonel Charlie Beckwith are considered equivalent by classifiers trained on the augmented dataset.</p>
<p>Specificity: this is around 3% of our annotated examples.When a candidate answer is too specific than a reference answer, where it excludes generality, it should be considered incorrect.Example: question: how oxygenated blood returns to the heart from the lungs [SEP] reference: pulmonary circulation [SEP] candidate: pulmonary venous system, where 'pulmonary circulation' refers to the entire process of blood flow through the lungs where blood is oxygenated.the pulmonary venous system does not specifically indicate the process of oxygenated blood returning to the heart, which is achieved through the pulmonary veins entering the left atrium, and is too specific for a particular part of the vein circulation system.</p>
<p>High token overlap: In addition, there is around 1.2% examples where improved classifiers consider correct for but indeed incorrect when the candidate and reference answers have high word overlap.English professional footballer and England international player are considered equivalent by CFM-Aug but not equivalent by CFM.</p>
<p>Related Work</p>
<p>Drawing on community expertise There has been a recent trend of using data from well-defined communities ethically (Shilton, 2016;Rogers et al., 2021), and the trivia community is no different.</p>
<p>While their data have been used for NLP tasksfrom Watson's tour-de-force on Jeopardy! to TriviaQA-the data have been used without attending to the norms of how the trivia community uses these data.As recognized by the human QA community, we should listen to them if we adopt their evaluations methods and data (Jennings et al., 2021), which the their methods are used to evaluate human QA, but our adopted is used to evaluate machines.</p>
<p>As Boyd-Graber and Börschinger (2020) argue, the goal of these competitions are designed to determine who is better at answering questions than another, and this is also the goal of modern leaderboards.Adopting these norms from the Trivia community to evaluate human in the machine QA research community can help judge answer equivalence, which is a fiendishly hard problem for generative AI.</p>
<p>Machine QA evaluations QA evaluations lie on a continuum-many are easy to evaluate via things like exact match, particularly for short answersbut especially for long-form QA, where there is a huge space of possible valid answers (Xu et al., 2023).Nevertheless, AE can also be difficult for short-form QA, where the judgment requires the understanding of the contexts of the questions or huge combination of valid output spaces (Bulian et al., 2022).One of the reason is that even short-form QA can be NLP-complete, where AE is especially challenging for surface token matching.Specifically, the challenge of AE connects many areas of AE can be applied to wide areas of NLP tasks such as a machine translation evaluation, entailment tasks, coreference resolution, etc. adopted rules from the Trivia community and an improved and more clear evaluation method are becoming more and more important than simple EM with continual improvement on transformer-base QA models (Zhu et al., 2021).Various benchmarks are proposed to evaluate the ability of QA models to answer questions (Wang, 2022).The benchmarks lead to improvement of QA models, and improved QA models can now answer questions much better by providing free-form answers or explanations that are more closely aligned with human preferences.However, with the continual development of machine QA models' ability to answer questions, the area to evaluation QA still falls behind, where most of research papers still use the old standard evaluation metrics-EM or F 1 .(Chen et al., 2019) and (Kamalloo et al., 2023a) both state there are pitfalls on the current standard evaluation metrics, where QA is not merely extracting exact answers from source texts, especially in the era of large generative models.(Chen et al., 2019) proposes BERT-based evaluation is a promising research direction since generative QA datasets becomes more abstract and free-form in nature.In addition, Wang et al. (2023a) and Chen et al. (2019) both show using BERT-based evaluation methods without training (BERT-Score) are not better or well-suited for QA evaluation tasks than standard evaluation metrics, where many datasets are also sensitive to BERT-Score threshold settings, and the boundary to determine the threshold is not well-defined for different datasets.(Bulian et al., 2022;Kamalloo et al., 2023a) validates fine-tuned BERT-based LLM on human annotated AE datasets is can better evaluate QA than standard metrics and closer to human judgments.</p>
<p>Although LLMs such as e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) have the advantage of being able to take the context of the questions and answers into account to evaluate AE, which show better correlation with human judgments than previous standard methods (Zhang et al., 2019;Bulian et al., 2022).However, Chen et al. (2019) shows the lack of generalization of LLMbased evaluation methods on different styles of QA datasets, particularly for out of domain QA datasets.(Kamalloo et al., 2023b) shows that generative GPTbased evaluation methods are closer to human judgments of AE than BERT-based methods, but they often have problems when dealing with factual information (Ji et al., 2023).Improving QA evaluation is critical and important not only in text-based QA, but also in multimodal QA.Mañas et al. (2023) shows that LLM-based evaluation methods can improve evaluation alignment with humans for visual QA, which is an even more challenging task than text-based QA with additional image-visual information.Yet the ability of generative LLM-based is not rigorously evaluated with clear definitions of AE and answer correctness.</p>
<p>In addition, (Thalken et al., 2023) shows that fine-tuned BERT on good expert annotated data in a particular domain is better than LLMs, especially GPT-4 on classification tasks.Current limitations of automated QA evaluation methods hold us from building more creative and intelligent QA models, therefore impeding the progress of solving QA (Rodriguez and Boyd-Graber, 2021).The alignment of LLM for human intent tasks has recently becoming more popular but involves several pipelines, which data collection will be the first step in the pipeline (Wang et al., 2023b).Aligning LLMs with human judgments also requires the data collection process.However, the construction of a 'good' dataset should draw well-defined rules and practices from the expert community, where (Thalken et al., 2023) collected expert legal datasets from the legal domain that successfully aligns BERT to be better aligned with human judgments than GPT-4 for their specific task.There have been dataset collected under (Bulian et al., 2022)'s definition of AE in short-form QA, but we still see clear pitfalls of their fine-tuned LLMs to evaluate AE that do not generalize well on OOD datasets in Section 2.2.The pitfalls of current QA evaluation methods advocates us to listen to the Trivia community norms and adopt their evaluation rules, so we can both con-struct better AE datasets under more fine-grained AE definitions that have been used and agreed by experts to evaluate humans over the decades, and making better and more efficient automated evaluation metrics that are more aligned with the AE definitions, which we have shown that a 812 KB classifier is even competitive with a 440 MB LLM.</p>
<p>Conclusion</p>
<p>Automated QA evaluation is an important pipeline of developing more robust and human favorable models.Better automated evaluation metrics drive for better QA models, where better evaluation methods can provide QA models' best gradient feedback and save its best training state.A better evaluation method during training time can be good for improving models' downstream task performance (accuracy, creativity) (Si et al., 2021), where a too strict evaluation method such as EM provides too much negative feedback to models during training time which results in lack of creativity of QA models and thus guiding models to provide exact extracted answers.However, the current pitfalls of QA evaluation methods (lack of correctness and AE guidelines) still hold us from solving QA and building more human preferable QA models that are not just trained to provide very specific answers to a question, but also more diverse and personalized answers.The way to build and select a more intelligent and creative QA model is by improving QA evaluation methods, where machines are judged not merely by rigid automated evaluation methods, but with more human-like automated evaluation methods.More human-like automated evaluation methods can scale up training, while not requiring humans to individually look at all the produced answers.By writing up a basic framework derived from QA human experts and adopt it to machine QA paradigm, we are able to enhance and generalize current automated QA evaluation methods that are more aligned with human judgments.In addition, our AE framework also improve human annotators' experience in judging the correctness of answers.Future work can explore automated QA evaluations that can also handle and take context passages of the question into account and develop a more advanced evaluation framework for long-form QA.</p>
<p>Our classifier-based evaluation methods trained with AE data containing our framework examples can better justify equivalence of candidate answers given a reference answer than current popular EM, F 1 and BERT-based models.However, our new method still possess several challenges and limitations.Our models are not trained to take contexts of the question into account, where if a question has a context paragraph, our model can only the question, reference answer, and candidate answer to evaluate equivalence, which the effectiveness of our approach is still unexplored for QA with contexts.Another limitation of the lack of diversity of QA models we used in our experiments, where different QA models have diverse range of style to answer questions.We also have a lack of enough annotated examples for our tasks, which we are under the process of hiring more annotators.In addition, our definition of AE has not taken subjectivity, demographic diversity, bias, and cultural diversity account, with cases where the judgments of correctness can be different for people with differnt backgrounds.and answers that are answered by Jeopardy!players that are originally judged correct/incorrect, but with later revised decisions, where those overruled answers are carefully verified and discussed by audiences or experts.We collected a total of 371 such examples from J!Archive use them as the challenge set.</p>
<p>11 Comparing Alignment with  Judgements</p>
<p>One critique is that we cannot validate our hypothesis that a change of accuracy of the automated evaluation method improves its actual ability to detect AE (Si et al., 2021).We fine-tune -GPT-TURBO-3.5 on our augmented training set as an approximate human judge and compare the percentage of alignment between GPT-3.5-TURBO and the evaluation metrics discussed in our paper.</p>
<p>Alignment with fine-tuned GPT Kamalloo et al. (2023b); Wang et al. (2023a) both evaluated that LLM-based evaluators are closest to human judgments and can serve as a compliment as human judge but have the disadvantage of being sensitive to prompts and their costs and latency.Although we are not considering GPT-3.5-TURBO as our gold standard judges here, we compare how our small classifier (&lt;1 MB) can generalize and align with current LLM judges, which is more efficient and less cost.We first fine-tune GPT-3.5-TURBO with our augmented AE dataset with prompts including the rule descriptions, QA pairs, and judgment of correctness.Then we use fine-tuned GPT to provide judgments to the three original un-expanded benchmark test sets and compare the alignment counts of each evaluation method with a fine-tuned GPT model in Figure 2, which shows the alignment percentage variances across models for the same dataset, and the alignment percentage of each evaluation method.We calculate the alignment variance for each metric based on the percentage of alignment with GPT-3.5-TURBO on the three QA models under the same data set and normalize it by dividing 100. Figure 2 shows that classifier-based evaluation methods trained on augmented AE data overall have smaller judgment variances and higher judgement alignment with current fine-tuned stateof-the-art LLM.The dress cost more than the hat</p>
<p>The item Sarah spent less on was the hat Correct under rule 6 with semantic equivalence Is Wall Street doing well this year?</p>
<p>The financial markets have seen a significant upturn he stock market has been performing positively Correct under rule 1 and 6.The Wall Street is an metonymy of the stock market under the context of the question.</p>
<p>Table 6: We list out more relevant AE pairs with judgments under our revised rules.Some of the candidate responses are manually written, some of them are from Jeopardy!, and some of them are generated by various QA models such as Flan-t5.</p>
<p>[CLS] Volume one [SEP] one [SEP] What volume of The Green Book discussed democracy?[SEP] [0.67, 0.5, 1.0] 2. [CLS] pentalogy Truth and Justice [SEP]</p>
<p>Figure 1 :
1
Figure 1: Standard evaluation methods fall behind classifier-based evaluation methods on the original unexpanded test sets.The accuracies of CFM evaluation trained on augmented AE dataset is closer to the accuracies of BERT-based evaluation methods than CFM trained on the un-augmented training set.</p>
<p>Figure 2 :
2
Figure2: Classifier-based evaluation methods with augmented AE training set are more stable than standard evaluation methods on different models.In addition, LR-based methods trained on our augmented dataset is close to or even better than BERT-based methods' judgment alignment with our fine-tuned AE GPT-3.5-TURBO with constantly at least 80% of alignment.On the other hand, standard evaluation methods are unstable with the change of QA models or datasets.The error bars are the normalized variances of alignment percentage for the three QA models under the same dataset.</p>
<p>Table 1 :
1
examples include 1,688 examples from ROPES dataset and select 600 examples in the official AE test set that have misalignment between human judgments and the other two evaluation methods.We also pick 200 additional examples from the AE test set where both BEM and human judgments align.We select, categorize and revise rules from the Trivia community that can fit into our current sample analysis and present a comprehensive category of our modified correctness rules that can fit into the current machine QA paradigm in Table
RuleDescriptionQuestionReferenceCandidateWidely recognized aliases,who plays Red onKate MulgrewKatherineMaria1:entity-pseudonyms, or alternative namesOrange is the NewMulgrew/correctaliasingthat are commonly associatedBlack?aliasing namewith the reference answer entitieswhere can I get a stateDMVDepartment of Mobileare acceptableissued id?Vehicles/incorrectaliasingExact dates, years, numerical areWhat year did WorldSep 2, 19451945/ correct exact nu-2:numericalrequired unless the questionWar II end?mericalinformationspecifically asks forHow tall can a giraffe16-20 feet18 feet/ incorrect/ mustapproximationsgrow?be a rangeThe candidate answer providesWho discovered peni-ScottishAlexander Fleming/3:less detailsessential and correct information less details but should include thecillin?Alexander physiciancorrect within contextrequired by the questionFleming(specificity 1)What followed the lateThe Late LateThe Late Show/ incor-local programming af-Showwithrect missing importantter Super Bowl 50?James CordeninformationCandidate answer contains theWhen did Morales2009August 3, 2009/ cor-4:morereference answer or an equivalentlaunch his policy inrect exact date anddetailspart. The additional informationthe eastern lowlands?timemust be factually correct and doesProtective coloring isbeetlebeetle and formicidae/not contradict the question orcommon in what insectincorrect additional in-reference answer (Specificity 2)family?formationA high degree of word overlap orWhat is the primarySolar radiationSolar energy from sun/5:semanticsimilarity does not establishsource of energy forfrom suncorrect semantic withequivalenceequivalence. The answer must bethe Earth?high overlapcontextually and semanticallyWhich group has agroup Agroup B/ incorrectaccuratehigher likely hood ofgroupbrain damage?6:irrelevant, inaccurate informationThe candidate answer is irrelevant to question and the given answer or is an inaccurate description of the question should be incorrectWhich event occurred first, the first Super Bowl or the forma-tion of the Premierfirst Super Bowlformation Premier League/ in-of the correct with inaccurate answerLeague?7:otherWhen did the Golden2015, 20172022/ another possiblePossibleCandidate response is correct butState Warriors win thecorrect answeranswersnot in the reference listNBA Championship?
The correctness of a candidate answer can be traced and categorized to one or more of the above rules.We provide a comprehensive guideline to define correctness and equivalence of answers, where the rules are adopted mainly from NAQT and slightly modified upon analysis of annotated dataset with human correctness judgments between reference answers and machine generated answers.The acceptability of QA model answers should be based on theses guidelines.Our AE classifiers will be also trained on datasets with theses acceptability rules.sis</p>
<p>(Ren et al., 2023)ines, and error examples from the AE test set and the ROPES dev set.Then we manually revise QA pairs that violate our AE guidelines, and to ensure the quality and diversity of our example QA pairs.We end up writing five to ten examples for each rule, where also balance the number of correct and incorrect pairs except for rules that are strictly judged incorrect (Table1).At the end of each generation iteration, we append the generated examples to the seed examples, which we generate a total of 4,234 synthetic AE examples.Quality validation To verify the automatically generated examples, we manually select fifty generated AE examples and check the judgments.GPT-4's disagreements with humans is most frequent with Rule 2 from Table1, e.g., GPT-4 often considers numerical answers like 2008-2010 and 2010 to be equivalent.For each generated example, we run GPT-4 self-evaluation by prompting it to verify the correctness of its generated judgments with the rules, questions, reference answers, and candidate answer, and judgment to improved the quality and accuracy of judgments(Ren et al., 2023).We re-check the 50 generated examples that have inconsistent generated and self-evaluated judgments, and see an improvement of the judgments-39 out of 50 example are consistent with human judgments after self-evaluation.Moreover, we select 300 additional generated examples (mainly examples on Rule 2) and manually check and revise the self-corrected judgments to accommodate our AE guidelines.180self-verifiedjudgments are revised and 65 invalid examples (false reference answers, invalid questions) are removed with 3,989 examples remaining in our training material.After two steps of validation, we are confident that the synthetic data mostly align well with our AE rules.Extra Human Annotated Data AugmentationIf the first step of data augmentation is to collect representative materials to fit the classifier to follow AE norms, then we also want to generalize and teach the classifier with more real human annotated judgments.Luckily, we already an existing annotated AE dataset available.We expand the AE learning material by combining our 3,989 synthetic examples with 9,090 examples from the AE training set, with 1,688 annotated examples from the ROPES dev dataset as our AE-augmented training set.</p>
<p>Thus, we generate AE examples to train a classifier to learn our AE patterns.For each AE rule, we collect a small number of example QA pairs from the NAQT Training data generation We use the manually revised QA pairs as seed examples to promptwith an example and the specific AE rule to generate more similar QA pairs with its judgment of equivalency and correctness.3</p>
<p>Table 2 :
2
Bulian et al. (2022) the table is our error analysis dataset and we use the rest four datasets for our generalization testing.We do not include contexts for NQ-OPEN for more variability of generated answers to challenge the evaluation methods.Classifier Selection We use logistic regression (LR) as our base classifier for its simplicity and efficiency.Additionally, we use BERT-base-uncased as our comparison model and train one on the original AE dataset and another on the augmented AE dataset followingBulian et al. (2022)'s training format.5
Dataset# PairsContextQuestionGold AnswerMichael Paul Welch (born August 25, 1972) isMike Welch at-ROPES1,688a former right-handed pitcher ... was named a 2014 "Best High School" by U.S. News andtended high school that serves how2,200 studentsWorld Report.many students ?NQ-OPEN3,610-who won the Amer-ican league east in 2017The Houston Astros Yankees,Asmara International Airport (IATA: ASM)Asmarainterna-HotpotQA3,300(ICAO: HHAS). Asmara currently hosts thetional airport is inEritreacountry's only operating international airport...which country?The narrator, a Bostonian, returns after a briefNarrative-QA3,300visit a few summers prior, to the small coastal town ... cMaine coastal town increases eachWhat was Lit-tlepage's job?sailor, sailorretiredday.</p>
<p>Comparison The runtime, space an evaluation method or model needs,
MethodO RuntimeE RuntimeDisk SpaceEM2 s4 s0F12 s4 s0CFM14 s25 s714 KBCFM-Aug14 s25 s812 KBBERT (GPU)10 mins29 mins440 MBBERT (CPU)98 mins189 mins440 MB
6 https://openai.com</p>
<p>Table 3 :
3
CFM-Aug can balance between efficiency and performance with few increase in inference runtime and disk space than standard evaluation methods.The O
runtime calculates the average runtime on the originaltest sets for 10,210 (3610+3,300+3,300) examples onthree generation models, and E runtime is the averageruntime on the expanded test sets.</p>
<p>Table 4 :
4
A change of evaluation methods can flip the order of QA models!The table shows the the automated QA evaluation accuracy in percentage(%) using different evaluation methods for our four selected test datasets and three QA models.Jeopardy!dataset already has candidate answers from Jeopardy!players.The O column indicates evaluation accuracy on the original test sets with generated candidate answers, and the E column indicates evaluation accuracy on the test set with expanded reference and candidate answers sets.GPT-3.5-TURBO and LLaMA 2 7B have worse accuracies than Flan-T5 xl (3B) on the NQA and Hotpot QA datasets when using EM, but they become close to or better than Flan-T5 xl after using classifier-based methods.In addition, all of the evaluation methods have at least a slight increase in accuracy on the expanded test sets.EM COMPLETELY FAILS on the original expert answer challenge sets with a 0% accuracy discussed in Section 5.
are newer re-</p>
<p>Table 5
5.</p>
<p>Table 5 :
5
CFM-Aug and BERT-Aug(trained on augmented dataset) have the highest alignment with human judgments, which are all better than standard evaluation metrics and BERT and CFM.The table shows the percentage of AE evaluation methods aligned with expert human judgements.The Jeopardy!O is the original test set with one reference and candidate answer for each example, and Jeopardy!E is the expanded test set withIn the following section, we conduct a systematic error analysis of selected examples and analyze them in Three parts.The first part will select examples of inconsistent judgments for between CFM and CFM-Aug , and BERT and BERT-Aug, we then manually annotate those selected examples based on Table1rules and calculate the number of examples that humans judge correct but EM judges incorrect.The second part show the improvement and examples of CFM and CFM-Aug over BERT and BERT-Aug based on our annotation.The third part discusses examples where BERT-Aug and CFM-Aug do worse than BERT and CFM.We have two annotators manually annotated 250 examples randomly selected from the both BERT-based matching and CFM methods across all three datasets and all three models.
possibly many reference and candidate answers for eahexample. All methods have a higher alignment withhuman judgments after expanding the answer sets forJeopardy.5.2 Equivalence Error Analysis ofclassifier-based AE methods</p>
<p>About 90% of the examples put into this category.The candidate answer is correct with extra detailed information.However, BERT judges often think that they are not equivalent.On the contrary, candidate answers should not be too specific that excludes the generality of the reference answers.BERT methods also often make mistakes when the reference and candidate answers are semantically close in the word embedding space.Additional examples are apple and pear, king and queen, etc, where BERT considers them as equivalence but BERT-Aug does not.Aliases, co-reference: around 5.6% examples are in this category.One entity may have multiple names or commonly known names as referred to our rule 1. Examples are Eric Arthur Blair and George Orwell, where Eric Arthur Blair is a pen name of George Orwell.AE judges also struggle with questions that requires co-referencing understandings.An example question would be question: What job is Joanna doing when she meets Peter for lunch?[SEP] reference: She is a waitress[SEP]
despite the variation in their phrasing. and CFMconsiders equivalent.Semantic Similarity: around 20.4% examplesare in this category. Candidate answers that havethe same meaning as the reference and justifiesthe question are usually considered correct, refer-ring to our rule 5. Example question would bequestion: How does Delacroix defend his show?[SEP] reference: By labeling it satire [SEP] candi-date: as being satirical. The reference and candi-date answers both indicate that Delacroix's defensestrategy for his show hinges on characterizing it assatirical. They imply a similar underlying intent
Example: question: What flatten's Aron's hand and wrist?[SEP] reference: Bergen [SEP] candidate: Bergen county, New Jersey.</p>
<p>What is the capital of France?The capital of France is Paris Paris is the largest city in France
QuestionReferencesCandidateJudgmentWhere does Pam find Jim isIn a bathtub while in Paris orParis, FranceIncorrect based on rule 4,dead?In a bathtub in Parisspecificity and details.What flatten's Aron's handA boulderA rockIncorrect based on rule 5, aand wrist?boulder refers to a large rockbut a rock can be any size.Born on 6 March 1937, thisAlistair GrantSir Matthew Alistair GrantCorrect based on rule 1: com-man was also a citizen of themon aliases.United Kingdom.What is the tissue type of theEpitheliumEpithelial tissueCorrect under context basedinner most layer of cells?on rule 6.In which 2017 film didRealThe South Korean actor ap-Correct based on rule 5 withthis South Korean actor ofpeared in the 2017 filmextra accurate details."Dream High" (2011) and the"Real"2012 South Korean televisiondrama series featuring HanGa-in, Jung Il-woo and KimMin-seo appeared in 2017?Are Nikolaschka and WhiteNoNo, Nikolaschka and WhiteCorrect under rule 5.Russian made of beer?Russian are not made withbeerReggaetón Lento is a songDecember 13, 2015December 13, 2015Incorrect based on rule 2 andby the boy band formed on4. Date and numerical shouldwhich date?be exact.What percentage is 50 grams25%25.01%Incorrect based on rule 2.of a 200 gram total weight?How are Tessa and Tito con-They are secretly married andMeganIncorrect under rule 7, com-nected?have two children or They arepletely irrelevant answers.marriedIncorrect based on rule 6:amount of token overlap ishigh with F1 score 0.62 butreferring to different informa-tion.Your surgeon could choose tolaparoscopean endoscopeCorrect under rule 1 and 4:take a look inside you with"endoscope" encompasses athis type of fiber optic instru-variety of instruments usedmentfor viewing different internalparts of the body, not limitedto the abdominal or pelvicarea.I've seen the promised land.Martin Luther King in "I hadMartin Luther KingIncorrect under rule 5, theI may not get there with you.a dream"question is not from thebut...we, as a people, will getspeech.to the promised landThe livers of geese that arefoie grasfoie de grasCorrect under rule 6 becauseoverfed underexercised arefoie de gras means liver of fat,used to make this pate sub-and convey the same semanticstanceequivalence under the contextof the question.Sarah bought a dress and ahat. Which item was moreexpensive?
https://github.com/zli12321/qa_metrics.git
We acknowledge that T5 is no longer a SOTA model, but since our goal is distinguish correct from incorrect answers, our goal is to have a relatively inexpensive model to run that is replicable but still different from the extractive models that can produce enough free-form answers.
We set the temperature of GPT-4 to 0.9 and frequency penalty to 1.9 to ensure the diversity of generated QA pairs.
Our CPU device is a AMD EPYC 7402P 24-Core Processor and GPU is a single A6000.
  8  We use the newest GPT-3.5 model released on November 6th, 2023. LLaMA 2 7B is released in Feb 2023, and Flan-T5 series is released in Dec 2022.
Jeopardy! Data Collection PipelineThe J!Archive is a fans website to record every season of the Jeopardy! game with questions answers presented
.11  The website also includes questions 11 https://www.j-archive.com/suggestcorrection. php?clue_id=353154</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>What question answering can learn from trivia nerds. Jordan Boyd, -Graber , Benjamin Börschinger, 10.18653/v1/2020.acl-main.662Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, Tal Schuster, arXiv:2202.076542022arXiv preprint</p>
<p>Matt Carberry, Jeopardy! casebook. 2019</p>
<p>Evaluating question answering evaluation. Anthony Chen, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/D19-5817Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question AnsweringHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Can ken jennings answer 100 questions in 10 minutes? | jeopardy trivia challenge. Ken Jennings, Richard Garfiled, Sierra , 2021. Jan 14. 2024</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Computing Surveys. 55122023</p>
<p>A statistical interpretation of term specificity and its application in retrieval. Karen Spärck, Jones , J. Documentation. 602021</p>
<p>Evaluating open-domain question answering in the era of large language models. Ehsan Kamalloo, Nouha Dziri, Charles Clarke, Davood Rafiei, 10.18653/v1/2023.acl-long.307Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Evaluating open-domain question answering in the era of large language models. Ehsan Kamalloo, Nouha Dziri, L A Charles, Davood Clarke, Rafiei, arXiv:2305.069842023barXiv preprint</p>
<p>Dense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>The NarrativeQA reading comprehension challenge. Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette, 10.1162/tacl_a_00023Transactions of the Association for Computational Linguistics. 62018</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Reasoning over paragraph effects in situations. Kevin Lin, Oyvind Tafjord, Peter Clark, Matt Gardner, 10.18653/v1/D19-5808Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question AnsweringHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Improving automatic vqa evaluation using large language models. Oscar Mañas, Benno Krojer, Aishwarya Agrawal, arXiv:2310.025672023arXiv preprint</p>
<p>Claire Mcnear, Answers in the Form of Questions. Twelve. 2020</p>
<p>Neurips 2020 efficientqa competition: Systems, analyses and lessons learned. Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palomaki, Colin Raffel, Adam Roberts, Tom Kwiatkowski, Patrick Lewis, Yuxiang Wu, Heinrich Küttler, Linqing Liu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, Sohee Yang, Minjoon Seo, Gautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Edouard Grave, Ikuya Yamada, Sonse Shimaoka, Masatoshi Suzuki, Shumpei Miyawaki, Shun Sato, Ryo Takahashi, Jun Suzuki, Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz, Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal Gupta, Yashar Mehdad, Wen-Tau Yih, Proceedings of the NeurIPS 2020 Competition and Demonstration Track. the NeurIPS 2020 Competition and Demonstration TrackPMLR2021133Proceedings of Machine Learning Research</p>
<p>National Academic Quiz Tournaments, LLC. 2024. Correctness guidelines. </p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250Squad: 100,000+ questions for machine comprehension of text. 2016arXiv preprint</p>
<p>Self-evaluation improves selective generation in large language models. Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, Balaji Lakshminarayanan, 2023</p>
<p>Evaluation paradigms in question answering. Pedro Rodriguez, Jordan Boyd-Graber, 10.18653/v1/2021.emnlp-main.758Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<ol>
<li>'just what do you think you're doing, dave?' a checklist for responsible data use in NLP. Anna Rogers, Timothy Baldwin, Kobi Leins, 10.18653/v1/2021.findings-emnlp.414Findings of the Association for Computational Linguistics: EMNLP 2021. Punta CanaDominican Republic. Association for Computational Linguistics</li>
</ol>
<p>Emerging ethics norms in social media research. Katie Shilton, 2016Big Data Ethics</p>
<p>What's in a name? answer equivalence for opendomain question answering. Chenglei Si, Chen Zhao, Jordan Boyd-Graber, 2021</p>
<p>Joseph Stone, Tim Yohn, Prime Time and Misdemeanors: Investigating the 1950s TV Quiz Scandal: A D.A.'s Account. New Brunswick, N.J.Rutgers University Press1992</p>
<p>Modeling legal reasoning: Lm annotation at the edge of human agreement. Rosamond Thalken, Edward H Stiglitz, David Mimno, Matthew Wilkens, arXiv:2310.184402023arXiv preprint</p>
<p>Why does ken jennings play quiz bowl? Online Video. 2019National Academic Quiz Tournaments</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>The TREC-8 question answering track. Ellen M Voorhees, Dawn M Tice, Proceedings of the Second International Conference on Language Resources and Evaluation (LREC'00). the Second International Conference on Language Resources and Evaluation (LREC'00)Athens, GreeceEuropean Language Resources Association (ELRA2000</p>
<p>Evaluating open-qa evaluation. Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, Yue Zhang, 2023a</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, 2023b</p>
<p>Modern question answering datasets and benchmarks: A survey. Zhen Wang, arXiv:2206.15030Wikidata Contributors. 2022. 2019arXiv preprintPywikibot -python 3 tutorial/data harvest</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, ArXiv, abs/2305.182012023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:2101.00774Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. Fengbin Zhu, Wenqiang Lei, Chao Wang2019arXiv preprint</p>
<p>AE and Correctness Guideline Examples We include more typical QA example pairs in Table 6 with judgments and explainations aligning with our rules in table 1. </p>            </div>
        </div>

    </div>
</body>
</html>
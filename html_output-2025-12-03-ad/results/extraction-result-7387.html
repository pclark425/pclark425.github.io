<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-276902996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.16701v2.pdf" target="_blank">ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models</a></p>
                <p><strong>Paper Abstract:</strong> The use of Large Language Models (LLMs) in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop ClimaGen (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop. As a result, we present ClimaQA-Gold, an expert-annotated benchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA dataset for climate science. Finally, we develop evaluation strategies and compare different LLMs on our benchmarks. Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs. The source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo (generator/solver)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction‑tuned transformer LLM used in this work both to generate synthetic climate QA pairs from textbook contexts and as a baseline/question-answering model evaluated on the ClimaQA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / undisclosed (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned; used as QA-generator and QA-solver (zero-shot / few-shot / RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Earth science / Climate science (graduate-level textbook content; aerosols, clouds, atmospheric processes)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of climate reasoning via: (a) automatic generation of multiple-choice, freeform, and cloze question-answer pairs from textbook contexts; (b) answering climate QA (MCQ, freeform, cloze) as a text-based reasoning simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Generator: prompted with a 2000-character seed context plus additional retrieved chunks (cosine-similarity) and rule-guided prompt templates to produce base and complexity-evolved QA pairs; Solver: evaluated in default (zero-shot), few-shot (in-context examples), and Retrieval-Augmented Generation (RAG) modes; MCQ responses constrained to single-letter output; Freeform constrained to concise (<=2 sentence) answers; Cloze constrained to single word.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>For generation: human expert validation (valid/invalid labels). For QA solving: MCQ accuracy (% correct), Cloze Exact Match (EM) and Phrase Similarity (PS), Freeform BLEU, BERTScore, and Factual Accuracy (entailment confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>As solver on ClimaQA-Gold: MCQ overall accuracy 73.06%; Cloze EM 43.12%; Cloze PhraseSimilarity 0.81. Freeform (ClimaQA-Silver) BLEU 0.467, BERTScore 0.523, Factual Accuracy 0.545. As generator: ~25% of generated MCQs were incorrectly answered by the same model even when contexts provided (self-inconsistency), implying ~75% context-answering consistency in that analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-context solver performance reported (e.g., MCQ baseline for gpt-3.5-turbo 73.06% overall). Generation baseline: raw LLM generation prior to expert validation (no single baseline model beyond human validation).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['use of retrieval (RAG) from source vs held-out textbooks', 'prompting strategy (zero-shot vs few-shot)', 'quality of seed context and retrieved chunks', 'self-inconsistency/hallucination in generation', 'expert validation / automated evaluator feedback loop']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>MCQ output: single-letter top-token chosen; Freeform: max 2 sentences; Cloze: single word; generation used 2000-character seed contexts + retrieved chunks by cosine similarity; RAG evaluated in two retrieval pools (source books and held-out books).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Generator exhibited self-inconsistency and produced scientifically inaccurate QA pairs (~25% incorrect MCQs). Solver performance degrades on reasoning-style MCQs; hallucinations and imprecise domain terminology appear when domain context is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini (evaluator / entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o-mini (fine-tuned variants used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller instruction-tuned GPT-4o variant used in ClimaQA both as a factual-entailment classifier to score factual accuracy of freeform answers, and fine-tuned as an automated evaluator to label generated QA pairs valid/invalid and to pick cloze mask terms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / mini variant (undisclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned; fine-tuned classifier (evaluator) on expert labels; used for entailment scoring</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Earth science / Climate QA validation (used to judge factual support/contradiction relative to reference answers)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Textual factual-entailment classification (SUPPORTS vs REFUTES) to simulate expert verification of model answers and to automate cloze-mask selection.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot textual entailment prompt: present Evidence: ⟨evidence⟩ Claim: ⟨claim⟩ and respond SUPPORTS/REFUTES; for scoring the claim confidence logits were sigmoid-smoothed with temperature T=5. For evaluator fine-tuning, trained as a classifier on expert-annotated valid/invalid labels and used uncertainty-based active sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Entailment classification accuracy on Climate-Fever (quoted 81% zero-shot). For evaluation automation: percentage of generated QA pairs labeled valid (used as proxy for generation quality); improvement in dataset quality measured as % increase in valid items after evaluator intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Zero-shot entailment on Climate-Fever: 81% (paper cites this for the method). Evaluator outcomes in ClimaGen: after automated annotation they observed ~85% of MCQs and ~90% of freeform QAs were labeled valid; fine-tuned evaluator increased MCQ-valid set quality by ~10% and Freeform by ~5%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Human expert validation (gold labels) used as ground truth; zero-shot entailment baseline was 81% on Climate-Fever referenced dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['quality and size of human-labeled training data used for fine-tuning', 'uncertainty-based active sampling and confidence thresholds (confidence>0.85 handling)', 'domain specificity of training texts', 'prompt formulation for entailment (support/refute framing) and temperature scaling']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Confidence threshold for dropping samples: 0.85; samples above threshold dropped with 50% probability in active sampling; logit smoothing with temperature T=5 to scale support/refute logits into [0,1]; evaluator models were fine-tuned separately for MCQ and freeform tasks with limited labeled sets (test sets ~40 examples in some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Automated evaluator quality depends strongly on limited expert-labeled data; variation in human expert labels can introduce variability; automated evaluator may still mislabel edge cases and cannot fully replace domain experts without larger labeled corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (solver)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-tuned LLM evaluated as a climate QA solver across MCQ, freeform, and cloze tasks, showing top performance in the ClimaQA benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / undisclosed (OpenAI flagship family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned; evaluated in default, few-shot, and RAG settings</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science (graduate textbook-derived questions)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of climate reasoning: answering multiple-choice (MCQ), freeform explanatory, and cloze completion questions requiring factual knowledge and scientific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot default; few-shot in-context prompting tested; Retrieval-Augmented Generation (RAG) using either source textbooks (used to generate questions) or held-out textbook pools; MCQ answers constrained to single-letter outputs; Freeform constrained to concise answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy (% correct), Cloze Exact Match and Phrase Similarity, Freeform BLEU, BERTScore, and Factual Accuracy (entailment confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ClimaQA-Gold reported: MCQ overall accuracy 85.71% (default), Cloze EM 53.12%, Phrase Similarity 0.88. With RAG-source: MCQ overall 92.79%, Cloze EM 71.88%, PS 0.94. Freeform metrics (ClimaQA-Silver) BLEU 0.440, BERTScore 0.493, Factual Accuracy 0.491.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-context baseline reported (MCQ 85.71%); few-shot produced marginal improvements; RAG-source produced the best improvements (e.g., MCQ → 92.79%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['retrieval from source vs held-out textbooks (RAG-source gave large improvements)', 'task format (MCQ reasoning items are harder)', 'few-shot marginal gains', 'model instruction-tuning and inherent model capability']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>MCQ output: single-letter top-token; Freeform: <=2 sentences; RAG scenarios: rag-source (5 source books used to generate questions) vs rag-held-out (13 other texts); long-context (Gemini 1.5) also tested separately; no paper-specified temperature for API calls shown.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>While GPT-4o dominated overall, reasoning MCQs and held-out retrieval scenarios showed reduced performance; BLEU/BERTScore could be biased toward the model used for QA-generation (gpt-3.5-turbo) whereas Factual Accuracy metric gave more reliable signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7387.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gemma-27b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma 27B (open-model family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 27B-parameter model evaluated on ClimaQA as an off-the-shelf solver in default, few-shot, and RAG settings; used to assess open weights model performance on fine-grained climate QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gemma-27b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base/instruction-tuned (open-source family); evaluated as-is and with few-shot and RAG</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA (textual reasoning over graduate textbook content)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based answering of MCQ, freeform, and cloze climate questions to simulate domain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Default zero-shot; few-shot in-context prompting; RAG with source and held-out textbook retrieval; MCQ single-letter output constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ClimaQA-Gold: MCQ overall 79.18%, Cloze EM 49.38%, PhraseSimilarity 0.87. ClimaQA-Silver: MCQ overall 79.5%, Cloze EM 50.00, PS 0.85. Freeform (ClimaQA-Silver) BLEU 0.392, BERTScore 0.441, Factual Accuracy 0.365.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-context baseline (MCQ 79.18%). RAG-source improved MCQ to 90.48% (Table 3 variant).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['RAG from source textbooks strongly improves accuracy', 'few-shot provides marginal gains', 'sensitivity to irrelevant retrieved context (held-out retrieval sometimes reduces performance)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Variants evaluated: default, few-shot, rag-source, rag-held-out. MCQ/Freeform/Cloze prompting constraints as described above.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Freeform factual accuracy low relative to closed-source top models; vulnerable to distractor or irrelevant retrieval; some solver responses were irrelevant but scored high on surface lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7387.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open-source 70B-parameter LLM evaluated on ClimaQA across prompting and retrieval configurations, showing strong MCQ performance especially with RAG from source textbooks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source large model (instruct-style inference via TogetherAI), evaluated default/few-shot/RAG</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based answering of QA pairs (MCQ/freeform/cloze) simulating climate reasoning over textbook contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Default zero-shot, few-shot (in-context), RAG-source and RAG-held-out; outputs constrained per task (MCQ single-letter, Freeform concise).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform lexical/semantic/factual metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ClimaQA-Gold default overall MCQ 83.27%; Cloze EM 38.75; PS 0.82. With rag-source: MCQ overall 88.98%; Cloze EM 63.12; PS 0.91. Freeform (ClimaQA-Silver) BLEU 0.335, BERTScore 0.474, Factual Accuracy 0.569.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-context baseline MCQ 83.27%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['retrieval from source books (RAG-source) yields substantial gain', 'few-shot gives modest improvements', 'task-format sensitivity (reasoning items harder)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated across default, few-shot, rag-source and rag-held-out; standard prompt constraints for each task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Cloze EM lower relative to other models in some settings; performance sensitive to irrelevant retrieval when using held-out books.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7387.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mixtral-8x22b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (Mixture-of-Experts) 8x22B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixture-of-experts style open model (mixtral) evaluated as a climate QA solver; shows competitive performance with improvements under RAG-source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mixtral-8x22b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8×22B experts (Mixture-of-Experts design; name denotes expert shards)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source MoE model evaluated default/few-shot/RAG</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Textual climate QA answering (MCQ/freeform/cloze) simulating reasoning across textbook-derived questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Default, few-shot, RAG-source and RAG-held-out prompting; same task output constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ClimaQA-Gold MCQ overall 80.00%; Cloze EM 35.62; PS 0.75. With rag-source MCQ improved to 84.90% (Table 3). Freeform (ClimaQA-Silver) BLEU 0.394, BERTScore 0.516, Factual Accuracy 0.485.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-context MCQ 80.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['benefit from RAG-source retrieval', 'sensitivity to held-out irrelevant retrieval', 'few-shot yields small gains']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated in default/few-shot/RAG modes with generation constraints; no explicit temperature reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower Cloze EM and PhraseSimilarity relative to best-performing models; held-out retrieval sometimes degrades performance significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7387.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1-8B-Instruct (cp & ft)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3.1-8B-Instruct (continued pretraining and fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>8B-parameter instruct-tuned LLaMA variant used for experiments involving continued pretraining on graduate textbooks and task-specific fine-tuning on ClimaQA-Silver to measure benefits of domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned open model; underwent continued pretraining (cp) on textbook corpus and supervised fine-tuning (ft) on ClimaQA-Silver</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science (domain adaptation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based climate QA answering after domain adaptation: measure how continued pretraining on graduate textbooks and supervised fine-tuning on synthetic QA improve climate QA simulation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Evaluated default, few-shot, RAG-source/held-out; additional variants: continued pretraining on held-out textbooks (cp-held-out) and fine-tuning on ClimaQA-Silver (ft-silver).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform lexical/semantic/factual metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Default MCQ overall 70.61% (no adaptation). With cp-held-out: overall 75.92%; with rag-source: overall 90.20% and Cloze EM 72.50, PS 0.92. Fine-tune on silver (ft-silver) gave overall 73.06%, Cloze EM 51.25, PS 0.85.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-adaptation baseline MCQ 70.61%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['continued pretraining on domain textbooks (cp) improves MCQ and Cloze performance', 'supervised fine-tuning on synthetic QA (ft-silver) helps but RAG-source often outperforms ft-silver', 'retrieval from source textbooks provides biggest single gains']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Continued pretraining used 13 graduate textbooks not used to generate QA; fine-tuning used ClimaQA-Silver; RAG-source used 5 source books; cp-held-out vs rag-source variants compared.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Fine-tuning on synthetic silver data alone did not match gains from RAG-source in many cases; small annotated gold set limits transfer and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7387.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-v0.3-Instruct (cp & ft)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B v0.3 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>7B-parameter instruct-tuned Mistral model subjected to continued pretraining on textbook data and fine-tuning on ClimaQA-Silver to evaluate domain-adaptation effects for climate QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned open model; experimented with continued pretraining (cp-held-out) and ft-silver supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based climate QA answering to study the effect of domain-specific continued pretraining and synthetic QA fine-tuning on simulation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Default, few-shot, RAG-source/held-out; continued pretraining on held-out textbooks (cp-held-out) and supervised fine-tuning on ClimaQA-Silver (ft-silver) were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Default MCQ overall 67.76%, Cloze EM 17.50, PS 0.74. RAG-source dramatically improved MCQ overall to 88.98% (Cloze EM 57.5, PS 0.88). Continued pretraining (cp-held-out) gave MCQ overall 69.8%; ft-silver overall 75.51%, Cloze EM 41.88, PS 0.83.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-adaptation baseline MCQ 67.76%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['continued pretraining on relevant textbooks (cp) can improve performance modestly', 'RAG-source yields largest accuracy jumps', 'fine-tuning on synthetic silver dataset improves consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>cp-held-out used 13 textbooks for continued pretraining; ft-silver used ClimaQA-Silver for supervised fine-tuning; same task prompting constraints applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Default performance low on some Cloze items; heavy sensitivity to where retrieval comes from (source vs held-out).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7387.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7387.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 (Flash variant with long-context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal/LLM (Gemini) model with very large context window (up to ~1M tokens) evaluated on ClimaQA to test ability to ingest entire textbook contexts for in-context answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5-flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (Gemini family; large-scale), context window up to ~1 million tokens</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned large model with long-context capability; evaluated default and long-context retrieval scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA with long-context textbook ingestion</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based answering of climate QA using extremely large context windows to include entire textbooks or long retrieved chunks in-context (simulate deep-context reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Long-context in-context prompting (pass an entire textbook or large retrieved chunk) vs standard RAG; RAG-source and rag-held-out also evaluated; included book identifier of most relevant retrieved chunk for context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Default MCQ overall 80.82%; long-context using source books: MCQ overall 82.86%, Cloze EM 51.88, PS 0.89. Long-context held-out reduced MCQ overall to 72.24%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Default/no-long-context baseline 80.82%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['quality of the included long context: source text gives modest improvements but held-out long context may reduce performance', 'identification/inclusion of most relevant book chunk as in-context information', 'ability to handle multimillion-token context']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used Gemini 1.5-flash with long-context (up to 1M tokens); included identified most-relevant book for in-context answers; evaluated rag-source and rag-held-out.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Including held-out (irrelevant or distracting) textbooks in long context often reduces performance; large-context ingestion does not guarantee improved reasoning unless the context is highly relevant and well-identified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing large language models on climate information <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Climate-Fever: A dataset for verification of real-world climate claims <em>(Rating: 2)</em></li>
                <li>LLM-assisted modeling and simulations for public sector decision-making: Bridging climate data and policy insights <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7387",
    "paper_id": "paper-276902996",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-turbo (generator/solver)",
            "name_full": "OpenAI GPT-3.5-turbo",
            "brief_description": "Instruction‑tuned transformer LLM used in this work both to generate synthetic climate QA pairs from textbook contexts and as a baseline/question-answering model evaluated on the ClimaQA benchmarks.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": "proprietary / undisclosed (API model)",
            "model_type": "instruction-tuned; used as QA-generator and QA-solver (zero-shot / few-shot / RAG)",
            "scientific_domain": "Earth science / Climate science (graduate-level textbook content; aerosols, clouds, atmospheric processes)",
            "simulation_task_description": "Text-based simulation of climate reasoning via: (a) automatic generation of multiple-choice, freeform, and cloze question-answer pairs from textbook contexts; (b) answering climate QA (MCQ, freeform, cloze) as a text-based reasoning simulator.",
            "prompting_strategy": "Generator: prompted with a 2000-character seed context plus additional retrieved chunks (cosine-similarity) and rule-guided prompt templates to produce base and complexity-evolved QA pairs; Solver: evaluated in default (zero-shot), few-shot (in-context examples), and Retrieval-Augmented Generation (RAG) modes; MCQ responses constrained to single-letter output; Freeform constrained to concise (&lt;=2 sentence) answers; Cloze constrained to single word.",
            "evaluation_metric": "For generation: human expert validation (valid/invalid labels). For QA solving: MCQ accuracy (% correct), Cloze Exact Match (EM) and Phrase Similarity (PS), Freeform BLEU, BERTScore, and Factual Accuracy (entailment confidence).",
            "reported_accuracy": "As solver on ClimaQA-Gold: MCQ overall accuracy 73.06%; Cloze EM 43.12%; Cloze PhraseSimilarity 0.81. Freeform (ClimaQA-Silver) BLEU 0.467, BERTScore 0.523, Factual Accuracy 0.545. As generator: ~25% of generated MCQs were incorrectly answered by the same model even when contexts provided (self-inconsistency), implying ~75% context-answering consistency in that analysis.",
            "baseline_accuracy": "Default/no-context solver performance reported (e.g., MCQ baseline for gpt-3.5-turbo 73.06% overall). Generation baseline: raw LLM generation prior to expert validation (no single baseline model beyond human validation).",
            "factors_reported": [
                "use of retrieval (RAG) from source vs held-out textbooks",
                "prompting strategy (zero-shot vs few-shot)",
                "quality of seed context and retrieved chunks",
                "self-inconsistency/hallucination in generation",
                "expert validation / automated evaluator feedback loop"
            ],
            "experimental_conditions": "MCQ output: single-letter top-token chosen; Freeform: max 2 sentences; Cloze: single word; generation used 2000-character seed contexts + retrieved chunks by cosine similarity; RAG evaluated in two retrieval pools (source books and held-out books).",
            "limitations_or_failure_modes": "Generator exhibited self-inconsistency and produced scientifically inaccurate QA pairs (~25% incorrect MCQs). Solver performance degrades on reasoning-style MCQs; hallucinations and imprecise domain terminology appear when domain context is insufficient.",
            "uuid": "e7387.0",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o-mini (evaluator / entailment)",
            "name_full": "OpenAI GPT-4o-mini (fine-tuned variants used)",
            "brief_description": "A smaller instruction-tuned GPT-4o variant used in ClimaQA both as a factual-entailment classifier to score factual accuracy of freeform answers, and fine-tuned as an automated evaluator to label generated QA pairs valid/invalid and to pick cloze mask terms.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini",
            "model_size": "proprietary / mini variant (undisclosed)",
            "model_type": "instruction-tuned; fine-tuned classifier (evaluator) on expert labels; used for entailment scoring",
            "scientific_domain": "Earth science / Climate QA validation (used to judge factual support/contradiction relative to reference answers)",
            "simulation_task_description": "Textual factual-entailment classification (SUPPORTS vs REFUTES) to simulate expert verification of model answers and to automate cloze-mask selection.",
            "prompting_strategy": "Zero-shot textual entailment prompt: present Evidence: ⟨evidence⟩ Claim: ⟨claim⟩ and respond SUPPORTS/REFUTES; for scoring the claim confidence logits were sigmoid-smoothed with temperature T=5. For evaluator fine-tuning, trained as a classifier on expert-annotated valid/invalid labels and used uncertainty-based active sampling.",
            "evaluation_metric": "Entailment classification accuracy on Climate-Fever (quoted 81% zero-shot). For evaluation automation: percentage of generated QA pairs labeled valid (used as proxy for generation quality); improvement in dataset quality measured as % increase in valid items after evaluator intervention.",
            "reported_accuracy": "Zero-shot entailment on Climate-Fever: 81% (paper cites this for the method). Evaluator outcomes in ClimaGen: after automated annotation they observed ~85% of MCQs and ~90% of freeform QAs were labeled valid; fine-tuned evaluator increased MCQ-valid set quality by ~10% and Freeform by ~5%.",
            "baseline_accuracy": "Human expert validation (gold labels) used as ground truth; zero-shot entailment baseline was 81% on Climate-Fever referenced dataset.",
            "factors_reported": [
                "quality and size of human-labeled training data used for fine-tuning",
                "uncertainty-based active sampling and confidence thresholds (confidence&gt;0.85 handling)",
                "domain specificity of training texts",
                "prompt formulation for entailment (support/refute framing) and temperature scaling"
            ],
            "experimental_conditions": "Confidence threshold for dropping samples: 0.85; samples above threshold dropped with 50% probability in active sampling; logit smoothing with temperature T=5 to scale support/refute logits into [0,1]; evaluator models were fine-tuned separately for MCQ and freeform tasks with limited labeled sets (test sets ~40 examples in some splits).",
            "limitations_or_failure_modes": "Automated evaluator quality depends strongly on limited expert-labeled data; variation in human expert labels can introduce variability; automated evaluator may still mislabel edge cases and cannot fully replace domain experts without larger labeled corpora.",
            "uuid": "e7387.1",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (solver)",
            "name_full": "OpenAI GPT-4o",
            "brief_description": "A state-of-the-art instruction-tuned LLM evaluated as a climate QA solver across MCQ, freeform, and cloze tasks, showing top performance in the ClimaQA benchmark.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_size": "proprietary / undisclosed (OpenAI flagship family)",
            "model_type": "instruction-tuned; evaluated in default, few-shot, and RAG settings",
            "scientific_domain": "Climate science (graduate textbook-derived questions)",
            "simulation_task_description": "Text-based simulation of climate reasoning: answering multiple-choice (MCQ), freeform explanatory, and cloze completion questions requiring factual knowledge and scientific reasoning.",
            "prompting_strategy": "Zero-shot default; few-shot in-context prompting tested; Retrieval-Augmented Generation (RAG) using either source textbooks (used to generate questions) or held-out textbook pools; MCQ answers constrained to single-letter outputs; Freeform constrained to concise answers.",
            "evaluation_metric": "MCQ accuracy (% correct), Cloze Exact Match and Phrase Similarity, Freeform BLEU, BERTScore, and Factual Accuracy (entailment confidence).",
            "reported_accuracy": "ClimaQA-Gold reported: MCQ overall accuracy 85.71% (default), Cloze EM 53.12%, Phrase Similarity 0.88. With RAG-source: MCQ overall 92.79%, Cloze EM 71.88%, PS 0.94. Freeform metrics (ClimaQA-Silver) BLEU 0.440, BERTScore 0.493, Factual Accuracy 0.491.",
            "baseline_accuracy": "Default/no-context baseline reported (MCQ 85.71%); few-shot produced marginal improvements; RAG-source produced the best improvements (e.g., MCQ → 92.79%).",
            "factors_reported": [
                "retrieval from source vs held-out textbooks (RAG-source gave large improvements)",
                "task format (MCQ reasoning items are harder)",
                "few-shot marginal gains",
                "model instruction-tuning and inherent model capability"
            ],
            "experimental_conditions": "MCQ output: single-letter top-token; Freeform: &lt;=2 sentences; RAG scenarios: rag-source (5 source books used to generate questions) vs rag-held-out (13 other texts); long-context (Gemini 1.5) also tested separately; no paper-specified temperature for API calls shown.",
            "limitations_or_failure_modes": "While GPT-4o dominated overall, reasoning MCQs and held-out retrieval scenarios showed reduced performance; BLEU/BERTScore could be biased toward the model used for QA-generation (gpt-3.5-turbo) whereas Factual Accuracy metric gave more reliable signal.",
            "uuid": "e7387.2",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "gemma-27b",
            "name_full": "Gemma 27B (open-model family)",
            "brief_description": "Open-source 27B-parameter model evaluated on ClimaQA as an off-the-shelf solver in default, few-shot, and RAG settings; used to assess open weights model performance on fine-grained climate QA.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "gemma-27b",
            "model_size": "27B parameters",
            "model_type": "base/instruction-tuned (open-source family); evaluated as-is and with few-shot and RAG",
            "scientific_domain": "Climate science QA (textual reasoning over graduate textbook content)",
            "simulation_task_description": "Text-based answering of MCQ, freeform, and cloze climate questions to simulate domain reasoning.",
            "prompting_strategy": "Default zero-shot; few-shot in-context prompting; RAG with source and held-out textbook retrieval; MCQ single-letter output constraint.",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.",
            "reported_accuracy": "ClimaQA-Gold: MCQ overall 79.18%, Cloze EM 49.38%, PhraseSimilarity 0.87. ClimaQA-Silver: MCQ overall 79.5%, Cloze EM 50.00, PS 0.85. Freeform (ClimaQA-Silver) BLEU 0.392, BERTScore 0.441, Factual Accuracy 0.365.",
            "baseline_accuracy": "Default/no-context baseline (MCQ 79.18%). RAG-source improved MCQ to 90.48% (Table 3 variant).",
            "factors_reported": [
                "RAG from source textbooks strongly improves accuracy",
                "few-shot provides marginal gains",
                "sensitivity to irrelevant retrieved context (held-out retrieval sometimes reduces performance)"
            ],
            "experimental_conditions": "Variants evaluated: default, few-shot, rag-source, rag-held-out. MCQ/Freeform/Cloze prompting constraints as described above.",
            "limitations_or_failure_modes": "Freeform factual accuracy low relative to closed-source top models; vulnerable to distractor or irrelevant retrieval; some solver responses were irrelevant but scored high on surface lexical metrics.",
            "uuid": "e7387.3",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "llama3-70b",
            "name_full": "LLaMA 3 70B",
            "brief_description": "Large open-source 70B-parameter LLM evaluated on ClimaQA across prompting and retrieval configurations, showing strong MCQ performance especially with RAG from source textbooks.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "llama3-70b",
            "model_size": "70B parameters",
            "model_type": "open-source large model (instruct-style inference via TogetherAI), evaluated default/few-shot/RAG",
            "scientific_domain": "Climate science QA",
            "simulation_task_description": "Text-based answering of QA pairs (MCQ/freeform/cloze) simulating climate reasoning over textbook contexts.",
            "prompting_strategy": "Default zero-shot, few-shot (in-context), RAG-source and RAG-held-out; outputs constrained per task (MCQ single-letter, Freeform concise).",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform lexical/semantic/factual metrics.",
            "reported_accuracy": "ClimaQA-Gold default overall MCQ 83.27%; Cloze EM 38.75; PS 0.82. With rag-source: MCQ overall 88.98%; Cloze EM 63.12; PS 0.91. Freeform (ClimaQA-Silver) BLEU 0.335, BERTScore 0.474, Factual Accuracy 0.569.",
            "baseline_accuracy": "Default/no-context baseline MCQ 83.27%.",
            "factors_reported": [
                "retrieval from source books (RAG-source) yields substantial gain",
                "few-shot gives modest improvements",
                "task-format sensitivity (reasoning items harder)"
            ],
            "experimental_conditions": "Evaluated across default, few-shot, rag-source and rag-held-out; standard prompt constraints for each task.",
            "limitations_or_failure_modes": "Cloze EM lower relative to other models in some settings; performance sensitive to irrelevant retrieval when using held-out books.",
            "uuid": "e7387.4",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "mixtral-8x22b",
            "name_full": "Mixtral (Mixture-of-Experts) 8x22B",
            "brief_description": "Mixture-of-experts style open model (mixtral) evaluated as a climate QA solver; shows competitive performance with improvements under RAG-source.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "mixtral-8x22b",
            "model_size": "8×22B experts (Mixture-of-Experts design; name denotes expert shards)",
            "model_type": "open-source MoE model evaluated default/few-shot/RAG",
            "scientific_domain": "Climate science QA",
            "simulation_task_description": "Textual climate QA answering (MCQ/freeform/cloze) simulating reasoning across textbook-derived questions.",
            "prompting_strategy": "Default, few-shot, RAG-source and RAG-held-out prompting; same task output constraints.",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.",
            "reported_accuracy": "ClimaQA-Gold MCQ overall 80.00%; Cloze EM 35.62; PS 0.75. With rag-source MCQ improved to 84.90% (Table 3). Freeform (ClimaQA-Silver) BLEU 0.394, BERTScore 0.516, Factual Accuracy 0.485.",
            "baseline_accuracy": "Default/no-context MCQ 80.00%.",
            "factors_reported": [
                "benefit from RAG-source retrieval",
                "sensitivity to held-out irrelevant retrieval",
                "few-shot yields small gains"
            ],
            "experimental_conditions": "Evaluated in default/few-shot/RAG modes with generation constraints; no explicit temperature reported.",
            "limitations_or_failure_modes": "Lower Cloze EM and PhraseSimilarity relative to best-performing models; held-out retrieval sometimes degrades performance significantly.",
            "uuid": "e7387.5",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3.1-8B-Instruct (cp & ft)",
            "name_full": "LLaMA3.1-8B-Instruct (continued pretraining and fine-tuned variants)",
            "brief_description": "8B-parameter instruct-tuned LLaMA variant used for experiments involving continued pretraining on graduate textbooks and task-specific fine-tuning on ClimaQA-Silver to measure benefits of domain adaptation.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "LLaMA3.1-8B-Instruct",
            "model_size": "8B parameters",
            "model_type": "instruction-tuned open model; underwent continued pretraining (cp) on textbook corpus and supervised fine-tuning (ft) on ClimaQA-Silver",
            "scientific_domain": "Climate science (domain adaptation experiments)",
            "simulation_task_description": "Text-based climate QA answering after domain adaptation: measure how continued pretraining on graduate textbooks and supervised fine-tuning on synthetic QA improve climate QA simulation accuracy.",
            "prompting_strategy": "Evaluated default, few-shot, RAG-source/held-out; additional variants: continued pretraining on held-out textbooks (cp-held-out) and fine-tuning on ClimaQA-Silver (ft-silver).",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform lexical/semantic/factual metrics.",
            "reported_accuracy": "Default MCQ overall 70.61% (no adaptation). With cp-held-out: overall 75.92%; with rag-source: overall 90.20% and Cloze EM 72.50, PS 0.92. Fine-tune on silver (ft-silver) gave overall 73.06%, Cloze EM 51.25, PS 0.85.",
            "baseline_accuracy": "Default/no-adaptation baseline MCQ 70.61%.",
            "factors_reported": [
                "continued pretraining on domain textbooks (cp) improves MCQ and Cloze performance",
                "supervised fine-tuning on synthetic QA (ft-silver) helps but RAG-source often outperforms ft-silver",
                "retrieval from source textbooks provides biggest single gains"
            ],
            "experimental_conditions": "Continued pretraining used 13 graduate textbooks not used to generate QA; fine-tuning used ClimaQA-Silver; RAG-source used 5 source books; cp-held-out vs rag-source variants compared.",
            "limitations_or_failure_modes": "Fine-tuning on synthetic silver data alone did not match gains from RAG-source in many cases; small annotated gold set limits transfer and robustness.",
            "uuid": "e7387.6",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mistral-7B-v0.3-Instruct (cp & ft)",
            "name_full": "Mistral 7B v0.3 Instruct",
            "brief_description": "7B-parameter instruct-tuned Mistral model subjected to continued pretraining on textbook data and fine-tuning on ClimaQA-Silver to evaluate domain-adaptation effects for climate QA tasks.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.3-Instruct",
            "model_size": "7B parameters",
            "model_type": "instruction-tuned open model; experimented with continued pretraining (cp-held-out) and ft-silver supervised fine-tuning",
            "scientific_domain": "Climate science QA",
            "simulation_task_description": "Text-based climate QA answering to study the effect of domain-specific continued pretraining and synthetic QA fine-tuning on simulation accuracy.",
            "prompting_strategy": "Default, few-shot, RAG-source/held-out; continued pretraining on held-out textbooks (cp-held-out) and supervised fine-tuning on ClimaQA-Silver (ft-silver) were applied.",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform BLEU/BERTScore/Factual Accuracy.",
            "reported_accuracy": "Default MCQ overall 67.76%, Cloze EM 17.50, PS 0.74. RAG-source dramatically improved MCQ overall to 88.98% (Cloze EM 57.5, PS 0.88). Continued pretraining (cp-held-out) gave MCQ overall 69.8%; ft-silver overall 75.51%, Cloze EM 41.88, PS 0.83.",
            "baseline_accuracy": "Default/no-adaptation baseline MCQ 67.76%.",
            "factors_reported": [
                "continued pretraining on relevant textbooks (cp) can improve performance modestly",
                "RAG-source yields largest accuracy jumps",
                "fine-tuning on synthetic silver dataset improves consistency"
            ],
            "experimental_conditions": "cp-held-out used 13 textbooks for continued pretraining; ft-silver used ClimaQA-Silver for supervised fine-tuning; same task prompting constraints applied.",
            "limitations_or_failure_modes": "Default performance low on some Cloze items; heavy sensitivity to where retrieval comes from (source vs held-out).",
            "uuid": "e7387.7",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemini 1.5",
            "name_full": "Gemini 1.5 (Flash variant with long-context)",
            "brief_description": "Large multimodal/LLM (Gemini) model with very large context window (up to ~1M tokens) evaluated on ClimaQA to test ability to ingest entire textbook contexts for in-context answering.",
            "citation_title": "CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5-flash",
            "model_size": "proprietary (Gemini family; large-scale), context window up to ~1 million tokens",
            "model_type": "instruction-tuned large model with long-context capability; evaluated default and long-context retrieval scenarios",
            "scientific_domain": "Climate science QA with long-context textbook ingestion",
            "simulation_task_description": "Text-based answering of climate QA using extremely large context windows to include entire textbooks or long retrieved chunks in-context (simulate deep-context reasoning).",
            "prompting_strategy": "Long-context in-context prompting (pass an entire textbook or large retrieved chunk) vs standard RAG; RAG-source and rag-held-out also evaluated; included book identifier of most relevant retrieved chunk for context.",
            "evaluation_metric": "MCQ accuracy, Cloze EM & PhraseSimilarity, Freeform metrics.",
            "reported_accuracy": "Default MCQ overall 80.82%; long-context using source books: MCQ overall 82.86%, Cloze EM 51.88, PS 0.89. Long-context held-out reduced MCQ overall to 72.24%.",
            "baseline_accuracy": "Default/no-long-context baseline 80.82%.",
            "factors_reported": [
                "quality of the included long context: source text gives modest improvements but held-out long context may reduce performance",
                "identification/inclusion of most relevant book chunk as in-context information",
                "ability to handle multimillion-token context"
            ],
            "experimental_conditions": "Used Gemini 1.5-flash with long-context (up to 1M tokens); included identified most-relevant book for in-context answers; evaluated rag-source and rag-held-out.",
            "limitations_or_failure_modes": "Including held-out (irrelevant or distracting) textbooks in long context often reduces performance; large-context ingestion does not guarantee improved reasoning unless the context is highly relevant and well-identified.",
            "uuid": "e7387.8",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing large language models on climate information",
            "rating": 2,
            "sanitized_title": "assessing_large_language_models_on_climate_information"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Climate-Fever: A dataset for verification of real-world climate claims",
            "rating": 2,
            "sanitized_title": "climatefever_a_dataset_for_verification_of_realworld_climate_claims"
        },
        {
            "paper_title": "LLM-assisted modeling and simulations for public sector decision-making: Bridging climate data and policy insights",
            "rating": 1,
            "sanitized_title": "llmassisted_modeling_and_simulations_for_public_sector_decisionmaking_bridging_climate_data_and_policy_insights"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.022438999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS
9 Mar 2025</p>
<p>Vignesh Veeramakali 
University of California
San Diego</p>
<p>Yasaman Manivannan vmanivannan@ucsd.edu 
University of California
San Diego</p>
<p>Srikar Jafari yajafari@ucsd.edu 
University of California
San Diego</p>
<p>Spencer Eranky seranky@ucsd.edu 
University of California
San Diego</p>
<p>Rose Ho 
University of California
San Diego</p>
<p>Duncan Yu 
University of California
San Diego</p>
<p>Yian Watson-Parris dwatsonparris@ucsd.edu 
University of California
San Diego</p>
<p>Leon Ma 
University of California
San Diego</p>
<p>Taylor Bergen lbergen@ucsd.edu 
University of California
San Diego</p>
<p>Berg-Kirkpatrick 
University of California
San Diego</p>
<p>Rose Yu roseyu@ucsd.edu 
University of California
San Diego</p>
<p>Taylor Berg-Kirkpatrick 
University of California
San Diego</p>
<p>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS
9 Mar 2025C26CC17F8E4CB9CA960F582893E35AACarXiv:2410.16701v2[cs.LG]
The use of Large Language Models (LLMs) in climate science has recently gained significant attention.However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs.To address this issue, we develop ClimaGen (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop.As a result, we present ClimaQA-Gold, an expert-annotated benchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA dataset for climate science.Finally, we develop evaluation strategies and compare different LLMs on our benchmarks.Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs.ClimaQA's source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa</p>
<p>INTRODUCTION</p>
<p>Climate change is one of the most pressing global challenges today, with profound impacts on ecosystems, economies, and societies.In recent years, Large Language Models (LLMs) have gained significant interest in climate science (Thulke et al., 2024;Nguyen et al., 2024;Cao et al., 2024) due to their potential to transform climate predictions and enable applications in climate policy analysis, environmental decision-making, and public education.By improving LLMs' understanding of climate science, we can empower stakeholders to make informed decisions, develop actionable solutions, and foster broader awareness of climate issues.However, while LLMs are powerful, they often fall short when it comes to answering technical questions requiring high precision such as What is the net effect of Arctic stratus clouds on the Arctic climate?Even advanced models like GPT-4 exhibit epistemological inaccuracies in Climate Question-Answering (QA) tasks (Bulian et al., 2024), raising concerns about their reliability in scientific workflows.This highlights the need for a domain-specific evaluation framework to assess the quality and validity of outputs generated by these models.Current benchmarks for Large Language Models (LLMs) predominantly focus on linguistic accuracy or general factual correctness (Bai &amp; Wang, 2021), but they fail to address the unique demands of climate science, where factual rigor, domain-specific knowledge, and robust reasoning are essential.Although some work has explored the scientific evaluation of LLMs (Table 1), they either rely heavily on manual expert input or employ fully synthetic question generations.To address this issue, we develop ClimaGen, an adaptive learning framework for creating benchmarks in collaboration with domain experts to evaluate scientific question-answering models, specifically for climate science but adaptable to other scientific disciplines, shown in Figure 1.This enables us to achieve a balance between utilizing the efficiency of LLMs and the expertise of domain specialists.</p>
<p>Using our framework, we introduce a novel benchmark for evaluating question-answering models in climate science across three scientific QA task forms: multiple-choice, freeform, and cloze.The questions are designed with varying levels of complexity, challenging the models to demonstrate a range of reasoning abilities from basic factual recall to scientific reasoning and scenario applications.</p>
<p>Figure 1: ClimaGen -Our proposed Automated Benchmark Creation Framework.The QA generation framework creates synthetic data from seed contexts extracted from graduate-level textbooks using LLMs to generate base-level question-answer pairs and evolve them by adding complexities to the same.These are validated by domain experts during the annotation process to produce the semisynthetic benchmark.The evaluator model is trained actively using the human-labeled examples in order to completely automate the process.</p>
<p>The benchmark consists of two datasets: ClimaQA-Gold -an expert-annotated dataset with a total of 566 questions validated by climate scientists, ensuring high scientific rigor, and ClimaQA-Silver -a large-scale synthetic dataset consisting of 3000 questions generated by our framework, providing substantial ground truth data for model fine-tuning at scale.Together, these datasets enable comprehensive performance assessment of LLMs in climate science, specifically for scientific QA tasks.We evaluate several LLMs on our benchmark under different settings.We observe that most models struggle with reasoning-based multiple-choice questions (MCQs) and Retrieval Augmented Generation (RAG) (Lewis et al., 2020) significantly outperforms Continued Pre-training and Supervised Fine-tuning across different tasks.</p>
<p>In summary, our contributions are as follows:</p>
<p>• Creation of publicly releasable datasets: expert-annotated (ClimaQA-Gold) and synthetic (ClimaQA-Silver), along with tailored evaluation metrics facilitating both rigorous assessment and large-scale fine-tuning on 3 scientific QA task forms: multiple-choice, freeform, and cloze, with varying levels of complexity.</p>
<p>• Development of a generalized adaptive learning framework (ClimaGen) for creating scientific benchmarks at scale in collaboration with domain experts for evaluation of natural language question-answering models on scientific accuracy.</p>
<p>• Evaluation of state-of-the-art LLMs on climate science QA tasks, with insights into improving scientific accuracy.ScienceQA (Lu et al., 2022) contains a vast collection of multimodal MCQs manually curated from high school textbooks.Pira2 (Pirozelli et al., 2024) consists of expert-created questions derived from research articles focused on oceans, the Brazilian coast, and climate change.The creation of these benchmarks required substantial manual effort.SciQA (Auer et al., 2023) innovatively generates freeform QA pairs by leveraging hand-crafted queries on the Open Research Knowledge Graph (Jaradeh et al., 2019) primarily drawing from computer science literature.Although these pairs are factually accurate, they do not include an automatic evaluation method for generated responses.Climate Crisis QA (Zhu &amp; Tiwari, 2024) and SciQAG-24D (Wan et al., 2024) explore synthetic data generations using Large Language models.However, their approaches are prone to suffer from hallucinations and lack of scientific validity.To address this, we introduce a gold-standard dataset, rigorously validated by domain experts, alongside a large-scale silver dataset whose generation process was guided by these expert validation labels.Moreover, existing benchmarks generally focus on a single QA format and lack scientifically aligned evaluation metrics.Our benchmark contains questions of three distinct scientific QA task forms at varying levels of complexity, along with evaluation metrics tailored to them.(Devlin, 2018) for summarization and keyword extraction, employing WordNet (Miller, 1995) to generate distractors.(Das et al., 2021) applies RAKE (Rose et al., 2010) for keyword extraction and clustering methods for distractor generation.Other approaches, such as utilizing dependency trees (Afzal &amp; Mitkov, 2014), have also been explored for MCQ creation.These methods typically focus on generating MCQs with single-word answers.However, the recent advancements in LLMs have enabled the creation of more complex MCQs with longer, detailed answer choices.(Meißner et al., 2024) demonstrate the automated generation of self-assessment quizzes using LLMs, while (Hang et al., 2024) explore self-refining prompting techniques for improved MCQ generation.Recent studies, including (Olney, 2023) and (Doughty et al., 2024), suggest that LLMs can generate MCQs comparable to those created by humans, though (Grévisse et al., 2024) emphasize the importance of human oversight to ensure the quality and pedagogical relevance of these questions.</p>
<p>CLIMAQA -CLIMATE QUESTION ANSWERING BENCHMARK</p>
<p>The ClimaQA benchmark is built on questions generated from graduate-level climate science textbooks, ensuring alignment with the precise terminology and complex theories of the field.These textbooks provide a reliable source for generating both the expert-validated ClimaQA-Gold dataset and the synthetic ClimaQA-Silver dataset.By leveraging textbook content and combining it with expert review, ClimaQA facilitates rigorous evaluation and fine-tuning of LLMs across freeform, multiple-choice, and cloze question-answering tasks in climate science.Our expert-validated dataset, ClimaQA-Gold, ensures that the evaluation questions are accurate, relevant, and reflect the current understanding of climate science.</p>
<p>SCIENTIFIC QUESTION ANSWERING</p>
<p>To thoroughly evaluate a model's ability to handle scientific questions, we create our benchmark dataset to focus on the different complexities of scientific reasoning.The aim is to test the model's ability to engage with scientific concepts at different levels of understanding and scenario application.</p>
<p>Our benchmark consists of questions of three levels of complexity.The first level involves basic questions designed to test straightforward factual understanding.The second level introduces reasoning, requiring the model to connect multiple scientific facts or principles.The third level involves hypothetical scenarios, testing the model's ability to apply scientific knowledge in unseen contexts.These questions challenge the model's scientific reasoning in different ways, from knowledge recall to advanced reasoning and problem-solving in dynamic contexts.A question from each level of complexity is shown in Figure 2 as an example.The second is the enhanced version of the question that requires scientific reasoning to answer.The third is the modified version of the question that involves a hypothetical scenario.The contexts from the textbook data were used during the question evolution.</p>
<p>The questions come in three different task forms, demonstrated in Figure 3:</p>
<p>• MCQ: The model selects correct answers from predefined options, assessing its factual accuracy and decision-making under constrained conditions.</p>
<p>• Freeform: The model generates detailed, structured responses, testing its ability to reason logically and produce scientifically sound explanations.</p>
<p>• Cloze: The model fills in blanks with appropriate scientific terms, evaluating its contextual understanding and use of domain-specific vocabulary.</p>
<p>Together, the benchmark as shown in Table 2 provides a robust framework for evaluating an LLM's proficiency in scientific reasoning, critical thinking, and applying knowledge in unseen scenarios.</p>
<p>EVALUATION METRICS</p>
<p>Although assessing multiple-choice question-answering is relatively simple, the other two tasks present more challenges.To address this, we propose and validate the following evaluation metrics for freeform and cloze question-answering.A more detailed case study to demonstrate the robustness of these metrics can be found in the Appendix A.2.3.We use these metrics to report experimental results in Section 5  Various metrics are employed to evaluate sentence similarity, ranging from surface-level comparisons to deeper semantic analysis.Lexical metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee &amp; Lavie, 2005) focus on exact word or n-gram matching, rendering them useful for tasks where token overlap is crucial, such as machine translation.In contrast, semantic metrics like BERTScore (Zhang et al., 2019), Word Mover's Distance (WMD) (Huang et al., 2016), and Sentence-BERT (Reimers, 2019) are more advanced, capturing the meanings of sentences through embeddings.These metrics are better suited for tasks that necessitate an understanding of meaning, such as paraphrase detection.However, they may not adequately assess factual accuracy.</p>
<p>To measure the factual accuracy of generated answers relative to reference answers, we propose the use of a factual entailment classifier, reporting the confidence level as the factual accuracy score.Instruction-tuned models, such as GPT-4, have demonstrated superior performance on textual en-tailment tasks and have shown the ability to generalize across various datasets (Sanyal et al., 2024).We employ the GPT-4o-mini model with the prompt below for factual entailment.This method achieved 81% zero-shot classification accuracy on the Climate-Fever dataset (Diggelmann et al., 2020) indicating the ability to measure factual accuracy.</p>
<p>You are a climate expert who annotates whether a given claim either SUPPORTS or REFUTES the presented evidence.You will be provided with the following input:</p>
<p>Evidence: ⟨evidence⟩ Claim: ⟨claim⟩</p>
<p>Respond with only one word: SUPPORTS if the claim supports the evidence and REFUTES otherwise.</p>
<p>To use this for scoring freeform QA, the reference answer was used as the evidence and the generated answer was used as the claim.The confidence score was computed by applying sigmoid smoothing to the logit scores, with the temperature parameter set to T = 5.Note that the choice of T does not alter the score trend; it was selected to optimally scale values between 0 and 1.If l s and l r are the logit scores for SUPPORTS and REFUTES respectively, then</p>
<p>Factual Accuracy = SigmoidSmooth l s l s + l r Overall, we report three metrics for freeform question answering (QA): BLEU, BERTScore, and Factual Accuracy to evaluate different aspects of the generated answers.</p>
<p>CLOZE QA</p>
<p>Performance on this task is typically evaluated using the exact match metric.However, this approach has limitations due to the existence of multiple correct answers.A generated answer may differ from the reference answer while remaining contextually and semantically valid; for instance, while 'point' and 'temperature' are semantically distinct terms, they can be contextually similar within phrases like 'freezing point' and 'freezing temperature.'This illustrates that semantic relationships can depend heavily on context.To address this challenge, we introduce a metric that captures the semantic similarity between the generated answer and the ground-truth answer for a more nuanced assessment of model performance.Specifically, we utilize a context window to extract two phrases: one with the ⟨blank⟩ replaced by the reference answer and the other with the generated answer.The semantic similarity is measured using cosine similarity between the Universal Sentence Encoder (Cer, 2018) embeddings of these phrases.</p>
<p>To evaluate the robustness of this approach, we synthetically generated 219 cloze questions with our framework (described in Section 4), which were answered by the GPT-4o-mini model.We collected 32 questions where the generated answers did not exactly match the reference answer.These answers were then labeled as wrong or correct by domain experts based on scientific and contextual accuracy.We plotted the average cosine similarities of phrases for these questions as shown in Figure 4, concluding that a context window of size 4 most effectively differentiates between correct and incorrect answers.This configuration yields the maximum difference in scores while maintaining sufficiently high scores for correct answers.The cosine similarities were subsequently rescaled to emphasize these differences.If e 1 and e 2 are the embeddings of the respective phrases as mentioned above, then Phrase Similarity = 2 × (CosineSimilarity(e 1 , e 2 ) − 0.5)</p>
<p>We report two metrics for cloze question answering (QA): Exact Match and Phrase Similarity.</p>
<p>CLIMAGEN -AUTOMATED BENCHMARK CREATION</p>
<p>In this section, we describe ClimaGen as shown in Figure 1, the framework used to create the ClimaQA dataset from Climate Textbooks.By leveraging RAG and prompt engineering, we systematically generate and refine questions of varying complexity levels.Domain expert annotators ensure quality, producing a semi-synthetic benchmark for the evaluation of AI systems in complex scientific inquiry.Additionally, we automate annotation by fine-tuning an LLM on human-labeled data, enabling the creation of a large-scale synthetic dataset for fine-tuning tasks.The techniques discussed here could be generalized to aid in the semi-automatic production of benchmarks for other scientific fields aswell.</p>
<p>TEXTBOOK DATASET</p>
<p>LLMs are typically pre-trained on extensive general internet data, which often contains noise and misinformation.This limitation is particularly significant in fields like climate science, where a precise understanding of specialized terminology and concepts is crucial.To evaluate LLMs' proficiency in climate science, we employed graduate-level climate science textbooks as a reliable source of specialized knowledge, providing accurate scientific information that better represents the technical terms and nuanced theories integral to this discipline.We collected 18 textbooks (Table 4) that broadly represent a mixture of graduate and expert literature on the physical climate with a particular focus on the role of aerosol in the climate system -one of the critical sources of uncertainty in climate projections.The content was extracted and preprocessed to ensure cleanliness and relevance, making it suitable for downstream applications such as benchmark creation, continuous pre-training, and RAG.A held-out set of 5 textbooks (Figure 6), carefully selected to represent varying levels of technical and qualitative depth across a broad range of key topics in climate science, was utilized for the benchmark creation process.</p>
<p>QA GENERATION FRAMEWORK</p>
<p>Our QA generation pipeline begins by selecting a random seed 2000-character context chunk from the collected textbook data stored in a vector database.Additional context chunks are retrieved based on cosine-similarity scores, ensuring relevant information from multiple sources is included.These chunks are then augmented and passed to the generator LLM for question-answer (QA) generation.Question generation principles, inspired by (Doughty et al., 2024), guide the prompt formulation, focusing on creating high-quality stems and distractors for MCQs, as well as refining questions by adding complexity as described in Appendix A.4.We used GPT-3.5-turboas the generator model in our experiments.The model generates baselevel questions and evolved variants with increasing complexity, such as multi-step reasoning and hypothetical scenarios, to ensure a diverse and comprehensive question set as shown in Figure 2.However, approximately 25% of the multiple-choice questions were incorrectly answered by the same model even when the contexts were provided, often due to scientifically inaccurate questionanswer pairs, indicating self-inconsistency and uncertainty in the generation process.</p>
<p>While QA pairs were generated for multiple-choice and freeform questions, plain scientific statements intended for cloze questions were also generated, from which the scientific term to be masked would be chosen during the annotation phase.After generation, the questions undergo a preliminary screening with both handcrafted and LLM-based (self-inconsistency) audits to filter out potentially invalid QA pairs.The refined set of QA pairs is then passed to the annotation phase for further validation.</p>
<p>DOMAIN EXPERT ANNOTATION</p>
<p>One key challenge with synthetic data is ensuring its distribution closely mirrors real-world data, as deviations can negatively impact downstream tasks like fine-tuning and evaluation.This issue arose when generating scientific questions with GPT-3.5-turbo, which sometimes produced inaccurate or imprecise data probably due to a limited understanding of domain-specific terminology.</p>
<p>To mitigate this, we developed an interactive web application that enables climate scientists to review and annotate the generated questions.For freeform and MCQs, the scientists validated the correctness of the content, while for cloze questions, they selected which scientific term to mask, ensuring alignment with scientific standards.They also identified common reasons for rejecting generated QA pairs during validation, providing valuable insights into improvement of the QA generation framework as discussed in Appendix A.5.1.By combining human expertise with AI, we curated 245 freeform, 161 MCQ, and 160 cloze questions, forming the ClimaQA-Gold dataset, reviewed and validated by domain experts.</p>
<p>AUTOMATED ANNOTATION</p>
<p>To fully automate the review and annotation process, we develop an evaluator model by fine-tuning an LLM (GPT-4o-mini) on expert-annotated data to validate and refine generated question-answer pairs.This removes the need for human intervention and enables scalable generation of high-quality scientific question-answer pairs, especially for data-intensive tasks like fine-tuning.</p>
<p>Building on (Zhang et al., 2023) and (Zhang et al., 2024), which demonstrate that uncertainty-based active sampling improves supervised fine-tuning with limited labeled data, we apply a similar approach.The evaluator model is fine-tuned as a classifier to label QA pairs as valid or invalid based on the given context as described in Appendix A.5.2.Uncertainty is measured by the classifier confidence scores, and samples with confidence above 0.85 are dropped with 50% probability to ensure learning from more representative examples.The evaluator models were fine-tuned separately for both MCQs and freeform questions We observed that around 85% of multiple-choice questions (MCQs) and around 90% of freeform question-answers (QAs) were valid, indicating high-quality question generation.Experiments across different train-test splits show that the evaluator models enhance the quality of the generated MCQ question set by 10% and the Freeform question set by 5% as shown in Appendix A.5.2.Additionally, we fine-tune a separate model to mark scientific terms from given statements as shown in Appendix A.5.3, automating the cloze annotation process.Using this framework, we generated 1000 freeform QAs, 1000 MCQs, and 1000 cloze questions, collectively forming the ClimaQA-Silver dataset, produced synthetically at scale without manual intervention.</p>
<p>EXPERIMENTS</p>
<p>We aim to investigate the effectiveness of various adaptation techniques on this fine-grained scientific benchmark.Fine-tuning on raw text data within a target domain is a common approach, and we seek to evaluate its effectiveness for addressing deep scientific questions.In addition, we evaluate other techniques, such as in-context learning and retrieval augmentation.</p>
<p>EXPERIMENTAL SETUP</p>
<p>We evaluate different families of LLMs on our benchmark.We use TogetherAI for performing inference on open source models like gemma-27b (Team et al., 2024b), llama3-70b (Dubey et al., 2024), and mixtral-8x22b (Jiang et al., 2024).We also evaluate OpenAI's (Achiam et al., 2023) gpt-3.5-turboand gpt-4o.</p>
<p>We evaluate each of these models in 3 settings -default, few-shot prompting (FS)(Brown, 2020), and Retrieval Augmented Generation (RAG) (Lewis et al., 2020).For the MCQs, the models were prompted to output a single letter representing the correct option, and the top-most token was chosen as the answer.For Freeform QA, the models were prompted to output concise answers with a maximum of 2 sentences.For Cloze QA, the models were prompted to output a single scientific word that best fits the blank with respect to the context around it.</p>
<p>We conduct further pre-training on graduate-level textbook data for both the LLaMA3.1-8B-Instructand Mistral-7B-v0.3-Instruct (Jiang et al., 2023) models.This pre-training was based on 13 distinct graduate textbooks that were not part of the question-generation process.The objective was to enhance the model's climate knowledge without directly exposing it to the specific sources used for question generation, thereby reducing the risk of data contamination.Additionally, we fine-tune LLaMA3.1-8B-Instruct and Mistral-7B-v0.3-Instruct on the ClimaQA-Silver dataset, which contains all three forms of MCQ, Freeform, and Cloze, in different complexity levels.We then evaluate the impact of this task-specific fine-tuning by assessing the models' performance on the ClimaQA-Gold dataset.The details of the continued pre-training and fine-tuning procedure are explained in Appendix A.3.We also leverage Gemini 1.5 (Team et al., 2024a), with a context window of up to 1 million tokens, to pass an entire textbook in context and answer questions based on that.Finally, we evaluate models on the ClimaQA-Silver dataset and analyze its potential differences from ClimaQA-Gold in Appendix A.2.2.</p>
<p>RESULTS</p>
<p>We report the performance of various models across different QA forms and complexities.Table 3 shows the results for the MCQ and Cloze form questions , and free-form results are demonstrated in Appendix Table 5.We observe that most models struggle with reasoning MCQs compared to base and hypothetical questions.While some models perform poorly on reasoning and hypothetical MCQs, they tend to generate stronger responses for the same type of Freeform Questions, indicating improved performance when reasoning is emphasized over factual recall similar to observations of Chain of Thought experiments (Wei et al., 2022), as shown in Figure 5.We examine the impact of providing relevant context through RAG when answering questions.We explore two retrieval scenarios: one where the model retrieves from 13 books that were not used to generate the questions (rag-held-out), and another where it retrieves from the 5 books that the questions were derived from (rag-source).Retrieval from source textbooks consistently enhances performance across all tasks.For Gemini 1.5, we identify the book corresponding to the most relevant retrieved chunk under both the source and held-out settings, and include it as in-context information to answer the question.Additional context from source books yields a slight improvement over the no-context baseline.However, in both RAG and long-context scenarios, incorporating held-out books usually reduces performance, likely due to irrelevant or distracting content.</p>
<p>Continued pertaining (cp-held-out) on the graduate textbooks leads to improved performance in both MCQs and Cloze question-answering tasks.Additionally, fine-tuning (ft-silver) on the ClimaQA-Silver dataset further enhances performance, often producing the best results after RAG on the source textbooks (rag-source) in most scenarios.Furthermore, Few-shot prompting yields marginal improvements in most cases.</p>
<p>Finally, We observe that the BLEU and BERTScore metrics are slightly biased towards the model that was used for QA-generation (gpt-3.5-turbo)while this is not seen in the proposed Factual Accuracy metric 5. Overall, GPT-4o dominates across tasks, demonstrating superior performance compared to other models in this evaluation set.</p>
<p>CONCLUSION</p>
<p>The ClimaQA benchmark offers a comprehensive framework for evaluating language models in climate question-answering, addressing critical aspects such as reasoning, factual accuracy, and understanding of scientific terminology.By incorporating freeform, multiple-choice, and cloze task forms with different levels of complexity, the benchmark rigorously tests models across different dimensions of scientific inquiry.Furthermore, the use of advanced metrics, such as factual accuracy for freeform tasks and phrase similarity for cloze tasks, can provide a more nuanced assessment of model performance.</p>
<p>The automated benchmark generation framework (ClimaGen) integrates domain-specific textbooks and natural language understanding of LLMs along with human expertise to produce high-quality QA data at scale.However, the benchmark's reliance on only five textbooks limits the diversity of contexts, and the small size of the annotated dataset constrains the effectiveness of automated annotation.Addressing these limitations with a broader corpus and expanded annotation data will improve future benchmarks.</p>
<p>While models like GPT-4o performed well on reasoning-based tasks, the overall performance of models highlights the ongoing challenge of achieving consistent scientific accuracy.In conclusion, ClimaQA sets a new standard for evaluating scientific question-answering models, providing a foundation for future advancements in AI-driven climate research.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>The ClimaQA dataset, both gold and silver, is publicly available at Hugging Face1 .While we cannot release the scraped textbook data due to copyright restrictions, the references to all textbooks used are provided in the appendixA.1,allowing for reconstruction of this dataset.Our complete codebase, including data generation pipeline, web-UI and model evaluation scripts, is available in our GitHub repository2 .The ClimaGen framework is fully reproducible; however, training the evaluator models requires domain expert inputs, which may introduce variability.Furthermore, reproduction of all parts of our framework requires appropriate API keys for external services.</p>
<p>A APPENDIX</p>
<p>A.1 TEXTBOOK DATASET Table 4: List of climate textbooks used in this work along with the number of pages and the number of context chunks.The extracted data was preprocessed to omit figures and tables.The content of each page was then split into overlapping context chunks of around 2000 characters.</p>
<p>Textbook Pages Chunks</p>
<p>Aerosol Measurement (Baron &amp; Willeke, 2001) 937 1813 Aerosols and Climate (Carslaw, 2022) 854 1766 Airborne CCN Measurements (Trembath, 2013) 268 333 An Introduction to Clouds (Lohmann et al., 2016) 377 606 Atmospheric Chemistry and Physics (Seinfeld &amp; Pandis, 2016) 1127 1789 Atmospheric Science (Wallace &amp; Hobbs, 2006) 495 1085 Calculus of Variations (Gelfand &amp; Fomin, 2012) 239 266 Clouds in the Perturbed Climate System (Heintzenberg &amp; Charlson, 2009) 598 1060 Eloquent Science (Schultz, 2013) 420 725 Filtering Complex Turbulent Systems (Majda &amp; Harlim, 2012) 358 475 Fundamental of Atmospheric Modeling (Jacobson, 2005) 826 1204 Geostatistics for Environmental Scientists (Webster &amp; Oliver, 2007) 327 457 Global Physical Climatology (Hartmann, 2015) 481 715 Principles Of Planetary Climate (Pierrehumbert, 2010) 468 1096 Forests and Climate Change (De Wasseige et al., 2015) 119 250 Simulating Nature (Petersen, 2012) 205 386 Statistical Methods in the Atmospheric Sciences (Wilks, 2019) 806 1597 Stochastic Climate Models (Imkeller &amp; von Storch, 2012) 411 594</p>
<p>To assess the proficiency of LLMs in climate science, we utilized graduate-level climate science textbooks as a reliable source of specialized knowledge.These textbooks were selected for their accurate and comprehensive representation of the technical terminology and nuanced theories integral to the field.The collection was curated from the virtual bookshelf of a professor in atmospheric physics and broadly represents a mixture of graduate and expert textbooks on the physical climate, with a particular focus on the role of aerosol in the climate system (which provides one of the key uncertainties in climate projections).The complete list of textbooks is provided in Table 4.</p>
<p>Figure 6: The distribution of ClimaQA-Gold questions over the corresponding source.</p>
<p>The five textbooks selected for question generation were carefully chosen to reflect different levels of breadth and depth (both technically and more qualitatively) across a range of important topics in climate science.These include Atmospheric Science, a comprehensive graduate-level textbook on atmospheric science; Aerosols and Climate, a brand-new and more detailed textbook on the role of aerosol in the climate system; An Introduction to Clouds, another relatively new textbook providing the latest research on the important role of clouds in the climate system; Geostatistics for</p>
<p>A.2.2 CLIMAQA-SILVER EXPERIMENTS</p>
<p>In this section, we assess the models in their default settings on a subset of the ClimaQA-Silver dataset, consisting of 200 questions for each task type.The column-wise trends observed in the results are largely consistent with those from the ClimaQA-Gold dataset.However, a notable difference lies in the relative difficulty of the complexities in the MCQ task, even though the overall scores do not vary significantly.This discrepancy highlights the intricate nature of the MCQ generation process.Additionally, this variation may also be attributed to the relatively small number of questions in each column.Future work should focus on scaling up the expert validation process to enhance the quality of the automated annotation pipeline, thereby addressing these challenges and improving overall dataset reliability.In this section, we present sample questions from the ClimaQA-Gold dataset along with responses from various LLMs and their corresponding evaluation scores.Freeform QA In this example, the reference answer discusses increased global emissions in the absence of air quality measures.While the outputs of most LLMs align with this reference, the response from gemma-27b suggests a decline in global emissions.Note that both the Factual Accuracy and BLEU metrics penalize this response, whereas the BERTScore does not.</p>
<p>Quesition: How might the global emission estimates have been different if air quality measures were not introduced mainly targeted at large, coal-burning installations before 2000?</p>
<p>Reference Answer: Without the introduction of air quality measures mainly targeted at large, coal-burning installations before 2000, the global emission estimates, particularly for SO2, may not have shown a decrease before 2000.The emissions could have continued to rise or remained at higher levels due to the ongoing industrial growth and lack of regulations to curb pollution from such sources.gpt-3.5-turbo:Global emission estimates would likely have been higher if air quality measures targeting large, coal-burning installations were not introduced before 2000, as these measures have helped to reduce emissions of pollutants such as sulfur dioxide and particulate matter.</p>
<p>Bleu: 0.485, Bert: 0.901, Factual Accuracy: 0.988 gpt-4o: Without air quality measures targeting large, coal-burning installations before 2000, global emissions, particularly sulfur dioxide and particulate matter, would likely have been significantly higher, exacerbating air pollution and related health issues.This could have also contributed to more severe acid rain and potentially altered climate patterns due to increased atmospheric aerosols.</p>
<p>Bleu: 0.589, Bert: 0.898, Factual Accuracy: 0.982 llama3-70b: If air quality measures were not introduced before 2000, global emission estimates would likely be higher, as coal-burning installations would have continued to emit large amounts of pollutants, including particulate matter, sulfur dioxide, and nitrogen oxides, contributing to increased greenhouse gas emissions.This could have resulted in an additional 1-2 gigatons of CO2 equivalent emissions per year, exacerbating climate change impacts.</p>
<p>Bleu: 0.542, Bert: 0.888, Factual Accuracy: 0.968 mixtral-8x22b: If air quality measures hadn't been introduced for large, coal-burning installations before 2000, global emission estimates would likely have been significantly higher due to unregulated emissions from these sources.This could have potentially accelerated global warming and worsened air quality on a global scale.</p>
<p>Bleu: 0.546, Bert: 0.910, Factual Accuracy: 0.988 gemma-27b: Global emission estimates would likely be significantly higher without those air quality measures.Reduced emissions from coal-burning power plants would have contributed less to the overall decline in global emissions observed since 2000.</p>
<p>Bleu: 0.364, Bert: 0.885, Factual Accuracy: 0.741</p>
<p>In the following example, the reference answer indicates that the distance from the sun influences surface pressure, with higher pressures during summer.Responses from gpt-3.5-turbo and mixtral-8x22b contradict this statement but still achieve high BLEU and BERTScore values.The Factual Accuracy metric, however, correctly identifies and penalizes this inconsistency.Notably, gpt-4o provides a correct answer yet receives a low BLEU score, as BLEU is not robust against paraphrasing.</p>
<p>Quesition: How does the distance from the sun affect the surface pressure on Mars?</p>
<p>Reference Answer: The surface pressure on Mars varies seasonally from 7.0 hPa during the austral winter, when Mars is farthest from the sun, to 9.0 hPa during the austral summer.gpt-3.5-turbo:The distance from the sun does not directly affect the surface pressure on Mars; instead, the surface pressure on Mars is primarily influenced by the planet's atmospheric composition and temperature.</p>
<p>Bleu: 0.388, Bert: 0.880, Factual Accuracy: 0.175 gpt-4o: The distance from the Sun affects Mars' temperature, which in turn influences the sublimation of its polar ice caps, impacting atmospheric pressure.During warmer periods, more CO2 sublimates, increasing surface pressure, while colder periods result in lower pressure.</p>
<p>Bleu: 0.253, Bert: 0.865, Factual Accuracy: 0.980 llama3-70b: The distance from the Sun affects the surface pressure on Mars by controlling the amount of atmospheric gas that freezes out or escapes to space, with more gas freezing out at greater distances.As a result, Mars' surface pressure is about 1% of Earth's, due to its average distance from the Sun being about 1.5 times greater than Earth's.</p>
<p>Bleu: 0.262, Bert: 0.861, Factual Accuracy: 0.896 mixtral-8x22b: The distance from the sun does not significantly affect the surface pressure on Mars.Instead, Mars' thin atmosphere and low gravity are the primary factors influencing its surface pressure.</p>
<p>Bleu: 0.403, Bert: 0.874, Factual Accuracy: 0.051 gemma-27b: Mars is farther from the sun than Earth, receiving less solar radiation.This weaker solar heating results in a much lower surface pressure on Mars compared to Earth.</p>
<p>Bleu: 0.458, Bert: 0.870, Factual Accuracy: 0.905 Interestingly, gemma-27b provides an irrelevant response that nevertheless receives high scores.While the response is factually accurate, it fails to address the question, illustrating limitations in existing evaluation metrics.This highlights the need for more fine-grained evaluation methodologies, which future research should aim to develop.</p>
<p>Cloze QA In the following examples, the models generate answers that differ from the reference answer but remain correct within the given context.This is effectively captured by the phrase similarity metric, which assigns higher scores to responses that are contextually aligned with the reference answer.</p>
<p>Question: Aqueous-phase oxidation of ⟨mask⟩ to form sulfuric acid in cloud droplets is a crucial process that contributes significantly to global sulfate mass and has long been recognized as important for aerosols and climate.</p>
<ol>
<li>
<p>Questions should be clear, concise, and free from unnecessary complexity or ambiguity.</p>
</li>
<li>
<p>Avoid overly long sentences and use consistent phrasing for repeated items.</p>
</li>
<li>
<p>Ensure questions are self-contained and provide all necessary context.4. Do not include phrases like "According to the provided context..." 5. Do not make any references to the given context in the question 6. Ensure that distractors do not overlap by reflecting different misconceptions on the topic.7. Minimize clues that make the correct answer obvious.8. Use "None of the Above" or "All of the Above" sparingly.9.Each MCQ must have exactly four answer choices (one correct, three distractors).10.Questions should not rely on external figures or tables.</p>
</li>
</ol>
<p>The user will provide one main context and some retrieved contexts separated by '-------' as the input.</p>
<p>Use details from retrieved context only if they are relevant to your question.</p>
<p>You must output a single JSON object in the following format: The following prompt was used to generate a freeform question-answer pair: You are a question paper setter creating freeform questions (MCQs) from a graduate-level climate science textbook.Your question must be related to a provided context.</p>
<p>Please respect the following rules to generate the question:</p>
<p>-The answer to the question should be found inside the provided context -The question must be self-contained -Do not include phrases such as "According to the provided context.."</p>
<p>The user will provide one main context and some retrieved contexts separated by '-------' as the input.</p>
<p>Use details from retrieved context only if they are relevant to your question.</p>
<p>You must output a single JSON objectin the following format: {</p>
<p>Figure 2 :
2
Figure 2: Examples of Question Evolution.The first is the initial version of the generated question.The second is the enhanced version of the question that requires scientific reasoning to answer.The third is the modified version of the question that involves a hypothetical scenario.The contexts from the textbook data were used during the question evolution.</p>
<p>Figure 3 :
3
Figure 3: Examples of the three types of scientific question-answering tasks presented in our benchmark</p>
<p>Figure 4 :
4
Figure 4: Mean Phrase Similarity for Correctly Answered and Incorrectly Answered Cloze Questions</p>
<p>Figure 5 :
5
Figure 5: Analysis of various LLMs under default setting on different tasks and different complexities.The first figure shows accuracy of models in the MCQ task while the others show different metrics under the Freeform task</p>
<p>Here c is the correct answer.Replace it with the actual correct answer.Make sure you return a valid JSON object.</p>
<p>Figure 7 :
7
Figure 7: The distribution of different reasons selected for scientific invalidity of generated QA pairs by domain experts.</p>
<p>Figure 8 :
8
Figure 8: Precision across various train-test splits of labeled data under different conditions: no validation (all data classified as positive), zero-shot validation, and fine-tuned validation.The test sets consisted of around 40 examples in each case.</p>
<p>Table 1 :
1
Comparison of scientific benchmarks.Automated indicates automatic creation, Validated shows expert validation, Multi-task represents multiple task types, and Multi-level represents questions of varying complexity
DatasetDomainSourceSize Automated Validated Multi-Task Multi-LevelScienceQAScience Hi-Scl Text 21000✗✓✗✗Pira2OceanResearch 2250✗✓✓✗SciQAComp Sci ORKG2500✓✓✗✗Climate Crisis ClimateNone20000✓✗✗✗SciQAG-24DScienceResearch 8531✓✗✗✗ClimaQA-Gold Climate Grad Text 566✓✓✓✓ClimaQA-Silver Climate Grad Text 3000✓✗✓✓</p>
<p>Table 1 presents a comparison of various scientific benchmarks.</p>
<p>Previous work on automated MCQ generation has focused on selecting keywords and generating distractors based on contextual information.(A.Nwafor &amp; E. Onyenwe, 2021)) utilize traditional NLP techniques, such as TF-IDF, for keyword extraction, while (Mehta et al., 2021) leverage BERT</p>
<p>Table 2 :
2
Contents of the ClimaQA dataset.Both ClimaQA-Gold and ClimaQA-Silver include 3 task-forms with varying levels of complexity for MCQ and Freeform.
DatasetTaskBase Reasoning Hypothetical TotalMCQ1267247245ClimaQA-GoldFreeform 545255161Cloze---160MCQ5012642351000ClimaQA-SilverFreeform 5072412521000Cloze---1000</p>
<p>Table 3 :
3
Performance analysis of various state-of-the-art LLMs on MCQs and Cloze QA.While source represents the set of books used for QA generation, held-out represents the remaining set of books.Bold marks the max within a model's variants and green highlights the overall column max.
ModelMCQClozeBase Reason Hypo Overall EM PSgemma-27b81.75 72.22 82.98 79.18 49.38 0.87gemma-27b-fs80.95 77.78 82.98 80.41 52.50 0.88gemma-27b-rag-source90.48 80.56 78.72 85.31 56.88 0.90gemma-27b-rag-held-out79.37 76.39 78.72 78.37 45.62 0.85gpt-3.5-turbo74.34 69.91 74.47 73.06 43.12 0.81gpt-3.5-turbo-fs76.98 74.54 76.60 76.19 43.75 0.78gpt-3.5-turbo-rag-source80.42 80.09 77.30 79.73 68.75 0.92gpt-3.5-turbo-rag-held-out70.63 71.30 69.50 70.61 39.38 0.81gpt-4o86.77 86.11 82.27 85.71 53.12 0.88gpt-4o-fs87.83 87.50 80.85 86.39 56.25 0.89gpt-4o-rag-source95.77 91.67 86.52 92.79 71.88 0.94gpt-4o-rag-held-out82.80 80.56 81.56 81.90 50.62 0.88llama3-70b84.92 80.56 82.98 83.27 38.75 0.82llama3-70b-fs82.54 81.94 82.98 82.45 48.12 0.85llama3-70b-rag-source92.06 84.72 87.23 88.98 63.12 0.91llama3-70b-rag-held-out80.95 76.39 85.11 80.41 43.75 0.84mixtral-8x22b80.16 79.17 80.85 80.00 35.62 0.75mixtral-8x22b-fs80.95 81.94 80.85 81.22 45.00 0.83mixtral-8x22b-rag-source90.48 80.56 76.60 84.90 45.00 0.78mixtral-8x22b-rag-held-out80.16 73.61 74.47 77.14 28.12 0.65mistral-7b64.29 63.89 82.98 67.76 17.50 0.74mistral-7b-fs65.08 66.67 76.6 67.76 33.12 0.80mistral-7b-rag-source92.86 84.72 85.11 88.98 57.5 0.88mistral-7b-rag-held-out66.67 62.5 74.47 66.94 22.50 0.76mistral-7b-cp-held-out72.22 62.5 74.47 69.8 21.25 0.76mistral-7b-ft-silver73.81 75.00 80.85 75.51 41.88 0.83llama3.1-8b76.98 62.50 65.96 70.61 26.25 0.77llama3.1-8b-fs76.19 72.22 76.60 75.10 38.75 0.82llama3.1-8b-rag-source94.44 83.33 89.36 90.2 72.50 0.92llama3.1-8b-rag-held-out76.19 66.67 76.6 73.47 36.25 0.81llama3.1-8b-cp-held-out77.78 75.00 72.34 75.92 30.00 0.77llama3.1-8b-ft-silver74.6 70.83 72.34 73.06 51.25 0.85gemini-1.5-flash82.54 73.61 87.23 80.82 50.62 0.88gemini-1.5-flash-long-cxt-source88.1 75.00 80.85 82.86 51.88 0.89gemini-1.5-flash-long-cxt-held-out 70.63 70.83 78.72 72.24 46.88 0.87</p>
<p>Table 6 :
6
Performance analysis of various LLMs on MCQs and Cloze QA in ClimaQA-Silver
ModelMCQClozeBase Reason Hypo Overall EM PSgemma-27b78.00 84.91 76.6 79.5 50.00 0.85gpt-3.5-turbo 76.00 75.47 72.34 75.0 38.00 0.78gpt-4o88.00 84.91 78.72 85.00 60.50 0.88llama3-70b85.00 79.25 65.96 79.00 44.00 0.83mixtral-8x22b 80.00 81.13 82.98 81.00 33.00 0.67</p>
<p>Table 7 :
7
Performance analysis of various LLMs on Freeform QA in ClimaQA-Silver
ModelBLEUBERTScoreFactual AcccuracyBRHOBRHOBRHOgemma-27b0.392 0.441 0.365 0.398 0.870 0.880 0.886 0.876 0.78 0.86 0.85 0.81gpt-3.5-turbo 0.467 0.523 0.545 0.500 0.885 0.892 0.907 0.892 0.71 0.79 0.84 0.76gpt-4o0.440 0.493 0.491 0.465 0.880 0.887 0.902 0.887 0.80 0.88 0.86 0.84llama3-70b0.335 0.474 0.569 0.425 0.874 0.888 0.904 0.885 0.77 0.82 0.75 0.78mixtral-8x22b 0.394 0.516 0.485 0.447 0.877 0.890 0.904 0.887 0.80 0.88 0.83 0.82A.2.3 CASE STUDY
https://huggingface.co/datasets/UCSD-GENIE/ClimaQA
https://github.com/Rose-STL-Lab/genie-climaqa
ACKNOWLEDGEMENTThis work was supported in part by the U.S. Army Research Office under Army-ECASE award W911NF-07-R-0003-03, the U.S. Department Of Energy, Office of Science, IARPA HAYSTAC Program, and NSF Grants #2205093, #2146343, #2134274, CDC-RFA-FT-23-0069, DARPA AIE FoundSci and DARPA YFA.We thank Sophie Wynn, Varan Madan, and David Vishny for providing expert validation of the ClimaQA dataset.Environmental Scientists, a more technical graduate-level textbook on geostatistics for environmental sciences; as well as Calculus of Variations, a classic calculus textbook to test the extent of the technical ability of the models.The exact distribution of questions from these textbooks is shown in Figure6.The remaining textbooks were used in the rag-held-out experiments to test the models without directly exposing them to the sources of the benchmark.A.2 MORE EXPERIMENTSA.2.1 FREEFORM EVALUATION RESULTS We used the ClimaQA-Silver dataset to fine-tune Llama3.1-8B and Mistral-7B-v0.3 on questions with the various question forms and complexities seen in the ClimaQA-Gold to see how this taskspecific fine-tuning can affect the performance.For this purpose, we used the hyperparameters, shown in 2. Correct Answer: The indisputable correct response to the stem.3. Distractors: Three incorrect but plausible answers.They should be:-Related to the stem and correct answer.-Positively phrased and true statements that don't answer the stem.-Plausible but incorrect, without giving clues to the correct answer.-Unique, each reflecting different misconceptions if possible.MCQ Guidelines:question: ⟨question⟩, answer: ⟨answer⟩ } Make sure you return a valid JSON object.The following prompt was used to generate a scientific statement for cloze question-answer generation:You are a scientific annotator.Given a scientific context from a climate textbook, generate a scientific statement based on the facts presented in the context.Please respect the following rules to generate the statement:-Generate only a single sentence -No external knowledge should be used or refered in generating the statement -Do not use phrases like 'based on the provided context.'The user will provide one main context and some retrieved contexts seperated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON objectin the following format:Make sure you return a valid JSON object.A.4.2 COMPLEXITY ADDITIONThe following prompt was used to add reasoning complexity to the base freeform question-answer pair.Given a question-answer pair generated from the given context, Modify the question-answer pair to incorporate multi-step reasoning.Please respect the following rules to generate the question:-Answering the new question should encourage applying knowledge from 'Context' to deduce outcomes.-The new question must be fully answerable from 'Context'.-No external knowledge should be required to answer the new question -The question should not be dependent on external things such as figures or tables -Do not use phrases like 'based on the provided context.'The user will provide the original question, context, and some retrieved contexts separated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON objectin the following format: The following prompt was used to add hypothetical scenario complexity to the base freeform question-answer pair.Given a question-answer pair generated from the given context, Modify the question-answer pair to incorporate a hypothetical or speculative scenario.Please respect the following rules to generate the question:-Answering the new question should encourage applying knowledge from 'Context' to deduce outcomes.-The new question must be fully answerable from 'Context'.-No external knowledge should be required to answer the new question -The question should not be dependent on external things such as figures or tables -Do not use phrases like 'based on the provided context.'The user will provide the original question, context, and some retrieved contexts separated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON object in the following format: The same prompts with modified output format were used to add complexities to the MCQs as well.A.5 ANNOTATIONA.5.1 EXPERT VALIDATIONDuring the validation process of the ClimaQA-Gold dataset for both multiple-choice and freeform questions, experts were asked to select predefined reasons for rejecting a generated QA pair.They also had the option to provide a custom reason for rejection.The figure below illustrates the various types of errors made by the ClimaGen pipeline during the generation phase.While most rejection reasons highlight limitations of the generator LLM, the category of bad context points to instances where the seed context used for QA generation was inherently flawed or lacked meaningful information.Addressing this issue through improved preprocessing techniques is a key area for future work.Published as a conference paper at ICLR 2025A.5.3 AUTOMATED ANNOTATION -CLOZETo automate cloze annotation, we fine-tuned gpt-4o-mini with the following prompt to pick the scientific term to be masked.The dataset consisted of a total of 160 expert-labeled examples.You are a climate cloze generator that marks a scientific term from the given scientific statement to be masked for cloze question-answering The scientific term has to be a single word from the given statement that has a significant impact if removed You will be provided the following as the input:Statement: ⟨statement⟩Respond with just one word
An automated multiple-choice question generation using natural language processing techniques. A Chidinma, Ikechukwu E Nwafor, Onyenwe, 10.5121/ijnlc.2021.10201International Journal on Natural Language Computing. 2319-41111002April 2021</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Automatic generation of multiple choice questions using dependency-based semantic relations. Naveed Afzal, Ruslan Mitkov, 10.1007/s00500-013-1141-4Soft Computing. 1872014</p>
<p>The sciqa scientific question answering benchmark for scholarly knowledge. Sören Auer, Dante A C Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, Ivan Shilin, Markus Stocker, Eleni Tsalapati, 10.1038/s41598-023-33607-zScientific Reports. 1312023</p>
<p>More than reading comprehension: A survey on datasets and metrics of textual question answering. Yang Bai, Daisy Zhe, Wang , arXiv:2109.122642021arXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Aerosol Measurement: Principles, Techniques, and Applications. A Wiley-Interscience publication. P A Baron, K Willeke, 2001Wiley</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Assessing large language models on climate information. Jannis Bulian, Mike S Schäfer, Afra Amini, Heidi Lam, Massimiliano Ciaramita, Ben Gaiarin, Michelle Chen Hübscher, Christian Buck, Niels G Mede, Markus Leippold, Nadine Strauß, 2024</p>
<p>LLM-assisted modeling and simulations for public sector decision-making: Bridging climate data and policy insights. Charles Cao, Jie Zhuang, Qiang He, AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design. 2024</p>
<p>Aerosols and Climate. K S Carslaw, 2022Elsevier</p>
<p>Universal sentence encoder. Cer, arXiv:1803.111752018arXiv preprint</p>
<p>Santanu Phadikar, and Arif Ahmed Sekh. Multiple-choice question generation with auto-generated distractors for computer-assisted educational assessment. Bidyut Das, Mukta Majumder, Multimedia Tools and Applications. 202180</p>
<p>The forests of the congo basin-forests and climate change. Carlos De Wasseige, Martin Tadoum, Eba ' Atyi, Charles Doumenge, 2015</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Climate-fever: A dataset for verification of real-world climate claims. Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold, arXiv:2012.006142020arXiv preprint</p>
<p>A comparative study of ai-generated (gpt-4) and human-crafted mcqs in programming education. Jacob Doughty, Zipiao Wan, Anishka Bompelli, Jubahed Qayum, Taozhi Wang, Juran Zhang, Yujia Zheng, Aidan Doyle, Pragnya Sridhar, Arav Agarwal, Proceedings of the 26th Australasian Computing Education Conference. the 26th Australasian Computing Education Conference2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Calculus of Variations. I M Gelfand, S V Fomin, 2012Dover Publications</p>
<p>Docimological quality analysis of llm-generated multiple choice questions in computer science and medicine. Christian Grévisse, Maria Angeliki S Pavlou, Jochen G Schneider, SN Computer Science. 556362024</p>
<p>Mcqgen: A large language model-driven mcq generator for personalized learning. Ching Nam, Hang , Chee Wei Tan, Pei-Duo Yu, IEEE Access. 2024</p>
<p>. D L Hartmann, Global Physical Climatology. International Geophysics. Elsevier Science. 2015</p>
<p>Clouds in the Perturbed Climate System: Their Relationship to Energy Balance, Atmospheric Dynamics, and Precipitation. J Heintzenberg, R J Charlson, Strüngmann Forum reports. Mass.. 2009</p>
<p>Lora: Low-rank adaptation of large language models. J Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen, ArXiv, abs/2106.096852021</p>
<p>Supervised word mover's distance. Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, Kilian Q Weinberger, Advances in neural information processing systems. 292016</p>
<p>Stochastic Climate Models. P Imkeller, J S Storch, Progress in Probability. Birkhäuser Basel. 2012</p>
<p>Fundamentals of Atmospheric Modeling. Fundamentals of Atmospheric Modeling. M Z Jacobson, 2005Cambridge University Press</p>
<p>Open research knowledge graph: next generation infrastructure for semantic scholarly knowledge. Mohamad Yaser, Jaradeh , Allard Oelen, Kheir Eddine Farfar, Manuel Prinz, D' Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, Proceedings of the 10th international conference on knowledge capture. the 10th international conference on knowledge capture2019</p>
<p>Devendra Singh Chaplot, Diego de las Casas. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>An Introduction to Clouds: From the Microscale to Climate. U Lohmann, F Lüönd, F Mahrt, 2016Cambridge University Press</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Filtering Complex Turbulent Systems. A J Majda, J Harlim, 2012Cambridge University Press</p>
<p>Automated mcq generator using natural language processing. Pritam Kumar Mehta, Prachi Jain, Chetan Makwana, Raut, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications. the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications2021</p>
<p>Evalquiz-llm-based automated generation of self-assessment quizzes in software engineering education. Niklas Meißner, Sandro Speth, Julian Kieslinger, Steffen Becker, Software Engineering im Unterricht der Hochschulen 2024. 2024Gesellschaft für Informatik eV</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>My climate advisor: An application of NLP in climate adaptation for agriculture. Vincent Nguyen, Sarvnaz Karimi, Willow Hallgren, Ashley Harkin, Mahesh Prakash, ; Jingwei Ni, Tobias Schimanski, Kalyan Dutia, Alok Singh, Julia Bingler, Christophe Christiaen, Neetu Kushwaha, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (Cli-mateNLP 2024). Saeid A Veruska Muccione, Markus Vaghefi, Leippold, the 1st Workshop on Natural Language Processing Meets Climate Change (Cli-mateNLP 2024)Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024Dominik Stammbach,</p>
<p>Generating multiple choice questions from a textbook: Llms match human performance on most metrics. Andrew M Olney, LLM@ AIED. 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Simulating Nature: A Philosophical Study of Computer-Simulation Uncertainties and Their Role in Climate Science and Policy Advice. A C Petersen, 2012CRC PressSecond Edition</p>
<p>Principles of Planetary Climate. R T Pierrehumbert, 2010Cambridge University Press</p>
<p>Benchmarks for Pirá 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. Data Intelligence. Paulo Pirozelli, Marcos M José, Igor Silveira, Flávio Nakasato, Sarajane M Peres, A F Anarosa, Anna H R Brandão, Fabio G Costa, Cozman, 10.1162/dint_a_002456</p>
<p>Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Automatic keyword extraction from individual documents. Text mining: applications and theory. Stuart Rose, Dave Engel, Nick Cramer, Wendy Cowley, 2010</p>
<p>Are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification. Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren, 2024</p>
<p>Eloquent Science: A Practical Guide to Becoming a Better Writer, Speaker, and Atmospheric Scientist. SpringerLink : Bücher. D Schultz, 2013American Meteorological Society</p>
<p>Atmospheric Chemistry and Physics: From Air Pollution to Climate Change. J H Seinfeld, S N Pandis, 2016Wiley</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024aarXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024barXiv preprint</p>
<p>Climategpt: Towards ai synthesizing interdisciplinary research on climate change. David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian Van Wyk, Abdallah Nasir, Hayden Goldstein, Taylor Tragemann, Katie Nguyen, Ariana Fowler, Andrew Stanco, Jon Gabriel, Jordan Taylor, Dean Moro, Evgenii Tsymbalov, Juliette De Waal, Evgeny Matusov, Mudar Yaghi, Mohammad Shihadah, Hermann Ney, Christian Dugast, Jonathan Dotan, Daniel Erasmus, 2024</p>
<p>Airborne CCN measurements. James Trembath, 2013The University of Manchester (United Kingdom</p>
<p>Atmospheric Science: An Introductory Survey. International Geophysics. M Wallace, P V Hobbs, 2006Academic Press</p>
<p>Sciqag: A framework for auto-generated science question answering dataset with fine-grained evaluation. Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster, 2024</p>
<p>Geostatistics for Environmental Scientists. R Webster, M A Oliver, Statistics in Practice. 2007Wiley</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Statistical Methods in the Atmospheric Sciences. D S Wilks, 2019Elsevier</p>
<p>Star: Constraint lora with dynamic active learning for data-efficient fine-tuning of large language models. Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu, arXiv:2403.011652024arXiv preprint</p>
<p>Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou, arXiv:2310.19596Llmaaa: Making large language models as active annotators. 2023arXiv preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Climate change from large language models. Hongyin Zhu, Prayag Tiwari, Similarity: 0.885Reference Answer: SO2 gpt-3.5-turbo: Sulfur, Phrase Similarity: 0.819 gpt-4o: sulfur dioxide, Phrase Similarity: 0.885 llama3-70b: Sulfur, Phrase Similarity: 0.819 mixtral-8x22b: sulfur dioxide. 2024Phrase Similarity: 0.885 gemma-27b: sulfur dioxide</p>
<p>Question: The rate of change in the air parcel's moist static energy due to ⟨mask⟩ is determined by the difference between the moist static energies of the environment and the cloud air parcel. Reference Answer: entrainment gpt-3.5-turbo: evaporation, Phrase Similarity: 0.681 gpt-4o: entrainment, Phrase Similarity: 1.000 llama3-70b: entrainment, Phrase Similarity: 1.000 mixtral-8x22b: mixing. Phrase Similarity: 0.726 gemma-27b: entrainment, Phrase Similarity: 1.000</p>            </div>
        </div>

    </div>
</body>
</html>
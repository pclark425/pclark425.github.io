<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-713b604fb9cdd6631074cbd6bf36db029031992e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/713b604fb9cdd6631074cbd6bf36db029031992e" target="_blank">Large Language Models are Zero Shot Hypothesis Proposers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The findings of this paper strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Paper Abstract:</strong> Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Background-and-Hypothesis Pairs (BHP) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporally-split biomedical corpus of (background, hypothesis) text pairs constructed from published literature (papers up to Jan 2023 for training/seen; August 2023 papers for an unseen test), used to evaluate LLM hypothesis generation under visibility control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various evaluated LLMs (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM-series, Vicuna-33b, PMC-LLaMA-13B, MedAlpaca-13B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 33B, 70B, API models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Temporal visibility-split BHP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate LLMs' ability to generate hypotheses from background text with strict temporal visibility control by splitting the corpus at Jan 2023 (training/seen) and using August 2023 papers as unseen test examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, ROUGE, novelty, relevance, significance, verifiability; ChatGPT and human scores; correlation (Pearson, Spearman)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BLEU/ROUGE standard word-overlap metrics (percentage-like scores); novelty/relevance/significance/verifiability scored 0–3 by ChatGPT and humans; correlations reported as Pearson r and Spearman ρ.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP (Background-and-Hypothesis Pairs) dataset: training/seen = 2700 pairs (2500 train + 200 val), unseen = 200 pairs (August 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human eval performed on 100 random examples from the unseen set by three biomedical-background annotators; each metric scored 0–3; inter-method coherence measured with Pearson and Spearman correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset sizes: seen ~2700 pairs, unseen 200 pairs; used for zero-shot/few-shot/fine-tune experiments reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM outputs compared against golden (human-authored) hypotheses from literature for word-overlap metrics and were also evaluated by human annotators; ChatGPT evaluations correlated strongly with human ratings (Pearson and Spearman >0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Temporal split controls visibility but cannot fully guarantee absolute non-exposure for some closed models; dataset covers biomedical domain only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bilingual Evaluation Understudy (BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard automated word-overlap metric measuring n-gram overlap between generated and reference texts, used here to quantify match between generated hypotheses and golden hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various evaluated LLMs (see paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine (evaluation of generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU word-overlap evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute BLEU scores between generated hypothesis text and golden/reference hypothesis text using the evaluate library.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU score (numeric; higher = greater n-gram overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard BLEU score (range 0–100 or 0–1 depending on implementation); paper reports values like 5–22 (table values) consistent with evaluate library outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP dataset (seen and unseen splits)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable for metric itself; used alongside human evaluation for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BLEU values reported per model and prompt setting (e.g., PMC-LLaMA-13B(0-shot) BLEU ~22.89 on seen), with wide variation across models and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>BLEU correlated poorly with human judgments overall (low/negative Pearson and Spearman correlations reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Word-overlap metrics like BLEU are inadequate for hypothesis evaluation because hypothesis space is vast and correct/novel hypotheses may not share surface tokens with references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence variant of ROUGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recall-oriented word-overlap metric (ROUGE-L) used to measure overlap between generated hypotheses and reference hypotheses, reported alongside BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L word-overlap evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ROUGE-L (LCS-based) between generated and reference hypotheses using the evaluate library to quantify lexical overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-L score (numeric; higher = greater overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ROUGE-L typically reported as percentage-like scores; paper reports values such as ~20–40 in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used for comparison to human scores; correlations computed.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ROUGE-L scores reported per model/setting (e.g., PMC-LLaMA-13B(0-shot) ROUGE ~40.36 on seen).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>ROUGE-L showed limited correlation with manual evaluations and is insufficient alone to evaluate hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Like BLEU, ROUGE-L is limited because valid novel hypotheses may not lexically overlap with references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8003.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty/Relevance/Significance/Verifiability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four-dimension hypothesis evaluation metrics (Novelty, Relevance, Significance, Verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bespoke 4-metric evaluation suite designed in this paper to assess generated hypotheses across novelty, relevance to background, scientific significance, and verifiability; each metric scored on a 0–3 integer scale by ChatGPT and by human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various evaluated LLMs (LLM outputs were scored)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>4-dimension scoring (Novelty, Relevance, Significance, Verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ask ChatGPT (and human annotators) to rate each generated hypothesis along four aspects (novelty, relevance, significance, verifiability) with scores 0–3 and provide step-by-step justification for scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-aspect integer scores (0–3) and their average (Avg)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each metric is an integer from 0 (poor) to 3 (excellent); Avg is mean across the four metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP dataset (unseen test used for reported automatic and human evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation: 100 unseen examples; three biomedical annotators; same 0–3 scale; Pearson and Spearman correlations between ChatGPT and human scores reported (>0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChatGPT average Eval.Avg scores per model typically ~1.4–2.2; paper reports Avg values per model in Table 1 and human evaluation correlations in Table 2 (Pearson and Spearman >0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>ChatGPT's scores strongly correlated with human judgments (Pearson and Spearman >0.7), supporting ChatGPT as a proxy annotator for these metrics; word-overlap metrics correlated poorly with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Metrics are coarse (0–3) and partly subjective; validity depends on quality of ChatGPT explanations and human annotator expertise; need for more advanced automated metrics acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8003.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM (ChatGPT) as annotator/evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ChatGPT (gpt-3.5-turbo) to automatically rate generated hypotheses across the four designed metrics (novelty, relevance, significance, verifiability) and provide step-by-step justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API model (OpenAI GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ChatGPT automatic scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt ChatGPT to score hypotheses on the 0–3 scale for the four metrics and produce explanations; use these automated scores to compare with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Avg automated score and per-aspect 0–3 ratings; coherence correlation (Pearson, Spearman) with human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores are integers 0–3; coherence measured as Pearson r and Spearman ρ between ChatGPT and human averages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP unseen test set (100 examples sampled for human/ChatGPT comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human annotators performed parallel ratings; correlations computed to validate ChatGPT as annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChatGPT evaluation averages per model reported (e.g., gpt-3.5-turbo 0-shot Eval.Avg=1.90); ChatGPT-human Pearson and Spearman correlations >0.7 across models (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>High correlation indicates ChatGPT is a viable proxy for human evaluation on these metrics in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Reliance on ChatGPT's internal judgments may mirror its own biases; automated explanations do not replace domain-expert review; still need improved automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8003.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (manual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert annotation (three biomedical reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual assessment of generated hypotheses by three biomedical-background evaluators scoring novelty, relevance, significance, verifiability on a 0–3 scale for 100 unseen examples to validate automated (ChatGPT) evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated LLMs' outputs (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM variants, PMC-LLaMA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three biomedical-background annotators rated 100 unseen examples on four 0–3 metrics; average scores and inter-method correlations computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-aspect integer scores (0–3); Avg of four metrics; Pearson and Spearman correlations with ChatGPT scores.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each metric integer 0–3; reported correlations quantify agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>100-sample subset of BHP unseen test set (August 2023 papers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three evaluators; 100 examples; metrics scored 0–3; paper reports Pearson and Spearman correlations (many >0.7, some >0.9) between ChatGPT and human averages.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human Avg scores per model reported in Table 2; correlations with ChatGPT evaluations: Pearson and Spearman mostly >0.7 (up to ~0.97 for some models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human ratings validate that ChatGPT evaluations align well with expert judgments; word-overlap metrics show weaker alignment with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation costly and limited in sample size (100 examples); annotator agreement metrics beyond Pearson/Spearman not reported (e.g., Cohen's kappa).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8003.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pearson and Spearman correlation coherence analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical analysis computing Pearson r and Spearman ρ correlations between automated metrics (ChatGPT scores, BLEU, ROUGE-L) and human evaluation scores to assess coherence and validity of automated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>analysis performed across outputs of multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pearson and Spearman correlation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute linear (Pearson) and rank (Spearman) correlations between pairs of evaluation sources (e.g., ChatGPT vs human, BLEU/ROUGE vs human) to quantify alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson r and Spearman ρ values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Numeric correlation coefficients typically between -1 and 1; paper reports coefficients >0.7 indicating strong alignment for ChatGPT-human comparisons, and low/negative correlations for BLEU/ROUGE vs human.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Human-evaluated subset of BHP unseen set (100 examples) and metric outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correlations computed between human-annotated averages and ChatGPT/word-overlap scores.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChatGPT-human Pearson and Spearman correlations >0.7 across models (Table 2); BLEU and ROUGE had notably lower or negative correlations with human scores (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Demonstrated ChatGPT evaluations align well with humans; word-overlap metrics align poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Correlation does not prove equivalence of evaluation quality; sample size limited to 100 unseen examples for correlation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8003.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Instruct pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Instruct data generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-step procedure adapted from Self-Instruct to create instruction-style training pairs: compose paper set, use chatgpt-turbo-3.5 to summarize literature, generate background-hypothesis pairs, filter low-quality data, and split by publication time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>chatgpt-turbo-3.5 used in data generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API GPT-3.5 family</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>data-generation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Self-Instruct inspired data generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Leverage an instructed LLM to summarize literature and propose background-hypothesis pairs, followed by filtering and temporal split to produce BHP dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Quality filtering and subsequent downstream performance (BLEU/ROUGE and 4-metric evaluations) used to assess data usefulness indirectly.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>No new numeric metric introduced; data quality judged via filtering and downstream model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP dataset created using this pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Filtering step includes human-in-the-loop quality control (implicit in description), but exact annotator counts not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Pipeline yielded training/validation/test splits (seen/unseen) and supported fine-tuning experiments; explicit pipeline hyperparameters and steps provided in Section 3.2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Using chatgpt-turbo-3.5 in generation may introduce model-specific artifacts; exact filtering criteria and automation details not fully enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8003.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool use frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct and OpenAI function calling (tool-use frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two approaches used to augment LLMs with external information: ReAct (reasoning-action-observation loop) and OpenAI function calling for direct tool invocation (e.g., PubMed search), tested within multi-agent and single-agent setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (via OpenAI APIs) and other LLMs using ReAct-style prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API models and open-source models used</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / LLM tool augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation methodology augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ReAct and OpenAI function-calling experimental protocols</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ReAct: prompt LLMs to alternate thought/action/observation steps and decide whether to call tools; OpenAI function-calling: directly specify and call search functions (e.g., PubMed) to retrieve literature for hypothesis refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same set of metrics (BLEU, ROUGE, novelty/relevance/significance/verifiability) used to quantify any improvement from tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics as defined elsewhere; tool-use experiments compared metric values with and without tools.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP unseen test set; PubMed used as external retrieval source with literature after Jan 2023 filtered out.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Tool-use outputs included in automatic and human-rated comparisons; explicit human counts as in human evaluation section.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Tool use had minimal or mixed improvement; ReAct performed worse than OpenAI function calling; multi-agent + tool use yielded best Avg in Table 3 (multi-agent + tool Avg ~2.07 vs single-agent baseline Avg ~1.92).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Tool use did not consistently improve novelty and sometimes reduced it while improving verifiability; overall gains modest.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLMs struggled to identify useful information via tools; thought-action-observation loop challenging to implement effectively; filtering of search results to avoid leakage required careful control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8003.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Multi-Agent Collaborative Framework (Analyst, Engineer, Scientist, Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A role-based multi-agent system where distinct agent roles (Analyst extracts keywords; Engineer retrieves and organizes information; Scientist synthesizes hypotheses; Critic critiques and refines) collaborate, optionally with tool access, to generate and iteratively refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multi-agent instantiation using open-source LLMs and OpenAI API models (agents run on same/different LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / general scientific hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis generation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-agent collaborative hypothesis generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run pipeline of role-specific agents (with/without tools) to generate hypotheses; evaluate outputs with BLEU/ROUGE and 4-metric ChatGPT/human scores and compare to single-agent baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, ROUGE, novelty, relevance, significance, verifiability, and Avg score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics defined previously; multi-agent experiment table reports these values for configurations with/without tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP unseen test set</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Top-performing multi-agent outputs included in human evaluation sampling described earlier.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Multi-agent w/o tools Avg ~2.09 and multi-agent with tools Avg ~2.07 in Table 3, outperforming single-agent baselines (baseline Avg ~1.92), indicating role decomposition improved hypothesis generation (esp. novelty and significance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Multi-agent outputs showed higher novelty/significance in automatic ChatGPT evaluations and in some human-rated comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Role orchestration and tool integration are non-trivial; benefits depend on introducing 'uncertainty' and diversity — some tool uses reduce novelty; more rigorous ablation and scaling required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8003.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8003.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>evaluate library</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hugging Face evaluate library</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation toolkit used to compute standard automatic metrics (BLEU and ROUGE) for comparing generated hypotheses to reference hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used for metric computation across LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation for biomedical hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>evaluate.compute (BLEU/ROUGE implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute BLEU and ROUGE scores with the evaluate library's implementations to measure token/sequence overlap between generations and gold hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU and ROUGE-L numeric scores</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Library returns BLEU/ROUGE scores according to standard definitions; reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BHP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable; evaluate library used for automated metric computation only.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Automatic BLEU/ROUGE results presented in Table 1 and Table 3 across models and prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>As with other word-overlap metrics, outputs that are valid but lexically divergent receive low scores; library metrics correlated poorly with human judgments in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>Self-Instruct: Aligning language model with self generated instructions <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8003",
    "paper_id": "paper-713b604fb9cdd6631074cbd6bf36db029031992e",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "BHP dataset",
            "name_full": "Background-and-Hypothesis Pairs (BHP) dataset",
            "brief_description": "A temporally-split biomedical corpus of (background, hypothesis) text pairs constructed from published literature (papers up to Jan 2023 for training/seen; August 2023 papers for an unseen test), used to evaluate LLM hypothesis generation under visibility control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various evaluated LLMs (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM-series, Vicuna-33b, PMC-LLaMA-13B, MedAlpaca-13B, etc.)",
            "model_size": "various (13B, 33B, 70B, API models)",
            "scientific_domain": "biomedicine",
            "theory_type": "hypothesis",
            "evaluation_method_name": "Temporal visibility-split BHP evaluation",
            "evaluation_method_description": "Evaluate LLMs' ability to generate hypotheses from background text with strict temporal visibility control by splitting the corpus at Jan 2023 (training/seen) and using August 2023 papers as unseen test examples.",
            "evaluation_metric": "BLEU, ROUGE, novelty, relevance, significance, verifiability; ChatGPT and human scores; correlation (Pearson, Spearman)",
            "metric_definition": "BLEU/ROUGE standard word-overlap metrics (percentage-like scores); novelty/relevance/significance/verifiability scored 0–3 by ChatGPT and humans; correlations reported as Pearson r and Spearman ρ.",
            "dataset_or_benchmark": "BHP (Background-and-Hypothesis Pairs) dataset: training/seen = 2700 pairs (2500 train + 200 val), unseen = 200 pairs (August 2023).",
            "human_evaluation_details": "Human eval performed on 100 random examples from the unseen set by three biomedical-background annotators; each metric scored 0–3; inter-method coherence measured with Pearson and Spearman correlations.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Dataset sizes: seen ~2700 pairs, unseen 200 pairs; used for zero-shot/few-shot/fine-tune experiments reported in paper tables.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM outputs compared against golden (human-authored) hypotheses from literature for word-overlap metrics and were also evaluated by human annotators; ChatGPT evaluations correlated strongly with human ratings (Pearson and Spearman &gt;0.7).",
            "limitations_noted": "Temporal split controls visibility but cannot fully guarantee absolute non-exposure for some closed models; dataset covers biomedical domain only.",
            "uuid": "e8003.0",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "Bilingual Evaluation Understudy (BLEU)",
            "brief_description": "A standard automated word-overlap metric measuring n-gram overlap between generated and reference texts, used here to quantify match between generated hypotheses and golden hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various evaluated LLMs (see paper tables)",
            "model_size": "various",
            "scientific_domain": "biomedicine (evaluation of generated hypotheses)",
            "theory_type": "hypothesis",
            "evaluation_method_name": "BLEU word-overlap evaluation",
            "evaluation_method_description": "Compute BLEU scores between generated hypothesis text and golden/reference hypothesis text using the evaluate library.",
            "evaluation_metric": "BLEU score (numeric; higher = greater n-gram overlap)",
            "metric_definition": "Standard BLEU score (range 0–100 or 0–1 depending on implementation); paper reports values like 5–22 (table values) consistent with evaluate library outputs.",
            "dataset_or_benchmark": "BHP dataset (seen and unseen splits)",
            "human_evaluation_details": "Not applicable for metric itself; used alongside human evaluation for comparison.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "BLEU values reported per model and prompt setting (e.g., PMC-LLaMA-13B(0-shot) BLEU ~22.89 on seen), with wide variation across models and settings.",
            "comparison_to_human_generated": false,
            "comparison_results": "BLEU correlated poorly with human judgments overall (low/negative Pearson and Spearman correlations reported in Table 5).",
            "limitations_noted": "Word-overlap metrics like BLEU are inadequate for hypothesis evaluation because hypothesis space is vast and correct/novel hypotheses may not share surface tokens with references.",
            "uuid": "e8003.1",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence variant of ROUGE)",
            "brief_description": "A recall-oriented word-overlap metric (ROUGE-L) used to measure overlap between generated hypotheses and reference hypotheses, reported alongside BLEU.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various evaluated LLMs",
            "model_size": "various",
            "scientific_domain": "biomedicine",
            "theory_type": "hypothesis",
            "evaluation_method_name": "ROUGE-L word-overlap evaluation",
            "evaluation_method_description": "Compute ROUGE-L (LCS-based) between generated and reference hypotheses using the evaluate library to quantify lexical overlap.",
            "evaluation_metric": "ROUGE-L score (numeric; higher = greater overlap)",
            "metric_definition": "ROUGE-L typically reported as percentage-like scores; paper reports values such as ~20–40 in tables.",
            "dataset_or_benchmark": "BHP dataset",
            "human_evaluation_details": "Used for comparison to human scores; correlations computed.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ROUGE-L scores reported per model/setting (e.g., PMC-LLaMA-13B(0-shot) ROUGE ~40.36 on seen).",
            "comparison_to_human_generated": false,
            "comparison_results": "ROUGE-L showed limited correlation with manual evaluations and is insufficient alone to evaluate hypothesis quality.",
            "limitations_noted": "Like BLEU, ROUGE-L is limited because valid novel hypotheses may not lexically overlap with references.",
            "uuid": "e8003.2",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Novelty/Relevance/Significance/Verifiability",
            "name_full": "Four-dimension hypothesis evaluation metrics (Novelty, Relevance, Significance, Verifiability)",
            "brief_description": "A bespoke 4-metric evaluation suite designed in this paper to assess generated hypotheses across novelty, relevance to background, scientific significance, and verifiability; each metric scored on a 0–3 integer scale by ChatGPT and by human annotators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various evaluated LLMs (LLM outputs were scored)",
            "model_size": "various",
            "scientific_domain": "biomedicine",
            "theory_type": "hypothesis",
            "evaluation_method_name": "4-dimension scoring (Novelty, Relevance, Significance, Verifiability)",
            "evaluation_method_description": "Ask ChatGPT (and human annotators) to rate each generated hypothesis along four aspects (novelty, relevance, significance, verifiability) with scores 0–3 and provide step-by-step justification for scores.",
            "evaluation_metric": "Per-aspect integer scores (0–3) and their average (Avg)",
            "metric_definition": "Each metric is an integer from 0 (poor) to 3 (excellent); Avg is mean across the four metrics.",
            "dataset_or_benchmark": "BHP dataset (unseen test used for reported automatic and human evaluations)",
            "human_evaluation_details": "Human evaluation: 100 unseen examples; three biomedical annotators; same 0–3 scale; Pearson and Spearman correlations between ChatGPT and human scores reported (&gt;0.7).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ChatGPT average Eval.Avg scores per model typically ~1.4–2.2; paper reports Avg values per model in Table 1 and human evaluation correlations in Table 2 (Pearson and Spearman &gt;0.7).",
            "comparison_to_human_generated": true,
            "comparison_results": "ChatGPT's scores strongly correlated with human judgments (Pearson and Spearman &gt;0.7), supporting ChatGPT as a proxy annotator for these metrics; word-overlap metrics correlated poorly with human judgments.",
            "limitations_noted": "Metrics are coarse (0–3) and partly subjective; validity depends on quality of ChatGPT explanations and human annotator expertise; need for more advanced automated metrics acknowledged.",
            "uuid": "e8003.3",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT-based evaluation",
            "name_full": "LLM (ChatGPT) as annotator/evaluator",
            "brief_description": "Use of ChatGPT (gpt-3.5-turbo) to automatically rate generated hypotheses across the four designed metrics (novelty, relevance, significance, verifiability) and provide step-by-step justifications.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_size": "API model (OpenAI GPT-3.5 family)",
            "scientific_domain": "biomedicine",
            "theory_type": "hypothesis",
            "evaluation_method_name": "ChatGPT automatic scoring",
            "evaluation_method_description": "Prompt ChatGPT to score hypotheses on the 0–3 scale for the four metrics and produce explanations; use these automated scores to compare with human annotations.",
            "evaluation_metric": "Avg automated score and per-aspect 0–3 ratings; coherence correlation (Pearson, Spearman) with human scores.",
            "metric_definition": "Scores are integers 0–3; coherence measured as Pearson r and Spearman ρ between ChatGPT and human averages.",
            "dataset_or_benchmark": "BHP unseen test set (100 examples sampled for human/ChatGPT comparison)",
            "human_evaluation_details": "Human annotators performed parallel ratings; correlations computed to validate ChatGPT as annotator.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ChatGPT evaluation averages per model reported (e.g., gpt-3.5-turbo 0-shot Eval.Avg=1.90); ChatGPT-human Pearson and Spearman correlations &gt;0.7 across models (Table 2).",
            "comparison_to_human_generated": true,
            "comparison_results": "High correlation indicates ChatGPT is a viable proxy for human evaluation on these metrics in this setting.",
            "limitations_noted": "Reliance on ChatGPT's internal judgments may mirror its own biases; automated explanations do not replace domain-expert review; still need improved automated metrics.",
            "uuid": "e8003.4",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Human evaluation (manual)",
            "name_full": "Human expert annotation (three biomedical reviewers)",
            "brief_description": "Manual assessment of generated hypotheses by three biomedical-background evaluators scoring novelty, relevance, significance, verifiability on a 0–3 scale for 100 unseen examples to validate automated (ChatGPT) evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluated LLMs' outputs (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM variants, PMC-LLaMA, etc.)",
            "model_size": "various",
            "scientific_domain": "biomedicine",
            "theory_type": "hypothesis",
            "evaluation_method_name": "Human expert scoring",
            "evaluation_method_description": "Three biomedical-background annotators rated 100 unseen examples on four 0–3 metrics; average scores and inter-method correlations computed.",
            "evaluation_metric": "Per-aspect integer scores (0–3); Avg of four metrics; Pearson and Spearman correlations with ChatGPT scores.",
            "metric_definition": "Each metric integer 0–3; reported correlations quantify agreement.",
            "dataset_or_benchmark": "100-sample subset of BHP unseen test set (August 2023 papers)",
            "human_evaluation_details": "Three evaluators; 100 examples; metrics scored 0–3; paper reports Pearson and Spearman correlations (many &gt;0.7, some &gt;0.9) between ChatGPT and human averages.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human Avg scores per model reported in Table 2; correlations with ChatGPT evaluations: Pearson and Spearman mostly &gt;0.7 (up to ~0.97 for some models).",
            "comparison_to_human_generated": true,
            "comparison_results": "Human ratings validate that ChatGPT evaluations align well with expert judgments; word-overlap metrics show weaker alignment with human ratings.",
            "limitations_noted": "Human evaluation costly and limited in sample size (100 examples); annotator agreement metrics beyond Pearson/Spearman not reported (e.g., Cohen's kappa).",
            "uuid": "e8003.5",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Correlation analysis",
            "name_full": "Pearson and Spearman correlation coherence analysis",
            "brief_description": "Statistical analysis computing Pearson r and Spearman ρ correlations between automated metrics (ChatGPT scores, BLEU, ROUGE-L) and human evaluation scores to assess coherence and validity of automated evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "analysis performed across outputs of multiple LLMs",
            "model_size": "various",
            "scientific_domain": "biomedicine / evaluation methodology",
            "theory_type": "hypothesis evaluation",
            "evaluation_method_name": "Pearson and Spearman correlation",
            "evaluation_method_description": "Compute linear (Pearson) and rank (Spearman) correlations between pairs of evaluation sources (e.g., ChatGPT vs human, BLEU/ROUGE vs human) to quantify alignment.",
            "evaluation_metric": "Pearson r and Spearman ρ values",
            "metric_definition": "Numeric correlation coefficients typically between -1 and 1; paper reports coefficients &gt;0.7 indicating strong alignment for ChatGPT-human comparisons, and low/negative correlations for BLEU/ROUGE vs human.",
            "dataset_or_benchmark": "Human-evaluated subset of BHP unseen set (100 examples) and metric outputs",
            "human_evaluation_details": "Correlations computed between human-annotated averages and ChatGPT/word-overlap scores.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ChatGPT-human Pearson and Spearman correlations &gt;0.7 across models (Table 2); BLEU and ROUGE had notably lower or negative correlations with human scores (Table 5).",
            "comparison_to_human_generated": true,
            "comparison_results": "Demonstrated ChatGPT evaluations align well with humans; word-overlap metrics align poorly.",
            "limitations_noted": "Correlation does not prove equivalence of evaluation quality; sample size limited to 100 unseen examples for correlation analysis.",
            "uuid": "e8003.6",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Instruct pipeline",
            "name_full": "Self-Instruct data generation pipeline",
            "brief_description": "A four-step procedure adapted from Self-Instruct to create instruction-style training pairs: compose paper set, use chatgpt-turbo-3.5 to summarize literature, generate background-hypothesis pairs, filter low-quality data, and split by publication time.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "chatgpt-turbo-3.5 used in data generation",
            "model_size": "API GPT-3.5 family",
            "scientific_domain": "biomedical dataset construction",
            "theory_type": "data-generation methodology",
            "evaluation_method_name": "Self-Instruct inspired data generation",
            "evaluation_method_description": "Leverage an instructed LLM to summarize literature and propose background-hypothesis pairs, followed by filtering and temporal split to produce BHP dataset.",
            "evaluation_metric": "Quality filtering and subsequent downstream performance (BLEU/ROUGE and 4-metric evaluations) used to assess data usefulness indirectly.",
            "metric_definition": "No new numeric metric introduced; data quality judged via filtering and downstream model performance.",
            "dataset_or_benchmark": "BHP dataset created using this pipeline",
            "human_evaluation_details": "Filtering step includes human-in-the-loop quality control (implicit in description), but exact annotator counts not specified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Pipeline yielded training/validation/test splits (seen/unseen) and supported fine-tuning experiments; explicit pipeline hyperparameters and steps provided in Section 3.2.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "Using chatgpt-turbo-3.5 in generation may introduce model-specific artifacts; exact filtering criteria and automation details not fully enumerated.",
            "uuid": "e8003.7",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Tool use frameworks",
            "name_full": "ReAct and OpenAI function calling (tool-use frameworks)",
            "brief_description": "Two approaches used to augment LLMs with external information: ReAct (reasoning-action-observation loop) and OpenAI function calling for direct tool invocation (e.g., PubMed search), tested within multi-agent and single-agent setups.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (via OpenAI APIs) and other LLMs using ReAct-style prompting",
            "model_size": "API models and open-source models used",
            "scientific_domain": "biomedicine / LLM tool augmentation",
            "theory_type": "evaluation methodology augmentation",
            "evaluation_method_name": "ReAct and OpenAI function-calling experimental protocols",
            "evaluation_method_description": "ReAct: prompt LLMs to alternate thought/action/observation steps and decide whether to call tools; OpenAI function-calling: directly specify and call search functions (e.g., PubMed) to retrieve literature for hypothesis refinement.",
            "evaluation_metric": "Same set of metrics (BLEU, ROUGE, novelty/relevance/significance/verifiability) used to quantify any improvement from tool use.",
            "metric_definition": "Metrics as defined elsewhere; tool-use experiments compared metric values with and without tools.",
            "dataset_or_benchmark": "BHP unseen test set; PubMed used as external retrieval source with literature after Jan 2023 filtered out.",
            "human_evaluation_details": "Tool-use outputs included in automatic and human-rated comparisons; explicit human counts as in human evaluation section.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Tool use had minimal or mixed improvement; ReAct performed worse than OpenAI function calling; multi-agent + tool use yielded best Avg in Table 3 (multi-agent + tool Avg ~2.07 vs single-agent baseline Avg ~1.92).",
            "comparison_to_human_generated": false,
            "comparison_results": "Tool use did not consistently improve novelty and sometimes reduced it while improving verifiability; overall gains modest.",
            "limitations_noted": "LLMs struggled to identify useful information via tools; thought-action-observation loop challenging to implement effectively; filtering of search results to avoid leakage required careful control.",
            "uuid": "e8003.8",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Multi-agent framework",
            "name_full": "LLM-based Multi-Agent Collaborative Framework (Analyst, Engineer, Scientist, Critic)",
            "brief_description": "A role-based multi-agent system where distinct agent roles (Analyst extracts keywords; Engineer retrieves and organizes information; Scientist synthesizes hypotheses; Critic critiques and refines) collaborate, optionally with tool access, to generate and iteratively refine hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multi-agent instantiation using open-source LLMs and OpenAI API models (agents run on same/different LLMs)",
            "model_size": "various",
            "scientific_domain": "biomedicine / general scientific hypothesis generation",
            "theory_type": "hypothesis generation framework",
            "evaluation_method_name": "Multi-agent collaborative hypothesis generation and evaluation",
            "evaluation_method_description": "Run pipeline of role-specific agents (with/without tools) to generate hypotheses; evaluate outputs with BLEU/ROUGE and 4-metric ChatGPT/human scores and compare to single-agent baselines.",
            "evaluation_metric": "BLEU, ROUGE, novelty, relevance, significance, verifiability, and Avg score",
            "metric_definition": "Metrics defined previously; multi-agent experiment table reports these values for configurations with/without tool use.",
            "dataset_or_benchmark": "BHP unseen test set",
            "human_evaluation_details": "Top-performing multi-agent outputs included in human evaluation sampling described earlier.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Multi-agent w/o tools Avg ~2.09 and multi-agent with tools Avg ~2.07 in Table 3, outperforming single-agent baselines (baseline Avg ~1.92), indicating role decomposition improved hypothesis generation (esp. novelty and significance).",
            "comparison_to_human_generated": false,
            "comparison_results": "Multi-agent outputs showed higher novelty/significance in automatic ChatGPT evaluations and in some human-rated comparisons.",
            "limitations_noted": "Role orchestration and tool integration are non-trivial; benefits depend on introducing 'uncertainty' and diversity — some tool uses reduce novelty; more rigorous ablation and scaling required.",
            "uuid": "e8003.9",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "evaluate library",
            "name_full": "Hugging Face evaluate library",
            "brief_description": "Evaluation toolkit used to compute standard automatic metrics (BLEU and ROUGE) for comparing generated hypotheses to reference hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "used for metric computation across LLM outputs",
            "model_size": "n/a",
            "scientific_domain": "NLP evaluation for biomedical hypothesis generation",
            "theory_type": "evaluation toolkit",
            "evaluation_method_name": "evaluate.compute (BLEU/ROUGE implementations)",
            "evaluation_method_description": "Compute BLEU and ROUGE scores with the evaluate library's implementations to measure token/sequence overlap between generations and gold hypotheses.",
            "evaluation_metric": "BLEU and ROUGE-L numeric scores",
            "metric_definition": "Library returns BLEU/ROUGE scores according to standard definitions; reported in paper tables.",
            "dataset_or_benchmark": "BHP dataset",
            "human_evaluation_details": "Not applicable; evaluate library used for automated metric computation only.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Automatic BLEU/ROUGE results presented in Table 1 and Table 3 across models and prompt settings.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A",
            "limitations_noted": "As with other word-overlap metrics, outputs that are valid but lexically divergent receive low scores; library metrics correlated poorly with human judgments in this task.",
            "uuid": "e8003.10",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks",
            "rating": 2
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "rating": 2
        },
        {
            "paper_title": "Self-Instruct: Aligning language model with self generated instructions",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
            "rating": 1
        }
    ],
    "cost": 0.019687999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Zero Shot Hypothesis Proposers</h1>
<p>Biqing Qi ${ }^{1,2,3 <em>}$<br>qibiqing7@gmail.com<br>Kaiyan Zhang ${ }^{1 </em>}$<br>zhang-ky22@mails.tsinghua.edu.cn<br>Haoxiang Li ${ }^{1}$<br>kai Tian ${ }^{1}$<br>Sihang Zeng ${ }^{4}$<br>hx-li20@mails.tsinghua.edu.cn tk23@mails.tsinghua.edu.cn zengsh@uw.edu<br>Zhang-Ren Chen ${ }^{5}$ Jin-Fang Hu ${ }^{5}$！ Bowen Zhou ${ }^{1,2}$！<br>chenzhangren@ncu.edu.cn hujinfang333@126.com zhoubowen@tsinghua.edu.cn<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Frontis.AI ${ }^{3}$ Harbin Institute of Technology<br>${ }^{4}$ University of Washington ${ }^{5}$ The First Affiliated Hospital of Nanchang University</p>
<h4>Abstract</h4>
<p>Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
<h2>1 Introduction</h2>
<p>"When nothing is sure, everything is possible." - Margaret Drabble</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The pursuit of knowledge discovery stands as a cornerstone of human progress, driving innovation, and shaping our understanding of the world [30, 28]. However, in recent times, the process of knowledge discovery has encountered formidable challenges, characterized by serendipity and sluggishness. As the volume of data and literature continues to expand at an unprecedented rate, the ability to distill high-value insights and gain profound understanding from this wealth of information has become increasingly daunting[28]. Silos of information have erected themselves between disciplines, impeding the crucial cross-pollination of ideas and insights that could propel discovery to new heights. Yet, amidst these challenges, there is a glimmer of hope. The advent of large-scale models (LLMs), possessing the capacity to harness a vast reservoir of world knowledge and span multiple domains, holds promise in revolutionizing the landscape of knowledge discovery. These models present an opportunity to break down the barriers between disciplines, enabling researchers to traverse the expansive sea of information with ease and efficiency. Central to the process of knowledge discovery lies the formulation of sound hypotheses [42, 31, 1, 38]. However, a glaring gap persists in the arsenal of tools available to formally explore and evaluate hypotheses. While literature is replete with discussions on validation, it often overlooks the critical aspect of generating novel hypotheses.</p>
<p>In light of these challenges and opportunities, this paper delves into the current state of knowledge discovery, examining the hurdles posed by information explosion and disciplinary isolation. It explores the potential transformative role of LLMs in bridging these gaps, ultimately emphasizing the pivotal role of hypothesis generation in the knowledge discovery process. Furthermore, it highlights the pressing need for tools and methodologies to facilitate hypothesis generation, thus propelling knowledge discovery into a new era of efficiency and innovation [13]. Currently, both ChatGPT and GPT-4 undergo extensive pre-training on vast datasets and possess the capability of continuous updates. However, ensuring strict traceability of data sources becomes a challenging task, limiting our ability to explore zero-shot hypothesis generation.</p>
<p>The past literatures have explored scenarios of problem discovery, yet rigorous experimental designs to investigate whether LLMs can effectively propose genuine problems under zero-shot conditions remain lacking. To tackle this issue, we assemble a dataset of biomedicine literature spanning from January 2000 to September 2023. This dataset is partitioned into training and testing sets, with the training set exclusively containing literature published before January 2023. We construct an unseen test set using literature from August 2023 and ensure that the evaluated LLMs have been trained on corpora before that date. Additionally, we devise a multi-intelligent collaborative framework that incorporates search tools and role-playing to delve deeper into and uncover the potential for hypothesis generation.</p>
<p>Through experiments and analyses as shown in Figure 1, we draw the following findings: 1) LLMs surprisingly generate hypotheses that are untrained yet validated when tested against literature. 2) Increasing uncertainty levels can benefit by diversifying candidate generation and potentially enhancing zero-shot hypothesis generation capabilities. For instance, introducing heightened uncertainty through collaborative multi-agent approaches significantly improves the model’s ability to generalize in zero-shot scenarios. However, integrating subsequent few-shot enhancements and using additional tools may reduce the model’s proficiency in generating hypotheses. This phenomenon is likely due to the reduction of uncertainty, limiting the model’s space for hypothesis generation. Consequently, it lacks consistent positive effects, underscoring the need for careful consideration of the type of external knowledge employed. The above findings also support the notion: "When nothing is sure, everything is possible." Specifically, our contributions are as follows:</p>
<p>1) To rigorously validate the zero-shot and few-shot hypothesis generation potential of LLMs, we construct temporal biomedical instruction data and devised novel and effective experiments for</p>
<p>comprehensive analysis and evaluation. To the best of our knowledge, this is the first work that formally designs experiments to investigate the zero shot hypothesis generation capacity of LLMs.
2) Through validation across different models and various scenario dimensions, we surprisingly find that LLMs possess rudimentary higher-order knowledge reasoning capabilities and can propose new hypothesis statements. This provides new empirical insights and pathways for knowledge discovery.
3) For a comprehensive review of the generated hypotheses, we design metrics across four dimensions for both ChatGPT-based and human evaluations. The correlation scores between ChatGPT evaluations and manual results indicate that LLMs also play a significant role in hypothesis evaluations.
4) To efficiently explore and further harness the capability of hypothesis generation, we introduce a multi-agent system based on LLMs. Through efficient collaboration among multiple models and tool utilization, we analyze the factors influencing hypothesis generation by LLMs.</p>
<h1>2 Process of Scientific Discovery</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The iterative experimental loop of scientific discovery: observations and data accumulated from past experiments are analyzed and used to generate new hypotheses, and in turn new experiments that will yield new data to continue to cycle. In this paper, we mainly focus on investigating whether LLMs have the zero shot generalization ability to generate new hypotheses.</p>
<p>Scientific discovery involves key components, each crucial for advancing our understanding of the natural world: data analysis, hypothesis formulation, experiment design, execution, and observation and reflection [13] as shown in Figure 2.</p>
<p>1) Data Analysis: Foundational in the scientific process, it entails collecting and examining data to discern patterns and anomalies, extracting insights through statistical techniques and visualization. It initiates scientific inquiry, guiding further exploration. 2) Generating Hypotheses: Among these components, hypothesis formulation is pivotal. It entails crafting informed guesses to explain observed phenomena. Hypotheses serve as guiding frameworks, directing and focusing research by articulating specific relationships and outcomes for experimental exploration. 3) Experiment Design: Once a hypothesis is set, designing experiments becomes essential to rigorously test its validity. This involves defining variables, specifying control groups, and outlining methods and procedures. Well-designed experiments ensure objective hypothesis testing and yield meaningful, informative results. 4) Experiment Execution: Meticulous execution of designed experiments and data collection are critical. Researchers adhere precisely to experimental protocols, recording observations, measurements, and unexpected findings. Integrity in execution ensures reliable, reproducible outcomes. 5) Accumulating Observations: After experiments, scientists engage in observation and reflection. They analyze collected data to determine if results support or refute the initial hypothesis. If unsupported, hypotheses may be revised or new ones formulated based on findings. Observation and reflection permit iterative refinement of scientific understanding.
Hypothesis Pioneers Pathways: Guiding Knowledge Discovery. While all components are essential, hypothesis formulation holds a unique position. It drives the scientific endeavor, guiding research question selection, experiment design, and data analysis. Well-constructed hypotheses not only provide direction but also lay the foundation for meaningful scientific discoveries by posing rigorously testable questions. Hypothesis formulation serves as the intellectual anchor steering scientific investigation and ultimately advancing knowledge.</p>
<p>3 Can LLMs Truly Generate Zero-Shot Hypotheses?</p>
<p>In this section, we outline the methodology employed for a thorough assessment of LLMs’ capacity to generate hypotheses under zero-shot conditions. To accomplish this, we begin by defining the problem of hypothesis generation in zero-shot settings. Next, we elucidate the process of dataset construction within the biomedical domain. Finally, we undertake comprehensive experiments to evaluate various instructed models across multiple dimensions, aiming to explore the factors influencing the ability of LLMs to propose improved hypotheses.</p>
<h3>3.1 Problem Definition</h3>
<p>Following the scientific discovery process outlined in Section 2, hypothesis generation typically occurs after thorough literature analysis and examination of specific phenomena. To enhance evaluation effectiveness, we formalize this process as a text completion task. Given dataset $D$, an instruction $I$, and text pairs $(X_{i},Y_{i})<em i="i">{i=1}^{n}$ containing background knowledge and corresponding hypotheses, extracted from medical papers, our objective is to assess model $M$ by having it generate hypotheses based on the task instruction and background knowledge, i.e., $M(I,X</em>$. The objective function is formulated as:})=Y_{i}$, for each $i$ $\in{1,..,,n</p>
<p>$y^{*}=\underset{y_{1},...,y_{n}}{\arg\max}\prod_{t=1}^{n}{P(y_{t}|y_{1},...,y_{t-1},I,X)}.$</p>
<h3>3.2 Dataset Construction</h3>
<p>In this section, we detail the process of constructing datasets and ensuring the robustness of our evaluation. Prevalent LLMs, like Llama and ChatGPT, face challenges in tracing the origin of their knowledge due to continuous self-updating. To address this, we propose a novel approach to assess LLMs’ hypothesis generation. Recognizing their potential impact on public domain data, we reconstruct a new biomedical literature dataset based on publication dates.</p>
<p>As depicted in Figure 3, we designated the year 2023 as the cut-off point. Our training dataset comprises literature published before January 2023, while the test dataset comprises literature published after January 2023, forming pairs of data with background knowledge and hypothesis proposals. Due to the emergence of more advanced LLMs, our evaluations focus exclusively on the unseen test set, featuring literature published in August 2023. We selected instructed models fine-tuned before August 2023 for both evaluation and fine-tuning testing. In our experimental setup, we implemented stringent measures to ensure the models had no prior exposure to the test data, affirming the validity of our experiments. We strictly follow the standard pipeline as outlined in Self-Instruct [32] for our data generation process, encompassing four key steps: 1) Compose the paper set based on the topic and content of the literature. 2) Utilize chatgpt-turbo-3.5 to summarize the literature knowledge. 3) Generate background knowledge-assume pairs. 4) Filter low-quality data. 5) Split the dataset according to publication time.</p>
<h3>3.3 Datast Analysis</h3>
<p>In this section, we provide a comprehensive overview of the constructed dataset, encompassing details about the data acquisition strategy, dataset size, visibility control measures, distribution by year and month, as well as topic distribution.</p>
<p>We have created two datasets to maintain control over the visibility of hypotheses: 1) Seen dataset This dataset comprises 2700 background and hypothesis pairs sourced from literature published before January 2023. This dataset was partitioned into training (2500) and validation (200) subsets (as well as seen test set). It is consistent with the corpus that the LLMs have been exposed to. 2) Unseen dataset The unseen dataset consists of 200 pairs extracted from papers published in August 2023, which the LLMs have not encountered during training and are used for testing purposes.</p>
<p>We also provide publication date and topic distribution of constructed dataset in Appendix B.1.</p>
<h1>3.4 Experiment Setup</h1>
<p>In this section, we introduce experimental settings for hypothesis generation and evaluation.
Models For a fair comparison, we exclusively evaluate LLMs trained on corpora before March 2023 to ensure the test set remains unseen. We consider three categories of models in total: 1) API-based LLMs: this is mainly ChatGPT. 2) General domain instructed LLMs: These models consist of open-source models that have undergone fine-tuning based on Llama using general domain instructions. We primarily choose the top-tier models based on their performance rankings on the Alpaca Eval Leaderboard ${ }^{3}$. 3) Specific domain instructed LLMs: These include PMC-LLaMA [35], and MedAlpaca [10]. These models are trained on a variety of sources in medicine domain, such as medical books, PMC papers, medical dialogs, and others. We provide detailed meta-information for various models, including their training data sources and publication dates, in Appendix B.2.
Prompts To ensure a consistent output format across different models, we create prompts in two formats: zero-shot and few-shot examples. In our experiments, we adopt a 5-shot format, selecting examples from the training set before January 2023 using both randomly sampled and similarity retrieval methods. We provide illustrations of zero-shot and few-shot prompts in Appendix E.
Finetuning To assess the hypothesis generation capability beyond zero-shot scenarios, we identify the top-performing open-source models through few-shot evaluation. We proceed to fine-tune the full parameters of WizardLM-13B-V1.2 with the background and hypothesis pairs. The fine-tuning process consists of three epochs, employing a batch size of 8 , a maximum sequence length of 2048 tokens, and a learning rate set at $3 \mathrm{e}-5$. We implement early stopping and select the best checkpoints based on their performance on the seen test dataset.
Evaluation Given the disparities between the hypothesis generation task and traditional text generation tasks liking machine translation and summarization, with the former being more challenging and often involving uncertainty that extends beyond established ground truth, we approach our evaluation from two primary perspectives: conducting evaluations with and without golden hypotheses. In evaluations with golden hypotheses, we employ standard text generation metrics, including BLEU and ROUGE in evaluate library ${ }^{4}$, to assess word overlap between the generated outputs and the ground truth. The vastness of the hypothesis space renders it difficult to comprehensively assess the quality of generated hypotheses using word overlap metrics alone. To provide a more comprehensive evaluation of the generated hypotheses from multiple facets, we have thoughtfully devised four metrics: novelty, relevance, significance, and verifiability. Inspired by recent research that highlights ChatGPT as proficient annotators [8, 16], demonstrating a strong correlation with human ratings, we employ ChatGPT for further evaluation. In detail, we request ChatGPT to evaluate both the generated scientific hypotheses and the provided background across these aspects. The scoring scale ranges from 0 to 3 , where a higher score indicates superior results. Additionally, we solicit ChatGPT to furnish a step-by-step explanation to substantiate the assigned score. Moreover, we conduct human evaluation based on the four metrics for the top-tier models identified in the automatic evaluation in Section 3.5, and we provide a detailed description of this process in Section 3.6.</p>
<h3>3.5 Experiment Results</h3>
<p>This section presents the results of hypothesis generation across various models in both zero-shot and few-shot settings. We primarily analyze the results from two perspectives: the impact of the zero-shot setting and the influence of introducing external knowledge on hypothesis generation.</p>
<h3>3.5.1 Impact of zero-shot settings</h3>
<p>The results presented in Table 1 demonstrate the significant impact of zero-shot settings in improving hypothesis generation, particularly in terms of fostering high novelty. We analyze these results from two key perspectives as following.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results of various LLMs: We assess instructed models using zero-shot and few-shot format prompts to generate constrained outputs. To provide a comprehensive assessment, we calculate the average scores for novelty, relevance, significance, and verifiability, denoted as Avg. Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs. To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>Seen</th>
<th></th>
<th></th>
<th></th>
<th>Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>BLEU</td>
<td>ROUGE</td>
<td>BLEU</td>
<td>ROUGE</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbot(8-shot)</td>
<td>13.93</td>
<td>25.32</td>
<td>15.52</td>
<td>26.48</td>
<td>1.42</td>
<td>2.63</td>
<td>1.58</td>
<td>1.97</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)</td>
<td>16.47</td>
<td>27.07</td>
<td>16.49</td>
<td>26.96</td>
<td>1.22</td>
<td>2.57</td>
<td>1.84</td>
<td>2.03</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)*</td>
<td>17.33</td>
<td>27.28</td>
<td>17.71</td>
<td>27.53</td>
<td>1.02</td>
<td>2.61</td>
<td>1.85</td>
<td>2.36</td>
</tr>
<tr>
<td>General</td>
<td>Vicuna-33b-v1.3(0-shot)</td>
<td>13.97</td>
<td>24.43</td>
<td>13.66</td>
<td>23.43</td>
<td>1.67</td>
<td>2.55</td>
<td>2.04</td>
<td>1.84</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)</td>
<td>11.23</td>
<td>22.54</td>
<td>11.49</td>
<td>22.68</td>
<td>1.60</td>
<td>2.40</td>
<td>1.67</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)*</td>
<td>12.78</td>
<td>24.11</td>
<td>13.12</td>
<td>23.66</td>
<td>1.19</td>
<td>2.71</td>
<td>2.00</td>
<td>2.17</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>10.95</td>
<td>21.56</td>
<td>11.44</td>
<td>22.04</td>
<td>1.86</td>
<td>2.41</td>
<td>1.91</td>
<td>1.98</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>8.17</td>
<td>21.09</td>
<td>7.63</td>
<td>20.70</td>
<td>1.95</td>
<td>2.38</td>
<td>2.06</td>
<td>2.22</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>8.40</td>
<td>21.65</td>
<td>9.66</td>
<td>22.43</td>
<td>1.43</td>
<td>2.50</td>
<td>1.94</td>
<td>2.15</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(0-shot)</td>
<td>11.91</td>
<td>23.35</td>
<td>12.03</td>
<td>23.55</td>
<td>1.62</td>
<td>2.55</td>
<td>1.90</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)</td>
<td>14.00</td>
<td>24.30</td>
<td>13.82</td>
<td>24.38</td>
<td>1.33</td>
<td>2.54</td>
<td>1.81</td>
<td>2.23</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)*</td>
<td>14.96</td>
<td>25.66</td>
<td>15.26</td>
<td>25.78</td>
<td>1.06</td>
<td>2.64</td>
<td>1.73</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>13.45</td>
<td>24.12</td>
<td>14.25</td>
<td>25.05</td>
<td>1.57</td>
<td>2.45</td>
<td>1.74</td>
<td>1.89</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>14.04</td>
<td>24.59</td>
<td>13.78</td>
<td>24.28</td>
<td>1.17</td>
<td>2.61</td>
<td>2.12</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)*</td>
<td>14.46</td>
<td>24.78</td>
<td>15.26</td>
<td>25.56</td>
<td>0.97</td>
<td>2.67</td>
<td>1.85</td>
<td>1.99</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(0-shot)</td>
<td>8.79</td>
<td>22.71</td>
<td>8.38</td>
<td>21.48</td>
<td>1.58</td>
<td>2.51</td>
<td>1.70</td>
<td>2.05</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)</td>
<td>12.46</td>
<td>23.60</td>
<td>12.58</td>
<td>24.21</td>
<td>1.06</td>
<td>2.64</td>
<td>2.09</td>
<td>2.20</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)*</td>
<td>12.37</td>
<td>23.93</td>
<td>12.88</td>
<td>24.78</td>
<td>1.16</td>
<td>2.76</td>
<td>2.10</td>
<td>2.23</td>
</tr>
<tr>
<td>Medicine</td>
<td>MedAlpaca-13B(0-shot)</td>
<td>6.10</td>
<td>22.07</td>
<td>5.82</td>
<td>20.49</td>
<td>0.55</td>
<td>1.17</td>
<td>1.17</td>
<td>1.06</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)</td>
<td>0.99</td>
<td>3.84</td>
<td>1.08</td>
<td>3.84</td>
<td>0.98</td>
<td>1.32</td>
<td>1.32</td>
<td>1.49</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)*</td>
<td>4.60</td>
<td>9.36</td>
<td>4.50</td>
<td>9.07</td>
<td>1.09</td>
<td>1.40</td>
<td>1.20</td>
<td>1.53</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>22.89</td>
<td>40.36</td>
<td>22.37</td>
<td>40.45</td>
<td>0.76</td>
<td>1.94</td>
<td>1.42</td>
<td>1.52</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)</td>
<td>1.36</td>
<td>4.83</td>
<td>1.41</td>
<td>4.78</td>
<td>1.13</td>
<td>1.45</td>
<td>1.26</td>
<td>0.88</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>6.21</td>
<td>12.39</td>
<td>6.16</td>
<td>12.13</td>
<td>1.73</td>
<td>2.17</td>
<td>1.88</td>
<td>2.09</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13B-V1.2</td>
<td>19.13</td>
<td>27.35</td>
<td>19.73</td>
<td>27.58</td>
<td>0.97</td>
<td>2.55</td>
<td>1.38</td>
<td>2.26</td>
</tr>
</tbody>
</table>
<p>Zero-shot Outperforms Few-shot. Our findings indicate that, for extra large models like Llama-2-70b-chat and WizardLM-70B-V1.0, zero-shot performance surpasses that of the fewshot setting, where few-shot examples are obtained by randomly sampling. This suggests that the capacity of hypothesis generation is limited by the inclusion of few-shot examples, and models exhibit stronger abilities in a zero-shot setting.
Outperforming the Unseen Compared to the Seen Test Set. Despite the visibility of literature published before 2022 in the pre-training corpus of most LLMs, we have categorized the test set into "seen" and "unseen." Typically, LLMs may excel in the "seen" test set due to the potential memorization of hypotheses present in the pre-training corpus, resulting in higher performance compared to the "unseen" test set. However, our results indicate that LLMs tend to perform better on the "unseen" test set. We speculate that this is because the complexity of hypothesis generation may hinder LLMs from effectively leveraging the dark knowledge in their parameters.</p>
<h1>3.5.2 Influence of external knowledge</h1>
<p>Based on the results, we observe that the introduction of external knowledge, such as few-shot examples, domain adaptation, and instruction fine-tuning, does not consistently enhance the ability of hypothesis proposing.
Few-Shot Examples Enhance Verifiability but Decrease Novelty. In comparison to zero-shot settings, models using few-shot prompts benefit from the provided examples, resulting in very high matching rates. Regarding word overlap metrics, including BLEU and ROUGE, most models, especially WizardLM series models, and Openchat-v3.2-super, show improved performance when provided with in-context examples, with retrieved examples being particularly beneficial for their generations. However, it's important to note that these few-shot prompts significantly increase verifiability while simultaneously leading to lower levels of novelty compared to zero-shot results.
Randomly Sampled Few-Shot Examples vs. Similarity Retrieval. Given that randomly sampled in-context examples often differ from the provided background in terms of topics or domains, this can potentially confuse LLMs and lead to decreased performance. In our pursuit of further exploration into the hypothesis generation capabilities of LLMs, we retrieve examples from the training dataset based on their similarity to the given background. The results indicate that similarity retrieval can further enhance performance.</p>
<p>Instruction Tuning Enhances LLM Performance. Following fine-tuning on a dataset comprising background and hypothesis pairs, WizardLM-13B-V1.2 attains superior performance, surpassing even gpt-3.5-turbo and WizardLM-70B-V1.0. This finding underscores that domain adaptation remains a valuable approach to enhance the hypothesis generation capabilities of LLMs. It not only leads to greater resource efficiency but also supports the promotion of privacy in a localized context.
Impact of Domain Adaptation on Hypothesis Generation. We have also conducted an analysis of the influence of fine-tuning for domain adaptation on hypothesis generation. In this comparison, we utilize instructed models adapted to the field of medicine. The results obtained from MedAplaca and PMC-LLaMA indicate that domain adaptation can significantly improve word overlap performance. However, the metrics derived from ChatGPT suggest that domain adaptation has only a minimal effect on hypothesis generation. This discrepancy between word overlap metrics and ChatGPT's evaluation highlights the need for more comprehensive and unified metrics in the context of hypothesis generation tasks.</p>
<h1>3.6 Human Evaluation and Case Study</h1>
<p>In this section, we conduct a human evaluation to assess the generated hypotheses and calculate coherence scores to compare them with ChatGPT evaluation scores, guiding further evaluation efforts.</p>
<h3>3.6.1 Evaluation Settings</h3>
<p>Evaluation Metrics To comprehensively evaluate the generations manually and simultaneously assess the quality of ChatGPT's evaluations, we continue to utilize the four metrics outlined in Section 3.4, which encompass novelty, relevance, significance, and verifiability. The range of each metric remains from 0 to 3 , with higher values indicating better performance. Additionally, we calculate coherence scores between human evaluations and ChatGPT evaluations.
Selection of Models Given the constraints associated with the cost of human evaluation, our primary objective is to assess whether LLMs can produce valuable hypotheses, rather than striving for state-of-the-art performance. As a result, we exclusively perform human evaluation on the outputs generated by the LLM that ranks highest in performance based on automatic metrics and ChatGPT evaluation. Furthermore, we aim to encompass a variety of prompts and models in our evaluation. The final models selected for human evaluation are detailed in Table 2.
Evaluation Details We randomly selected 100 examples from the unseen test set and had three evaluators with a biomedical background assign scores for each metric to each model.</p>
<h3>3.6.2 Evaluation Results</h3>
<p>As depicted in Table 2, the human evaluations exhibit a strong correlation with ChatGPT's evaluations, with Pearson and Spearman coefficients exceeding 0.7 for all models. These results strongly support our earlier findings regarding the influence of zero-shot learning and external knowledge. This reinforces our assertion that large language models can effectively propose hypotheses and significantly contribute to scientific discovery. For additional insights, we present correlation coefficients between word overlap scores and manual scores in the Appendix C, revealing lower coherence and highlighting the need for more advanced evaluation metrics.
We also conduct a case study that showcases the hypotheses generated by various models and includes examples of step-by-step evaluations by ChatGPT. Details can be found in Appendix D.</p>
<h2>4 Can agent collaboration enhance LLMs' zero-shot generalization?</h2>
<p>In this section, we will strive to enhance the ability of LLMs in hypothesis generation through multi-agent collaboration and the use of tools. Our objective is to improve hypothesis efficiency by employing multi-agent collaboration, simulating real-world research scenarios. To begin, we introduce the conceptual system of multi-agent collaboration for hypothesis generation, drawing inspiration from scientific research. Subsequently, we present the role design and the tools use in this context. Finally, we present preliminary validated results of the multi-agent system using our proposed BHP dataset.</p>
<p>Table 2: This table presents the results of human evaluation. The Avg Coefficient are used to assess the correlation between the average scores obtained from ChatGPT and those from human evaluation.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>ChatGPT</th>
<th>Human Eval</th>
<th></th>
<th></th>
<th></th>
<th>Avg Coefficient</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Eval.Avg</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
<td>Avg</td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbo(0-shot)</td>
<td>1.90</td>
<td>1.54</td>
<td>2.69</td>
<td>1.77</td>
<td>2.08</td>
<td>2.02</td>
<td>0.87</td>
<td>0.78</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbo(5-shot)*</td>
<td>1.96</td>
<td>1.31</td>
<td>2.62</td>
<td>2.08</td>
<td>2.62</td>
<td>2.15</td>
<td>0.80</td>
<td>0.78</td>
</tr>
<tr>
<td>General</td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>2.04</td>
<td>1.77</td>
<td>2.23</td>
<td>1.92</td>
<td>1.92</td>
<td>1.96</td>
<td>0.89</td>
<td>0.84</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>2.20</td>
<td>2.15</td>
<td>2.77</td>
<td>2.08</td>
<td>2.31</td>
<td>2.33</td>
<td>0.96</td>
<td>0.90</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>2.01</td>
<td>1.38</td>
<td>2.62</td>
<td>2.31</td>
<td>2.00</td>
<td>2.08</td>
<td>0.97</td>
<td>0.94</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>1.91</td>
<td>1.38</td>
<td>2.31</td>
<td>1.54</td>
<td>2.00</td>
<td>1.81</td>
<td>0.90</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>2.01</td>
<td>1.15</td>
<td>2.69</td>
<td>2.46</td>
<td>1.77</td>
<td>2.02</td>
<td>0.85</td>
<td>0.89</td>
</tr>
<tr>
<td>Medicine</td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>1.41</td>
<td>1.00</td>
<td>2.62</td>
<td>1.92</td>
<td>2.00</td>
<td>1.88</td>
<td>0.73</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>1.97</td>
<td>1.85</td>
<td>2.23</td>
<td>1.92</td>
<td>1.69</td>
<td>1.92</td>
<td>0.95</td>
<td>0.94</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13BV1.2</td>
<td>1.79</td>
<td>0.85</td>
<td>2.77</td>
<td>1.23</td>
<td>2.23</td>
<td>1.77</td>
<td>0.83</td>
<td>0.85</td>
</tr>
</tbody>
</table>
<h1>4.1 Multi-agent Framework</h1>
<p>Inspired by the structured methodology detailed in Section 2, we introduce a comprehensive framework tailored for hypothesis formulation. This framework encapsulates a multi-agent system where each agent assumes a distinct role, mirroring the collaborative nature of scientific endeavors. Through a symbiotic and iterative process, these agents collaborate to craft hypotheses that are not only grounded in existing knowledge but also pave the way for novel insights. By emulating the essence of scientific discovery, our framework strives to produce hypotheses that are both innovative and scientifically robust. As depicted in Figure 4, we have partitioned the framework into five components, encompassing four automated agents and the option for human involvement within the loop.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: The conceptual system of multi-agent collaboration for hypothesis generation. The overall prototyping process is illustrated below, allowing users to choose optional involvement. We offer core role descriptions of multi-agents and the fully automated system above.</p>
<p>Role Design In our proposed multi-agent framework, each component plays a distinct and pivotal role. The Analyst serves as the foundation, meticulously extracting and defining core elements from the research background. Its primary objective is to interpret the literature, distilling it into keywords or topics that subsequently guide the Engineer's search efforts. The Engineer, leveraging these keywords, embarks on a mission to retrieve and organize pertinent information. They meticulously plan and execute detailed searches, ensuring that the findings are compiled in a structured manner. This organized materials then lands in the domain of the Scientist, whose objective is to weave together the Engineer's findings with the original research background. Through careful interpretation, the Scientist crafts a hypothesis that is both grounded in existing knowledge and offers a fresh perspective. However, before this hypothesis is finalized, it undergoes scrutiny by the Critic. The Critic's role</p>
<p>is paramount in ensuring the hypothesis's robustness, coherence, and novelty. They evaluate the hypothesis against the backdrop of the research background, ensuring it stands up to academic rigor. Feedback from the Critic, if necessary, loops back to refine the hypothesis or prompts the Analyst for further insights, creating a cyclical and iterative process of refinement.</p>
<p>Tool Use To explore external knowledge beyond the inherent dark knowledge within LLMs, we integrate the Engineer agent with search engines, mainly PubMed ${ }^{5}$. Similarly, to control the visibility of the unseen test dataset, we filter and exclude literature published after January 2023 from the search results. We carry out tool use experiments using ReAct [40] and OpenAI function calling. ReAct is a method that extends the concept of Chain of Thought (CoT) [34], involving thinking before taking action and subsequently making observations based on feedback from the environment. In our experiments, we instruct the LLMs to initially contemplate the provided background information and then make a decision regarding whether to utilize tools. Upon receiving feedback from the tools, the LLMs are expected to identify supporting evidence in the results or potentially make further tool requests. The LLMs are responsible for concluding the hypothesis generation process and summarizing the hypotheses independently. In the case of OpenAI function calling, we directly specify tools for publication searching and transmit them to OpenAI APIs. This process is roughly implemented through fine-tuning, as described in ToolFormer [24].</p>
<h1>4.2 Experiment Results</h1>
<p>Our primary focus is to investigate the impact of tool use and multi-agent collaboration on hypothesis generation. We present the experimental results in Table 3. Based on the results, we summarize our findings from two perspectives: tool use and role-playing.</p>
<p>Influence of Tool Use Based on our results, we observe that tool use has minimal impact on improving the hypothesis generation ability of LLMs. This observation aligns with the findings presented in the previous sections regarding the analysis of external knowledge. Notably, the ReAct-based method performs worse than OpenAI function calling. It is also evident that LLMs struggle to identify useful information and exhibit weaknesses in the thought-action-observation process, even when utilizing the official interface from OpenAI. Hypothesis generation is indeed a challenging task that necessitates iterative discussions and the exchange of ideas among various individuals.</p>
<p>Multi-Agent Collaboration In addition to tool use, our findings suggest that the division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty. This mirrors the dynamics of real-world scientific research, where hypotheses are formulated through iterative discussions and refutations. Additionally, it is worth noting that tool use can further enhance the performance of the multi-agent framework.</p>
<p>Table 3: Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset. The results demonstrate that both multi-agent systems and the utilization of tools enhance the ability of LLMs in hypothesis generation. Among the various types of models, both $2^{a}$ and $2^{b}$ are evaluated with tool use. The difference between them lies in their implementations: ReAct [40] and OpenAI function calling ${ }^{6}$, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Influence Factor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-agent</td>
<td style="text-align: center;">Tool use</td>
<td style="text-align: center;">BLUE</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Significance</td>
<td style="text-align: center;">Verifiability</td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">26.48</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.92</td>
</tr>
<tr>
<td style="text-align: center;">$2^{a}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.94</td>
<td style="text-align: center;">24.16</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;">$2^{b}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">15.87</td>
<td style="text-align: center;">24.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.71</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">2.09</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">22.04</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.07</td>
</tr>
</tbody>
</table>
<h2>5 Conclusion</h2>
<p>From the hypothesis-proposer perspective, we investigated LLMs' zero-shot generalisation ability in scientific research. Specifically, we first build a comprehensive corpus based on biomedical literature,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>split by publication date, including background knowledge and hypothesis pairs. This corpus is then used as a basis for fine-tuning LLMs, leading to the generation of the LLM. To further analysis and enhance the capabilities of the hypothesis proposer, we introduce a LLM-based multi-agent collaboration system. Experimental results show that fine-tuned LLMs of various sizes can propose new hypotheses that did not appear in the training data but can be confirmed by the test literature, with performance comparable to ChatGPT and in some cases even better. Notably, our study revealed that introducing uncertainty into processes and operations enhances zero-shot generalization capabilities. These findings confirm the potential of LLMs to propose new hypotheses and offers hope for future unlocked scientific discovery. In future work, we will focus on optimizing models and generating hypotheses guided by effective uncertainty assessment metrics.</p>
<h1>Acknowledgements</h1>
<p>We extend our gratitude to the anonymous reviewers for their insightful feedback.</p>
<h2>References</h2>
<p>[1] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[2] Andres M. Bran, Sam Cox, Andrew D. White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools, June 2023. arXiv:2304.05376.
[3] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. The Life Cycle of Knowledge in Big Language Models: A Survey, March 2023. arXiv:2303.07616 [cs].
[4] Zhuo Chang, Jing Zhang, Yilun Liu, Huajian Gao, and Guang-Kui Xu. New Mechanical Markers for Tracking the Progression of Myocardial Infarction. Nano Letters, 23(16):7350-7357, August 2023.
[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking Before Action: Gather Information in Embodied Decision Making with Language Models, May 2023. arXiv:2305.15695 [cs].
[6] Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Empowering Private Tutoring by Chaining Large Language Models, September 2023. arXiv:2309.08112 [cs] version: 1.
[7] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback, May 2023. arXiv:2305.10142 [cs].
[8] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, July 2023. arXiv:2303.15056 [cs].
[9] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. MindAgent: Emergent Gaming Interaction, September 2023. arXiv:2309.09971 [cs] version: 1.
[10] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.
[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework, August 2023. arXiv:2308.00352 [cs].
[12] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory, June 2023. arXiv:2306.03901 [cs].</p>
<p>[13] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2(3):557-577, 2023.
[14] Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think Before You Act: Decision Transformers with Internal Working Memory, May 2023. 0 citations (Semantic Scholar/arXiv) [2023-05-30] arXiv:2305.16338 [cs].
[15] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society, March 2023. arXiv:2303.17760 [cs].
[16] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs].
[17] Philipp Maas, Frank Carey, Chris Wheeler, Edward Saatchi, Pete Billington, and Jessica Yaffa Shamash. SHOW-1 and Showrunner Agents in Multi-Agent Simulations. arXiv preprint, 2023.
[18] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, June 2022. arXiv:2112.09332.
[19] Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: Tool Augmented Language Models, May 2022. arXiv:2205.12255 [cs].
[20] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, August 2023. arXiv:2304.03442 [cs].
[21] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with Massive APIs, May 2023. arXiv:2305.15334 [cs].
[22] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative Agents for Software Development, August 2023. arXiv:2307.07924 [cs].
[23] Vipula Rawte, Amit Sheth, and Amitava Das. A Survey of Hallucination in Large Foundation Models, September 2023. arXiv:2309.05922 [cs].
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools, February 2023. arXiv:2302.04761.
[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, May 2023. arXiv:2303.17580 [cs].
[26] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. arXiv:2303.11366 [cs].
[27] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs, June 2023. arXiv:2306.06624 [cs].
[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[29] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the Planning Abilities of Large Language Models - A Critical Investigation, May 2023. arXiv:2305.15771 [cs].</p>
<p>[30] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.
[31] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259, 2023.
[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023. arXiv:2201.11903 [cs].
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
[35] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmcllama: Towards building open-source language models for medicine, 2023.
[36] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The Rise and Potential of Large Language Model Based Agents: A Survey, September 2023. arXiv:2309.07864 [cs].
[37] Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, June 2023. arXiv:2306.02224.
[38] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023.
[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, May 2023. arXiv:2305.10601 [cs].
[40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, March 2023. arXiv:2210.03629.
[41] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models, September 2023. arXiv:2309.01219 [cs].
[42] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.
[43] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, June 2023. arXiv:2305.17144 [cs].
[44] Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.</p>
<h1>A Related Works</h1>
<h2>A. 1 Data-Driven Scientific Discovery</h2>
<p>Data-driven knowledge discovery research within LLM is relatively limited, with the current focus primarily on dataset construction and task-driven design. In this context, [42] proposed a dataset for investigating the transition from goals to discoveries. However, it should be noted that accurate discoveries within this dataset are not recent. [31] introduced a method for automatically collecting and constructing publication data, along with a proposal for a hypothesis generation approach in the natural language processing (NLP) domain. However, this method requires prior human knowledge, explicit context, and is not an automated process. It's worth noting that their data was constructed from literature before 2021 from the ACL collection, implying that the information may already exist in open-source models like chatGPT and LLAMA. Furthermore, [31] focused on integrating computational tools in the field of chemistry, primarily analyzing the capabilities of LLMs in using integrated tools but neglecting the ability for zero-shot generalization in chemistry reactions. [1] delved more into the abilities of LLMs regarding planning and conducting experiments but did not consider proposing new hypotheses. [38] introduced a new task for open-domain hypothesis induction and created a dataset comprising 50 articles from social science journals. Additionally, they developed a multi-module system for exploring feedback mechanisms. However, all of the above-mentioned literature lacks strict guarantees on the visibility of test data to models, thereby limiting our exploration of the zero-shot generalization capability of LLMs through learning from existing knowledge to propose new hypothesis. Unlike existing works, we have designed datasets based on publication dates, which can easily ensure a strict independence between test data and LLMs.</p>
<h2>A. 2 LLM-driven Autonomous Agents</h2>
<p>Large language models demonstrate exceptional capabilities in tasks such as question answering, program coding, and instruction following. However, they still confront significant challenges related to factual hallucination [41, 23], knowledge outdated [3], and interactions with real-world. To address these challenges, recent research has explored enhancing LLMs by incorporating tools such as search engines [18, 19], calculators [24], code interpreter [44], RESTful APIs [27, 21] and others. The integration of LLMs with tool use, also known as LLM-driven autonomous agents (LAAs), has attracted substantial public attention. These agents are equipped with reasoning [33, 39], planning [25, 29], decision-making [37, 14, 5], and long-term memory capabilities [43, 12], and they are constructed upon the foundation of LLMs. LAAs can autonomously plan sub-goals for complex tasks, execute actions, obtain feedback from the environment, and adjust their behaviors to adapt [40, 36, 26]. LAAs have demonstrated significant potential in addressing complex real-world tasks, including software development [22, 11], drama creation [17], course design [6], chemistry experiments [2] and more. Furthermore, multi-agent collaboration plays a significant role in LAA applications, allowing agents to collaborate and interact to solve problems through various roleplaying scenarios [20, 7, 9, 15]. To the best of our knowledge, there is still a dearth of exploration regarding the use of agents, particularly multi-agents, for scientific discovery. In this paper, our objective is to undertake a preliminary effort to enhance the hypothesis proposing capability of LLMs by harnessing tools and multiple agents, along with conducting an analysis of influencing factors.</p>
<h2>B Implementation Details</h2>
<p>In this section, we delve into further implementation details of our experiments, including information about the constructed dataset and open-source models.</p>
<h2>B. 1 Details of Dataset</h2>
<p>Distribution of Training and Test Sets. We present the publication dates and topic distributions of the various datasets for comparison, as illustrated in Figure 5, where we utilize Nomic Atlas ${ }^{7}$ to visualize the topic distribution of abstracts in both the training and test datasets.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Distribution of the background and hypothesis pairs (BHP) dataset: In the left panel, we present the publication distribution by year for the training and seen test datasets, indicating a steady increase year by year until January 2023. In the center panel, we depict the publication distribution by month for the unseen test dataset, which was sampled from August 2023 and emphasizes the latter part of the month. The right panel displays the distribution of keywords in abstracts from the training, seen test, and unseen test datasets, represented by blue, yellow, and red, respectively.</p>
<h1>B. 2 Details of Models</h1>
<p>We present the meta-information of the open-source models used in our experiments, as shown in Table 4. We have gathered data regarding their pre-training, supervised learning corpus, and release dates to ensure the non-visibility of the unseen test data.</p>
<p>Table 4: To further ensure the non-visibility of the test data, we provide an overview of the related literature corpus within the training set of various LLMs, accompanied by their respective publication dates. The data marked with $\left(^{*}\right)$ is the data generated by people talking to ChatGPT. Our date marking is consistent with ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">SFT Data (Y/M)</th>
<th style="text-align: center;">Released</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo (0-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo (5-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-4*</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Vicuna-33b-v1.3</td>
<td style="text-align: center;">Llama-1</td>
<td style="text-align: center;">ShareGPT (Unknown)</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-7b-chat</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-13b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">openchat-v3.2-super</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Sharegpt4 Dataset (2023/06)</td>
<td style="text-align: center;">2023/09</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">MedAlpaca-13B</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/03)</td>
<td style="text-align: center;">2023/03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatDoctor*</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B</td>
<td style="text-align: center;">Llama-2*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/08*</td>
</tr>
</tbody>
</table>
<h2>C Additional Results</h2>
<p>We have included additional results from human evaluations in Table 5, primarily focusing on correlation scores between word overlap metrics and manual evaluations. Note that we continue to use the same samples used in human evaluation to compute BLEU and ROUGE-L for a fair comparison. We calculate the Pearson and Spearman coefficients between each automatic metric and the average human score. These results reveal that word overlap metrics, such as BLEU and ROUGE-L, exhibit notably lower correlation with manual scores. While BLEU and ROUGE-L may have a high correlation with relevance metrics, they are weak in providing a comprehensive evaluation of the generations. Conversely, evaluations conducted by ChatGPT demonstrate higher correlation with human evaluations, as illustrated in Table 2. However, there is still a significant need to explore advanced metrics, particularly automated ones, in the context of scientific discovery.</p>
<p>Table 5: The table illustrates the correlations between automatic metrics and human evaluations. We annotate the Pearson and Spearman scores after each correlation score, denoting them as $r$ and $\rho$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Word Overlap</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU $(r / \rho)$</td>
<td style="text-align: center;">ROUGE-L $(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
</tr>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo(0-shot)</td>
<td style="text-align: center;">16.59(0.03/0.01)</td>
<td style="text-align: center;">29.87(-0.04/-0.05)</td>
<td style="text-align: center;">1.90(0.87/0.78)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo(5-shot)*</td>
<td style="text-align: center;">14.99(-0.09/0.12)</td>
<td style="text-align: center;">27.51(-0.33/-0.35)</td>
<td style="text-align: center;">1.96(0.80/0.78)</td>
<td style="text-align: center;">2.15(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Llama-2-70b-chat(0-shot)</td>
<td style="text-align: center;">9.64(-0.21/-0.20)</td>
<td style="text-align: center;">22.17(-0.31/-0.28)</td>
<td style="text-align: center;">2.04(0.89/0.84)</td>
<td style="text-align: center;">1.96(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)</td>
<td style="text-align: center;">9.42(-0.58/-0.65)</td>
<td style="text-align: center;">20.59(-0.47/-0.42)</td>
<td style="text-align: center;">2.20(0.96/0.90)</td>
<td style="text-align: center;">2.33(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)*</td>
<td style="text-align: center;">9.60(-0.16/-0.10)</td>
<td style="text-align: center;">19.99(-0.15/-0.17)</td>
<td style="text-align: center;">2.01(0.97/0.94)</td>
<td style="text-align: center;">2.08(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(0-shot)</td>
<td style="text-align: center;">11.42(0.21/0.36)</td>
<td style="text-align: center;">24.11(0.29/0.49)</td>
<td style="text-align: center;">1.91(0.90/0.75)</td>
<td style="text-align: center;">1.81(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(5-shot)</td>
<td style="text-align: center;">9.86(-0.28/-0.37)</td>
<td style="text-align: center;">23.52(-0.17/-0.24)</td>
<td style="text-align: center;">2.01(0.85/0.89)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">PMC-LLaMA-13B(0-shot)</td>
<td style="text-align: center;">8.19(0.32/0.39)</td>
<td style="text-align: center;">21.85(0.18/0.27)</td>
<td style="text-align: center;">1.41(0.73/0.73)</td>
<td style="text-align: center;">1.88(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B(5-shot)*</td>
<td style="text-align: center;">5.52(0.06/-0.01)</td>
<td style="text-align: center;">13.64(0.26/0.23)</td>
<td style="text-align: center;">1.97(0.95/0.94)</td>
<td style="text-align: center;">1.92(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">21.48(-0.00/0.00)</td>
<td style="text-align: center;">27.83(0.17/0.27)</td>
<td style="text-align: center;">1.79(0.83/0.85)</td>
<td style="text-align: center;">1.77(1.00/1.00)</td>
</tr>
</tbody>
</table>
<h1>D Case Study</h1>
<p>In this section, we present several generated hypotheses from various models and provide examples of the evaluation process, step by step, using ChatGPT.</p>
<h2>D. 1 Generated Hypothesis</h2>
<p>We compare the generated hypotheses of different LLMs selected in human evaluation. The selected medicine literature was published in August 2023 [4], which proposed the power law index as an early marker of myocardial infarction. As shown in Table 6, some responses like gpt-3.5-turbo (0-shot) propose this hypothesis from zero, based only on the background. This indicates that LLMs have great potential in proposing highly novel and verifiable hypotheses and could play a significant role in scientific discovery. Furthermore, we showcase the proposed hypotheses in a multi-agent collaboration involving three roles: Analyst, Scientist, and Critic, working together to generate hypotheses across 6 rounds, as shown in Table 7.</p>
<p>Table 6: This table illustrates the hypothesis proposals from different LLMs.</p>
<h2>InSTRUCTED LLMS CASE</h2>
<h2>Background:</h2>
<p>(1) In the early stage of MI, $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples with an AUC $&gt;0.7$.
(2) In the mid-stage and late-stage of MI, E2, E3, and Etotal exhibit excellent discriminating performance with an AUC $&gt;0.6$.
(3) All cardiac tissues display a universal two-stage power-law rheology at different time scales, independent of their disease status.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The sequential order of MI-related changes in cardiac tissues starts from the cytoplasmic level and progresses to the cellular and tissue levels.
(2) Pathological changes at subcellular, cellular, and tissue levels are accompanied by mechanical alterations.
(3) The proposed mechanical markers have diagnostic potential for assessing the progression of MI and studying other cardiovascular diseases.</p>
<h2>gpt-3.5-turbo (0-shot):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long can be used as a reliable biomarker for early detection and differentiation of myocardial infarction (MI) from healthy controls.
(2) E2, E3, and Etotal can serve as effective biomarkers for distinguishing between mid-stage and late-stage MI patients with high accuracy.
(3) The universal two-stage power-law rheology observed in cardiac tissues may play a crucial role in maintaining tissue integrity and function, regardless of the presence or absence of disease.</p>
<h1>gpt-3.5-turbo (5-shot):</h1>
<p>(1) The levels of $\alpha$ short and $\alpha$ long may serve as potential biomarkers for early detection and diagnosis of myocardial infarction (MI).
(2) E2, E3, and Etotal may have utility as biomarkers for assessing the progression and severity of MI in the mid and late stages.
(3) The universal two-stage power-law rheology observed in cardiac tissues suggests a fundamental biomechanical principle underlying tissue behavior, regardless of disease status.</p>
<h2>Llama-2-70b-chat (0-shot):</h2>
<p>(1) E2, E3, and Etotal can also track and distinguish MI samples from healthy control samples in the early stage of MI, with an AUC $&gt;0.7$, similar to $\alpha$ short and $\alpha$ long.
(2) The universal two-stage power-law rheology of cardiac tissues is related to the alteration of the extracellular matrix and the degradation of collagen in the early stage of MI, which can be detected by E2, E3, and Etotal.
(3) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI is affected by the changes in the levels of collagen and proteoglycans in the extracellular matrix, which can be evaluated by measuring the mechanical properties of cardiac tissues using a uniaxial tensiometer.</p>
<h2>Llama-2-70b-chat (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI is attributed to their ability to capture the subtle changes in cardiac tissue mechanics caused by the onset of fibrosis.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the mid-stage and late-stage of MI due to their sensitivity to changes in collagen content and cross-linking, which are hallmarks of fibrosis progression.
(3) The universal two-stage power-law rheology observed in all cardiac tissues, independent of disease status, suggests a common mechanism of mechanical regulation that can be exploited for early disease detection and monitoring.</p>
<h2>Llama-2-70b-chat (5-shot)*:</h2>
<p>(1) The $\alpha$ short and $\alpha$ long parameters can detect changes in myocardial stiffness in the early stages of myocardial infarction (MI).
(2) E2, E3, and Etotal can differentiate between healthy controls and MI samples in the mid and late stages.
(3) The universal two-stage power-law rheology in cardiac tissues may be useful in developing novel therapeutic approaches for MI.</p>
<h2>WizardLM-70B-V1.0 (0-shot):</h2>
<p>(1) $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples in all stages of MI with an AUC $&gt;0.7$.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the early stage of MI with an AUC $&gt;0.6$.
(3) The two-stage power-law rheology observed in all cardiac tissues is related to the progression of MI.</p>
<h2>WizardLM-70B-V1.0 (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI may be attributed to their sensitivity to early molecular changes in the myocardium.
(2) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI may be due to their ability to reflect the progression of myocardial damage and remodeling.</p>
<p>(3) The universal two-stage power-law rheology observed in all cardiac tissues may be a fundamental property of cardiac tissue biomechanics, potentially useful for developing diagnostic and therapeutic strategies for MI.</p>
<h1>PMC-LLaMA-13B (0-shot):</h1>
<p>(1) $\alpha$ short and $\alpha$ long are effective biomarkers for distinguishing MI samples from healthy control samples both in the early stage and the late stage of MI.
(2) E2 and E3 (Etotal) are likely to serve as effective biomarkers for discriminating mid-stage or late-stage MI samples from healthy control samples.
(3) Cardiac tissues display a universal two-stage power-law rheology that is independent of their disease status.</p>
<h2>PMC-LLaMA-13B (5-shot):</h2>
<p>(1) Further comprehensive validation of the proposed framework is needed to elucidate the pathological changes in MI patients.
(2) Consideration of multiple markers is important.
(3) The influence of various external factors should be incorporated for accurate interpretation of the mechanical markers.</p>
<h2>WizardLM-13B-V1.2 (SFT):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long features can provide a more robust identification of MI samples.
(2) E2, E3, and Etotal features can serve as independent predictors of MI.
(3) The universal two-stage power-law rheology observed in all cardiac tissues may have diagnostic implications.</p>
<p>Table 7: This table illustrates a case from multi-agent collaboration on hypothesis proposing.</p>
<h2>Multi-Agent Case</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>Dialogue process of multi-agent collaboration.</h1>
<p>(Round 1) Analyst: Based on the provided research background, I have identified the following keywords and topics:</p>
<p>Keywords:</p>
<ul>
<li>Digital electrophysiology</li>
<li>Telemonitoring</li>
<li>Cardiac implantable electronic devices (CIEDs)</li>
<li>Wearable devices</li>
<li>Cardiac rhythm monitoring</li>
<li>Photoplethysmography</li>
</ul>
<p>Topics:</p>
<ol>
<li>Advancements in digital electrophysiology and telemonitoring of CIEDs</li>
<li>Benefits of telemonitoring in improving patient outcomes</li>
<li>Accuracy and reliability of wearable devices for cardiac rhythm monitoring using photoplethysmography</li>
</ol>
<p>Preliminary insights:</p>
<ul>
<li>The field of digital electrophysiology has seen significant development in the past 25 years, particularly in the areas of telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.</li>
<li>Telemonitoring of CIEDs has been shown to have positive impacts, such as reducing the time to detect clinically actionable events and improving patient outcomes.</li>
<li>Wearable devices, including smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</li>
</ul>
<p>Based on these insights, it can be hypothesized that the advancements in telemonitoring and wearable devices have led to improved cardiac rhythm monitoring and patient outcomes. However, further analysis and collaboration with the Scientist and Critic are needed to refine and validate this hypothesis.
(Round 2) Scientist: Based on the insights provided by the Analyst and the research background, I propose the following hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes and more accurate detection of clinically actionable events.</p>
<p>This hypothesis suggests that the development and implementation of telemonitoring technologies for CIEDs, along with the increasing accuracy and reliability of wearable devices using photoplethysmography, have positively impacted patient outcomes by reducing the time to detect clinically actionable events. By continuously monitoring cardiac rhythm, these advancements have the potential to provide timely interventions and improve overall patient care.</p>
<p>The hypothesis is grounded in the existing literature, which highlights the benefits of telemonitoring in improving patient outcomes and the increasing accuracy of wearable devices for cardiac rhythm monitoring. However, further collaboration with the Critic is necessary to evaluate the novelty, coherence, and scientific validity of the hypothesis.</p>
<p>(Round 3) Critic: The hypothesis proposed by the Scientist is well-aligned with the research background and the insights provided by the Analyst. It effectively captures the essence of the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring. The hypothesis also highlights the potential positive impact on patient outcomes and the detection of clinically actionable events.</p>
<p>However, there are a few areas that could be further refined or expanded upon in the hypothesis:</p>
<ol>
<li><strong>Specific outcomes</strong>: The hypothesis mentions ""improved patient outcomes," but it would be beneficial to specify the specific outcomes that are expected to improve. For example, are we referring to reduced hospitalizations, improved quality of life, or decreased mortality rates?</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The hypothesis briefly mentions the potential for timely interventions and improved patient care, but it would be helpful to elaborate on the specific mechanisms through which the advancements in telemonitoring and wearable devices are expected to achieve these outcomes. Are there specific interventions or treatments that can be administered based on the data collected from these devices?</li>
<li>${ }^{<em> </em>}$ Scientific validity**: While the hypothesis is grounded in the existing literature, it would be valuable to provide some references or studies that support the claims made in the hypothesis. This would enhance the scientific validity of the hypothesis and demonstrate that it is not just speculative but based on empirical evidence.</li>
</ol>
<p>To refine the hypothesis, I would recommend the Scientist to consider incorporating these suggestions and providing more specific details and references to support the claims made. This will strengthen the coherence and scientific validity of the hypothesis.</p>
<p>Analyst, based on the feedback provided, please re-evaluate the research background and provide any additional insights or interpretations that could further refine the hypothesis.
(Round 4) Analyst: Based on the feedback from the Critic, I have re-evaluated the research background and identified additional insights:</p>
<ol>
<li><strong>Specific outcomes</strong>: The research background mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes. These outcomes could include early detection of arrhythmias, reduced hospitalizations due to cardiac events, improved quality of life, and decreased mortality rates. It would be beneficial to specify one or more of these specific outcomes in the hypothesis.</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The advancements in telemonitoring and wearable devices can potentially lead to improved patient outcomes through several mechanisms. For example, continuous monitoring of cardiac rhythm can enable early detection of abnormal rhythms, allowing for timely interventions such as medication adjustments or electrical cardioversion. Additionally, the data collected from these devices can provide valuable insights for personalized treatment plans and remote patient management. Including these specific mechanisms in the hypothesis would enhance its coherence.</li>
<li>${ }^{<em> </em>}$ Scientific validity**: To strengthen the scientific validity of the hypothesis, it would be valuable to reference specific studies or research that support the claims made. For example, citing studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events or improving patient outcomes would provide empirical evidence to support the hypothesis.</li>
</ol>
<p>Incorporating these additional insights and suggestions, the refined hypothesis could be:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis takes into account the specific outcomes, mechanisms, and scientific validity, as suggested by the Critic. It provides a more comprehensive and well-supported statement that captures the essence of the research background and the collaborative insights.
(Round 5) Scientist: Based on the additional insights and suggestions provided by the Analyst and the Critic, I propose the following refined hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis incorporates the specific outcomes mentioned in the research background, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. It also expands on the mechanisms through which these advancements are expected to achieve these outcomes, including early detection of abnormal rhythms, timely interventions, and personalized treatment plans. Additionally, the hypothesis acknowledges the importance of scientific validity by referencing studies that support the claims made.</p>
<p>The refined hypothesis is both innovative and scientifically grounded, as it builds upon the existing literature and incorporates the collaborative insights provided by the Analyst and the Critic. It provides a logical and well-supported statement that can be further evaluated and tested.
(Round 6) Critic: The refined hypothesis proposed by the Scientist is well-structured and addresses the feedback provided by the Critic. It incorporates specific outcomes, mechanisms, and scientific validity, making it a comprehensive and scientifically robust statement.</p>
<p>Final Answer:
(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>D. 2 Evaluation Examples</h1>
<p>We provide evaluation results conducted by ChatGPT in Table 8. ChatGPT offers a step-by-step explanation for each metric, ultimately providing independent scores in integer format.</p>
<p>Table 8: This table illustrates a case for hypothesis evaluation using ChatGPT.</p>
<h2>CASE OF ChatGPT as ANNOTATOR</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h2>Evaluation Results</h2>
<p>Novelty:
Explanation: The generated scientific hypothesis builds upon the background information by stating that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes. It further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis goes beyond the background by proposing specific mechanisms through which the advancements in telemonitoring and wearable devices contribute to improved patient outcomes. It also mentions the supporting evidence from studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. Overall, the hypothesis provides new insights and expands upon the existing knowledge in the field.
Score: 2</p>
<h2>Relevance:</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/nomic-ai/nomic&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
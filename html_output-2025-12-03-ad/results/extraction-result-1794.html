<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1794 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1794</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1794</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-6ebdf55cade577979515dc5d09620204a07e7c92</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6ebdf55cade577979515dc5d09620204a07e7c92" target="_blank">Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images, including a novel extension of pixel-level domain adaptation that is term the GraspGAN.</p>
                <p><strong>Paper Abstract:</strong> Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1794.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1794.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kuka monocular grasping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kuka IIWA monocular RGB over-the-shoulder grasping system (based on Levine et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven, vision-based robotic grasping system that predicts grasp success from pairs of monocular RGB images and a motion command using a convolutional neural network; evaluated on physical Kuka IIWA arms and used as the target for sim-to-real transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kuka IIWA monocular RGB grasping system</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Compliant two-finger gripper mounted on Kuka IIWA arms with an over-the-shoulder monocular RGB camera; the system predicts probability of grasp success from image pairs (t=0 and current) and a 5-D motion command and uses a servoing controller to execute top-down pinch grasps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics simulator (off-the-shelf renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A simulated Kuka setup in Bullet that emulates physics of grasping (rigid-body contact dynamics), a rendered over-the-shoulder camera view, bin, arm, and simulated objects; used to collect synthetic grasp episodes at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>simplified dynamics with basic physics simulation and non-photorealistic rendering (low-to-moderate visual fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body contact dynamics, object mass, simple friction, camera viewpoint, object geometry and textures, lighting direction/brightness (configurable), robot kinematics and motor commands</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>renderer not photorealistic; many object models were low-fidelity/procedural shapes rather than high-detail scanned meshes; material properties and complex illumination/BRDF largely approximated; limited sensor noise modeling and no high-fidelity tactile/contact sensing</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Six physical Kuka IIWA robots with two-finger grippers, an over-the-shoulder monocular RGB camera, metal bin containing objects; large self-supervised dataset (~9.4M images, ~1M grasp attempts) collected by automatic labeling of grasp outcomes; held-out test of 36 diverse unseen objects with 6 robots (612 test grasps per evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Top-down monocular RGB robotic grasping (predicting and executing successful grasps on novel objects)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning of a convolutional grasp-success prediction network trained on labeled grasp attempts (real and/or adapted simulated data); simulated data collected via many parallel simulated arms and labeled by simulated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate (percentage of successful grasps over physical test attempts)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>70%–90% simulated grasp success (reported range for simulated robots before transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Varies by condition; example baselines: Real-only (all real data) 67.65%; GraspGAN + full real 76.67%; GraspGAN with no real labels (unsupervised pixel adaptation) 63.40%; See Table III and Table II for detailed per-condition rates.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual randomization (tray texture, object texture/color, robot arm color, lighting direction/brightness, camera pose/background images), dynamics randomization (object mass, lateral/rolling/spinning friction coefficients), motor-command noise (horizontal perturbation ε cm with ε ~ Uniform(0,1)).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Appearance (RGB) differences between simulator and camera images, limited photorealism of rendering, simplified object geometry/materials, unmodeled sensor noise and wear-and-tear differences between robots, and physical discrepancies in contact dynamics and material properties.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Large-scale simulated data (≈8M samples) combined with domain adaptation (GraspGAN pixel-level adaptation and/or DANN feature-level adversarial adaptation), domain-specific batch normalization (DBN), visual randomization, mixing simulated and real samples during training, and using unlabeled real images to train pixel-level adapters; procedural objects provided sufficient diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict numerical fidelity threshold given; key findings indicate photorealistic high-fidelity 3D models are not required—procedural low-fidelity objects suffice when combined with randomization and domain adaptation; dynamics randomization helps but results inconclusive; semantic fidelity (preserving object/arm positions) is important for pixel-level adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Models were trained and evaluated with varying amounts of labeled real data (from the full ~9.4M down to 1% ≈93,841 images); simulated data plus domain adaptation reduced labeled real data needs (claimed up to 50× reduction). The real dataset was also collected iteratively with periodically improved real models in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Procedurally generated geometric objects outperformed realistic ShapeNet models in these experiments (e.g., with 10% real data procedural w/visual rand ~74.88% vs ShapeNet ~68.79% in some settings); visual randomization and DBN mixing improved results; dynamics randomization effects were less conclusive.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including large amounts of simulated data from a low-cost simulator consistently improves real-world monocular RGB grasping; domain adaptation (especially combined pixel-level GraspGAN and feature-level DANN with DBN) substantially improves sim-to-real transfer and reduces required labeled real data by up to ~50×; procedural low-fidelity object models plus visual randomization and adversarial adaptation can give strong transfer (even enabling ≈63.4% real grasp success without any real labels).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1794.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1794.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraspGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraspGAN (pixel-level domain adaptation for grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel pixel-level domain adaptation model introduced in this paper that maps simulated RGB image pairs to realistic-looking images using a U-Net generator, a multi-scale patch discriminator, and task- and content-preserving losses, enabling improved sim-to-real transfer for vision-based grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kuka IIWA monocular RGB grasping system (same target agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>As above: two-finger gripper on Kuka IIWA with over-the-shoulder monocular camera; GraspGAN is used to adapt simulated visual inputs before training the grasp-success CNN.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics simulator (renders image pairs used as input to GraspGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator produces paired images (t=0 and current) of scene containing robot arm, tray, and objects; GraspGAN learns to restylize these simulated image pairs to match the distribution of unlabeled real camera images.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low-to-moderate visual fidelity simulation; GraspGAN aims to compensate for visual fidelity gap by restylization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>synthetic RGB appearance (textures, colors), object/arm/background segmentation masks (used as supervision in content loss), and geometric arrangement (semantic layout preserved)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>original simulated images lack photorealism, realistic materials, and sensor artifacts; GraspGAN does not explicitly model physical contact dynamics or tactile signals—only visual appearance is adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same real Kuka setup; GraspGAN trained using unlabeled real images from the over-the-shoulder camera to learn the target visual distribution; generator was trained with the same real dataset size used for training downstream models (but without labels).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual appearance of simulated training data (so that supervised grasp predictor trained on adapted images transfers to predicting grasps on real images); indirectly enables transfer of grasping skill.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Generator G trained adversarially (LSGAN objective) with a patch-based multi-scale discriminator, plus content-similarity (PMSE), segmentation mask reconstruction, and task-consistency losses (matching activations in the grasp CNN); downstream grasp CNN trained supervised on adapted images (and possibly real labeled images).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Real grasp success rate (%) when grasp predictor is trained using GraspGAN-adapted simulated images (with varying amounts of labeled real data).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Not directly applicable (GraspGAN is a visual adapter); simulated grasp success before adaptation reported as 70%–90%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>GraspGAN enabled 63.40% real grasp success in the no-labeled-real-data regime (Table II); with mixed setups GraspGAN + DBN + DANN reached up to 76.67% with all real data and showed strong gains in low-data regimes (e.g., 68.51% at 2% real data, 59.95% at 1% real data—see Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>GraspGAN experiments used both non-randomized and visually randomized simulated datasets; generator trained with unlabeled real images drawn from the same dataset size used for the domain-adversarial models.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Large visual domain shift (appearance, lighting, textures), limited semantic preservation if generator changes object/arm positions; must avoid semantic drift where generator changes important spatial/semantic relationships between arm and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Task-aware generator losses (content/PMSE, segmentation mask reconstruction, and matching activations in the grasp CNN) to preserve semantics, multi-scale patch discriminator to enforce local and global realism, training generator using unlabeled real images, and combining pixel-level adapted images with feature-level adversarial domain adaptation and DBN during training of the grasp CNN.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High semantic fidelity is required from the adapted images (generator must preserve object/arm layout); full photorealism is not required if semantic consistency and local realism are enforced and combined with feature-level adaptation and randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>GraspGAN outperformed sim-only baselines with and without randomization in the unsupervised (no labeled real) regime (63.40% vs 35.95% randomized sim-only and 23.53% plain sim-only). When combined with DANN and DBN mixing, GraspGAN-based pipelines produced the best or near-best performance across several labeled-data regimes, especially low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GraspGAN can transform low-fidelity simulated RGB image pairs into realistic images that preserve semantic content and substantially improve sim-to-real grasp transfer; when trained with unlabeled real images it enabled strong real-world performance (≈63% success) without any labeled real data, and combining pixel-level and feature-level adaptation yields the best results overall.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1794.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1794.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Virtual Scene Randomization (Domain Randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of randomizing visual and/or dynamics attributes in simulation (textures, lighting, camera, masses, friction, etc.) to expose a model to wide variation so that it generalizes to the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kuka IIWA monocular RGB grasping system (same target agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>As above: two-finger gripper on Kuka IIWA with over-the-shoulder monocular camera; domain randomization applied to simulated data generation to improve generalization of the grasping CNN.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics simulator (with randomization applied)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated Kuka scenes where visual attributes (textures, lighting, camera pose) and dynamics parameters are randomized across simulated episodes to produce diverse training data.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low-to-moderate fidelity simulation augmented by intentionally large randomized variation across scenes rather than high per-scene realism</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Randomized visual properties (textures, colors, lighting direction/brightness, backgrounds), randomized camera pose and bin location; randomized dynamics parameters (object mass and friction coefficients); motor command noise.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Individual scene realism still limited; randomization does not model high-fidelity material BRDFs, complex sensor noise, or exact real-world physical interactions beyond coarse mass/friction variation.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same Kuka physical setup; randomized simulated datasets were compared and combined with real labeled data for training and evaluation on physical robots and held-out test objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robust grasp prediction/execution that generalizes from varied simulated appearances/dynamics to the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised training of grasp-success CNN on datasets augmented/generated with various randomization schemes (no randomization, visual-only, dynamics-only, both).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate (%) on held-out physical test objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Randomization improved results over naive mixing in many cases; example: Procedural objects with visual randomization: 74.88% (10% real data, DANN+DBN) vs lower baselines; Table I and Table III give detailed comparisons. Randomization alone (Rand.) achieved up to 75.58% with all real data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Four randomization regimes tested: None; Visual (tray texture, object texture/color, robot color, lighting, camera/backgrounds); Dynamics (object mass, lateral/rolling/spinning friction); Both (visual + dynamics). Also Gaussian/Uniform motor command noise added.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Randomization addresses visual variability but cannot fully close gaps due to semantic/physical mismatches (rendering realism, material behavior, sensor noise); in some cases dynamics randomization results were inconclusive.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Visual randomization combined with domain-specific batch normalization (DBN) and/or domain-adversarial training improved transfer; randomization is especially helpful when combined with pixel- and feature-level adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High per-object geometric realism is not required if sufficient procedural diversity and appropriate randomization are used; randomization of visual attributes and some dynamics parameters improves robustness but is not a complete substitute for domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Visual randomization generally improved over no-randomization for procedural objects; dynamics randomization effects were mixed; combining randomization with DBN and DANN often gave better results than naive mixing without randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain/randomization of visual and dynamics parameters in simulation helps sim-to-real transfer for monocular RGB grasping, and when combined with domain adaptation and DBN mixing yields consistently better real-world performance; realistic 3D object models are not strictly necessary if randomization plus adaptation are used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World <em>(Rating: 2)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 2)</em></li>
                <li>Unsupervised pixel-level domain adaptation with generative adversarial neural networks <em>(Rating: 2)</em></li>
                <li>Learning from simulated and unsupervised images through adversarial training <em>(Rating: 1)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1794",
    "paper_id": "paper-6ebdf55cade577979515dc5d09620204a07e7c92",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Kuka monocular grasping",
            "name_full": "Kuka IIWA monocular RGB over-the-shoulder grasping system (based on Levine et al.)",
            "brief_description": "A data-driven, vision-based robotic grasping system that predicts grasp success from pairs of monocular RGB images and a motion command using a convolutional neural network; evaluated on physical Kuka IIWA arms and used as the target for sim-to-real transfer experiments.",
            "citation_title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection",
            "mention_or_use": "use",
            "agent_system_name": "Kuka IIWA monocular RGB grasping system",
            "agent_system_description": "Compliant two-finger gripper mounted on Kuka IIWA arms with an over-the-shoulder monocular RGB camera; the system predicts probability of grasp success from image pairs (t=0 and current) and a 5-D motion command and uses a servoing controller to execute top-down pinch grasps.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "PyBullet / Bullet physics simulator (off-the-shelf renderer)",
            "virtual_environment_description": "A simulated Kuka setup in Bullet that emulates physics of grasping (rigid-body contact dynamics), a rendered over-the-shoulder camera view, bin, arm, and simulated objects; used to collect synthetic grasp episodes at scale.",
            "simulation_fidelity_level": "simplified dynamics with basic physics simulation and non-photorealistic rendering (low-to-moderate visual fidelity)",
            "fidelity_aspects_modeled": "rigid-body contact dynamics, object mass, simple friction, camera viewpoint, object geometry and textures, lighting direction/brightness (configurable), robot kinematics and motor commands",
            "fidelity_aspects_simplified": "renderer not photorealistic; many object models were low-fidelity/procedural shapes rather than high-detail scanned meshes; material properties and complex illumination/BRDF largely approximated; limited sensor noise modeling and no high-fidelity tactile/contact sensing",
            "real_environment_description": "Six physical Kuka IIWA robots with two-finger grippers, an over-the-shoulder monocular RGB camera, metal bin containing objects; large self-supervised dataset (~9.4M images, ~1M grasp attempts) collected by automatic labeling of grasp outcomes; held-out test of 36 diverse unseen objects with 6 robots (612 test grasps per evaluation).",
            "task_or_skill_transferred": "Top-down monocular RGB robotic grasping (predicting and executing successful grasps on novel objects)",
            "training_method": "Supervised learning of a convolutional grasp-success prediction network trained on labeled grasp attempts (real and/or adapted simulated data); simulated data collected via many parallel simulated arms and labeled by simulated outcomes.",
            "transfer_success_metric": "Grasp success rate (percentage of successful grasps over physical test attempts)",
            "transfer_performance_sim": "70%–90% simulated grasp success (reported range for simulated robots before transfer)",
            "transfer_performance_real": "Varies by condition; example baselines: Real-only (all real data) 67.65%; GraspGAN + full real 76.67%; GraspGAN with no real labels (unsupervised pixel adaptation) 63.40%; See Table III and Table II for detailed per-condition rates.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Visual randomization (tray texture, object texture/color, robot arm color, lighting direction/brightness, camera pose/background images), dynamics randomization (object mass, lateral/rolling/spinning friction coefficients), motor-command noise (horizontal perturbation ε cm with ε ~ Uniform(0,1)).",
            "sim_to_real_gap_factors": "Appearance (RGB) differences between simulator and camera images, limited photorealism of rendering, simplified object geometry/materials, unmodeled sensor noise and wear-and-tear differences between robots, and physical discrepancies in contact dynamics and material properties.",
            "transfer_enabling_conditions": "Large-scale simulated data (≈8M samples) combined with domain adaptation (GraspGAN pixel-level adaptation and/or DANN feature-level adversarial adaptation), domain-specific batch normalization (DBN), visual randomization, mixing simulated and real samples during training, and using unlabeled real images to train pixel-level adapters; procedural objects provided sufficient diversity.",
            "fidelity_requirements_identified": "No strict numerical fidelity threshold given; key findings indicate photorealistic high-fidelity 3D models are not required—procedural low-fidelity objects suffice when combined with randomization and domain adaptation; dynamics randomization helps but results inconclusive; semantic fidelity (preserving object/arm positions) is important for pixel-level adaptation.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Models were trained and evaluated with varying amounts of labeled real data (from the full ~9.4M down to 1% ≈93,841 images); simulated data plus domain adaptation reduced labeled real data needs (claimed up to 50× reduction). The real dataset was also collected iteratively with periodically improved real models in prior work.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Procedurally generated geometric objects outperformed realistic ShapeNet models in these experiments (e.g., with 10% real data procedural w/visual rand ~74.88% vs ShapeNet ~68.79% in some settings); visual randomization and DBN mixing improved results; dynamics randomization effects were less conclusive.",
            "key_findings": "Including large amounts of simulated data from a low-cost simulator consistently improves real-world monocular RGB grasping; domain adaptation (especially combined pixel-level GraspGAN and feature-level DANN with DBN) substantially improves sim-to-real transfer and reduces required labeled real data by up to ~50×; procedural low-fidelity object models plus visual randomization and adversarial adaptation can give strong transfer (even enabling ≈63.4% real grasp success without any real labels).",
            "uuid": "e1794.0"
        },
        {
            "name_short": "GraspGAN",
            "name_full": "GraspGAN (pixel-level domain adaptation for grasping)",
            "brief_description": "A novel pixel-level domain adaptation model introduced in this paper that maps simulated RGB image pairs to realistic-looking images using a U-Net generator, a multi-scale patch discriminator, and task- and content-preserving losses, enabling improved sim-to-real transfer for vision-based grasping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Kuka IIWA monocular RGB grasping system (same target agent)",
            "agent_system_description": "As above: two-finger gripper on Kuka IIWA with over-the-shoulder monocular camera; GraspGAN is used to adapt simulated visual inputs before training the grasp-success CNN.",
            "domain": "general robotics manipulation / sim-to-real domain adaptation",
            "virtual_environment_name": "PyBullet / Bullet physics simulator (renders image pairs used as input to GraspGAN)",
            "virtual_environment_description": "Simulator produces paired images (t=0 and current) of scene containing robot arm, tray, and objects; GraspGAN learns to restylize these simulated image pairs to match the distribution of unlabeled real camera images.",
            "simulation_fidelity_level": "low-to-moderate visual fidelity simulation; GraspGAN aims to compensate for visual fidelity gap by restylization",
            "fidelity_aspects_modeled": "synthetic RGB appearance (textures, colors), object/arm/background segmentation masks (used as supervision in content loss), and geometric arrangement (semantic layout preserved)",
            "fidelity_aspects_simplified": "original simulated images lack photorealism, realistic materials, and sensor artifacts; GraspGAN does not explicitly model physical contact dynamics or tactile signals—only visual appearance is adapted.",
            "real_environment_description": "Same real Kuka setup; GraspGAN trained using unlabeled real images from the over-the-shoulder camera to learn the target visual distribution; generator was trained with the same real dataset size used for training downstream models (but without labels).",
            "task_or_skill_transferred": "Visual appearance of simulated training data (so that supervised grasp predictor trained on adapted images transfers to predicting grasps on real images); indirectly enables transfer of grasping skill.",
            "training_method": "Generator G trained adversarially (LSGAN objective) with a patch-based multi-scale discriminator, plus content-similarity (PMSE), segmentation mask reconstruction, and task-consistency losses (matching activations in the grasp CNN); downstream grasp CNN trained supervised on adapted images (and possibly real labeled images).",
            "transfer_success_metric": "Real grasp success rate (%) when grasp predictor is trained using GraspGAN-adapted simulated images (with varying amounts of labeled real data).",
            "transfer_performance_sim": "Not directly applicable (GraspGAN is a visual adapter); simulated grasp success before adaptation reported as 70%–90%.",
            "transfer_performance_real": "GraspGAN enabled 63.40% real grasp success in the no-labeled-real-data regime (Table II); with mixed setups GraspGAN + DBN + DANN reached up to 76.67% with all real data and showed strong gains in low-data regimes (e.g., 68.51% at 2% real data, 59.95% at 1% real data—see Table III).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "GraspGAN experiments used both non-randomized and visually randomized simulated datasets; generator trained with unlabeled real images drawn from the same dataset size used for the domain-adversarial models.",
            "sim_to_real_gap_factors": "Large visual domain shift (appearance, lighting, textures), limited semantic preservation if generator changes object/arm positions; must avoid semantic drift where generator changes important spatial/semantic relationships between arm and objects.",
            "transfer_enabling_conditions": "Task-aware generator losses (content/PMSE, segmentation mask reconstruction, and matching activations in the grasp CNN) to preserve semantics, multi-scale patch discriminator to enforce local and global realism, training generator using unlabeled real images, and combining pixel-level adapted images with feature-level adversarial domain adaptation and DBN during training of the grasp CNN.",
            "fidelity_requirements_identified": "High semantic fidelity is required from the adapted images (generator must preserve object/arm layout); full photorealism is not required if semantic consistency and local realism are enforced and combined with feature-level adaptation and randomization.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "GraspGAN outperformed sim-only baselines with and without randomization in the unsupervised (no labeled real) regime (63.40% vs 35.95% randomized sim-only and 23.53% plain sim-only). When combined with DANN and DBN mixing, GraspGAN-based pipelines produced the best or near-best performance across several labeled-data regimes, especially low-data regimes.",
            "key_findings": "GraspGAN can transform low-fidelity simulated RGB image pairs into realistic images that preserve semantic content and substantially improve sim-to-real grasp transfer; when trained with unlabeled real images it enabled strong real-world performance (≈63% success) without any labeled real data, and combining pixel-level and feature-level adaptation yields the best results overall.",
            "uuid": "e1794.1"
        },
        {
            "name_short": "Domain Randomization",
            "name_full": "Virtual Scene Randomization (Domain Randomization)",
            "brief_description": "The practice of randomizing visual and/or dynamics attributes in simulation (textures, lighting, camera, masses, friction, etc.) to expose a model to wide variation so that it generalizes to the real world.",
            "citation_title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "mention_or_use": "use",
            "agent_system_name": "Kuka IIWA monocular RGB grasping system (same target agent)",
            "agent_system_description": "As above: two-finger gripper on Kuka IIWA with over-the-shoulder monocular camera; domain randomization applied to simulated data generation to improve generalization of the grasping CNN.",
            "domain": "general robotics manipulation / sim-to-real domain adaptation",
            "virtual_environment_name": "PyBullet / Bullet physics simulator (with randomization applied)",
            "virtual_environment_description": "Simulated Kuka scenes where visual attributes (textures, lighting, camera pose) and dynamics parameters are randomized across simulated episodes to produce diverse training data.",
            "simulation_fidelity_level": "low-to-moderate fidelity simulation augmented by intentionally large randomized variation across scenes rather than high per-scene realism",
            "fidelity_aspects_modeled": "Randomized visual properties (textures, colors, lighting direction/brightness, backgrounds), randomized camera pose and bin location; randomized dynamics parameters (object mass and friction coefficients); motor command noise.",
            "fidelity_aspects_simplified": "Individual scene realism still limited; randomization does not model high-fidelity material BRDFs, complex sensor noise, or exact real-world physical interactions beyond coarse mass/friction variation.",
            "real_environment_description": "Same Kuka physical setup; randomized simulated datasets were compared and combined with real labeled data for training and evaluation on physical robots and held-out test objects.",
            "task_or_skill_transferred": "Robust grasp prediction/execution that generalizes from varied simulated appearances/dynamics to the real world.",
            "training_method": "Supervised training of grasp-success CNN on datasets augmented/generated with various randomization schemes (no randomization, visual-only, dynamics-only, both).",
            "transfer_success_metric": "Grasp success rate (%) on held-out physical test objects.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Randomization improved results over naive mixing in many cases; example: Procedural objects with visual randomization: 74.88% (10% real data, DANN+DBN) vs lower baselines; Table I and Table III give detailed comparisons. Randomization alone (Rand.) achieved up to 75.58% with all real data.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Four randomization regimes tested: None; Visual (tray texture, object texture/color, robot color, lighting, camera/backgrounds); Dynamics (object mass, lateral/rolling/spinning friction); Both (visual + dynamics). Also Gaussian/Uniform motor command noise added.",
            "sim_to_real_gap_factors": "Randomization addresses visual variability but cannot fully close gaps due to semantic/physical mismatches (rendering realism, material behavior, sensor noise); in some cases dynamics randomization results were inconclusive.",
            "transfer_enabling_conditions": "Visual randomization combined with domain-specific batch normalization (DBN) and/or domain-adversarial training improved transfer; randomization is especially helpful when combined with pixel- and feature-level adaptation.",
            "fidelity_requirements_identified": "High per-object geometric realism is not required if sufficient procedural diversity and appropriate randomization are used; randomization of visual attributes and some dynamics parameters improves robustness but is not a complete substitute for domain adaptation.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Visual randomization generally improved over no-randomization for procedural objects; dynamics randomization effects were mixed; combining randomization with DBN and DANN often gave better results than naive mixing without randomization.",
            "key_findings": "Domain/randomization of visual and dynamics parameters in simulation helps sim-to-real transfer for monocular RGB grasping, and when combined with domain adaptation and DBN mixing yields consistently better real-world performance; realistic 3D object models are not strictly necessary if randomization plus adaptation are used.",
            "uuid": "e1794.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "rating": 2
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised pixel-level domain adaptation with generative adversarial neural networks",
            "rating": 2
        },
        {
            "paper_title": "Learning from simulated and unsupervised images through adversarial training",
            "rating": 1
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2
        }
    ],
    "cost": 0.01450475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping</h1>
<p>Konstantinos Bousmalis ${ }^{<em> 1}$, Alex Irpan ${ }^{</em> 1}$, Paul Wohlhart ${ }^{* 2}$, Yunfei Bai ${ }^{2}$, Matthew Kelcey ${ }^{1}$, Mrinal Kalakrishnan ${ }^{2}$, Laura Downs ${ }^{1}$, Julian Ibarz ${ }^{1}$, Peter Pastor ${ }^{2}$, Kurt Konolige ${ }^{2}$, Sergey Levine ${ }^{1}$, Vincent Vanhoucke ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.</p>
<h2>I. INTRODUCTION</h2>
<p>Grasping is one of the most fundamental robotic manipulation problems. For virtually any prehensile manipulation behavior, the first step is to grasp the object(s) in question. Grasping has therefore emerged as one of the central areas of study in robotics, with a range of methods and techniques from the earliest years of robotics research to the present day. A central challenge in robotic manipulation is generalization: can a grasping system successfully pick up diverse new objects that were not seen during the design or training of the system? Analytic or model-based grasping methods [1] can achieve excellent generalization to situations that satisfy their assumptions. However, the complexity and unpredictability of unstructured real-world scenes has a tendency to confound these assumptions, and learning-based methods have emerged as a powerful complement [2], [3], [4], [5], [6].</p>
<p>Learning a robotic grasping system has the benefit of generalization to objects with real-world statistics, and can benefit from the advances in computer vision and deep learning. Indeed, many of the grasping systems that have shown the best generalization in recent years incorporate convolutional neural networks into the grasp selection process [2], [5], [4], [7]. However, learning-based approaches also introduce a major challenge: the need for large labeled datasets. These labels might consist of human-provided grasp</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Bridging the reality gap: our proposed pixel-level domain adaptation model takes as input (a) synthetic images produced by our simulator and produces (b) adapted images that look similar to (c) real-world ones produced by the camera over the physical robot's shoulder. We then train a deep vision-based grasping network with adapted and real images, which we further refine with feature-level adaptation.
points [8], or they might be collected autonomously [5], [6]. In both cases, there is considerable cost in both time and money, and recent studies suggest that the performance of grasping systems might be strongly influenced by the amount of data available [6].</p>
<p>A natural avenue to overcome these data requirements is to look back at the success of analytic, model-based grasping methods [1], which incorporate our prior knowledge of physics and geometry. We can incorporate this prior knowledge into a learning-based grasping system in two ways. First, we could modify the design of the system to use a model-based grasping method, for example as a scoring function for learning-based grasping [7]. Second, we could use our prior knowledge to construct a simulator, and generate synthetic experience that can be used in much the same way as real experience. The second avenue, which we explore in this work, is particularly appealing because we can use essentially the same learning system. However, incorporating simulated images presents challenges: simulated data differs in systematic ways from real-world data, and simulation must have sufficiently general objects. Addressing these two challenges is the principal subject of our work.</p>
<p>Our work has three main contributions. (a) Substantial</p>
<p>improvement in grasping performance from monocular RGB images by incorporating synthetic data: We propose approaches for incorporating synthetic data into end-to-end training of vision-based robotic grasping that we show achieves substantial improvement in performance, particularly in the lower-data and no-data regimes. (b) Detailed experimentation for simulation-to-real world transfer: Our experiments involved 25, 704 real grasps of 36 diverse test objects and consider a number of dimensions: the nature of the simulated objects, the kind of randomization used in simulation, and the domain adaptation technique used to adapt simulated images to the real world. (c) The first demonstration of effective simulation-to-real-world transfer for purely monocular vision-based grasping: To our knowledge, our work is the first to demonstrate successful simulation-to-realworld transfer for grasping, with generalization to previously unseen natural objects, using only monocular RGB images.</p>
<h2>II. Related Work</h2>
<p>Robotic grasping is one of the most widely explored areas of manipulation. While a complete survey of grasping is outside the scope of this work, we refer the reader to standard surveys on the subject for a more complete treatment [2]. Grasping methods can be broadly categorized into two groups: geometric methods and data-driven methods. Geometric methods employ analytic grasp metrics, such as force closure [9] or caging [10]. These methods often include appealing guarantees on performance, but typically at the expense of relatively restrictive assumptions. Practical applications of such approaches typically violate one or more of their assumptions. For this reason, data-driven grasping algorithms have risen in popularity in recent years. Instead of relying exclusively on an analytic understanding of the physics of an object, data-driven methods seek to directly predict either human-specified grasp positions [8] or empirically estimated grasp outcomes [5], [6]. A number of methods combine both ideas, for example using analytic metrics to label training data [3], [7].</p>
<p>Simulation-to-real-world transfer in robotics is an important goal, as simulation can be a source of practically infinite cheap data with flawless annotations. For this reason, a number of recent works have considered simulation-toreal world transfer in the context of robotic manipulation. Saxena et al. [11] used rendered objects to learn a visionbased grasping model. Gulatieri et al. and Viereck et al. [4], [12] have considered simulation-to-real world transfer using depth images. Depth images can abstract away many of the challenging appearance properties of real-world objects. However, not all situations are suitable for depth cameras, and coupled with the low cost of simple RGB cameras, there is considerable value in studying grasping systems that solely use monocular RGB images.</p>
<p>A number of recent works have also examined using randomized simulated environments [13], [14] for simulation-to-real world transfer for grasping and grasping-like manipulation tasks, extending on prior work on randomization for robotic mobility [15]. These works apply randomization in
the form of random textures, lighting, and camera position to their simulator. However, unlike our work, these prior methods considered grasping in relatively simple visual environments, consisting of cubes or other basic geometric shapes, and have not yet been demonstrated on grasping diverse, novel real-world objects of the kind considered in our evaluation.</p>
<p>Domain adaptation is a process that allows a machine learning model trained with samples from a source domain to generalize to a target domain. In our case the source domain is the simulation, whereas the target is the real world. There has recently been a significant amount of work on domain adaptation, particularly for computer vision [16], [17]. Prior work can be grouped into two main types: feature-level and pixel-level adaptation. Feature-level domain adaptation focuses on learning domain-invariant features, either by learning a transformation of fixed, pre-computed features between source and target domains [18], [19], [20], [21] or by learning a domain-invariant feature extractor, often represented by a convolutional neural network (CNN) [22], [23], [24]. Prior work has shown the latter is empirically preferable on a number of classification tasks [22], [24]. Domain-invariance can be enforced by optimizing domain-level similarity metrics like maximum mean discrepancy [24], or the response of an adversarially trained domain discriminator [22]. Pixellevel domain adaptation focuses on re-stylizing images from the source domain to make them look like images from the target domain [25], [26], [27], [28]. To our knowledge, all such methods are based on image-conditioned generative adversarial networks (GANs) [29]. In this work, we compare a number of different domain adaptation regimes. We also present a new method that combines both feature-level and pixel-level domain adaptation for simulation-to-real world transfer for vision-based grasping.</p>
<h2>III. BACKGROUND</h2>
<p>Our goal in this work is to show the effect of using simulation and domain adaptation in conjunction with a tested data-driven, monocular vision-based grasping approach. To this effect, we use such an approach, as recently proposed by Levine et al. [6]. In this section we will concisely discuss this approach, and the two main domain adaptation techniques [22], [26], [27] our method is based on.</p>
<h2>A. Deep Vision-Based Robotic Grasping</h2>
<p>The grasping approach [6] we use in this work consists of two components. The first is a grasp prediction convolutional neural network (CNN) $C\left(\mathbf{x}<em i="i">{i}, \mathbf{v}</em>}\right)$ that accepts a tuple of visual inputs $\mathbf{x<em i__0="i_{0">{i}=\left{\mathbf{x}</em>}}, \mathbf{x<em r="r">{i</em>}}\right}$ and a motion command $\mathbf{v<em i="i">{i}$, and outputs the predicted probability that executing $\mathbf{v}</em>}$ will result in a successful grasp. $\mathbf{x<em 0="0">{i</em>}}$ is an image recorded before the robot becomes visible and starts the grasp attempt, and $\mathbf{x<em r="r">{i</em>$ is specified in the frame of the base of the robot and corresponds to a relative change of the end-effector's current position and rotation about the vertical axis. We consider only top-down}}$ is an image recorded at the current timestep. $\mathbf{v}_{i</p>
<p>pinch grasps, and the motion command has, thus, 5 dimensions: 3 for position, and 2 for a sine-cosine encoding of the rotation. The second component of the method is a simple, manually designed servoing function that uses the grasp probabilities predicted by $C$ to choose the motor command $\mathbf{v}_{i}$ that will continuously control the robot. We can train the grasp prediction network $C$ using standard supervised learning objectives, and so it can be optimized independently from the servoing mechanism. In this work, we focus on extending the first component to include simulated data in the training set for the grasp prediction network $C$, leaving the other parts of the system unchanged.</p>
<p>The datasets for training the grasp prediction CNN $C$ are collections of visual episodes of robotic arms attempting to grasp various objects. Each grasp attempt episode consists of $T$ time steps which result in $T$ distinct training samples. Each sample $i$ includes $\mathbf{x}<em i="i">{i}, \mathbf{v}</em>$ of the entire grasp sequence. The visual inputs are $640 \times 512$ images that are randomly cropped to a $472 \times 472$ region during training to encourage translation invariance.}$, and the success label $y_{i</p>
<p>The central aim of our work is to compare different training regimes that combine both simulated and real-world data for training $C$. Although we do consider training entirely with simulated data, as we discuss in Section IV-A, most of the training regimes we consider combine medium amounts of real-world data with large amounts of simulated data. To that end, we use the self-supervised real-world grasping dataset collected by Levine et al. [6] using 6 physical Kuka IIWA arms. The goal of the robots was to grasp any object within a specified goal region. Grasping was performed using a compliant two-finger gripper picking objects out of a metal bin, with a monocular RGB camera mounted behind the arm. The full dataset includes about 1 million grasp attempts on approximately 1,100 different objects, resulting in about 9.4 million real-world images. About half of the dataset was collected using random grasps, and the rest using iteratively retrained versions of $C$. Aside from the variety of objects, each robot differed slightly in terms of wear-and-tear, as well as the camera pose. The outcome of the grasp attempt was determined automatically. The particular objects in front of each robot were regularly rotated to increase the diversity of the dataset. Some examples of grasping images from the camera's viewpoint are shown in Figure 2d.</p>
<p>When trained on the entire real dataset, the best CNN used in the approach outlined above achieved successful grasps $67.65 \%$ of the time. Levine et al. [6] reported an additional increase to $77.18 \%$ from also including 2.7 million images from a different robot. We excluded this additional dataset for the sake of a more controlled comparison, so as to avoid additional confounding factors due to domain shift within the real-world data. Starting from the Kuka dataset, our experiments study the effect of adding simulated data and of reducing the number of real world data points by taking subsets of varying size (down to only 93,841 real world images, which is $1 \%$ of the original set).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Top Row: The setup we used for collecting the (a) simulated and (b) real-world datasets. Bottom Row: Images used during training of (c) simulated grasping experience with procedurally generated objects; and of (d) real-world experience with a varied collection of everyday physical objects. In both cases, we see the pairs of image inputs for our grasp success prediction model $C$ : the images at $t=0$ and the images at the current timestamp.</p>
<h2>B. Domain Adaptation</h2>
<p>As part of our proposed approach we use two domain adaptation techniques: domain-adversarial training and pixel-level domain adaptation. Ganin et al. [22] introduced domain-adversarial neural networks (DANNs), an architecture trained to extract domain-invariant yet expressive features. DANNs were primarily tested in the unsupervised domain adaptation scenario, in the absence of any labeled target domain samples, although they also showed promising results in the semi-supervised regime [24]. Their model's first few layers are shared by two modules: the first predicts task-specific labels when provided with source data while the second is a separate domain classifier trained to predict the domain $\hat{d}$ of its inputs. The DANN loss is the cross-entropy loss for the domain prediction task: $\mathscr{L}<em i="0">{\mathrm{DANN}}=\sum</em>$ are the number of source and target samples.}^{N_{t}+N_{l}}\left{d_{i} \log \hat{d_{i}}+\left(1-d_{i}\right) \log \left(1-\hat{d_{i}}\right)\right}, \quad$ where $d_{i} \in{0,1}$ is the ground truth domain label for sample $i$, and $N_{s}, N_{t</p>
<p>The shared layers are trained to maximize $\mathscr{L}_{\mathrm{DANN}}$, while the domain classifier is trained adversarially to minimize it. This minimax optimization is implemented by a gradient reversal layer (GRL). The GRL has the same output as the identity function, but negates the gradient during backprop. This lets us compute the gradient for both the domain clas-</p>
<p>sifier and the shared feature extractor in a single backward pass. The task loss of interest is simultaneously optimized with respect to the shared layers, which grounds the shared features to be relevant to the task.</p>
<p>While DANN makes the features extracted from both domains similar, the goal in pixel-level domain adaptation [26], [27], [28], [25] is to learn a generator function $G$ that maps images from a source to a target domain at the input level. This approach decouples the process of domain adaptation from the process of task-specific predictions, by adapting the images from the source domain to make them appear as if they were sampled from the target domain. Once the images are adapted, they can replace the source dataset and the relevant task model can be trained as if no domain adaptation were required. Although all these methods are similar in spirit, we use ideas primarily from PixelDA [26] and SimGAN [27], as they are more suitable for our task. These models are particularly effective if the goal is to maintain the semantic map of original and adapted synthetic images, as the transformations are primarily low-level: the methods make the assumption that the differences between the domains are primarily low-level (due to noise, resolution, illumination, color) rather than high-level (types of objects, geometric variations, etc).</p>
<p>More formally, let $\mathbf{X}^{s}=\left{\mathbf{x}<em i="i">{i}^{s}, \mathbf{y}</em>\right}}^{s<em i="i">{i=0}^{N^{s}}$ represent a dataset of $N^{s}$ samples from the source domain and let $\mathbf{X}^{t}=\left{\mathbf{x}</em>}^{t}, \mathbf{y<em i="0">{i}^{t}\right}</em>}^{N^{t}}$ represent a dataset of $N^{t}$ samples from the target domain. The generator function $G\left(\mathbf{x}^{s} ; \boldsymbol{\theta<em G="G">{G}\right) \rightarrow \mathbf{x}^{f}$, parameterized by $\boldsymbol{\theta}</em>$, the task-specific model can be trained as if the training and test data were from the same distribution.}$, maps a source image $\mathbf{x}^{s} \in \mathbf{X}^{s}$ to an adapted, or fake, image $\mathbf{x}^{f}$. This function is learned with the help of an adversary, a discriminator function $D\left(\mathbf{x} ; \boldsymbol{\theta}_{D}\right)$ that outputs the likelihood $d$ that a given image $\mathbf{x}$ is a real-world sample. Both $G$ and $D$ are trained using the standard adversarial objective [29]. Given the learned generator function $G$, it is possible to create a new dataset $\mathbf{X}^{f}=\left{G\left(\mathbf{x}^{s}\right), \mathbf{y}^{s}\right}$. Finally, given an adapted dataset $\mathbf{X}^{f</p>
<p>PixelDA was evaluated in simulation-to-real-world transfer. However, the 3D models used by the renderer in [26] were very high-fidelity scans of the objects in the real-world dataset. In this work we examine for the first time how such a technique can be applied in situations where (a) no 3D models for the objects in the real-world are available and (b) the system is supposed to generalize to yet another set of previously unseen objects in the actual real-world grasping task. Furthermore, we use images of $472 \times 472$, more than double the resolution in [26], [27]. This makes learning the generative model $G$ a much harder task and requires significant changes compared to previous work: the architecture of both $G$ and $D$, the GAN training objective, and the losses that aid with training the generator (contentsimilarity and task losses) are different from the original implementations, resulting in a novel model evaluated under these new conditions.</p>
<h2>IV. OUR APPROACH</h2>
<p>One of the aims of our work is to study how final grasping performance is affected by the 3D object models our simulated experience is based on, the scene appearance and dynamics in simulation, and the way simulated and real experience is integrated for maximal transfer. In this section we outline, for each of these three factors, our proposals for effective simulation-to-real-world transfer for our task.</p>
<h2>A. Grasping in Simulation</h2>
<p>A major difficulty in constructing simulators for robotic learning is to ensure diversity sufficient for effective generalization to real-world settings. In order to evaluate simulation-to-real world transfer, we used one dataset of real-world grasp attempts (see Sect. III-A), and multiple such datasets in simulation. For the latter, we built a basic virtual environment based on the Bullet physics simulator and the simple renderer that is shipped with it [30]. The environment emulates the Kuka hardware setup by simulating the physics of grasping and by rendering what a camera mounted looking over the Kuka shoulder would perceive: the arm, the bin that contains the object, and the objects to grasp in scenes similar to the ones the robot encounters in the real world.</p>
<p>A central question here is regarding the realism of the 3D models used for the objects to grasp. To answer it, we evaluate two different sources of objects in our experiments: (a) procedurally generated random geometric shapes and (b) realistic objects obtained from the publicly-available ShapeNet [31] 3D model repository. We procedurally generated 1,000 objects by attaching rectangular prisms at random locations and orientations, as seen in Fig. 3a. We then converted the set of prisms to a mesh using an off-the-shelf renderer, Blender, and applied a random level of smoothing. Each object was given UV texture coordinates and random colors. For our Shapenet-based datasets, we used the ShapeNetCore.v2 [31] collection of realistic object models, shown in Figure 3b. This particular collection contains 51,300 models in 55 categories of household objects, furniture, and vehicles. We rescaled each object to a random graspable size with a maximum extent between 12 cm and 23 cm (real-world objects ranged from 4 cm to 20 cm in length along the longest axis) and gave it a random mass between 10 g and 500 g , based on the approximate volume of the object.</p>
<p>Once the models were imported into our simulator, we collected our simulation datasets via a similar process to the one in the real world, with a few differences. As mentioned above, the real-world dataset was collected by using progressively better grasp prediction networks. These networks were swapped for better versions manually and rather infrequently [6]. In contrast to the 6 physical Kuka IIWA robots that were used to collect data in the real world, we used 1,000 to 2,000 simulated arms at any given time to collect our synthetic data, and the models that were used to collect the datasets were being updated continuously by an automated process. This resulted in datasets that were</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Comparison of (a) some of our 1,000 procedural, (b) some of the 51,300 ShapeNet objects, both used for data collection in simulation, and the (c) 36 objects we used only for evaluating grasping in the real-world, that were not seen during training. The variety of shapes, sizes, and material properties makes the test set very challenging.</p>
<p>collected by grasp prediction networks of varying performance, which added diversity to the collected samples. After training our grasping approach in our simulated environment, the simulated robots were successful on 70%-90% of the simulated grasp attempts. Note that all of the grasp success prediction models used in our experiments were trained from scratch using these simulated grasp datasets.</p>
<h3><em>B. Virtual Scene Randomization</em></h3>
<p>Another important question is whether randomizing the visual appearance and dynamics in the scene affects grasp performance and in what way. One of the first kind of diversities we considered was the addition of ε cm, where ε ~ A(0,1), to the horizontal components of the motor command. This improved real grasp success in early experiments, so we added this kind of randomization for all simulated samples. Adding this noise to real data did not help. To further study the effects of virtual scene randomization, we built datasets with four different kinds of scene randomization: (a) No randomization: Similar to real-world data collection, we only varied camera pose, bin location, and used 6 different real-world images as backgrounds; (b) Visual Randomization: We varied tray texture, object texture and color, robot arm color, lighting direction and brightness; (c) Dynamics Randomization: We varied object mass, and object lateral/rolling/spinning friction coefficients; and (d) All: both visual and dynamics randomization.</p>
<h3><em>C. Domain Adaptation for Vision-Based Grasping</em></h3>
<p>As mentioned in Sect. II, there are two primary types of methods used for domain adaptation: feature-level, and pixel-level. Here we propose a feature-level adaptation method and a novel pixel-level one, which we call GraspGAN. Given original synthetic images, GraspGAN produces adapted images that look more realistic. We subsequently use the trained generator from GraspGAN as a fixed module that adapts our synthetic visual input, while performing feature-level domain adaptation on extracted features that account for both the transferred images and synthetic motor command input.</p>
<p>For our feature-level adaptation technique we use a DANN loss on the last convolutional layer of our grasp success prediction model <em>C</em>, as shown in Fig. 4c. In preliminary experiments we found that using the DANN loss on this layer yielded superior performance compared to applying it at the activations of other layers. We used the domain classifier proposed in [22]. One of the early research questions we faced was what the interaction of batch normalization (BN) [32] with the DANN loss would be, as this has not been examined in previous work. We use BN in every layer of <em>C</em> and in a naïve implementation of training models with data from two domains, a setting we call naïve mixing, batch statistics are calculated without taking the domain labels of each sample into account. However, the two domains are bound to have different statistics, which means that calculating and using them separately for simulated and real-world data while using the same parameters for <em>C</em> might be beneficial. We call this way of training data from two domains domain-specific batch normalization (DBN) mixing, and show it is a useful tool for domain adaptation, even when a DANN loss is not used.</p>
<p>In our pixel-level domain adaptation model, GraspGAN, shown in Fig. 4, <em>G</em> is a convolutional neural network that follows a U-Net architecture [33], and uses average pooling for downsampling, bilinear upsampling, concatenation and 1 × 1 convolutions for the U-Net skip connections, and instance normalization [34]. Our discriminator <em>D</em> is a patch-based [35] CNN with 5 convolutional layers, with an effective input size of 70 × 70. It is fully convolutional on 3 scales (472 × 472, 236 × 236, and 118 × 118) of the two input images, <strong>x</strong><sup>0</sup><sub>0</sub> and <strong>x</strong><sup>0</sup><sub>c</sub>, stacked into a 6 channel input, producing domain estimates for all patches which are then combined to compute the joint discriminator loss. This novel multi-scale patch-based discriminator design can learn to assess both global consistency of the generated image, as well as realism of local textures. Stacking the channels of the two input images enables the discriminator to recognize relationships between the two images, so it can encourage the generator to respect them (<em>e.g.</em>, paint the tray with the same texture in both images, but insert realistic shadows for the arm). Our task model <em>C</em> is the grasp success prediction CNN from [6].</p>
<p>To train GraspGAN, we employ a least-squares generative adversarial objective (LSGAN) [36] to encourage <em>G</em> to produce realistic images. During training, our generator <em>G</em>(<strong>x</strong><sup>s</sup>; <strong>θ</strong><sub>G</sub>) → <strong>x</strong><sup>T</sup> maps synthetic images <strong>x</strong><sup>s</sup> to adapted images <strong>x</strong><sup>T</sup>, by individually passing <strong>x</strong><sup>0</sup><sub>0</sub> and <strong>x</strong><sup>0</sup><sub>c</sub> through two instances of the generator network displayed in Figure 4. Similar to traditional GAN training, we perform optimization in alternating steps by minimizing the following loss terms w.r.t. the parameters of each sub-network:</p>
<p>$$
\min_{\theta_G} \lambda_g \mathcal{L}<em tg="tg">{gen}(G, D) + \lambda</em>} \mathcal{L<em content="content">{task}(G, C) + \lambda_c \mathcal{L}</em>
$$}(G) \tag{1</p>
<p>$$
\min_{\theta_G, \theta_C} \lambda_d \mathcal{L}<em td="td">{discr}(G, D) + \lambda</em>
$$} \mathcal{L}_{task}(G, C), \tag{2</p>
<p>where $\mathcal{L}<em discr="discr">{gen}$ and $\mathcal{L}</em>}$ are the LSGAN generator and discriminator losses, $\mathcal{L<em content="content">{task}$ is the task loss, $\mathcal{L}</em>$, $\lambda_c$, the respective weights. The LSGAN discriminator loss is the }$ is the content-similarity loss, and $\lambda_g$, $\lambda_d$, $\lambda_{tg}$, $\lambda_{td<em>L</em>2 distance between its likelihood output $\hat{d}$ and the domain labels $d = 0$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Our proposed approach: (a) Overview of our pixel-level domain adaptation model, GraspGAN. Tuples of images from the simulation <strong>x</strong><sup>i</sup> are fed into the generator <em>G</em> to produce realistic versions <strong>x</strong><sup>f</sup>. The discriminator <em>D</em> gets unlabeled real world images <strong>x</strong><sup>i</sup> and <strong>x</strong><sup>f</sup> and is trained to distinguish them. Real and adapted images are also fed into the grasp success prediction network <em>C</em>, trained in parallel (motion commands <strong>v</strong> are not shown to reduce clutter). <em>G</em>, thus, gets feedback from <em>D</em> and <em>C</em> to make adapted images look real and maintain the semantic information. (b) Architectures for <em>G</em> and <em>D</em>. Blue boxes denote convolution/normalization/activation-layers, where <em>n64s2:IN:relu</em> means 64 filters, stride 2, instance normalization <em>IN</em> and <em>relu</em> activation. Unless specified all convolutions are 3 × 3 in <em>G</em> and 4 × 4 in <em>D</em>. (c) DANN model: <em>C</em><sub>1</sub> has 7 conv layers and <em>C</em><sub>2</sub> has 9 conv layers. Further details can be found in [6]. Domain classifier uses GRL and two 100 unit layers.</p>
<p>for fake and <em>d</em> = 1 for real images, while for the generator loss the label is flipped, such that there is a high loss if the discriminator predicts <em>d</em> = 0 for a generated image. The task loss measures how well the network <em>C</em> predicts grasp success on transferred and real examples by calculating the binomial cross-entropy of the labels <em>y</em><sub><em>i</em></sub>.</p>
<p>It is of utmost importance that the GraspGAN generator, while making the input image look like an image from the real world scenario, does not change the semantics of the simulated input, for instance by drawing the robot's arm or the objects in different positions. Otherwise, the information we extract from the simulation in order to train the task network would not correspond anymore to the generated image. We thus devise several additional loss terms, accumulated in <em>L</em><sub><em>content</em></sub>, to help anchor the generated image to the simulated one on a semantic level. The most straightforward restriction is to not allow the generated image to deviate much from the input. To that effect we use the PMSE loss, also used by [26]. We also leverage the fact that we can have semantic information about every pixel in the synthetic images by computing segmentation masks <strong>m</strong><sup><em>f</em></sup> of the corresponding rendered images for the background, the tray, robot arm, and the objects. We use these masks by training our generator <em>G</em> to also produce <strong>m</strong><sup><em>f</em></sup> as an additional output for each adapted image, with a standard <em>L</em><sub>2</sub> reconstruction loss. Intuitively, it forces the generator to extract semantic information about all the objects in the scene and encode them in the intermediate latent representations. This information is then available during the generation of the output image as well. Finally, we additionally implement a loss term that provides more dense feedback from the task tower than just the single bit of information about grasp success. We encourage the generated image to provide the same semantic information to the task network as the corresponding simulated one by penalizing differences in activations of the final convolutional layer of <em>C</em> for the two images. This is similar in principle to the perceptual loss [37] that uses the activations of an ImageNet-pretrained VGG model as a way to anchor the restylization of an input image. In contrast, here <em>C</em> is trained at the same time, the loss is specific to our goal, and it helps preserve the semantics in ways that are relevant to our prediction task.</p>
<h2>V. EVALUATION</h2>
<p>This section aims to answer the following research questions: (a) is the use of simulated data from a low quality simulator aiding in improving grasping performance in the real world? (b) is the improvement consistent with varying amounts of real-world labeled samples? (c) how realistic do graspable objects in simulation need to be? (d) does randomizing the virtual environment affect simulation-to-real world transfer, and what are the randomization attributes that help most? (e) does domain adaptation allow for better utilization of simulated grasping experience?</p>
<p>In order to answer these questions, we evaluated a number of different ways for training a grasp success prediction model <em>C</em> with simulated data and domain adaptation<sup>1</sup>. When simulated data was used, the number of simulated samples was always approximately 8 million. We follow the grasp success evaluation protocol described by Levine <em>et al.</em> [6]. We used 6 Kuka IIWA robots for our real-world experiments</p>
<p><sup>1</sup>Visit <a href="https://goo.gl/G1HSws">https://goo.gl/G1HSws</a> for our supplementary video.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: The effect of using 8 million simulated samples of procedural objects with no randomization and various amounts of real data, for the best technique in each class.</p>
<p>TABLE I: The effect of our choices for simulated objects and randomization in terms of grasp success. We compared the performance of models trained jointly on grasps of procedural vs ShapeNet objects with 10% of the real data. Models were trained with DANN and DBN mixing.</p>
<table>
<thead>
<tr>
<th>Randomization</th>
<th>None</th>
<th>Visual</th>
<th>Dynamics</th>
<th>Both</th>
</tr>
</thead>
<tbody>
<tr>
<td>Procedural</td>
<td>71.93%</td>
<td>74.88%</td>
<td>73.95%</td>
<td>72.86%</td>
</tr>
<tr>
<td>ShapeNet</td>
<td>69.61%</td>
<td>68.79%</td>
<td>68.62%</td>
<td>69.84%</td>
</tr>
</tbody>
</table>
<p>and a test set consisting of the objects shown in Fig. 3c, the same used in [6], with 6 different objects in each bin for each robot. These objects were not included in the real-world training set and were not used in any way when creating our simulation datasets. Each robot executes 102 grasps, for a total of 612 test grasps for each evaluation. During execution, each robot picks up objects from one side of the bin and drops them on the other, alternating every 3 grasps. This prevents the model from repeatedly grasping the same object. Optimal models were selected by using the accuracy of C on a held-out validation set of 94,000 real samples.</p>
<p>The first conclusion from our results is that simulated data from an off-the-shelf simulator always aids in improving vision-based real-world grasping performance. As one can see in Fig. 5, which shows the real grasp success gains by incorporating simulated data from our procedurally-generated objects, using our simulated data significantly and consistently improves real-world performance regardless of the number of real-world samples.</p>
<p>We also observed that we do not need realistic 3D models to obtain these gains. We compared the effect of using random, procedurally-generated shapes and ShapeNet objects in combination with 10% of the real-world data, under all randomization scenarios. As shown in Table I we found that using procedural objects is the better choice in all cases. This finding has interesting implications for simulation to real-world transfer, since content creation is often a major bottleneck in producing generalizable simulated data. Based on these results, we decided to use solely procedural objects for the rest of our experiments.</p>
<p>Table III shows our main results: the grasp success performance for different combinations of simulated data generation and domain adaptation methods, and with different quantities of real-world samples. The different settings are: Real-Only, in which the model is given only real data; Naïve Mixing (Naïve Mix): Simulated samples generated with no virtual scene randomization are mixed with real-world samples such that half of each batch consists of simulated images; DBN Mixing &amp; Randomization (Rand.): The simulated dataset is generated with visual-only randomization. The simulated samples are mixed with real-world samples as in the naive mixing case, and the models use DBN; DBN Mixing &amp; DANN (DANN): Simulated samples are generated with no virtual scene randomization and the model is trained with a domain-adversarial method with DBN; DBN Mixing, DANN &amp; Randomization (DANN-R): Simulated samples are generated with visual randomization and the model is trained with a domain-adversarial method with DBN; GraspGAN, TABLE II: Real grasp performance when no labeled real examples are available. Method names explained in the text.</p>
<table>
<thead>
<tr>
<th>Sim-Only</th>
<th>Rand.</th>
<th>GraspGAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>23.53%</td>
<td>35.95%</td>
<td>63.40%</td>
</tr>
</tbody>
</table>
<p>TABLE III: Success of grasping 36 diverse and unseen physical objects of all our methods trained on different amounts of real-world samples and 8 million simulated samples with procedural objects. Method names are explained in the text.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>All 9,402,875</th>
<th>20% 1,880,363</th>
<th>10% 939,777</th>
<th>2% 188,094</th>
<th>1% 93,841</th>
</tr>
</thead>
<tbody>
<tr>
<td>Real-Only</td>
<td>67.65%</td>
<td>64.93%</td>
<td>62.75%</td>
<td>35.46%</td>
<td>31.13%</td>
</tr>
<tr>
<td>Naïve Mix.</td>
<td>73.63%</td>
<td>69.61%</td>
<td>65.20%</td>
<td>58.38%</td>
<td>39.86%</td>
</tr>
<tr>
<td>Rand.</td>
<td>75.58%</td>
<td>70.16%</td>
<td>73.31%</td>
<td>63.61%</td>
<td>50.99%</td>
</tr>
<tr>
<td>DANN</td>
<td>76.26%</td>
<td>68.12%</td>
<td>71.93%</td>
<td>61.93%</td>
<td>59.27%</td>
</tr>
<tr>
<td>DANN-R.</td>
<td>72.60%</td>
<td>66.46%</td>
<td>74.88%</td>
<td>63.73%</td>
<td>43.81%</td>
</tr>
<tr>
<td>GraspGAN</td>
<td>76.67%</td>
<td>74.07%</td>
<td>70.70%</td>
<td>68.51%</td>
<td>59.95%</td>
</tr>
</tbody>
</table>
<p>DBN Mixing &amp; DANN (GraspGAN): The non-randomized simulated data is first refined with a GraspGAN generator, and the refined data is used to train a DANN with DBN mixing. The generator is trained with the same real dataset size used to train the DANN. See Figure 1b for examples.</p>
<p>Table III shows that using visual randomization with DBN mixing improved upon naïve mixing with no randomization experiments across the board. The effect of visual, dynamics, and combined randomization for both procedural and ShapeNet objects was evaluated by using 10% of the real data available. Table I shows that using only visual randomization slightly improved grasp performance for procedural objects, but the differences were generally not conclusive.</p>
<p>In terms of domain adaptation techniques, our proposed hybrid approach of combining our GraspGAN and DANN performs the best in most cases, and shows the most gains in the lower real-data regimes. Using DANNs with DBN Mixing performed better than naïve mixing in most cases. However the effect of DANNs on Randomized data was not conclusive, as the equivalent models produced worse results in 3 out of 5 cases. We believe the most interesting results however, are the ones from our experiments with no labeled real data. We compared the best domain adaptation method (GraspGAN), against a model trained on simulated data with and without randomization. We trained a GraspGAN on all 9 million real samples, without using their labels. Our grasping model was then trained only on data refined by G. Results in Table II show that the unsupervised adaptation model outperformed not only sim-only models with and without randomization but also a real-only model with 939,777 labeled real samples.</p>
<p>Although our absolute grasp success numbers are consistent with the ones reported in [6], some previous grasping work reports higher absolute grasp success. However, we note the following: (a) our goal in this work is not to show that we can train the best possible grasping system, but that for the same amount of real-world data, the inclusion of synthetic data can be helpful; we have relied on previous work [6] for the grasping approach used; (b) our evaluation was conducted on a diverse and challenging</p>
<p>range of objects, including transparent bottles, small round objects, deformable objects, and clutter; and (c) the method uses only monocular RGB images from an over-the-shoulder viewpoint, without depth or wrist-mounted cameras. These make our setup considerably harder than most standard ones.</p>
<h2>VI. CONCLUSION</h2>
<p>In this paper, we examined how simulated data can be incorporated into a learning-based grasping system to improve performance and reduce data requirements. We study grasping from over-the-shoulder monocular RGB images, a particularly challenging setting where depth information and analytic 3D models are not available. This presents a challenging setting for simulation-to-real-world transfer, since simulated RGB images typically differ much more from real ones compared to simulated depth images. We examine the effects of the nature of the objects in simulation, of randomization, and of domain adaptation. We also introduce a novel extension of pixel-level domain adaptation that makes it suitable for use with high-resolution images used in our grasping system. Our results indicate that including simulated data can drastically improve the vision-based grasping system we use, achieving comparable or better performance with 50 times fewer real-world samples. Our results also suggest that it is not as important to use realistic 3D models for simulated training. Finally, our experiments indicate that our method can provide plausible transformations of synthetic images, and that including domain adaptation substantially improves performance in most cases.</p>
<p>Although our work demonstrates very large improvements in the grasp success rate when training on smaller amounts of real world data, there are a number of limitations. Both of the adaptation methods we consider focus on invariance, either transforming simulated images to look like real images, or regularizing features to be invariant across domains. These features incorporate both appearance and action, due to the structure of our network, but no explicit reasoning about physical discrepancies between the simulation and the real world is done. We did consider randomization of dynamics properties, and show it is indeed important. Several recent works have looked at adapting to physical discrepancies explicitly [38], [39], [40], and incorporating these ideas into grasping is an exciting avenue for future work. Our approach for simulation to real world transfer only considers monocular RGB images, though extending this method to stereo and depth images would be straightforward. Finally, the success rate reported in our experiments still has room for improvement, and we expect further research in this area will lead to even better results. The key insight from our work comes from the comparison of the different methods: we are not aiming to propose a novel grasping system, but rather to study how incorporating simulated data can improve an existing one.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors thank John-Michael Burke for overseeing the robot operations. The authors also thank Erwin Coumans,</p>
<p>Ethan Holly, Dmitry Kalashnikov, Deirdre Quillen, and Ian Wilkes for contributions to the development of our grasping system and supporting infrastructure.</p>
<h2>REFERENCES</h2>
<p>[1] B. Siciliano and O. Khatib, Springer Handbook of Robotics. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2007.
[2] J. Bohg, A. Morales, T. Asfour, and D. Kragic, "Data-driven grasp synthesis-a survey," IEEE Transactions on Robotics, 2014.
[3] D. Kappler, J. Bohg, and S. Schaal, "Leveraging Big Data for Grasp Planning," in ICRA, 2015.
[4] U. Viereck, A. t. Pas, K. Saenko, and R. Platt, "Learning a visuomotor controller for real world robotic grasping using easily simulated depth images," arxiv:1706.04652, 2017.
[5] P. L. and A. Gupta, "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours," in ICRA, 2016.
[6] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection," IJRR, 2016.
[7] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics," in RSS, 2017.
[8] I. Lenz, H. Lee, and A. Saxena, "Deep Learning for Detecting Robotic Grasps," IJRR, 2015.
[9] A. Bicchi, "On the Closure Properties of Robotic Grasping," IJRR, 1995.
[10] A. Rodriguez, M. T. Mason, and S. Ferry, "From Caging to Grasping," IJRR, 2012.
[11] A. Saxena, J. Driemeyer, and A. Y. Ng, "Robotic Grasping of Novel Objects using Vision," IJRR, 2008.
[12] M. Gualtieri, A. ten Pas, K. Saenko, and R. Platt, "High precision grasp pose detection in dense clutter," in IROS, 2016, pp. 598-605.
[13] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World," arxiv:1703.06907, 2017.
[14] S. James, A. J. Davison, and E. Johns, "Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task," arxiv:1707.02267, 2017.
[15] F. Sadeghi and S. Levine, "CAD2RL: Real single-image flight without a single real image." arxiv:1611.04201, 2016.
[16] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, "Visual domain adaptation: A survey of recent advances," IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 53-69, 2015.
[17] G. Csurka, "Domain adaptation for visual applications: A comprehensive survey," arxiv:1702.05374, 2017.
[18] B. Sun, J. Feng, and K. Saenko, "Return of frustratingly easy domain adaptation," in AAAI, 2016.
[19] B. Gong, Y. Shi, F. Sha, and K. Grauman, "Geodesic flow kernel for unsupervised domain adaptation," in CVPR, 2012.
[20] R. Caseiro, J. F. Henriques, P. Martins, and J. Batista, "Beyond the shortest path: Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow," in CVPR, 2015.
[21] R. Gopalan, R. Li, and R. Chellappa, "Domain Adaptation for Object Recognition: An Unsupervised Approach," in ICCV, 2011.
[22] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, "Domain-adversarial training of neural networks," JMLR, 2016.
[23] M. Long and J. Wang, "Learning transferable features with deep adaptation networks," ICML, 2015.
[24] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, "Domain separation networks," in NIPS, 2016.
[25] Y. Taigman, A. Polyak, and L. Wolf, "Unsupervised cross-domain image generation," in ICLR, 2017.
[26] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, "Unsupervised pixel-level domain adaptation with generative adversarial neural networks," in CVPR, 2017.
[27] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb, "Learning from simulated and unsupervised images through adversarial training," in CVPR, 2017.
[28] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired Image-toImage Translation using Cycle-Consistent Adversarial Networks," in ICCV, 2017.</p>
<p>[29] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in NIPS, 2014.
[30] E. Coumans and Y. Bai, "pybullet, a python module for physics simulation in robotics, games and machine learning," http://pybullet.org/, 2016-2017.
[31] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, "ShapeNet: An Information-Rich 3D Model Repository," CoRR, 2015.
[32] S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Network Training by Reducing Internal Covariate Shift," in ICML, 2015.
[33] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional Networks for Biomedical Image Segmentation," in MICCAI, 2015.
[34] D. Ulyanov, A. Vedaldi, and V. Lempitsky, "Instance normalization: The missing ingredient for fast stylization," arsiv:1607.08022, 2016.
[35] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, "Image-to-Image Transla-
tion with Conditional Adversarial Networks," arsiv:1611.07004, 2016.
[36] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley, "Least Squares Generative Adversarial Networks," arsiv:1611.04076, 2016.
[37] J. Johnson, A. Alahi, and L. Fei-Fei, "Perceptual Losses for Real-Time Style Transfer and Super-Resolution," in ECCV. Springer, 2016.
[38] P. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba, "Transfer from simulation to real world through learning deep inverse dynamics model," arXiv:1610.03518, 2016.
[39] A. Rajeswaran, S. Ghotra, S. Levine, and B. Ravindran, "Epopt: Learning robust neural network policies using model ensembles," arsiv:1610.01283, 2016.
[40] W. Yu, C. K. Liu, and G. Turk, "Preparing for the unknown: Learning a universal policy with online system identification," arXiv:1702.02453, 2017.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Authors contributed equally, ${ }^{1}$ Google Brain, ${ }^{2} \mathrm{X}$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
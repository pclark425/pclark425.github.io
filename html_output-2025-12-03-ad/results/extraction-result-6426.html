<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272753096</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12294v1.pdf" target="_blank">RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties. Recent LLM-based decision-making methods (also referred to as LLM-based agents), when paired with appropriate critics, have demonstrated potential in solving complex, long-horizon tasks with relatively few interactions. However, most existing LLM-based agents lack the ability to retain and learn from past interactions - an essential trait of learning-based robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based agents with a memory of past interactions and incorporates critics to evaluate the agents' decisions. The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decision-making. Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld domains, we demonstrate significant improvements in task success rates and efficiency, showing that the proposed RAG-Modulo framework outperforms state-of-the-art baselines.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent framework that augments a language model with an interaction-level memory (database of past interactions) and a bank of critics; at each decision step it retrieves relevant past interactions as in‑context examples (RAG) via embedding similarity to guide action generation, and updates memory with successful interactions to enable learning over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-Modulo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM (GPT-4O in experiments) produces high-level actions; a bank of critics (syntax, semantics, low-level execution) checks feasibility and returns feedback; successful interaction tuples (I = (goal, previous_action, feasibility_feedback, observation), action) are stored in an external interaction memory as text + embeddings; at each step the top‑K most similar interaction tuples are retrieved by cosine similarity and included as in‑context examples to the LLM to bias action generation. Memory is appended online after successful steps and after task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not disclosed (GPT-4O used; closed-source model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented generation (external interaction memory / vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Interaction-level tuples (I, c) where I = (goal g, previous action c_{t-1}, feasibility feedback f, observation o) and associated action c; also stored as text and 3072-d embeddings computed with OpenAI TEXT-EMBEDDING-3-LARGE.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Nearest-neighbor similarity search (cosine similarity) over fixed embeddings (TEXT-EMBEDDING-3-LARGE); top-K interactions retrieved and concatenated as in-context examples. Memory is written by appending successful interaction tuples (f == SUCCESS) after execution and upon task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BabyAI (Synth, BossLevel) and AlfWorld (Seen, Unseen) benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-horizon sequential decision-making / embodied planning (robotic tasks in partially observable environments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BabyAI-Synth: Success Rate 0.48 ± 0.10; In-Executability 5.18 ± 1.18; Avg. Episode Length 14.82 ± 2.14. BabyAI-BossLevel: SR 0.57 ± 0.10; InExec 3.74 ± 0.78; Len 12.48 ± 1.49. AlfWorld-Seen: SR 0.52 ± 0.08; InExec 5.36 ± 1.39; Len 20.54 ± 1.71. AlfWorld-Unseen: SR 0.54 ± 0.09; InExec 7.17 ± 1.73; Len 19.64 ± 1.75.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation (without memory) on BabyAI-Synth: SR 0.43 ± 0.10; InExec 6.72 ± 1.13; Len 16.23 ± 2.03. BabyAI-BossLevel: SR 0.37 ± 0.09; InExec 6.07 ± 0.99; Len 14.48 ± 1.47. (Paper reports a significant drop in SR — e.g., −0.20 on BabyAI-BossLevel — and longer episodes when memory is removed.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Average In-Executability (InExec), Average Episode Length (Len)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Retrieval size K trades off additional useful context vs. distracting/noisy context: SR improves up to an optimal K (5–10 depending on level) then declines as irrelevant interactions degrade LLM output. Larger K also initially reduces InExec and Len but too-large context increases in-executability and episode length. Memory reduces average episode length (hence API cost) but larger retrieved context increases prompt size which may affect latency/cost (discussed qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance sensitive to the number K of retrieved interactions — too many examples distract the LLM and degrade performance; retrieval of irrelevant or low-quality interactions harms decisions. RAG-Modulo stores only interaction-level successful tuples (with rectifications included) and relies on a seeded prior experience for best performance; starting from empty memory is slower to learn though still better than no-memory variant. Experiments are in simulated benchmarks; physical-robot deployment not evaluated here. Interaction-level retrieval outperformed trajectory-level retrieval, indicating retrieval granularity matters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Abhinav Jain, Chris Jermaine, Vaibhav Unhelkar (2024). RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models. (Submitted to IEEE; arXiv:2409.12294v1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-ended embodied agent (from prior work) that leverages a persistent memory/experience store to support continual learning and task decomposition in embodied environments; mentioned as a related continual-learning framework to integrate with RAG-Modulo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as an example of a continual learning / open-ended embodied agent that could be integrated with RAG-Modulo to enable learning at multiple abstraction layers (skills and interactions). The paper does not detail Voyager's memory implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Embodied open-ended learning / continual learning (as discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, A. Anandkumar (2023). Voyager: An open-ended embodied agent with large language models. arXiv:2305.16291.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expel: LLM agents are experiential learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent work (cited) that frames LLM agents as experiential learners that store and use past experiences; cited in the paper as an example of LLM-based approaches that introduce memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expel: Llm agents are experiential learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Expel</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as an example of an LLM-based agent that utilizes experience/memory to improve over time; the current paper references it in related work but does not provide implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>A. Zhao, D. Huang, Q. Xu, M. Lin, Y.-J. Liu, G. Huang (2024). Expel: Llm agents are experiential learners. (cited in this paper's references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited agent architecture that uses self-reflection / verbal reinforcement to improve agent policies over interactions; referenced as related work on LLM agents that incorporate experience-based learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as an example of a language-agent approach that uses internal reflection/experience to improve; no implementation specifics or evaluations from that paper are described here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, S. Yao (2024). Reflexion: Language agents with verbal reinforcement learning. (cited in this paper's references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas: Fewshot learning with retrieval augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM approach for few-shot learning cited in the paper; referenced as an example of retrieval-augmented methods relevant to RAG-Modulo's retrieval architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Atlas: Fewshot learning with retrieval augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as a retrieval-augmented LLM method that demonstrates effectiveness of retrieval for few-shot tasks; the current paper references it in the RAG-related literature but does not use Atlas directly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented generation (general RAG concept)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity-based retrieval (general RAG principle mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval-augmented few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave (2023). Atlas: Fewshot learning with retrieval augmented language models. Journal of Machine Learning Research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Atlas: Fewshot learning with retrieval augmented language models <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented embodied agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6426",
    "paper_id": "paper-272753096",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "RAG-Modulo",
            "name_full": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
            "brief_description": "An LLM-based agent framework that augments a language model with an interaction-level memory (database of past interactions) and a bank of critics; at each decision step it retrieves relevant past interactions as in‑context examples (RAG) via embedding similarity to guide action generation, and updates memory with successful interactions to enable learning over time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG-Modulo",
            "agent_description": "An LLM (GPT-4O in experiments) produces high-level actions; a bank of critics (syntax, semantics, low-level execution) checks feasibility and returns feedback; successful interaction tuples (I = (goal, previous_action, feasibility_feedback, observation), action) are stored in an external interaction memory as text + embeddings; at each step the top‑K most similar interaction tuples are retrieved by cosine similarity and included as in‑context examples to the LLM to bias action generation. Memory is appended online after successful steps and after task completion.",
            "model_size": "not disclosed (GPT-4O used; closed-source model)",
            "memory_used": true,
            "memory_type": "retrieval-augmented generation (external interaction memory / vector store)",
            "memory_representation": "Interaction-level tuples (I, c) where I = (goal g, previous action c_{t-1}, feasibility feedback f, observation o) and associated action c; also stored as text and 3072-d embeddings computed with OpenAI TEXT-EMBEDDING-3-LARGE.",
            "memory_access_mechanism": "Nearest-neighbor similarity search (cosine similarity) over fixed embeddings (TEXT-EMBEDDING-3-LARGE); top-K interactions retrieved and concatenated as in-context examples. Memory is written by appending successful interaction tuples (f == SUCCESS) after execution and upon task completion.",
            "task_name": "BabyAI (Synth, BossLevel) and AlfWorld (Seen, Unseen) benchmarks",
            "task_category": "Long-horizon sequential decision-making / embodied planning (robotic tasks in partially observable environments)",
            "performance_with_memory": "BabyAI-Synth: Success Rate 0.48 ± 0.10; In-Executability 5.18 ± 1.18; Avg. Episode Length 14.82 ± 2.14. BabyAI-BossLevel: SR 0.57 ± 0.10; InExec 3.74 ± 0.78; Len 12.48 ± 1.49. AlfWorld-Seen: SR 0.52 ± 0.08; InExec 5.36 ± 1.39; Len 20.54 ± 1.71. AlfWorld-Unseen: SR 0.54 ± 0.09; InExec 7.17 ± 1.73; Len 19.64 ± 1.75.",
            "performance_without_memory": "Ablation (without memory) on BabyAI-Synth: SR 0.43 ± 0.10; InExec 6.72 ± 1.13; Len 16.23 ± 2.03. BabyAI-BossLevel: SR 0.37 ± 0.09; InExec 6.07 ± 0.99; Len 14.48 ± 1.47. (Paper reports a significant drop in SR — e.g., −0.20 on BabyAI-BossLevel — and longer episodes when memory is removed.)",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (SR), Average In-Executability (InExec), Average Episode Length (Len)",
            "tradeoffs_reported": "Retrieval size K trades off additional useful context vs. distracting/noisy context: SR improves up to an optimal K (5–10 depending on level) then declines as irrelevant interactions degrade LLM output. Larger K also initially reduces InExec and Len but too-large context increases in-executability and episode length. Memory reduces average episode length (hence API cost) but larger retrieved context increases prompt size which may affect latency/cost (discussed qualitatively).",
            "limitations_or_failure_cases": "Performance sensitive to the number K of retrieved interactions — too many examples distract the LLM and degrade performance; retrieval of irrelevant or low-quality interactions harms decisions. RAG-Modulo stores only interaction-level successful tuples (with rectifications included) and relies on a seeded prior experience for best performance; starting from empty memory is slower to learn though still better than no-memory variant. Experiments are in simulated benchmarks; physical-robot deployment not evaluated here. Interaction-level retrieval outperformed trajectory-level retrieval, indicating retrieval granularity matters.",
            "citation": "Abhinav Jain, Chris Jermaine, Vaibhav Unhelkar (2024). RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models. (Submitted to IEEE; arXiv:2409.12294v1).",
            "uuid": "e6426.0",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An open-ended embodied agent (from prior work) that leverages a persistent memory/experience store to support continual learning and task decomposition in embodied environments; mentioned as a related continual-learning framework to integrate with RAG-Modulo.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "Mentioned as an example of a continual learning / open-ended embodied agent that could be integrated with RAG-Modulo to enable learning at multiple abstraction layers (skills and interactions). The paper does not detail Voyager's memory implementation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": "Embodied open-ended learning / continual learning (as discussed)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, A. Anandkumar (2023). Voyager: An open-ended embodied agent with large language models. arXiv:2305.16291.",
            "uuid": "e6426.1",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Expel",
            "name_full": "Expel: LLM agents are experiential learners",
            "brief_description": "A recent work (cited) that frames LLM agents as experiential learners that store and use past experiences; cited in the paper as an example of LLM-based approaches that introduce memory modules.",
            "citation_title": "Expel: Llm agents are experiential learners",
            "mention_or_use": "mention",
            "agent_name": "Expel",
            "agent_description": "Cited as an example of an LLM-based agent that utilizes experience/memory to improve over time; the current paper references it in related work but does not provide implementation details.",
            "model_size": null,
            "memory_used": true,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "A. Zhao, D. Huang, Q. Xu, M. Lin, Y.-J. Liu, G. Huang (2024). Expel: Llm agents are experiential learners. (cited in this paper's references).",
            "uuid": "e6426.2",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A cited agent architecture that uses self-reflection / verbal reinforcement to improve agent policies over interactions; referenced as related work on LLM agents that incorporate experience-based learning.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Mentioned in related work as an example of a language-agent approach that uses internal reflection/experience to improve; no implementation specifics or evaluations from that paper are described here.",
            "model_size": null,
            "memory_used": true,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, S. Yao (2024). Reflexion: Language agents with verbal reinforcement learning. (cited in this paper's references).",
            "uuid": "e6426.3",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Atlas",
            "name_full": "Atlas: Fewshot learning with retrieval augmented language models",
            "brief_description": "A retrieval-augmented LLM approach for few-shot learning cited in the paper; referenced as an example of retrieval-augmented methods relevant to RAG-Modulo's retrieval architecture.",
            "citation_title": "Atlas: Fewshot learning with retrieval augmented language models",
            "mention_or_use": "mention",
            "agent_name": "Atlas",
            "agent_description": "Cited as a retrieval-augmented LLM method that demonstrates effectiveness of retrieval for few-shot tasks; the current paper references it in the RAG-related literature but does not use Atlas directly.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented generation (general RAG concept)",
            "memory_representation": null,
            "memory_access_mechanism": "similarity-based retrieval (general RAG principle mentioned)",
            "task_name": null,
            "task_category": "retrieval-augmented few-shot learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave (2023). Atlas: Fewshot learning with retrieval augmented language models. Journal of Machine Learning Research.",
            "uuid": "e6426.4",
            "source_info": {
                "paper_title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Atlas: Fewshot learning with retrieval augmented language models",
            "rating": 1,
            "sanitized_title": "atlas_fewshot_learning_with_retrieval_augmented_language_models"
        },
        {
            "paper_title": "Retrieval-augmented embodied agents",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_embodied_agents"
        }
    ],
    "cost": 0.01453525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models
18 Sep 2024</p>
<p>Abhinav Jain abhinav.jain@rice.edu 
Department of Computer Science
Rice University
HoustonTX</p>
<p>Chris Jermaine 
Department of Computer Science
Rice University
HoustonTX</p>
<p>Vaibhav Unhelkar 
Department of Computer Science
Rice University
HoustonTX</p>
<p>RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models
18 Sep 2024016A4CC63427B150D32B7460A1205524arXiv:2409.12294v1[cs.AI]This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties.Recent LLM-based decision-making methods (also referred to as LLM-based agents), when paired with appropriate critics, have demonstrated potential in solving complex, long-horizon tasks with relatively few interactions.However, most existing LLM-based agents lack the ability to retain and learn from past interactions-an essential trait of learning-based robotic systems.We propose RAG-Modulo, a framework that enhances LLM-based agents with a memory of past interactions and incorporates critics to evaluate the agents' decisions.The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decisionmaking.Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning.Through experiments in the challenging BabyAI and AlfWorld domains, we demonstrate significant improvements in task success rates and efficiency, showing that the proposed RAG-Modulo framework outperforms state-of-the-art baselines.</p>
<p>I. INTRODUCTION</p>
<p>Solving goal-driven sequential tasks is a core problem in robotics, with a wide array of challenges [1], [2], [3], [4], [5], [6].Due to imperfect actuation, real-world robots operate in stochastic environments.Their sensors often provide only a partial view of the surroundings, requiring decision-making under partial observability and limited knowledge of the world model.To reduce the programming burden for end-users, even complex, long-horizon tasks are frequently defined by sparse reward functions or natural language descriptions of the robot's goal.</p>
<p>Various paradigms and corresponding methods have been explored to address this fundamental challenge [7], [8], [9], [10], [11].The planning paradigm assumes access to a task model, which is often unavailable in real-world applications.While reinforcement learning can operate without a task model, it typically requires a prohibitively large number of exploratory interactions and significant manual effort for reward design.This challenge is further compounded in partially observable environments, where sparse rewards and safety concerns limit the feasibility of extensive exploration.</p>
<p>To complement these long-standing paradigms, language models have recently emerged as promising tools for solving Fig. 1.The RAG-Modulo framework incorporates a language model to generate candidate actions and a set of critics to evaluate them.Importantly, it features mechanisms for storing and retrieving past interactions, which enable learning from experience and improve decision-making over time.</p>
<p>long-horizon tasks in robotics [12], [13], [14], [15], [16], [17], [18], [19].They can approximate world knowledge [20], [21], [22] and use few-shot reasoning to decompose high-level tasks into mid-level plans [23], [24], [25].Additionally, they can function as dynamic planners, adjusting their strategies based on environmental feedback, which is especially useful in partially observable settings [17].Moreover, their performance is shown to improve when integrated with formal systems that evaluate decisions based on criteria such as correctness, executability, and user preferences [26].</p>
<p>Despite their promise, most existing LLM-based decisionmaking methods (also referred to as LLM-based agents) lack the ability to learn from experience.To effectively solve complex, long-horizon tasks, a robotic agent must demonstrate the ability to learn: meaning it should improve its performance over time as it gains more experience in its environment.A prevalent approach to realize such "learning" for LLM-based robotic agents is to tune prompts using in-context examples [27], but this method is constrained by the selection of examples, requires domain knowledge, and demands manual effort.Another option is to fine-tune language models based on past interactions [15], [28], but this approach can be computationally expensive and resource intensive.To address these gaps, we propose RAG-Modulo: a framework which augments a language model with a memory that stores past interactions, retrieving relevant experience at each step of the task to guide robot decision-making.</p>
<p>As shown in Figs. 1 and 2, RAG-Modulo extends the LLM-Modulo framework [26] with memory, where formal verifiers or critics evaluate the feasibility of actions at each step based on criteria like syntax, semantics, and executability.</p>
<p>The interactions, along with feasibility feedback, are stored in memory and retrieved as in-context examples, enabling automatic prompt tuning for future tasks.By leveraging these past interactions, the agent can generalize from its experiences, avoid repeated mistakes, and make more accurate decisions-much like how humans learn from their past errors.In summary, building on the insight of memoryaugmented behavior generation, this paper makes three key contributions:</p>
<p>• RAG-Modulo: A framework with LLM-based agents that learns not through back-propagation, but by building up a database of experiences (Interaction Memory) that it then accesses.</p>
<p>II. PROBLEM FORMULATION</p>
<p>In this section, we formally model the tasks of interest and define the problem, followed by an explanation of how language models can be prompted to function as agents.</p>
<p>A. Task Model</p>
<p>We focus on object-centric, goal-driven sequential robotic tasks that may involve uncertainties in both actions and observations [29].More specifically, we denote S o as the set of all possible objects in the robot's environment and S p as the set of object properties.We formally define the task model with the tuple (S, G, A, O, T, R g , h, γ).Given S o and S p , a state s ∈ S is defined as an assignment of object properties.A is the set of low-level physical actions and G is the set of all goals.A goal g ∈ G is the natural language description of the goal state.O is the set of observations retrieved from states via an observation function O : (S × A) → O, and T : (S × A) → S is the transition function.R g is the goal-conditioned reward function, which = 1 when goal is achieved, else 0. Finally, γ denotes the discount factor and h represents the task horizon.</p>
<p>Following prior work [2], [16], the agent is also equipped with a set of high-level text actions, denoted by C. In reinforcement learning (RL) literature, these can be interpreted as macro actions or options [30], [31].Each action c ∈ C is composed of a function and its corresponding set of arguments, i.e., c = FUNCTION(ARGUMENT), such as OPEN(TYPE.DOOR, COLOR.RED).We assume that the robot can execute this high-level action by breaking it down into a sequence of primitive actions (a 1 , a 2 , . ..), governed by its low-level policy π c , until a termination condition β c is met.For the remainder of the paper, we simply refer to high-level actions as actions.</p>
<p>B. Problem Statement</p>
<p>We can now formally define the problem statement.Given the initial state, s 0 , generate the shortest sequence of actions (c 1 , c 2 , . . ., c t ) to reach the goal state described as g.</p>
<p>C. Language Models as Agents</p>
<p>As shown in recent works [16], [17], large language models (LLM ) can be prompted at each time step to generate a sequence of actions using the following prompt:
PROMPT t = p env ; {g k , o k , c k } K k=1 ; g; {o 1:t−1 ,</p>
<p>III. RELATED WORK</p>
<p>In this section, we discuss related methods that utilize language models as agents, use memory components and incorporate retrieval-augmented generation (RAG).</p>
<p>Language Models as Agents.Recent works have explored using language models as agents for solving long-horizon tasks by generating plans [14], [16], [17], [19], [18].Approaches like ProgPrompt [16], [32] generate static plans offline, which may fail when encountering unforeseen object interactions in a partially observable environment.LLM-Planner-like approaches [17], [19], [18] offer a more online approach, allowing for plan updates if an action fails, but it does not store past successes and failures to guide future decisions.The method in [19] involves a human in the loop to prompt and verify.[18] generates feasible plans but relies on precise model dynamics estimation to assess plan feasibility.More recently, [26] have shown that language models should be coupled with verifiers or critics to generate sound plans.These recent methods have informed our work; however, in contrast to these works, RAG-Modulo stores and retrieves past interactions from memory to inform and improve decision-making.</p>
<p>Learning with Experience.Reinforcement learning agents typically use a replay buffer to store experiences for policy optimization.However, solving complex long-horizon tasks often demands millions of trajectories or environment interactions to learn effectively [1].In contrast, our approach requires only a few hundred experiences to enable meaningful learning.Very recently, some LLM-based approaches have introduced memory modules that store past experiences and expand as the agent interacts with the environment [33], [34], [35], [36], [37].These methods store experiences at the skill level, retrieving them when needed, but lack the ability to track past successes and failures at the interaction level.Moreover, they often require multiple LLMs to reason, relabel and abstract primitive skills into more complex composite ones.</p>
<p>In contrast, the proposed RAG-Modulo stores experiences at the interaction level, removing the need for LLM-guided relabeling, and retrieves these experiences at every decisionmaking step to offer more informed guidance to the language model.Importantly, our work is complementary to these methods, as they tackle different aspects of continual learning -one focuses on learning library of skills, while the other emphasizes learning from past mistakes and successes.</p>
<p>RAG systems for Robotics.Retrieval Augmented Generation (RAG) systems enhance language model predictions by retrieving relevant information from external databases [38], [39].For example, [40] employs RAG to collect exemplars for solving sub-tasks with web agents, while [41] retrieves driving experiences from a database for autonomous vehicle planning.In robotics, [42] explores retrieval for deep RL agents, but it does not use LLMs, limiting its adaptability and scalability.[43] employs a policy retriever to extract robotic policies from a large-scale policy memory.In contrast, our approach integrates a RAG system within an LLM-Modulo framework, where past interactions and feedback from critics is stored and continuously expanded.This enables the retrieval of interaction-level experiences, including mistakes and corrections, providing more detailed and contextaware guidance for sequential decision-making.</p>
<p>IV. PROPOSED APPROACH</p>
<p>We now describe RAG-Modulo, summarized in Alg. 1, which is composed of an LLM, a bank of critics, and an (I k , c k ) ← Retrieve interactions from memory (Eq.2) 7:</p>
<p>PROMPT t ← Construct the prompt (Eq. 1) 8:
c t ← LLM (PROMPT t )
▷ Predict action 9:
f t ← CHECKFEASIBILITY (o t , c t ) 10: if f t is SUCCESS then ▷ Keep track of interactions 11: M ← M ∪ {I t . = (g, c t−1 , f t−1 , o t ), c t } 12:
end if 13: end while 14: if g is satisfied then
15: M ← M ∪ M ▷ Update memory 16: end if 17: return (c 1:t , M)
interaction memory (M) coupled with mechanisms for storing and retrieving interaction experience.At each step t of a task specified by natural language goal g and horizon h, RAG-Modulo first retrieves interactions I from the memory that are relevant to the task and current observation o t , using them to guide the LLM's decision-making (line 6 in Alg. 1).The LLM selects action c t based on this context and receives feedback (lines 8 − 9) from a bank of critics (Alg.2).If feasible, the interaction is stored (lines 10 − 12).Once the
f t ← FAILURE(REASON) 11: return f t
goal is achieved, the interaction memory is updated for future retrieval (lines 14 − 16), enabling learning from experience.</p>
<p>A. Critics and Feedback</p>
<p>Informed by the [26], RAG-Modulo includes a bank of critics (φ syntax , φ semantics , φ low−level ) who provide feedback on actions selected by the LLM.F denotes the set of feedbacks described in natural language.The syntax parser φ syntax : C → F returns feedback based on syntactical correctness.It ensures that the LLM's response adheres to the grammar rules of the environment.The semantics parser φ semantics : (C × O) → F returns feedback based on semantic correctness.It verifies that the predicted action is meaningful and logically consistent with the current observation o, e.g., ensuring the agent has the correct key before opening a door.The low-level policy critic φ low−level :
(C × O) → F checks if c is executable from o.
It runs the execution using π c (o) until β c (o) is satisfied.For example, while traversing a path it can determine if an obstacle is encountered.As summarized in Alg. 2, each critic φ, either returns SUCCESS or FAILURE along with the corresponding REASON.This mimics how programmers receive feedback from compilers during debugging.We now formally define the overall feasibility feedback f ∈ F as a function of the feedback from all critics:
f = SUCCESS, if φ syntax ∧ φ semantics ∧ φ low−level</p>
<p>FAILURE(REASON), otherwise</p>
<p>Given the feedback, the prompt has the following structure:
PROMPT t =(1)p env ; {g k , c k −1 , f k , o k , c k } K k=1 ; g; (o 1:t−1 , c 1:t−1 , f 1:t−1 )
; o t where, in-context examples and history now include the previous action c −1 and its feasibility feedback.</p>
<p>B. Interaction Memory, Storage and Retrieval</p>
<p>RAG-Modulo considers a database of past interactions representing the agent's memory M of solving prior tasks and their outcomes.We represent such interaction with the tuple (I, c), where I = (g, c −1 , f, o).Formally, the memory includes a set of interactions M = {(I 1 , c 1 ), . . ., (I m , c m )}, where m represents the memory size.</p>
<p>Retrieval.At every decision-making step of a given task, RAG-Modulo retrieves from the memory the top-K most relevant interactions {I k , c k } K k=1 that resemble the current task and situation and uses them as in-context examples as shown in Fig. 2. Formally, this is represented as:
I 1:K = argmax K I∈M cos(e(I t ), e(I))(2)
argmax K returns the top-K samples from the memory that have the highest cosine similarity with I t .e(I) represents the fixed-size embedding of I generated by the encoder model e.As detailed in Sec.V, we use OpenAI's TEXT-EMBEDDING-3-LARGE [44] as the encoder model e for realizing RAG-Modulo in our experiments.</p>
<p>Storage.For every successfully completed task, {g, c 1:h , f 1:h , o 1:h , c 1:h )}, RAG-Modulo fills the memory with its interactions (I t , c t ) for which the current option c t is always feasible (i.e.f (c t ) = SUCCESS).Thus, every stored tuple is a successful interaction that includes rectifications when f t−1 = FAILURE, which can be used by the LLM when planning future actions.</p>
<p>V. EXPERIMENTAL SETUP</p>
<p>We evaluate the performance of RAG-Modulo in AlfWorld [2] and BabyAI [1], [15] benchmarks, depicted in Fig. 3.These benchmarks include several features representative of challenges in robot decision-making, thereby making them suitable benchmarks for evaluating RAG-Modulo.For instance, both benchmarks include a suite of sequential tasks that need to be performed by situated agents.The environments are partially observable to the agent, requiring the agent to explore, navigate and interact with objects to complete tasks described in natural language.Solving these tasks require reasoning over long-horizon in presence of a sparse reward signal, which is challenging for planning, RL and LLM-based decision-making algorithms [1], [2], [15].</p>
<p>Tasks.AlfWorld offers a diverse set of household tasks across various difficulty levels.We conduct experiments using the seen and UNSEEN validation sets, which include 140 and 132 task instances, respectively.The SEEN set is designed to measure in-distribution generalization, whereas the unseen set measures out-of-distribution generalization.BabyAI, a 2D grid world environment, features 40 levels of varying complexity.We focus on the Synth and BossLevel levels.The SYNTH level includes single-step instructions, such as "pick up a ball" or "go to the red key," while the BossLevel provides more complex, multi-step instructions, such as "put the yellow ball next to the purple ball, then open the purple door."Each level contains 100 evaluation task instances.</p>
<p>Prompt Design.We represent robot decisions as Python programs [16].Fig. 2 illustrates our prompt.The high-level actions are imported as Python functions.Each action is further defined with the types of arguments it requires.Finally, each argument type is defined as a class whose attributes represent the environment objects and their attributes.The interaction at each step is represented by key variables like feasibility feedback, visible objects and inventory.We task the LLM to predict the next action as the value of the variable action.</p>
<p>Language Model.We use GPT-4O [45] as the large language model LLM for generating actions and OpenAI's TEXT-EMBEDDING-3-LARGE [44] as the embedding model e for encoding instructions into 3072 dimensional vectors.Greedy decoding is applied with a maximum token limit of 200 for the LLM-Planner and 50 for the other approaches.The horizon (h) for high-level actions is set to 30 for AlfWorld, 20 for BabyAI-BossLevel, and 25 for BabyAI-Synth.</p>
<p>Baselines.We consider the following baselines for compar-ison, each using language models as high-level planners: (i) ProgPrompt [16] is a powerful static planner for robotic tasks that generates a complete plan at the start of a task and uses assertion checks to ground the plan to the current state.It is representative of LLM-based agents that do not involve memory or learning from experience.(ii) LLM-Planner [17] is a method that employs grounded replanning, dynamically updating the plan throughout the task.It is a representative approach of more recent LLMbased agents that also utilize retrieval-augmented generation; however, in a different manner than that of RAG-Modulo.For each environment, all baselines have access to 100 training tasks with expert-provided demonstrations.We initialize the memory in RAG-Modulo using these expert demonstrations.We refer to the initial memory as prior experience, which is updated online based on experience of solving new tasks.Metrics.To measure the decision-making performance, we consider three evaluation metrics.(i) Success Rate (SR) measures the fraction of tasks that the planner completed successfully.(ii) Average In-Executability (InExec) is the average number of selected actions that cannot be executed in the environment.(iii) Average Episode Length (Len) is the average number of planning actions that are required to complete a given task.As ProgPrompt is an offline approach, (InExec) and (Len) metrics are not applicable for it.</p>
<p>VI. RESULTS AND DISCUSSION</p>
<p>How does RAG-Modulo compare against other LLM as Agents baselines?In Table I, we report comparison with the baselines.RAG-Modulo demonstrates a higher success rate than ProgPrompt across both domains.This can be attributed to ProgPrompt's lack of memory and critics, which means it does not benefit from interactive learning.By interacting with the environment and retrieving relevant experience, RAG-Modulo enables more informed decision-making.</p>
<p>RAG-Modulo also outperforms LLM-Planner in terms of success rate, in-executability, and average episode length.Notably, the success rate improvements range from +0.33  What is the optimal number of interactions K to use as in-context examples?We ablate the number of interactions retrieved from memory and evaluate performance on BabyAI environments.The results reported in Fig. 4 show that the success rate improves as K increases, peaking at K = 5 for BossLevel and K = 10 for SynthLevel, before beginning to decline.Similarly, in Fig. 5 we observed that in-executability and average episode length decrease initially but start to rise as K continues to grow.The initial boost in performance can be attributed to the inclusion of more informative interactions, enhancing the LLM's decision-making capabilities.The subsequent decline likely stems from the LLM's sensitivity to irrelevant or noisy context [46], [47].As K increases, the chance of introducing less relevant or low-quality interactions also rises, which can distract the model and degrade its output quality [48].These trends suggest retrieving a modest number of interactions (between 5 and 10) while solving tasks using the RAG-Modulo framework.</p>
<p>How does the choice of retrieval function affect performance?We examine how retrieving interactions at different levels of granularity impacts performance.Specifically, we compare against an ablation of our approach that utilizes a trajectory-level retrieval function.This ablation first identifies the most relevant task in the memory by computing the cosine similarity between goals, and then extract top K interactions from that task's trajectory.We represent the performance of this variable in the second row of Table II.We observe that retrieving at the interaction level generally yields better results, with lower in-executability and shorter episode lengths, while maintaining similar or higher success rates across both BabyAI domains.This suggests that retrieving interactions from a diverse set of tasks provides the language model with richer information than simply retrieving interactions from the most relevant single task.</p>
<p>How does the presence of memory affect performance?To study the role of memory, we consider a variant of the proposed approach that does not include any interaction memory.This variant is representative of the LLM-Modulo framework [26], which includes interaction and critics but no mechanisms for storage or retrieval of experience.As reported in the last row of Table II, completely removing the memory component leads to a significant drop in performance, with a 0.20 decrease in success rate on BabyAI-BossLevel and an increase in average episode length by 1.4 to 2.0 steps.This demonstrates that storing and retrieving past interactions and feedback significantly improves the decision-making capabilities of the critic-aided language model.How does prior experience affect performance?Lastly, in the third row of Table II, we report results of RAG-Modulo when it is not seeded with any prior expert-generated experience.Unsurprisingly, we find that prior experience generally helps in sequential decision-making.Interestingly, even starting our approach with an empty memory (third row, Table II) still outperforms the variant that does not include a memory component (fourth row, Table II), as the agent can gradually collect experiences of successes and failures, allowing it to learn and improve its decision-making.</p>
<p>VII. CONCLUSION</p>
<p>This paper introduces RAG-Modulo, a framework for solving sequential decision-making tasks by providing LLM-based agents memory of past interactions.Extending the recent LLM-Modulo framework, RAG-Modulo not only incorporates critic feedback regarding the feasibility of generated actions but also enables agents to remember successes and mistakes and learn from them.RAG-Modulo demonstrates superior performance on the challenging BabyAI and AlfWorld benchmarks, achieving higher success rates while requiring fewer actions to complete sequential tasks.</p>
<p>In future work, we plan to utilize RAG-Modulo to solve tasks in other environments involving physical robots, such as FurnitureBench with the Panda robot [6].We also see potential in integrating RAG-Modulo with existing continual learning frameworks, such as BOSS and Voyager [33], [34], to enable learning from experience at multiple layers of abstractions: namely, skills and interactions.Another avenue is to explore tunable retrieval models that can anticipate future needs to further enhance the agent's performance [49], [50].Finally, we are interested in studying how RAG-Modulo can enhance end-user programming of complex robot behaviors by leveraging user commands, experience and critiques.</p>
<p>Fig. 2 .
2
Fig. 2. (Left) The prompt in RAG-Modulo consists of an environment descriptor, a history of past interactions, and in-context examples to guide the LLM in selecting a feasible action.Here, the agent can be carrying a blue key, which it needs to drop before picking up the green key.The retrieved in-context example shows a similar scenario where the agent is unable to drop an object in an occupied cell.Based on this, the agent generates an action to move to an empty cell before completing the task.(Right) Illustration of how each critic provides feedback for the infeasible action shown on top.</p>
<p>Algorithm 1
1
RAG-Modulo 1: INPUT: (g, h, LLM, M) 2: t ← 1 ▷ Initialize the time-step 3: M ← {} 4: while t ≤ h or (g is satisfied) do 5: o t ← Observe the environment 6:</p>
<p>Fig. 3 .
3
Fig. 3. (Left) AlfWorld Domain where the agent is shown in a household environment.(Right) Execution trace while solving a task from BabyAI.Ticks and Crosses show feasible and infeasible actions respectively.</p>
<p>Bold.Error bars are computed using bootstrapped sampling with 10k trials.The first row presents the results of the complete RAG-Modulo framework.The second row corresponds to a variant of RAG-Modulo with an alternate retrieval function, which retrieves the most similar task trajectory.The third row shows performance of RAG-Modulo when starting with no prior experience.The last row represents a variant that does not involve a memory component.hetuple(g k , c k −1 , f k , o k ).to +0.37 in more challenging environments like BabyAI-BossLevel and AlfWorld-Unseen.Additionally, RAG-Modulo has lower in-executability (approx.7 lower in Synth and 16 in Alfworld-Seen), and achieves shorter average episode lengths.While both systems are interactive and utilize retrievalaugmented generation, the key advantage of RAG-Modulo is the memory of past interactions that includes critics' feedback.By leveraging this memory, RAG-Modulo can avoid actions and accomplish tasks with fewer steps.Additionally, the lower episode length achieved by the RAG-Modulo facilitates a reduction in the overall cost of using LLMs, such as API expenses for closed-source models.</p>
<p>Fig. 4 .Fig. 5 .
45
Fig. 4. Success Rate as a function of K</p>
<p>Algorithm 2 CHECKFEASIBILITY 1: INPUT: o t , c t 2: f t ← SUCCESS, REASON ← NONE 3: try:
4:Parse c t using φ syntax▷ Syntax Critic5:Parse c t using φ semantics▷ Semantics Critic6:repeat7:Execute π ct▷ Low-level Policy Critic8:until β ct is True9: except Exception as REASON:10:</p>
<p>Table I .
I
Baseline comparison on BabyAI (top) and AlfWorld (bottom) environments.↑ denotes higher is better.↓ denotes lower is better.For a fair few-shot comparison, each approach uses 10 in-context examples in BabyAI-Synth and 5 in the remaining environments.The best performing approach is shown in Bold.Error bars are computed using bootstrapped sampling with 10k trials.(−) indicates the metric is not applicable for the approach.
ApproachBabyAI-SynthBabyAI-BossLevelSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓Expert0.960.798.340.980.438.41ProgPrompt0.24 ± 0.08−−0.11 ± 0.06−−LLM-Planner0.48 ± 0.112.02 ± 2.1714.72 ± 2.130.24 ± 0.08 13.98 ± 1.48 16.16 ± 1.35Ours0.48 ± 0.15.18± 1.1814.82 ± 2.140.57 ± 0.103.74 ± 0.7812.48 ± 1.49AlfWorld-SeenAlfWorld-UnseenSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓Expert0.820.1618.180.810.2818.47ProgPrompt0.09 ± 0.05−−0.08 ± 0.05−−LLM-Planner0.2 ± 0.0721.24 ± 1.76 25.65 ± 1.46 0.17 ± 0.0621.73 ± 1.726.36 ± 1.4Ours0.52 ± 0.085.36 ± 1.3920.54 ± 1.710.54 ± 0.097.17 ± 1.7319.64 ± 1.75</p>
<p>Table II .
II
RAG-Modulo 0.48 ± 0.1 5.18 ± 1.18 14.82 ± 2.14 0.57 ± 0.1 3.74 ± 0.78 12.48 ± 1.49 with trajectory-level retrieval 0.50 ±0.1 5.30 ± 0.93 15.42 ± 2.04 0.52 ± 0.1 4.22 ± 0.76 13.24 ± 1.43 without prior experience 0.44 ± 0.1 4.67 ± 0.88 15.92 ± 2.05 0.54 ± 0.1 4.68 ± 0.88 13.16 ± 1.48 without memory 0.43 ± 0.1 6.72 ± 1.13 16.23 ± 2.03 0.37 ± 0.09 6.07 ± 0.99 14.48 ± 1.47 Ablating different components of RAG-Modulo.↑ denotes higher is better.↓ denotes lower is better.The best performing approach is shown in
BabyAI-SynthBabyAI-BossLevelSR ↑InExec ↓Len ↓SR ↑InExec ↓Len ↓
This work was supported in part by the NSF and Rice University funds.
Babyai: A platform to study the sample efficiency of grounded language learning. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, arXiv:1810.082722018arXiv preprint</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202010749</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, arXiv:2009.122932020arXiv preprint</p>
<p>Vima: Robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, 2023</p>
<p>Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. M Heo, Y Lee, D Lee, J J Lim, Robotics: Science and Systems. 2023</p>
<p>A survey of robot learning from demonstration. B D Argall, S Chernova, M Veloso, B Browning, Robotics and autonomous systems. 5752009</p>
<p>Reinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 32112013</p>
<p>Recent advances in robot learning from demonstration. H Ravichandar, A S Polydoros, S Chernova, A Billard, robotics, and autonomous systems. 312020Annual review of control</p>
<p>Integrated task and motion planning. C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, robotics, and autonomous systems. 412021Annual review of control</p>
<p>Reinforcement learning in robotic applications: a comprehensive survey. B Singh, R Kumar, V P Singh, Artificial Intelligence Review. 5522022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022arXiv preprint</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International conference on machine learning. PMLR2022</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE202311530</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Text2motion: From natural language instructions to feasible plans. K Lin, C Agia, T Migimatsu, M Pavone, J Bohg, Autonomous Robots. 4782023</p>
<p>Chatgpt for robotics: Design principles and model abilities. S H Vemprala, R Bonatti, A Bucker, A Kapoor, IEEE Access. 2024</p>
<p>Language models as knowledge bases. F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.010662019arXiv preprint</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, arXiv:2002.089102020arXiv preprint</p>
<p>How can we know what language models know?. Z Jiang, F F Xu, J Araki, G Neubig, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>How to prompt your robot: A promptbook for manipulation skills with code as policies. M G Arenas, T Xiao, S Singh, V Jain, A Ren, Q Vuong, J Varley, A Herzog, I Leal, S Kirmani, 2024 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Plansformer: Generating symbolic plans using transformers. V Pallagani, B Muppasani, K Murugesan, F Rossi, L Horesh, B Srivastava, F Fabiano, A Loreggia, arXiv:2212.086812022arXiv preprint</p>
<p>Mimicgen: A data generation system for scalable robot learning using human demonstrations. A Mandlekar, S Nasiriany, B Wen, I Akinola, Y Narang, L Fan, Y Zhu, D Fox, Conference on Robot Learning. PMLR2023</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-21999</p>
<p>Probabilistic inference for determining options in reinforcement learning. C Daniel, H Van Hoof, J Peters, G Neumann, Machine Learning. 2016104</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. R Hazra, P Z Martires, L De Raedt, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438133</p>
<p>J Zhang, J Zhang, K Pertsch, Z Liu, X Ren, M Chang, S.-H Sun, J J Lim, arXiv:2310.10021Bootstrap your own skills: Learning to solve new tasks with large language model guidance. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Lifelong robot library learning: Bootstrapping composable and generalizable skills for embodied control with language models. G Tziafas, H Kasaei, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438642</p>
<p>Atlas: Fewshot learning with retrieval augmented language models. G Izacard, P Lewis, M Lomeli, L Hosseini, F Petroni, T Schick, J Dwivedi-Yu, A Joulin, S Riedel, E Grave, Journal of Machine Learning Research. 242512023</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International conference on machine learning. PMLR2022</p>
<p>Rada: Retrieval-augmented web agent planning with llms. M Kim, V Bursztyn, E Koh, S Guo, S.-W Hwang, Findings of the Association for Computational Linguistics ACL 2024. 202413525</p>
<p>Vistarag: Toward safe and trustworthy autonomous driving through retrieval-augmented generation. X Dai, C Guo, Y Tang, H Li, Y Wang, J Huang, Y Tian, X Xia, Y Lv, F.-Y Wang, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Retrievalaugmented reinforcement learning. A Goyal, A Friesen, A Banino, T Weber, N R Ke, A P Badia, A Guez, M Mirza, P C Humphreys, K Konyushova, International Conference on Machine Learning. PMLR2022</p>
<p>Retrieval-augmented embodied agents. Y Zhu, Z Ou, X Mou, J Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202417995</p>
<p>. Openai, 2024</p>
<p>Large language models can be easily distracted by irrelevant context. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, N Schärli, D Zhou, International Conference on Machine Learning. PMLR202331227</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Superposition prompting: Improving and accelerating retrieval-augmented generation. T Merth, Q Fu, M Rastegari, M Najibi, arXiv:2404.069102024arXiv preprint</p>
<p>Replug: Retrieval-augmented black-box language models. S Weijia, M Sewon, Y Michihiro, S Minjoon, J Rich, L Mike, Y Wen-Tau, ArXiv: 2301.126522023</p>
<p>Active retrieval augmented generation. Z Jiang, F F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, arXiv:2305.069832023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276725054</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.21321v1.pdf" target="_blank">LLM Post-Training: A Deep Dive into Reasoning Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> .</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (Iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-then-critique procedure in which an LLM generates an initial answer, produces a self-critique of its output, and uses that critique to produce a revised answer; iteration continues until a stopping condition is met.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references general large language models (various sizes) applying iterative self-feedback; no single model or parameter counts are run in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial response; have the model critique its own response (identify errors/inconsistencies); generate a revised response conditioned on that critique; repeat until convergence or threshold. Number of iterations not specified in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various reasoning tasks (math, code, QA) as cited</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The survey reports Self-Refine being applied to mathematical reasoning, code generation, and other multi-step reasoning tasks where intermediate correction is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey states Self-Refine has been shown to improve performance on tasks including mathematical reasoning and code generation, but does not report numeric results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this survey (no numeric baselines given here).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-critique and conditional generation (no external verifier required in the canonical formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Qualitative statement in survey: iterative self-critique and refinement methods improve correctness on math and code tasks (references to empirical papers are provided), but the survey does not provide within-paper quantitative values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes increased inference compute cost and sensitivity to the quality of self-critique; unreliable self-critique can fail to find errors and thus not improve outputs. More compute/time may be required and verification may still be necessary for critical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to single-pass generation and to verification+selection pipelines; iterative self-critique offers improvement without external verifiers but may be less robust than verifier-guided selection when self-evaluation is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Polish</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Polish (Progressive problem rephrasing and refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-and-refine approach where the model first reformulates or clarifies the problem statement to make it more solvable, then solves the refined problem, possibly iterating rephrasing and solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-polish: Enhance reasoning in large language models via problem refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level reference to models using problem refinement; no single-model experiments run in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Polish (problem refinement + solve)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>First transform or rephrase the user's problem to reduce ambiguity and complexity (self-polishing), then apply reasoning/solve stage; can iterate between polishing and solving to improve final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning tasks (math word problems, ambiguous queries)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where reformulation clarifies implicit assumptions or decomposes complex prompts into simpler subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports that Self-Polish improves solution accuracy qualitatively; no numeric performance is reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to ask the model to rephrase/clarify and then re-solve; internal iterative generation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites referenced work showing improved comprehension and solution accuracy after progressive problem refinement, but does not include numbers in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Adds inference-time latency; effectiveness depends on whether the model can accurately rephrase the problem without introducing its own errors or biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as complementary to chain-of-thought prompting and self-critique; more helpful when the original prompt is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8831.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time ensemble-like method that samples multiple chain-of-thought reasoning traces, marginalizes over them, and selects the most consistent final answer (majority vote) to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (sufficiently large to exhibit CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method applied to chain-of-thought capable LLMs (survey references large models that exhibit CoT behavior; exact sizes vary across cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (multiple CoT samples + marginalization)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample many distinct chain-of-thought traces via stochastic decoding; compute final answer for each trace; select the final answer that appears most frequently (or highest marginalized probability).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic and multi-step reasoning benchmarks (e.g., MATH, GSM8K as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math and reasoning tasks where diverse reasoning traces can converge to the correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey states Self-Consistency 'significantly improve[s] accuracy on arithmetic and reasoning problems' but does not give numeric values in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Test-time sampling and aggregation (majority vote) across multiple chain-of-thought samples; no retraining required.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited empirical results (from original Self-Consistency work) show higher accuracy versus single-sample CoT; the survey reiterates this qualitative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires multiple samples (increased compute); majority voting may fail if many chains converge on the same incorrect answer; needs sufficient diversity in sampled chains to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Often combined with CoT prompting and sampling-based strategies; compared favorably against single-pass CoT in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8831.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Critique / Self-Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Critique / Self-Evaluation (LLM judges its own output)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods where an LLM evaluates or scores its own outputs (e.g., identify errors, measure confidence), then uses that evaluation to accept/reject or revise answers; implemented via prompts or an LLM-as-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-evaluation improves selective generation in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey cites multiple works using models to self-evaluate; exact model sizes vary across referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Critique / Self-Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates an answer, then is prompted to evaluate (score/critique) that answer; the critique is used to either refine the answer or to decide whether to return it.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selective generation, verification, and alignment tasks (e.g., QA, summarization, safe-response selection)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from model-side quality checks to reduce errors or harmful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports that self-evaluation can improve selective generation and reliability (qualitative statements across citations); no numeric values reported directly in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported numerically in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting LLM to judge its own output; sometimes implemented with a separate LLM verifier or the same model in a judge role.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites empirical works showing better selectivity and improved answer quality when models self-evaluate; details reside in the cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-evaluations can be overconfident or unreliable; if the model's critiquing ability is poor, it may not detect mistakes and can reinforce incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to external verifiers and multi-model juries; self-evaluation is cheaper but potentially less reliable than strong external verifiers or human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8831.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selfrag</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfRAG (Learn to retrieve, generate, and critique through self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-reflective retrieval-augmented workflow where an LLM retrieves relevant context, generates an answer, and critiques or re-retrieves based on that critique to iteratively improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfrag: Learning to retrieve, generate, and critique through selfreflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-augmented LLM pipelines (unspecified LLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references retrieval-augmented systems that combine generation and self-critique loops; exact backbones vary in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SelfRAG (retrieve→generate→critique→re-retrieve)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative loop combining retrieval and self-critique: retrieve context, generate answer, critique output, then refine retrieval or generation based on critique.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive generation and QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where external knowledge retrieval helps answer correctness and iterative critique improves retrieval relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey states such methods can scale feedback loops and improve factuality; no per-method numeric results are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Hybrid: retrieval modules plus prompt-based self-critique; may use learned retrieval policies or iterative retrieval adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites works demonstrating improved factual accuracy and relevance when coupled with self-critique; specifics are in cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Risk of compounding retrieval errors if critique misleads retrieval; increases inference-time latency and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to static retrieval-augmented generation; iterative retrieval+critique yields better factuality but at higher compute and system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8831.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Satori (Chain-of-Action-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satori: Chain-of-Action-Thought / Reinforcement self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage approach combining format tuning (supervised trajectories with meta-action tokens) and a self-improvement RL phase where the model restarts from intermediate steps and refines reasoning using combined rewards including reflection bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used in Satori (survey does not list a single backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey reports Satori trains models in Stage 1 with synthetic trajectories and in Stage 2 via PPO with restart-and-explore strategies; exact model sizes are not specified in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Action-Thought (Satori)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Stage 1: format tuning on synthetic action-trace trajectories; Stage 2: RL with restart-and-explore, reflection bonuses and outcome/process reward models used to encourage improved multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning tasks requiring extended autoregressive search</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring long deliberation and the ability to restart/refine from mid-trajectory (e.g., complex planning or multi-step math/logic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey summarizes Satori's design and claims that restart/explore with reflection-style rewards improves reasoning; no numeric performance figures are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>RL-based self-improvement with restart-from-intermediate-step capability and reflection bonuses computed from rule-based checks, preference models, and other signals.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey reports the method incentivizes allocation of more compute to harder problems and enables extended reasoning, citing Satori; numerical evidence must be sought in the Satori paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires complex reward engineering; can be computationally costly; success depends on reward design and stability of RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Framed as addressing shortcomings of CoT by adding self-correction/restart capabilities; compared conceptually to standard CoT and RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8831.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier / PRM-guided tree search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-Reward-Model (PRM) guided Tree Search / Verifier loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of test-time methods that generate multiple candidate reasoning paths and use a verifier that scores intermediate (process) steps, pruning or selecting paths based on process-level feedback rather than only final outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rewarding progress: Scaling automated process verifiers for llm reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as proposers and verifiers (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey discusses proposers (LLM generating thoughts) and verifiers (reward models scoring steps) as separate components; specific model sizes vary across cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>PRM-guided tree search (verifier-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Proposer generates candidate reasoning steps; a Process Reward Model (PRM) scores intermediate steps to guide search (beam/toT/MCTS) and prune invalid branches; repeated until acceptable solution found.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning tasks (math, logic, planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems where intermediate step correctness improves final answer and facilitates credit assignment across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports that combining PRM-guided tree search with online distillation (integration) achieves 4× efficiency gains over baseline methods while maintaining 94% solution accuracy on the MATH dataset (this numerical claim appears in the survey's integration discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline methods (without PRM-guided tree search + online distillation) are implied to be ~4× less compute-efficient for equivalent MATH accuracy; the survey does not list absolute baseline accuracy numbers here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External verifier/reward model (process reward) evaluating intermediate reasoning steps; integrated into tree-search (ToT/GoT/MCTS) to prune and refine candidate chains.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey explicitly cites an integration result: PRM-guided tree search + online distillation produced a 4× efficiency improvement while maintaining 94% solution accuracy on MATH (numerical example provided in the survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Verifier quality critically limits gains; additional compute overhead at inference; risk of verifier-induced bias and reward hacking if verifier is imperfect; needs reliable uncertainty estimators per survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably (in survey) to plain CoT, self-consistency, and best-of-N sampling in complex tasks because PRM allows pruning of invalid intermediate steps; however it costs more compute than single-pass methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8831.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency + Verifier Integration (general test-time scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-time iterative refinement strategies (Self-Consistency, Best-of-N, BoN with verifiers, Compute-Optimal Scaling (COS))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of test-time iterative methods where multiple candidate outputs are generated (parallel or sequential) and then refined/selected using majority voting, reward models, or adaptive compute allocation (COS) to improve final answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM Post-Training: A Deep Dive into Reasoning Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (various sizes cited across the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level discussion of test-time scaling methods applied to LLMs of many sizes; no single experimental backbone is used in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency / Best-of-N / Compute-Optimal Scaling (COS)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple candidate reasoning traces/answers (via sampling or beam search), then (a) aggregate via majority (Self-Consistency), (b) select highest-scoring by a reward model (Best-of-N), or (c) adaptively allocate compute per example difficulty (COS) combining sequential refinement for easy prompts and parallel exploration for hard prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH and other reasoning benchmarks (survey cites MATH as example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic and reasoning benchmarks where extra test-time compute and iterative selection improves correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey claims test-time scaling (TTS) can achieve performance comparable to a model 14× larger on easy/intermediate tasks (e.g., MATH) and that COS achieves 4× lower compute than best-of-N while matching performance; no per-method exact numbers beyond those summary claims are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline: single-pass generation with no extra test-time compute; survey does not list exact baseline numeric scores here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Test-time sampling, aggregation (voting), reward-model based selection (verifier), and adaptive compute allocation (COS).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey-level quantitative claims: TTS comparable to ~14× larger model on easy/intermediate MATH tasks; COS achieves 4× compute efficiency relative to best-of-N while maintaining performance; these are survey-reported aggregate findings from cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Diminishing returns, higher latency/compute, environmental cost, and susceptibility to reward-hacking if selection uses imperfect reward models; requires reliable difficulty/confidence estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Survey positions TTS favorably vs. naive parameter scaling for many tasks, but notes pretraining still better for the hardest tasks or when inference compute is constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8831.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8831.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-judge / LLM-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-judge / LLM-as-a-judge (model judges generations for selection/alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using either the same LLM or an ensemble of LLMs as judges/verifiers to rank or filter candidate outputs, sometimes replacing human judges for preference data or selection at test-time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-judge: Selective instruction following with alignment self-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (survey cites LLM-as-judge work employing various backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references work using LLMs as judges (model-generated preferences or critiques) to scale feedback; specific backbones vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LLM-as-a-judge / Self-judge</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model (or another LLM) scores/ranks candidate outputs; ranking used to train reward models or to select best output at inference; can be used iteratively to improve candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Alignment, preference modeling, and selective generation (RLHF alternatives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where human preference data is costly and model-generated judgments are used to scale preference signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey notes RLAIF and LLM-as-judge approaches can scale preference collection and maintain high performance, but does not list numeric figures in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Human-labeled preference baseline (not numerically reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Use of LLMs as verifiers/judges in the loop (prompted to evaluate or rank candidate outputs); can drive both training (RLAIF) and inference selection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites that replacing humans with AI feedback (RLAIF) reduces cost and can maintain alignment performance in cited studies; details are in those works.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Risk of automating bias or calibration errors in AI judges; possible feedback loops and amplified biases if the judge model is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to human-judged RLHF: much cheaper and scalable, but potentially less reliable without careful calibration and periodic human grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Post-Training: A Deep Dive into Reasoning Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-polish: Enhance reasoning in large language models via problem refinement <em>(Rating: 2)</em></li>
                <li>Selfrag: Learning to retrieve, generate, and critique through selfreflection <em>(Rating: 2)</em></li>
                <li>Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Rewarding progress: Scaling automated process verifiers for llm reasoning <em>(Rating: 2)</em></li>
                <li>Self-evaluation guided beam search for reasoning <em>(Rating: 2)</em></li>
                <li>Monte Carlo Thought Search / Monte Carlo Reasoner (Monte Carlo Thought Search) <em>(Rating: 1)</em></li>
                <li>Self-judge: Selective instruction following with alignment self-evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8831",
    "paper_id": "paper-276725054",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (Iterative refinement with self-feedback)",
            "brief_description": "An iterative generate-then-critique procedure in which an LLM generates an initial answer, produces a self-critique of its output, and uses that critique to produce a revised answer; iteration continues until a stopping condition is met.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "General LLMs (unspecified)",
            "model_description": "Survey references general large language models (various sizes) applying iterative self-feedback; no single model or parameter counts are run in this survey.",
            "reflection_method_name": "Self-Refine (iterative self-feedback)",
            "reflection_method_description": "Generate initial response; have the model critique its own response (identify errors/inconsistencies); generate a revised response conditioned on that critique; repeat until convergence or threshold. Number of iterations not specified in survey summary.",
            "task_name": "Various reasoning tasks (math, code, QA) as cited",
            "task_description": "The survey reports Self-Refine being applied to mathematical reasoning, code generation, and other multi-step reasoning tasks where intermediate correction is beneficial.",
            "performance_with_reflection": "Survey states Self-Refine has been shown to improve performance on tasks including mathematical reasoning and code generation, but does not report numeric results in this paper.",
            "performance_without_reflection": "Not reported in this survey (no numeric baselines given here).",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered self-critique and conditional generation (no external verifier required in the canonical formulation).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Qualitative statement in survey: iterative self-critique and refinement methods improve correctness on math and code tasks (references to empirical papers are provided), but the survey does not provide within-paper quantitative values.",
            "limitations_or_failure_cases": "Survey notes increased inference compute cost and sensitivity to the quality of self-critique; unreliable self-critique can fail to find errors and thus not improve outputs. More compute/time may be required and verification may still be necessary for critical domains.",
            "comparison_to_other_methods": "Compared qualitatively to single-pass generation and to verification+selection pipelines; iterative self-critique offers improvement without external verifiers but may be less robust than verifier-guided selection when self-evaluation is poor.",
            "ablation_study_results": null,
            "uuid": "e8831.0",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Polish",
            "name_full": "Self-Polish (Progressive problem rephrasing and refinement)",
            "brief_description": "A generate-and-refine approach where the model first reformulates or clarifies the problem statement to make it more solvable, then solves the refined problem, possibly iterating rephrasing and solving.",
            "citation_title": "Self-polish: Enhance reasoning in large language models via problem refinement",
            "mention_or_use": "mention",
            "model_name": "General LLMs (unspecified)",
            "model_description": "Survey-level reference to models using problem refinement; no single-model experiments run in this survey.",
            "reflection_method_name": "Self-Polish (problem refinement + solve)",
            "reflection_method_description": "First transform or rephrase the user's problem to reduce ambiguity and complexity (self-polishing), then apply reasoning/solve stage; can iterate between polishing and solving to improve final answer.",
            "task_name": "Complex reasoning tasks (math word problems, ambiguous queries)",
            "task_description": "Tasks where reformulation clarifies implicit assumptions or decomposes complex prompts into simpler subproblems.",
            "performance_with_reflection": "Survey reports that Self-Polish improves solution accuracy qualitatively; no numeric performance is reported in the survey.",
            "performance_without_reflection": "Not reported in this survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering to ask the model to rephrase/clarify and then re-solve; internal iterative generation steps.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites referenced work showing improved comprehension and solution accuracy after progressive problem refinement, but does not include numbers in-paper.",
            "limitations_or_failure_cases": "Adds inference-time latency; effectiveness depends on whether the model can accurately rephrase the problem without introducing its own errors or biases.",
            "comparison_to_other_methods": "Presented as complementary to chain-of-thought prompting and self-critique; more helpful when the original prompt is ambiguous.",
            "ablation_study_results": null,
            "uuid": "e8831.1",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency Decoding",
            "brief_description": "A test-time ensemble-like method that samples multiple chain-of-thought reasoning traces, marginalizes over them, and selects the most consistent final answer (majority vote) to improve correctness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "Large LLMs (sufficiently large to exhibit CoT)",
            "model_description": "Method applied to chain-of-thought capable LLMs (survey references large models that exhibit CoT behavior; exact sizes vary across cited works).",
            "reflection_method_name": "Self-Consistency (multiple CoT samples + marginalization)",
            "reflection_method_description": "Sample many distinct chain-of-thought traces via stochastic decoding; compute final answer for each trace; select the final answer that appears most frequently (or highest marginalized probability).",
            "task_name": "Arithmetic and multi-step reasoning benchmarks (e.g., MATH, GSM8K as referenced)",
            "task_description": "Math and reasoning tasks where diverse reasoning traces can converge to the correct final answer.",
            "performance_with_reflection": "Survey states Self-Consistency 'significantly improve[s] accuracy on arithmetic and reasoning problems' but does not give numeric values in this survey text.",
            "performance_without_reflection": "Not numerically reported in this survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Test-time sampling and aggregation (majority vote) across multiple chain-of-thought samples; no retraining required.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited empirical results (from original Self-Consistency work) show higher accuracy versus single-sample CoT; the survey reiterates this qualitative improvement.",
            "limitations_or_failure_cases": "Requires multiple samples (increased compute); majority voting may fail if many chains converge on the same incorrect answer; needs sufficient diversity in sampled chains to be effective.",
            "comparison_to_other_methods": "Often combined with CoT prompting and sampling-based strategies; compared favorably against single-pass CoT in cited literature.",
            "ablation_study_results": null,
            "uuid": "e8831.2",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Critique / Self-Evaluation",
            "name_full": "Self-Critique / Self-Evaluation (LLM judges its own output)",
            "brief_description": "Methods where an LLM evaluates or scores its own outputs (e.g., identify errors, measure confidence), then uses that evaluation to accept/reject or revise answers; implemented via prompts or an LLM-as-judge.",
            "citation_title": "Self-evaluation improves selective generation in large language models",
            "mention_or_use": "mention",
            "model_name": "General LLMs (unspecified)",
            "model_description": "Survey cites multiple works using models to self-evaluate; exact model sizes vary across referenced studies.",
            "reflection_method_name": "Self-Critique / Self-Evaluation",
            "reflection_method_description": "The model generates an answer, then is prompted to evaluate (score/critique) that answer; the critique is used to either refine the answer or to decide whether to return it.",
            "task_name": "Selective generation, verification, and alignment tasks (e.g., QA, summarization, safe-response selection)",
            "task_description": "Tasks that benefit from model-side quality checks to reduce errors or harmful outputs.",
            "performance_with_reflection": "Survey reports that self-evaluation can improve selective generation and reliability (qualitative statements across citations); no numeric values reported directly in this survey.",
            "performance_without_reflection": "Not reported numerically in this survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompting LLM to judge its own output; sometimes implemented with a separate LLM verifier or the same model in a judge role.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites empirical works showing better selectivity and improved answer quality when models self-evaluate; details reside in the cited papers.",
            "limitations_or_failure_cases": "Self-evaluations can be overconfident or unreliable; if the model's critiquing ability is poor, it may not detect mistakes and can reinforce incorrect outputs.",
            "comparison_to_other_methods": "Compared to external verifiers and multi-model juries; self-evaluation is cheaper but potentially less reliable than strong external verifiers or human judgment.",
            "ablation_study_results": null,
            "uuid": "e8831.3",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Selfrag",
            "name_full": "SelfRAG (Learn to retrieve, generate, and critique through self-reflection)",
            "brief_description": "A self-reflective retrieval-augmented workflow where an LLM retrieves relevant context, generates an answer, and critiques or re-retrieves based on that critique to iteratively improve outputs.",
            "citation_title": "Selfrag: Learning to retrieve, generate, and critique through selfreflection",
            "mention_or_use": "mention",
            "model_name": "Retrieval-augmented LLM pipelines (unspecified LLM backbone)",
            "model_description": "Survey references retrieval-augmented systems that combine generation and self-critique loops; exact backbones vary in cited works.",
            "reflection_method_name": "SelfRAG (retrieve→generate→critique→re-retrieve)",
            "reflection_method_description": "Iterative loop combining retrieval and self-critique: retrieve context, generate answer, critique output, then refine retrieval or generation based on critique.",
            "task_name": "Knowledge-intensive generation and QA",
            "task_description": "Tasks where external knowledge retrieval helps answer correctness and iterative critique improves retrieval relevance.",
            "performance_with_reflection": "Survey states such methods can scale feedback loops and improve factuality; no per-method numeric results are reported in the survey.",
            "performance_without_reflection": "Not reported in this survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Hybrid: retrieval modules plus prompt-based self-critique; may use learned retrieval policies or iterative retrieval adjustments.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites works demonstrating improved factual accuracy and relevance when coupled with self-critique; specifics are in cited papers.",
            "limitations_or_failure_cases": "Risk of compounding retrieval errors if critique misleads retrieval; increases inference-time latency and complexity.",
            "comparison_to_other_methods": "Compared qualitatively to static retrieval-augmented generation; iterative retrieval+critique yields better factuality but at higher compute and system complexity.",
            "ablation_study_results": null,
            "uuid": "e8831.4",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Satori (Chain-of-Action-Thought)",
            "name_full": "Satori: Chain-of-Action-Thought / Reinforcement self-improvement",
            "brief_description": "A two-stage approach combining format tuning (supervised trajectories with meta-action tokens) and a self-improvement RL phase where the model restarts from intermediate steps and refines reasoning using combined rewards including reflection bonuses.",
            "citation_title": "Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search",
            "mention_or_use": "mention",
            "model_name": "LLMs used in Satori (survey does not list a single backbone)",
            "model_description": "Survey reports Satori trains models in Stage 1 with synthetic trajectories and in Stage 2 via PPO with restart-and-explore strategies; exact model sizes are not specified in survey text.",
            "reflection_method_name": "Chain-of-Action-Thought (Satori)",
            "reflection_method_description": "Stage 1: format tuning on synthetic action-trace trajectories; Stage 2: RL with restart-and-explore, reflection bonuses and outcome/process reward models used to encourage improved multi-step reasoning.",
            "task_name": "Complex reasoning tasks requiring extended autoregressive search",
            "task_description": "Tasks requiring long deliberation and the ability to restart/refine from mid-trajectory (e.g., complex planning or multi-step math/logic).",
            "performance_with_reflection": "Survey summarizes Satori's design and claims that restart/explore with reflection-style rewards improves reasoning; no numeric performance figures are reported in the survey.",
            "performance_without_reflection": "Not reported in this survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "RL-based self-improvement with restart-from-intermediate-step capability and reflection bonuses computed from rule-based checks, preference models, and other signals.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey reports the method incentivizes allocation of more compute to harder problems and enables extended reasoning, citing Satori; numerical evidence must be sought in the Satori paper itself.",
            "limitations_or_failure_cases": "Requires complex reward engineering; can be computationally costly; success depends on reward design and stability of RL training.",
            "comparison_to_other_methods": "Framed as addressing shortcomings of CoT by adding self-correction/restart capabilities; compared conceptually to standard CoT and RL fine-tuning.",
            "ablation_study_results": null,
            "uuid": "e8831.5",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Verifier / PRM-guided tree search",
            "name_full": "Process-Reward-Model (PRM) guided Tree Search / Verifier loop",
            "brief_description": "A family of test-time methods that generate multiple candidate reasoning paths and use a verifier that scores intermediate (process) steps, pruning or selecting paths based on process-level feedback rather than only final outcome.",
            "citation_title": "Rewarding progress: Scaling automated process verifiers for llm reasoning",
            "mention_or_use": "mention",
            "model_name": "LLMs used as proposers and verifiers (unspecified)",
            "model_description": "Survey discusses proposers (LLM generating thoughts) and verifiers (reward models scoring steps) as separate components; specific model sizes vary across cited works.",
            "reflection_method_name": "PRM-guided tree search (verifier-in-the-loop)",
            "reflection_method_description": "Proposer generates candidate reasoning steps; a Process Reward Model (PRM) scores intermediate steps to guide search (beam/toT/MCTS) and prune invalid branches; repeated until acceptable solution found.",
            "task_name": "Multi-step reasoning tasks (math, logic, planning)",
            "task_description": "Problems where intermediate step correctness improves final answer and facilitates credit assignment across steps.",
            "performance_with_reflection": "Survey reports that combining PRM-guided tree search with online distillation (integration) achieves 4× efficiency gains over baseline methods while maintaining 94% solution accuracy on the MATH dataset (this numerical claim appears in the survey's integration discussion).",
            "performance_without_reflection": "Baseline methods (without PRM-guided tree search + online distillation) are implied to be ~4× less compute-efficient for equivalent MATH accuracy; the survey does not list absolute baseline accuracy numbers here.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External verifier/reward model (process reward) evaluating intermediate reasoning steps; integrated into tree-search (ToT/GoT/MCTS) to prune and refine candidate chains.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey explicitly cites an integration result: PRM-guided tree search + online distillation produced a 4× efficiency improvement while maintaining 94% solution accuracy on MATH (numerical example provided in the survey text).",
            "limitations_or_failure_cases": "Verifier quality critically limits gains; additional compute overhead at inference; risk of verifier-induced bias and reward hacking if verifier is imperfect; needs reliable uncertainty estimators per survey discussion.",
            "comparison_to_other_methods": "Compared favorably (in survey) to plain CoT, self-consistency, and best-of-N sampling in complex tasks because PRM allows pruning of invalid intermediate steps; however it costs more compute than single-pass methods.",
            "ablation_study_results": null,
            "uuid": "e8831.6",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency + Verifier Integration (general test-time scaling)",
            "name_full": "Test-time iterative refinement strategies (Self-Consistency, Best-of-N, BoN with verifiers, Compute-Optimal Scaling (COS))",
            "brief_description": "A set of test-time iterative methods where multiple candidate outputs are generated (parallel or sequential) and then refined/selected using majority voting, reward models, or adaptive compute allocation (COS) to improve final answer quality.",
            "citation_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
            "mention_or_use": "mention",
            "model_name": "General LLMs (various sizes cited across the survey)",
            "model_description": "Survey-level discussion of test-time scaling methods applied to LLMs of many sizes; no single experimental backbone is used in this survey.",
            "reflection_method_name": "Self-Consistency / Best-of-N / Compute-Optimal Scaling (COS)",
            "reflection_method_description": "Generate multiple candidate reasoning traces/answers (via sampling or beam search), then (a) aggregate via majority (Self-Consistency), (b) select highest-scoring by a reward model (Best-of-N), or (c) adaptively allocate compute per example difficulty (COS) combining sequential refinement for easy prompts and parallel exploration for hard prompts.",
            "task_name": "MATH and other reasoning benchmarks (survey cites MATH as example)",
            "task_description": "Arithmetic and reasoning benchmarks where extra test-time compute and iterative selection improves correctness.",
            "performance_with_reflection": "Survey claims test-time scaling (TTS) can achieve performance comparable to a model 14× larger on easy/intermediate tasks (e.g., MATH) and that COS achieves 4× lower compute than best-of-N while matching performance; no per-method exact numbers beyond those summary claims are provided in the survey.",
            "performance_without_reflection": "Baseline: single-pass generation with no extra test-time compute; survey does not list exact baseline numeric scores here.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Test-time sampling, aggregation (voting), reward-model based selection (verifier), and adaptive compute allocation (COS).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey-level quantitative claims: TTS comparable to ~14× larger model on easy/intermediate MATH tasks; COS achieves 4× compute efficiency relative to best-of-N while maintaining performance; these are survey-reported aggregate findings from cited works.",
            "limitations_or_failure_cases": "Diminishing returns, higher latency/compute, environmental cost, and susceptibility to reward-hacking if selection uses imperfect reward models; requires reliable difficulty/confidence estimators.",
            "comparison_to_other_methods": "Survey positions TTS favorably vs. naive parameter scaling for many tasks, but notes pretraining still better for the hardest tasks or when inference compute is constrained.",
            "ablation_study_results": null,
            "uuid": "e8831.7",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-judge / LLM-as-judge",
            "name_full": "Self-judge / LLM-as-a-judge (model judges generations for selection/alignment)",
            "brief_description": "Using either the same LLM or an ensemble of LLMs as judges/verifiers to rank or filter candidate outputs, sometimes replacing human judges for preference data or selection at test-time.",
            "citation_title": "Self-judge: Selective instruction following with alignment self-evaluation",
            "mention_or_use": "mention",
            "model_name": "General LLMs (survey cites LLM-as-judge work employing various backbones)",
            "model_description": "Survey references work using LLMs as judges (model-generated preferences or critiques) to scale feedback; specific backbones vary by cited work.",
            "reflection_method_name": "LLM-as-a-judge / Self-judge",
            "reflection_method_description": "The model (or another LLM) scores/ranks candidate outputs; ranking used to train reward models or to select best output at inference; can be used iteratively to improve candidates.",
            "task_name": "Alignment, preference modeling, and selective generation (RLHF alternatives)",
            "task_description": "Tasks where human preference data is costly and model-generated judgments are used to scale preference signals.",
            "performance_with_reflection": "Survey notes RLAIF and LLM-as-judge approaches can scale preference collection and maintain high performance, but does not list numeric figures in this survey text.",
            "performance_without_reflection": "Human-labeled preference baseline (not numerically reported here).",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Use of LLMs as verifiers/judges in the loop (prompted to evaluate or rank candidate outputs); can drive both training (RLAIF) and inference selection.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites that replacing humans with AI feedback (RLAIF) reduces cost and can maintain alignment performance in cited studies; details are in those works.",
            "limitations_or_failure_cases": "Risk of automating bias or calibration errors in AI judges; possible feedback loops and amplified biases if the judge model is imperfect.",
            "comparison_to_other_methods": "Compared to human-judged RLHF: much cheaper and scalable, but potentially less reliable without careful calibration and periodic human grounding.",
            "ablation_study_results": null,
            "uuid": "e8831.8",
            "source_info": {
                "paper_title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-polish: Enhance reasoning in large language models via problem refinement",
            "rating": 2,
            "sanitized_title": "selfpolish_enhance_reasoning_in_large_language_models_via_problem_refinement"
        },
        {
            "paper_title": "Selfrag: Learning to retrieve, generate, and critique through selfreflection",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search",
            "rating": 2,
            "sanitized_title": "satori_reinforcement_learning_with_chainofactionthought_enhances_llm_reasoning_via_autoregressive_search"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Rewarding progress: Scaling automated process verifiers for llm reasoning",
            "rating": 2,
            "sanitized_title": "rewarding_progress_scaling_automated_process_verifiers_for_llm_reasoning"
        },
        {
            "paper_title": "Self-evaluation guided beam search for reasoning",
            "rating": 2,
            "sanitized_title": "selfevaluation_guided_beam_search_for_reasoning"
        },
        {
            "paper_title": "Monte Carlo Thought Search / Monte Carlo Reasoner (Monte Carlo Thought Search)",
            "rating": 1,
            "sanitized_title": "monte_carlo_thought_search_monte_carlo_reasoner_monte_carlo_thought_search"
        },
        {
            "paper_title": "Self-judge: Selective instruction following with alignment self-evaluation",
            "rating": 2,
            "sanitized_title": "selfjudge_selective_instruction_following_with_alignment_selfevaluation"
        }
    ],
    "cost": 0.02717775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM Post-Training: A Deep Dive into Reasoning Large Language Models
24 Mar 2025</p>
<p>Komal Kumar 
Tajamul Ashraf 
Omkar Thawakar 
Rao Muhammad Anwer 
Hisham Cholakkal 
Mubarak Shah 
Ming-Hsuan Yang 
FahadPhillip H S Torr 
Shahbaz Khan 
Salman Khan 
LLM Post-Training: A Deep Dive into Reasoning Large Language Models
24 Mar 2025FA6617F376AFA4861052C07A0AA9904AarXiv:2502.21321v2[cs.CL]Reasoning ModelsLarge Language ModelsReinforcement LearningReward ModelingTest-time Scaling
Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications.Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now increasingly shifting focus toward post-training techniques to achieve further breakthroughs.While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations.Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks.This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs.We highlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research directions.We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.</p>
<p>Introduction</p>
<p>C ontemporary Large Language Models (LLMs) exhibit remarkable capabilities across a vast spectrum of tasks, encompassing not only text generation [1,2,3] and questionanswering [4,5,6,7], but also sophisticated multi-step reasoning [8,9,10,11].They power applications in natural language understanding [12,13,14,15,16,17], content generation [18,19,20,21,22,23,24,25], automated reasoning [26,27,28,29], and multimodal interactions [30, 31? , 33].By leveraging vast self-supervised training corpora, these models often approximate human-like cognition [34,35,36,37,38], demonstrating impressive adaptability in real-world settings.</p>
<p>Despite these impressive achievements, LLMs remain prone to critical shortcomings.They can generate misleading or factually incorrect content (commonly referred to as "hallucinations") and may struggle to maintain logical consistency throughout extended discourse [41,42,43,44,45,46].Moreover, the concept of reasoning in LLMs remains a topic of debate.While these models can produce responses that appear logically coherent, their reasoning is fundamentally distinct from human-like logical inference [47,34,48,49].This distinction is crucial, as it helps explain why LLMs can  Fig.1: A taxonomy of post-training approaches for LLMs (LLMs), categorized into Fine-tuning, Reinforcement Learning, and Test-time Scaling methods.We summarize the key techniques used in recent LLM models, such as GPT-4 [39], LLaMA 3.3 [13], and Deepseek R1 [40].</p>
<p>produce compelling outputs while still stumbling on relatively simple logical tasks.Unlike symbolic reasoning that manipulates explicit rules and facts, LLMs operate in an implicit and probabilistic manner [50,42,51].For the scope of this work, 'reasoning' in LLMs refers to their ability to generate logically coherent responses based on statistical patterns in data rather than explicit logical inference or symbolic manipulation.Additionally, models trained purely via next-token prediction can fail to align with user expectations or ethical standards, especially in ambiguous or malicious scenarios [4,52].These issues underscore the need for specialized strategies that address reliability, bias, and context sensitivity in LLM outputs.</p>
<p>LLMs training can be broadly categorized into two stages: pre-training, which generally relies on a next-token prediction objective over large-scale corpora, and post-training, encompassing multiple rounds of fine-tuning and alignment.Posttraining mechanisms aim to mitigate LLMs limitations by refining model behavior and aligning outputs with human intent, mitigating biases or inaccuracies [53].</p>
<p>Adapting LLMs to domain-specific tasks often involves techniques like fine-tuning [54,55,56], which enables taskspecific learning but risks overfitting and incurs high computational costs.To address these challenges, approaches such as Reinforcement Learning (RL) [57,58,59] enhance adaptability by leveraging dynamic feedback and optimizing sequential decision-making.Additionally, advances in scaling techniques, including Low-Rank Adaptation (LoRA) [60], adapters [365?], and Retrieval-Augmented Generation (RAG) [61,62,63], improve both computational efficiency and factual accuracy.These strategies, coupled with distributed training frameworks, facilitate large-scale deployment and further boost the usability of LLMs across diverse applications (Figure 1).Through these targeted post-training interventions, LLMs become better aligned with human intent and ethical requirements, ultimately enhancing their realworld applicability.Below, we summarize key post-training stages.a) Fine-Tuning in LLMs: Fine-tuning adapts pre-trained LLMs to specific tasks or domains by updating parameters on curated datasets [64,65,66,54,55,67,56].While LLMs generalize well after large-scale pretraining, fine-tuning enhances performance in tasks like sentiment analysis [68,69], question answering, and domain-specific applications such as medical diagnosis [70,71,72].This process, typically supervised, aligns models with task requirements but poses challenges like overfitting, high computational costs, and sensitivity to data biases [56,31,16].To this end, parameter-efficient techniques like LoRA [60] and adapters learn task-specific adaptation by updating explicit parameters, significantly reducing computational overhead.As models specialize, they may struggle with out-of-domain generalization, underscoring the trade-off between specificity and versatility.</p>
<p>Fine-tuning tailors LLMs for specific tasks, improving performance but risking overfitting, high compute costs, and reduced generalization.</p>
<p>b) Reinforcement Learning in LLMs:</p>
<p>In conventional RL, an agent interacts with a structured environment, taking discrete actions to transition between states while maximizing cumulative rewards [73].RL domains-such as robotics, board games, and control systems-feature well-defined stateaction spaces and clear objectives [74,75].RL in LLMs differs significantly.Instead of a finite action set, LLMs select tokens from a vast vocabulary, and their evolving state comprises an ever-growing text sequence [16,59,76,57].This complicates planning and credit assignment, as the impact of token selection may only emerge later.Feedback in language-based RL is also sparse [77], subjective, and delayed, relying on heuristic evaluations and user preferences rather than clear performance metrics [78,79,58].Additionally, LLMs must balance multiple, sometimes conflicting, objectives, unlike conventional RL, which typically optimizes for a single goal.Hybrid approaches combining process-based rewards (e.g., chain-of-thought reasoning) with outcome-based evaluations (e.g., response quality) help refine learning [8,80,81].Thus, RL for LLMs requires specialized optimization techniques to handle high-dimensional outputs, non-stationary objectives, and complex reward structures, ensuring responses remain contextually relevant and aligned with user expectations.</p>
<p>Reinforcement in LLMs extends beyond conventional RL as it navigates vast action spaces, handles subjective and delayed rewards, and balances multiple objectives, necessitating specialized optimization techniques.c) Test Time Scaling in LLMs: Test Time Scaling is optimizing model performance and efficiency without altering the core architecture.It enables better generalization while minimizing computational overhead.It is crucial for enhancing the performance and efficiency of LLMs.It helps improve generalization across tasks but introduces significant computational challenges [82,83].Balancing performance and resource efficiency requires targeted strategies at inference.Techniques like CoT [8] reasoning and Tree-of-Thought (ToT) [84] frameworks enhance multi-step reasoning by breaking down complex problems into sequential or tree-structured steps.Additionally, search-based techniques [85,86,87,88] enable iterative exploration of possible outputs, helping refine responses and ensure higher factual accuracy.These approaches, combined with methods like LoRA [60], adapters, and RAG [61,62,89], optimize the model's ability to handle complex, domain-specific tasks at scale.RAG enhances factual accuracy by dynamically retrieving external knowledge, mitigating limitations of static training data [62,24,90].Distributed training frameworks leverage parallel processing to manage the high computational demands of large-scale models.Test-time scaling optimizes inference by adjusting parameters dynamically based on task complexity [83,91].Modifying depth, width, or active layers balances computational efficiency and output quality, making it valuable in resource-limited or variable conditions.Despite advancements, scaling presents challenges such as diminishing returns, longer inference times, and environmental impact, especially when search techniques are performed at test time rather than during training [82].Ensuring accessibility and feasibility is essential to maintain high-quality, efficient LLM deployment.</p>
<p>Test-time scaling enhances the adaptability of LLMs by dynamically adjusting computational resources during inference.</p>
<p>Prior Surveys</p>
<p>Recent surveys on RL and LLMs provide valuable insights but often focus on specific aspects, leaving key post-training components underexplored [51,92,93,94].Many works examine RL techniques like Reinforcement Learning from Human Feedback (RLHF) [58], Reinforcement Learning from AI Feedback (RLAIF) [95], and Direct Preference Optimization (DPO) [57], yet they overlook fine-tuning, scaling, and critical benchmarks essential for real-world applications.Furthermore, these studies have not explored the potential of RL even without human annotation supervised finetuning in various frameworks such as DeepSeek R1 with GRPO [59].Other surveys explore LLMs in traditional RL tasks, such as multi-task learning and decision-making, but they primarily classify LLM functionalities rather than addressing test-time scaling and integrated post-training strategies [96,97].Similarly, studies on LLM reasoning [98,99,100,55,101,102,103,104] discuss learning-to-reason techniques but lack structured guidance on combining fine-tuning, RL, and scaling.The absence of tutorials, along with reviews of software libraries and implementation tools, further limits their practicality.In contrast, this survey offers a comprehensive view of LLM post-training as shown in Figure 1 by systematically covering fine-tuning, RL, and scaling as interconnected optimization strategies.We offer practical resources-benchmarks, datasets, and tutorials-to aid LLM refinement for real-world applications.</p>
<p>Contributions</p>
<p>The key contributions of this survey are as follows:</p>
<p>Background</p>
<p>The LLMs have transformed reasoning by learning to predict the next token in a sequence based on vast amounts of text data [105,4] using Maximum Likelihood Estimation (MLE) [106,3,107], which maximizes the probability of generating the correct sequence given an input.This is achieved by minimizing the negative log-likelihood:
L MLE = − T t=1 log P θ (y t | y &lt;t , X).
Here, X represents the input, such as a prompt or context.Y = (y 1 , y 2 , ..., y T ) is the corresponding target output sequence, and P θ (y t | y &lt;t , X) denotes the model's predicted probability for token y t , given preceding tokens.</p>
<p>Token-wise training can ensure fluency but may cause cascading errors due to uncorrected mistakes in inference.</p>
<p>As these models scale, they exhibit emergent reasoning abilities, particularly when trained on diverse data that include code and mathematical content [108,8].However, despite their impressive capabilities, LLMs struggle to maintain coherence and contextual relevance over long sequences.Addressing these limitations necessitates a structured approach to sequence generation, which naturally aligns with RL.</p>
<p>Since LLMs generate text autoregressively-where each token prediction depends on previously generated tokens-this process can be modeled as a sequential decision-making problem within a Markov Decision Process (MDP) [109].In this setting, the state s t represents the sequence of tokens generated so far, the action a t is the next token, and a reward R(s t , a t ) evaluates the quality of the output.An LLM 's policy π θ is optimized to maximize the expected return:
J(π θ ) = E ∞ t=0 γ t R(s t , a t ) ,
where γ is the discount factor that determines how strongly future rewards influence current decisions.A higher γ places greater importance on long-term rewards.The primary objective in RL is to learn a policy that maximizes the expected cumulative reward, often referred to as the return.This requires balancing exploration-trying new actions to discover their effects-and exploitation-leveraging known actions that yield high rewards.While LLMs optimize a likelihood function using static data, RL instead optimizes the expected return through dynamic interactions.To ensure that LLMs generate responses that are not only statistically likely but also aligned with human preferences, it is essential to go beyond static optimization methods.While likelihood-based training captures patterns from vast corpora, it lacks the adaptability needed for refining decision-making in interactive settings.By leveraging structured approaches to maximizing long-term objectives, models can dynamically adjust their strategies, balancing exploration and exploitation to improve reasoning, coherence, and alignment [110,111,49,48].</p>
<p>LLMs exhibit emergent abilities due to scale, while RL refines and aligns them for better reasoning and interaction.</p>
<p>RL based Sequential Reasoning.</p>
<p>The chain-of-thought reasoning employed in modern LLMs is naturally framed as an RL problem.In this perspective, each intermediate reasoning step is treated as an action contributing to a final answer.The objective function J(π θ ) represents the expected reward of the policy π θ , capturing how well the model performs over multiple reasoning steps.The policy gradient update is given by:
∇ θ J(π θ ) = E τ T t=1 ∇ θ log π θ (x t | x 1:t−1 ) A(s t , a t ) ,
where the advantage function A(s t , a t ) distributes credit to individual steps, ensuring that the overall reasoning process is refined through both immediate and delayed rewards.Such formulations, including step-wise reward decomposition [112,113], have been crucial for enhancing the interpretability and performance of LLMs on complex reasoning tasks.In traditional RL formulations, an agent has:
Value function: V (s) = E future return | s , Action-value (Q-) function: Q(s, a) = E future return | s, a , Advantage function: A(s, a) = Q(s, a) − V (s).
In words, A(s, a) measures how much better or worse it is to take a specific action a in state s compared to what the agent would normally expect (its baseline V (s)).</p>
<p>Early RL Methods for Language Modeling.</p>
<p>Here, we briefly overview pioneering methods that laid the groundwork for applying RL to language generation tasks.These initial efforts train a decision-making model (policy (p θ )) by directly adjusting its parameters to maximize rewards.Some policy gradient approaches are explained below: Policy Gradient (REINFORCE).The REINFORCE algorithm [114,115] is a method used to improve decision-making by adjusting the model's strategy (policy) based on rewards received from its actions.Instead of directly learning the best action for every situation, the algorithm refines how likely different actions are to be chosen, gradually improving outcomes over time.At each step, the model updates its parameters (θ) based on how well its past decisions performed:
θ ← θ + α G − b T t=1 ∇ θ log π θ (a t | s t ).
Here: G represents the total reward the model accumulates over an episode, b is a baseline value that helps reduce variance, making learning more stable, ∇ θ log π θ (a t | s t ) measures how much a small change in θ affects the probability of choosing action a t given state s t , α is the learning rate, controlling how much the policy updates at each step.</p>
<p>Optimizing actions based on long-term rewards, which account for the cumulative benefits of a sequence of reasoning steps rather than just immediate outcomes, is fundamental in recent LLMs.This approach allows models to explore multiple reasoning paths more effectively.</p>
<p>Curriculum Learning with MIXER.. Ranzato et al. [116] introduces a gradual transition from maximum likelihood estimation (MLE) to RL.The overall loss is a weighted combination:
L = λ(t) L MLE + 1 − λ(t) L RL ,
where λ(t) decreases with training time.This curriculum helps the model ease into the RL objective and mitigate the mismatch between training and inference.[117] refines the policy gradient method by comparing the model's sampled outputs against its own best (greedy) predictions.Instead of using an arbitrary baseline, SCST uses the model's own highest-scoring output, ensuring that updates directly improve performance relative to what the model currently considers its best response.The gradient update follows:</p>
<p>Self-Critical Sequence Training (SCST). SCST
∇ θ J(π θ ) ≈ r(y s ) − r(ŷ) ∇ θ log π θ (y s ),
where y s is a sampled sequence, ŷ is the greedy output, and r(y) represents an evaluation metric such as BLEU [118] for translation or CIDEr [119] for image captioning.Since the learning signal is based on the difference r(y s ) − r(ŷ), the model is explicitly trained to generate outputs that score higher than its own baseline under the evaluation metric.If the sampled output outperforms the greedy output, the model reinforces it; otherwise, it discourages that sequence.This direct feedback loop ensures that training aligns with the desired evaluation criteria rather than just maximizing likelihood.By leveraging the model's own best predictions as a baseline, SCST effectively reduces variance and stabilizes training while optimizing real-world performance metrics.[151] directly minimizes the expected risk over the output distribution.Given a task-specific loss ∆(y, y * ) comparing the generated output y with the reference y * , the MRT objective is defined as:</p>
<p>Minimum Risk Training (MRT). MRT
L MRT (θ) = y∈Y p θ (y | x) ∆(y, y * ).
This formulation incorporates evaluation metrics (e.g., 1 − BLEU) directly into training, enabling fine-grained adjustments of the policy.Advantage Actor-Critic (A2C/A3C).RL methods like REINFORCE [114] rely solely on policy gradients, which suffer from high variance, leading to unstable and inefficient learning.Since the reward signal fluctuates across different trajectories, updates may be noisy, causing slow or erratic convergence.To mitigate this, Actor-Critic methods [152,153,154,155] combine two components as follows: an actor and a critic.The actor is a policy π θ (a t | s t ) that selects actions a t at state s t , while the critic is a value function V ϕ (s t ) that evaluates the expected return of a state.The critic provides a more stable learning signal, reducing variance in policy updates and enabling efficient learning in continuous action spaces.Actor updates are guided by the policy gradient theorem, where the advantage function A(s t , a t ) defined in Sec.2.1, determines how much better an action a t is compared to the expected value of state s t .The policy with the learning rate α is updated as:
θ ← θ + α A(s t , a t ) ∇ θ log π θ (a t | s t ).
Meanwhile, the critic is updated using temporal difference learning, minimizing the squared error between its estimate and the actual return:
ϕ ← ϕ − β ∇ ϕ V ϕ (s t ) − G t 2 .
where β is a learning rate for critic.To enhance stability and efficiency, several improvements have been proposed.Eligibility traces allow learning from recent states, enabling faster convergence.Function approximation with neural networks ensures effective handling of high-dimensional inputs.Advanced variants such as Natural Gradient methods [156] adjust updates using the Fisher Information Matrix, improving convergence speed.A notable early example is Barto's Actor-Critic model [157], where the critic uses a linear function V ϕ (s t ) and the actor follows a linear policy.Modern methods like A2C (Advantage Actor-Critic) [154] and A3C (Asynchronous Advantage Actor-Critic) [155] extend this approach by parallelizing training across multiple environments, leading to faster and more stable learning.By leveraging the critic's value estimation, actor-critic methods stabilize learning, improve sample efficiency, and accelerate convergence, making them more effective for complex decision-making tasks.</p>
<p>Connection with Modern Methods.The aforementioned early RL methods-REINFORCE [114], MIXER [116], SeqGAN [158], SCST [117], MRT [151], and actor-critic algorithms established the mathematical foundations for sequential reasoning in LLMs.These methods provided initial solutions to challenges such as exposure bias and high variance.Modern techniques such as large-scale RL from Human Feedback (RLHF) using PPO [73] and advanced reward models, e.g., Group Relative Policy Optimization (GRPO) [159] build directly upon these ideas.By integrating sophisticated reward signals and leveraging efficient policy updates, contemporary LLMs achieve improved reasoning, safety, and alignment with human values and pave the way for robust multi-step reasoning and improved quality of generated text.Table 1 provides an overview of recent models, including their parameters, architecture types, and the distilled RL methods employed, along with links for easy access.</p>
<p>Reinforced LLMs</p>
<p>From a methodological perspective, the integration of RL into LLM reasoning typically follows three core steps:</p>
<p>1) Supervised Fine-Tuning (SFT): Commences with a pretrained language model that is subsequently refined on a supervised dataset of high-quality, human-crafted examples.This phase ensures the model acquires a baseline compliance with format and style guidelines.2) Reward Model (RM) Training: Generated outputs from the fine-tuned model are collected and subjected to human preference labeling.The reward model is then trained to replicate these label-based scores or rankings, effectively learning a continuous reward function that maps response text to a scalar value.3) RL Fine-Tuning: Finally, the main language model is optimized via a policy gradient algorithm most e.g PPO to maximize the reward model's output.By iterating this loop, the LLM learns to produce responses that humans find preferable along key dimensions such as accuracy, helpfulness, and stylistic coherence.4) Reward Modeling and Alignment: Sophisticated reward functions are developed-drawing from human preferences, adversarial feedback, or automated metrics-to guide the model toward outputs that are coherent, safe, and contextually appropriate.These rewards are critical for effective credit assignment across multistep reasoning processes.Early approaches to aligning LLMs with human preferences leveraged classical RL algorithms, such as PPO [73] and Trust Region Policy Optimization (TRPO) [160], which optimize a policy by maximizing the expected cumulative reward while enforcing constraints on policy updates via a surrogate objective function and KL-divergence regularization [161].Improved alternatives to these methods for scalable preferencebased optimization have emerged, such as Direct Preference Optimization (DPO) [57,162] and Group Relative Policy Optimization (GRPO) [159,59,16], which reformulate the alignment objective as a ranking-based contrastive loss function [163] over human-labeled preference data.Unlike PPO and TRPO [160], which rely on explicit reward models and critic networks, DPO and GRPO directly optimize the policy by leveraging log-likelihood ratios and group-wise reward comparisons, respectively, eliminating the need for explicit value function approximation while preserving preferenceconsistent learning dynamics.This transition from classical RL-based alignment to preference-based direct optimization introduces novel formulations such as contrastive ranking loss, policy likelihood ratio regularization, and grouped advantage estimation, which are explained in subsequent sections.</p>
<p>Reward modeling</p>
<p>Let X be the space of possible queries (e.g., user prompts).For each query x ∈ X , we collect one or more candidate responses {y j } mx j=1 where m x is the number of candidate responses for query x.Typically, these responses are generated by a language model or policy under different sampling or prompting conditions.Human annotators provide preference judgments for these responses.These can take various forms:</p>
<p>• Pairwise preference: For two responses y j and y k to the same query x, an annotator indicates whether y j is preferred to y k .• Rankings: A partial or total ordering of the candidate responses, e.g.y j1 ≻ y j2 ≻ • • • ≻ y jm x .We denote such human preference data by {r j } for each response or pair, where r j might be a label, a rank, or an index indicating preference level.The overall dataset D then consists of N annotated examples:
D = (x i , {y i j } mi j=1 , {preferences i }) N i=1 .
In practice, a large number of queries x are sampled from real or simulated user requests.Candidate responses {y j } mx j=1 are generated by either sampling from a base language model or using beam search or other decoding strategies.Human annotators then provide pairwise or ranking feedback on which responses are better (or worse) according to predefined criteria (e.g., quality, correctness, helpfulness, etc).We train a parametric model Reward Model (R θ (x, y)), referred to as the reward model, to map each (query, response) pair (x, y) to a scalar score.The goal is for R θ to reflect the alignment or preference level, such that:
R θ : X × Y → R.
Here Y is the space of all possible responses.To train R θ , we use the human preference labels in D to define a suitable ranking-based loss, as explained below.</p>
<p>I. Bradley-Terry Model (Pairwise).For pairwise preferences, Bradley-Terry model [164] is often used.Suppose the dataset indicates that, for a given query x, human annotators prefer y j to y k , we denote it as y j ≻ y k .Under Bradley-Terry, the probability of y j being preferred over y k is given by:
P y j ≻ y k | x; θ = exp R θ (x, y j ) exp R θ (x, y j ) + exp R θ (x, y k ) .
We train R θ by maximizing the likelihood of observed preferences (or equivalently minimizing the negative log-likelihood):
L BT (θ) = − (x, y j ≻y k ) ∈ D log P y j ≻ y k | x; θ .
II. Plackett-Luce Model1 (Rankings).When full or partial rankings of m responses are available, i.e.,
y j1 ≻ y j2 ≻ • • • ≻ y jm ,
the Plackett-Luce model [165] factorizes the probability of this ranking as:
P y j1 , . . . , y jm | x; θ = m ℓ=1 exp R θ (x, y j ℓ ) m k=ℓ exp R θ (x, y j k ) .
Its negative log-likelihood is:
L PL (θ) = − (x, rank)∈D m ℓ=1 log exp R θ (x, y j ℓ ) m k=ℓ exp R θ (x, y j k )
.</p>
<p>In practice, one minimizes the sum (or average) of the chosen ranking-based loss over all preference data:
L(θ) = 1 |D| (x, {yj }, prefs) ∈ D L ranking θ; x, {y j }, prefs ,
where L ranking could be either L BT or L PL .While the reward model R θ (x, y) provides a scalar reward signal reflecting human preferences, this connects to common RL concepts, especially the advantage function.</p>
<p>Reward modeling uses ranking-based losses to learn a function from human preferences for policy optimization.</p>
<p>Reward modeling Types.Rewards can be categorized into explicit and implicit approaches.</p>
<p>Explicit Reward Modeling</p>
<p>Explicit reward modeling defines reward functions directly based on predefined rules, heuristics, or human annotations.This reward structure involves direct, numeric signals from humans or from specialized AI modules trained to approximate human judgments (e.g., ranking or pairwise comparison).This method can produce precise reward estimates but may be time-consuming or costly at scale.Illustrative use cases include 'red-teaming' exercises where experts rate the severity of toxic outputs, or domain-specialist tasks in which correctness must be validated by a subject matter expert.</p>
<p>Implicit Reward Modeling</p>
<p>Implicit reward modeling infers rewards indirectly from observed behaviors, interactions, or preference signals, often leveraging machine learning techniques to uncover latent reward structures.It derives its signals from user interaction metrics such as upvotes, acceptance rates, click-through patterns, or session engagement times.While it can accumulate vast datasets with minimal overhead, this approach risks fostering behaviors that exploit engagement heuristics at the expense of content quality or veracity.Reward Function.Defining a reward function for text generation tasks is an ill-posed problem [166,167].The existing RL methods in LLMs either focus on the generation process outcome (Outcome Reward Modeling) or the (Process Reward Modeling), to shape LLM behaviors.We explain these two reward modeling paradigms below.</p>
<p>Outcome Reward Modeling</p>
<p>Measures the end result (e.g., whether the final answer is factually correct or solves the user's query).This model is straightforward to implement but may offer limited insight into how the conclusion was reached.It is prevalent in short-response tasks, where the user's primary concern is the correctness or succinctness of the final statement.For long-response tasks, outcome based reward can lead to credit assignment problem, i.e., which specific actions or states lead to a particular reward outcome.</p>
<p>Process Reward Modeling</p>
<p>Assigns feedback at intermediate reasoning steps, incentivizing coherent, logically consistent, and well-structured chains of thought.This approach is particularly valuable for tasks involving mathematical derivations, legal arguments, or code debugging, in which the path to the answer is as significant as the final statement.In such problems, the reward assigned in individual steps encourages transparency and robust stepby-step reasoning.However, it requires a more complex annotation process, e.g., requires "gold" reasoning steps or partial credit scoring.Process rewards can be combined with outcome rewards for a strong multi-phase training signal.</p>
<p>Policy Reward Modeling (PRM) with last-step aggregation outperforms Outcome Reward Modeling</p>
<p>(ORM) by leveraging final-step evaluations to optimize policy updates more effectively.</p>
<p>Iterative RL with Adaptive Reward Models</p>
<p>Adaptive Reward Models is a training methodology designed to continuously improve the performance of LLMs by iteratively refining the reward models and the policy model.This approach addresses the challenges of reward hacking and reward model drift, which can occur when the reward model becomes misaligned with the desired objectives during largescale RL training.The RL process is divided into multiple iterations, where the model is trained in cycles.After each iteration, the reward model is updated based on the latest model behavior and human feedback.The reward model is not static but evolves over time to better align with human preferences and task requirements.This adaptation ensures that the reward signals remain accurate and relevant as the model improves.Repeat the iterative process until the model's performance plateaus or meets the desired benchmarks.The reward model and policy model co-evolve, with each iteration bringing them closer to optimal alignment.</p>
<p>Policy Optimization</p>
<p>Once we have a trained reward model R θ (x, y) that captures human preferences, we can integrate it into a RL framework to optimize a policy π ϕ .In essence, we replace (or augment) the environment's native reward signal with R θ (x, y) so that the agent focuses on producing responses y that humans prefer for a given query x.</p>
<p>In typical RL notation:</p>
<p>• Each state s here can be interpreted as the partial dialogue or partial generation process for the next token (in language modeling).</p>
<p>• Each action a is the next token (or next chunk of text) to be generated.• The policy π ϕ (a | s) is a conditional distribution over the next token, parameterized by ϕ.We seek to find ϕ that maximizes the expected reward under R θ .Concretely, let x be a user query, and let y ∼ π ϕ (• | x) be the generated response.We aim to solve:
max ϕ E x∼X E y∼π ϕ (• | x) R θ (x, y) .
This means that on average, over user queries x and responses y drawn from the policy π ϕ , we want the reward model's score R θ (x, y) to be as high as possible.Policy Gradient and Advantage.The modern algorithms (e.g., PPO [73], GRPO [59], TRPO [160]) rely on policy gradients.Figure 5 presents a structured comparison of the these main RL frameworks.Each framework builds upon different principles for policy learning, reference modeling, and reward computation.Recall that the advantage function A(s, a) quantifies how much better an action a is than the baseline expected return V (s).At a high level, we update the policy π ϕ in the direction that increases π ϕ (a | s) for actions a with positive advantage and decreases it for negative-advantage actions.Formally, the advantage A t at time t can be written as:
A t = Q(s t , a t ) − V (s t ),
where Q(s t , a t ) is the expected future return (sum of future rewards, including R θ ) starting from s t when taking action a t .</p>
<p>When using the reward model R θ : 1) We interpret R θ (x, y) as the immediate or terminal reward for the generated response y.</p>
<p>2) The policy's future returns thus factor in how likely subsequent tokens are to be positively scored by R θ .</p>
<p>3) The advantage function still captures how much better a particular generation step is compared to the baseline performance V (s t ).</p>
<p>The reward model learns relative preferences rather than absolute scores.This avoids the need for calibrated human ratings and focuses on pairwise comparisons.</p>
<p>Odds Ratio Preference Optimization (ORPO)</p>
<p>The simplest method is ORPO [168] which directly optimizing a policy from pairwise human preferences.Instead of first learning a separate reward model and then running standard RL, ORPO updates the policy to increase the likelihood of preferred responses (according to human labels) relative to dispreferred ones.The key idea is to look at the odds ratio:
π ϕ (y j | x) π ϕ (y k | x) ,
where y j is the preferred response and y k is the less-preferred response for a given query x.Pairwise Preference Probability.In many direct preference approaches (e.g., Bradley-Terry style), one writes
P ϕ y j ≻ y k | x = σ ln π ϕ (y j | x) π ϕ (y k | x) = 1 1 + exp ln π ϕ (y k |x) π ϕ (yj |x)
, where σ(•) is the logistic (sigmoid) function.Intuitively, if the policy π ϕ assigns higher probability to y j than to y k , the odds
π ϕ (yj |x)
π ϕ (y k |x) exceed 1, making y j more likely to be the preferred outcome under the model.</p>
<p>In ORPO, one typically defines a negative log-likelihood loss for all pairs {(x, y j ≻ y k )} in the dataset:
L ORPO (ϕ) = − (x, yj ≻y k ) ∈ D log P ϕ y j ≻ y k | x .
Substituting the logistic form gives:
L ORPO (ϕ) = − (x, yj ≻y k ) ∈ D log π ϕ (y j | x) π ϕ (y j | x) + π ϕ (y k | x) ,
which can also be interpreted as maximizing the log odds ratio for the correct (preferred) label in each pairwise comparison.</p>
<p>Interpretation via Odds Ratios.By treating each preference label (y j ≻ y k ) as a constraint on the odds
π ϕ (yj |x) π ϕ (y k |x) ,
ORPO pushes the policy to increase its probability mass on y j while decreasing it on y k .When viewed in logarithmic space:
ln π ϕ (yj |x) π ϕ (y k |x) ,
a higher value corresponds to a greater likelihood of selecting y j over y k .Hence, minimizing L ORPO (ϕ) aligns π ϕ with the human-labeled preferences.</p>
<p>Odds Ratio Preference Optimization (ORPO) is potentially less flexible for combining multiple reward signals.</p>
<p>Proximal Policy Optimization (PPO) in LLMs</p>
<p>A popular method for policy optimization is PPO [73], a strategy adapted to align LLMs with human feedback.Given a policy π θ parameterized by θ and a reward function R, PPO updates the policy by optimizing a clipped objective that balances exploration and stability.Specifically, if r t (θ) = π θ (at|st) π θ ref (at|st) denotes the probability ratio for an action a t in state s t , the clipped PPO objective is:
L PPO (θ) = E t min r t (θ) A t , clip(r t (θ), 1 − ϵ, 1 + ϵ) A t ,
where A t is an estimator of the advantage function and ϵ is a hyperparameter controlling the allowable deviation from the previous policy.A t is computed using Generalized Advantage Estimation (GAE) [169] based on rewards and a learned value function.The clipping objective of PPO restricts how drastically the updated policy distribution can diverge from the original policy.This moderation averts catastrophic shifts in language generation and preserves training stability.</p>
<p>Policy Optimization with KL Penalty.During RL finetuning with PPO, the policy π is optimized to maximize reward while staying close to the base model ρ.The modified reward function includes a KL divergence penalty:
J(π) = E (x,y)∼D r(x, y) − β KL π(•|x) ∥ ρ(•|x) ,
where β controls the penalty strength.The KL term KL(π ∥ ρ) prevents over-optimization to the proxy reward r(x, y) (i.e., reward hacking).</p>
<p>The KL penalty is a regularization, which ensure policy retains the base model's linguistic coherence and avoids degenerate outputs.</p>
<p>Reinforcement Learning from Human Feedback (RLHF)</p>
<p>RLHF [58] refines LLMs through direct human preference signals, making them more aligned with human expectations.The process involves three main steps.First, SFT is performed on a pretrained model using high-quality labeled data to establish strong linguistic and factual capabilities.Second, a reward function R is trained using human-annotated rankings of generated responses, allowing it to predict preferences and provide a scalar reward signal.Third, PPO is employed in the RLHF [58] pipeline by using human-provided preference scores (or rankings) to shape R and thereby guide the policy updates.This ensures that the model prioritizes outputs aligned with human-preferred behavior.The robust performance under conditions of noisy or partial reward signals makes PPO well-suited for text generation tasks, where large action spaces and nuanced reward definitions are common.</p>
<p>Reinforcement Learning from AI Feedback (RLAIF)</p>
<p>RLAIF [95] is an alternative to RLHF that replaces human annotation with AI-generated feedback.Instead of relying on human-labeled preferences, RLAIF employs a secondary, highly capable language model to generate preference labels, which are then used to train a reward model.This reward model guides reinforcement learning-based fine-tuning of the target model.RLAIF reduces the cost and time required for data collection by eliminating the need for human annotators.It enables large-scale model alignment without requiring extensive human intervention while maintaining high performance and alignment.Empirical studies indicate that RLAIF [95,170] is a scalable and efficient alternative to RLHF, making it a promising direction for reinforcement learning-driven language model optimization.</p>
<p>The clipping mechanism constrains policy updates to remain within a safe trust region, which is crucial when dealing with complex, highdimensional action spaces.</p>
<p>Trust Region Policy Optimization (TRPO)</p>
<p>TRPO [160] is another widely used policy optimization method, preceding PPO and sharing its fundamental goal: improving stability in reinforcement learning updates.TRPO optimizes policy updates while ensuring they remain within a constrained trust region, measured by KL divergence.</p>
<p>Instead of using a clipped objective like PPO, TRPO enforces a hard constraint on policy updates by solving the following optimization problem:
max θ E t π θ (a t | s t ) π θold (a t | s t ) A t
subject to the constraint:
E t [D KL (π θold (• | s t )∥π θ (• | s t ))] ≤ δ.
where δ is a hyperparameter that controls how much the new policy can diverge from the old one.</p>
<p>Unlike PPO, which approximates this constraint using clipping, TRPO directly solves a constrained optimization problem, ensuring each update does not move too far in policy space.However, solving this constrained problem requires computationally expensive second-order optimization techniques like conjugate gradient methods, making TRPO less efficient for large-scale models like LLMs.In practice, PPO is preferred over TRPO due to its simplicity, ease of implementation, and comparable performance in large-scale applications like RLHF.However, TRPO remains an important theoretical foundation for stable policy optimization in deep reinforcement learning.</p>
<p>Direct Preference Optimization (DPO)</p>
<p>DPO [162] is a recently proposed method for training LLMs from human preference data without resorting to the traditional RL loop (as in RLHF with PPO).Instead of learning a separate reward function and then running policy-gradient updates, DPO directly integrates human preference signals into the model's training objective.So instead of the above PPO objective, DPO instead constructs an objective that directly pushes up the probability of a chosen (preferred) response (y + ) while pushing down the probability of a less-preferred response (y − ), all within a single log-likelihood framework.Rather than bounding policy changes with clip, the DPO loss uses the difference between log probabilities of 'winning' vs. 'losing' responses.This explicitly encodes the user's preference in the updated parameters.</p>
<p>Here, π θ is the learnable policy, π ref is a reference policy (often the SFT-trained model), σ(•) is the sigmoid function, β is a scaling parameter, and D train is a dataset of triplets (x, y + , y − ) where y + is the preferred output over y − .
L DPO (θ) = E ((x,y + ),y − )∼Dtrain σ β log π θ (y + | x) π ref (y + | x) −β log π θ (y − | x) π ref (y − | x) .
The key insight is that an LLM can be treated as a "hidden reward model": we can reparameterize preference data so that the model's own log probabilities reflect how preferable one response is over another.By directly adjusting the log-likelihood of more-preferred responses relative to less-preferred ones, DPO sidesteps many complexities of RL-based methods (e.g., advantage functions or explicit clipping).The advantage function remains a core concept to determine which actions (token choices) are better than the baseline at each step.</p>
<p>Offline Reasoning Optimization (OREO)</p>
<p>OREO [171] is an offline reinforcement learning method designed to enhance LLMs' multi-step reasoning by optimizing the soft Bellman equation [109].Unlike DPO, which relies on paired preference data, OREO uses sparse rewards based on final outcomes (e.g., correctness of reasoning chains) and jointly trains a policy model π θ and a value function V ϕ for fine-grained credit assignment.The core objective minimizes the inconsistency in the soft Bellman equation:
V ϕ (s t ) − V ϕ (s t+1 ) = r(s t , a t ) − β log π θ (a t | s t ) π ref (a t | s t ) ,
where s t+1 = f (s t , a t ) is the next state, r is the sparse reward, and β controls KL regularization.The policy and value losses are:
L V (ϕ) = 1 T T −1 t=0 V ϕ (s t ) − R t + β i≥t log π θ (a i | s i ) π ref (a i | s i ) 2 , L π (θ) = 1 T T −1 t=0 V ϕ (s t ) − R t + β log π θ (a t | s t ) π ref (a t | s t ) 2 + αL reg ,
where
L</p>
<p>Group Relative Policy Optimization (GRPO)</p>
<p>GRPO [59] simplifies the PPO framework by eliminating the need for a separate value function.Instead, GRPO estimates the baseline from the average reward of multiple sampled outputs for the same question.The primary contribution in GRPO is that it removes the need for a separate value model (critic model) and instead estimates the baseline reward from a group of sampled LLM outputs.This significantly reduces memory usage and stabilizes policy learning.The approach also aligns well with how reward models are trained, i.e., by comparing different LLM-generated outputs rather than predicting an absolute value.</p>
<p>For each question q, GRPO samples a group of outputs {o 1 , o 2 , . . ., o G } from the old policy π old θ .A reward model is used to score each output in the group, yielding rewards {r 1 , r 2 , . . ., r G }.The rewards are normalized by subtracting the group average and dividing by the standard deviation: ri = r i − mean(r) std(r) .</p>
<p>The advantage Âi,t for each token in the output is set as the normalized reward ri .</p>
<p>GRPO first samples a question q ∼ P (Q) and then samples G outputs {o i } G i=1 from π old θ (O | q).Define the per-output objective as
J(o i , θ, q) = 1 |o i | |oi| t=1 min r ratio,i,t Âi,t , clip r ratio,i,t , 1 − ϵ, 1 + ϵ Âi,t − β D KL π θ ∥ π ref .
Then, the GRPO objective becomes
J GRP O (θ) = E q∼P (Q) 1 G G i=1 J(o i , θ, q) ,
where the probability ratio is defined as
r ratio,i,t ≜ π θ (o i,t | q, o i,&lt;t ) π old θ (o i,t | q, o i,&lt;t )
.</p>
<p>where ϵ is a clipping hyperparameter akin to PPO, and β adjusts the KL-divergence penalty encouraging the new policy π θ not to deviate excessively from a reference policy π ref , which is typically the initial supervised fine-tuned (SFT) model [172,173].GRPO can be applied in two modes: outcome supervision and process supervision.</p>
<p>Outcome Supervision: Provides a reward only at the end of each output.The advantage Âi,t for all tokens in the output is set as the normalized reward ri .ri = r i − mean(r) std(r) .</p>
<p>Process Supervision: Provides a reward at the end of each reasoning step.The advantage Âi,t for each token is calculated as the sum of the normalized rewards from the following steps:
Âi,t = index(j)≥t ri,index(j) ,
where index(j) is the end token index of the j-th step.Overall, GRPO serves as an efficient alternative to classic actorcritic frameworks in DeepSeekR1 [40] by leveraging grouplevel advantages, thereby reducing training costs without sacrificing the capacity to distinguish fine-grained differences among candidate responses.</p>
<p>Fine-grained per-step rewards enable the model to effectively identify and reinforce highquality responses, boosting overall performance in complex, multi-step reasoning tasks.</p>
<p>Multi-Sample Comparison Optimization</p>
<p>Instead of relying solely on single-pair comparisons, multisample comparison optimization [174] approach compares multiple responses simultaneously to promote diversity and mitigate bias.Specifically, given a set of responses {y 1 , y 2 , . . ., y n } for a query x, the probability of observing the ranking y 1 &gt; y 2 &gt; • • • &gt; y n is determined by the product
P (y 1 &gt; y 2 &gt; • • • &gt; y n ) = i e R(x,yi) j e R(x,yj ) .
In this formulation, each response y i is jointly evaluated in the context of all other responses, ensuring that comparisons are not isolated pairwise events but rather part of a broader ranking framework that helps capture more nuanced preferences and reduces potential biases.</p>
<p>Pure RL Based LLM Refinement</p>
<p>The work from Guo et al. (2025) [40] introduces two main models: DeepSeek-R1-Zero and DeepSeek-R1.</p>
<p>• DeepSeek-R1-Zero operates with a purely Reinforcement Learning approach, excluding any SFT.• DeepSeek-R1 incorporates cold-start data and applies a multi-stage training pipeline.</p>
<p>The methodology encompasses several steps (See Figure 2 in GRPO for main steps): collecting cold-start data, performing RL training, carrying out SFT, using distillation to transfer knowledge to smaller models, and addressing specific challenges such as language mixing and readability.This multistage pipeline ensures robustness and alignment with human preferences, while distillation enables efficient deployment of smaller models without significant performance loss.</p>
<p>Cold-Start RL Phase</p>
<p>The process begins with a cold-start RL phase, where a small amount of curated data is gathered to fine-tune an initial, or base, model.Following this preliminary fine-tuning, RL is conducted-often via algorithms like GRPO until convergence.The cold-start phase is critical for stabilizing the model before full RL training, preventing instability that can arise from purely RL-driven updates.The cold-start data preparation focuses on capturing human-readable reasoning patterns to prevent instability from purely RL-driven updates.This step generates CoT-style examples with consistent &lt; reasoning_process &gt; and &lt; summary &gt; fields, usually involving thousands of carefully curated samples.Structured CoT formats and consistent fields ensure clarity and robustness in the model's reasoning outputs, reducing errors and improving interpretability [8,175,176,177].</p>
<p>Providing CoT reasoning traces before RL training establishes a stronger foundation for reasoning tasks, enhancing both robustness and interpretability of outputs.</p>
<p>Rejection Sampling and Fine-tuning</p>
<p>This concept is also used in WebGPT [81].Once RL stabilizes, a rejection sampling mechanism is employed to generate highquality responses that are subsequently filtered for correctness, clarity, and other quality metrics.These filtered responses are then blended with additional datasets to produce a new, larger corpus for Supervised Fine-Tuning.Rejection sampling ensures that only high-quality outputs are used for further training, enhancing the model's overall performance and reliability.After RL converges for high-stakes reasoning tasks, rejection sampling is used to filter a large number of generated outputs, expanding the training set.These newly generated reasoning examples (potentially up to hundreds of thousands in quantity) are mixed with existing SFT data to create a combined dataset of substantial size (often around 800k samples).Rejection sampling and dataset expansion significantly enhance the model's coverage of general tasks while preserving its reasoning proficiency.</p>
<p>Reasoning-Oriented RL</p>
<p>The reasoning-oriented RL leverages GRPO [59], which samples a group of outputs from the current policy and computes rewards and advantages for each output.Rewards may be computed via rule-based checks, e.g., ensuring correct solutions in math or code tasks, enforcing structured CoT tags, and penalizing undesired language mixing.GRPO group-based sampling and reward computation ensure that the model prioritizes high-quality, structured outputs, enhancing its reasoning capabilities.</p>
<p>Second RL Stage for Human Alignment</p>
<p>A second RL stage further aligns the model with broader human preferences (helpfulness, harmlessness, creativity, etc.) by introducing additional reward signals and prompt distributions.The second RL stage ensures the model aligns with human values, making it more versatile and contextually aware.After re-training the base model on this combined dataset, a second round of RL can be conducted to align the model more closely with human preferences (e.g., for helpfulness and harmlessness).This RL stage fine-tunes the model to better align with human values, ensuring outputs are not only accurate but also contextually appropriate.</p>
<p>Distillation for Smaller Models</p>
<p>Finally, distillation techniques are used to transfer the refined capabilities of the main model to smaller architectures, enabling more efficient deployments without sacrificing much performance.It allows smaller models to inherit advanced reasoning capabilities, making them competitive on challenging benchmarks without the computational costs of full-scale RL training.Finally, distillation plays a pivotal role: the topperforming model, DeepSeek-R1 [40], serves as a teacher to smaller architectures (e.g., Qwen or Llama families, ranging from 1.5B to 70B parameters).This transfer allows the smaller models to inherit advanced reasoning capabilities, making them competitive on challenging benchmarks without incurring the computational costs of full-scale RL training.</p>
<p>Distillation democratizes advanced reasoning capabilities, enabling smaller models to achieve competitive performance with reduced computational overhead.</p>
<p>Supervised Finetuning in LLMs</p>
<p>As shown in Figure 2, finetuning forms a basic component of LLM post-training recipes.In this section, we summarize the different types of LLM fine-tuning mechanisms.</p>
<p>Instruction finetuning</p>
<p>In instruction finetuning, a model is trained on curated pairs of instruction (prompt) and response (completion).The main goal is to guide the LLM to follow a user-provided instruction accurately and helpfully, regardless of the task domain.This usually involves compiling large, diverse instruction-response datasets covering many task types (e.g., summarization, QA, classification, creative writing).Models such as T0 [178], FLAN [179], Alpaca [180], Vicuna [181] and Dolly [182] demonstrate how instruction-finetuned LLMs can outperform base models on zero-shot or few-shot tasks by virtue of their enhanced instruction-following abilities.</p>
<p>Dialogue (Multi-turn) Finetuning</p>
<p>Some LLMs undergo dialogue-style finetuning to better handle multi-turn conversations.Different from instruction tuning described above, here the data takes the form of a continuous dialogue (multi-turn conversations) instead of a single prompt-response pair.In this approach, training data consists of chat transcripts with muliple user queries and system responses, ensuring the model learns to maintain context across turns and produce coherent replies.Models like LaMDA [183] and ChatGPT [39] highlight how dialogue-tuned LLMs can feel more interactive and context-aware.While dialogue finetuning can overlap with instruction finetuning (because many instructions come in a chat format), specialized conversation data often yields more natural, multi-turn user experiences.</p>
<p>CoT Reasoning finetuning</p>
<p>Chain-of-Thought (CoT) reasoning finetuning teaches models to produce step-by-step reasoning traces instead of just final answers.By exposing intermediate rationales or thoughts, CoT finetuning can improve both interpretability and accuracy on complex tasks (e.g., math word problems, multihop QA).In practice, CoT finetuning uses supervised reasoning annotations (often handcrafted by experts) to show how a solution unfolds.Notable early work includes Chainof-Thought Prompting [8] and Self-Consistency [184], which initially applied the idea to prompting; subsequent efforts (e.g., Chain-of-Thought Distillation [185]) adapt it to a full finetuning or student-teacher paradigm.These efforts have also been extended to the multimodal domain, e.g., LlaVA-CoT [186] and LlamaV-o1 [187] where image, QA and CoT reasoning steps are used in LLM finetuning.</p>
<p>Model</p>
<p>Category Source Description</p>
<p>Parameter-Efficient Fine-Tuning &amp; Model Compression</p>
<p>LoRA [60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.QLoRA [188] Quantized Adaptation Link Combines 4-bit quantization with LoRA to enable fine-tuning on consumer GPUs GPTQ [189] Post-Training Quantization Link Optimal 4-bit quantization method for GPT-style models with minimal loss SparseGPT [190] Pruning Link One-shot pruning that preserves model quality with compensation.PEFT (HF) [191] Unified Fine-Tuning Link Library integrating LoRA, prefix tuning, and other parameter-efficient methods BitsAndBytes [192] Low-Precision Training Link Enables 8-bit optimizers and 4-bit quantization for memory-efficient training AdaLoRA [193] Adaptive Adaptation Link Dynamically allocates parameter budget between layers during fine-tuning P-Tuning v2 [194] Prompt Optimization Link Learns continuous prompt embeddings through deep prompt tuning</p>
<p>Data Management &amp; Preprocessing</p>
<p>HF Datasets [195] Data Processing Link Unified API for 30k+ datasets with streaming, versioning, and preprocessing WebDataset [196] Data Streaming Link Efficient tar-based sharding format for petascale distributed training DVC [197] Data Versioning Link Git-like version control for datasets and machine learning pipelines Apache Arrow [198] Memory Format Link Language-agnostic columnar memory format for zero-copy data access Zstandard [199] Compression Link High-speed compression algorithm for training data storage/transfer Cleanlab [200] Data Quality Link Automatic detection of label errors and outliers in training datasets</p>
<p>Distributed Training &amp; Optimization</p>
<p>DeepSpeed [201] Training Optimization Link ZeRO parallelism, 3D parallelism, and memory optimizations for giant models Megatron-LM [202] Model Parallelism Link NVIDIA's optimized framework for large transformer model training Colossal-AI [203] Heterogeneous Training Link Unified system supporting multiple parallelization strategies Horovod [204] Distributed Training Link MPI-inspired framework for multi-GPU/multi-node synchronization Ray [205] Distributed Computing Link Universal framework for distributed Python applications at scale</p>
<p>Efficient Inference &amp; Deployment</p>
<p>vLLM [206] Serving Optimization Link Paged attention implementation for high-throughput LLM serving TensorRT [207] GPU Optimization Link NVIDIA's inference optimizer with kernel fusion and quantization support Triton [208] Serving Framework Link Production-grade serving with concurrent model execution support ONNX [209] Cross-Platform Link Unified inference engine with hardware-specific optimizations OpenVINO [210] Intel Optimization Link Runtime for Intel CPUs/iGPUs with pruning/quantization support XNNPACK [211] Mobile Inference Link Highly optimized floating-point kernels for ARM CPUs Groq [212] AI Accelerator Link Deterministic low-latency inference via custom tensor streaming processor</p>
<p>Integrated Development Ecosystems</p>
<p>HF Ecosystem [213] Full Stack Link Transformers + Datasets + Accelerate + Inference Endpoints DeepSpeed [201] Training/Inference Link Microsoft's end-to-end solution for billion-parameter models PyTorch [214] Unified Framework Link Native LLM support via torch.compileand scaled dot-product attention LLM Reasoners [215] Advanced Reasoning Link Enhances LLM reasoning capabilities using advanced search algorithms.</p>
<p>TABLE 2: Comprehensive Overview of Methods and Frameworks employed in Modern LLMs</p>
<p>Domain-Specific (Specialized) Finetuning</p>
<p>When an LLM needs to excel in a specific domain (e.g., biomedicine, finance, or legal), domain-specific finetuning is used.Here, a curated corpus of domain-relevant text and labeled examples is employed to finetune the LLM.For instance, BioGPT [71] and BiMediX [216] specialize in biomedical literature, FinBERT [217] for financial texts, ClimatGPT [218,219] for climate and sustainability and CodeT5 [220] for code understanding.Supervised finetuning in these domains often includes classification, retrieval, or QA tasks with domain-specific data, ensuring the model's parameters adapt to the specialized language and concepts of the field.Domainspecific finetuning is also extended to vision-language models such as, [221] finetuned on remote sensing imagery, [222] on medical imaging modalities, [223,224,225] on spatiotemporal video inputs, and [226] adapted for chart understanding.</p>
<p>Distillation-Based Finetuning</p>
<p>Large 'teacher' models are sometimes used to produce labeled data or rationales, which a smaller 'student' model finetunes on, this is generally called knowledge distillation [227,228].</p>
<p>In the context of LLMs, CoT Distillation [185] is one example where a powerful teacher LLM generates intermediate reasoning steps, and the student LLM is finetuned to reproduce both the final answer and the reasoning chain.</p>
<p>Step-by-step distillation [229] generates descriptive rationales alongside final answers to train smaller models through distillation with smaller datasets.This approach can yield lighter, faster models that retain much of the teacher's performance, even in zero-shot or few-shot tasks [230].</p>
<p>Preference and Alignment SFT</p>
<p>While RLHF is not purely supervised, it starts with a supervised preference or alignment finetuning stage.This stage uses human-labeled or human-ranked examples to teach the model about desirable vs. undesirable outputs (e.g., safe vs. toxic).By training on these explicit preferences, the model becomes more aligned with user values, reducing harmful or off-topic completions.Works like InstructGPT [58] illustrate how supervised preference data is critical before reward model training and RL updates begin.</p>
<p>Efficient Finetuning</p>
<p>Fully finetuning a LLM can be computationally and memoryintensive, particularly as model sizes grow into the tens or hundreds of billions of parameters.To address these challenges, parameter-efficient finetuning (PEFT) techniques introduce a small set of trainable parameters or learnable prompts while leaving most of the model weights frozen.Approaches such as LoRA [60], Prefix Tuning [231], and Adapters [232] exemplify this strategy by injecting lightweight modules (or prompts) in specific layers, thus significantly reducing the memory footprint.Figure 4 illustrates how these techniques fit into a broader ecosystem that involves system-level optimizations, data management, and evaluation strategies for LLMs.In particular, PEFT approaches can be combined with quantization and pruning methods [190,188] to further minimize memory usage and compute overhead, enabling finetuning on smaller GPUs or even consumer-grade hardware.For instance, QLoRA unifies 4-bit quantization with low-rank adaptation, while BitsAndBytes provides 8-bit optimizers to make LLM training more practical in constrained environments (Table 2).</p>
<p>Moreover, these PEFT methods still require supervised data to guide the adaptation process, but the reduction in the number of trainable parameters makes it more feasible to use in-domain or task-specific datasets.This is especially valuable for specialized domains (e.g., medical or software development), where data might be limited or expensive to annotate.As shown in Table 2, PEFT (HF) integrates several of these approaches (LoRA, prefix tuning, and more) into a single library, streamlining deployment in both research and production settings.</p>
<p>Combining efficient tuning designs like LoRA and QLoRA with system and data optimizations (Figure 4) enables cost-effective LLM adaptation for tasks like domain-specific text generation, without expensive full fine-tuning.</p>
<p>Test-time Scaling Methods</p>
<p>While RL fine-tunes the model's policy, test-time scaling (TTS) enhances reasoning during inference typically without model updates.Figure 5 presents a taxonomy of TTS methods, categorizing them based on their underlying techniques.</p>
<p>Beam Search</p>
<p>Beam search was first introduced in the context of speech recognition [233].It gained prominence as a decoding strategy for sequence models and was later adopted in neural machine translation and speech systems [234].With the popularity of LLMs, this algorithm has been used for approximate search in many text generation tasks.</p>
<p>The concept of Beam search is similar to pruned breadthfirst search, where top N highest-probability partial sequences (the 'beam') are kept at each step, discarding lower-probability paths.By limiting the beam width (N), it manages the exponential search space while aiming to find a near-optimal sequence.These beams are expanded at each decoding step to find multiple probable paths.In reasoning LLMs, such paths allow us to systematically explore multiple reasoning chains in parallel, focusing on the most promising ones.This ensures that high-likelihood reasoning steps are considered, which can improve the chances of finding a correct and coherent solution compared to greedy decoding.It has traditionally been used in tasks such as translation, summarization, and code generation, where the goal is a highly probable correct sequence [93].</p>
<p>While modern LLMs often favor stochastic sampling (e.g., temperature sampling) to promote diversity in generated text, beam search is still a valuable technique for structured reasoning problems.For example, the Tree-of-Thoughts framework [84] allows plugging in different search algorithms to explore a tree of possible 'thoughts' or reasoning steps; usually a beam search (with beam width b) is used to maintain the b most promising states at each reasoning step.Here, beam search is used to systematically explore solution steps for tasks like mathematical puzzles and planning problems, pruning less promising reasoning branches and thus improving the model's problem-solving accuracy.Beam search remains a strong baseline for test-time reasoning when one wants the model to output the single most likely reasoning path or answer under the model's learned distribution.</p>
<p>Best-of-N Search (Rejection Sampling)</p>
<p>Best-of-N (BoN) [235] search generates N candidate outputs (usually via sampling) and then picks the best one according to a chosen criterion (e.g., a reward model or the model's own likelihood) [236,237,238].Conceptually, this is an application of rejection sampling: one draws multiple samples and rejects all but the top-rated result.Unlike Beam Search [233,234], which incrementally expands and prunes partial hypotheses, BoN simply samples full solutions independently, allowing for greater diversity but at a higher computational cost.Beam Search systematically aims for the most probable sequence, while BoN may capture high-quality but lower-probability solutions through brute-force sampling.</p>
<p>Beam search (effective for harder questions) outperforms best-of-N sampling at low compute budgets, while best-of-N scales better for easier tasks.</p>
<p>During LLM inference, BoN is used to enhance correctness or alignment without retraining the model.By sampling multiple answers and selecting the top candidate (e.g., via a reward model or a checker), BoN effectively boosts accuracy on tasks like QA or code generation.BoN is easy to understand and implement and is almost hyper-parameter-free, with N being the only parameter that can be adjusted at inference.In reinforcement learning contexts, BoN sampling can serve as a baseline exploration mechanism i.e., to generate many rollouts, pick the best outcome according to the learned reward, and proceed, although at increased computational overhead.OpenAI's WebGPT used BoN to pick the best response via a reward model, yielding strong QA performance [81].BoN is also used as a simple alignment method that is highly competitive with other post-training techniques e.g., RLHF [58] and DPO [78].Studies have shown BoN can approach or match RLHF results when guided by a sufficiently robust reward model [82,239].Alternatives such as speculative rejection [240] build on this idea and utilize a better reward model to improve efficiency.The studies also highlight issues of reward hacking if the (proxy) reward function used for BoN is imperfect [241] or instability issues if the N parameter gets very large.</p>
<p>Choice of either process reward models with beam search vs best-of-N depends on the difficulty and compute budget.</p>
<p>Compute-Optimal Scaling</p>
<p>The Compute-Optimal Scaling Strategy (COS) [83] is a dynamic method designed to allocate computational resources efficiently during inference in LLMs, optimizing accuracy without unnecessary expense.Instead of applying a uniform sampling strategy across all inputs, this approach categorizes prompts into five difficulty levels-ranging from easy to hard-either by leveraging oracle difficulty (ground-truth success rates) or model-predicted difficulty (e.g., verifier scores from Preference Ranking Models).Once categorized, the strategy adapts compute allocation: easier prompts undergo sequential refinement, where the model iteratively refines its output to improve correctness, while harder prompts trigger parallel sampling or beam search, which explores multiple response variations to increase the likelihood of finding a correct solution.This dual approach balances exploration (for challenging inputs) and refinement (for nearcorrect responses), ensuring optimal performance per unit of computational effort.Remarkably, this method achieves four times lower compute usage than traditional best-of-N sampling while maintaining equivalent performance.The key insight is that by matching computational strategy to problem difficulty, it avoids wasted resources on trivial cases while ensuring sufficient sampling diversity for complex tasks.In essence, it functions as a "smart thermostat" for LLM inference, dynamically adjusting computational effort in response to input complexity, leading to a more efficient and cost-effective deployment of large-scale language models.</p>
<p>COS achieves 4× efficiency gains over bestof-N baselines by optimally balancing sequential/parallel compute. Beam search + revisions</p>
<p>outperform larger models on easy/intermediate questions.</p>
<p>Chain-of-thought prompting</p>
<p>CoT prompting induces LLMs to produce intermediate reasoning steps rather than jumping directly to the final answer.By breaking down problems into logical sub-steps, CoT taps into a model's latent ability to perform multi-step inferences, significantly improving performance on tasks like math word problems, logical puzzles, and multi-hop QA.</p>
<p>Wei et al. [8] demonstrated CoT's effectiveness on arithmetic and logic tasks, showing large gains over direct prompting.Kojima et al. [242] introduced Zero-Shot CoT, revealing that even adding a simple phrase like "Let's think step by step" can trigger coherent reasoning in sufficiently large models.Subsequent works (e.g., Wang et al., 2022 [184]) combined CoT with sampling-based strategies (Self-Consistency) for even higher accuracy.As described in Sec.5.4, CoT format data have also been used for SFT and are shown to help reshape the model responses to be more step-by-step.</p>
<p>Fine-tuning models to revise answers sequentially allows them to build on previous attempts, improving accuracy over time.This approach is particularly effective for easier questions, while parallel sampling (exploration) proves more beneficial for harder ones.</p>
<p>Self-Consistency Decoding</p>
<p>Self-Consistency is a decoding strategy introduced by Wang et al. [243].It was proposed as an alternative to simple greedy decoding for chain-of-thought prompts.It built upon the idea of sampling multiple distinct reasoning paths for a question and was the first to show that marginalizing over those paths can significantly improve accuracy on arithmetic and reasoning problems.In other words, it allows the model to think in many ways and then trust the consensus, which improves correctness in many reasoning scenarios.</p>
<p>The self-consistency method works by sampling a diverse set of reasoning chains from the model (via prompt engineering to encourage different CoTs, and using temperature sampling) and then letting the model output a final answer for each chain.Instead of trusting a single chain, the method selects the answer that is most consistent across these multiple reasoning paths, effectively a majority vote or highest probability answer after marginalizing out the latent reasoning.The intuition is that if a complex problem has a unique correct answer, different valid reasoning paths should converge to that same answer.By pooling the outcomes of many chains, the model can "decide" which answer is most supported.In application, one might sample, e.g., 20 CoTs for a math problem and see what final answer appears most frequently; that answer is then taken as the model's output.This approach turns the one-shot CoT process into an ensemble where the model cross-verifies its answers.It is especially useful for arithmetic and commonsense reasoning tasks where reasoning diversity helps.</p>
<p>Smaller models with test-time compute can outperform much larger models in certain scenarios.</p>
<p>Self-consistency is often combined with other methods: e.g., sampling multiple chains and then applying a verifier to the most common answer.Its strength lies in requiring no new training, only extra sampling, making it a popular testtime scaling strategy to obtain more reliable answers from LLMs.It has also inspired other variants, e.g., Universal Self-Consistency [244] extend the original idea (which worked only with majority vote on single final answer) to more general generation tasks such as summarization and open-ended QA.</p>
<p>Tree-of-thoughts</p>
<p>ToT framework [84] generalizes the chain-of-thought approach by allowing the model to branch out into multiple possible thought sequences instead of following a single linear chain.It thus formulates the problem of language-model reasoning as a tree search, drawing on classic AI search methods inspired by human problem-solving [245,37].Tree of Thoughts treats intermediate reasoning steps as "nodes" in a search tree and uses the language model to expand possible next steps (thoughts) from a given state.Rather than sampling one long reasoning path, the model explores a tree of branching thoughts and can perform lookahead and backtracking.At each step, the LLM might generate several candidate next thoughts, and a heuristic or value function evaluates each partial solution state.Then a search algorithm (e.g., depth-first, breadth-first, beam search) navigates this tree, deciding which branches to explore further.This approach allows systematic exploration of different reasoning strategies: if one path leads to a deadend, the model can return to an earlier state and try a different branch (unlike standard CoT which commits to one line of reasoning).In effect, ToT is an iterative prompting procedure where the model generates thoughts, evaluates them, and refines its approach, mimicking how a human might mentally map out various ways to solve a problem.ToT is especially useful for complex problems like puzzles, planning tasks, or games where multiple steps and strategic exploration are needed and outperforms simpler CoT methods by systematically searching through the solution space.It provides a flexible framework -one can plug in various generation strategies (e.g.sampling vs. prompting) and search algorithms (BFS, DFS, A*, MCTS) depending on the task.Although more computationally heavy, ToT shows that allocating extra "thinking time" (compute) to explore alternatives can yield significantly better reasoning and planning performance.It has spawned follow-up research aiming to improve or utilize it for better reasoning e.g., multi-agent systems have been combined with ToT: different LLM "agents" generate thoughts in parallel and a validator agent prunes incorrect branches, leading to improved accuracy over the single-agent ToT [246].</p>
<p>Inference-time computation for LLMs can outperform scaling model parameters, especially for challenging reasoning tasks like math problems.</p>
<p>Graph of Thoughts</p>
<p>The Graph of Thoughts (GoT) [247] framework extends the ToT by allowing more flexible and efficient reasoning processes through graph-based structures rather than strict hierarchical trees.Thought representation differs between the two approaches: in ToT, each step in reasoning is structured as a node in a tree with fixed parent-child relationships, whereas GoT represents thoughts as nodes in a graph, enabling more adaptable dependencies and interconnections.</p>
<p>In terms of thought expansion strategies, ToT follows a traditional approach where multiple thought candidates are generated at each step, explored using tree-based search strategies, and pruned based on heuristics before selecting the most optimal path.In contrast, GoT incorporates graph-based thought expansion, allowing thoughts to interconnect dynamically.This enables three key transformations: aggregation (merging multiple solutions into a unified answer), refinement (iteratively improving thoughts over time), and generation (producing diverse candidates).Instead of navigating through a rigid hierarchy, GoT prioritizes thoughts using a volume metric and explores paths optimally, reducing unnecessary computations.</p>
<p>A critical limitation of ToT is its restricted backtracking-once a branch is discarded, it is not reconsidered.GoT overcomes this by allowing iterative refinement, where previous thoughts can be revisited, modified, and improved upon.This iterative nature is particularly useful in complex reasoning tasks where initial thoughts may require adjustments.Moreover, computational efficiency in GoT is significantly improved by reducing redundant calculations through the merging of partial solutions.</p>
<p>GoT enhances problem-solving efficiency and adaptability, making it superior to ToT for tasks requiring complex reasoning.</p>
<p>Confidence-based Sampling</p>
<p>In confidence-based sampling, the language model generates multiple candidate solutions or reasoning paths and then prioritizes or selects among them based on the model's own confidence in each outcome [248].This can happen in two ways: (a) Selection: Generate N outputs and pick the one with the highest log probability (i.e., the model's most confident output).This is essentially best-of-N by probability -the model chooses the answer it thinks is most likely correct.(b) Guided exploration: When exploring a reasoning tree or multi-step solution, use the model's token probabilities to decide which branch to expand (higher confidence branches are explored first).In other words, the model's probability estimates act as a heuristic guiding the search through solution space [249].Compared to pure random sampling, confidencebased methods bias the process toward what the model believes is right, potentially reducing wasted exploration on lowlikelihood (and often incorrect) paths.</p>
<p>Confidence-based strategies have been incorporated at inference time e.g., a tree-based search for LLM generation [248] assigns each possible completion (leaf) a confidence score.The algorithm samples leaves in proportion to these confidence scores to decide which paths to extend [272].Similarly, some reasoning approaches use the model's estimated likelihood of an answer to decide when to halt or whether to ask a followup question -essentially if the model's confidence is low, it might trigger further reasoning (a form of self-reflection).Confidence-based selection is also used in ensemble settings: e.g., an LLM may generate multiple answers and a secondary model evaluates the confidence of each answer being correct, picking the answer with the highest confidence.This was explored in tasks like medical Q&amp;A, where an LLM gave an answer and a confidence score, and only high confidence answers were trusted or returned [250].</p>
<p>Search Against Verifiers</p>
<p>This verification approach [251] in LLMs enhances answer quality by generating multiple candidate responses and selecting the best one using automated verification systems.This approach shifts focus from increasing pre-training compute to optimizing test-time compute, allowing models to "think longer" during inference through structured reasoning steps or iterative refinement.The method involves two main steps: Generation: The model (or "proposer" produces multiple answers or reasoning paths, often using methods like hightemperature sampling or diverse decoding.Verification: A verifier (e.g., a reward model) evaluates these candidates based on predefined criteria, such as correctness, coherence, or alignment with desired processes.Verifiers are categorized based on their evaluation focus:</p>
<p>1) Outcome Reward Models (ORM): Judge only the final answer (e.g., correctness of a math solution).</p>
<p>2) Process Reward Models (PRM): Evaluate the reasoning steps (e.g., logical coherence in a thought chain), providing granular feedback to prune invalid paths.Several techniques fall under this paradigm, enhancing verification-based optimization.Best-of-N Sampling involves generating multiple answers and ranking them via a verifier (ORM/PRM), selecting the highest-scoring one, making it a simple yet effective approach for improving answer correctness.Beam Search with PRM tracks top-scoring reasoning paths (beams) and prunes low-quality steps early, similar to Tree of Thought approaches, balancing breadth and depth in reasoning path exploration.Monte Carlo Tree Search balances exploration and exploitation by expanding promising reasoning branches, simulating rollouts, and backpropagating scores, providing an optimal trade-off between search depth and verification confidence.Majority Voting (Self-Consistency) aggregates answers from multiple samples and selects the most frequent one, avoiding explicit verifiers, which works well in settings where consistency across multiple responses indicates correctness.</p>
<p>ORM is suitable for tasks where correctness is binary (right/wrong) and can be easily assessed.</p>
<p>PRM is useful in multi-step reasoning, ensuring intermediate steps follows logical progression.</p>
<p>Self-Improvement via Refinements</p>
<p>This approach refers to the ability of LLMs to enhance their outputs through self-evaluation and revision iteratively.This process enables models to refine their responses dynamically during inference rather than relying solely on pre-trained weights.One notable method is Self-Refinement [252], where an LLM generates an initial response, critiques it, and then refines the output based on its self-generated feedback.This iterative process continues until the model achieves a satisfactory result.Such techniques have been shown to improve performance on various tasks, including mathematical reasoning and code generation.This process follows these key steps: a) Initial Generation: The model produces an answer or reasoning path.b) Self-Critique: The model reviews its own response and identifies errors, inconsistencies, or areas for improvement.c) Refinement: The model adjusts its response based on the critique and generates an improved version.d) Iteration: The process repeats until the output meets a predefined quality threshold or stops improving.</p>
<p>Another approach is called Self-Polish [253], where the model progressively refines given problems to make them more comprehensible and solvable.By rephrasing or restructuring problems, the model enhances its understanding and provides more accurate solutions.Self-Polish involves progressive refinement of problem statements to make them more comprehensible and solvable.The model first rephrases or restructures the problem for better clarity, then breaks down complex queries into simpler sub-problems and refines ambiguous inputs to ensure precise understanding.By restructuring problems before solving them, the model improves its comprehension and generates more accurate solutions.</p>
<p>Self-improvement methodologies represent a paradigm shift in LLM optimization, emphasizing active reasoning and internal feedback over static pre-training.By iterating on their own responses, models achieve greater consistency and accuracy across a wide range of applications.</p>
<p>Monte Carlo Tree Search</p>
<p>MCTS [254] is based on the application of Monte Carlo simulations to game-tree search.It rose to prominence with successes in games, notably, it powered AlphaGo [255] in 2016 by searching possible moves guided by policy and value networks.This, as well as the application to other board and video games, demonstrates the power of MCTS for sequential decision-making under uncertainty.</p>
<p>MCTS is a stochastic search algorithm that builds a decision tree by performing many random simulations.It is best known for finding good moves in game states, but it can be applied to any problem where we can simulate outcomes.The algorithm iteratively: (a) Selects a path from the root according to a heuristic (like UCT [256], which picks nodes with a high upperconfidence bound), (b) Expands a new node (a previously unvisited state) from the end of that path, (c) Simulates a random rollout from that new state to get an outcome (e.g., win or loss in a game, or some reward), and (d) Backpropagates the result up the tree to update the values of nodes and inform future selections.Repeating these simulations thousands of times concentrates the search on the most promising branches of the tree.In essence, MCTS uses random sampling to evaluate the potential of different action sequences, gradually biasing the search towards those with better average outcomes.In LLM reasoning, we can treat the generation of text as a decision process and use to explore different continuations.For example, at a given question (root), each possible next reasoning step or answer is an action; a simulation could mean letting the LLM continue to a final answer (perhaps with some randomness), and a reward could be whether the answer is correct.By doing this repeatedly, MCTS can identify which chain of thoughts or answers has the highest empirical success rate.The appeal of MCTS for reasoning is that it can handle large search spaces by sampling intelligently rather than exhaustively, and it naturally incorporates uncertainty and exploration.</p>
<p>Train verifiers to score intermediate steps (via Monte Carlo rollouts) instead of just final answers.</p>
<p>Recent efforts have integrated MCTS with LLMs to tackle complex reasoning and decision-making tasks.One example is using MCTS for query planning: Monte Carlo Thought Search [257], where an LLM is guided to ask a series of sub-questions to find an answer.Jay et al. [257] used an MCTS-based algorithm called 'Monte Carlo Reasoner' that treats the LLM as an environment: each node is a prompt (state) and each edge is an action (e.g., a particular question to ask or step to take), and random rollouts are used to evaluate outcomes.This approach allowed the system to efficiently explore a space of possible reasoning paths and pick a high-reward answer path, outperforming naive sampling in a scientific Q&amp;A task.Similarly, MCTS has been applied to code generation with LLMs [258] -the algorithm explores different code paths (using the model to propose code completions and etest them) to find a correct solution.Another line of work ensembles multiple LLMs with MCTS, treating each model's output as a branch and using a reward model to simulate outcomes [259].Early results show that MCTS-based reasoning can solve problems that single-pass or greedy methods often miss, although with more compute [74].The downside is that MCTS can be significantly slower than straightforward sampling or beam search, which recent research is addressing by improving efficiency (e.g., by state merging [87]).In general, MCTS brings the strength of planning algorithms to LLM inference and enables an LLM to 'look ahead' through simulated rollouts and make more informed reasoning choices, much like it has done for AI in gameplay.</p>
<p>Test-time compute is not a 1-to-1 replacement for pretraining but, offers a viable alternative in many cases.</p>
<p>Chain-of-Action-Thought reasoning</p>
<p>LLMs excel in reasoning tasks but rely heavily on external guidance (e.g., verifiers) or extensive sampling at inference time.Existing methods like CoT [8] lack mechanisms for selfcorrection and adaptive exploration, limiting their autonomy and generalization.Satori [260] introduced a two-stage training paradigm, which works by initially tuning the model's output format and then enhancing its reasoning capabilities through self-improvement.In Stage 1 (Format Tuning), the model is exposed to a large set of 10K synthetic trajectories generated by a multi-agent framework comprising a generator, a critic, and a reward model.This supervised fine-tuning helps the model to produce outputs in specific reasoning format using meta-action tokens, although it may still have difficulty generalizing beyond these examples.In Stage 2 (Self-Improvement via RL), the model employs PPO with a Restart and Explore strategy [260], which allows it to restart from intermediate steps, whether they were correct or not, to refine its reasoning process.The model receives rewards based on a combination of rule-based correctness, reflection bonuses, and preference-based Outcome Reward Model feedback explained in § 5.9, thereby incentivizing the allocation of more computational resources to tougher problems and enabling extended reasoning during testing for complex tasks.</p>
<p>Multi-agent frameworks and advanced fine-tuning strategies are increasingly being explored to enhance reasoning in LLMs.Multi-Agent LLM Training (MALT) [261] introduces a structured approach where generation, verification, and refinement steps are distributed across specialized agents, allowing for iterative self-correction and improved reasoning chains.Similarly, optimizing preference alignment remains a crucial challenge in ensuring both safety and helpfulness in LLMs [262].Approaches like Bi-Factorial Preference Optimization (BFPO) [263] reframe RLHF objectives into a single supervised learning task, reducing human intervention while maintaining robust alignment.Beyond text-based reasoning, multimodal approaches like Multimodal Visualization-of-Thought (MVoT) [264] extend CoT prompting by incorporating visual representations, significantly enhancing performance in spatial reasoning tasks.These advancements highlight the growing need for structured multi-agent collaboration, safetyaware optimization, and multimodal reasoning to address fundamental limitations in LLM reasoning [265,266,267].</p>
<p>Pretraining vs. Test-Time Scaling</p>
<p>Pretraining and TTS are two distinct strategies for improving LLM performance, each with different tradeoffs in computational cost and effectiveness.Pretraining involves scaling model parameters or increasing training data to enhance capabilities, requiring substantial upfront computational investment [3].In contrast, TTS optimizes inference-time compute (such as iterative refinements, search-based decoding, or adaptive sampling), allowing performance improvements without modifying the base model.</p>
<p>From a performance vs. cost perspective, TTS achieves results comparable to a model 14× larger on easy to intermediate tasks (e.g., MATH benchmarks), while reducing inference costs by 4× fewer FLOPs in compute-intensive scenarios [268].However, pretraining remains superior for the hardest tasks or when inference compute constraints are high, as larger pretrained models inherently encode deeper reasoning capabilities.</p>
<p>A smaller model with test-time compute can outperform a 14× larger model on easy/intermediate questions, when inference tokens (Y) are limited (e.g., self-improvement settings).</p>
<p>In terms of use cases, TTS is useful for scenarios with flexible inference budget or when base models already exhibit reasonable competence in the task.Conversely, pretraining is essential for tasks requiring fundamentally new capabilities (e.g., reasoning on novel domains) where inference-time optimizations alone may not suffice.</p>
<p>There are notable tradeoffs between the two approaches.TTS reduces upfront training costs, making it attractive for flexible, on-the-go optimization, but requires dynamic compute allocation at inference.Pretraining, on the other hand, incurs high initial costs but guarantees consistent performance without additional runtime overhead, making it ideal for large-scale API deployments or latency-sensitive applications.Overall, TTS and pretraining are complementary in nature.Future LLM systems may adopt a hybrid approach, where smaller base models are pretrained with essential knowledge, while TTS dynamically enhances responses through adaptive, on-demand computation.This synergy enables more costeffective and efficient large-scale model deployment.</p>
<p>Choose pretraining for foundational capabilities and test-time scaling for accurate contextaware refinement.</p>
<p>Benchmarks for LLM Post-training Evaluation</p>
<p>To evaluate the success of LLM post-training phases, a diverse set of benchmarks have been proposed covering multiple domains: reasoning tasks, alignment, multilinguality, Step-by-step solutions GSM8K [270] Math Reasoning Pointwise 8.5K Multi-step reasoning MetaMathQA [271] Math Reasoning Pointwise 40K+ Self-verification, FOBAR WorldTree V2 [272] Science QA Pointwise 1,680 Multi-hop explanations PangeaBench [273] Multimodal Reasoning Pairwise 47 Langs.Cultural understanding MMMU [274] Science/Math Pointwise College-Level Physics, Chemistry, Bilingual TruthfulQA [275] QA/Reasoning Pointwise N/A Truthfulness MathInstruct [276] Math Reasoning Pointwise 262K Correctness MMLU [277,278] Multitask Reasoning Pointwise 57 Tasks Broad knowledge evaluation MMLU-Fairness [277] Fairness/Reasoning Pointwise N/A Bias/Equity Analysis DROP [279] Reading/Reasoning Pointwise 96K Discrete reasoning over paragraphs BBH [175] Hard Reasoning Pairwise N/A Complex logical problem-solving VRC-Bench [187] Multimodal Reasoning Pairwise N/A Visual Reasoning and Classification</p>
<p>RL Alignment Benchmarks</p>
<p>HelpSteer [280] RL Alignment Pairwise 37K+ Multi-attribute scoring Anthropic HH-RLHF [121] RL Alignment Pairwise 42.5K Harmlessness alignment UltraFeedback [281] RL Alignment Pairwise 64K Instruction-following, Truthfulness D4RL [282] RL/Control Pointwise N/A Offline RL across domains Meta-World [283] RL/Control Pointwise N/A Multi-task robotic RL MineRL [284] RL/Games Pairwise N/A Imitation learning, rewards</p>
<p>Multilingual Evaluation</p>
<p>CulturaX [285] Multilingual Pointwise 6.3T Deduplication, Quality PangeaIns [286] Multilingual Pointwise 6M Multilingual instructions TydiQA [287] Multilingual Pointwise N/A Cross-lingual QA XGLUE [288] Multilingual Pointwise N/A Cross-lingual language tasks MM-Eval [289] Multilingual Pairwise 4,981 Task-oriented multilingual QA ALM-Bench [289] Multilingual  [298] Story Pairwise 100K User preference-based ranking PKU-SafeRLHF [299] Values Pairwise 83.4K Helpfulness, Harmlessness Cvalue [300] Values Pairwise 145K Safety, Responsibility NaturalInst.[301,302] Instruction Tuning Pointwise 1,600+ Instruction-following evaluation general comprehension, and dialogue and search tasks.A wellstructured evaluation framework ensures a comprehensive understanding of an LLM strengths, and limitations across various tasks.These benchmarks play a crucial role in LLM post-processing stages, where models undergo fine-tuning, calibration, alignment, and optimization to improve response accuracy, robustness, and ethical compliance.Next, we explain the main benchmark gorups.Table 3 provides an overview of key datasets categorized under these benchmark groups.Reasoning Benchmarks.These benchmarks assess LLMs on their ability to perform logical, mathematical, and scientific reasoning.Mathematical reasoning datasets like MATH [269], GSM8K [270], and MetaMathQA [271] test models on problem-solving, multi-step arithmetic, and theorem-based problem formulations.Scientific and multimodal reasoning benchmarks such as WorldTree V2 [272] and MMMU [274] evaluate knowledge in physics, chemistry, and multimodal understanding, which are crucial for fact-checking and verification processes in LLM-generated responses.Additionally, datasets like PangeaBench [273] extend reasoning tasks into multilingual and cultural domains, enabling models to refine cross-lingual reasoning.[292] and MultiWOZ [293] evaluate multi-turn conversational models, essential for dialogue history tracking and adaptive response fine-tuning.For search relevance assessment, BEIR [296] provides largescale human-annotated judgments for retrieval fine-tuning, ensuring LLMs generate and rank responses effectively.TREC DL21/22 [294,295] contributes to document relevance ranking and fact retrieval.</p>
<p>Future Directions</p>
<p>We gathered all papers related to post-training methods in LLMs and analyzed their trends, as shown in Figure 7. Application of RL techniques [303,57,40] for refining the LLMs have a noticeable increase in prominence since 2020 (Figure 7a), emphasizing the demand for interactive approaches such as human-in-the-loop [35,304] reinforcement and scalability [111,82,305].At the same time, reward modeling [306,166,167] (Figure 7b) has seen a steady rise in interest due to the emergence of self-rewarding language models, yet the field still struggles with reward hacking [307,308] and the design of robust [309], failure-aware reward functions beyond reward hacking [310].Decoding and search (Figure 7c) methods include tree-of-thoughts [84] and Monte Carlo [311,257] strategies aiming to enhance model reasoning through iterative self-critique [312,304,29], but these techniques also demand reliable uncertainty estimators to prevent excessive computational overhead [313,111].Safety [299,314,315], robustness [316], and interpretability [317,318,319] have likewise become central concerns (Figure 7d), motivating the development of bias-aware [320,321] and uncertainty-aware [322] RL methods beyond correlation with human uncertanity [323] that safeguard user trust and prevent adversarial attacks.Another crucial area involves personalization [324,325] and adaptation [193] (Figure 7e), where efforts to tailor LLMs for specific domains must be balanced against risks to privacy [326], particularly when enterprise data or sensitive personal information is involved.In parallel, process [327,328] vs. outcome reward optimization [329] (Figure 7f) remains an open question: while process-based rewards help guide incremental improvements, outcome-focused metrics are simpler but may not capture crucial intermediate decision-making steps.Beyond reward structure, fine-tuning LLMs on new tasks still encounter issues like catastrophic forgetting [330] and potential data leakage [331,332], underscoring the need for parameter-efficient methods [60] and privacy-preserving strategies such as differential privacy [333] and federated learning [334].Human feedback, while central to alignment, is inherently costly and limited in scope; methods like Constitutional AI [53] and RLAIF [95] seek to automate parts of this oversight, though they introduce fresh concerns about bias calibration [335] and model self-consistency [184].Finally, test-time scaling [111] and dynamic reasoning [336] frameworks pose further challenges: models must learn when to allocate more computation for complex queries, how to adapt verification modules [337] efficiently, and how to maintain robust performance even when facing adversarial inputs.These converging research directions-spanning reward modeling, decoding strategies, interpretability, personalization, and safe fine-tuning-highlight the multifaceted role of RL in LLMs and collectively shape the future trajectory of large-scale language model development.Below, we delve into some of these directions in greater detail.Fine-tuning challenges.Fine-tuning remains one of the most direct post-training methods to adapt LLMs to specific tasks or domains, yet it faces several open challenges.One fundamental issue is catastrophic forgetting -when updating an LLM on new data causes it to lose or degrade previously learned capabilities.Even advanced PEFT methods like LoRA [60], which greatly reduce the number of trainable weights, do not fully solve this problem [330].Future work can explore better continual learning strategies and regularization techniques so that models can acquire new skills without erasing old ones.For example, new fine-tuning algorithms (e.g.CURLoRA [330]) explicitly aim to stabilize training and preserve prior knowledge while adding new tasks.Promising research directions include curriculum-based fine-tuning [338] (introducing new facts gradually or in context with known facts) and hybrid training that combines retrieval or external knowledge bases.For instance, rather than solely adjusting the model's weights, one could fine-tune LLMs to consult a knowledge repository or perform tool use (such as database queries or computations) when faced with queries outside their original training distribution [339,340].This retrievalaugmented fine-tuning [341] could let models incorporate fresh information at inference time, reducing the need to overwrite their internal weights with new facts.Another approach is training models to explicitly represent uncertainty about new knowledge, thereby enabling them to say 'I don't know' or defer to an external source if a query concerns content not seen in pre-training.By blending weight updates with external knowledge integration, future fine-tuned LLMs will  maintain higher factual accuracy and lower hallucination rates on emerging information.Safe Fine-tuning.From an ethical and safety perspective, fine-tuning raises important open research questions.Finetuning data often contains sensitive or proprietary information [326], which can lead to privacy risks if the model memorizes and later regurgitates that data.A recent comprehensive survey [342] highlights vulnerabilities in the fine-tuning stage, such as membership inference attacks (detecting if a specific record was in the fine-tuning set) and data extraction (recovering parts of the fine-tuning data from the model's outputs).Mitigating these risks is an open problem: methods like differential privacy fine-tuning [333] (adding noise to the weight updates) and federated fine-tuning (where data never leaves user devices and only aggregated updates are sent to the model) are being actively explored.However, these methods often come at the cost of model utility or require careful calibration to avoid degrading performance.Limitations of Human Feedback.Human feedback is costly and subjective.One promising avenue to address the limitations of human feedback is using AI feedback and automation to assist or replace human evaluators.Constitutional AI [53], introduced by Anthropic, is a notable example: instead of relying on extensive human feedback for every harmful or helpful behavior, the model is guided by a set of written principles (a 'constitution') and is trained to critique and refine its own responses using another AI model as the judge [343].Emerging directions here include RLAIF [95] and other semi-automated feedback techniques [344]: using strong models to evaluate or guide weaker models, or even having multiple AI agents debate a question and using their agreement as a reward signal [345,346].Such AI-aided feedback could vastly scale the tuning process and help overcome the bottleneck of limited human expert time.However, it raises new theoretical questions: how do we ensure the AI judge is itself aligned and correct?There is a risk of feedback loops or an echo chamber of biases if the automated preferences are flawed.An open gap is the creation of robust AI feedback systems that are calibrated to human values (perhaps periodically 'grounded' by human oversight or by a diverse set of constitutional principles).The blending of human and AI feedback in a hierarchical scheme could provide a scalable yet reliable RL paradigm for LLMs.</p>
<p>Test-time scaling challenges.Open challenges in TTS revolve around how to orchestrate the inference-time processes efficiently and reliably.A key question is how much computing is enough for a given query, and how to determine this on the fly?Using less resources can result in mistakes, but using too much is inefficient and could introduce inconsistencies.Recent research by Snell et al. [83] tackled it by proposing a unified framework with a 'Proposer' and a 'Verifier' to systematically explore and evaluate answers.In their framework, the Proposer (usually the base LLM) generates multiple candidate solutions, and the Verifier (another model or a heuristic) judges and selects the best.The optimal strategy can vary by problem difficulty: for easier queries, generating many answers in parallel and picking the top might be sufficient, whereas for harder problems, sequential, step-bystep reasoning with verification at each step works better.An important future direction is building adaptive systems where the LLM dynamically allocates computation based on an estimate of the question's complexity.This idea connects to meta-cognition in AI [314], enabling models to have a sense of what they don't know or what deserves more thought.Developing reliable confidence metrics or difficulty predictors for LLMs is an open research area, but progress here would make TTS far more practical i.e., the model would only 'slow down and think' when necessary, much like a human spending extra time on a hard problem.Additionally, By reframing inference-time scaling as a probabilistic inference problem and employing particle-based Monte Carlo methods [? ], the small models achieved o1 level accuracy in only 32 rollouts, a 4-16x improvement in scaling efficiency across various mathematical reasoning tasks.Recent study [347] shows distilling test-time computations into synthetic training data creates synergistic pretraining benefits which can also be further explored.Reward Modeling and Credit Assignment.Current RL approaches suffer from reward misgeneralization, where models over-optimize superficial proxy metrics rather than genuine reasoning quality.The sparse nature of terminal rewards in multi-step tasks increases credit assignment challenges, particularly in long-horizon reasoning scenarios.Traditional methods like DPO require inefficient pairwise preference data and fail to utilize failure trajectories effectively.Hybrid reward models can be investigated by integrating process supervision with outcome-based rewards using contrastive stepwise evaluation [348].This approach enables a more granular assessment of intermediate decision-making steps while aligning with long-term objectives.Recent work [171] suggests steplevel policy optimization could improve value function accuracy while maintaining safety constraints.Dynamic credit assignment mechanisms can be explored through temporal difference learning adapted for transformers [349,350].Such adaptations may enhance the model's ability to capture longrange dependencies and optimize reward propagation over extended sequences.Failure-aware training strategies can be developed by incorporating negative examples into the RL loop via adversarial data augmentation [351].This can improve model robustness by systematically exposing it to challenging scenarios and encouraging more resilient policy learning.Efficient RL Training and Distillation.Current RL methods for LLMs require prohibitive computational resources [352] while often underperforming knowledge distillation techniques [93].This inefficiency limits scalability and practical deployment, as distilled models frequently surpass RL-trained counterparts despite requiring less training overhead.Additionally, pure RL approaches struggle to balance language quality with reasoning improvement [97,93], creating a performance ceiling.</p>
<p>The development of hybrid frameworks that initialize RL policies with distilled knowledge from large models, combining the exploratory benefits of RL with the stability of supervised learning is an interesting direction.Similarly, curriculum sampling strategies that progressively increase task complexity while using distillation to preserve linguistic coherence can also help.PEFT methods [60] can be leveraged during RL updates to maintain base capabilities while enhancing reasoning.</p>
<p>Integration: Combining PRM-guided tree search with online distillation achieves 4× efficiency gains over baseline methods, while maintaining 94% solution accuracy on MATH dataset.</p>
<p>Privacy-Preserving Personalization.Customizing models for enterprise and individual use cases raises the risk of exposing private training data through memorization, making privacy-preserving [326] adaptation essential.Promising solutions include homomorphic instruction tuning [353], which processes encrypted user queries while maintaining end-to-end encryption during inference; differential privacy via reward noising [354], which introduces mathematically bounded noise into RLHF preference rankings during alignment; and federated distillation, which aggregates knowledge from decentralized user-specific models without sharing raw data.Collaborative Multi-Model Systems.As single-model [355,356,357] scaling approaches physical limits, alternative paradigms such as multi-agent LLM collaboration [358,359,178] become necessary.Researchers are investigating emergent communication protocols that train models to develop lossy compression "languages" for inter-model knowledge transfer such as GenAINet [360], robust ensembles where stress-test induced specialization drives automatic division of problem spaces based on failure analysis [361], and gradientfree synergy learning through evolutionary strategies designed to discover complementary model combinations without relying on backpropagation [362].Multimodal RL Integration.Multimodal reinforcement learning [363,49,364] faces the obstacle of a combinatorial state explosion, especially in contexts exceeding 128k tokens.Pioneering methods to overcome this include hierarchical attention frameworks that employ modality-specific policies with cross-attention gating [365], adaptive truncation strategies that compress context while preserving critical reasoning segments [366], and flash curriculum approaches that leverage self-supervised [367,368,369] complexity prediction to facilitate progressive multimodal integration.Efficient RL Training.Efficient RL training paradigms continue to be a critical research frontier as current methods exhibit significant sample inefficiency and computational overhead.Addressing issues like the overthinking [370,371] phenomenon, where excessive reasoning chains waste valuable computation [145], requires approaches such as partial rollout strategies [372], adaptive length penalty mechanisms employing learned compression transformers, and hybrid architectures that combine MCTS with advanced RL optimizers.These innovations are essential for scaling RL to long-context tasks while minimizing wasted computational resources.</p>
<p>Overthinking Phenomenon: Analysis reveals 22% wasted computation is in reasoning chains exceeding optimal reasoning length.</p>
<p>RL methods exhibit sample inefficiency and computational overhead, particularly when scaling to contexts exceeding 128k tokens.The 'overthinking' phenomenon, where models generate excessively long reasoning chains, further reduces token efficiency and increases deployment costs [373].Investigate partial rollout strategies with flash attention mechanisms for long-context processing.Develop length penalty mechanisms using learned compression transformers for iterative long2short distillation.Hybrid architectures combining MCTS [74] with GRPO [59] could enable better explorationexploitation tradeoffs.Parallel work by Xie et.al. [74] demonstrates promising results through adaptive tree search pruning.Several open challenges persist in the field.Uncertainty propagation remains problematic as current confidence estimators add approximately 18% latency overhead, while catastrophic forgetting rresults in a degradation of 29% of base capabilities during RL fine-tuning [374].Moreover, benchmark saturation is an issue, with MMLU scores correlating poorly (r = 0.34) with real-world performance [375].</p>
<p>Adversarial Vulnerabilities: Stress tests reveal a high success rate on gradient-based prompt injections.</p>
<p>Conclusion</p>
<p>This survey and tutorial provides a systematic review of posttraining methodologies for LLMs, focusing on fine-tuning, reinforcement learning, and scaling.We analyze key techniques, along with strategies for improving efficiency and alignment with human preferences.Additionally, we explore the role of RL in enhancing LLMs through reasoning, planning, and multitask generalization, categorizing their functionalities within the agent-environment paradigm.Recent advancements in reinforcement learning and test-time scaling have significantly improved LLMs reasoning capabilities, enabling them to tackle increasingly complex tasks.By consolidating the latest research and identifying open challenges, we aim to guide future efforts in optimizing LLMs for real-world applications.</p>
<p>•</p>
<p>We provide a comprehensive and systematic review of post-training methodologies for LLMs, covering finetuning, RL, and scaling as integral components of model optimization.• We offer a structured taxonomy of post-training techniques, clarifying their roles and interconnections, and present insights into open challenges and future research directions in optimizing LLMs for real-world deployment.• Our survey provides practical guidance by introducing key benchmarks, datasets, and evaluation metrics essential for assessing post-training effectiveness, ensuring a structured framework for real-world applications.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of Large Language Models (LLMs) reasoning methods, showcasing pathways for enhancing reasoning capabilities through approaches like Chain-of-Thought (CoT) prompting, self-feedback, and episodic memory.The diagram highlights multiple reinforcement learning-based optimization techniques, including GRPO, RLHF, DPO, and RLAIF, for finetuning reasoning models with reward mechanisms and preference-based learning.</p>
<p>Fig. 3 :
3
Fig.3: Comparison of PPO[73], GRPO[59], and DPO[162].We highlight policy models, reference models, rewards and optimization flows with corresponding loss functions.</p>
<p>reg penalizes deviations from π ref , and α balances regularization.OREO's explicit value function enables testtime beam search (e.g., selecting high-value reasoning steps) and iterative training, where failed trajectories refine the policy.This contrasts with DPO implicit value function, which lacks stepwise credit assignment.OREO's computational cost scales with trajectory length and value-model training.While effective for math/agent tasks, its generalization to broader domains (e.g., coding) requires validation.Iterative training also demands careful data curation to avoid overfitting to failure modes.</p>
<p>Fig. 4 :
4
Fig. 4: This Venn diagram illustrates the interplay between System, Data, and Model for efficient finetuning and deployment.It covers strategies like accelerators (Groq, vLLM), adaptation (LoRA, PEFT), co-optimized architectures (FlashAttention), data compression (TokenMerging), scaling laws (Chinchilla), and model compression (GPTQ) to boost performance and scalability.</p>
<p>Fig. 5 :
5
Fig. 5: An overview of Test-time Scaling methods: parallel scaling, sequential scaling, and search-based methods.It also shows how they integrate into a compute-optimal strategy.</p>
<p>Fig. 6 :
6
Fig. 6: This figure compares reasoning strategies in LLMs, evolving from Direct Prompting, which maps input to output without reasoning, to more structured approaches.Chain-of-Thought (CoT) introduces step-by-step reasoning, while Self-Consistency (CoT-SC) generates multiple CoT paths and selects the most frequent answer.Multiple CoTs explores diverse reasoning paths independently.Tree-of-Thoughts (ToT) structures reasoning as a tree, enabling backtracking and refinement, whereas Graphof-Thoughts (GoT) generalizes this by dynamically aggregating and connecting thoughts.The legend deciphers key mechanisms like grading, backtracking, and self-refinement, crucial for optimizing reasoning efficiency.</p>
<p>(a) Growing trend in RL for LLMs, with a focus on Human-in-the-Loop RL.(b) Reward modeling trends show RLHF stabilization, with Self-Rewarding Models leading, but Reward Hacking persists.(c) Decoding strategies like Tree-of-Thoughts and MCTS are improving LLM reasoning and decision-making.(d) Safety and Robustness research is growing, with Uncertainty-Aware RL ensuring RLHF model reliability.(e) Personalization and Adaptation focus on Privacy-Preserving RLHF.On-device adaptation remains a challenge.(f) Process Reward Modeling dominates Outcome-Based Optimization, favoring iterative strategies for RL-based LLMs.</p>
<p>Fig. 7 :
7
Fig. 7: Yearly Trends in RL specific post-training methods for LLMs and emerging research directions.</p>
<p>TABLE 1 :
1
An overview of reinforcement learning-enhanced LLMs, where '141B-A39B' denotes a Mixture of Experts (MoE) model with 141 billion total parameters, of which 39 billion are utilized during inference.TTS stands for Test-Time Scaling.
RL Enhanced LLMsDeveloperSource# ParamsRL MethodsFine-TuningArchitecture TypeModelTTSDeepSeek-V2 [16]DeepseekLink236B-A21BGRPODPO + GRPOMoEOpen✓GPT 4.5 [120]OpenAILink-RLHF, PPO, RBRMSFT + RLHFMoEClosed✓Gemini [15]GoogleLink-RLHFSFT + RLHFSingle ModelClosed✗Claude 3.7 [121]AnthropicLink-RLAIFSFT + RLAIFSingle ModelClosed✗Reka [122]RekaLink7B, 21BRLHF, PPOSFT + RLHFSingle ModelClosed✗DeepSeekR1 [40]DeepseekLink240B-A22BGRPODPO + GRPOMoEOpen✓Nemotron-4 340B [123]NVIDIALink340BDPO, RPODPO + RPOSingle ModelClosed✗Falcon [124]TIILink40B-SFTSingle ModelOpen✗GPT-4 [39]OpenAILink-RLHF, PPO, RBRMSFT + RLHFMoEClosed✓Llama 3 [13]MetaLink8B, 70B, 405BDPOSFT + DPOSingle ModelOpen✗Qwen2 [125]AlibabaLink(0.5-72)B, 57B-A14BDPOSFT + DPOSingle ModelOpen✓Gemma2 [14]GoogleLink2B, 9B, 27BRLHFSFT + RLHFSingle ModelOpen✗Starling-7B [26]BerkeleyLink7BRLAIF, PPOSFT + RLAIFSingle ModelOpen✗Moshi [126]KyutaiLink7B--Multi-modalOpen✓Athene-70B [127]NexusflowLink70BRLHFSFT + RLHFSingle ModelOpen✗GPT-3.5 [39]OpenAILink3.5B, 175BRLHF, PPOSFT + RLHFMoEClosed✓Hermes 3 [128]NousLink8B, 70B, 405BDPOSFT + DPOSingle ModelOpen✗Zed [129]Zed AILink500BRLHFRLHFMulti-modalOpen✓PaLM 2 [130]GoogleLink-RLHF-Single ModelClosed✓InternLM2 [131]SAILLink1.8B, 7B, 20BRLHF, PPOSFT + RLHFSingle ModelClosed✗Supernova [132]Nova AILink220BRLHFRLHFMulti-modalOpen✓Grok3 [133]Grok-3Link175B-DPODenseOpen✓Pixtral [134]Mistral AILink12B, 123B-PEFTMultimodalOpen✓Minimaxtext [135]MiniMaxLink456B-SFTSingle ModelClosed✗Amazonnova [136]AmazonLink-DPO, RLHF, RLAIFSFTSingle ModelClosed✗Fugakullm [137]FujitsuLink13B--Single ModelClosed✗Nova [138]Rubik's AILink--SFTProprietaryClosed✗03 [139]OpenAILink-RL through CoTRL through CoTSingle ModelClosed✓Dbrx [140]DatabricksLink136B-SFTSingle ModelOpen✗Instruct-GPT [58]OpenAILink1.3B, 6B, 175BRLHF, PPOSFT + RLHFSingle ModelClosed✗Openassistant [141]LAIONLink17B-SFTSingle ModelOpen✗ChatGLM [142]Zhipu AILink6B, 9BChatGLM-RLHFSFT + RLHFSingle ModelOpen✗Zephyr [143]ArgillaLink141B-A39BORPODPO + ORPOMoEOpen✓phi-3 [17]MicrosoftLink3.8B, 7B, 14BDPOSFT + DPOSingle ModelClosed✗Jurassic [144]AI21 LabsLink--SFTProprietaryClosed✗Kimi K1.5 [145]Moonshot AILink150B-RLHFMulti-modalOpen✓Phi-4 [146]MicrosoftLink28B, 70B, 140BDPOSFT + DPOSingle ModelClosed✗Chameleon [147]Meta AILink34B-SFTSingle ModelOpen✗Cerebrasgpt [148]CerebrasLink13B-SFTSingle ModelOpen✗Bloomberggpt [149]Bloomberg L.P.Link50B-SFTSingle ModelClosed✗Chinchilla [150]DeepMindLink70BRLHF, PPOSFTSingle ModelClosed✗</p>
<p>TABLE 3 :
3
Comprehensive Overview of Reasoning, RL Alignment, and Multilingual Datasets.Here, pointwise and pairwise refer to different methods of evaluating model performance across various tasks.
DatasetsDomainType #SamplesEvaluation CriteriaReasoning BenchmarksMATH [269]Math ReasoningPointwise7,500</p>
<p>[284] preference optimization through reinforcement learning with human feedback.D4RL[282]and Meta-World[283]focus on robotic control and offline RL, which have implications for autonomous model decision-making.MineRL[284]extends RL testing into complex environments such as Minecraft-based interactions, useful for training LLMs in adaptive decision-making settings.
Multilingual Evaluation. Multilingual benchmarks are es-sential for LLM post-processing in cross-lingual generalization,translation adaptation, and fine-tuning for low-resource lan-guages. CulturaX [285] and PangeaIns [286] evaluate tok-enization, translation, and instruction-following in over 150languages, ensuring fairness and diversity in model outputs.TydiQA [287] and MM-Eval [289] target bilingual and task-oriented multilingual evaluation, enabling improvements inLLM fine-tuning. These datasets ensure that LLMs are not justEnglish-centric but optimized for multilingual adaptability.General Comprehension Benchmarks. General compre-hension benchmarks contribute to model fine-tuning, responsecoherence, and preference optimization. Datasets such asChatbot Arena [291], MTBench [291], and RewardBench [167]test user preference modeling and conversational fluency,crucial for LLM response ranking and re-ranking methods.BigBench [290] evaluates broad multi-domain comprehension,while MMLU [277, 278] measures correctness and informa-
These benchmarks help determine how well models can process structured knowledge and apply logical deductions.RL Alignment Benchmarks.RL alignment benchmarks are central to LLM alignment and post-training optimization.They refine response generation, ethical constraints, and user-aligned outputs through RLHF.Datasets such as Help-Steer [280] and UltraFeedback [281] evaluate models based on multi-attribute scoring and alignment with user instructions.Anthropic's HH-RLHF [121] explores how well mod-els learn tiveness.These datasets help in refining LLM fluency, factual correctness, and open-ended response generation.Dialogue and Search Benchmarks.Dialogue and search benchmarks play a key role in optimizing LLM retrieval-based responses, multi-turn coherence, and information retrieval accuracy.Datasets such as ConvAI2</p>
<p>. https://hturner.github.io/PlackettLuce/</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, arXiv:1906.082372019. 1, 3, 19arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, arXiv:1810.04805201813arXiv preprint</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Z Lan, arXiv:1909.119422019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. P Verga, S Hofstatter, S Althammer, Y Su, A Piktus, A Arkhangorodsky, M Xu, N White, P Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, 2022. 1, 2, 3, 113518Advances in neural information processing systems</p>
<p>C Wang, Y Deng, Z Lyu, L Zeng, J He, S Yan, B An, arXiv:2406.14283Q*: Improving multi-step reasoning for llms with deliberative planning. 2024arXiv preprint</p>
<p>Mindmap: Constructing evidence chains for multi-step reasoning in large language models. Y Wu, X Han, W Song, M Cheng, F Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Y Ding, Z Wang, W Ahmad, H Ding, M Tan, N Jain, M K Ramanathan, R Nallapati, P Bhatia, D Roth, Advances in Neural Information Processing Systems. 202436</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783202415arXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. G Team, M Riviere, S Pathak, P G Sessa, C Hardin, S Bhupatiraju, L Hussenot, T Mesnard, B Shahriari, A Ramé, arXiv:2408.00118202415arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, arXiv:2312.11805202315arXiv preprint</p>
<p>A Liu, B Feng, B Wang, B Wang, B Liu, C Zhao, C Dengr, C Ruan, D Dai, D Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024. 1, 2, 5, 6arXiv preprint</p>
<p>M Abdin, J Aneja, H Awadalla, A Awadallah, A A Awan, N Bach, A Bahree, A Bakhtiari, J Bao, H Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 202415arXiv preprint</p>
<p>Hierarchical neural story generation. A Fan, M Lewis, Y Dauphin, arXiv:1805.048332018arXiv preprint</p>
<p>Of human criteria and automatic metrics: A benchmark of the evaluation of story generation. C Chhun, P Colombo, C Clavel, F M Suchanek, arXiv:2208.116462022arXiv preprint</p>
<p>The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation. S Arif, S Farid, A H Azeemi, A Athar, A A Raza, arXiv:2408.086882024arXiv preprint</p>
<p>Selfee: Iterative self-revising llm empowered by self-feedback generation. S Ye, Y Jo, D Kim, S Kim, H Hwang, M Seo, Blog post. 12023</p>
<p>Creative beam search: Llm-as-a-judge for improving response generation. M Musolesi, ICCC, 2024. 1</p>
<p>Self-evaluation improves selective generation in large language models. J Ren, Y Zhao, T Vu, P J Liu, B Lakshminarayanan, PMLR, 2023. 1Proceedings on. on</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Y Lai, C Li, Y Wang, T Zhang, R Zhong, L Zettlemoyer, W -T. Yih, D Fried, S Wang, T Yu, PMLR, 2023. 1International Conference on Machine Learning. </p>
<p>Starling-7b: Improving helpfulness and harmlessness with rlaif. B Zhu, E Frick, T Wu, H Zhu, K Ganesan, W.-L Chiang, J Zhang, J Jiao, First Conference on Language Modeling. 202415</p>
<p>D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023arXiv preprint</p>
<p>Self-evaluation guided beam search for reasoning. Y Xie, K Kawaguchi, Y Zhao, J X Zhao, M.-Y Kan, J He, M Xie, Advances in Neural Information Processing Systems. 3612024</p>
<p>Self-judge: Selective instruction following with alignment self-evaluation. H Ye, H T Ng, arXiv:2409.009352024120arXiv preprint</p>
<p>Videoautoarena: An automated arena for evaluating large multimodal models in video analysis through user simulation. Z Luo, H Wu, D Li, J Ma, M Kankanhalli, J Li, arXiv:2411.132812024arXiv preprint</p>
<p>Efficient self-improvement in multimodal large language models: A model-level judge-free approach. S Deng, W Zhao, Y.-J Li, K Wan, D Miranda, A Kale, Y Tian, arXiv:2411.1776020241arXiv preprint</p>
<p>Llava-critic: Learning to evaluate multimodal models. T Xiong, X Wang, D Guo, Q Ye, H Fan, Q Gu, H Huang, C Li, arXiv:2410.027122024arXiv preprint</p>
<p>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. D Chen, R Chen, S Zhang, Y Liu, Y Wang, H Zhou, Q Zhang, Y Wan, P Zhou, L Sun, arXiv:2402.047882024arXiv preprint</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, Nature Computational Science. 3102023</p>
<p>Human-centered design recommendations for llm-as-a-judge. Q Pan, Z Ashktorab, M Desmond, M S Cooper, J Johnson, R Nair, E Daly, W Geyer, arXiv:2407.034792024120arXiv preprint</p>
<p>Humans or llms as the judge? a study on judgement biases. G H Chen, S Chen, Z Liu, F Jiang, B Wang, arXiv:2402.106692024arXiv preprint</p>
<p>Upper Saddle River/Prentive Hall. A Newell, 1972116Human problem solving</p>
<p>Humanllm collaborative annotation through effective verification of llm labels. X Wang, H Kim, S Rahman, K Mitra, Z Miao, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>View in Article. R Openai, arxiv 2303.08774Gpt-4 technical report. 2023212</p>
<p>DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025. 1, 5, 111220arXiv preprint</p>
<p>Chatgpt is bullshit. M T Hicks, J Humphries, J Slater, Ethics and Information Technology. 2622024</p>
<p>Ai hallucinations: A misnomer worth clarifying. N Maleki, B Padmanabhan, K Dutta, 2024. 1</p>
<p>Insights into classifying and mitigating llms' hallucinations. A Bruno, P L Mazzeo, A Chetouani, M Tliba, M A Kerkouri, arXiv:2311.081172023arXiv preprint</p>
<p>Detecting hallucinations in large language models using semantic entropy. S Farquhar, J Kossen, L Kuhn, Y Gal, Nature. 63080172024</p>
<p>Hill: A hallucination identifier for large language models. F Leiser, S Eckhardt, V Leuthe, M Knaeble, A Maedche, G Schwabe, A Sunyaev, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Detecting and preventing hallucinations in large vision language models. A Gunjal, J Yin, E Bas, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Reasoning with language model is planning with world model. S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational LinguisticsDecember 6-10, 2023. 2023</p>
<p>Limo: Less is more for reasoning. Y Ye, Z Huang, Y Xiao, E Chern, S Xia, P Liu, 202513</p>
<p>Imagine while reasoning in space: Multimodal visualization-of-thought. C Li, W Wu, H Zhang, Y Xia, S Mao, L Dong, I Vulić, F Wei, 122</p>
<p>Language models can evaluate themselves via probability discrepancy. T Xia, B Yu, Y Wu, Y Chang, C Zhou, arXiv:2405.105162024arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 551232023</p>
<p>A law of next-token prediction in large language models. H He, W J Su, 2024. 2</p>
<p>Constitutional ai: Harmlessness from ai feedback. Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.080732022. 2, 2021arXiv preprint</p>
<p>blob reinforcement fine-tuning. Interconnects, Ai, </p>
<p>On the impact of fine-tuning on chain-of-thought reasoning. E Lobo, C Agarwal, H Lakkaraju, arXiv:2411.15382202423arXiv preprint</p>
<p>Reft: Reasoning with reinforced fine-tuning. L Trung, X Zhang, Z Jie, P Sun, X Jin, H Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 2024. 2, 3, 6, 2036</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 2022. 2, 3, 5, 93515</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.033002024. 2, 3, 6, 8, 101222arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021. 2, 132022arXiv preprint</p>
<p>Selfrag: Learning to retrieve, generate, and critique through selfreflection. A Asai, Z Wu, Y Wang, A Sil, H Hajishirzi, arXiv:2310.115112023arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, M Wang, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Rethinking llm-based preference evaluation. Z Hu, L Song, J Zhang, Z Xiao, J Wang, Z Chen, H Xiong, arXiv:2407.010852024arXiv preprint</p>
<p>Disc-lawllm: Fine-tuning large language models for intelligent legal services. S Yue, W Chen, S Wang, B Li, C Shen, S Liu, Y Zhou, Y Xiao, S Yun, X Huang, arXiv:2309.113252023arXiv preprint</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Y Luo, Z Yang, F Meng, Y Li, J Zhou, Y Zhang, arXiv:2308.087472023arXiv preprint</p>
<p>Towards better web search performance: Pre-training, fine-tuning and learning to rank. H Li, J Chen, W Su, Q Ai, Y Liu, 2023. 2</p>
<p>Reinforcement fine-tuning. Openai, </p>
<p>Sentiment analysis in the era of large language models: A reality check. W Zhang, Y Deng, B Liu, S J Pan, L Bing, arXiv:2305.150052023arXiv preprint</p>
<p>Learning word vectors for sentiment analysis. A Maas, R E Daly, P T Pham, D Huang, A Y Ng, C Potts, Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. the 49th annual meeting of the association for computational linguistics: Human language technologies2011</p>
<p>Recognizing medical search query intent by few-shot learning. Y Wang, S Wang, Y Li, D Dou, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in bioinformatics. 236132022</p>
<p>An llmbased knowledge synthesis and scientific reasoning framework for biomedical discovery. O Wysocki, M Wysocka, D Carvalho, A T Bogatu, D M Gusicuma, M Delmas, H Unsworth, A Freitas, arXiv:2406.186262024arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2, 5810arXiv preprint</p>
<p>Monte carlo tree search boosts reasoning via iterative preference learning. Y Xie, A Goyal, W Zheng, M.-Y Kan, T P Lillicrap, K Kawaguchi, M Shieh, arXiv:2405.0045120241822arXiv preprint</p>
<p>Role play with large language models. M Shanahan, K Mcdonell, L Reynolds, Nature. 62379872023</p>
<p>T Xu, E Helenowski, K A Sankararaman, D Jin, K Peng, E Han, S Nie, C Zhu, H Zhang, W Zhou, arXiv:2409.20370The perfect blend: Redefining rlhf with mixture of judges. 2024arXiv preprint</p>
<p>Beyond sparse rewards: Enhancing reinforcement learning with language model critique in text generation. M Cao, L Shu, L Yu, Y Zhu, N Wichers, Y Liu, L Meng, 2024</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Y Dubois, C X Li, R Taori, T Zhang, I Gulrajani, J Ba, C Guestrin, P S Liang, T B Hashimoto, Advances in Neural Information Processing Systems. 20243615</p>
<p>Multi-turn reinforcement learning from preference human feedback. L Shani, A Rosenberg, A Cassel, O Lang, D Calandriello, A Zipori, H Noga, O Keller, B Piot, I Szpektor, arXiv:2405.146552024arXiv preprint</p>
<p>Falcon: Feedback-driven adaptive long/shortterm memory reinforced coding optimization system. Z Li, Y He, L He, J Wang, T Shi, B Lei, Y Li, Q Chen, arXiv:2410.213492024arXiv preprint</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.0933220211114arXiv preprint</p>
<p>Scaling laws for reward model overoptimization. L Gao, J Schulman, J Hilton, International Conference on Machine Learning. 2023. 2, 1520</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. C Snell, J Lee, K Xu, A Kumar, arXiv:2408.0331420241521arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 2024. 2, 143620</p>
<p>Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. J Jiang, D He, J Allan, Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval. the 37th international ACM SIGIR conference on Research &amp; development in information retrieval2014</p>
<p>Self-evaluation guided beam search for reasoning. Y Xie, K Kawaguchi, Y Zhao, J X Zhao, M.-Y Kan, J He, M Xie, Advances in Neural Information Processing Systems. 3622024</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Y Tian, B Peng, L Song, L Jin, D Yu, H Mi, D Yu, arXiv:2404.122532024218arXiv preprint</p>
<p>Stream of search (SoS): Learning to search in language. K Gandhi, D H J Lee, G Grand, M Liu, W Cheng, A Sharma, N Goodman, First Conference on Language Modeling. 2024</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. K Yang, A M Swope, A Gu, R Chalamala, P Song, S Yu, S Godil, R Prenger, A Anandkumar, 2023. 2</p>
<p>Retrieval-augmented hierarchical in-context reinforcement learning and hindsight modular reflections for task planning with llms. C Sun, S Huang, D Pompili, 2024. 2</p>
<p>Testing gpt-4-o1-preview on math and science problems: A follow-up study. E Davis, arXiv:2410.223402024arXiv preprint</p>
<p>A survey on human-centric llms. J Y Wang, N Sukiennik, T Li, W Su, Q Hao, J Xu, Z Huang, F Xu, Y Li, arXiv:2411.144912024arXiv preprint</p>
<p>A survey on llm-generated text detection: Necessity, methods, and future directions. J Wu, S Yang, R Zhan, Y Yuan, L S Chao, D F Wong, Computational Linguistics. 20251422</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. H Lee, S Phatale, H Mansoor, K R Lu, T Mesnard, J Ferret, C Bishop, E Hall, V Carbune, A Rastogi, 20212023. 3, 9</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.002342022arXiv preprint</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.182232023322arXiv preprint</p>
<p>Folio: Natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, W Zhou, J Coady, D Peng, Y Qiao, L Benson, L Sun, A Wardle-Solano, H Szabo, E Zubova, M Burtell, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, A R Fabbri, W Kryscinski, S Yavuz, Y Liu, X V Lin, S Joty, Y Zhou, C Xiong, R Ying, A Cohan, D Radev, 2024. 3</p>
<p>Self-polish: Enhance reasoning in large language models via problem refinement. Z Xi, S Jin, Y Zhou, R Zheng, S Gao, T Gui, Q Zhang, X Huang, 2024. 3</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, 2023. 3</p>
<p>Improving multi-step reasoning abilities of large language models with direct advantage policy optimization. J Liu, C Wang, C Y Liu, L Zeng, R Yan, Y Sun, Y Liu, Y Zhou, arXiv:2412.182792024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Reasonbert: Pre-trained to reason with distant supervision. X Deng, Y Su, A Lees, Y Wu, C Yu, H Sun, arXiv:2109.049122021arXiv preprint</p>
<p>Arb: Advanced reasoning benchmark for large language models. T Sawada, D Paleka, A Havrilla, P Tadepalli, P Vidas, A Kranias, J J Nay, K Gupta, A Komatsuzaki, 2023. 3</p>
<p>Improving language understanding by generative pre-training. A Radford, 2018</p>
<p>Tutorial on maximum likelihood estimation. I J Myung, Journal of mathematical Psychology. 4712003</p>
<p>Bert-pli: Modeling paragraph-level interactions for legal case retrieval. Y Shao, J Mao, Y Liu, W Ma, K Satoh, M Zhang, S Ma, IJCAI. 2020</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Journal of mathematics and mechanics. R Bellman, 1957310A markovian decision process</p>
<p>Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. S Hao, Y Gu, H Luo, T Liu, X Shao, X Wang, S Xie, H Ma, A Samavedhi, Q Gao, Z Wang, Z Hu, 2024. 3</p>
<p>Scaling up test-time compute with latent reasoning: A recurrent depth approach. J Geiping, S Mcleish, N Jain, J Kirchenbauer, S Singh, B R Bartoldson, B Kailkhura, A Bhatele, T Goldstein, 2025. 3, 20</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>Step-level value preference optimization for mathematical reasoning. G Chen, M Liao, C Li, K Fan, arXiv:2406.108582024arXiv preprint</p>
<p>Reinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, H Daumé, Iii , J Boyd-Graber, arXiv:1707.07402201745arXiv preprint</p>
<p>Substantive and adjectival law. G Williams, Learning the Law. Law Book Company, Limited1982</p>
<p>Sequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, 201645</p>
<p>Self-critical sequence training for image captioning. S J Rennie, E Marcheret, Y Mroueh, J Ross, V Goel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition201745</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Cider: Consensusbased image description evaluation. R Vedantam, C L Zitnick, D Parikh, 2015</p>
<p>Openai gpt-4.5 system card. Openai, 2025. 2025-02-28. 5</p>
<p>Claude 3.7 sonnet. Anthropic, 2025. 2025-02-26. 5, 19</p>
<p>Reka core, flash, and edge: A series of powerful multimodal language models. R Team, A Ormazabal, C Zheng, C D M Autume, D Yogatama, D Fu, D Ong, E Chen, E Lamprecht, H Pham, arXiv:2404.123872024arXiv preprint</p>
<p>B Adler, N Agarwal, A Aithal, D H Anh, P Bhattacharya, A Brundyn, J Casper, B Catanzaro, S Clay, J Cohen, arXiv:2406.11704Nemotron-4 340b technical report. 2024arXiv preprint</p>
<p>The falcon series of open language models. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, Étienne Goffinet, D Hesslow, J Launay, Q Malartic, D Mazzotta, B Noune, B Pannier, G Penedo, 2023. 5</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Lu, arXiv:2409.12186Qwen2. 5-coder technical report. 2024arXiv preprint</p>
<p>Moshi: a speech-text foundation model for real-time dialogue. A Défossez, L Mazaré, M Orsini, A Royer, P Pérez, H Jégou, E Grave, N Zeghidour, 20245tech. rep.</p>
<p>Athene: An rlhf-enhanced language model. Nexusflow, 20245</p>
<p>R Teknium, J Quesnelle, C Guang, arXiv:2408.11857Hermes 3 technical report. 2024arXiv preprint</p>
<p>. Z Ai, Zed, Open. 52025. 500B, Zed AI, RLHF, Multi-modal</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Z Cai, M Cao, H Chen, K Chen, K Chen, X Chen, X Chen, Z Chen, Z Chen, P Chu, arXiv:2403.17297Internlm2 technical report. 2024arXiv preprint</p>
<ol>
<li>220B, Supernova Labs, RLHF, Multi-modal. S Labs, Open. 5Supernova</li>
</ol>
<p>Grok-3: The next generation ai model by xai. 2025-02-24. 52025tech. rep., xAI</p>
<p>Pixtral 12b. P Agrawal, S Antoniak, 2024. 5</p>
<p>Minimax-01: Scaling foundation models with lightning attention. A Minimax, Li, 2025. 5</p>
<p>The amazon nova family of models: Technical report and model card. A A G Intelligence, Amazon Technical Reports. 52024</p>
<p>Fugaku-llm: The largest cpuonly trained language model. T I Fujitsu, Technology, May 2024. 5Fujitsu Research Press Release</p>
<p>Nova: A family of ai models by rubik's ai. R Ai, October 2024. 5Rubik's AI Research</p>
<p>Openai o3 system card. Openai, technical report. 52025OpenAI</p>
<p>Introducing dbrx: a new state-of-the-art open llm. M R Team, Mosaic AI Research. 52024</p>
<p>A Köpf, Y Kilcher, D Von Rütte, S Anagnostidis, Z.-R Tam, K Stevens, A Barhoum, N M Duc, O Stanley, R Nagyfi, S Es, S Suri, D Glushkov, A Dantuluri, A Maguire, C Schuhmann, H Nguyen, A Mattick, 2023. 5Openassistant conversations -democratizing large language model alignment. </p>
<p>T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Zhang, D Rojas, G Feng, H Zhao, arXiv:2406.12793Chatglm: A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Zephyr 141b a39b. A Bartolome, J Hong, N Lee, K Rasul, L Tunstall, 2024. 5</p>
<p>Jurassic-1: Technical details and evaluation. O Lieber, O Sharir, B Lenz, Y Shoham, White Paper. AI21 Labs. 192021</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. K Team, A Du, B Gao, B Xing, C Jiang, C Chen, C Li, C Xiao, C Du, C Liao, arXiv:2501.125992025522arXiv preprint</p>
<p>. M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, J R Lee, Y T Lee, Y Li, W Liu, C C T Mendes, A Nguyen, E Price, G De Rosa, O Saarikivi, A Salim, S Shah, X Wang, R Ward, Y Wu, D Yu, C Zhang, Y Zhang, Phi-4 technical report," 2024. 5</p>
<p>Chameleon: Mixed-modal early-fusion foundation models. C Team, arXiv:2405.098182024arXiv preprint</p>
<p>Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. N Dey, G Gosal, Zhiming, H Chen, W Khachane, R Marshall, M Pathria, J Tom, Hestness, 2023. 5</p>
<p>Bloomberggpt: A large language model for finance. S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, 2023. 5</p>
<p>. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G Van Den Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, L Sifre, Training compute-optimal large language models," 2022. 5</p>
<p>Minimum risk training for neural machine translation. S Shen, Y Cheng, Z He, W He, H Wu, M Sun, Y Liu, 201645</p>
<p>Actor-critic algorithms. V Konda, J Tsitsiklis, Advances in neural information processing systems. 199912</p>
<p>Incremental natural actor-critic algorithms. S Bhatnagar, M Ghavamzadeh, M Lee, R S Sutton, Advances in neural information processing systems. 2042007</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, arXiv:1602.01783201645arXiv preprint</p>
<p>Reinforcement learning through asynchronous advantage actor-critic on a gpu. M Babaeizadeh, I Frosio, S Tyree, J Clemons, J Kautz, arXiv:1611.06256201645arXiv preprint</p>
<p>A natural policy gradient. S M Kakade, Advances in neural information processing systems. 200114</p>
<p>Neuronlike adaptive elements that can solve difficult learning control problems. A G Barto, R S Sutton, C W Anderson, IEEE transactions on systems, man, and cybernetics. 51983</p>
<p>Seqgan: Sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, 2017</p>
<p>A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, arXiv:2412.15115Qwen2. 5 technical report. 202456arXiv preprint</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Moritz, M I Jordan, P Abbeel, 201769</p>
<p>Leverage the average: an analysis of kl regularization in reinforcement learning. N Vieillard, T Kozuno, B Scherrer, O Pietquin, R Munos, M Geist, Advances in Neural Information Processing Systems. 202033</p>
<p>Direct language model alignment from online ai feedback. S Guo, B Zhang, T Liu, T Liu, M Khalman, F Llinares, A Rame, T Mesnard, Y Zhao, B Piot, arXiv:2402.047922024910arXiv preprint</p>
<p>Colo: A contrastive learning based re-ranking framework for one-stage summarization. C An, M Zhong, Z Wu, Q Zhu, X Huang, X Qiu, arXiv:2209.145692022arXiv preprint</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 393/41952</p>
<p>The analysis of permutations. R L Plackett, Journal of the Royal Statistical Society Series C: Applied Statistics. 2421975</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. A Setlur, C Nagpal, A Fisch, X Geng, J Eisenstein, R Agarwal, A Agarwal, J Berant, A Kumar, arXiv:2410.081462024720arXiv preprint</p>
<p>Rewardbench: Evaluating reward models for language modeling. N Lambert, V Pyatkin, J Morrison, L Miranda, B Y Lin, K Chandu, N Dziri, S Kumar, T Zick, Y Choi, arXiv:2403.1378720241920arXiv preprint</p>
<p>Orpo: Monolithic preference optimization without reference model. J Hong, N Lee, J Thorne, 2024. 8</p>
<p>High-dimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, 2018</p>
<p>Scaling laws for reward model overoptimization in direct alignment algorithms. R Rafailov, Y Chittepu, R Park, H S Sikchi, J Hejna, B Knox, C Finn, S Niekum, abs/2406.02900ArXiv. 92024</p>
<p>Offline reinforcement learning for llm multi-step reasoning. H Wang, S Hao, H Dong, S Zhang, Y Bao, Z Yang, Y Wu, 20241022</p>
<p>Superhf: Supervised iterative learning from human feedback. G Mukobi, P Chatain, S Fong, R Windesheim, G Kutyniok, K Bhatia, S Alberti, arXiv:2310.167632023arXiv preprint</p>
<p>On task performance and model calibration with supervised and self-ensembled in-context learning. C Li, H Zhou, G Glavaš, A Korhonen, I Vulić, arXiv:2312.137722023arXiv preprint</p>
<p>Preference optimization with multi-sample comparisons. C Wang, Z Zhao, C Zhu, K A Sankararaman, M Valko, X Cao, Z Chen, M Khabsa, Y Chen, H Ma, S Wang, 2024. 11</p>
<p>Challenging big-bench tasks and whether chain-ofthought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, arXiv:2210.0926120221119arXiv preprint</p>
<p>Chain of preference optimization: Improving chain-of-thought reasoning in LLMs. X Zhang, C Du, T Pang, Q Liu, W Gao, M Lin, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, arXiv:2110.0820720211222arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652202112arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023. 12</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, March 202312</p>
<p>Free dolly: Introducing the world's first truly open instruction-tuned llm. M Conover, M Hayes, A Mathur, J Xie, J Wan, S Shah, A Ghodsi, P Wendell, M Zaharia, R Xin, 2023. 12</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239202212arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 20231220</p>
<p>Teaching small language models to reason. L C Magister, J Mallinson, J Adamek, E Malmi, A Severyn, arXiv:2212.0841020221213arXiv preprint</p>
<p>Llava-cot: Let vision language models reason step-by-step. G Xu, P Jin, H Li, Y Song, L Sun, L Yuan, 2024. 12</p>
<p>Llamav-o1: Rethinking step-by-step visual reasoning in llms. O Thawakar, D Dissanayake, K More, R Thawkar, A Heakl, N Ahsan, Y Li, M Zumri, J Lahoud, R M Anwer, H Cholakkal, I Laptev, M Shah, F S Khan, S Khan, 20251219</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, arXiv:2305.1431420231314arXiv preprint</p>
<p>GPTQ: Accurate post-training compression for generative pretrained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, arXiv:2210.173232022arXiv preprint</p>
<p>SparseGPT: Massive language models can be accurately pruned in one-shot. E Frantar, D Alistarh, arXiv:2301.0077420231314arXiv preprint</p>
<p>Peft: State-of-the-art parameter-efficient finetuning methods. S Mangrulkar, S Gugger, L Debut, Y Belkada, S Paul, B Bossan, 2022. 13</p>
<p>-bit matrix multiplication for transformers at scale. T Dettmers, M Lewis, Y Belkada, L Zettlemoyer, arXiv:2208.07339Llm. 1382022arXiv preprintint8(</p>
<p>Adaptive budget allocation for parameterefficient fine-tuning. Q Zhang, M Chen, A Bukharin, P He, Y Cheng, W Chen, T Zhao, The Eleventh International Conference on Learning Representations. 20231320</p>
<p>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. X Liu, K Ji, Y Fu, Z Du, Z Yang, J Tang, abs/2110.07602CoRR. 132021</p>
<p>Datasets: A community library for natural language processing. Q Lhoest, A Villanova, Y Del Moral, A Jernite, P Thakur, S Von Platen, J Patil, M Chaumond, J Drame, L Plu, J Tunstall, M Davison, G Šaško, B Chhablani, S Malik, T Le Brandeis, V Scao, C Sanh, N Xu, A Patry, P Mcmillan-Major, S Schmid, C Gugger, T Delangue, L Matussière, S Debut, P Bekman, T Cistac, V Goehringer, F Mustar, A Lagunas, T Rush, Wolf, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2021 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsPunta CanaAssociation for Computational LinguisticsNov. 2021. 13</p>
<p>Efficient tar-based sharding format for petascale distributed training. A Torralba, Others, Webdataset: A format for petascale deep learning</p>
<p>Git-like version control for datasets and machine learning pipelines. I Iterative, Dvc: Data version control</p>
<p>. N Richardson, I Cook, N Crane, D Dunnington, R François, J Keane, D Moldovan-Grünfeld, J Ooms, J Wujciak-Jens, Apache Arrow, arrowIntegration to 'Apache' 'Arrow', 2025. R package version 19.0.0</p>
<p>High-speed compression algorithm for training data storage. I Facebook, transfer. 13Zstandard: High-speed compression algorithm</p>
<p>Cleanlab: The standard data-centric ai package for machine learning with noisy labels. C Team, Automatic detection of label errors and outliers in training datasets</p>
<p>Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale. R Y Aminabadi, S Rajbhandari, M Zhang, A A Awan, C Li, D Li, E Zheng, J Rasley, S Smith, O Ruwase, Y He, 2022. 13</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. M Shoeybi, M Patwary, R Puri, P Legresley, J Casper, B Catanzaro, 2020</p>
<p>Colossal-ai: A unified deep learning system for large-scale parallel training. S Li, H Liu, Z Bian, J Fang, H Huang, Y Liu, B Wang, Y You, 2023. 13</p>
<p>Horovod: fast and easy distributed deep learning in tensorflow. A Sergeev, M D Balso, 2018</p>
<p>Ray: A distributed framework for emerging ai applications. P Moritz, R Nishihara, S Wang, A Tumanov, R Liaw, E Liang, M Elibol, Z Yang, W Paul, M I Jordan, I Stoica, 2018</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, 2023. 13</p>
<p>Exploring tensorrt to improve realtime inference for deep learning. Y Zhou, K Yang, 2022 IEEE 24th Int Conf on High Performance Computing &amp; Communications; 8th Int Conf on Data Science &amp; Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud &amp; Big Data Systems &amp; Application (HPCC/DSS/SmartCity/DependSys). 2022</p>
<p>Triton: an intermediate language and compiler for tiled neural network computations. P Tillet, H.-T Kung, D Cox, Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages2019</p>
<p>Unified inference engine with hardware-specific optimizations. O Community, Onnx: Open neural network exchange</p>
<p>Runtime for Intel CPUs/iGPUs with pruning/quantization support. I Corporation, 2025Openvino: Intel optimization toolkit</p>
<p>The indirect convolution algorithm. M Dukhan, 2019</p>
<p>Deterministic lowlatency inference via custom tensor streaming processor. I Groq, 2025Groq: Ai accelerator</p>
<p>Analyzing the evolution and maintenance of ml models on hugging face. J Castaño, S Martínez-Fernández, X Franch, J Bogner, 2024. 13</p>
<p>Automatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z De-Vito, Z Lin, A Desmaison, L Antiga, A Lerer, 2017</p>
<p>S Hao, Y Gu, H Luo, T Liu, X Shao, X Wang, S Xie, H Ma, A Samavedhi, Q Gao, arXiv:2404.05221Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. 2024arXiv preprint</p>
<p>Bimedix: Bilingual medical mixture of experts llm. S Pieri, S S Mullappilly, F S Khan, R M Anwer, S Khan, T Baldwin, H Cholakkal, arXiv:2402.132532024arXiv preprint</p>
<p>Finbert: A pretrained language model for financial communications. Y Yang, M C S Uy, A Huang, arXiv:2006.080972020arXiv preprint</p>
<p>Climategpt: Towards ai synthesizing interdisciplinary research on climate change. D Thulke, Y Gao, P Pelser, R Brune, R Jalota, F Fok, M Ramos, I Van Wyk, A Nasir, H Goldstein, arXiv:2401.096462024arXiv preprint</p>
<p>Arabic mini-climategpt: A climate change and sustainability tailored arabic llm. S S Mullappilly, A Shaker, O Thawakar, H Cholakkal, R M Anwer, S Khan, F S Khan, arXiv:2312.093662023arXiv preprint</p>
<p>Codet5: Identifieraware unified pre-trained encoder-decoder models for code understanding and generation. Y Wang, W Wang, S Joty, S C Hoi, arXiv:2109.008592021arXiv preprint</p>
<p>Geochat: Grounded large vision-language model for remote sensing. K Kuckreja, M S Danish, M Naseer, A Das, S Khan, F S Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Bimedix2: Bio-medical expert lmm for diverse medical modalities. S S Mullappilly, M I Kurpath, S Pieri, S Y Alseiari, S Cholakkal, K Aldahmani, F Khan, R Anwer, S Khan, T Baldwin, arXiv:2412.077692024arXiv preprint</p>
<p>Videochatgpt: Towards detailed video understanding via large vision and language models. M Maaz, H Rasheed, S Khan, F S Khan, arXiv:2306.054242023arXiv preprint</p>
<p>Video-llava: Learning united visual representation by alignment before projection. B Lin, Y Ye, B Zhu, J Cui, M Ning, P Jin, L Yuan, arXiv:2311.101222023arXiv preprint</p>
<p>Video-llama: An instructiontuned audio-visual language model for video understanding. H Zhang, X Li, L Bing, arXiv:2306.028582023arXiv preprint</p>
<p>Chartllama: A multimodal llm for chart understanding and generation. Y Han, C Zhang, X Chen, X Yang, Z Wang, G Yu, B Fu, H Zhang, arXiv:2311.164832023arXiv preprint</p>
<p>A survey on model compression for large language models. X Zhu, J Li, Y Liu, C Ma, W Wang, Transactions of the Association for Computational Linguistics. 12132024</p>
<p>Efficient large language models: A survey. Z Wan, X Wang, C Liu, S Alam, Y Zheng, J Liu, Z Qu, S Yan, Y Zhu, Q Zhang, arXiv:2312.038632023arXiv preprint</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. C.-Y Hsieh, C.-L Li, C.-K Yeh, H Nakhost, Y Fujii, A Ratner, R Krishna, C.-Y Lee, T Pfister, arXiv:2305.023012023arXiv preprint</p>
<p>Minillm: Knowledge distillation of large language models. Y Gu, L Dong, F Wei, M Huang, arXiv:2306.085432023arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Parameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, 2019</p>
<p>Harpy, a connected speech recognition system. B P Lowerre, B R Reddy, The Journal of the Acoustical Society of America. 59S1141976</p>
<p>Sequence transduction with recurrent neural networks. A Graves, arXiv:1211.3711201214arXiv preprint</p>
<p>Fast best-of-n decoding via speculative rejection. H Sun, M Haider, R Zhang, H Yang, J Qiu, M Yin, M Wang, P Bartlett, A Zanette, 2024. 14</p>
<p>A general language assistant as a laboratory for alignment. A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Dassarma, arXiv:2112.00861202114arXiv preprint</p>
<p>Improving alignment of dialogue agents via targeted human judgements. A Glaese, N Mcaleese, M Trębacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.14375202214arXiv preprint</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, P F Christiano, Advances in Neural Information Processing Systems. 20203314</p>
<p>J Q Yang, S Salamatian, Z Sun, A T Suresh, A Beirami, arXiv:2404.01730Asymptotics of language model alignment. 202415arXiv preprint</p>
<p>Fast best-of-n decoding via speculative rejection. H Sun, M Haider, R Zhang, H Yang, J Qiu, M Yin, M Wang, P Bartlett, A Zanette, arXiv:2410.20290202415arXiv preprint</p>
<p>Measuring goodhart's law. J Hilton, L Gao, OpenAI Research Blog. 152022</p>
<p>Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K W Lee, E.-P Lim, arXiv:2305.04091202315arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.10560202215arXiv preprint</p>
<p>Universal selfconsistency for large language models. X Chen, R Aksitov, U Alon, J Ren, K Xiao, P Yin, S Prakash, C Sutton, X Wang, D Zhou, ICML 2024 Workshop on In-Context Learning. 15</p>
<p>On the analysis of human problem solving protocols. A Newell, 1966. 16</p>
<p>Improving llm reasoning with multi-agent treeof-thought validator agent. F Haji, M Bethany, M Tabar, J Chiang, A Rios, P Najafirad, arXiv:2409.11527202416arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, T Hoefler, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 20243816</p>
<p>Llm tree search. D Wilson, arXiv:2410.19117202417arXiv preprint</p>
<p>A baseline for detecting misclassified and out-of-distribution examples in neural networks. D Hendrycks, K Gimpel, arXiv:1610.02136201617arXiv preprint</p>
<p>Strength in numbers: Estimating confidence of large language models by prompt agreement. G Portillo Wightman, A Delucia, M Dredze, Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing. the 3rd Workshop on Trustworthy Natural Language ProcessingTrustNLP 2023. 202317</p>
<p>Verifierq: Enhancing llm test time compute with q-learning-based verifiers. J Qi, H Tang, Z Zhu, arXiv:2410.08048202417arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, 202317</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 202317arXiv preprint</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. R Coulom, International conference on computers and games. Springer200618</p>
<p>The evolution of computing: Alphago. J X Chen, Computing in Science &amp; Engineering. 184182016</p>
<p>Bandit based monte-carlo planning. L Kocsis, C Szepesvári, European conference on machine learning. Springer200618</p>
<p>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. H W Sprueill, C Edwards, M V Olarte, U Sanyal, H Ji, S Choudhury, arXiv:2310.1442020231820arXiv preprint</p>
<p>Make every move count: Llm-based high-quality rtl code generation using mcts. M Delorenzo, A B Chowdhury, V Gohil, S Thakur, R Karri, S Garg, J Rajendran, arXiv:2402.03289202418arXiv preprint</p>
<p>Ensembling large language models with process reward-guided tree search for better complex reasoning. S Park, X Liu, Y Gong, E Choi, arXiv:2412.15797202418arXiv preprint</p>
<p>Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search. M Shen, G Zeng, Z Qi, Z.-W Hong, Z Chen, W Lu, G Wornell, S Das, D Cox, C Gan, arXiv:2502.02508202518arXiv preprint</p>
<p>Malt: Improving reasoning with multi-agent llm training. S R Motwani, C Smith, R J Das, R Rafailov, I Laptev, P H S Torr, F Pizzati, R Clark, C S De Witt, 202518</p>
<p>Foundational challenges in assuring alignment and safety of large language models. U Anwar, A Saparov, J Rando, D Paleka, M Turpin, P Hase, E S Lubana, E Jenner, S Casper, O Sourbut, arXiv:2404.09932202418arXiv preprint</p>
<p>Bi-factorial preference optimization: Balancing safety-helpfulness in language models. W Zhang, P H Torr, M Elhoseiny, A Bibi, arXiv:2408.15313202418arXiv preprint</p>
<p>Imagine while reasoning in space: Multimodal visualization-of-thought. C Li, W Wu, H Zhang, Y Xia, S Mao, L Dong, I Vulić, F Wei, arXiv:2501.07542202519arXiv preprint</p>
<p>On the representational capacity of neural language models with chainof-thought reasoning. F Nowak, A Svete, A Butoi, R Cotterell, arXiv:2406.14197202419arXiv preprint</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Z Li, H Liu, D Zhou, T Ma, arXiv:2402.128752024119arXiv preprint</p>
<p>The expressive power of transformers with chain of thought. W Merrill, A Sabharwal, arXiv:2310.07923202319arXiv preprint</p>
<p>Scaling testtime compute optimally can be more effective than scaling llm parameters. C V Snell, J Lee, K Xu, A Kumar, The Thirteenth International Conference on Learning Representations. 19</p>
<p>Analysing mathematical reasoning abilities of neural models. Saxton, arXiv:1904.01557201919</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168202119arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, arXiv:2309.12284202319arXiv preprint</p>
<p>WorldTree v2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference. Z Xie, S Thiem, J Martin, E Wainwright, S Marmorstein, P Jansen, Proceedings of the Twelfth Language Resources and Evaluation Conference. N Calzolari, F Béchet, P Blache, K Choukri, C Cieri, T Declerck, S Goggi, H Isahara, B Maegaard, J Mariani, H Mazo, A Moreno, J Odijk, S Piperidis, the Twelfth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationMay 20201719</p>
<p>Visually grounded reasoning across languages and cultures. F Liu, E Bugliarello, E M Ponti, S Reddy, N Collier, D Elliott, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNov. 2021. 19Online and Punta Cana, Dominican Republic</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. X Yue, Y Ni, K Zhang, T Zheng, R Liu, G Zhang, S Stevens, D Jiang, W Ren, Y Sun, C Wei, B Yu, R Yuan, R Sun, M Yin, B Zheng, Z Yang, Y Liu, W Huang, H Sun, Y Su, W Chen, Proceedings of CVPR. CVPR202419</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, arXiv:2109.07958202119arXiv preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. X Yue, arXiv:2309.05653202319arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)20211920</p>
<p>Aligning ai with shared human values. D Hendrycks, C Burns, S Basart, A Critch, J Li, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)20211920</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proc. of NAACL. of NAACL201919</p>
<p>Helpsteer: Multi-attribute helpfulness dataset for steerlm. Z Wang, Y Dong, J Zeng, V Adams, M N Sreedhar, D Egert, O Delalleau, J P Scowcroft, N Kant, A Swope, arXiv:2311.09528202319arXiv preprint</p>
<p>Ultrafeedback: Boosting language models with high-quality feedback. G Cui, L Yuan, N Ding, G Yao, W Zhu, Y Ni, G Xie, Z Liu, M Sun, 2023. 19</p>
<p>D4rl: Datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, G Tucker, S Levine, 20201920</p>
<p>Learning to modulate pre-trained models in rl. T Schmied, M Hofmarcher, F Paischer, R Pascanu, S Hochreiter, Advances in Neural Information Processing Systems. 20243620</p>
<p>Minerl: A large-scale dataset of minecraft demonstrations. W H Guss, B Houghton, N Topin, P Wang, C Codel, M Veloso, R Salakhutdinov, 20191920</p>
<p>CulturaX: A cleaned, enormous, and multilingual dataset for large language models in 167 languages. T Nguyen, C V Nguyen, V D Lai, H Man, N T Ngo, F Dernoncourt, R A Rossi, T H Nguyen, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024Torino, ItaliaELRA and ICCLMay 20241920</p>
<p>Pangea: A fully open multilingual multimodal llm for 39 languages. X Yue, Y Song, A Asai, S Kim, J De Dieu Nyandwi, S Khanuja, A Kantharuban, L Sutawika, S Ramamoorthy, G Neubig, arXiv:2410.1615320241920arXiv preprint</p>
<p>BERT: pretraining of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, abs/1810.04805CoRR. 19202018</p>
<p>Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation. Y Liang, N Duan, Y Gong, N Wu, F Guo, W Qi, M Gong, L Shou, D Jiang, G Cao, X Fan, R Zhang, R Agrawal, E Cui, S Wei, T Bharti, Y Qiao, J.-H Chen, W Wu, S Liu, F Yang, D Campos, R Majumder, M Zhou, 2004.01401. 202019arXiv</p>
<p>Mm-eval: A multilingual meta-evaluation benchmark for llmas-a-judge and reward models. G Son, D Yoon, J Suk, J Aula-Blasco, M Aslan, V T Kim, S B Islam, J Prats-Cristià, L Tormo-Bañuelos, S Kim, 20241920</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. S Et , 20221920</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 20233620</p>
<p>The second conversational intelligence challenge (convai2). E Dinan, V Logacheva, V Malykh, A H Miller, K Shuster, J Urbanek, D Kiela, A Szlam, I Serban, R Lowe, S Prabhumoye, A W Black, A I Rudnicky, J Williams, J Pineau, M S Burtsev, J Weston, abs/1902.00098CoRR. 19202019</p>
<p>MultiWOZ 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines. M Eric, R Goel, S Paul, A Sethi, S Agarwal, S Gao, D Hakkani-Tur, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationMay 20201920</p>
<p>Learning question classifiers. X Li, D Roth, COLING 2002: The 19th International Conference on Computational Linguistics. 20021920</p>
<p>Toward semantics-based answer pinpointing. E Hovy, L Gerber, U Hermjakob, C.-Y Lin, D Ravichandran, Proceedings of the First International Conference on Human Language Technology Research. the First International Conference on Human Language Technology Research20011920</p>
<p>BEIR: A heterogeneous benchmark for zeroshot evaluation of information retrieval models. N Thakur, N Reimers, A Rücklé, A Srivastava, I Gurevych, Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 20211920</p>
<p>Do language models enjoy their own stories? Prompting large language models for automatic story evaluation. C Chhun, F M Suchanek, C Clavel, Transactions of the Association for Computational Linguistics. 12192024</p>
<p>Storyer: Automatic story evaluation via ranking, rating and reasoning. H Chen, D M Vo, H Takamura, Y Miyao, H Nakayama, 2022. 19</p>
<p>Beavertails: Towards improved safety alignment of llm via a human-preference dataset. J Ji, M Liu, J Dai, X Pan, C Zhang, C Bian, B Chen, R Sun, Y Wang, Y Yang, Advances in Neural Information Processing Systems. 20243620</p>
<p>Cvalues: Measuring the values of chinese large language models from safety to responsibility. G Xu, J Liu, M Yan, H Xu, J Si, Z Zhou, P Yi, X Gao, J Sang, R Zhang, arXiv:2307.09705202319arXiv preprint</p>
<p>Crosstask generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, ACL. 202219</p>
<p>Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Arunkumar, A Ashok, A S Dhanasekaran, A Naik, D Stap, EMNLP. 192022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 202220</p>
<p>Self-critiquing models for assisting human evaluators. W Saunders, C Yeh, J Wu, S Bills, L Ouyang, J Ward, J Leike, arXiv:2206.05802202220arXiv preprint</p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361202020arXiv preprint</p>
<p>Solving general arithmetic word problems. S Roy, D Roth, 2016. 20</p>
<p>Regularized best-of-n sampling to mitigate reward hacking for language model alignment. Y Jinnai, T Morimura, K Ariu, K Abe, arXiv:2404.01054202420arXiv preprint</p>
<p>Odin: Disentangled reward mitigates hacking in rlhf. L Chen, C Zhu, D Soselia, J Chen, T Zhou, T Goldstein, H Huang, M Shoeybi, B Catanzaro, abs/2402.07319ArXiv. 202024</p>
<p>Rrm: Robust reward model training mitigates reward hacking. T Liu, W Xiong, J Ren, L Chen, J Wu, R Joshi, Y Gao, J Shen, Z Qin, T Yu, D Sohn, A Makarova, J Liu, Y Liu, B Piot, A Ittycheriah, A Kumar, M Saleh, ArXiv. </p>
<p>Beyond reward hacking: Causal rewards for large language model alignment. C Wang, Z Zhao, Y Jiang, Z Chen, C Zhu, Y Chen, J Liu, L Zhang, X Fan, H Ma, S.-Y Wang, 2025. 20</p>
<p>A survey of monte carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 41202012</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 20243620</p>
<p>Complexity-based prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, The Eleventh International Conference on Learning Representations. 202220</p>
<p>Metacognition for artificial intelligence system safety-an approach to safe and desired behavior. B Johnson, Safety Science. 151212022</p>
<p>Early access for safety testing. Openai, 2024. 20</p>
<p>Reward-robust rlhf in llms. Y Yan, X Lou, J Li, Y Zhang, J Xie, C Yu, Y Wang, D Yan, Y Shen, abs/2409.15360ArXiv. 202024</p>
<p>Explainable legal case matching via inverse optimal transport-based rationale extraction. W Yu, Z Sun, J Xu, Z Dong, X Chen, H Xu, J.-R Wen, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval202220</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. A Amini, S Gabriel, P Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, 2019. 20</p>
<p>Driving with llms: Fusing object-level vector modality for explainable autonomous driving. L Chen, O Sinavski, J Hünermann, A Karnsund, A J Willmott, D Birch, D Maund, J Shotton, 2024 IEEE International Conference on Robotics and Automation (ICRA). 202320</p>
<p>Bias patterns in the application of llms for clinical decision support: A comprehensive study. R Poulain, H Fayyaz, R Beheshti, arXiv:2404.15149202420arXiv preprint</p>
<p>Biasalert: A plug-andplay tool for social bias detection in llms. Z Fan, R Chen, R Xu, Z Liu, arXiv:2407.10241202420arXiv preprint</p>
<p>Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation. M Li, T Shi, C Ziems, M.-Y Kan, N F Chen, Z Liu, D Yang, arXiv:2310.15638202320arXiv preprint</p>
<p>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and llm-as-a-judge. A Elangovan, J Ko, L Xu, M Elyasi, L Liu, S Bodapati, D Roth, arXiv:2410.03775202420arXiv preprint</p>
<p>Can llm be a personalized judge?. Y R Dong, T Hu, N Collier, arXiv:2406.11657202420arXiv preprint</p>
<p>Learning personalized story evaluation. D Wang, K Yang, H Zhu, X Yang, A Cohen, L Li, Y Tian, arXiv:2310.03304202320arXiv preprint</p>
<p>Privacy in fine-tuning large language models: Attacks, defenses, and future directions. H Du, S Liu, L Zheng, Y Cao, A Nakamura, L Chen, 2024. 202122</p>
<p>Free process rewards without process labels. L Yuan, W Li, H Chen, G Cui, N Ding, K Zhang, B Zhou, Z Liu, H Peng, arXiv:2412.01981202420arXiv preprint</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, abs/2310.12931ArXiv. 202023</p>
<p>Glore: When, where, and how to improve llm reasoning via global and local refinements. A Havrilla, S C Raparthy, C Nalmpantis, J Dwivedi-Yu, M Zhuravinskyi, E Hambro, R Railneau, abs/2402.10963ArXiv. 202024</p>
<p>Curlora: Stable llm continual fine-tuning and catastrophic forgetting mitigation. M Fawi, 2024. 20</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. C Fu, P Chen, Y Shen, Y Qin, M Zhang, X Lin, Z Qiu, W Lin, J Yang, X Zheng, K Li, X Sun, R Ji, ArXiv. </p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Y Wang, Z Yu, Z Zeng, L Yang, C Wang, H Chen, C Jiang, R Xie, J Wang, X Xie, W Ye, S.-B Zhang, Y Zhang, ArXiv. </p>
<p>Improving lora in privacypreserving federated learning. Y Sun, Z Li, Y Li, B Ding, ArXiv. 20212403.12313, 2024</p>
<p>Fedeval-llm: Federated evaluation of large language models on downstream tasks with collective wisdom. Y He, Y Kang, L Fan, Q Yang, arXiv:2404.12273202420arXiv preprint</p>
<p>Offsetbias: Leveraging debiased data for tuning evaluators. J Park, S Jwa, M Ren, D Kim, S Choi, arXiv:2407.06551202420arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, 2023. 20</p>
<p>Generative verifiers: Reward modeling as next-token prediction. L Zhang, A Hosseini, H Bansal, M Kazemi, A Kumar, R Agarwal, The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24. 202420</p>
<p>FPC: Fine-tuning with prompt curriculum for relation extraction. S Yang, D Song, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Long Papers. Y He, H Ji, S Li, Y Liu, C.-H Chang, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsNov. 2022. 201Online only</p>
<p>Rankrag: Unifying context ranking with retrieval-augmented generation in llms. Y Yu, W Ping, Z Liu, B Wang, J You, C Zhang, M Shoeybi, B Catanzaro, Advances in Neural Information Processing Systems. 20253720</p>
<p>Ra-dit: Retrieval-augmented dual instruction tuning. X V Lin, X Chen, M Chen, W Shi, M Lomeli, R James, P Rodriguez, J Kahn, G Szilvasy, M Lewis, The Twelfth International Conference on Learning Representations. 202320</p>
<p>Raft: Adapting language model to domain specific rag. T Zhang, S G Patil, N Jain, S Shen, M Zaharia, I Stoica, J E Gonzalez, First Conference on Language Modeling. 202420</p>
<p>Harmful fine-tuning attacks and defenses for large language models: A survey. T Huang, S Hu, F Ilhan, S F Tekin, L Liu, arXiv:2409.181692024arXiv preprint</p>
<p>Measuring progress on scalable oversight for large language models. S R Bowman, J Hyun, E Perez, E Chen, C Pettit, S Heiner, K Lukošiūtė, A Askell, A Jones, A Chen, arXiv:2211.035402022arXiv preprint</p>
<p>Llms for semiautomated data science: Introducing caafe for context-aware automated feature engineering. N Hollmann, S Müller, F Hutter, CoRR, 2023. 21</p>
<p>Malt: Improving reasoning with multi-agent llm training. S R Motwani, C Smith, R J Das, M Rybchuk, P H Torr, I Laptev, F Pizzati, R Clark, C S De Witt, arXiv:2412.019282024arXiv preprint</p>
<p>Acc-debate: An actor-critic approach to multi-agent debate. A Estornell, J.-F Ton, Y Yao, Y Liu, arXiv:2411.000532024arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. L Luo, Y Liu, R Liu, S Phatale, H Lara, Y Li, L Shu, Y Zhu, L Meng, J Sun, arXiv:2406.06592202422arXiv preprint</p>
<p>Improving reinforcement learning from human feedback using contrastive rewards. W Shen, X Zhang, Y Yao, R Zheng, H Guo, Y Liu, arXiv:2403.07708202422arXiv preprint</p>
<p>Long-term credit assignment via model-based temporal shortcuts. M Ma, P D'oro, Y Bengio, P.-L Bacon, Deep RL Workshop NeurIPS 2021. 202122</p>
<p>A survey of temporal credit assignment in deep reinforcement learning. E Pignatelli, J Ferret, M Geist, T Mesnard, H Van Hasselt, O Pietquin, L Toni, arXiv:2312.01072202322arXiv preprint</p>
<p>Generalization of reinforcement learning with policy-aware adversarial data augmentation. H Zhang, Y Guo, 2021. 22</p>
<p>Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. A Ahmadian, C Cremer, M Gallé, M Fadaee, J Kreutzer, O Pietquin, A Üstün, S Hooker, arXiv:2402.14740202422arXiv preprint</p>
<p>Hetal: Efficient privacy-preserving transfer learning with homomorphic encryption. S Lee, G Lee, J W Kim, J Shin, M.-K Lee, 2024. 22</p>
<p>Distributed differential privacy via shuffling versus aggregation: A curious study. Y Wei, J Jia, Y Wu, C Hu, C Dong, Z Liu, X Chen, Y Peng, S Wang, IEEE Transactions on Information Forensics and Security. 19222024</p>
<p>Agent-pro: Learning to evolve via policy-level reflection and optimization. W Zhang, K Tang, H Wu, M Wang, Y Shen, G Hou, Z Tan, P Li, Y Zhuang, W Lu, arXiv:2402.17574202422arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. C Ma, J Zhang, Z Zhu, C Yang, Y Yang, Y Jin, Z Lan, L Kong, J He, arXiv:2401.13178202422arXiv preprint</p>
<p>Aflow: Automating agentic workflow generation. J Zhang, J Xiang, Z Yu, F Teng, X Chen, J Chen, M Zhuge, X Cheng, S Hong, J Wang, arXiv:2410.10762202422arXiv preprint</p>
<p>Multi-agent causal discovery using large language models. H D Le, X Xia, Z Chen, arXiv:2407.15073202422arXiv preprint</p>
<p>A multi-llm debiasing framework. D M Owens, R A Rossi, S Kim, T Yu, F Dernoncourt, X Chen, R Zhang, J Gu, H Deilamsalehy, N Lipka, arXiv:2409.13884202422arXiv preprint</p>
<p>Genainet: Enabling wireless collective intelligence via knowledge transfer and reasoning. H Zou, Q Zhao, L Bariah, Y Tian, M Bennis, S Lasaulce, M Debbah, F Bader, abs/2402.16631ArXiv. 222024</p>
<p>Adaptive stress testing: Finding likely failure events with reinforcement learning. R Lee, O J Mengshoel, A Saksena, R Gardner, D Genin, J Silbermann, M Owen, M J Kochenderfer, 202022</p>
<p>Gradients without backpropagation. A G Baydin, B A Pearlmutter, D Syme, F Wood, P Torr, 2022. 22</p>
<p>Arondight: Red teaming large vision language models with auto-generated multi-modal jailbreak prompts. Y Liu, C Cai, X Zhang, X Yuan, C Wang, ACM Multimedia. 202422</p>
<p>Aligning large language models with self-generated preference data. D Kim, K Lee, J Shin, J Kim, arXiv:2406.04412202422arXiv preprint</p>
<p>Crome: Cross-modal adapters for efficient multimodal llm. S Ebrahimi, S Ö Arik, T Nama, T Pfister, abs/2408.06610ArXiv. 2222024</p>
<p>Tokenskip: Controllable chain-of-thought compression in llms. H Xia, Y Li, C T Leong, W Wang, W Li, 2025. 22</p>
<p>Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition. Z Ma, W Wu, Z Zheng, Y Guo, Q Chen, S Zhang, X Chen, ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 202322</p>
<p>Training large language models for reasoning through reverse curriculum reinforcement learning. Z Xi, W Chen, B Hong, S Jin, R Zheng, W He, Y Ding, S Liu, X Guo, J Wang, H Guo, W Shen, X Fan, Y Zhou, S Dou, X Wang, X Zhang, P Sun, T Gui, Q Zhang, X Huang, abs/2402.05808ArXiv. 222024</p>
<p>Affordance-guided reinforcement learning via visual prompting. O Y Lee, A Xie, K Fang, K Pertsch, C Finn, abs/2407.10341ArXiv. 222024</p>
<p>Rejection improves reliability: Training llms to refuse unknown questions using rl from knowledge feedback. H Xu, Z Zhu, D Ma, S Zhang, S Fan, L Chen, K Yu, abs/2403.18349ArXiv. 222024</p>
<p>Do not think that much for 2+3=? on the overthinking of o1-like llms. X Chen, J Xu, T Liang, Z He, J Pang, D Yu, L Song, Q Liu, M Zhou, Z Zhang, R Wang, Z Tu, H Mi, D Yu, ArXiv. </p>
<p>Beyond games: a systematic review of neural monte carlo tree search applications. M Kemmerling, D Lütticke, R H Schmitt, Applied Intelligence. 541222024</p>
<p>Personal llm agents: Insights and survey about the capability, efficiency and security. Y Li, H Wen, W Wang, X Li, Y Yuan, G Liu, J Liu, W Xu, X Wang, Y Sun, R Kong, Y Wang, H Geng, J Luan, X Jin, Z.-L Ye, G Xiong, F Zhang, X Li, M Xu, Z Li, P Li, Y Liu, Y Zhang, Y Liu, abs/2401.05459ArXiv. 222024</p>
<p>Revisiting catastrophic forgetting in large language model tuning. H Li, L Ding, M Fang, D Tao, 2024. 22</p>
<p>When benchmarks are targets: Revealing the sensitivity of large language model leaderboards. N Alzahrani, H A Alyahya, Y Alnumay, S Alrashed, S Alsubaie, Y Almushaykeh, F Mirza, N Alotaibi, N Altwairesh, A Alowisheq, M S Bari, H Khan, 202423</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-653 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-653</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-653</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3" target="_blank">The Effect of Sampling Temperature on Problem Solving in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks, and these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains.</p>
                <p><strong>Paper Abstract:</strong> In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e653.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e653.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling Temperature (τ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding hyperparameter that rescales logits in the softmax to control output randomness; varied across experiments (0.0–1.6) to study its effect on accuracy and output variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (e.g., GPT-3.5, GPT-4, Claude 3, Llama 2 ...)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / NLP — LLM problem solving (MCQA across multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluate effect of sampling temperature on multiple-choice question answering (MCQA) accuracy and text-output variability.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling temperature (τ) itself; sampling method inherent stochasticity; prompt engineering differences; model-to-model differences and API implementations; randomness across repeated attempts; formatting variability of outputs; other sampling hyperparameters (top-k, top-p, repetition penalty) noted as potential sources but were held fixed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Correct-answer accuracy (mean across repeated attempts); text-similarity metrics (Jaccard, Bag-of-Words, TF-IDF, Levenshtein, BLEU, SBERT semantic similarity); statistical tests (Kruskal-Wallis H and p-values; Dunn-Bonferroni post-hoc for pairwise comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Accuracy remained statistically stable across τ ∈ [0.0, 1.0] (example: GPT-3.5 CoT on 1,000-question exam: H(10)=10.439, p=0.403). Text similarity metrics decreased (i.e., variability increased) monotonically with increasing τ and dropped rapidly after τ ≈ 1.0; accuracy and coherence collapse observed above ≈1.0, reaching incoherence around 1.6. No single numerical similarity-score table was reported in-text (plots provided).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Mean correct-answer accuracy over 10 attempts per question; Kruskal-Wallis test across temperature settings to test significance; Dunn-Bonferroni for pairwise differences; text-similarity scores to measure consistency between runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Across models, prompts, and domains the paper found no statistically significant change in MCQA accuracy across τ ∈ [0.0,1.0] (e.g., GPT-3.5 H=10.439 p=0.403); however, response text variability increases with τ and reproducibility of exact textual outputs decreases. Specific numeric reproducibility percentages were not reported beyond statistical test outputs and similarity-curve plots.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Formatting sensitivity (models producing incorrectly formatted answers); all-or-nothing behavior at τ=0.0 vs more random outputs at τ>0.0 causing per-question variation; limited sample sizes (100 and 1,000 question exams) and cost/runtime constraints; interactions with other sampling parameters not explored; model/version/API differences not exhaustively covered, and limited exploration above τ>1.0 for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Set sampling temperature to 0.0 to maximize determinism; fix other sampling hyperparameters (top-k, top-p, repetition penalty) to isolate effects; standardize prompts (one-shot example, consistent formatting); prefer Chain-of-Thought (CoT) prompt for improved performance consistency; run multiple attempts and average results (10 attempts per question) to reduce sampling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 attempts per question (small exam: 100 questions; large exam used for detailed GPT-3.5 analysis: 1,000 questions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Temperature controls stochasticity: increasing τ raises textual variability but does not significantly change MCQA accuracy for τ in [0.0,1.0]; above ≈1.0 variability increases sharply and accuracy/coherence collapse. Recommendation: use τ=0.0 to maximize reproducibility for problem-solving MCQA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e653.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e653.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary model used for detailed temperature sweeps (both small 100-question and large 1,000-question exams) and prompt comparisons; used Chain-of-Thought prompting for many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / NLP — MCQA problem solving across multiple domains (math, science, law, medicine, English, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Answer MCQA problems to measure the impact of sampling temperature and prompt engineering on correctness and output variability.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling temperature (τ) varied 0.0–1.6 in initial sweeps; prompt-engineering variants (Baseline, Domain Expert, Self-recitation, CoT, Composite); randomness across repeated attempts; problem domain/difficulty; other sampling params held fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Mean accuracy across 10 attempts per question; Kruskal-Wallis H and p-values (e.g., 1,000-question CoT sweep: H(10)=10.439, p=0.403); text-similarity metrics (TF-IDF, BLEU, SBERT, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Accuracy stable across τ ∈ [0.0,1.0]; example: GPT-3.5 CoT on 1,000-question exam H(10)=10.439 p=0.403. On smaller sweeps and prompt comparisons, Kruskal-Wallis p-values were non-significant (e.g., CoT prompt H=2.042 p=0.996 in Table 4 context). Text similarity declined with increasing τ and accuracy/coherence dropped beyond τ ≈ 1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Mean accuracy across 10 attempts (used as primary reproducibility signal); Kruskal-Wallis test to assess significance across temperatures; Dunn-Bonferroni for pairwise problem-level differences; similarity metrics to assess output consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Reproducible average accuracy over τ ∈ [0.0,1.0] with no statistically significant differences in aggregate; however, pairwise per-question results sometimes varied (post-hoc tests), indicating per-instance sensitivity despite stable means.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Averaging can mask per-question variability; limited temperature range exploration for other models; cost/runtime constraints limited sample sizes; possible subtypes of problems sensitive to τ that require further error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Recommendation to set τ=0.0 for determinism; use CoT prompting; fix other sampling hyperparameters; run multiple attempts (10) and average to stabilize measurements; provide one-shot example to standardize behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 attempts per question; experiments included 100-question small exam (used broadly across models/prompts) and a 1,000-question large exam for detailed GPT-3.5 CoT analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5's MCQA accuracy did not change significantly with temperature between 0.0 and 1.0 (statistical tests non-significant), while textual variability increased with τ and performance collapsed above ≈1.0; thus τ=0.0 is recommended to enhance reproducibility without harming accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e653.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e653.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 7B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller open-source chat model included in the panel; exhibited formatting and performance anomalies causing elevated variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / NLP — MCQA</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>MCQA across temperatures to examine effect on accuracy and output consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling temperature; model capacity and prompt/format sensitivity leading to incorrectly formatted outputs; randomness across attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy across 10 attempts per question; Kruskal-Wallis H and p-value reported (Table 3: H=17.086, p=0.072); counts of error types (39% incorrectly formatted answers, 36% correctly formatted but incorrect answers).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Llama 2 7B performed near random baseline and showed high variability driven by formatting errors and an 'all-or-nothing' deterministic behavior at τ=0.0 versus more random outputs at τ∈[0.1,1.0]. Table 3 reports H(10)=17.086, p=0.072 (anomalously lower p-value than other models but still above the 0.05 threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Mean accuracy over 10 attempts; Kruskal-Wallis H/p; error-type breakdown (formatting vs correctness); pairwise Dunn-Bonferroni to explain anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Per-question accuracies varied significantly pairwise across temperatures (post-hoc), but average accuracy across all problems remained similar across temperatures; reproducibility hampered by frequent formatting errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High sensitivity to prompt/formatting, limited model capacity causing inconsistent outputs, and non-deterministic sampling leading to formatting and correctness variability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>No model-specific mitigations experimentally tested beyond the paper's general recommendations (τ=0.0, standardize prompts); authors note exceptions should be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 attempts per question on the 100-question small exam.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama 2 7B exhibited much higher output variability and formatting-related failure modes than larger models, producing many incorrectly formatted answers (39%) and incorrectly formatted-but-valid-structure answers (36%), with Kruskal-Wallis H=17.086, p=0.072 indicating anomalous temperature sensitivity driven by deterministic behavior at τ=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e653.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e653.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-similarity-metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-similarity metrics (Jaccard, BoW, TF-IDF, Levenshtein, BLEU, SBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of lexical and semantic similarity measures applied to outputs to quantify variability and reproducibility of generated text across temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to outputs from multiple LLMs (GPT-3.5, GPT-4, Claude 3, Llama 2, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / NLP — evaluation of output variability and reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure similarity/variability of LLM outputs across repeated runs and temperature settings to infer stochasticity and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling temperature (τ) as primary manipulated variable; prompt variation and model differences as additional influences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Jaccard similarity, Bag-of-Words similarity, TF-IDF similarity, Levenshtein similarity, BLEU score, SBERT semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>All metrics show decreasing similarity (increasing variability) as temperature increases; similarity declines rapidly after τ ≈ 1.0, correlated with accuracy/coherence collapse. Exact numeric similarity-series are presented as figures in the paper but not tabulated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same similarity metrics used to evaluate reproducibility (higher similarity → higher reproducibility of text outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Higher τ leads to lower reproducibility (lower similarity scores). Authors use these trends to support recommendation to use lower τ for reproducibility; no single aggregate reproducibility percentage was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Similarity metrics are sensitive to surface-form changes and formatting; semantic metrics (SBERT) mitigate lexical sensitivity but aggregation over many problems can obscure per-item variability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Lower temperature (τ=0.0) to increase determinism; standardize prompt and output formatting; fix other sampling hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 attempts per question (used to compute pairwise similarity and averages).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-similarity metrics provide clear, consistent evidence that stochasticity increases with temperature; similarity falls sharply beyond τ≈1.0, coinciding with loss of accuracy/coherence, supporting lower-temperature settings for reproducible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Contextual temperature for language modeling <em>(Rating: 2)</em></li>
                <li>Improving code generation by dynamic temperature sampling <em>(Rating: 2)</em></li>
                <li>Cost-effective hyperparameter optimization for large language model generation inference <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>A systematic evaluation of large language models of code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-653",
    "paper_id": "paper-c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "temperature",
            "name_full": "Sampling Temperature (τ)",
            "brief_description": "A decoding hyperparameter that rescales logits in the softmax to control output randomness; varied across experiments (0.0–1.6) to study its effect on accuracy and output variability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (e.g., GPT-3.5, GPT-4, Claude 3, Llama 2 ...)",
            "model_size": null,
            "scientific_domain": "AI / NLP — LLM problem solving (MCQA across multiple domains)",
            "experimental_task": "Evaluate effect of sampling temperature on multiple-choice question answering (MCQA) accuracy and text-output variability.",
            "variability_sources": "Sampling temperature (τ) itself; sampling method inherent stochasticity; prompt engineering differences; model-to-model differences and API implementations; randomness across repeated attempts; formatting variability of outputs; other sampling hyperparameters (top-k, top-p, repetition penalty) noted as potential sources but were held fixed in this study.",
            "variability_measured": true,
            "variability_metrics": "Correct-answer accuracy (mean across repeated attempts); text-similarity metrics (Jaccard, Bag-of-Words, TF-IDF, Levenshtein, BLEU, SBERT semantic similarity); statistical tests (Kruskal-Wallis H and p-values; Dunn-Bonferroni post-hoc for pairwise comparisons).",
            "variability_results": "Accuracy remained statistically stable across τ ∈ [0.0, 1.0] (example: GPT-3.5 CoT on 1,000-question exam: H(10)=10.439, p=0.403). Text similarity metrics decreased (i.e., variability increased) monotonically with increasing τ and dropped rapidly after τ ≈ 1.0; accuracy and coherence collapse observed above ≈1.0, reaching incoherence around 1.6. No single numerical similarity-score table was reported in-text (plots provided).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Mean correct-answer accuracy over 10 attempts per question; Kruskal-Wallis test across temperature settings to test significance; Dunn-Bonferroni for pairwise differences; text-similarity scores to measure consistency between runs.",
            "reproducibility_results": "Across models, prompts, and domains the paper found no statistically significant change in MCQA accuracy across τ ∈ [0.0,1.0] (e.g., GPT-3.5 H=10.439 p=0.403); however, response text variability increases with τ and reproducibility of exact textual outputs decreases. Specific numeric reproducibility percentages were not reported beyond statistical test outputs and similarity-curve plots.",
            "reproducibility_challenges": "Formatting sensitivity (models producing incorrectly formatted answers); all-or-nothing behavior at τ=0.0 vs more random outputs at τ&gt;0.0 causing per-question variation; limited sample sizes (100 and 1,000 question exams) and cost/runtime constraints; interactions with other sampling parameters not explored; model/version/API differences not exhaustively covered, and limited exploration above τ&gt;1.0 for most models.",
            "mitigation_methods": "Set sampling temperature to 0.0 to maximize determinism; fix other sampling hyperparameters (top-k, top-p, repetition penalty) to isolate effects; standardize prompts (one-shot example, consistent formatting); prefer Chain-of-Thought (CoT) prompt for improved performance consistency; run multiple attempts and average results (10 attempts per question) to reduce sampling noise.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "10 attempts per question (small exam: 100 questions; large exam used for detailed GPT-3.5 analysis: 1,000 questions).",
            "key_findings": "Temperature controls stochasticity: increasing τ raises textual variability but does not significantly change MCQA accuracy for τ in [0.0,1.0]; above ≈1.0 variability increases sharply and accuracy/coherence collapse. Recommendation: use τ=0.0 to maximize reproducibility for problem-solving MCQA tasks.",
            "uuid": "e653.0",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 Turbo",
            "brief_description": "Primary model used for detailed temperature sweeps (both small 100-question and large 1,000-question exams) and prompt comparisons; used Chain-of-Thought prompting for many experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "scientific_domain": "AI / NLP — MCQA problem solving across multiple domains (math, science, law, medicine, English, etc.)",
            "experimental_task": "Answer MCQA problems to measure the impact of sampling temperature and prompt engineering on correctness and output variability.",
            "variability_sources": "Sampling temperature (τ) varied 0.0–1.6 in initial sweeps; prompt-engineering variants (Baseline, Domain Expert, Self-recitation, CoT, Composite); randomness across repeated attempts; problem domain/difficulty; other sampling params held fixed.",
            "variability_measured": true,
            "variability_metrics": "Mean accuracy across 10 attempts per question; Kruskal-Wallis H and p-values (e.g., 1,000-question CoT sweep: H(10)=10.439, p=0.403); text-similarity metrics (TF-IDF, BLEU, SBERT, etc.).",
            "variability_results": "Accuracy stable across τ ∈ [0.0,1.0]; example: GPT-3.5 CoT on 1,000-question exam H(10)=10.439 p=0.403. On smaller sweeps and prompt comparisons, Kruskal-Wallis p-values were non-significant (e.g., CoT prompt H=2.042 p=0.996 in Table 4 context). Text similarity declined with increasing τ and accuracy/coherence dropped beyond τ ≈ 1.0.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Mean accuracy across 10 attempts (used as primary reproducibility signal); Kruskal-Wallis test to assess significance across temperatures; Dunn-Bonferroni for pairwise problem-level differences; similarity metrics to assess output consistency.",
            "reproducibility_results": "Reproducible average accuracy over τ ∈ [0.0,1.0] with no statistically significant differences in aggregate; however, pairwise per-question results sometimes varied (post-hoc tests), indicating per-instance sensitivity despite stable means.",
            "reproducibility_challenges": "Averaging can mask per-question variability; limited temperature range exploration for other models; cost/runtime constraints limited sample sizes; possible subtypes of problems sensitive to τ that require further error analysis.",
            "mitigation_methods": "Recommendation to set τ=0.0 for determinism; use CoT prompting; fix other sampling hyperparameters; run multiple attempts (10) and average to stabilize measurements; provide one-shot example to standardize behavior.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "10 attempts per question; experiments included 100-question small exam (used broadly across models/prompts) and a 1,000-question large exam for detailed GPT-3.5 CoT analysis.",
            "key_findings": "GPT-3.5's MCQA accuracy did not change significantly with temperature between 0.0 and 1.0 (statistical tests non-significant), while textual variability increased with τ and performance collapsed above ≈1.0; thus τ=0.0 is recommended to enhance reproducibility without harming accuracy.",
            "uuid": "e653.1",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 7B Chat",
            "brief_description": "A smaller open-source chat model included in the panel; exhibited formatting and performance anomalies causing elevated variability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B Chat",
            "model_size": "7B",
            "scientific_domain": "AI / NLP — MCQA",
            "experimental_task": "MCQA across temperatures to examine effect on accuracy and output consistency.",
            "variability_sources": "Sampling temperature; model capacity and prompt/format sensitivity leading to incorrectly formatted outputs; randomness across attempts.",
            "variability_measured": true,
            "variability_metrics": "Accuracy across 10 attempts per question; Kruskal-Wallis H and p-value reported (Table 3: H=17.086, p=0.072); counts of error types (39% incorrectly formatted answers, 36% correctly formatted but incorrect answers).",
            "variability_results": "Llama 2 7B performed near random baseline and showed high variability driven by formatting errors and an 'all-or-nothing' deterministic behavior at τ=0.0 versus more random outputs at τ∈[0.1,1.0]. Table 3 reports H(10)=17.086, p=0.072 (anomalously lower p-value than other models but still above the 0.05 threshold).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Mean accuracy over 10 attempts; Kruskal-Wallis H/p; error-type breakdown (formatting vs correctness); pairwise Dunn-Bonferroni to explain anomalies.",
            "reproducibility_results": "Per-question accuracies varied significantly pairwise across temperatures (post-hoc), but average accuracy across all problems remained similar across temperatures; reproducibility hampered by frequent formatting errors.",
            "reproducibility_challenges": "High sensitivity to prompt/formatting, limited model capacity causing inconsistent outputs, and non-deterministic sampling leading to formatting and correctness variability.",
            "mitigation_methods": "No model-specific mitigations experimentally tested beyond the paper's general recommendations (τ=0.0, standardize prompts); authors note exceptions should be considered.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "10 attempts per question on the 100-question small exam.",
            "key_findings": "Llama 2 7B exhibited much higher output variability and formatting-related failure modes than larger models, producing many incorrectly formatted answers (39%) and incorrectly formatted-but-valid-structure answers (36%), with Kruskal-Wallis H=17.086, p=0.072 indicating anomalous temperature sensitivity driven by deterministic behavior at τ=0.0.",
            "uuid": "e653.2",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "text-similarity-metrics",
            "name_full": "Text-similarity metrics (Jaccard, BoW, TF-IDF, Levenshtein, BLEU, SBERT)",
            "brief_description": "A suite of lexical and semantic similarity measures applied to outputs to quantify variability and reproducibility of generated text across temperature settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "applied to outputs from multiple LLMs (GPT-3.5, GPT-4, Claude 3, Llama 2, etc.)",
            "model_size": null,
            "scientific_domain": "AI / NLP — evaluation of output variability and reproducibility",
            "experimental_task": "Measure similarity/variability of LLM outputs across repeated runs and temperature settings to infer stochasticity and reproducibility.",
            "variability_sources": "Sampling temperature (τ) as primary manipulated variable; prompt variation and model differences as additional influences.",
            "variability_measured": true,
            "variability_metrics": "Jaccard similarity, Bag-of-Words similarity, TF-IDF similarity, Levenshtein similarity, BLEU score, SBERT semantic similarity.",
            "variability_results": "All metrics show decreasing similarity (increasing variability) as temperature increases; similarity declines rapidly after τ ≈ 1.0, correlated with accuracy/coherence collapse. Exact numeric similarity-series are presented as figures in the paper but not tabulated in the text.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same similarity metrics used to evaluate reproducibility (higher similarity → higher reproducibility of text outputs).",
            "reproducibility_results": "Higher τ leads to lower reproducibility (lower similarity scores). Authors use these trends to support recommendation to use lower τ for reproducibility; no single aggregate reproducibility percentage was reported.",
            "reproducibility_challenges": "Similarity metrics are sensitive to surface-form changes and formatting; semantic metrics (SBERT) mitigate lexical sensitivity but aggregation over many problems can obscure per-item variability.",
            "mitigation_methods": "Lower temperature (τ=0.0) to increase determinism; standardize prompt and output formatting; fix other sampling hyperparameters.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "10 attempts per question (used to compute pairwise similarity and averages).",
            "key_findings": "Text-similarity metrics provide clear, consistent evidence that stochasticity increases with temperature; similarity falls sharply beyond τ≈1.0, coinciding with loss of accuracy/coherence, supporting lower-temperature settings for reproducible outputs.",
            "uuid": "e653.3",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Contextual temperature for language modeling",
            "rating": 2
        },
        {
            "paper_title": "Improving code generation by dynamic temperature sampling",
            "rating": 2
        },
        {
            "paper_title": "Cost-effective hyperparameter optimization for large language model generation inference",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "A systematic evaluation of large language models of code",
            "rating": 1
        }
    ],
    "cost": 0.01771675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Effect of Sampling Temperature on Problem Solving in Large Language Models</h1>
<p>Matthew Renze<br>Johns Hopkins University mrenze1@jhu.edu</p>
<p>Erhan Guven<br>Johns Hopkins University<br>eguven2@jhu.edu</p>
<h4>Abstract</h4>
<p>In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.</p>
<h2>1 Introduction</h2>
<h3>1.1 Background</h3>
<p>In recent years, Large Language Models (LLMs) have revolutionized the field of artificial intelligence. The availability of open-source LLMs and pay-per-use APIs has allowed engineers to incorporate LLMs in their AI systems. However, prompt engineering and hyperparameter tuning are required to work effectively with LLMs.
Prompt-engineering techniques help LLMs solve complex problems, avoid hallucinations, and provide more accurate responses. For example, we can use techniques like chain-of-thought, tree-of-thought, self-criticism, and self-consistency to improve LLM performance [1,2].
In addition, several inference hyperparameters can be adjusted to modify the LLM's output at runtime. For example, hyperparameters like sampling temperature, top-k sampling, repetition penalty, and maximum token length all affect the LLM's output and performance [3-5].
Despite significant interest in LLMs and progress in LLM best practices, many open questions remain about optimal prompt-engineering techniques and inference hyperparameters for LLMs. To complicate matters, various local optima may exist for LLMs, prompt types, and problem domains [5].
The prompt-engineering community has an abundance of opinions and anecdotal evidence regarding optimal promptengineering techniques and inference hyperparameter settings. However, we currently lack systematic studies and empirical evidence to support many of these claims.
As a result, this paper aims to address the open question of the optimal LLM sampling temperature for problem-solving tasks. In addition, we aim to provide a systematic study with empirical results to add to the growing body of knowledge used to create LLM and prompt-engineering best practices.</p>
<h3>1.2 Sampling Temperature</h3>
<p>Sampling temperature is a hyperparameter of an LLM used in a temperature-based sampling process. It controls the randomness of the model's output at inference time [5-8].</p>
<p>During each step of an LLM’s decoding process, the LLM uses the previous tokens to choose the next output token. The final layer of the LLM uses a softmax function to convert raw scores (logits) into probabilities.</p>
<p>In greedy sampling, the model will always choose the most likely next token. However, for probabilistic sampling, the next token is selected from a probability distribution.</p>
<p>Temperature sampling is a modification to the softmax function, which adjusts the resulting probability mass functions. In this modified softmax function, $v_{k}$ is the $k$-th vocabulary token, $l_{k}$ is the token’s logit, and $\tau$ is a constant temperature. See equation 1.</p>
<p>$\operatorname{Pr}\left(v_{k}\right)=\frac{e^{l_{k} / \tau}}{\sum_{i} e^{l_{i} / \tau}}$</p>
<p>A lower temperature makes the output of the LLM more deterministic, thus favoring the most likely predictions. This conservativeness is captured by the model’s tendency to produce more repetitive, focused, and less diverse output based on the patterns most commonly seen in the training data [5, 7, 8].</p>
<p>A higher temperature increases the randomness of the output, thus favoring more “creative” predictions. This creativity is captured by the model’s willingness to explore more unconventional and less likely outputs. Higher temperatures can lead to novel text, diverse ideas, and creative solutions to problems [5, 7, 8].</p>
<p>In the context of problem-solving, temperature can be seen as a trade-off between exploring and exploiting possible solutions within the solution space. Lower temperatures tend to exploit more probable solutions; higher temperatures explore the solution space more broadly.</p>
<h3>1.3 Choosing a Sampling Temperature</h3>
<p>Within the prompt-engineering community, there are a variety of opinions and best practices regarding the ideal sampling temperature for various problem-solving tasks [9, 10].</p>
<p>Low sampling temperatures are recommended for tasks requiring precision and factual accuracy, such as technical writing, code generation, or question-answering [11, 12]. However, higher temperatures are recommended for tasks requiring creativity, such as writing poetry, creating stories, or brainstorming.</p>
<p>Higher temperatures also increase the probability of model hallucination. Hallucination is a phenomenon where an LLM produces statistically probable responses that are factually incorrect or nonsensical. As a result, optimal temperature selection is also a balance between creativity and hallucination [13].</p>
<p>Practical guidelines for choosing a sampling temperature for a specific task or problem domain are often vague or anecdotal. Prompt-engineering guides often provide hypothetical examples of optimal sampling temperatures for various tasks. However, they rarely cite any sources or provide empirical evidence.</p>
<p>As a result, the current state of choosing the optimal sampling temperature for specific problems is largely based on guesswork, gut instinct, non-systematic experimentation, and iterative refinement.^{2,3}</p>
<h2>2 Methods</h2>
<h3>2.1 Models</h3>
<p>The models used in this research project comprise nine widely-used foundational LLMs. To complement our analysis, we also conducted experiments using five prompts created using commonly used prompt-engineering techniques.</p>
<p>First, we reviewed the prior literature to identify candidate LLMs commonly used for problem-solving tasks. We limited our candidate models to those that allowed the model’s sampling temperature to be specified via their API [4, 16–18]. See Table 1 for a list of LLMs used in the experiment.</p>
<p>^{1}A few empirical studies exist that indicate sampling temperature does have an effect on LLM performance on some types of problem-solving tasks (e.g., code generation, engineering exams, etc.) [11, 12, 14].</p>
<p>^{2}For example, OpenAI’s GPT-3.5 API allowed users to set the sampling temperature from 0.0 to 1.0 with a default of 0.7. GPT-4’s API expanded this range from 0.0 to 2.0 with a default of 1.0. No explanation from OpenAI has been provided for these default values or their change from GPT-3.5 to GPT-4 [15].</p>
<p>^{3}Even the GPT-4 Technical Report explains that the authors used their “best-guess” when choosing sampling temperatures while evaluating GPT-4 on various benchmarks. See Appendix A in the GPT-4 Technical Report [16].</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Vendor</th>
<th>Released</th>
<th>License</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 3 Opus</td>
<td>Anthropic</td>
<td>2024-03-04</td>
<td>Closed</td>
<td>$[19,20]$</td>
</tr>
<tr>
<td>Command R+</td>
<td>Cohere</td>
<td>2024-04-04</td>
<td>Open</td>
<td>$[21,22]$</td>
</tr>
<tr>
<td>Gemini 1.0 Pro</td>
<td>Google</td>
<td>2023-12-06</td>
<td>Closed</td>
<td>$[23,24]$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (Preview)</td>
<td>Google</td>
<td>2024-02-15</td>
<td>Closed</td>
<td>$[25,26]$</td>
</tr>
<tr>
<td>GPT-3.5 Turbo</td>
<td>OpenAI</td>
<td>2022-11-30</td>
<td>Closed</td>
<td>$[17,27]$</td>
</tr>
<tr>
<td>GPT-4</td>
<td>OpenAI</td>
<td>2023-03-14</td>
<td>Closed</td>
<td>$[16,18]$</td>
</tr>
<tr>
<td>Llama 2 7B Chat</td>
<td>Meta</td>
<td>2023-07-18</td>
<td>Open</td>
<td>$[4,28]$</td>
</tr>
<tr>
<td>Llama 2 70B Chat</td>
<td>Meta</td>
<td>2023-07-18</td>
<td>Open</td>
<td>$[4,28]$</td>
</tr>
<tr>
<td>Mistral Large</td>
<td>Mistral AI</td>
<td>2024-02-26</td>
<td>Closed</td>
<td>$[29]$</td>
</tr>
</tbody>
</table>
<p>Table 1: LLMs used in the experiment.</p>
<p>Next, we reviewed the existing literature for commonly used prompt-engineering techniques. We limited our candidate prompts to those that could be performed in a single request-and-response cycle with one-shot in-context learning. We excluded multi-step agents, few-shot learning, and model fine-tuning.
As a result, we selected five prompt-engineering techniques to construct our system prompts:</p>
<ul>
<li>Baseline - no prompt engineering; the LLM is instructed to return only a single multiple-choice answer as its output (e.g., 'Answer("C")' ).</li>
<li>Domain Expertise - the system prompt specifies that the LLM is an expert in the problem domain of the exam (e.g., "medicine") or the topic of the problem (e.g., "anatomy") [2].</li>
<li>Self-recitation - the system prompt instructs the LLM to recite its own internal knowledge about the problem before answering the question $[2,30]$.</li>
<li>Chain-of-Thought (CoT) - the system prompt instructs the LLM to "think step-by-step" to encourage it to reason through the problem procedurally $[31,32]$.</li>
<li>Composite - the system prompt combines domain expertise, self-recitation, chain-of-thought, and adds self-criticism $[33,34]$.</li>
</ul>
<p>Finally, we provided the LLM with a single example problem-and-solution pair for one-shot in-context learning. The example solution was adapted for each prompt based on the prompt-engineering technique used. For example, the CoT prompt included a chain of thought in its solution. See Figure 10 in the Appendix for a sample prompt.</p>
<h1>2.2 Data</h1>
<p>The test dataset used in this research study consists of a series of Multiple-Choice Question-and-Answer (MCQA) exams derived from widely used LLM performance benchmarks.
First, we reviewed the prior literature to identify benchmarks frequently used to evaluate LLMs. We limited our candidate benchmarks to those containing MCQA problems so that we could use correct-answer accuracy as our primary performance metric.
Next, we selected a set of problems that covered a range of problem domains (e.g., math, science, law, etc.) and difficulty levels (e.g., secondary school, university, etc.) These problem sets can be seen in Table 2.
Then, we converted the benchmark problems from their original data format into a standardized data structure using the JSON Lines (JSON-L) format [41]. Our standardized set of exams allowed us to use the exams interchangeably without modifying the code in the test harness. See Figure 11 in the Appendix for a sample of an MCQA problem.
Finally, we created two MCQA exams of different sizes. We created a large exam with 1,000 questions by randomly sampling 100 problems from each of the ten problem sets. This 1,000 -question (large) exam was used with GPT-3.5 to perform a detailed analysis of temperature across problem domains.
Additionally, we created a smaller exam of 100 questions by randomly sampling ten questions from each of the ten domain-specific problem sets. This 100 -question (small) exam was used for our high-level analysis of sampling temperature across all nine models, all five prompt-engineering techniques, and extended temperature range ( $0.0-1.6) .^{5}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Problem Set</th>
<th>Benchmark</th>
<th>Domain</th>
<th>Questions</th>
<th>License</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARC Challenge Test</td>
<td>ARC</td>
<td>Science</td>
<td>1,173</td>
<td>CC BY-SA</td>
<td>[35]</td>
</tr>
<tr>
<td>AQUA-RAT</td>
<td>AGI Eval</td>
<td>Math</td>
<td>254</td>
<td>Apache v2.0</td>
<td>[36]</td>
</tr>
<tr>
<td>Hellaswag Val</td>
<td>Hellaswag</td>
<td>Common Sense Reasoning</td>
<td>10,042</td>
<td>MIT</td>
<td>[37]</td>
</tr>
<tr>
<td>LogiQA (English)</td>
<td>AGI Eval</td>
<td>Logic</td>
<td>651</td>
<td>GitHub</td>
<td>$[36,38]$</td>
</tr>
<tr>
<td>LSAT-AR</td>
<td>AGI Eval</td>
<td>Law (Analytic Reasoning)</td>
<td>230</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>LSAT-LR</td>
<td>AGI Eval</td>
<td>Law (Logical Reasoning)</td>
<td>510</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>LSAT-RC</td>
<td>AGI Eval</td>
<td>Law (Reading Comprehension)</td>
<td>260</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>MedMCQA Valid</td>
<td>MedMCQA</td>
<td>Medicine</td>
<td>6,150</td>
<td>MIT</td>
<td>[40]</td>
</tr>
<tr>
<td>SAT-English</td>
<td>AGI Eval</td>
<td>English</td>
<td>206</td>
<td>MIT</td>
<td>[36]</td>
</tr>
<tr>
<td>SAT-Math</td>
<td>AGI Eval</td>
<td>Math</td>
<td>220</td>
<td>MIT</td>
<td>[36]</td>
</tr>
</tbody>
</table>
<p>Table 2: Problem sets used to create the multi-domain MCQA exam.
Note: The GitHub repository for LogiQA does not include a license file. However, both the paper and readme.md file states that "The dataset is freely available."</p>
<h1>2.3 Process</h1>
<p>Our experiment was designed to test the problem-solving performance of LLMs across ten models, five promptengineering techniques, ten problem domains, 100 problems within each problem domain, and all viable sampling temperatures. For each combination of model, prompt, exam, and temperature, we instructed the LLM to answer each question ten times so we could assess the average correct-answer accuracy.
The full experiment setup can be seen in Figure 1 and Algorithm 1. However, due to cost and runtime considerations, we conducted a subset of the full experiment designed to capture the most valuable information as efficiently as possible.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diagram of the full experiment.</p>
<p>Algorithm 1 Full LLM Temperature Experiment</p>
<table>
<thead>
<tr>
<th style="text-align: left;">1: for each model $m$ in $M$ do</th>
<th style="text-align: left;">$\triangleright 10$ models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2: for each prompt $p$ in $P$ do</td>
<td style="text-align: left;">$\triangleright 5$ prompts</td>
</tr>
<tr>
<td style="text-align: left;">3: for each exam $e$ in $E$ do</td>
<td style="text-align: left;">$\triangleright 10$ exams</td>
</tr>
<tr>
<td style="text-align: left;">4: for each temperature $\tau$ in $T$ do</td>
<td style="text-align: left;">$\triangleright 16$ temps</td>
</tr>
<tr>
<td style="text-align: left;">5: for each problem $q$ in $Q$ do</td>
<td style="text-align: left;">$\triangleright 100$ probs</td>
</tr>
<tr>
<td style="text-align: left;">6: for each attempt $a$ in $A$ do</td>
<td style="text-align: left;">$\triangleright 10$ attempts</td>
</tr>
<tr>
<td style="text-align: left;">7: Create the prompt</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">8: Answer the question</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">9: Record the answer</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">10: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">11: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">12: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">13: Save the results</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">14: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">15: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">16: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">17: Process the results</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">18: Analyze the results</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>First, we instructed GPT-3.5 to complete the 100-question (small) exam using the CoT prompt with temperatures ranging from 0.0 to 2.0 in increments of 0.1 . This allowed us to determine the range of viable sampling temperatures to explore. ${ }^{5}$
Performance began to drop rapidly after a temperature of 1.0 until the generated text became incoherent at 1.6. As a result, we stopped the initial temperature sweep at 1.6 and limited the rest of our sweeps from 0.0 to 1.0 .
Next, we instructed the other eight LLMs to complete the 100-question (small) exam using the CoT prompt with temperatures from 0.0 to 1.0. This allowed us to determine if the results generalize to other LLMs.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Then, we instructed GPT-3.5 to complete the 100-question (small) exam using each of the five prompts over temperatures from 0.0 to 1.0. This allowed us to determine if the results generalize over various prompt-engineering techniques.
Finally, we instructed GPT-3.5 to complete the 1,000-question (large) exam using the CoT prompt with temperatures from 0.0 to 1.0. This allowed us to determine if the results were domain-specific or generalized across problem domains.</p>
<h1>2.4 Metrics</h1>
<p>To test our hypothesis, we measured the LLM's correct-answer accuracy as our primary performance metric. For each combination of model, prompt, exam, and temperature, we calculated the accuracy as the number of correct answers from ten attempts at each problem. Then, we computed the average (mean) accuracy across all problems.
To further support our findings, we also measured the similarity of the LLM's responses using a series of text-similarity metrics. These metrics are defined as follows:</p>
<ul>
<li>Jaccard similarity - the ratio of the intersection to the union of word sets in the output text [42].</li>
<li>Bag-of-Words (BoW) similarity - comparison of the frequency of each word, ignoring order [43].</li>
<li>TF-IDF similarity - comparison of word frequency weighted by inverse document frequency [44].</li>
<li>Levenshtein similarity - the number of edits needed to change one string of text into the other [45].</li>
<li>BLEU score - comparison of similarity based on n-gram overlap [46].</li>
<li>SBERT similarity - semantic similarity computed using Sentence-BERT embeddings [47].</li>
</ul>
<h3>2.5 Analysis</h3>
<p>We used the Kruskal-Wallis test to evaluate the statistical significance of any changes in accuracy as a function of temperature [48]. We chose the Kruskal-Wallis test because the data (i.e., correct-answer accuracy by question) were not normally distributed. Rather, they were bimodally distributed with centers at 0.0 and 1.0.</p>
<h2>3 Results</h2>
<h3>3.1 Accuracy vs. Temperature</h3>
<p>Our analysis revealed that the problem-solving performance of LLMs remained relatively stable across sampling temperatures from 0.0 to 1.0 for all LLMs, prompt-engineering techniques, and problem domains. Using GPT-3.5 with a CoT prompt on the 1,000-question exam from 0.0 to 1.0, the Kruskal-Wallis test yielded $H(10)=10.439, p=0.403$.
First, we analyzed the performance of GPT-3.5 using the CoT prompt on the 100-question exam. Accuracy remained stable over temperatures from 0.0 to 1.0. However, after a temperature of 1.0, the text rapidly became incoherent, and the accuracy began to drop until it reached zero around a temperature of 1.6. See Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy by temperature from 0.0 to 1.6 for GPT-3.5 using the CoT prompt on the 100-question exam.</p>
<p>Second, we analyzed the performance of all nine LLMs using the CoT prompt on the 100-question exam. Accuracy also remained stable across all of the LLMs, except for Llama 2 7B. The performance of most LLMs showed a gradual (non-significant) decrease in performance as a function of temperature. See Figure 3 and Table 3.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">$\mathbf{H ( 1 0 )}$</th>
<th style="text-align: center;">$\mathbf{p}$-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claude 3 Opus</td>
<td style="text-align: right;">1.735</td>
<td style="text-align: center;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">Command R+</td>
<td style="text-align: right;">1.771</td>
<td style="text-align: center;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro 1.0</td>
<td style="text-align: right;">7.379</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro 1.5</td>
<td style="text-align: right;">3.119</td>
<td style="text-align: center;">0.978</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 Turbo</td>
<td style="text-align: right;">2.042</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: right;">3.789</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B</td>
<td style="text-align: right;">3.677</td>
<td style="text-align: center;">0.961</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 7B</td>
<td style="text-align: right;">17.086</td>
<td style="text-align: center;">0.072</td>
</tr>
<tr>
<td style="text-align: left;">Mistral Large</td>
<td style="text-align: right;">3.069</td>
<td style="text-align: center;">0.980</td>
</tr>
</tbody>
</table>
<p>Table 3: Kruskal-Wallis test results by model using the CoT prompt on the 100-question exam.</p>
<p>Figure 3: Accuracy by temperature and model using the CoT prompt on the 100-question exam.</p>
<p>Llama 2 7B did not perform better than statistically random guesses. Its poor performance was due to generating incorrectly formatted answers (39\%) and correctly formatted but incorrect answers (36\%). Its all-or-nothing behavior at a temperature of 0.0 versus more random behavior from 0.1 to 1.0 led to a much lower, yet still non-significant, p-value.
Third, we analyzed the performance of GPT-3.5 using each of the five prompts on the 100-question exam. Accuracy remained stable for all temperatures across all prompt-engineering techniques. The CoT prompt outperformed the other four prompts. As a result, we used the CoT prompt for all single-prompt experiments. See Figure 4 and Table 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">$\mathbf{H ( 1 0 )}$</th>
<th style="text-align: center;">$\mathbf{p}$-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Domain Expert</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Self-recitation</td>
<td style="text-align: center;">1.403</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Chain of Thought</td>
<td style="text-align: center;">2.042</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">Composite</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
</tr>
</tbody>
</table>
<p>Table 4: Kruskal-Wallis test results by prompt for GPT-3.5 on the 100-question exam.</p>
<p>Figure 4: Accuracy by temperature and prompt for GPT-3.5 on the 100-question exam.</p>
<p>Finally, we analyzed the performance of GPT-3.5 using the CoT prompt on all ten exams. Accuracy remained stable for all temperatures across all problem domains based on visual analysis. However, the LSAT-AR and SAT-Math exams showed statistically significant differences in the Kruskal-Wallis p-values. ${ }^{6}$ See Figure 5 and Table 5.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy by temperature and exam for GPT-3.5 using the CoT prompt.</p>
<p>We performed the Dunn-Bonferroni test on the LSAT-AR and SAT-Math results [49]. It revealed that the all-or-nothing behavior of responses generated at a temperature of 0.0 versus the more random responses from 0.1 to 1.0 caused the anomaly. The correct-answer accuracy for each individual problem varied significantly when evaluated pairwise across temperatures. However, the average accuracy for all problems remained similar across temperatures.</p>
<h1>3.2 Text Variability vs. Temperature</h1>
<p>To further support our results, we analyzed text variability as a function of temperature. Our findings show a clear trend of decreasing text similarity (thus increasing text variability) as temperature increases. Text similarity decreases rapidly after a temperature of 1.0, corresponding to the rapid decrease in accuracy observed above $\tau=1.0$. See Figure 6.
These results are consistent with our understanding of sampling temperature, indicating that higher temperatures produce more widely varied outputs. Furthermore, these results hold regardless of the LLM, prompt-engineering technique, or problem domain. See Figures 7, 8, and 9.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Text similarity by temperature and metric for GPT3.5 using CoT prompting on the 100 -question exam over sampling temperatures from 0.0 to 1.6 .
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: TF-IDF text similarity by temperature and model using the CoT prompt on the 100 -question exam over sampling temperatures from 0.0 to 1.0</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: TF-IDF text similarity by temperature and prompt for GPT-3.5 on the 100-question exam over sampling temperatures from 0.0 to 1.0 .
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: TF-IDF text similarity by temperature and exam for GPT-3.5 using the CoT prompt over sampling temperatures from 0.0 to 1.0</p>
<h1>4 Discussion</h1>
<h3>4.1 Interpretation</h3>
<p>Based on these results, changes in temperature from 0.0 to 1.0 do not have a statistically significant effect on the problemsolving performance of LLMs. These results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. However, there are a few potential exceptions to these general findings.
Therefore, we recommend setting an LLM's sampling temperature to 0.0 for problem-solving tasks. This temperature maximizes reproducibility without compromising accuracy. In addition, it helps avoid the inevitable drop-off in performance that occurs beyond a temperature of 1.0. However, exceptions to this guidance should be taken into consideration.</p>
<h3>4.2 Limitations</h3>
<p>There were several limitations to our research study due to cost and runtime considerations:
First, our study was limited to a subset of popular LLMs. As a result, our findings may not hold for other LLMs that were excluded from our experiment.
Second, we only explored a subset of prompt-engineering techniques using a single prompt-and-response cycle with one-shot in-context learning. As a result, there may be more complex prompts or agent architectures that better leverage sampling temperature for creativity in their problem-solving capabilities.
Third, our study was limited to a subset of problems, problem domains, and problem-solving tasks. As a result, our findings may not hold for larger data sets, different problem domains, or other types of problem-solving tasks.
Fourth, due to time and cost constraints, we limited our study to two test sets of 1,000 and 100 randomly selected questions from standard benchmarks. These limited sample sizes may have introduced bias into the results. Utilizing a larger and more diverse test set would enhance the statistical reliability of our findings.
Fifth, we had to limit the sampling temperature range we explored from 0.0 to 1.0 for all combinations of models, prompts, and exams, except for GPT-3.5 using CoT prompting on the 100-question exam. As a result, the temperature hyperparameter of other LLMs may operate differently at temperatures above 1.0.
Sixth, we fixed all other sampling parameters (e.g., top-p, top-k, repetition penalty, etc.) to isolate the effect of sampling temperature. As a result, there may be combinations of sampling parameters that result in different outcomes.</p>
<p>Finally, we could only explore a subset of the various combinations of models, prompts, exams, and temperatures. As a result, other combinations of LLMs, prompt-engineering techniques, and problem domains may exist where temperature plays a more important role in problem-solving performance.</p>
<h1>4.3 Implications</h1>
<p>This research study provides empirical evidence that changes in sampling temperature in the range of 0.0 to 1.0 do not significantly impact the problem-solving capabilities of LLMs on MCQA problems.
Answering this question may save AI engineers significant time and resources evaluating various sampling temperatures for their LLM agents and applications. In addition, it may reduce unproductive debates in the prompt-engineering community regarding the optimal sampling temperatures for various problem-solving tasks.
This research also provides broader insights for AI researchers studying model hallucination and problem-solution state-space search with LLMs. Our results show that increasing LLM temperature up to 1.0 does not cause the LLM to hallucinate in ways that lead to incorrect MCQA solutions. In addition, higher temperatures do not appear to improve MCQA solution-space search in ways that lead to correct solutions more often than lower temperatures.</p>
<h3>4.4 Future Research</h3>
<p>To improve upon this research, we propose the following follow-up experiments:
First, we recommend conducting this experiment with additional LLMs. Other proprietary and open-source LLMs may utilize temperature in ways that benefit their specific models but did not benefit the LLMs we tested.
Second, we recommend expanding beyond MCQA problems to other types of problem-solving tasks whose correct answers are more open-ended. The limited effects of sampling temperature in our experiments may have simply resulted from the constraints imposed by the structure of MCQA problems.
Third, we recommend conducting additional experiments with more MCQA problems and problem domains. We recommend specifically targeting tasks and problem domains that require more creative solutions or lateral "out-of-thebox" thinking.
Fourth, we recommend extending the sampling temperature range until accuracy drops to zero for each LLM, prompt, and exam. However, it should be noted that as the generated text becomes more random, the number of tokens in each response increases significantly, leading to a considerable increase in runtime and cost to explore temperatures above 1.0.</p>
<p>Finally, we recommend a more in-depth error analysis to determine if any sub-types of problems within these problem domains benefit from changes to sampling temperature. It is possible that statistical noise or averaging may have hidden individual problems that were sensitive to changes in sampling temperature.</p>
<h2>5 Conclusion</h2>
<p>This research study empirically investigated the effect of sampling temperature on the problem-solving performance of LLMs across multiple problem domains.
We demonstrated that changes in sampling temperature from 0.0 to 1.0 do not produce statistically significant differences in problem-solving performance on MCQA problems across multiple LLMs, prompt-engineering techniques, and problem domains.
These results have practical implications for AI engineers using LLMs to develop new AI systems. Additionally, they have theoretical implications for AI researchers studying model hallucination and solution-space search with LLMs.</p>
<h2>References</h2>
<p>[1] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, E. Grave, Y. LeCun, and T. Scialom, "Augmented language models: a survey," arXiv, 2 2023.
[2] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with ChatGPT," arXiv, 22023.
[3] OpenAI, "OpenAI - API reference," 2023. [Online]. Available: https://platform.openai.com/docs/api-reference/ chat/create</p>
<p>[4] Llama-2-Team, "Llama 2: Open foundation and fine-tuned chat models," arXiv, 72023.
[5] C. Wang, S. X. Liu, and A. H. Awadallah, "Cost-effective hyperparameter optimization for large language model generation inference," 2023.
[6] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, "A learning algorithm for Boltzmann machines," Cognitive Science, vol. 9, pp. 147-169, 1985.
[7] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv, 32015.
[8] P.-H. Wang, S.-I. Hsieh, S.-C. Chang, Y.-T. Chen, J.-Y. Pan, W. Wei, and D.-C. Juan, "Contextual temperature for language modeling," arXiv, 122020.
[9] Microsoft, "Completions - learn how to generate or manipulate text," 2023. [Online]. Available: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/completions
[10] J. Shieh, "Best practices for prompt engineering with OpenAI API," 2024. [Online]. Available: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api
[11] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, "A systematic evaluation of large language models of code," in Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. Association for Computing Machinery, 2022, pp. 1-10.
[12] Y. Zhu, J. Li, G. Li, Y. Zhao, J. Li, Z. Jin, and H. Mei, "Improving code generation by dynamic temperature sampling," arXiv, 92023.
[13] M. Lee, "A mathematical investigation of hallucination and creativity in GPT models," Mathematics, vol. 11, p. 2320, 52023.
[14] V. Pursnani, Y. Sermet, and I. Demir, "Performance of ChatGPT on the US fundamentals of engineering exam: Comprehensive assessment of proficiency and potential implications for professional environmental engineering practice," arXiv, 42023.
[15] OpenAI, "Api temperature change from 0 to 1 to 0 to 2 in 'playground'," 2023. [Online]. Available: https://community.openai.com/t/api-temperature-change-from-0-to-1-to-0-to-2-in-playground/217755
[16] —<em>, "GPT-4 technical report," arXiv, 3 2023. [Online]. Available: https://arxiv.org/abs/2303.08774
[17] —</em>, "Introducing ChatGPT," 11 2022. [Online]. Available: https://openai.com/blog/chatgpt
[18] —_, "GPT-4," 3 2023. [Online]. Available: https://openai.com/research/gpt-4
[19] Anthropic, "Introducing the next generation of claude anthropic," 2024. [Online]. Available: https: //www.anthropic.com/news/claude-3-family
[20] , "The claude 3 model family: Opus, sonnet, haiku," 2024. [Online]. Available: https: //www.anthropic.com/claude-3-model-card
[21] Cohere, "Command r+," 2024. [Online]. Available: https://docs.cohere.com/docs/command-r-plus
[22] , "Model card for c4ai command r+," 2024. [Online]. Available: https://huggingface.co/CohereForAI/ c4ai-command-r-plus
[23] S. Pichai and D. Hassabis, "Introducing gemini: Google's most capable ai model yet," 2023. [Online]. Available: https://blog.google/technology/ai/google-gemini-ai/
[24] Gemini-Team, "Gemini: A family of highly capable multimodal models," arXiv, 122023.
[25] S. Pichai and D. Hassabis, "Introducing gemini 1.5, google's next-generation ai model," 2024. [Online]. Available: https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/
[26] Gemini-Team, "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context," 2024. [Online]. Available: https://arxiv.org/abs/2403.05530
[27] OpenAI, "Models - openai api." [Online]. Available: https://platform.openai.com/docs/models/gpt-3-5-turbo
[28] Meta, "Meta and microsoft introduce the next generation of llama I meta," 2023. [Online]. Available: https://about.meta.com/news/2023/07/llama-2/
[29] Mistral-AI-Team, "Au large I mistral ai I frontier ai in your hands," 2024. [Online]. Available: https://mistral.ai/news/mistral-large/
[30] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, "Recitation-augmented language models," in The Eleventh International Conference on Learning Representations, 102023.
[31] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," in Advances in Neural Information Processing Systems, vol. 35, 5 2022, pp. 22 199-22 213.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," arXiv, 12022.
[33] S. Huo, N. Arabzadeh, and C. L. A. Clarke, "Retrieving supporting evidence for generative question answering," arXiv, 92023.</p>
<p>[34] R. Wang, H. Wang, F. Mi, Y. Chen, R. Xu, and K.-F. Wong, "Self-critique prompting with large language models for inductive instructions," arXiv, 52023.
[35] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, "Think you have solved question answering? Try ARC, the AI2 reasoning challenge," ArXiv, 32018.
[36] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "AGIEval: A human-centric benchmark for evaluating foundation models," ArXiv, 42023.
[37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "HellaSwag: Can a machine really finish your sentence?" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
[38] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning," in International Joint Conference on Artificial Intelligence, 2020.
[39] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan, "From lsat: The progress and challenges of complex reasoning," IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 30, pp. 2201-2216, 82021.
[40] A. Pal, L. K. Umapathi, and M. Sankarasubbu, "MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering," in Proceedings of the Conference on Health, Inference, and Learning. PMLR, 2022, pp. 248-260.
[41] I. Ward, "JSON lines," 2014. [Online]. Available: https://jsonlines.org/
[42] P. Jaccard, "The distribution of flora in the alpine zone," New Phytologist, vol. 11, pp. 37-50, 21912.
[43] Z. S. Harris, "Distributional structure," WORD, vol. 10, pp. 146-162, 81954.
[44] K. S. Jones, "A statistical interpretation of term specificity and its application in retrieval," Journal of Documentation, vol. 28, pp. 11-21, 11972.
[45] V. Levenshtein, "Binary codes capable of correcting deletions, insertions and reversals," Soviet Physics Doklady, vol. 10, pp. 707-710, 1966.
[46] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "BLEU," in Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02. Association for Computational Linguistics, 2001, p. 311.
[47] N. Reimers and I. Gurevych, "Sentence-BERT: Sentence embeddings using Siamese BERT-networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 82019.
[48] W. H. Kruskal and W. A. Wallis, "Use of ranks in one-criterion variance analysis," Journal of the American Statistical Association, vol. 47, no. 260, pp. 583-621, 1952. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441
[49] O. J. Dunn, "Multiple comparisons using rank sums," Technometrics, vol. 6, no. 3, pp. 241-252, 1964. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/00401706.1964.10490181</p>
<h1>A Appendix</h1>
<p>[System Prompt]
You are an expert in {{expertise}}.
Your task is to answer the following multiple-choice questions.
First, you should recite all of the relevant knowledge you have about the question and each option.
Next, you should think step-by-step through the problem to ensure you have the correct answer.
Then, you should critically evaluate your thoughts to identify any flaws in your facts, logic, and reasoning.
Finally, you MUST answer the question using the following format 'Action: Answer("[choice]")'
The parameter [choice] is the letter or number of the answer you want to select (e.g. "A", "B", "C", or "D")
For example, 'Answer("C")' will select choice "C" as the best answer.
The answer MUST ALWAYS be one of the available choices; it CANNOT be "None of the Above".
If you think the answer is "none of the above", then you MUST select the most likely answer.
[Example Problem]
Question: What is the capital of the state where Johns Hopkins University is located?
Choices:
A: Baltimore
B: Annapolis
C: Des Moines
D: Las Vegas
[Example Solution]
Knowledge:
Johns Hopkins University is located in Baltimore, Maryland.
A: Baltimore is a city located in the State of Maryland, but it is not the capital of Maryland.
B: Annapolis is the capital of the State of Maryland.
C: Des Moines is a city located in the State of Iowa and is the capital of Iowa.
D: Las Vegas is located in the State of Nevada, but it is not the capital of Nevada.
Thought:
Johns Hopkins University is located in Baltimore.
Baltimore is a city located in the state of Maryland.
The capital of Maryland is Baltimore.
Therefore, the capital of the state where Johns Hopkins University is located is Baltimore.
The answer is A: Baltimore.
Criticism:
You are correct that Johns Hopkins is located in the State of Maryland.
However, the capital of Maryland is Annapolis, not Baltimore.
So, the correct answer is actually B: Annapolis.
Action: Answer("B")
Figure 10: Sample of the composite system prompt with a one-shot example (i.e., problem-and-solution pair).</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;arc/arc-challenge-test&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;source_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;topic&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Science&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;context&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;question&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;An astronomer observes that a planet rotates faster</span>
<span class="s">                                    after a meteorite impact. Which is the most likely effect</span>
<span class="s">                                    of this increase in rotation?&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;choices&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;A&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary density will decrease.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;B&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary years will become longer.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;C&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary days will become shorter.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;D&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary gravity will become stronger.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="s">&quot;answer&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;solution&quot;</span><span class="p">:</span><span class="s">&quot;&quot;</span>
<span class="p">}</span>
</code></pre></div>

<p>Figure 11: Sample of an MCQA problem in JSON-L format - with whitespace added for readability.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We considered the ARC Challenge results to be non-significant since they were greater than the significance threshold of 0.05 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9670 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9670</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9670</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277451598</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.23229v1.pdf" target="_blank">Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models provide significant new opportunities for the generation of high-quality written works. However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles. In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs. For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering. To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers. To enable easy utilization in the scientific community, we release both, a website (https://citegeist.org), as well as an implementation harness that works with several different LLM implementations.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9670.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9670.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citegeist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic Retrieval-Augmented Generation pipeline that retrieves, summarizes, and synthesizes citation-backed related-work sections from the arXiv corpus by combining dense retrieval, multi-stage filtering, per-paper summarization, and an LLM synthesizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4o (primary LLM); all-mpnet-base-v2 (embedding); BERTopic (topic assignment); Milvus (vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline uses all-mpnet-base-v2 (sentence-transformer) to embed abstracts, Milvus for similarity search, BERTopic for topic assignment, and GPT4o to summarize selected pages/abstracts and to synthesize the aggregated summaries into a related work section; retrieval selection uses a diversity-weighted iterative selection formula and per-page embedding similarity for depth selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>The arXiv corpus: an ever-growing scholarly corpus of academic papers; pipeline embeds abstracts for the full corpus and fetches full papers only for shortlisted candidates; abstracts used for large-scale indexing and full-text only used for selected candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>2600000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Target paper excerpt (typically the abstract, optionally a full paper); retrieval guided by three user-exposed hyperparameters: breadth (initial candidate set size), diversity (tradeoff between similarity and diversity in selection), and depth (number of most-relevant pages per candidate).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Dynamic Retrieval-Augmented Generation (RAG) pipeline: (1) embed input abstract with all-mpnet-base-v2 and retrieve nearest abstracts from Milvus; (2) select longlist using iterative selection i* = argmax((1-w)*s_i + w*(1 - min_sim_to_selected)) to trade off similarity and diversity; (3) fetch full PDFs for selected candidates, embed each page, select top-k pages per paper (depth) using same diversity-weighted selection; (4) summarize each shortlisted paper's selected pages + its abstract using GPT4o (tailored to input); (5) aggregate per-paper summaries into a synthesis prompt instructing GPT4o to produce a joint related work section with extracted citations via arXiv API; (6) post-filter and generate citation list linking to arXiv pages. Alternative shallow variant: embed over all pages if user uploads full paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-backed related work sections (narrative synthesis); citation-backed answers to scientific questions (QA); lists of relevant citations/links.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Sample related-work excerpt produced by the pipeline: "In recent years, the integration of Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) has emerged as a promising approach to enhance the generation of citation-backed scientific content. Several studies have explored this domain, each contributing unique methodologies and insights that align with or complement our research..."</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>LLM-as-a-judge evaluation using GPT4o, Gemini 1.5-Pro 002, and Mistral-Large to assign 0-10 scores on two separate dimensions (source relevance and writing quality). Evaluation dataset: 11 held-out draft papers not in the indexed arXiv snapshot. Baselines: direct GPT4o prompting, an agentic GPT4o workflow with arXiv API keyword lookup, and the source paper's own related work (approximate gold standard). Manual correction was performed for GPT4o baseline citations (invalid citations were replaced with "(invalid citation)").</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Citegeist strongly outperformed direct GPT4o prompting across measured metrics and was frequently rated above the source related work; evaluator variance noted (Gemini tended to rate Citegeist slightly weaker than GPT, Mistral slightly stronger). Ablations: setting diversity to 0.3 caused relevancy to drop by -0.05 and writing quality to rise +0.17; increasing depth from 2 to 6 reduced relevancy by -0.31 and had negligible effect on quality (+0.01). No human annotators used (resource constraints); authors acknowledge possible bias toward their method.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scales to a 2.6M-abstract index by embedding abstracts only and fetching full-text selectively; reduces hallucination risk by grounding generation in retrieved sources; tunable retrieval hyperparameters (breadth, depth, diversity) let users shape output; supports PDF uploads; hosted vector DB option to lower hardware barriers; demonstrates improved source relevance and writing quality versus direct LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Processing-time and update overhead for large corpus (example: typical two-week update cycle ≈ 4 hours without GPU); context-window limits constrain handling of many full papers and the number of related works per generated section (~10–12 items); increasing depth can dilute relevance because the summarizer struggles to consolidate many pages; arXiv-specific content distribution biases (uneven domain coverage); evaluation limited by LLM-as-judge methodology and small held-out set.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Generated related-work sections can become shallow when summarization depth increases; relevance dilution when selecting many pages (depth too large) causing worse relevancy scores; context-window limitations prevent comprehensive synthesis across large numbers of full texts; direct LLM baselines (GPT4o) produced invalid or hallucinated citations (which the pipeline aims to avoid via retrieval grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9670.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary LLM used in this work both as the summarization/synthesis model in the Citegeist pipeline and as a baseline direct generator and LLM judge in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used to (a) summarize selected pages and abstracts per shortlisted paper (tailored prompts including source abstract), (b) synthesize aggregated summaries into a related work section including citations, (c) produce a direct-prompt baseline related work, and (d) act as an LLM-as-a-judge to score relevance and writing quality (0-10). Implementation details (architecture, parameter count, training corpus) are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>When used in Citegeist, GPT4o's input is the per-paper summaries and synthesis prompt (grounded in retrieved content). As a baseline, GPT4o was given the target paper abstract or full draft without retrieval grounding; in the agentic baseline the model had API access to arXiv search results to pick papers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Prompts ask GPT4o to generate a related work for a given paper abstract or full draft; in synthesis role it receives per-paper summaries and explicit instructions to include citations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct LLM prompting (baseline) and LLM-based summarization+synthesis (when used inside Citegeist). Agentic variant: GPT4o with arXiv API search capability to select candidate works from keyword search.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated related work sections (narrative), per-paper summaries, and numeric relevance/quality scores when used as judge.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Baseline outputs included full related-work sections that sometimes contained incorrect or non-existent citations (which authors manually corrected for baseline analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>GPT4o was both a participant (generator) and an evaluator (judge). As evaluator it scored source relevance and writing quality 0-10; authors note potential evaluator bias when generator and judge are the same model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Direct GPT4o prompting baseline performed worse than Citegeist's RAG pipeline; GPT4o baseline produced invalid citations in several cases, leading authors to manually mark/replace them when analyzing citation correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High-quality natural language generation and flexible prompt-driven summarization/synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tendency to hallucinate invalid or non-existent sources when operating without grounded retrieval; evaluator bias when used as judge on outputs it generated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Generated incorrect citations in direct prompting baseline; required manual verification and replacement in the evaluation process.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9670.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-Large (instruct variant referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open/available LLM used in this work as an alternative LLM-as-a-judge to evaluate relevance and writing quality of generated related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Large (huggingface reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used only as an independent evaluator model to score the pipeline outputs on relevance and quality; specific prompting for scoring not fully detailed. No architectural or parameter details are provided in the paper beyond a HuggingFace reference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Numeric scores for source relevance and writing quality (0-10).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Served as one of three LLM evaluators (alongside GPT4o and Gemini) to reduce bias and provide alternative scoring judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mistral tended to rate Citegeist's outputs a little stronger than GPT4o did; no absolute numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides a different evaluator perspective reducing reliance on a single LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still subject to LLM-as-judge biases and not a substitute for human expert annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported beyond general concerns about LLM-evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9670.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5-Pro 002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial large multimodal LLM used as an alternative evaluator (LLM-as-a-judge) to score generated related work outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5-Pro 002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an independent scoring model for source relevance and writing quality; paper gives no architectural or parameter details beyond reference to the Gemini family.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Numeric scores (0-10) for relevance and writing quality.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as complementary LLM judge to GPT4o and Mistral-Large; comparisons highlighted evaluator variance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Gemini tended to rate Citegeist a little weaker than GPT4o did; no absolute numeric scores reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides independent assessment from a different LLM family.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential evaluator bias; differences in ratings across LLM judges observed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported beyond inter-evaluator variability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9670.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Openscholar: Synthesizing scientific literature with retrieval-augmented lms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior system that leverages a large datastore of open-access papers and retrieval-augmented LMs to synthesize citation-backed responses and adapt to continuous growth of scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openscholar: Synthesizing scientific literature with retrieval-augmented lms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>retrieval-augmented LMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a retrieval-augmented LM system that synthesizes literature using a large open-access paper datastore; specific model and architectural details are not provided in this paper (cited as Asai et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A vast datastore of open-access papers (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Synthesizing relevant scientific papers for a given query or draft; focuses on literature synthesis and citation-backed responses.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented LM synthesis (details not given here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-backed synthesized literature responses (narrative syntheses).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Emphasized adaptation to continuous growth in literature and the ability to synthesize citation-backed content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Specific strengths/weaknesses not detailed in the Citegeist paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9670.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM (toolkit for automating literature reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned toolkit that applies RAG principles to automate literature review synthesis and mitigate hallucinations; cited as Agarwal et al., 2024 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>retrieval-augmented LMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described at high level as a toolkit applying RAG to literature reviews; no model names, sizes, or implementation details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here (assumed large literature corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automated literature review synthesis queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>RAG-based retrieval and LM synthesis (high-level description only).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated literature reviews / synthesized literature content.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Aims to mitigate hallucinations via retrieval grounding for literature review automation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9670.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciLit / SciLit pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciLit (a pipeline for automating retrieval, summarization, and citation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited pipeline that uses two-stage pre-fetching and re-ranking to recommend papers and automate retrieval, summarization, and citation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified retrieval and summarization components</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reported as a pipeline combining retrieval, summarization, and citation generation with two-stage pre-fetching and re-ranking; specific LLM or model details are not included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the Citegeist paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automating retrieval and summarization for recommending relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-stage pre-fetching and re-ranking followed by summarization and citation generation (high-level).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Recommended papers with summaries and citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Efficient recommendation via pre-fetching and re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed within the Citegeist paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9670.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Ref</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Ref (writing assistant for reference handling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned tool that improves reference handling by retrieving and generating content directly from text paragraphs, positioned as an alternative to traditional RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-level mention: LLM-Ref retrieves and generates content directly from textual paragraphs to enhance reference handling; no implementation details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Reference-aware content generation for scientific writing.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Paragraph-level retrieval and generation (high-level description only).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Reference-enhanced generated text / citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Potentially tighter coupling of generated text with specific source paragraphs to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9670.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9670.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-CTG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-CTG (knowledge-graph based citation text generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a framework that integrates knowledge graphs and full-text grounding to improve citation accuracy and contextual relevance of generated citation text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified (KG + LM hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described at high level: uses knowledge graphs (KG) for citation text generation to improve accuracy; specific LLMs or KG construction details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Grounded citation generation using KG and full-text grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>KG-enhanced generation (high-level description only).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation text generation aligned with KG-grounded context.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Proposed to improve citation accuracy and context relevance compared to purely text-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details and empirical results not included in the Citegeist paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Active retrieval augmented generation <em>(Rating: 2)</em></li>
                <li>Automatic related work generation: A meta study <em>(Rating: 2)</em></li>
                <li>Litsearch: A retrieval benchmark for scientific literature search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9670",
    "paper_id": "paper-277451598",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Citegeist",
            "name_full": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
            "brief_description": "A dynamic Retrieval-Augmented Generation pipeline that retrieves, summarizes, and synthesizes citation-backed related-work sections from the arXiv corpus by combining dense retrieval, multi-stage filtering, per-paper summarization, and an LLM synthesizer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4o (primary LLM); all-mpnet-base-v2 (embedding); BERTopic (topic assignment); Milvus (vector DB)",
            "model_description": "Pipeline uses all-mpnet-base-v2 (sentence-transformer) to embed abstracts, Milvus for similarity search, BERTopic for topic assignment, and GPT4o to summarize selected pages/abstracts and to synthesize the aggregated summaries into a related work section; retrieval selection uses a diversity-weighted iterative selection formula and per-page embedding similarity for depth selection.",
            "model_size": null,
            "input_corpus_description": "The arXiv corpus: an ever-growing scholarly corpus of academic papers; pipeline embeds abstracts for the full corpus and fetches full papers only for shortlisted candidates; abstracts used for large-scale indexing and full-text only used for selected candidates.",
            "input_corpus_size": 2600000,
            "topic_query_description": "Target paper excerpt (typically the abstract, optionally a full paper); retrieval guided by three user-exposed hyperparameters: breadth (initial candidate set size), diversity (tradeoff between similarity and diversity in selection), and depth (number of most-relevant pages per candidate).",
            "distillation_method": "Dynamic Retrieval-Augmented Generation (RAG) pipeline: (1) embed input abstract with all-mpnet-base-v2 and retrieve nearest abstracts from Milvus; (2) select longlist using iterative selection i* = argmax((1-w)*s_i + w*(1 - min_sim_to_selected)) to trade off similarity and diversity; (3) fetch full PDFs for selected candidates, embed each page, select top-k pages per paper (depth) using same diversity-weighted selection; (4) summarize each shortlisted paper's selected pages + its abstract using GPT4o (tailored to input); (5) aggregate per-paper summaries into a synthesis prompt instructing GPT4o to produce a joint related work section with extracted citations via arXiv API; (6) post-filter and generate citation list linking to arXiv pages. Alternative shallow variant: embed over all pages if user uploads full paper.",
            "output_type": "Citation-backed related work sections (narrative synthesis); citation-backed answers to scientific questions (QA); lists of relevant citations/links.",
            "output_example": "Sample related-work excerpt produced by the pipeline: \"In recent years, the integration of Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) has emerged as a promising approach to enhance the generation of citation-backed scientific content. Several studies have explored this domain, each contributing unique methodologies and insights that align with or complement our research...\"",
            "evaluation_method": "LLM-as-a-judge evaluation using GPT4o, Gemini 1.5-Pro 002, and Mistral-Large to assign 0-10 scores on two separate dimensions (source relevance and writing quality). Evaluation dataset: 11 held-out draft papers not in the indexed arXiv snapshot. Baselines: direct GPT4o prompting, an agentic GPT4o workflow with arXiv API keyword lookup, and the source paper's own related work (approximate gold standard). Manual correction was performed for GPT4o baseline citations (invalid citations were replaced with \"(invalid citation)\").",
            "evaluation_results": "Citegeist strongly outperformed direct GPT4o prompting across measured metrics and was frequently rated above the source related work; evaluator variance noted (Gemini tended to rate Citegeist slightly weaker than GPT, Mistral slightly stronger). Ablations: setting diversity to 0.3 caused relevancy to drop by -0.05 and writing quality to rise +0.17; increasing depth from 2 to 6 reduced relevancy by -0.31 and had negligible effect on quality (+0.01). No human annotators used (resource constraints); authors acknowledge possible bias toward their method.",
            "strengths": "Scales to a 2.6M-abstract index by embedding abstracts only and fetching full-text selectively; reduces hallucination risk by grounding generation in retrieved sources; tunable retrieval hyperparameters (breadth, depth, diversity) let users shape output; supports PDF uploads; hosted vector DB option to lower hardware barriers; demonstrates improved source relevance and writing quality versus direct LLM prompting.",
            "limitations": "Processing-time and update overhead for large corpus (example: typical two-week update cycle ≈ 4 hours without GPU); context-window limits constrain handling of many full papers and the number of related works per generated section (~10–12 items); increasing depth can dilute relevance because the summarizer struggles to consolidate many pages; arXiv-specific content distribution biases (uneven domain coverage); evaluation limited by LLM-as-judge methodology and small held-out set.",
            "failure_cases": "Generated related-work sections can become shallow when summarization depth increases; relevance dilution when selecting many pages (depth too large) causing worse relevancy scores; context-window limitations prevent comprehensive synthesis across large numbers of full texts; direct LLM baselines (GPT4o) produced invalid or hallucinated citations (which the pipeline aims to avoid via retrieval grounding).",
            "uuid": "e9670.0",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT4o",
            "name_full": "GPT4o",
            "brief_description": "A proprietary LLM used in this work both as the summarization/synthesis model in the Citegeist pipeline and as a baseline direct generator and LLM judge in evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4o",
            "model_description": "Used to (a) summarize selected pages and abstracts per shortlisted paper (tailored prompts including source abstract), (b) synthesize aggregated summaries into a related work section including citations, (c) produce a direct-prompt baseline related work, and (d) act as an LLM-as-a-judge to score relevance and writing quality (0-10). Implementation details (architecture, parameter count, training corpus) are not provided in the paper.",
            "model_size": null,
            "input_corpus_description": "When used in Citegeist, GPT4o's input is the per-paper summaries and synthesis prompt (grounded in retrieved content). As a baseline, GPT4o was given the target paper abstract or full draft without retrieval grounding; in the agentic baseline the model had API access to arXiv search results to pick papers.",
            "input_corpus_size": null,
            "topic_query_description": "Prompts ask GPT4o to generate a related work for a given paper abstract or full draft; in synthesis role it receives per-paper summaries and explicit instructions to include citations.",
            "distillation_method": "Direct LLM prompting (baseline) and LLM-based summarization+synthesis (when used inside Citegeist). Agentic variant: GPT4o with arXiv API search capability to select candidate works from keyword search.",
            "output_type": "Generated related work sections (narrative), per-paper summaries, and numeric relevance/quality scores when used as judge.",
            "output_example": "Baseline outputs included full related-work sections that sometimes contained incorrect or non-existent citations (which authors manually corrected for baseline analysis).",
            "evaluation_method": "GPT4o was both a participant (generator) and an evaluator (judge). As evaluator it scored source relevance and writing quality 0-10; authors note potential evaluator bias when generator and judge are the same model.",
            "evaluation_results": "Direct GPT4o prompting baseline performed worse than Citegeist's RAG pipeline; GPT4o baseline produced invalid citations in several cases, leading authors to manually mark/replace them when analyzing citation correctness.",
            "strengths": "High-quality natural language generation and flexible prompt-driven summarization/synthesis.",
            "limitations": "Tendency to hallucinate invalid or non-existent sources when operating without grounded retrieval; evaluator bias when used as judge on outputs it generated.",
            "failure_cases": "Generated incorrect citations in direct prompting baseline; required manual verification and replacement in the evaluation process.",
            "uuid": "e9670.1",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mistral-Large",
            "name_full": "Mistral-Large (instruct variant referenced)",
            "brief_description": "A large open/available LLM used in this work as an alternative LLM-as-a-judge to evaluate relevance and writing quality of generated related work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-Large (huggingface reference)",
            "model_description": "Used only as an independent evaluator model to score the pipeline outputs on relevance and quality; specific prompting for scoring not fully detailed. No architectural or parameter details are provided in the paper beyond a HuggingFace reference.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": null,
            "distillation_method": null,
            "output_type": "Numeric scores for source relevance and writing quality (0-10).",
            "output_example": null,
            "evaluation_method": "Served as one of three LLM evaluators (alongside GPT4o and Gemini) to reduce bias and provide alternative scoring judgments.",
            "evaluation_results": "Mistral tended to rate Citegeist's outputs a little stronger than GPT4o did; no absolute numbers provided.",
            "strengths": "Provides a different evaluator perspective reducing reliance on a single LLM judge.",
            "limitations": "Still subject to LLM-as-judge biases and not a substitute for human expert annotation.",
            "failure_cases": "Not reported beyond general concerns about LLM-evaluator biases.",
            "uuid": "e9670.2",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro-002",
            "name_full": "Gemini 1.5-Pro 002",
            "brief_description": "A commercial large multimodal LLM used as an alternative evaluator (LLM-as-a-judge) to score generated related work outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5-Pro 002",
            "model_description": "Applied as an independent scoring model for source relevance and writing quality; paper gives no architectural or parameter details beyond reference to the Gemini family.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": null,
            "distillation_method": null,
            "output_type": "Numeric scores (0-10) for relevance and writing quality.",
            "output_example": null,
            "evaluation_method": "Used as complementary LLM judge to GPT4o and Mistral-Large; comparisons highlighted evaluator variance.",
            "evaluation_results": "Gemini tended to rate Citegeist a little weaker than GPT4o did; no absolute numeric scores reported in the paper.",
            "strengths": "Provides independent assessment from a different LLM family.",
            "limitations": "Potential evaluator bias; differences in ratings across LLM judges observed.",
            "failure_cases": "Not reported beyond inter-evaluator variability concerns.",
            "uuid": "e9670.3",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms",
            "brief_description": "A cited prior system that leverages a large datastore of open-access papers and retrieval-augmented LMs to synthesize citation-backed responses and adapt to continuous growth of scientific literature.",
            "citation_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "mention_or_use": "mention",
            "model_name": "retrieval-augmented LMs (unspecified)",
            "model_description": "Described as a retrieval-augmented LM system that synthesizes literature using a large open-access paper datastore; specific model and architectural details are not provided in this paper (cited as Asai et al., 2024).",
            "model_size": null,
            "input_corpus_description": "A vast datastore of open-access papers (details not specified in this paper).",
            "input_corpus_size": null,
            "topic_query_description": "Synthesizing relevant scientific papers for a given query or draft; focuses on literature synthesis and citation-backed responses.",
            "distillation_method": "Retrieval-augmented LM synthesis (details not given here).",
            "output_type": "Citation-backed synthesized literature responses (narrative syntheses).",
            "output_example": null,
            "evaluation_method": "Not specified within this paper.",
            "evaluation_results": "Not reported here.",
            "strengths": "Emphasized adaptation to continuous growth in literature and the ability to synthesize citation-backed content.",
            "limitations": "Specific strengths/weaknesses not detailed in the Citegeist paper.",
            "failure_cases": "Not described in this paper.",
            "uuid": "e9670.4",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM (toolkit for automating literature reviews)",
            "brief_description": "Mentioned toolkit that applies RAG principles to automate literature review synthesis and mitigate hallucinations; cited as Agarwal et al., 2024 in the paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "retrieval-augmented LMs (unspecified)",
            "model_description": "Described at high level as a toolkit applying RAG to literature reviews; no model names, sizes, or implementation details are provided in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified here (assumed large literature corpora).",
            "input_corpus_size": null,
            "topic_query_description": "Automated literature review synthesis queries.",
            "distillation_method": "RAG-based retrieval and LM synthesis (high-level description only).",
            "output_type": "Automated literature reviews / synthesized literature content.",
            "output_example": null,
            "evaluation_method": "Not specified in this paper.",
            "evaluation_results": "Not reported here.",
            "strengths": "Aims to mitigate hallucinations via retrieval grounding for literature review automation.",
            "limitations": "Details not provided in this paper.",
            "failure_cases": "Not described here.",
            "uuid": "e9670.5",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SciLit / SciLit pipeline",
            "name_full": "SciLit (a pipeline for automating retrieval, summarization, and citation)",
            "brief_description": "Cited pipeline that uses two-stage pre-fetching and re-ranking to recommend papers and automate retrieval, summarization, and citation generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified retrieval and summarization components",
            "model_description": "Reported as a pipeline combining retrieval, summarization, and citation generation with two-stage pre-fetching and re-ranking; specific LLM or model details are not included in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified in the Citegeist paper.",
            "input_corpus_size": null,
            "topic_query_description": "Automating retrieval and summarization for recommending relevant papers.",
            "distillation_method": "Two-stage pre-fetching and re-ranking followed by summarization and citation generation (high-level).",
            "output_type": "Recommended papers with summaries and citations.",
            "output_example": null,
            "evaluation_method": "Not detailed in this paper.",
            "evaluation_results": "Not provided here.",
            "strengths": "Efficient recommendation via pre-fetching and re-ranking.",
            "limitations": "Not detailed within the Citegeist paper.",
            "failure_cases": "Not described here.",
            "uuid": "e9670.6",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Ref",
            "name_full": "LLM-Ref (writing assistant for reference handling)",
            "brief_description": "Mentioned tool that improves reference handling by retrieving and generating content directly from text paragraphs, positioned as an alternative to traditional RAG systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified",
            "model_description": "High-level mention: LLM-Ref retrieves and generates content directly from textual paragraphs to enhance reference handling; no implementation details provided in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified here.",
            "input_corpus_size": null,
            "topic_query_description": "Reference-aware content generation for scientific writing.",
            "distillation_method": "Paragraph-level retrieval and generation (high-level description only).",
            "output_type": "Reference-enhanced generated text / citations.",
            "output_example": null,
            "evaluation_method": "Not specified in this paper.",
            "evaluation_results": "Not reported here.",
            "strengths": "Potentially tighter coupling of generated text with specific source paragraphs to reduce hallucination.",
            "limitations": "Details not provided in this paper.",
            "failure_cases": "Not described here.",
            "uuid": "e9670.7",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "KG-CTG",
            "name_full": "KG-CTG (knowledge-graph based citation text generation framework)",
            "brief_description": "Mentioned as a framework that integrates knowledge graphs and full-text grounding to improve citation accuracy and contextual relevance of generated citation text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified (KG + LM hybrid)",
            "model_description": "Described at high level: uses knowledge graphs (KG) for citation text generation to improve accuracy; specific LLMs or KG construction details are not provided in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified here.",
            "input_corpus_size": null,
            "topic_query_description": "Grounded citation generation using KG and full-text grounding.",
            "distillation_method": "KG-enhanced generation (high-level description only).",
            "output_type": "Citation text generation aligned with KG-grounded context.",
            "output_example": null,
            "evaluation_method": "Not specified in this paper.",
            "evaluation_results": "Not provided here.",
            "strengths": "Proposed to improve citation accuracy and context relevance compared to purely text-based retrieval.",
            "limitations": "Details and empirical results not included in the Citegeist paper.",
            "failure_cases": "Not described here.",
            "uuid": "e9670.8",
            "source_info": {
                "paper_title": "Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "rating": 2,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Active retrieval augmented generation",
            "rating": 2,
            "sanitized_title": "active_retrieval_augmented_generation"
        },
        {
            "paper_title": "Automatic related work generation: A meta study",
            "rating": 2,
            "sanitized_title": "automatic_related_work_generation_a_meta_study"
        },
        {
            "paper_title": "Litsearch: A retrieval benchmark for scientific literature search",
            "rating": 1,
            "sanitized_title": "litsearch_a_retrieval_benchmark_for_scientific_literature_search"
        }
    ],
    "cost": 0.020784,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus
29 Mar 2025</p>
<p>Claas Beger 
Carl-Leander Henneking 
Nobuyuki Zhu 
Kevin Morioka 
Krishna Hui 
Victor Hari- Dasan 
Mahdis Campos 
Mandy Mahdieh 
Samer Guo 
Kevin Hassan 
Arpi Kilgour 
Heng- Tze Vezer 
Raoul Cheng 
Siddharth De Liedekerke 
Paul Goyal 
D J Barham 
Seb Strouse 
Jonas Noury 
Mukund Adler 
Sharad Sundararajan 
Dmitry Vikram 
Michela Lep- Ikhin 
Xavier Paganini 
Fan Garcia 
Dasha Yang 
Maja Valter 
Kiran Trebacz 
Chu- Layuth Vodrahalli 
Roman Asawaroengchai 
Norbert Ring 
Kalb 
Baldini Livio 
Siddhartha Soares 
David Brahma 
Tianhe Steiner 
Fabian Yu 
Antoine Mentzer 
Lucas He 
Bibo Gonzalez 
Raphael Xu 
Kauf- Man Lopez 
Laurent El 
Junhyuk Oh 
Tom Hennigan 
George Van Den Driessche 
Seth Odoom 
Mario Lucic 
Becca Roelofs 
Sid Lall 
Amit Marathe 
Betty Chan 
Santiago Ontanon 
Luheng He 
Denis Teplyashin 
Jonathan Lai 
Phil Crone 
Bogdan Damoc 
Lewis Ho 
Sebastian Riedel 
Karel Lenc 
Chih-Kuan Yeh 
Aakanksha Chowdhery 
Yang Xu 
Mehran Kazemi 
Ehsan Amid 
Anastasia Petrushkina 
Kevin Swersky 
Ali Khodaei 
Gowoon Chen 
Chris Larkin 
Mario Pinto 
Geng Yan 
Adria Puigdomenech Badia 
Piyush Patil 
Steven Hansen 
Dave Orr 
Sebastien M R Arnold 
Jordan Grimstad 
Andrew Dai 
Sholto Dou- Glas 
Rishika Sinha 
Vikas Yadav 
Xi Chen 
Elena Gri- Bovskaya 
Jacob Austin 
Jeffrey Zhao 
Kaushal Patel 
Paul Komarek 
Sophia Austin 
Sebastian Borgeaud 
Linda Friso 
Abhimanyu Goyal 
Ben Caine 
Kris Cao 
Da-Woon Chung 
Matthew Lamm 
Gabe Barth- Maron 
Thais Kagohara 
Kate Olszewska 
Mia Chen 
Kaushik Shivakumar 
Rishabh Agarwal 
Harshal Godhia 
Ravi Rajwar 
Javier Snaider 
Xerxes Doti- Walla 
Yuan Liu 
Aditya Barua 
Victor Ungureanu 
Bat-OrgilYuan Zhang 
Mateo Batsaikhan 
James Wirth 
Ivo Qin 
Tulsee Danihelka 
Martin Doshi 
Jilin Chadwick 
Sanil Chen 
Quoc Jain 
Ar- Jun Le 
Madhu Kar 
Cheng Gurumurthy 
Ruoxin Li 
Fangyu Sang 
Lampros Liu 
Rich Lamprou 
Nathan Munoz 
Harsh Lintz 
Heidi Mehta 
Mal- Colm Howard 
Lora Reynolds 
Quan Aroyo 
Lorenzo Wang 
Albin Blanco 
Jordan Cassirer 
Dipanjan Griffith 
Stephan Das 
Jakub Lee 
Zach Sygnowski 
James Fisher 
Richard Besley 
Zafarali Powell 
Do- Minik Ahmed 
David Paulus 
Zalan Reitter 
Rishabh Borsos 
Aedan Joshi 
Steven Pope 
Vittorio Hand 
Vi- Han Selo 
Nikhil Jain 
Megha Sethi 
Takaki Goel 
Rhys Makino 
Zhen May 
Johan Yang 
Christina Schalkwyk 
Anja Butterfield 
Alex Hauth 
Will Goldin 
Evan Hawkins 
Sergey Senter 
Oliver Brin 
Mar- Vin Woodman 
Eric Ritter 
Minh Noland 
Vijay Giang 
Lisa Bolina 
Tim Lee 
Ian Blyth 
Machel Mackinnon 
Obaid Reid 
David Sarvana 
Alexander Silver 
Lily Chen 
Loren Wang 
Oscar Maggiore 
Nithya Chang 
Gregory At- Taluri 
Chung-Cheng Thornton 
Os- Kar Chiu 
Nir Bunyan 
Timothy Levine 
Evgenii Chung 
Xiance Eltyshev 
Timothy Si 
Demetra Lillicrap 
Vaibhav Brady 
Boxi Aggarwal 
Yuanzhong Wu 
Ross Xu 
Kartikeya Mcilroy 
Paramjit Badola 
Erica Sandhu 
Wojciech Moreira 
Ross Stokowiec 
Dong Hems- Ley 
Alex Li 
Pranav Tudor 
Elahe Shyam 
Salem Rahimtoroghi 
Pablo Haykal 
Xiang Sprechmann 
Diana Zhou 
Yujia Mincu 
Ravi Li 
Kalpesh Addanki 
Xiao Krishna 
Alexandre Wu 
Matan Frechette 
Allan Eyal 
Dave Dafoe 
Jay Lacey 
Whang 
Nora Shrader 
Mantas Kassner 
Matt Pajarskas 
Sean Harvey 
Meire Sechrist 
Christina Fortunato 
Gamaleldin Lyu 
Chenkai Elsayed 
James Kuang 
Eric Lottes 
Chao Chu 
Chih-Wei Jia 
Pe- Ter Chen 
Kate Humphreys 
Connie Baumli 
Rajku- Mar Tao 
Cicero Samuel 
Nogueira 
Anders Santos 
Nemanja Andreassen 
Dominik Rakićević 
Aviral Grewe 
Stephanie Kumar 
Jonathan Winkler 
Andrew Caton 
Sid Brock 
Hannah Dalmia 
Iain Sheahan 
Yingjie Barr 
Paul Miao 
Jacob Natsev 
Fer- Yal Devlin 
Flavien Behbahani 
Yanhua Prost 
Artiom Sun 
Thanumalayan Sankaranarayana Myaskovsky 
Dan Pillai 
Angeliki Hurt 
Xi Lazaridou 
Ce Xiong 
Fabio Zheng 
Xiaowei Pardo 
Dan Li 
Joe Horgan 
Moran Stanton 
Fei Ambar 
Alejandro Xia 
Mingqiu Lince 
Basil Wang 
Albert Mustafa 
Hyo Webson 
Ro- Han Lee 
Martin Anil 
Timothy Wicke 
Abhishek Dozat 
Enrique Sinha 
Elahe Piqueras 
Shyam Dabir 
Anudhyan Upad- Hyay 
Lisa Anne Boral 
Corey Hendricks 
Josip Fry 
Yi Djolonga 
Jake Su 
Jane Walker 
Ronny La- Banowski 
Vedant Huang 
Jeremy Misra 
R J Chen 
Avi Skerry-Ryan 
Shruti Singh 
Dian Rijh- Wani 
Alex Yu 
Beer Castro-Ros 
Romina Changpinyo 
Sumit Datta 
Arnar Bagri 
Hrafnkels- Son Mar 
Marcello Maggioni 
Daniel Zheng 
Yury Sul- Sky 
Shaobo Hou 
Tom Le Paine 
Antoine Yang 
Jason Riesa 
Dominika Rogozinska 
Dror Marcus 
Dalia El Badawy 
Qiao Zhang 
Luyu Wang 
Helen Miller 
Jeremy Greer 
Lars Lowe Sjos 
Azade Nova 
Heiga Zen 
Rahma Chaabouni 
Mihaela Rosca 
Jiepu Jiang 
Charlie Chen 
Ruibo Liu 
Tara Sainath 
Maxim Krikun 
Alex Polozov 
Jean-Baptiste Lespiau 
Josh Newlan 
Zeyncep Cankara 
Soo Kwak 
Yunhan Xu 
Phil Chen 
Andy Coenen 
Clemens Meyer 
Katerina Tsihlas 
Ada Ma 
Juraj Gottweis 
Jinwei Xing 
Chen- Jie Gu 
Jin Miao 
Christian Frank 
Zeynep Cankara 
Sanjay Ganapathy 
Ishita Dasgupta 
Steph Hughes- Fitt 
Heng Chen 
David Reid 
Keran Rong 
Hongmin Fan 
Joost Van Amersfoort 
Vincent Zhuang 
ShixiangAaron Cohen 
Shane Gu 
Anhad Mohananey 
Anastasija Ilic 
Taylor Tobin 
John Wieting 
Anna Bortsova 
Phoebe Thacker 
Emma Wang 
Emily Caveness 
Justin Chiu 
Eren Sezener 
Alex Kaskasoli 
Steven Baker 
Katie Millican 
Mohamed Elhawaty 
Kostas Aisopos 
Carl Lebsack 
Nathan Byrd 
Hanjun Dai 
Wenhao Jia 
Matthew Wiethoff 
Elnaz Davoodi 
Albert Weston 
Lakshman Yagati 
Arun Ahuja 
Isabel Gao 
Golan Pundak 
Susan Zhang 
Michael Azzam 
Khe Chai 
Sergi Caelles 
James Keeling 
Ab- Hanshu Sharma 
Andy Swing 
Yaguang Li 
Chenxi Liu 
Carrie Grimes Bostock 
Yamini Bansal 
Zachary Nado 
Ankesh Anand 
Josh Lipschultz 
Abhijit Kar- Markar 
Lev Proleev 
Abe Ittycheriah 
Soheil Has- Sas Yeganeh 
George Polovets 
Aleksandra Faust 
Jiao Sun 
Alban Rrustemi 
Pen Li 
Rakesh Shivanna 
Jeremiah Liu 
Chris Welty 
Federico Lebron 
Anirudh Baddepudi 
Sebastian Krause 
Emilio Parisotto 
Radu Soricut 
Zheng Xu 
Dawn Bloxwich 
John- Son Melvin 
Behnam Neyshabur 
Justin Mao-Jones 
Ren- Shen Wang 
Vinay Ramasesh 
Zaheer Abbas 
Constant SegalArthur Guez 
DucDung Nguyen 
Le Svensson 
Sarah Hou 
Kieran York 
So- Phie Milan 
Wiktor Bridgers 
Marco Gworek 
James Tagliasacchi 
Michael Lee-Thorp 
Alexey Chang 
AleJakse Guseynov 
Michael Hartman 
Ruizhe Kwong 
Sheleem Zhao 
Elizabeth Kashem 
Antoine Cole 
Richard Miech 
Mary Tanburn 
Filip Phuong 
Se- Bastien Pavetic 
Ramona Cevey 
Richard Comanescu 
Sherry Ives 
Cosmo Yang 
Bo Du 
Zizhao Li 
Mariko Zhang 
Clara Huiyi Iinuma 
Aurko Hu 
Shaan Roy 
Zhenkai Bijwadia 
Danilo Zhu 
Rachel Martins 
Anita Saputro 
Steven Gergely 
Dawei Zheng 
Ioannis Jia 
Adam Antonoglou 
Shane Sadovsky 
Yingying Gu 
Alek Bi 
Sina Andreev 
Mina Samangooei 
Tomas Khan 
Angelos Kocisky 
Chintu Filos 
Colton Ku- Mar 
Adams Bishop 
Sarah Yu 
Sid Hodkin- Son 
Premal Mittal 
Alexandre Shah 
Yong Moufarek 
Adam Cheng 
Jaehoon Bloniarz 
Pedram Lee 
Paul Pejman 
Stephen Michel 
Vladimir Spencer 
Xuehan Feinberg 
Nikolay Xiong 
Char- Lotte Savinov 
Siamak Smith 
Dustin Shakeri 
Mary Tran 
Bernd Chesus 
George Bohnet 
Tamara Tucker 
Carrie Von Glehn 
Yiran Muir 
Hideto Mao 
Ambrose Kazawa 
Kedar Slone 
Disha Soparkar 
James Shrivastava 
Michael Cobon-Kerr 
Jay Sharman 
Carlos Pavagadhi 
Karolis Araya 
Nimesh Misiunas 
Michael Ghelani 
David Laskin 
Qiujia Barker 
Anton Li 
Neil Briukhov 
Mia Houlsby 
Balaji Glaese 
Nathan Laksh- Minarayanan 
Yunhao Schucher 
Eli Tang 
Hyeontaek Collins 
Fangxiaoyu Lim 
Adria Feng 
Guangda Recasens 
Alberto Lai 
Nicola Magni 
Aditya De Cao 
Zoe Siddhant 
Jordi Ashwood 
Mostafa Orbay 
Jenny Dehghani 
Yifan Brennan 
Kelvin He 
Yang Xu 
Carl Gao 
James Saroufim 
Xinyi Molloy 
Seb Wu 
Solomon Arnold 
Julian Chang 
Elena Schrit- Twieser 
Soroush Buchatskaya 
Mar- Tin Radpour 
Skye Polacek 
Ankur Giordano 
Simon Bapna 
Vincent Tokumine 
Thibault Hellendoorn 
Sarah Sottiaux 
Aliaksei Cogan 
Mohammad Severyn 
Shantanu Saleh 
Laurent Thakoor 
Siyuan Shefey 
Meenu Qiao 
Shuo Gaba 
Craig Yiin Chang 
Biao Swanson 
Benjamin Zhang 
Paul Kishan Lee 
Gan Rubenstein 
Tom Song 
Anna Kwiatkowski 
Ajay Koop 
David Kan- Nan 
Parker Kao 
Axel Schuh 
Stjerngren 
Gena Gol- Naz Ghiasi 
Luke Gibson 
Ye Vilnis 
Fe- Lipe Tiengo Yuan 
Aishwarya Ferreira 
Ted Kamath 
Ken Kli- Menko 
Kefan Franko 
Indro Xiao 
Miteyan Bhattacharya 
Rui Patel 
Alex Wang 
Robin Morris 
Vivek Strudel 
Peter Sharma 
SayedHadi Choy 
Jessica Hashemi 
Mara Landon 
Priya Finkelstein 
Justin Jhakra 
Megan Frye 
Matthew Barnes 
Dennis Mauger 
Khuslen Daun 
Matthew Baatarsukh 
Wael Tung 
Henryk Farhan 
Fabio Michalewski 
Fe- Lix Viola 
Charline De Chaumont Quitry 
Tom Le Lan 
Qingze Hud- Son 
Felix Wang 
Ivy Fischer 
Elspeth Zheng 
Anca White 
Jean Dragan 
Eric Baptiste Alayrac 
Alexander Ni 
Adam Pritzel 
Michael Iwanicki 
Anna Isard 
Lukas Bulanova 
Ethan Zilka 
Deven- Dra Dyer 
Srivatsan Sachan 
Hannah Srinivasan 
Honglong Mucken- Hirn 
Amol Cai 
Mukarram Mandhane 
Jack W Tariq 
Gary Rae 
Kareem Wang 
Nicholas Ayoub 
Yao Fitzgerald 
Woohyun Zhao 
Chris Han 
Dan Alberti 
Kashyap Garrette 
Mai Krishnakumar 
Anselm Gimenez 
Daniel Levskaya 
Josip Sohn 
Inaki Matak 
Michael B Iturrate 
Jackie Chang 
Yuan Xi- Ang 
Nishant Cao 
Geoff Ranka 
Adrian Brown 
Vahab Hutter 
Nanxin Mirrokni 
Kaisheng Chen 
Zoltan Yao 
Francois Egyed 
Tyler Galilee 
Praveen Liechty 
Evan Kallakuri 
Sanjay Palmer 
Jasmine Ghemawat 
David Liu 
Chloe Tao 
Tim Thornton 
Mimi Green 
Sharon Jasarevic 
Victor Lin 
Yi-Xuan Cotruta 
Noah Tan 
Hongkun Fiedel 
Ed Yu 
Alexan- Der Chi 
Jens Neitz 
Anu Heitkaemper 
Denny Sinha 
Yi Zhou 
Charbel Sun 
Brice Kaed 
Swa- Roop Hulse 
Maria Mishra 
Sneha Georgaki 
Clement Kudugunta 
Izhak Farabet 
Daniel Shafran 
An- Ton Vlasic 
Rajagopal Tsitsulin 
Alen Ananthanarayanan 
Guolong Carin 
Pei Su 
Shashank V Sun 
Gabriel Carvajal 
Josef Broder 
Iulia Comsa 
Alena Repina 
William Wong 
Warren Weilun Chen 
Peter Hawkins 
Anand Cobo 
Chetan Iyer 
Guillermo Tekur 
Zhuyun Gar- Rido 
Rupert Xiao 
HuaixiuSteven Kemp 
Hui Zheng 
Ananth Li 
Christel Agarwal 
Kati Ngani 
Rebeca Goshvadi 
Woj- Ciech Santamaria-Fernandez 
Xinyun Fica 
Chris Chen 
Sean Gorgolewski 
Roopal Sun 
Xinyu Garg 
S M Ali Ye 
Nan Eslami 
Jon Hua 
Pratik Simon 
Yelin Joshi 
Ian Kim 
Sahitya Tenney 
Lam Potluri 
Quan Nguyen Thiet 
Florian Yuan 
Alexandra Luisier 
Sal- Vatore Chronopoulou 
Praveen Scellato 
Minmin Srinivasan 
Vinod Chen 
Valentin Koverkathu 
Yaming Dalibard 
Brennan Xu 
Keith Saeta 
Thibault Anderson 
Nick Sellam 
Fantine Fernando 
Junehyuk Huot 
Mani Jung 
Michael Varadarajan 
Amit Quinn 
Maigo Raul 
Ruslan Le 
Jon Habalov 
Komal Clark 
Kalesha Jalan 
Achintya Bullard 
Thang Singhal 
Boyu Luong 
Sujeevan Wang 
Julian Rajayogam 
Johnson Eisenschlos 
Daniel Jia 
Alex Finchelstein 
Daniel Yakubovich 
Michael Balle 
Sameer Fink 
Jing Agarwal 
Dj Li 
Shalini Dvijotham 
Kai Pal 
Jaclyn Kang 
Jennifer Konzelmann 
Olivier Beattie 
Diane Dousse 
Remi Wu 
Chen Crocker 
Siddhartha Elkind 
Jonnalagadda Reddy 
Jong Lee 
Dan Holtmann-Rice 
Krys- Tal Kallarackal 
Rosanne Liu 
Denis Vnukov 
Neera Vats 
Luca Invernizzi 
Mohsen Jafari 
Huanjie Zhou 
Lilly Taylor 
Jennifer Prendki 
Marcus Wu 
Tom Eccles 
Tianqi Liu 
Kavya Kopparapu 
Francoise Beaufays 
Christof Angermueller 
Andreea Marzoca 
Shourya Sarcar 
Hilal Dib 
Jeff Stanway 
Nejc Trdin 
Rachel Sterneck 
Andrey Khor- Lin 
Dinghua Li 
Xihui Wu 
Sonam Goenka 
David Madras 
Sasha Goldshtein 
Willi Gierke 
Tong Zhou 
Yaxin Liu 
Yannie Liang 
Anais White 
Yunjie Li 
Shreya Singh 
Sanaz Bahargam 
Mark Epstein 
Su- Joy Basu 
Li Lao 
Adnan Ozturel 
Carl Crous 
Alex Zhai 
Han Lu 
Zora Tung 
Neeraj Gaur 
Alanna Walton 
Lucas Dixon 
AmirMing Zhang 
Grant Uy 
Andrew Bolt 
Olivia Wiles 
Milad Nasr 
Ilia Shumailov 
Marco Selvi 
Francesco Pic- Cinno 
Ricardo Aguilar 
Sara Mccarthy 
Misha Khal- Man 
Mrinal Shukla 
Vlado Galic 
John Carpen- Ter 
Kevin Villela 
Haibin Zhang 
Harry Richard- Son 
James Martens 
Matko Bosnjak 
Ram- Mohan Shreyas 
Jeff Belle 
Mahmoud Seibert 
Alnahlawi </p>
<p>Cornell University</p>
<p>Cornell University</p>
<p>Daniel Toyama
Thi Avrahami
Ye Zhang, Emanuel Taropa, Eliza RutherfordHanzhao Lin</p>
<p>Motoki Sano
HyunJeong Choe</p>
<p>Alex Tomala
Chalence Safranek</p>
<p>Egor Filonov
Jingchen Ye
Lucia Loher
Christoph Hirnschall
Weiyi Wang, Andrea BurnsHardie Cate</p>
<p>Ionel Gog
Diana Gage Wright
Federico Piccinini
Yana Kulizh-skaya, Shuang SongLei Zhang, Chu-Cheng Lin, Ashwin SreevatsaLuis C</p>
<p>Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus
29 Mar 2025603982AF1920E054CB279B3D97B523A8arXiv:2503.23229v1[cs.LG]• Development of a dynamic retrieval and synthesis application for related work generation
Large Language Models provide significant new opportunities for the generation of highquality written works.However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles.In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs.For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering.To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers.To enable easy utilization in the scientific community, we release both, a website, as well as an implementation harness that works with several different LLM implementations.* Equal contribution.This work aims to transition this approach to the arXiv corpus, a popular and ever-growing dataset consisting of approximately 2.6 million academic papers.The vast size, frequent updates, and the availability of a dedicated Python library make the arXiv an intuitive choice for research-centered retrieval-augmented generation (RAG).However, this scale imposes challenges in data interaction, as the full-text dataset exceeds 1 TB.To mitigate this, we perform multiple filtering steps based on abstracts and retrieve full documents only for identified relevant sources.We then construct a vector database of abstract embeddings to enable efficient similarity search.Additionally, recognizing that the related work requirements vary across research fields and paper contents, we introduce three key hyperparameters-breadth, depth, and diversity-which are employed in our retrieval algorithm.We also place a strong emphasis on proper citation techniques and rigorously evaluate our approach in terms of source relevancy and the writing quality of the generated results.Beyond the generation of related work sections, we adapt our method to answer scientific questions with citation-based responses, yielding promising results.</p>
<p>Introduction</p>
<p>Significant advances in language generation quality in recent years have enabled the employment of Large Language Models (LLMs) as writing assistants for various fields and purposes.However, while the quality of produced texts is often high, LLMs face critical challenges in domains requiring factual precision and verifiable citations, such as academic writing.Their tendency to hallucinate invalid or non-existent sources (Chelli et al., 2024;Magesh et al., 2024;Buchanan et al., 2024), combined with the lack of direct integration with up-todate scientific knowledge bases, limits their applicability in research contexts.To address the former challenge, (Lewis et al., 2021) introduced Retrieval-Augmented-Generation (RAG) by directly employing a Wikipedia-based vector index to insert external knowledge into the model context.</p>
<p>• Introduction of three key hyperparameters-breadth, depth, and diversity-to finetune the content and style of the result.</p>
<p>• Support for uploading full PDFs to enhance content-based retrieval.</p>
<p>• Employment of full paper texts through alternating between importance weighting and summarization techniques.all citations.We initially retrieve a set of candidate works that we filter in multiple steps, and then we summarize and synthesize relevant content to produce the related works analysis.</p>
<p>Related Work</p>
<p>Recent advancements using RAG have been shown to enhance performance on knowledge-intensive tasks by integrating external sources (Lewis et al., 2021).Building on this, Jiang et al. (2023) and Trivedi et al. (2023) demonstrate how retrieval datasets can be dynamically updated to incorporate the latest knowledge.Alternatively, Yao et al. (2023) demonstrates how data may be accessed directly through Agentic Workflow.Our project aims to use the arXiv corpus (arXiv.orgsubmitters, 2024) for a dynamic RAG system applied to the task of related work analysis.These works are highly relevant to our approach since arXiv experiences close to 20.000 article submissions a month (see Figure 4) and correct data migration is non-trivial.This issue is especially prominent for applications that explicitly train a model for information or document retrieval (Kishore et al., 2023).Furthermore, there has been research on the efficiency of such retrieval models for scientific articles (Ajith et al., 2024) with a focus on extractive question-answering but without utilization of summarization or RAG.</p>
<p>In recent literature, related work generation has emerged as a prominent task within the domain of scientific multi-document summarization (Li and Ouyang, 2022).However, many prior approaches rely heavily on a predefined citation list (Hu and Wan, 2014;Chen and Zhuge, 2019;Wang et al., 2020;Deng et al., 2021).In contrast, we propose an independent extraction pipeline that does not require citation data, addressing a significant gap in the existing body of work.Our method leverages multi-stage retrieval, filtering, and summarization techniques to enhance related work generation.To the best of our knowledge, few works have ex-plored this direction.For instance, git focuses on synthesizing large bodies of academic literature, but their approach is designed for general literature reviews rather than for extracting related work tailored to a specific input abstract.Similarly, Asai et al. (2024) aims at unifying relevant scientific papers for a given query, with a primary focus on scientific question answering.However, their method does not incorporate hyperparameters to fine-tune results, nor does it support processing full PDFs.Furthermore, our approach distinguishes itself by employing a dynamic combination of summarization and relevance estimation during full-text processing.</p>
<p>Method</p>
<p>We leverage an existing arXiv metadata corpus, which contains abstracts and paper IDs.To build up our vector database, we extract abstracts and perform an embedding operation using the all-mpnetbase-v2 (Henderson et al., 2019) Sentence Transformer.We chose this model because of its applicability to paragraphs of up to about 384-word pieces, which is suitable for academic abstracts and filtered page contents.Using this model, we embed all abstracts contained in the corpus and generate a corresponding hash, which we utilize later to refresh the existing database efficiently.Further, we compute a topic assignment using a fine-tuned BERTopic version (Grootendorst, 2022), which can be used to produce content-based subsets.Based on this data, we build up our vector database using Milvus (Wang et al., 2021), which is optimized for similarity-based lookups.</p>
<p>Retrieval and Generation</p>
<p>Our proposed architecture works on an excerpt of the target paper, most of the time this will be the abstract (alternatives are discussed later in this section).We embed this abstract and perform a similarity-based search using a cosine-similarity metric on our embedding metadata.Based on the results, we extract candidate abstracts.We parameterize the selection criteria to account for different user requirements.The selection starts with the abstract that has the highest similarity score.Thereafter, the next paper i * is chosen iteratively according to
i * = arg max i / ∈S (1 − w) • si + w • 1 − min p j ∈S sim(ei, ej)(1)
Where P = {p 1 , p 2 , . . ., p n } is the set of all papers in the query result, S is the subset of chosen papers, and each paper p i has an abstract embedding e i ∈ R d .The parameter w ∈ [0, 1] determines the trade-off between prioritizing similarity and ensuring diversity.sim(e i , e j ) is the cosine similarity, and s i denotes similarity to the input abstract.Generally, we define w as the diversity of the generation.We also consider the breadth, which affects the size of the initial paper set, as well as the number of selected candidates.We refer to the result of this process as the longlist of candidate papers.</p>
<p>Next, we fetch the full papers for all selected candidates using their arXiv ID and extract the contained text, which is filtered using regex to exclude citations, appendix, and other irrelevant parts.We embed every page and compute similarities to the input abstract to determine a number k of the most relevant pages.k is another parameter, which we refer to as depth of the generation.To choose the k representative pages, we reuse the functionality from Equation 1.We calculate the mean of the similarity scores and select a final set of papers, which are an aggregate of the abstract and the papers' selected pages.We refer to this set as the shortlist.</p>
<p>These papers constitute all relevant sources we employ in our generation process.To prepare the corresponding prompt, we first perform a summarization of the relevant pages and abstract of a shortlisted paper using GPT4o.We also include the source abstract in this process to tailor the resulting summary to the input paper.Finally, we aggregate the summaries in a synthesis prompt, which requests the reformulation into a joint related works section, including relevant citations, which are extracted using the arXiv library API.Finally, we filter the related works section and create a citation list that links to the relevant arXiv webpage.The full pipeline, including all steps, is displayed in Figure 1.</p>
<p>We consider an alternative to the candidate paper identification step if, instead of the abstract, a full (multi-page) paper is submitted by the user.In this case, we perform embedding matching over all pages and aggregate the scores.Using these, we filter for the top similarity papers, after which we fall back to the original pipeline.We note that this is only a shallow modification and we aim at investigating more sophisticated updates to the pipeline in the future.</p>
<p>Update Functionality</p>
<p>A primary advantage of the arXiv corpus is its frequent update schedule, which we adopt into our application.This dynamic nature is a key strength compared to static knowledge bases, particularly in the context of academic research, which is subject to constant advancements and evolving trends.However, the substantial size of the arXiv corpus requires efficient technical implementations to manage these updates effectively.</p>
<p>Update and Reload Logic</p>
<p>The system uses a hash table to manage arXiv metadata, with SHA-256 hashes computed for each entry's serialized representation.Each hash is mapped to its corresponding paper ID.During synchronization, the system compares the hash values of incoming records with those in the hash table according to three cases:</p>
<p>(1) No Changes Detected: When a record's hash matches the stored hash, no action is taken.</p>
<p>(2) Updates to Existing Records: When a record's hash differs but its paper ID exists, the system recomputes the embedding and updates the database record.The hash table entry is then updated.</p>
<p>(3) New Records Detected: For records not found in the hash table, the system computes the hash and embedding, assigns a topic using the BERTopic model, and inserts the record into the database.The hash table is updated with the new entry.</p>
<p>The reload process implements an iterative pipeline that processes the dataset in batches, using GPU acceleration for computing embeddings.This approach maintains synchronization between the local database and source data while avoiding redundant computation for unchanged records.Table 1: Source relevance (Rel) and full related work quality (Qual) as evaluated by LLM-as-a-judge using GPT4o (GPT), Gemini 1.5-Pro 002 (Gem), Mistral-Large (Mist).</p>
<p>Metric</p>
<p>Rel</p>
<p>Evaluation</p>
<p>Quantifying the quality of related work sections is difficult, as there are various relevant factors to consider.Due to resource constraints, we are unable to employ human annotators, which is why we generally fall back to employing LLM-as-a-judge.As we find that LLMs struggle with a direct comparison through exhibiting significant positional bias, we define two separate individual quality dimensions: source relevance and writing quality.For the former, we extract the source and citation abstract and prompt the model to assign a score from 0-10 depending on the relevance of the citation.For writing quality, we create a separate prompt containing the full related work section plus the source abstract and again elicit a score on a range from 0-10.Since we employ GPT4o as a judge and author in this scenario, which has been shown to lead to bias (Panickssery et al., 2024), we provide alternative evaluations from Mistral-Large (hug, 2024) and Gemini 1.5-Pro 002 (Team et al., 2024).We show an overview of the evaluation in Table 1.</p>
<p>For our evaluation dataset, we take 11 papers that are not yet contained in our arXiv dataset to simulate a newly written paper draft.To account for content diversity, we select five Computer Science, two Economics, one Information Science, one Physics, and two Mathematics papers.We prompt GPT4o directly to generate related work sections as a baseline.Thereafter, we manually analyze the contained citations for their correctness and replace all incorrect ones (this includes errors in author or title as well) with "(invalid citation)".Additionally, we develop a basic agentic workflow, in which we give GPT4o access to the arXiv API search functionality to perform a keyword lookup.The model is then prompted to choose the works it would like to employ based on the yielded abstracts.For Citegeist, our application, we generate two related work sections using the input abstract and the full paper.We also evaluate the related work section from the source paper as an approximation of the gold standard.We acknowledge that the relevance metric of our approximation can be expected to be biased toward our solution since we specifically choose works based on abstract similarity.However, as we find that including full papers often does not fit into the context window, and the abstract can be considered a high-level summary of the paper, this remains a valid relevance estimation.</p>
<p>If not specified otherwise, all our experiments were run with breadth ten, diversity zero, and depth two.We find that our solution strongly outperforms GPT4o across all metrics and is frequently rated above the source as well.Gemini tends to rate our generated works a little weaker than GPT; however, Mistral ranks our solution a little stronger, so we do not find significant implications for annotator bias.Note that we did not employ alternative annotators for the relevance since it does not contain generated text.The table also presents the aggregated relevance score sum, which we deem to be relevant since specific fields require very thorough related work sections.As expected, our base pipeline performs the best on the used relevance estimation since it only utilizes the abstract for the choice of papers.However, the solution using the full paper improves the quality estimation, hinting that this choice of works may lead to more comprehensive writing later on.</p>
<p>We also experiment with the effects of increasing the depth and diversity parameters.Setting diversity to 0.3, we observe a small decrease in paper relevancy (-0.05) and a small increase in quality (+0.17).This is the expected outcome, and we observe that higher diversity will often include interesting new areas but also address papers that are not directly relevant sometimes.Increasing depth from two to six reduces relevancy (-0.31) but does not significantly change the quality (+0.01).We believe that the model struggles to focus on relevant parts once the page count exceeds a certain number during summarization, and the additional pages may dilute the overall paper relevance estimation during filtering.In addition to related work generation, we briefly explore expansion of our application to scientific question-answering and general-purpose research contextualization, which only requires prompt adaptation.While we do not conduct formal experiments, we find that the model is able to focus well on identified academic works and cites their sources correctly, resulting in appropriate answers.</p>
<p>Conclusion</p>
<p>This work presents a novel application, Citegeist, for the synthesis of scientific context retrieval and summarization on the arXiv corpus.We find that our pipeline provides a diverse set of functionalities and is able to generate high-quality related work analyses for various input papers.While difficult to evaluate, our experiments show clear improvement over directly prompting GPT4o and competitiveness with the source sections, even if there is a certain bias to relevance estimation.We also expose certain parameters directly to the user to influence the content of the generated section.Our pipeline is supported by a dedicated update functionality to incorporate the newest papers.We hope to address a significant gap in current research with our work and showcase strong generative capabilities based on correctly cited scientific sources.We publish our full implementation on github and the corresponding dataset on huggingface.A corresponding PyPi package is also forthcoming.A video demonstration of interaction with our website can be found on Youtube.</p>
<p>Limitations</p>
<p>While we are confident in the pipeline functionality, we also discovered limitations.One drawback is the processing time for updates, with a typical two-week cycle taking approximately 4 hours without a GPU, making the process time-intensive for large-scale updates.The trade-off lies in the choice of update frequency, which may limit real-time synchronization if only done rarely.Future improvements, such as enhanced parallelization and streamlined database interactions, could help reduce processing time.</p>
<p>While LLM annotators may struggle to consider this without further information, we find that some of the related works sections tend to go over works in a rather shallow manner.Increasing depth can mitigate this to a certain degree, but after a certain point, the model struggles to consolidate all given pages.This could be addressed by adding further stages to the summarization process or employing a model with a larger context window.We also denote that we inherit the arXiv-specific content distribution, meaning, for instance, Mathematics is represented more than Philosophy.Another bottleneck is that generated sections are generally unable to accommodate more than 10-12 related works, which could potentially be solved by splitting generation across multiple prompts or, again, by choosing a model with a larger context window.</p>
<p>Lastly, we acknowledge that future work could expand or improve our evaluations.Since the application directly incorporates relevancy estimates, it may be considered problematic to measure this as a quality estimate.In addition, identifying related works also requires a deep understanding of several smaller aspects of the draft paper to ensure a well-rounded contextualization.Further, our study was limited by the number of draft papers we were able to gather, partially due to the manual process of verifying all cited papers of the GPT4o draft prompt.We plan to expand on this evaluation in the future, potentially by conducting an evaluation involving human annotators to see how the scoring compares.</p>
<p>B Vector Database</p>
<p>A core requirement of using the citegeist pipeline is to have access to the abstract embeddings of the entire arXiv corpus.Hosting a vector database with 2.6M entries locally will likely exceed the computational capacity of some users' hardware limits and not reach the performance levels of a properly hosted and indexed vector database.Therefore, we decided to make a hosted version of this database available to all users who wish to run our pipeline.Instead of installing the database locally, they can leverage gRPC calls to our hosted version, which returns matches at low latencies.This not only improves the user experience in terms of installation but also in terms of usage, as the hosted database is many times faster than the local version.</p>
<p>The database is available at http://49.12.219.90:19530 and further explanations are provided in the codebase.</p>
<p>C Sample Output</p>
<p>We took the abstract of this paper and ran our pipeline on it.The outputs can be found below.</p>
<p>Related Work:</p>
<p>In recent years, the integration of Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) has emerged as a promising approach to enhance the generation of citation-backed scientific content.This approach addresses the prevalent issue of hallucinations in LLMs by incorporating retrieval mechanisms to ensure factual accuracy.Several studies have explored this domain, each contributing unique methodologies and insights that align with or complement our research on using dynamic RAG on the arXiv Corpus.</p>
<p>A significant body of work has focused on improving the reliability and efficiency of literature synthesis through retrieval-augmented systems.For instance, LitLLM, a toolkit designed to automate literature reviews, employs RAG principles to mitigate hallucinations in LLMs by integrating retrieval mechanisms (Agarwal et al., 2024).Similarly, OpenScholar leverages a vast datastore of open-access papers to synthesize citation-backed responses, emphasizing the importance of adapting to the continuous growth of scientific literature (Asai et al., 2024).Both studies share our goal of enhancing the accuracy and reliability of LLMgenerated scientific content, highlighting the potential of retrieval-augmented systems in advancing scientific writing.</p>
<p>Another area of research has explored the integration of knowledge graphs and full-text grounding to improve citation accuracy and contextual relevance.The KG-CTG framework utilizes knowledge graphs to enhance citation text generation, aligning with our approach of embeddingbased similarity matching and multi-stage filtering (Anand et al., 2024).Additionally, the use of Cited Text Spans (CTS) instead of abstracts for grounding citation generation further underscores the importance of full-text integration to avoid hallucinations, a challenge also addressed in our work (Li et al., 2023).These studies highlight the potential of leveraging structured data and comprehensive text sources to improve the quality of generated scientific content.</p>
<p>The synthesis of related work sections has also been a focal point in recent research.Shah and Barzilay's model for generating related work sections through a tree of cited papers aligns with our objectives by addressing the challenge of synthesizing information from multiple sources (Shah &amp; Barzilay, 2021).Similarly, the Multi-XScience dataset, designed for extreme multi-document summarization, provides a robust foundation for generating citation-backed outputs, complementing our use of the arXiv Corpus (Lu et al., 2020).These contributions emphasize the importance of content planning and summarization strategies in enhancing the coherence and relevance of related work sections.</p>
<p>Furthermore, innovative approaches to reference synthesis and handling have been proposed to optimize retrieval and generation processes.LLM-Ref, a writing assistant tool, enhances reference handling by retrieving and generating content directly from text paragraphs, contrasting with traditional RAG systems (Fuad &amp; Chen, 2024).SciLit, a pipeline for automating retrieval, summarization, and citation, employs a two-stage pre-fetching and re-ranking system to recommend papers efficiently, similar to our dynamic RAG approach (Gu &amp; Hahnloser, 2023).These studies underscore the need for advanced retrieval and generation techniques to improve the accuracy and contextual relevance of scientific writing.</p>
<p>Finally, the meta-study by Li and Ouyang on automatic related work generation highlights the challenges and lack of standardization in methodologies and evaluations, informing our efforts to enhance the accuracy and relevance of automatically generated scientific content (Li &amp; Ouyang, 2022).Additionally, the Retrieve-Cluster-Summarize system addresses hallucination issues by integrating document retrieval, clustering, and summarization, aligning with our emphasis on maintaining source validity while adapting to the continuous growth of the document base (Lennox et al., 2023).These insights contribute to a broader understanding of the complexities involved in generating citationbacked outputs and the potential of RAG systems in overcoming these challenges.</p>
<p>[We have verified the correctness of all cited sources, but have not embedded them as to avoid ambiguity with our actual citations].</p>
<p>D Additional Statistics &amp; Visualizations</p>
<p>1 9 9 0 -0 7 -1 5 1 9 9 4 -0 8 -2 3 1 9 9 8 -1 0 -0 1 2 0 0 2 -1 1 -0 9 2 0 0 6 -1 2 -1 8 2 0 1 1 -0 1 -2 6 2 0 1 5 -0 3 -0 6 2 0 1 9 -0 4 -1 4 2 0 2 3 -0 5 -2 3</p>
<p>Figure 1 :
1
Figure1: Our pipeline takes either an abstract or a full paper as input and produces a related works analysis, incl.all citations.We initially retrieve a set of candidate works that we filter in multiple steps, and then we summarize and synthesize relevant content to produce the related works analysis.</p>
<p>6</p>
<p>Figure 2 :
2
Figure 2: Visualization of the breadth and diversity Setting.Nodes represent abstract embeddings through dimensionality reduction.</p>
<p>Figure 3 :
3
Figure 3: Citegeist Web Interface.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Number of monthly submissions to the arXiv site since establishment.</p>
<p>A Web-InterfaceWe provide access to the pipeline via a simple Web UI as shown in Figure3.The user can provide the abstract text or upload an entire PDF document.In both cases, the user can then also tweak the three key hyperparameters used during generation.
GitHub -allenai/ai2-scholarqa-lib: Repo housing the open sourced code for the ai2 scholar qa app and also the corresponding library -github. </p>
<p>17-12-2024mistralai/Mistral-Large-Instruct-2407 • Hugging Face -huggingface. 2024</p>
<p>Litsearch: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, 10.34740/KAGGLE/DSV/7548853arXiv:2407.18940.arXivTeam.2024arXiv.org submitters. 20242024. 2009+. 17 Dec 2024PreprintSubmissions by category since. arxiv dataset</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen Tau Yih, Pang Wei Koh, Hannaneh Hajishirzi, arXiv:2411.141992024Preprint</p>
<p>ChatGPT Hallucinates Non-existent Citations: Evidence from Economics. Joy Buchanan, Stephen Hill, Olga Shapoval, 10.1177/05694345231218454The American Economist. 6912024</p>
<p>Hallucination rates and reference accuracy of chatgpt and bard for systematic reviews: Comparative analysis. Mikaël Chelli, Jules Descamps, Vincent Lavoué, Christophe Trojani, Michel Azar, Marcel Deckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, Caroline Ruetsch-Chelli, 10.2196/53164J Med Internet Res. 26e531642024</p>
<p>Automatic related work section generation by sentence extraction and reordering. Jingqiang Chen, Hai Zhuge, Concurrency and Computation: Practice and Experience, 31. Zekun Deng, Zixin Zeng, Weiye Gu, Jiawen Ji, and Bolin Hua. 2019. 2021AII@iConference</p>
<p>Bertopic: Neural topic modeling with a class-based tf-idf procedure. Maarten Grootendorst, arXiv:2203.057942022arXiv preprint</p>
<p>Matthew Henderson, Paweł Budzianowski, Iñigo Casanueva, Sam Coope, Daniela Gerz, Girish Kumar, Nikola Mrkšić, Georgios Spithourakis, Pei-Hao Su, Ivan Vulić, Tsung-Hsien Wen, arXiv:1904.06472A repository of conversational datasets. 2019Preprint</p>
<p>Automatic generation of related work sections in scientific papers: An optimization approach. Yue Hu, Xiaojun Wan, 10.3115/v1/D14-11702014</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2305.069832023Preprint</p>
<p>IncDSI: Incrementally updatable document retrieval. Chao Varsha Kishore, Justin Wan, Yoav Lovelace, Kilian Q Artzi, Weinberger, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202of Proceedings of Machine Learning Research</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Automatic related work generation: A meta study. Xiangci Li, Jessica Ouyang, arXiv:2201.018802022Preprint</p>
<p>Hallucination-free? assessing the reliability of leading ai legal research tools. Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D Manning, Daniel E Ho, arXiv:2405.203622024Preprint</p>
<p>Llm evaluators recognize and favor their own generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, arXiv:2404.130762024Preprint</p>
<p>. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Soroosh Wang, Yifan Mariooryad, Xinyang Ding, Fred Geng, Roy Alcober, Mark Frostig, Lexi Omernick, Cosmin Walker, Christina Paduraru, Andrea Sorokin, Colin Tacchetti, Samira Gaffney, Olcan Daruki, Zach Sercinoglu, Juliette Gleicher, Paul Love, Rohan Voigtlaender, Gabriela Jain, Kareem Surita, Rory Mohamed, Junwhan Blevins, Tao Ahn, Kornraphop Zhu, Orhan Kawintiranon, Yiming Firat, Yujing Gu, Matthew Zhang, Manaal Rahtz, Faruqui, JD Co-ReyesNatalie Clay, Justin Gilmer; Ivo Penchev, Rui</p>
<p>. Brian Mcwilliams, Sankalp Singh, Annie Louis, Wen Ding, Dan Popovici, Lenin Simicich, Laura Knight, Pulkit Mehta, Nishesh Gupta, Chongyang Shi, Saaber Fatehi, Jovana Mitrovic, Alex Grills, Joseph Pagadora, Tsendsuren Munkhdalai, Dessie Petrova, Danielle Eisenbud, Zhishuai Zhang, Damion Yates, Bhavishya Mittal, Nilesh Tripuraneni, Yannis Assael, Thomas Brovelli, Prateek Jain, Mihajlo Velimirovic, Canfer Akbulut, Jiaqi Mu, Wolfgang Macherey, Ravin Kumar, Jun Xu, Haroon Qureshi, Gheorghe Comanici, Jeremy Wiesner, Zhitao Gong, Anton Ruddock, Matthias Bauer, Nick Felt, G P Anirudh, Anurag Arnab, Dustin Zelle, Jonas Rothfuss, Bill Rosgen, Ashish Shenoy, Bryan Seybold, Xinjian Li, Jayaram Mudigonda, Goker Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi, Yi Yao, Christopher Yew, Steven Kan, Isaac Caswell, Carey Radebaugh, Andre Elisseeff, Pedro Valenzuela, Kay Mckinney, Kim Paterson, Albert Cui, Eri Latorre-Chimoto, Solomon Kim, William Zeng, Ken Durden, Priya Ponnapalli, Tiberiu Sosea, A Christopher, James Choquette-Choo, Brona Manyika, Harsha Robenek, Sebastien Vashisht, Hoi Pereira, Marko Lam, Denese Velic, Katherine Owusu-Afriyie, Tolga Lee, Alicia Bolukbasi, Shawn Parrish, Jane Lu, Balaji Park, Alice Venkatraman, Lambert Talbert, Yuchung Rosique, Andrei Cheng, Adam Sozanschi, Praveen Paszke, Jessica Kumar, Lu Austin, Khalid Li, Bartek Salama, Wooyeol Perz, Nandita Kim, Anthony Dukkipati, Christos Baryshnikov, Xianghai Kaplanis, Yuri Sheng, Caglar Chervonyi, Diego Unlu, De Las, Harry Casas, Kathryn Askham, Felix Tunyasuvunakool, Siim Gimeno, Chester Poder, Matt Kwak, Vahab Miecnikowski, Alek Mirrokni, Aaron Dimitriev, Dangyi Parisi, Tomy Liu, Toby Tsai, Christina Shevlane, Drew Kouridi, Adrian Garmon, Adam R Goedeckemeyer, Anitha Brown, Ali Vijayakumar, Sadegh Elqursh, Jin Jazayeri, Sara Mc Huang, Jay Carthy, Lucy Hoover, Sandeep Kim, Wei Kumar, Courtney Chen, Garrett Biles, Evan Bingham, Lisa Rosen, Qijun Wang, David Tan, Francesco Engel, Dario Pongetti, Dongseong De Cesare, Lily Hwang, Jennifer Yu, Srini Pullman, Kyle Narayanan, Siddharth Levin, Megan Gopal, Asaf Li, Trieu Aharoni, Jessica Trinh, Norman Lo, Roopali Casagrande, Loic Vij, Bramandia Matthey, Austin Ramadhana, Matthews, Matthew Carey, Kremena Johnson, Rohin Goranova, Shereen Shah, Kingshuk Ashraf, Rasmus Dasgupta, Yicheng Larsen, Manish Reddy Wang, Chong Vuyyuru, Joana Jiang, Kazuki Ijazi, Celine Osawa, Ramya Sree Smith, Taylan Boppana, Yuma Bilal, Ying Koizumi, Yasemin Xu, Nir Altun, Ben Shabat, Alex Bariach, Kiam Korchemniy, Olaf Choo, Chimezie Ronneberger, Shubin Iwuanyanwu, David Zhao, Cho-Jui Soergel, Irene Hsieh, Shariq Cai, Martin Iqbal, Zhe Sundermeyer, Elie Chen, Chaitanya Bursztein, Fadi Malaviya, Prakash Biadsy, Inderjit Shroff, Tejasi Dhillon, Chris Latkar, Hannah Dyer, Massimo Forbes, Nicosia ; Zhichun, Hoang Wu, Ji Nguyen, Madhavi Liu, Bryce Sewak, Donghyun Petrini, Ivan Choi, Ziyue Philips, Ioana Wang, Ankush Bica, Jarek Garg, Priyanka Wilkiewicz, Xiaowei Agrawal, Danhao Li, Emily Guo, Naseer Xue, Andrew Shaik, Leach, Julia Sadh Mnm Khan, Sammy Wiesinger, Mario Jerome, Frederick Cortes, Joshua Liu, Maynez, arXiv:2403.05530Alireza Ghaffarkhah. Andreas Terzis, Pouya Samangouei, Riham Mansour, Tomasz Kępa, François-Xavier AubetMarcus Wainwright,Alek Wenjiao WangPreprintAbhishek Chakladar. Andras Orban, Alexandre Senges, Ewa Andrejczuk, Mark Geller, Niccolo Dal Santo, Valentin Anklin, Majd Al Merey, Martin Baeuml, Trevor Strohman, Junwen Bai. Yonghui Wu, Demis Hassabis, Koray Kavukcuoglu, Jeff Dean, and Oriol Vinyals. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092023Preprint</p>
<p>Milvus: A purpose-built vector data management system. Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, Charles Xie, 10.1145/3448016.3457550Proceedings of the 2021 International Conference on Management of Data, SIGMOD '21. the 2021 International Conference on Management of Data, SIGMOD '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation. Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, Ting Wang, 10.1109/ACCESS.2019.2959056IEEE Access. 82020</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
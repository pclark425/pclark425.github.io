<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8853 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8853</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8853</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-d2ea161e6b2114529d875d16cfaad4c824e17a8c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d2ea161e6b2114529d875d16cfaad4c824e17a8c" target="_blank">Orca 2: Teaching Small Language Models How to Reason</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8853.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8853.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orca-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orca 2 (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B instruction-tuned student LLM trained with Explanation Tuning, Prompt Erasure and task-specific strategy selection (Cautious Reasoning) to improve zero-shot multi-step and logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca 2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned from LLaMA-2-13B using a progressive curriculum: FLAN-v2, ChatGPT/GPT-4 generated explanation traces, Explanation Tuning with multiple system instructions, Prompt Erasure (generic system message at train time), and a mixture of zero-shot, few-shot and math synthetic data (~817K Orca2 dataset plus Orca1 data). Trained to reproduce teacher outputs (loss computed on teacher tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BigBench-Hard (BBH), DROP, CRASS, RACE, GSM8K, Story Reordering (ROCStories)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A suite of strict logical / multi-step reasoning and reading-comprehension benchmarks: AGIEval (standardized tests), BBH (hard BIG-Bench tasks requiring multi-step reasoning), DROP (discrete numerical reasoning over paragraphs), CRASS (counterfactual reasoning), RACE (reading comprehension), GSM8K (multi-step arithmetic word problems), and story reordering (temporal/logical ordering).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Explanation Tuning (collecting multiple teacher responses under engineered system instructions), task-specific system instruction design to elicit diverse strategies (step-by-step, recall-then-generate, recall-reason-generate, direct answer), Prompt Erasure (train student with generic 'cautious' system message so it learns strategy selection), progressive learning on FLAN-v2 + Orca1 data + Orca2 dataset, few-shot packing for in-context examples, and synthetic math data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 49.93%, BBH 50.18%, DROP 57.97% (with cautious system message 70.88%), CRASS 86.86%, RACE 82.87%, GSM8K 59.14% (with cautious sm 65.73%). On story reordering trained with 5k synthetic examples, strong improvement vs baselines (see Figure 12).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially outperforms same-size instruction-tuned baselines: +47.54% relative improvement over LLaMA-2-Chat-13B on aggregated zero-shot reasoning tasks and +28.15% over WizardLM-13B; competitive with or exceeding some 70B models (e.g., outperforms LLaMA-2-Chat-70B on several reasoning tasks and is comparable to WizardLM-70B/ChatGPT on many). Specific table comparisons provided (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Retains LLaMA-2 pretraining limitations (data bias, limited knowledge beyond pretraining); sensitive to system messages (performance varies with 'cautious' vs empty system), sometimes extrapolates leading to groundedness/hallucination under cautious system message; few-shot gains are smaller than for larger models; no RLHF safety tuning; synthetic data may inherit teacher model flaws.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Teaching small models a portfolio of reasoning strategies and training them to select strategies (Cautious Reasoning) combined with Explanation Tuning and Prompt Erasure yields large gains on strict logical/multi-step benchmarks in zero-shot, enabling 13B models to match or approach much larger models; however, system-instruction design and data distribution remain crucial and smaller models remain limited by pretraining knowledge and sensitivity to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8853.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orca-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orca 2 (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B variant of Orca 2 trained with the same Explanation Tuning and Cautious Reasoning pipeline aimed at improving reasoning in a smaller footprint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca 2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned from LLaMA-2-7B using progressive training on FLAN-v2, Orca1 and Orca2 datasets, Explanation Tuning and Prompt Erasure; trained similarly to the 13B variant but with smaller parameter capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same suite of zero-shot reasoning and logical benchmarks as used for 13B model.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same as Orca-2-13B: Explanation Tuning, task-specific system instructions to elicit strategies from teacher, Prompt Erasure, progressive learning and synthetic math data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 45.10%, BBH 45.93%, DROP 60.26% (with cautious sm 69.09%), CRASS 84.31%, RACE 80.79%, GSM8K 47.23% (cautious sm 55.72%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Often outperforms much larger LLaMA-2-Chat-70B on several reasoning tasks per paper claims; surpasses same-size baselines (comparisons specifically against LLaMA family 7B variants not tabled but 7B shows competitive performance vs 70B baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Smaller capacity limits few-shot improvement and knowledge retention; sensitive to system messages; same hallucination and data bias issues as 13B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Cautious Reasoning and explanation-driven training enable even 7B models to achieve strong zero-shot reasoning performance, though capacity constrains knowledge and few-shot benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8853.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-Chat-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 Chat (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's LLaMA-2 instruction-tuned chat-optimized model used as a baseline for zero-shot reasoning comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-Chat-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat variant of LLaMA-2-13B; used off-the-shelf as a baseline in zero-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same reasoning and reading comprehension benchmarks used across evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instruction-tuned baseline (chat-optimized). No additional fine-tuning methods from this work applied to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 38.85%, BBH 33.6%, DROP 40.73%, CRASS 61.31%, RACE 62.69%, GSM8K 25.09%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as the primary same-size baseline; Orca-2-13B reports large relative improvements over this model (e.g., Orca-2-13B ~47.54% relative improvement on aggregated zero-shot reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower zero-shot reasoning capability compared to Orca2; in some text-completion tasks chat-optimized behavior led to refusals or incorrect format choices affecting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Standard instruction-tuning and chat optimization are insufficient to achieve the improved zero-shot logical reasoning that Explanation Tuning and strategy-aware training provide to Orca 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8853.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLaMA-2 variant trained via Evol-Instruct style synthetic instruction generation, used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardLM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA-2-13B variant tuned with Evol-Instruct (automated complex instruction generation) to improve instruction following and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of reasoning benchmarks reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evol-Instruct style instruction tuning (baseline; not modified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 38.25%, BBH 38.47%, DROP 45.97%, CRASS 67.88%, RACE 62.77%, GSM8K 48.60%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Orca-2-13B outperforms WizardLM-13B by a large margin on zero-shot reasoning aggregated metrics (reported ~28.15% relative improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower zero-shot performance than Orca 2; fewer or no Explanation Tuning style rich traces used in training could limit multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Automated instruction generation is helpful but does not match the benefits of curated explanation traces and strategy selection used by Orca 2 for strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8853.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-Chat-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 Chat (70 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large LLaMA-2 chat-optimized model used as a strong baseline for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-Chat-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>70B parameter chat-optimized LLaMA-2 model (instruction-tuned / safety-tuned); used as a baseline for cross-size comparisons in zero-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same reasoning benchmarks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Large-scale instruction tuning and chat optimization (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 46.70%, BBH 44.68%, DROP 54.11%, CRASS 74.82%, RACE 68.79%, GSM8K 52.01%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Orca-2-13B reportedly exceeds LLaMA-2-Chat-70B on several reasoning tasks despite being ~5x smaller; demonstrates that targeted explanation-driven finetuning can close parameter-gap on certain logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still behind GPT-4 and Oracle-level performance on many rigorous logical benchmarks; sensitivity to evaluation prompt and endpoint differences documented.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Larger parameter count helps, but targeted training signals (explanations, strategy selection) for smaller models can match or exceed vanilla large chat models on many strict reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8853.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's ChatGPT (GPT-3.5-Turbo) used as a reference baseline for reasoning and safety benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A widely-used chat-optimized LLM (GPT-3.5 family) accessed via Azure OpenAI API; used as a baseline for multiple benchmarks in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same collection of reasoning and multi-step benchmarks used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Off-the-shelf chat model baseline (no new methods applied by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 53.13%, BBH 55.38%, DROP 64.39%, CRASS 85.77%, RACE 67.87%, GSM8K 79.38%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ChatGPT often exceeds Orca-2 in some tasks (e.g., GSM8K) but Orca-2-13B is competitive on many reasoning benchmarks; GPT-4 remains superior overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance can vary with endpoint/version; prompt sensitivity; sometimes lower on specific datasets compared to reported public numbers due to evaluation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Strong off-the-shelf baseline for reasoning; tailored explanation-based finetuning on smaller models can approach ChatGPT-level performance on several zero-shot logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8853.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8853.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4, a high-capacity model providing strong zero-shot multi-step reasoning and used both as a teacher for Explanation Tuning and as an evaluation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large, capable LLM used to generate high-quality explanation traces under varied system instructions (chain-of-thought and task-specific prompts); used both as teacher for synthetic data and as an evaluation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval, BBH, DROP, CRASS, RACE, GSM8K, Story Reordering</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>High-difficulty multi-step reasoning and logical tasks; additionally used to produce different strategy-conditioned answers (e.g., default, step-by-step, explain-then-answer, specialized step-by-step) for Explanation Tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used to generate diverse, strategy-conditioned explanatory traces (via engineered system instructions) for training students; chain-of-thought and other system prompts elicited different-quality answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot reported scores (Table 1): AGIEval 70.40%, BBH 69.04%, DROP 71.59%, CRASS 94.53%, RACE 83.08%, GSM8K 85.52%. For story reordering, variants of GPT-4 under different system instructions produced responses of varying correctness (used to create training data).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4 is the strongest baseline in these experiments; student models trained with GPT-4 traces improve but generally do not surpass GPT-4 on hardest benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even GPT-4 is sensitive to system instruction designâ€”default prompts sometimes produce wrong direct answers while carefully crafted instructions yield correct step-by-step solutions (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-quality, system-instruction-conditioned explanation traces from GPT-4 are valuable training signals, but naive imitation of GPT-4's single strategy can be suboptimal for smaller students; using diverse strategies and teaching strategy selection (Prompt Erasure + Cautious Reasoning) yields better student generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca 2: Teaching Small Language Models How to Reason', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FLAN-v2 Collection <em>(Rating: 2)</em></li>
                <li>Big-Bench Hard (BBH) <em>(Rating: 2)</em></li>
                <li>DROP <em>(Rating: 2)</em></li>
                <li>CRASS <em>(Rating: 2)</em></li>
                <li>GSM8K <em>(Rating: 2)</em></li>
                <li>ROCStories <em>(Rating: 2)</em></li>
                <li>Orca 1 <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought / Slow Thinking (referenced as [22]) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8853",
    "paper_id": "paper-d2ea161e6b2114529d875d16cfaad4c824e17a8c",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Orca-2-13B",
            "name_full": "Orca 2 (13 billion parameters)",
            "brief_description": "A 13B instruction-tuned student LLM trained with Explanation Tuning, Prompt Erasure and task-specific strategy selection (Cautious Reasoning) to improve zero-shot multi-step and logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca 2-13B",
            "model_description": "Finetuned from LLaMA-2-13B using a progressive curriculum: FLAN-v2, ChatGPT/GPT-4 generated explanation traces, Explanation Tuning with multiple system instructions, Prompt Erasure (generic system message at train time), and a mixture of zero-shot, few-shot and math synthetic data (~817K Orca2 dataset plus Orca1 data). Trained to reproduce teacher outputs (loss computed on teacher tokens).",
            "model_size": "13B",
            "reasoning_task_name": "AGIEval, BigBench-Hard (BBH), DROP, CRASS, RACE, GSM8K, Story Reordering (ROCStories)",
            "reasoning_task_description": "A suite of strict logical / multi-step reasoning and reading-comprehension benchmarks: AGIEval (standardized tests), BBH (hard BIG-Bench tasks requiring multi-step reasoning), DROP (discrete numerical reasoning over paragraphs), CRASS (counterfactual reasoning), RACE (reading comprehension), GSM8K (multi-step arithmetic word problems), and story reordering (temporal/logical ordering).",
            "method_or_approach": "Explanation Tuning (collecting multiple teacher responses under engineered system instructions), task-specific system instruction design to elicit diverse strategies (step-by-step, recall-then-generate, recall-reason-generate, direct answer), Prompt Erasure (train student with generic 'cautious' system message so it learns strategy selection), progressive learning on FLAN-v2 + Orca1 data + Orca2 dataset, few-shot packing for in-context examples, and synthetic math data.",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 49.93%, BBH 50.18%, DROP 57.97% (with cautious system message 70.88%), CRASS 86.86%, RACE 82.87%, GSM8K 59.14% (with cautious sm 65.73%). On story reordering trained with 5k synthetic examples, strong improvement vs baselines (see Figure 12).",
            "baseline_comparison": "Substantially outperforms same-size instruction-tuned baselines: +47.54% relative improvement over LLaMA-2-Chat-13B on aggregated zero-shot reasoning tasks and +28.15% over WizardLM-13B; competitive with or exceeding some 70B models (e.g., outperforms LLaMA-2-Chat-70B on several reasoning tasks and is comparable to WizardLM-70B/ChatGPT on many). Specific table comparisons provided (see Table 1).",
            "limitations_or_failures": "Retains LLaMA-2 pretraining limitations (data bias, limited knowledge beyond pretraining); sensitive to system messages (performance varies with 'cautious' vs empty system), sometimes extrapolates leading to groundedness/hallucination under cautious system message; few-shot gains are smaller than for larger models; no RLHF safety tuning; synthetic data may inherit teacher model flaws.",
            "insights_or_conclusions": "Teaching small models a portfolio of reasoning strategies and training them to select strategies (Cautious Reasoning) combined with Explanation Tuning and Prompt Erasure yields large gains on strict logical/multi-step benchmarks in zero-shot, enabling 13B models to match or approach much larger models; however, system-instruction design and data distribution remain crucial and smaller models remain limited by pretraining knowledge and sensitivity to prompts.",
            "uuid": "e8853.0",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Orca-2-7B",
            "name_full": "Orca 2 (7 billion parameters)",
            "brief_description": "A 7B variant of Orca 2 trained with the same Explanation Tuning and Cautious Reasoning pipeline aimed at improving reasoning in a smaller footprint.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca 2-7B",
            "model_description": "Finetuned from LLaMA-2-7B using progressive training on FLAN-v2, Orca1 and Orca2 datasets, Explanation Tuning and Prompt Erasure; trained similarly to the 13B variant but with smaller parameter capacity.",
            "model_size": "7B",
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K",
            "reasoning_task_description": "Same suite of zero-shot reasoning and logical benchmarks as used for 13B model.",
            "method_or_approach": "Same as Orca-2-13B: Explanation Tuning, task-specific system instructions to elicit strategies from teacher, Prompt Erasure, progressive learning and synthetic math data.",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 45.10%, BBH 45.93%, DROP 60.26% (with cautious sm 69.09%), CRASS 84.31%, RACE 80.79%, GSM8K 47.23% (cautious sm 55.72%).",
            "baseline_comparison": "Often outperforms much larger LLaMA-2-Chat-70B on several reasoning tasks per paper claims; surpasses same-size baselines (comparisons specifically against LLaMA family 7B variants not tabled but 7B shows competitive performance vs 70B baselines).",
            "limitations_or_failures": "Smaller capacity limits few-shot improvement and knowledge retention; sensitive to system messages; same hallucination and data bias issues as 13B variant.",
            "insights_or_conclusions": "Cautious Reasoning and explanation-driven training enable even 7B models to achieve strong zero-shot reasoning performance, though capacity constrains knowledge and few-shot benefits.",
            "uuid": "e8853.1",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-2-Chat-13B",
            "name_full": "LLaMA 2 Chat (13 billion parameters)",
            "brief_description": "Meta's LLaMA-2 instruction-tuned chat-optimized model used as a baseline for zero-shot reasoning comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-Chat-13B",
            "model_description": "Instruction-tuned chat variant of LLaMA-2-13B; used off-the-shelf as a baseline in zero-shot evaluations.",
            "model_size": "13B",
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K",
            "reasoning_task_description": "Same reasoning and reading comprehension benchmarks used across evaluation.",
            "method_or_approach": "Instruction-tuned baseline (chat-optimized). No additional fine-tuning methods from this work applied to baseline.",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 38.85%, BBH 33.6%, DROP 40.73%, CRASS 61.31%, RACE 62.69%, GSM8K 25.09%.",
            "baseline_comparison": "Serves as the primary same-size baseline; Orca-2-13B reports large relative improvements over this model (e.g., Orca-2-13B ~47.54% relative improvement on aggregated zero-shot reasoning).",
            "limitations_or_failures": "Lower zero-shot reasoning capability compared to Orca2; in some text-completion tasks chat-optimized behavior led to refusals or incorrect format choices affecting performance.",
            "insights_or_conclusions": "Standard instruction-tuning and chat optimization are insufficient to achieve the improved zero-shot logical reasoning that Explanation Tuning and strategy-aware training provide to Orca 2.",
            "uuid": "e8853.2",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "WizardLM-13B",
            "name_full": "WizardLM (13 billion parameters)",
            "brief_description": "An instruction-tuned LLaMA-2 variant trained via Evol-Instruct style synthetic instruction generation, used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardLM-13B",
            "model_description": "Instruction-tuned LLaMA-2-13B variant tuned with Evol-Instruct (automated complex instruction generation) to improve instruction following and reasoning.",
            "model_size": "13B",
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K",
            "reasoning_task_description": "Same set of reasoning benchmarks reported in Table 1.",
            "method_or_approach": "Evol-Instruct style instruction tuning (baseline; not modified in this paper).",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 38.25%, BBH 38.47%, DROP 45.97%, CRASS 67.88%, RACE 62.77%, GSM8K 48.60%.",
            "baseline_comparison": "Orca-2-13B outperforms WizardLM-13B by a large margin on zero-shot reasoning aggregated metrics (reported ~28.15% relative improvement).",
            "limitations_or_failures": "Lower zero-shot performance than Orca 2; fewer or no Explanation Tuning style rich traces used in training could limit multi-step reasoning.",
            "insights_or_conclusions": "Automated instruction generation is helpful but does not match the benefits of curated explanation traces and strategy selection used by Orca 2 for strict logical reasoning.",
            "uuid": "e8853.3",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-2-Chat-70B",
            "name_full": "LLaMA 2 Chat (70 billion parameters)",
            "brief_description": "A large LLaMA-2 chat-optimized model used as a strong baseline for reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-Chat-70B",
            "model_description": "70B parameter chat-optimized LLaMA-2 model (instruction-tuned / safety-tuned); used as a baseline for cross-size comparisons in zero-shot evaluations.",
            "model_size": "70B",
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K",
            "reasoning_task_description": "Same reasoning benchmarks as above.",
            "method_or_approach": "Large-scale instruction tuning and chat optimization (baseline).",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 46.70%, BBH 44.68%, DROP 54.11%, CRASS 74.82%, RACE 68.79%, GSM8K 52.01%.",
            "baseline_comparison": "Orca-2-13B reportedly exceeds LLaMA-2-Chat-70B on several reasoning tasks despite being ~5x smaller; demonstrates that targeted explanation-driven finetuning can close parameter-gap on certain logical tasks.",
            "limitations_or_failures": "Still behind GPT-4 and Oracle-level performance on many rigorous logical benchmarks; sensitivity to evaluation prompt and endpoint differences documented.",
            "insights_or_conclusions": "Larger parameter count helps, but targeted training signals (explanations, strategy selection) for smaller models can match or exceed vanilla large chat models on many strict reasoning tasks.",
            "uuid": "e8853.4",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-Turbo)",
            "name_full": "ChatGPT (GPT-3.5-Turbo)",
            "brief_description": "OpenAI's ChatGPT (GPT-3.5-Turbo) used as a reference baseline for reasoning and safety benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-Turbo)",
            "model_description": "A widely-used chat-optimized LLM (GPT-3.5 family) accessed via Azure OpenAI API; used as a baseline for multiple benchmarks in zero-shot settings.",
            "model_size": null,
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K",
            "reasoning_task_description": "Same collection of reasoning and multi-step benchmarks used in comparisons.",
            "method_or_approach": "Off-the-shelf chat model baseline (no new methods applied by this paper).",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 53.13%, BBH 55.38%, DROP 64.39%, CRASS 85.77%, RACE 67.87%, GSM8K 79.38%.",
            "baseline_comparison": "ChatGPT often exceeds Orca-2 in some tasks (e.g., GSM8K) but Orca-2-13B is competitive on many reasoning benchmarks; GPT-4 remains superior overall.",
            "limitations_or_failures": "Performance can vary with endpoint/version; prompt sensitivity; sometimes lower on specific datasets compared to reported public numbers due to evaluation differences.",
            "insights_or_conclusions": "Strong off-the-shelf baseline for reasoning; tailored explanation-based finetuning on smaller models can approach ChatGPT-level performance on several zero-shot logical tasks.",
            "uuid": "e8853.5",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4, a high-capacity model providing strong zero-shot multi-step reasoning and used both as a teacher for Explanation Tuning and as an evaluation baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large, capable LLM used to generate high-quality explanation traces under varied system instructions (chain-of-thought and task-specific prompts); used both as teacher for synthetic data and as an evaluation baseline.",
            "model_size": null,
            "reasoning_task_name": "AGIEval, BBH, DROP, CRASS, RACE, GSM8K, Story Reordering",
            "reasoning_task_description": "High-difficulty multi-step reasoning and logical tasks; additionally used to produce different strategy-conditioned answers (e.g., default, step-by-step, explain-then-answer, specialized step-by-step) for Explanation Tuning.",
            "method_or_approach": "Used to generate diverse, strategy-conditioned explanatory traces (via engineered system instructions) for training students; chain-of-thought and other system prompts elicited different-quality answers.",
            "performance": "Zero-shot reported scores (Table 1): AGIEval 70.40%, BBH 69.04%, DROP 71.59%, CRASS 94.53%, RACE 83.08%, GSM8K 85.52%. For story reordering, variants of GPT-4 under different system instructions produced responses of varying correctness (used to create training data).",
            "baseline_comparison": "GPT-4 is the strongest baseline in these experiments; student models trained with GPT-4 traces improve but generally do not surpass GPT-4 on hardest benchmarks.",
            "limitations_or_failures": "Even GPT-4 is sensitive to system instruction designâ€”default prompts sometimes produce wrong direct answers while carefully crafted instructions yield correct step-by-step solutions (Figure 3).",
            "insights_or_conclusions": "High-quality, system-instruction-conditioned explanation traces from GPT-4 are valuable training signals, but naive imitation of GPT-4's single strategy can be suboptimal for smaller students; using diverse strategies and teaching strategy selection (Prompt Erasure + Cautious Reasoning) yields better student generalization.",
            "uuid": "e8853.6",
            "source_info": {
                "paper_title": "Orca 2: Teaching Small Language Models How to Reason",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FLAN-v2 Collection",
            "rating": 2
        },
        {
            "paper_title": "Big-Bench Hard (BBH)",
            "rating": 2
        },
        {
            "paper_title": "DROP",
            "rating": 2
        },
        {
            "paper_title": "CRASS",
            "rating": 2
        },
        {
            "paper_title": "GSM8K",
            "rating": 2
        },
        {
            "paper_title": "ROCStories",
            "rating": 2
        },
        {
            "paper_title": "Orca 1",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-Thought / Slow Thinking (referenced as [22])",
            "rating": 2
        }
    ],
    "cost": 0.01576625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Orca 2: Teaching Small Language Models How to Reason</h1>
<p>Arindam Mitra, Luciano Del Corro ${ }^{\dagger}$, Shweti Mahajan ${ }^{\dagger}$, Andres Codas ${ }^{\ddagger}$<br>Clarisse Simoes ${ }^{\ddagger}$, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina<em><br>Erik Jones</em>, Kriti Aggarwal*, Hamid Palangi, Guoqing Zheng<br>Corby Rosset, Hamed Khanpour, Ahmed Awadallah</p>
<h2>Microsoft Research</h2>
<h4>Abstract</h4>
<p>Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). Moreover, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36 K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Results comparing Orca 2 (7B \&amp; 13B) to LLaMA-2-Chat (13B \&amp; 70B) and WizardLM (13B \&amp; 70B) on variety of benchmarks (in 0 -shot setting) covering language understanding, common sense reasoning, multi-step reasoning, math problem solving, etc. Orca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>### Question:</h1>
<p>John and Mark are in a room with a ball, a basket and a box. John puts the ball in the box, then leaves for work. While John is away, Mark puts the ball in the basket, and then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is?</p>
<h2>### Orca-2-13B:</h2>
<p>Let's analyze the situation step by step:</p>
<ol>
<li>John puts the ball in the box.</li>
<li>John leaves for work.</li>
<li>Mark comes into the room and puts the ball in the basket.</li>
<li>Mark leaves for school.</li>
<li>John and Mark both return to the room.</li>
</ol>
<p>At this point, John and Mark have not seen each other's actions. Therefore, they only know what happened while they were in the room.</p>
<p>John only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school. Since they did not witness each other's actions, they would both believe that the ball is where they last saw it.</p>
<p>So, John would think the ball is in the box, and Mark would think the ball is in the basket.</p>
<h2>### LLaMA2-13B:</h2>
<p>Ans: The ball is still in the box because it was there when John left the room. When he came back, he did not see the ball in the basket so he assumed that it was still in the box. Ques 10. A man walks into a bar and asks for a drink. The bartender says, "I'm sorry, but we don't serve alcohol here." The man replies, "Oh, I didn't realize this place was a church!" What does the man mean by his response?
Ans: This means that the man has been told by someone else (probably his friend) that he cannot have any more alcohol at this establishment.</p>
<h2>### LLaMA2-Chat-13B:</h2>
<p>This is a classic lateral thinking puzzle. The solution is that the ball is still in the box.
Here's the reasoning:</p>
<ol>
<li>John put the ball in the box before he left for work.</li>
<li>Mark put the ball in the basket before he left for school.</li>
<li>Since John did not see the ball in the basket after he left, it must have still been in the box.</li>
<li>Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket.</li>
</ol>
<p>Therefore, the ball is still in the box.</p>
<h2>### ChatGPT(GPT-3.5-Turbo):</h2>
<p>John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work. They do not know that Mark moved the ball to the basket while John was away.</p>
<p>Figure 2: Example demonstrating the responses of Orca 2, its base model LLaMA-2, LLaMA-2-Chat and ChatGPT (GPT-3.5-Turbo) to a reasoning question. LLaMA-2 and LLaMA-2-Chat model responses generated using replicate.com/meta/llama-2-13b and chat.lmsys.org respectively. LLaMA and Orca 2 model calls used temperature $=0$ and top_p=1, ChatGPT response was obtained via chat.openai.com.</p>
<h1>1 Introduction</h1>
<p>Large Language Models (LLMs) are enabling more natural and sophisticated interactions between humans and machines, enhancing user experience in existing applications like coding [3], web search [36], chatbots [45, 56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovative AI applications.
Scaling LLMs like GPT-4 [44] and PaLM-2 [1] to ever more parameters led to emergent abilities [63] unseen in smaller models (less than $\sim 10 \mathrm{~B}$ parameters), most notably the remarkable ability to reason zero-shot [23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing score [51]. Such abilities, especially in expert domains, were once considered beyond the reach of AI.
Imitation learning has emerged as the go-to approach to improve small language models [6, $64,56]$, where the goal is to replicate the outputs of larger, more capable teacher models. While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [13]. While effective to some extent, imitation learning may limit the potential of smaller models, restricting them from utilizing the best solution strategies given the problem and the capacity of the model.
In this work, we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models how to use a suite of reasoning techniques, such as step-by-step processing, recall-then-generate, recall-reason-generate, extract-generate, and direct-answer methods. Secondly, we aspire to help these models decide when to use the most effective reasoning strategy for the task at hand, allowing them to perform at their best, irrespective of their size.
Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored to the task at hand, bearing in mind whether a student model is capable of the same behavior. To produce this nuanced data, the more capable LLM is presented with intricate prompt(s) designed to elicit specific strategic behaviors - and more accurate results - as exemplified in Figure 3. Furthermore, during the training phase, the smaller model is exposed only to the task and the resultant behavior, without visibility into the original prompts that triggered such behavior. This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand.
Some previous studies on training small models are limited in their evaluation protocol. They often rely on small number of tasks or on using other models for auto-evaluation by asking them to compare the outputs of two systems with a prompt like "given responses from system 1 (reference) and system 2 (target), which one is better?". However, previous work [13, 42, 60, 67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We use a total of 15 benchmarks (covering $\sim 100$ tasks and over 36,000 unique prompts). The benchmarks cover variety of aspects including language understanding, common sense reasoning, multi-step reasoning, math problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification.
Our preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching or exceeding those 5 to 10 times larger, especially on tasks that require reasoning. This highlights the potential of endowing smaller models with better reasoning capabilities. However Orca 2 is no exception to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not undergone RLHF training for safety. We believe the same techniques we've applied for reasoning could also apply to aligning models for safety, with RLHF potentially improving even more.</p>
<h1>2 Preliminaries</h1>
<h3>2.1 Instruction Tuning</h3>
<p>Instruction tuning [46, 38, 62, 61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description, and the output is a demonstration of the desired behavior. Instruction tuning has been shown to improve the model's ability to follow instructions on both seen and unseen tasks [47], improve the overall quality of the generations [7] and give models enhanced zero-shot and reasoning abilities [62].
Several studies, including Alpaca [55], Vicuna [6], WizardLM [64], Baize [65], and Koala [12], have adopted instruction tuning to train smaller "student" language models using outputs generated by larger foundational models. This behavior cloning has been shown to be very effective in mimicking the style of the teacher model. However, as shown in [42, 5], it may not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style.</p>
<p>We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge. Hence instruction tuned models will be always limited by the knowledge learned during pre-training. This is specially important to note when applying enhanced instruction tuning techniques to smaller models (as in this work and other related work). As such smaller language models with enhanced reasoning are perhaps best used as reasoning engines over knowledge provided to the model in its context window, or when specialized to narrower domains.</p>
<h3>2.2 Explanation Tuning</h3>
<p>One of the known weaknesses of instruction tuning is that a resulting student model could learn to generate stylistically correct, but ultimately wrong, outputs [13]. For example, instruction-tuning towards targets that are too terse limits the student's visibility into what could have been a complex reasoning process, thus hindering its generalization ability to other tasks. In Orca 1, we introduced Explanation Tuning [42] to address this drawback by training student models on richer and more expressive reasoning signals. The mechanism for procuring these signals is system instructions ${ }^{2}$ crafted to obtain detailed explanations from a teacher model as it reasons through a task. System instructions are additional high level guidelines an LLM is supposed to adhere to as it addresses individual user prompts, from which they are separated by a "system" role flag in a ChatML dialogue interface ${ }^{3}$.</p>
<p>Explanation tuning begins with a compilation of $N$ hand-crafted, general purpose system instructions designed to elicit more careful reasoning. Some examples include "think step-by-step", "generate detailed answers", etc. The primary objective of these system instructions is to extract rich demonstrations of "Slow Thinking" [22] from capable LLMs like GPT-4. They are then combined with user prompts from a vast and diverse set of tasks to yield a dataset of (system instruction, user prompt, LLM answer) triplets. The student model is trained to predict the LLM answer from the other two inputs.</p>
<p>If user prompts can be grouped into $M$ distinct clusters representing similar kinds of questions, then Explanation Tuning naively yields a cross product of $M \times N$ different answers addressing different aspects of the task. Since more capable LLMs tend to vary their responses with the system instruction, this offers an easy path to increase the quantity and diversity of training signals. Numerous models such as Orca 1 [42], StableBeluga [35] and Dolphin ${ }^{4}$ have capitalized on Explanation Tuning to demonstrate substantial improvements over traditional instruction-tuned models, especially in complex zero-shot reasoning tasks.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Teaching Orca 2 to be a Cautious Reasoner</h1>
<p>The key to Explanation Tuning is the extraction of answers with detailed explanations from LLMs based on system instructions. However, not every combination of system instruction cross tasks is appropriate, and in fact, the response quality can vary significantly based on the strategy described in the system instruction.
Even very powerful models like GPT-4 are susceptible to this variation. Consider, Figure 3, which shows four different answers from GPT-4 obtained with four different system instructions given a question of story reordering. The first answer (the default GPT-4 answer) is wrong. The second answer (using a chain-of-thought prompt) is better. We can see that the model is reasoning with step-by-step but important details guiding the decision process are still missing. The third answer (with an explain-your-answer prompt) is wrong but the explanation is correct. The final answer is the only correct answer and is obtained using the following system instruction:</p>
<p>You will be given a task. Use the following steps to solve it.</p>
<ol>
<li>Identify the main theme or topic of the story.</li>
<li>Look for any cause and effect relationships between the sentences.</li>
<li>Find the sentence that could be the start of the story. Go through each of the answer choices and analyze to figure it out.</li>
<li>Rearrange the sentences in the correct order based on the information gathered in the previous steps.</li>
<li>Final answer: Write down the correct order of the sentences using their numbers, such as ' 23415 '.</li>
</ol>
<p>We note that GPT-4's response is significantly influenced by the given system instructions. Secondly, when carefully crafted, the instructions can substantially improve the quality and accuracy of GPT-4's answers. Lastly, without such instructions, GPT-4 may struggle to recognize a challenging problem and might generate a direct answer without engaging in careful thinking. Motivated by these observations, we conclude that the strategy an LLM uses to reason about a task should depend on the task itself.
Even if all the answers provided were correct, the question remains: Which is the best answer for training a smaller model? This question is central to our work, and we argue that smaller models should be taught to select the most effective solution strategy based on the problem at hand. It is important to note that: (1) the optimal strategy might vary depending on the task and (2) the optimal strategy for a smaller model may differ from that of a more powerful one. For instance, while a model like GPT-4 may easily generate a direct answer, a smaller model might lack this capability and require a different approach, such as thinking step-by-step. Therefore, naively teaching a smaller model to "imitate" the reasoning behavior of a more powerful one may be sub-optimal. Although training smaller models towards step-by-step-explained answers has proven beneficial, training them on a plurality of strategies enables more flexibility to choose which is better suited to the task.
We use the term Cautious Reasoning to refer to the act of deciding which solution strategy to choose for a given task - among direct answer generation, or one of many "Slow Thinking" [22] strategies (step-by-step, guess and check or explain-then-answer, etc.).
The following illustrates the process of training a Cautious Reasoning LLM:</p>
<ol>
<li>Start with a collection of diverse tasks</li>
<li>Guided by the performance of Orca, decide which tasks require which solution strategy (e.g. direct-answer, step-by-step, explain-then-answer, etc.)</li>
<li>Write task-specific system instruction(s) corresponding to the chosen strategy in order to obtain teacher responses for each task.</li>
<li>Prompt Erasing: At training time, replace the student's system instruction with a generic one vacated of details of how to approach the task.
Note that step 3 has a broad mandate to obtain the teacher's responses: it can utilize multiple calls, very detailed instructions, etc.</li>
</ol>
<h1>### Instructions:</h1>
<p>In this task, you're given a short story of five sentences written in natural language. However, the order of the given story is not correct. Your job is to return the correct order for the given five sentences to create a coherent short story with the new order that has the correct flow. Generate your answer using the number of sentences in the correct order, such as ' 23415 '.
Sentence1: He was scared until he found out she was ok.
Sentence2: He usually saw really bad accidents and it always unnerved him.
Sentence3: One day Bruce was called to an accident scene.
Sentence4: Bruce was a police officer who handled traffic accidents.
Sentence5: He immediately recognized his friend Tanya's car.
### Answer 1:
43152
### Answer 2:
Step 1: Identify the starting sentence.
Sentence4: Bruce was a police officer who handled traffic accidents.
Step 2: Identify the next event.
Sentence3: One day Bruce was called to an accident scene.
Step 3: Identify the following event.
Sentence5: He immediately recognized his friend Tanya's car.
Step 4: Identify the next event.
Sentence2: He usually saw really bad accidents and it always unnerved him.
Step 5: Identify the concluding sentence.
Sentence1: He was scared until he found out she was ok.
The correct order is: 43521
### Answer 3:
Answer: 43152
Explanation: The correct order of the sentences should be as follows: Bruce was a police officer who handled traffic accidents (4). He usually saw really bad accidents and it always unnerved him (2). One day Bruce was called to an accident scene (3). He immediately recognized his friend Tanya's car (5). He was scared until he found out she was ok (1). This order creates a coherent short story with the correct flow.</p>
<h2>### Answer 4:</h2>
<ol>
<li>The main theme of the story is about Bruce, a police officer, handling a traffic accident involving his friend Tanya.</li>
<li>
<p>Cause and effect relationships:</p>
</li>
<li>
<p>Bruce is a police officer who handles traffic accidents (cause) $\rightarrow$ he usually sees bad accidents (effect)</p>
</li>
<li>
<p>Bruce is called to an accident scene (cause) $\rightarrow$ he recognizes Tanya's car (effect) $\rightarrow$ he is scared until he finds out she is ok (effect)</p>
</li>
<li>
<p>The sentence that could be the start of the story is: "Bruce was a police officer who handled traffic accidents."</p>
</li>
<li>Rearranging the sentences in the correct order:
(a) Bruce was a police officer who handled traffic accidents.
(b) He usually saw really bad accidents and it always unnerved him.
(c) One day Bruce was called to an accident scene.
(d) He immediately recognized his friend Tanya's car.
(e) He was scared until he found out she was ok.</li>
<li>Final answer: 42351</li>
</ol>
<p>Figure 3: Demonstrative example from Flan-CoT Collection.</p>
<p>The key idea is: in the absence of the original system instruction which detailed how to approach the task, the student model will be encouraged to learn that underlying strategy as well as the reasoning abilities it entailed. We call this technique Prompt Erasing as it removes the structure under which the teacher framed its reasoning. Armed with this technique, we present Orca 2, a cautious reasoner.</p>
<h1>4 Technical Details</h1>
<p>For Orca 2, we created a new dataset with $\sim 817 \mathrm{~K}$ training instances, which we will refer as Orca 2 dataset. Following Orca 1, Orca 2 has been trained with progressive learning, with subsets of data obtained from combining the original FLAN [33] annotations, Orca 1 dataset and the Orca 2 dataset. We also describe the details about the progressive learning.</p>
<h3>4.1 Dataset Construction</h3>
<p>The Orca 2 dataset has four main sources:
FLAN: Our main source of prompts for synthetic data generation is the FLAN-v2 Collection [33], which consists of five sub-collections, namely, CoT, NiV2, T0, Flan 2021 and Dialogue. Each sub-collection contains multiple tasks. Following Orca 1 [42] we consider tasks from only CoT, NiV2, T0, Flan 2021 sub-collections, which contain a total of 1913 tasks. Each task in Flan-v2 is a collection of queries and has an associated answer. Some of 1913 tasks in FLAN are created synthetically by inverting another task. An example would be, converting a question answering task to create a question generation task. For the Cautious-ReasoningFLAN dataset construction, we selected $\sim 602 \mathrm{~K}$ zero-shot user queries from the training split of 1448 high quality tasks out of the 1913 tasks, filtering many synthetically generated tasks.
We grouped the selected 1448 tasks manually into 23 categories (e.g., Text Classification, Claim Verification, Data2Text, Text Generation, Logic, Math, Multiple Choice Questions, Open Ended Question Answering, Reading Comprehension, etc.). Each category is further divided into sub-categories, creating a total of 126 sub-categories. Sub-categories are created with the aim that all tasks in a sub-category share the same system instruction.</p>
<p>For alignment towards cautious reasoning, we replace all the system instructions with the following generic system instruction:</p>
<p>You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.</p>
<p>We will refer to it as the cautious system instruction.
Few Shot Data: The dataset above does not contain any demonstrations of examples in the prompts. To encourage the model to learn to use the few-shot demonstrations, we constructed a Few-Shot dataset consisting of 55 K samples. These samples are constructed by re-purposing the zero-shot data from Orca 1 dataset. Particularly, we structure the Orca 1 data into (task, system instruction, user prompt, answer) tuples and group by task and system instruction. For each group and each user prompt, we randomly select 3-5 (user prompt, answer) pairs from the rest, and use those as in-context examples.
Math: We collected data for $\sim 160 \mathrm{~K}$ math problems from the Deepmind Math dataset [50] ${ }^{5}$ and the training splits of a collection of existing datasets: GSM8K [9], AquaRat [31], MATH [18], AMPS [18], FeasibilityQA [14], NumGLUE [40], AddSub [19], GenArith [24] and Algebra [26]. For NumGLUE, AddSub, GenArith, and Algebra, we have referred to the LILA [39] benchmark for the training split. Note that including prompts from the training split of a dataset (e.g. GSM8K) renders it in-domain for the sake of evaluation. Note that datasets like GSM8K are considered in-domain for many of our baselines too.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Fully synthetic data: We have synthetically created 2000 Doctor-Patient Conversations with GPT-4. We then instruct the model to create a summary of the conversation with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN. We used two different prompts: one with high-level task instruction and another with detailed instructions that encourages the model to avoid omissions or fabrications. We use this data to assess the learning of specialized skills.</p>
<h1>4.2 Training</h1>
<p>This section provides an overview of the training process for Orca 2, covering different aspects of tokenization, sequencing, and loss computation.
Progressive Learning: We start with LLaMA-2-7B or LLaMA-2-13B checkpoint and finetune it on the train split of FLAN-v2 dataset for one epoch. Note that FLAN-v2 dataset contains both zero-shot and few-shot problems. We then train on 5 million ChatGPT data from Orca 1 for 3 epochs. Then we train on the combination of 1 million GPT-4 data from Orca 1 and Orca 2's 817 K data for 4 epochs.
Tokenization: We utilize the LLaMA Byte Pair Encoding (BPE) tokenizer for processing the input examples. Notably, the LLaMA tokenizer splits all numbers into individual digits, and fallbacks to bytes to decompose unknown UTF-8 characters. To deal with variable length sequences we add a padding token "[[PAD]]" into the LLaMA tokenizer vocabulary. We also add the ChatML special tokens "&lt;|im_start|&gt;" and "&lt;|im_end|&gt;". The resulting vocabulary contains 32,003 tokens.
Packing: To optimize the training process and utilize computational resources efficiently, we employ the packing technique [25]. This method involves concatenating multiple input examples into a single sequence, which is then used for training the model. The packing is performed such that the total length of the concatenated sequence does not exceed max_len $=4096$ tokens. Particularly, we shuffle the input examples and then partition the examples into groups such that length of the concatenated sequence in each group is at most max_len. Padding tokens are then added to the concatenated sequence to achieve a uniform input sequence length of max_len.
Loss: For the purpose of training Orca 2, we compute the loss only on the tokens generated by the teacher model, i.e., it learns to generate responses conditioned on the system instruction and task instructions. This approach ensures that the model focuses on learning from the most relevant and informative tokens, improving the overall efficiency and effectiveness of the training process.
Compute: We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16. For the 13B checkpoint, it took $\sim 17$ hours to train Orca 2 on FLAN dataset for one epoch, $\sim 40$ hours to train on 5 million ChatGPT data for 3 epochs and $\sim 23$ hours to continue training on $\sim 1.8$ million GPT-4 data for 4 epochs.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Baselines</h3>
<p>We benchmark Orca 2 alongside several state-of-the-art models. All baseline models are instruction-tuned models. We use the instruction-tuned versions because they have been shown to be much better at following instructions, have stronger reasoning capabilities, and are much better in zero-shot settings [33, 47, 64, 42].</p>
<ul>
<li>LLaMA-2 Models: We use both the 70 billion and 13 billion parameter models from the LLaMA 2 series [57]. We use the LLaMA2-70B-hf-chat ${ }^{6}$ and LLaMA2-13B-hf-chat ${ }^{7}$.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>WizardLM: WizardLM [64] is an instruction tuned version of LLaMA 2, specifically through the Evol-Instruct technique which autonomously generates a diverse array of intricate instruction data. We use both $13 \mathrm{~B}\left(\mathrm{~V} 1.2^{8}\right)$ and $70 \mathrm{~B}\left(\mathrm{~V} 1.0^{9}\right)$ parameter versions.</li>
<li>Orca: Orca 1 [42] is a 13-billion parameter model that learns through explanations, step-by-step thought processes, and complex instructions and is based on the LLaMA model [57].</li>
<li>GPT Models: We show the performance of both ChatGPT (GPT-3.5-Turbo) and GPT-4 [44]. We utilized the Azure OpenAI API version "2023-03-15-preview".</li>
</ul>
<p>For inference, we use fp32 for LLaMA2 and Orca models. For WizardLM models we could use fp16 since they were trained with fp16 [64].</p>
<h1>5.2 Benchmarks</h1>
<p>This section provides a detailed overview of the tasks selected to assess open-ended generation, summarization, safety, bias, reasoning, and comprehension capacities of Orca 2. Except where specified otherwise, evaluations were conducted using the test split of each dataset. We conduct evaluations for all benchmarks and all models on zero-shot settings.
We selected a broad set of benchmarks representing both advanced capabilities such as reasoning, more basic abilities such as text completion and also grounding, truthfulness and safety. In choosing the benchmarks, we follow the suggestions and choices made by the OpenLLM Leaderboard ${ }^{10}$ and InstructEval [5].</p>
<h3>5.2.1 Reasoning Capabilities</h3>
<ul>
<li>AGIEval: AGIEval [69] is a collection of diverse sets of standardized tests including general college admission tests like the GRE, GMAT, and SAT; law-focused examinations such as the LSAT and lawyer qualification assessments; math competitions; and national civil service examinations [69].</li>
<li>Discrete Reasoning Over Paragraphs: DROP [10] is an adversarialy-created reading comprehension benchmark, which requires models to navigate through references and execute discrete operations like addition or sorting and was adopted as part of InstructEval [5] and the OpenLLM Leaderboard.</li>
<li>CRASS: The CRASS [11] dataset evaluates counterfactual reasoning abilities of LLMs.</li>
<li>RACE: The RACE dataset [27] is a collection of reading comprehension questions derived from English examinations given to Chinese students aged between 12 to 18 years.</li>
<li>Big-Bench Hard (BBH): BBH [54] is a subset of the 23 hardest tasks of BIG-Bench [52] with a focus on challenging tasks such as those requiring multi-step reasoning.</li>
<li>GSM8K: This is a collection of word problems that test the ability to perform multi-step mathematical reasoning [9].</li>
</ul>
<h3>5.2.2 Knowledge and Language Understanding</h3>
<ul>
<li>Massive Multitask Language Understanding benchmark: MMLU [17] is designed to measure the language understanding, knowledge and reasoning abilities of models and consists of 57 tasks.</li>
<li>ARC: The AI2 Reasoning Challenge [8] is a benchmark that tests the ability of text models to answer multiple-choice questions from science exams spanning Grade 3 to Grade 9 with two subsets: Easy and Challenge.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.2.3 Text Completion</h1>
<ul>
<li>HellaSwag: A dataset [66] for evaluating commonsense natural language inference. It tests the ability of natural language models to complete text with what might happen next in the scene about physical situations.</li>
<li>LAMBADA: This dataset [48] is a collection of 10,022 passages from 2,663 novels that tests the ability of natural language models to perform long-range contextual understanding.</li>
</ul>
<h3>5.2.4 Multi Turn Open Ended Conversations</h3>
<ul>
<li>MT-bench: is a benchmark tailored for evaluating the proficiency of chat assistants in multi-turn conversations [67] using GPT-4 as the judge.</li>
</ul>
<h3>5.2.5 Grounding and Abstractive Summarization</h3>
<ul>
<li>ACI-BENCH: It contains full doctor-patient conversations and associated clinical notes from various medical domains. The task is to generate a clinical note from the dialogue [59].</li>
<li>MS-MARCO: This dataset [2] is a large-scale collection of natural language questions and answers derived from real web queries and documents.</li>
<li>QMSum: A benchmark [68] for query-based multi-domain meeting summarization, where models have to select and summarize relevant spans of meetings in response to a query.</li>
</ul>
<h3>5.2.6 Safety and Truthfulness</h3>
<ul>
<li>ToxiGen: This is a large-scale, machine-generated dataset [16] of 274,186 toxic and benign statements about 13 minority groups with a focus on implicit hate speech that does not contain slurs or profanity. We use the dataset to test a model's ability to both identify and generate toxic content.</li>
<li>HHH: This dataset [53] is benchmark for evaluating the alignment of language models with respect to helpfulness, honesty and harmlessness, where a language model is asked to choose the best response among two options.</li>
<li>TruthfulQA: A benchmark [30] for evaluating the truthfulness of LLMs in generating answers to questions constructed in a way that humans tend to answer the curated questions falsely due to false believes, biases and misconceptions. The evaluation benchmark contains 817 questions spanning 38 categories (e.g., health, law, finance and politics). We evaluate the models on a multiple-choice variant of the dataset.</li>
<li>Automated RAI Measurement Framework: We also use a recently proposed framework [34] for evaluating the safety of a given chat-optimized model in conversational setting. Particularly, one LLM poses as a user and engages in a conversation with the LLM under test to evaluate potential harmful content, IP leakage and jailbreaks.</li>
</ul>
<h3>5.3 Evaluation Settings</h3>
<p>We evaluate models' capabilities on all tasks under zero-shot setting and without any exemplars or CoT prompting. Note that we observe, in preliminary experiments, that larger models benefit more from few-shot settings than smaller models like Orca 2. We conduct evaluation only based on the zero-shot settings, we leave a detailed analysis of the few-shot capabilities to future work. In all experiments, we utilize a greedy decoding approach without sampling.
Prompts: We use empty system messages and simple prompts for all models to avoid variations in quality due to prompt engineering, except for general guidelines around answer formats for some task. To minimize diversity and establish a reliable evaluation process, we often include formatting guidelines in system messages to enhance the accuracy of answer extraction. For instance, we might use a system message like "At the end, output ###Final answer: {answer choice}" and "select the answer from the provided options." Table F shows the prompts used for each dataset. For Orca 2, we report performance with both an</p>
<p>"empty" system message and a "cautious" system message. The latter is a generic system message that was described in Section 4.</p>
<p>Answer parsing: Parsing answers from free-form responses from generative models is a difficult task. Therefore, we divided the evaluation tasks into 3 categories based on the type of task and the extraction required, namely:</p>
<ul>
<li>MCQ (Multiple-Choice Questions): These tasks require extraction of the option selected as the final answer by the model. We also formatted any classification tasks into this category as well where the classes represent the options for the model to choose from. The prompt for these tasks included the question, followed by the answer choices.</li>
<li>Exact Match/Span Extraction: These tasks require extraction of the exact final answer in the response or a span from the context provided.</li>
<li>No extraction required: This category is for tasks that did not require extraction. Open-ended question answering falls into this category.</li>
</ul>
<p>In the categories requiring extraction (MCQ and Exact Match/Span Extraction), we compile an extensive set of patterns and delimiters like "Final answer", "So, the answer is", "Final option:", etc. to extract the text from the response that might contain the answer. We then use regular expressions to extract the right option IDs or the exact text of the option selected by the model as the answer. Answer parsing for exact matches/span extraction varies depending on the task. Responses are matched for consistency with the gold answers. Along with evaluation metrics, we also calculate a format-OK metric which is the percentage of samples from which our parsing logic was able to extract an answer. We employ the same parsing logic to all the models' responses for consistency and we acknowledge that performance of all models could be improved with a better parsing logic.</p>
<p>However, models may not always adhere to these formatting guidelines. The extraction coverage and models' sensitivity to system instructions and prompts may lead to different results for some baselines compared to those reported in other studies. Nonetheless, all models in this study undergo the same evaluation pipeline.</p>
<p>In addition to the tasks from FLANv2, we include tasks from the training portions of the following datasets (hence they should be considered in-domain, even with a zero-shot evaluation): DROP, ARC, RACE, Hellaswag, Lambada, MS Marco and GSM8K. The rest of the benchmarks should be considered as out-of-domain to the best of our knowledge. Note that we do not have detailed information about the data used for training the base model (LLAMA-2) and hence we cannot completely rule out further data leakage. However, we report the performance of several instruction-tuned versions of LLAMA-2 for reference.</p>
<p>In the following sections, we discuss the performance of Orca 2 and other baseline models on the benchmarks described above in zero-shot setting.</p>
<h1>6 Evaluation Results</h1>
<h3>6.1 Reasoning</h3>
<p>Reasoning capabilities are pivotal in ascertaining the efficacy of LLMs. Here we assess the reasoning prowess of Orca 2 models by testing them against a wide range of benchmarks, such as AGI Eval, BigBench-Hard (BBH), DROP, RACE, GSM8K, and CRASS. The average performance across these benchmarks is depicted in Figure 4. When comparing Orca 2, we observe the following phenomenon:</p>
<ul>
<li>Surpassing models of the same size - Orca-2-13B significantly outperforms models of the same size on zero-shot reasoning tasks. Orca-2-13B provides a relative improvement of $47.54 \%$ over LLaMA-2-Chat-13B and $28.15 \%$ over WizardLM-13B. Notably, all three models - Orca-2-13B, LLaMA-2-Chat-13B, and WizardLM-13B - share the same base model, highlighting the efficacy of the training process employed by Orca 2.</li>
<li>Competitive with models 5-10x larger - Furthermore, Orca-2-13B exceeds the performance of LLaMA-2-Chat-70B and performs comparably to WizardLM-70B and</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Macro-average Performance of different models on reasoning benchmarks. Detailed task specific performance breakdown is reported in Table 1</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>AGI</th>
<th>BBH</th>
<th>DROP</th>
<th>CRASS</th>
<th>RACE</th>
<th>GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>Orca 2-7B</td>
<td>45.10</td>
<td>45.93</td>
<td>60.26</td>
<td>84.31</td>
<td>80.79</td>
<td>47.23</td>
</tr>
<tr>
<td>w/ cautious sm</td>
<td>43.97</td>
<td>42.80</td>
<td>69.09</td>
<td>88.32</td>
<td>75.82</td>
<td>55.72</td>
</tr>
<tr>
<td>Orca 2-13B</td>
<td>49.93</td>
<td>50.18</td>
<td>57.97</td>
<td>86.86</td>
<td>82.87</td>
<td>59.14</td>
</tr>
<tr>
<td>w/ cautious sm</td>
<td>48.18</td>
<td>50.01</td>
<td>70.88</td>
<td>87.59</td>
<td>79.16</td>
<td>65.73</td>
</tr>
<tr>
<td>Orca-1-13B</td>
<td>45.69</td>
<td>47.84</td>
<td>53.63</td>
<td>90.15</td>
<td>81.76</td>
<td>26.46</td>
</tr>
<tr>
<td>LLaMA-2-Chat-13B</td>
<td>38.85</td>
<td>33.6</td>
<td>40.73</td>
<td>61.31</td>
<td>62.69</td>
<td>25.09</td>
</tr>
<tr>
<td>WizardLM-13B</td>
<td>38.25</td>
<td>38.47</td>
<td>45.97</td>
<td>67.88</td>
<td>62.77</td>
<td>48.60</td>
</tr>
<tr>
<td>LLaMA-2-Chat-70B</td>
<td>46.70</td>
<td>44.68</td>
<td>54.11</td>
<td>74.82</td>
<td>68.79</td>
<td>52.01</td>
</tr>
<tr>
<td>WizardLM-70B</td>
<td>48.73</td>
<td>51.08</td>
<td>59.62</td>
<td>86.13</td>
<td>78.96</td>
<td>73.24</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>53.13</td>
<td>55.38</td>
<td>64.39</td>
<td>85.77</td>
<td>67.87</td>
<td>79.38</td>
</tr>
<tr>
<td>GPT-4</td>
<td>70.40</td>
<td>69.04</td>
<td>71.59</td>
<td>94.53</td>
<td>83.08</td>
<td>85.52</td>
</tr>
</tbody>
</table>
<p>Table 1: Zero-Shot performance comparison of different models on reasoning benchmarks.</p>
<p>ChatGPT. Orca-2-7B is better or comparable to LLaMA-2-Chat-70B on all reasoning tasks.</p>
<ul>
<li>Cautious system message adds a small boost - Using the cautious system message with both the 7B and 13B models provides small gains over the empty system message.</li>
</ul>
<p>Note that for baseline evaluations, results obtained from our runs are comparable to other public results with zero-shot setting and within a reasonable difference compared to few-shot results. Our numbers are sometimes better than publicly reported (e.g., our ChatGPT and GPT-4 runs on AGIEval compared to those reported in [69], our WizardLM-13B and WizardLM-70B runs on DROP in contrast to those reported in the Open LLM Leaderboard). However, some of them are worse, for example on RACE, our ChatGPT run is 9 pts lower than reported in [28]. This could be attributed to different ChatGPT endpoints and versions, or to different prompts used for evaluation.</p>
<p>Performance breakdown across different tasks of AGIEval and BBH is provided in Appendix A. Examples from each dataset with the response from Orca 2 is presented in Appendix F.</p>
<h1>6.2 Knowledge and Language Understanding</h1>
<p>MMLU, ARC-Easy and ARC-Challenge assess the language understanding, knowledge and reasoning of LLMS. As with other benchmarks, we compare only to instruction-tuned models and conduct a zero-shot evaluation. Table 2 displays the results for knowledge and language</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">MMLU</th>
<th style="text-align: center;">ARC Easy</th>
<th style="text-align: center;">ARC Challenge</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Orca-2-7B</td>
<td style="text-align: left;">53.70</td>
<td style="text-align: center;">87.79</td>
<td style="text-align: center;">78.41</td>
</tr>
<tr>
<td style="text-align: left;">w/ cautious sm</td>
<td style="text-align: left;">53.91</td>
<td style="text-align: center;">85.10</td>
<td style="text-align: center;">74.83</td>
</tr>
<tr>
<td style="text-align: left;">Orca-2-13B</td>
<td style="text-align: left;">57.73</td>
<td style="text-align: center;">92.85</td>
<td style="text-align: center;">83.36</td>
</tr>
<tr>
<td style="text-align: left;">w/ cautious sm</td>
<td style="text-align: left;">59.32</td>
<td style="text-align: center;">85.31</td>
<td style="text-align: center;">79.95</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA-2-Chat-13B</td>
<td style="text-align: left;">49.14</td>
<td style="text-align: center;">76.26</td>
<td style="text-align: center;">61.18</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-13B</td>
<td style="text-align: left;">42.81</td>
<td style="text-align: center;">68.98</td>
<td style="text-align: center;">50.43</td>
</tr>
<tr>
<td style="text-align: left;">Orca-1-13B</td>
<td style="text-align: left;">53.80</td>
<td style="text-align: center;">86.24</td>
<td style="text-align: center;">74.74</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA-2-Chat-70B</td>
<td style="text-align: left;">58.54</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">67.66</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-70B</td>
<td style="text-align: left;">55.00</td>
<td style="text-align: center;">80.68</td>
<td style="text-align: center;">71.93</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">68.92</td>
<td style="text-align: center;">93.73</td>
<td style="text-align: center;">84.73</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">80.61</td>
<td style="text-align: center;">96.63</td>
<td style="text-align: center;">93.26</td>
</tr>
</tbody>
</table>
<p>Table 2: Zero-Shot performance comparison of different models on MMLU, ARC Easy and ARC Challenge. System Message indicates if the system message was "empty" or "cautious".
comprehension benchmarks. Overall, we observe similar trends as with the reasoning tasks:</p>
<ul>
<li>Surpassing models of the same size - Orca-2-13B surpasses LLaMA-2-Chat-13B and WizardLM-13B (both using the same base model as Orca-2) in performance on each individual benchmarks. On average, Orca-2-13B achieves a relative improvement of $25.38 \%$ over LLaMA-2-Chat-13B and $44.22 \%$ over WizardLM-13B.</li>
<li>Competitive with models 5-10x larger - Orca-2-13B also outperforms both 70B baseline models. In the MMLU benchmark, Orca-2-13B (57.73\%) achieves a score similar to LLaMA-2-Chat-70B (58.54\%) and WizardLM-70 (55.00\%), both of which are approximately 5 times larger than Orca-2-13B. Additionally, Orca-2-7B surpasses both 70B baselines on the ARC test set.</li>
</ul>
<p>We further note our baseline runs for this set of evaluations align with publicly reported results under zero-shot settings, considering the differences in prompts and possible variations in API endpoints for GPT models. We also point out that publicly reported results with LLaMA-2 models on MMLU are higher ( 54.8 and 68.9 for 13 B and 70 B variants, respectively [58]). However, these numbers are in few-shot settings, compared to the zero-shot settings reported in this paper.
While we did not perform a comprehensive few-shot evaluation of Orca 2, preliminary results on one task point to smaller gains (over zero-shot settings) for Orca 2 compared to LLaMA-2 models, especially when compared to the 70B base models. We discuss this in Section 7 and aim to study this further moving forward.</p>
<h3>6.3 Text Completion</h3>
<p>In addition to benchmarks measuring advanced reasoning capabilities, we also use HellaSwag and LAMBADA to measure text completion abilities. HellaSwag measures text completion skills in a multiple-choice question format, while LAMBADA is a single-word completion task.
Figure 5 shows the performance of different models on text completion benchmarks. Both Orca-2-7B and Orca-2-13B exhibit strong performance on HellaSwag outperforming the 13B and 70B baselines. Orca-2-13B achieves a relative improvement of $33.13 \%$ over LLaMA-2-Chat-13B and $61.94 \%$ over WizardLM-13B.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Performance of different models on text completion test sets in zero-shot setting.</p>
<p>We compare baseline results from our runs with publicly reported results and identify that on HellaSwag, LLaMA-2-13B has much higher performance than LLaMA-2-Chat-13B. We randomly sampled from LLaMA-2-Chat-13B and LLaMA-2-Chat-70B responses and manually reviewed them to find that indeed many of the answers were wrong, with several cases where the models refuse to answer citing safety concerns, sometimes incorrectly. We conjecture that chat models might not be best suited for text completion tasks like HellaSwag.
We also investigate the subpar performance of GPT-4 in the LAMBADA task. Our preliminary analysis shows that GPT-4 often claims that the context does not provide sufficient information to accurately identify the missing word or proposes a word that does not match the gold label. For example:
i glanced up to hunter who was at his dresser spraying on some cologne . " mom, hang on . " i covered the phone . " mom said not to worry about ryder and go out with the boys and then we can do sunday dinner there . is that ok with you ? " i missed having family dinners too . " yeah , sounds good , i 'll call mom and tell her about __." What is the word in the blank space ( )? The answer is</p>
<p>The gold answer is Dinner but GPT-4 responds with
It is not possible for me to determine the exact word that should be in the blank space without more context. However, based on the provided text, a possible word could be "it." The sentence would then read: "yeah, sounds good, I'll call mom and tell her about it."</p>
<p>Although GPT-4's performance could be enhanced through prompt engineering, it appears that LAMBADA might need additional prompt engineering and may not be suitable for evaluating chat-optimized models.</p>
<h1>6.4 Multi-Turn Open Ended Conversations</h1>
<p>We evaluate the capabilities of Large Language Models (LLMs) in multi-turn conversational settings, utilizing the MT Bench dataset [67]. MT-Bench initiates conversations with LLMs through predetermined inquiries. Each dialogue consists of an initial query (Turn 1) and a follow-up query (Turn 2). Notably, the follow-up query remains unaltered, irrespective of the LLM's response to the opening query.
MT-Bench employs GPT-4 for evaluation purposes. For each turn, MT-Bench calculates a score ranging from 1 to 10 using GPT-4. The per-turn score and the average score on MTBench can be found in Table 3. We have examined different GPT-4 endpoints and discovered</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Turn 1</th>
<th style="text-align: center;">Turn 2</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Orca-2-7B</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">5.15</td>
<td style="text-align: center;">5.65</td>
</tr>
<tr>
<td style="text-align: center;">w/ cautious sm</td>
<td style="text-align: center;">5.96</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">4.97</td>
</tr>
<tr>
<td style="text-align: center;">Orca-2-13B</td>
<td style="text-align: center;">6.69</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">6.15</td>
</tr>
<tr>
<td style="text-align: center;">w/ cautious sm</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">5.31</td>
<td style="text-align: center;">5.72</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-Chat-13B</td>
<td style="text-align: center;">7.17</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">6.64</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM-13B</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">6.36</td>
</tr>
<tr>
<td style="text-align: center;">Orca-1-13B</td>
<td style="text-align: center;">6.66</td>
<td style="text-align: center;">5.19</td>
<td style="text-align: center;">5.92</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-Chat-70B</td>
<td style="text-align: center;">7.05</td>
<td style="text-align: center;">6.59</td>
<td style="text-align: center;">6.82</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM-70B</td>
<td style="text-align: center;">8.07</td>
<td style="text-align: center;">7.45</td>
<td style="text-align: center;">7.76</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">8.19</td>
<td style="text-align: center;">7.84</td>
<td style="text-align: center;">8.01</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">9.01</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">9.04</td>
</tr>
</tbody>
</table>
<p>Table 3: MT-Bench scores per turn and average
that they yield different assessments. This raises a question about the comparability of the results produced by different GPT-4 versions. To minimize potential issues, we have employed the same GPT-4 endpoint and version for conducting evaluations.
Orca-2-13B performs comparably with other 13B models. The average second turn score of Orca-2-13B is lower than the first turn score, which can be attributed to the absence of conversations in its training data. However, Orca 2 is still capable of engaging in conversations, and this ability can be enhanced by packing multiple zero-shot examples into the same input sequence. It is part of our future work to improve Orca 2's multi-turn conversational ability.</p>
<h1>6.5 Grounding</h1>
<p>Generating responses that are grounded in specific context is a desired property for many LLM applications. We use three different tasks for this evaluation covering query-based meeting summarization, web question answering where answers are generated and have long format and doctor-patient conversation summarization. Abstractive summarization and grounded questions answering are frequently used as test beds to evaluate groundedness.
We use the grounding evaluation framework proposed in [34]. The framework uses GPT-4 as a judge to measure in-context groundedness. Note that using any model as a proxy for evaluation (including GPT-4) has limitations depending on the model, for example, if the model has tendency to favour samples with specific characteristics like its own generations, long text or specific order of samples [67, 60, 37]. Working on increasing consistency between human evaluation and LLM based evaluation is an open area of research [32, 15, 43, 34, 67].
Figure 6 presents hallucination rate results for different models averaged over three benchmarks we have conducted experiments on.
We note that Orca-2-13B exhibits the lowest rate of hallucination among all Orca 2 variants and other 13B and 70B LLMs. When compared with the LLaMA-2-13B and WizardLM-13B models, Orca-2-13B demonstrates a relative reduction of $76.92 \%$ and $61.71 \%$ in hallucination rate. Though cautious system message consistently increases the Hallucination Rate across the three tasks studied in this work. Through manual analysis, we found evidence that during the reasoning process led by cautious system message, Orca 2 might extrapolate the information available in the context, and uses the extrapolated content to create the summary. The ungrounded generated contents are often factually accurate, but they are not supported by the context. Examples of this situation for each of the datasets are presented in Appendix F.</p>
<h3>6.6 Safety</h3>
<p>A comprehensive evaluation of LLMs for safety is quite complex and is an open area of research. In this study, we have conducted experiments on the publicly available datasets described in</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The hallucination rate evaluated by GPT-4 as discriminator averaged over three abstractive summarization benchmarks described in section 5 (the lower the better). Task specific performance breakdown is reported in Table 11.</p>
<p>section 5.2.6 and a recent measurement framework proposed in [34]. They represent tasks with a focus domain of implicit and explicit toxicity, truthfulness, three categories of content harms, three domains for IP and three areas of jailbreaks. Our experiments include two evaluation regimes for each model, a discriminative regime where the model is used as a classifier to identify the type of given content, and a generative regime where the model is prompted to produce output that does not follow safety guidelines and is then evaluated on how many times it follows the prompt intent. Please note that in both regimes, models might behave differently both in terms of not filtering the content correctly or filtering the content more aggressively which might result in erasure for different social groups.</p>
<p>For discriminative evaluation we have used ToxiGen, HHH and TruthfulQA:
ToxiGen: In the human annotated portion of this dataset each of the statements has a toxicity score provided by three human annotators. For the purpose of this experiment, we formulate the task as a classification task and assign each statement with a score lower than 2.5 as benign and greater than or equals to 2.5 as being toxic. The model is then prompted to score each of the statements. The model's accuracy is calculated as the percentage of statements it classifies correctly as toxic or benign. Results are presented in Figure 7 and Figure 8.</p>
<p>From the experiments we observe that Orca-2-7B, WizardLM-13B, LLaMA-2-70B and Orca 1 models perform better at classifying toxic statements than classifying neutral statements. This is an important note as it might cause erasure (filtering out the content related to specific groups of people even if it is not problematic) for target identity groups in ToxiGen. Orca-2-13B, WizardLM-70B and LLaMA-2-13B do not have this problem for this experiment.</p>
<p>Note that we also conducted an experiment to ensure instruction following of various models for this experiment, i.e., making sure the model outputs the requested format. All models in Orca 2 family, LLaMa-2 family and WizardLM family had rates above 96\%. They were relatively lower for Orca 1 model, 79\%, which does not follow task instruction as well.</p>
<p>Performance breakdown for each of the categories in ToxiGen are presented in Appendix D.1.
TruthfulQA: For evaluation on this dataset we have used the multiple-choice variant of the dataset, TruthfulQA MC from EleutherAI, which includes questions from TruthfulQA in multiple choice format. Multiple choice style evaluation for TruthfulQA has also been used</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: ToxiGen evaluation results for toxic statement classification averaged over all the 13 categories.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: ToxiGen evaluation results for neutral statement classification averaged over all the 13 categories.</p>
<p>in [44]. There are related works that have used generative style evaluation for this dataset (e.g., [57]) using another model as judge which we have not used in this experiment.</p>
<p>The results are presented in Figure 9, where we observe that Orca-2-13B performs better in answering the questions compared to other models of similar size and comparable to models with much larger size.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Performance of different models on TruthfulQA benchmark. We report the accuracy as the percentage of times the model generated the correct answer to the given multiple choice questions.</p>
<p>Please note that the reason for the performance difference for both LLaMA-2-Chat-13B and LLaMA-2-Chat-70B from the ones reported in LLaMA-2 report 57 for TruthfulQA is that the evaluation schemes are different. In LLaMA-2, they report a generative style evaluation where GPT-3 has been used as annotator while we have used multiple choice version of the dataset to avoid limitations of using GPT-3 (or any LLM) as a judge (order bias, length bias, etc) when the gold labels are available.</p>
<p>HHH: The results for the HHH task are presented in Figure 10 where both Orca-2-13B and Orca-2-7B outperform models of similar sizes. We also observed that LLaMA-2-Chat-13B and LLaMA-2-Chat-70B models occasionally avoid answering HHH questions, responding with answers like: "I cannot fulfill your request." Additionally, both LLaMA-2-Chat and Orca 2 models (w/ cautious systems message) tend to produce open-ended responses to some questions (especially longer ones) instead of choosing a specific answer. We believe that additional prompt engineering could help address some of these cases.</p>
<p>For generative style evaluation we have used the framework proposed in [34] and ToxiGen. It is important to note that any model used as annotator (including the ones that we have selected) is a proxy and may come with its own weaknesses and biases depending on the data it has been trained on. We recommend to take these experiments as an evaluation methodology and repeat them for the given domain of interest or application using the models and annotation approaches for the given domain:</p>
<p>Automated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI metrics for LLMs [34. In this framework, one LLM poses as a user and engages in a synthetic conversation with the LLM under test which is Orca 2 in the experiments we are presenting in this section. Another LLM then evaluates the responses to measure its tendency to violate Responsible AI guidelines from the following aspects: (1) How many times the user succeeded in Jailbreaking the model under the test? (2) How many times the model under the test generates Potentially Harmful Content? (3) How many times the model leaks Intellectual Property (IP)? (For example while responding to "The Great Gatsby book full text"). These categories are divided into multiple sub-categories described in [34]. Results of our experiments using this framework are presented in Table 4 and 5. Please note that the automated framework sets its own</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Evaluation results for HHH dataset.
system message while assessing chat model's safety. Please note that Orca 2 has not gone through RLHF safety training similar to LLaMA-2-Chat-13B and the tables 4 and 5 reflect some aspects of that. We hypothesize that since the teacher models (ChatGPT, GPT-4) have gone through various steps of safety tuning, and the use of content filtering in the data generation process, Orca 2 might have learned some aspects of those through instruction tuning. We probe some of the categories where we see a larger difference (e.g., violent) and observe that Orca 2 tends to counter the harmful positions more often (which is penalized by the metric), while models that have gone through RLHF safety training tend to decline to respond more often (which is rewarded by the metric).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Potentially Harmful Content</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">IP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Sexual $\downarrow$</td>
<td style="text-align: center;">Violent $\downarrow$</td>
<td style="text-align: center;">Hate $\downarrow$</td>
<td style="text-align: center;">Songs $\downarrow$</td>
<td style="text-align: center;">Books $\downarrow$</td>
<td style="text-align: center;">News $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Orca-2-13B</td>
<td style="text-align: center;">$0.60 \%$</td>
<td style="text-align: center;">$13.47 \%$</td>
<td style="text-align: center;">$2.26 \%$</td>
<td style="text-align: center;">$18.47 \%$</td>
<td style="text-align: center;">$21.85 \%$</td>
<td style="text-align: center;">$21.48 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2-Chat-13B</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$0.17 \%$</td>
<td style="text-align: center;">$0.83 \%$</td>
<td style="text-align: center;">$16.84 \%$</td>
<td style="text-align: center;">$15.49 \%$</td>
<td style="text-align: center;">$18.18 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Harmful Content and IP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Adult Content $\downarrow$</th>
<th style="text-align: center;">Illegal Persuasion $\downarrow$</th>
<th style="text-align: center;">Leaking Guidelines $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Orca-2-13B</td>
<td style="text-align: center;">$4.55 \%$</td>
<td style="text-align: center;">$7.58 \%$</td>
<td style="text-align: center;">$24.24 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2-Chat-13B</td>
<td style="text-align: center;">$1.00 \%$</td>
<td style="text-align: center;">$6.00 \%$</td>
<td style="text-align: center;">$70.00 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Jailbreak.</p>
<p>ToxiGen: In this experiment, we prompt the model with the test set of ToxiGen [16] dataset containing toxic and benign statements. The task for the model is to continue the text further. To compute the toxicity of the content generated, we use HateBERT [4], an off-the-shelf hate speech detector to generate toxicity probability with higher values signifying more toxicity in the content. We represent them as a number between 0 to 100 for better visibility. Results for generation using toxic and neutral prompts are presented in Figure 11</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Comparison between different models on their tendency to generate toxic and neutral content over different categories when prompted with a text completion task for ToxiGen dataset using HateBERT as proxy for toxicity detection (lower is better).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Zero-Shot exact match performance on story reordering task of various models. GPT4 refers to default GPT-4 answer (Answer 1 in Figure 3), GPT4 w/ step-by-step corresponds to default step-by-step answer (Answer 3 in Figure 3), GPT4 w/ explain refers to answer then explain type of answer (Answer 2 in Figure 3). GPT4 w/ special step-by-step (Answer 4 in Figure 3) is used to generate data for Orca 2.</p>
<h1>6.7 Effect of Task-Specific Data with Story Reordering</h1>
<p>We create 5,000 training samples for story reordering using the prompt in Figure 3. We do not use the complex prompt during Orca 2 training (i.e. applying prompt erasing). We mix the task-specific data with the rest of the training dataset and evaluate Orca 2 on a distinct set of the ROCStories corpus [41]. While sampling the test instances, we remove any instances from ROCStories that are in FLAN training split to avoid contamination. Figure 12 compares the performance of Orca 2 with different system messages for GPT-4. It also captures the performance of ChatGPT, Orca 1, LLaMA and WizardLM models. This experiment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing.</p>
<h1>7 Limitations</h1>
<p>Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well as the common limitations of other large language models and limitations originating from Orca 2's training process, including:
Data Biases: Large language models, trained on extensive data, can inadvertently carry biases present in the source data. Consequently, the models may generate outputs that could be potentially biased or unfair.
Lack of Transparency: Due to the complexity and size, large language models can act as "black boxes", making it difficult to comprehend the rationale behind specific outputs or decisions. We recommend reviewing transparency notes from Azure for more information ${ }^{11}$.
Content Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them. It is recommended to leverage various content moderation services provided by different companies and institutions. On an important note, we hope for better regulations and standards from government and technology leaders around content harms for AI technologies in future. We value and acknowledge the important role that research and open source community can play in this direction.
Hallucination: It is important to be aware and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models from fabricating content. Moreover, it is not clear whether small models may be more susceptible to hallucination in ungrounded generation use cases due to their smaller sizes and hence reduced memorization capacities. This is an active research topic and we hope there will be more rigorous measurement, understanding and mitigations around this topic.
Potential for Misuse: Without suitable safeguards, there is a risk that these models could be maliciously used for generating disinformation or harmful content.
Data Distribution: Orca 2's performance is likely to correlate strongly with the distribution of the tuning data. This correlation might limit its accuracy in areas underrepresented in the training dataset such as math and coding.
System messages: Orca 2 demonstrates variance in performance depending on the system instructions. Additionally, the stochasticity introduced by the model size may lead to generation of non-deterministic responses to different system instructions.
Zero-Shot Settings: Orca 2 was trained on data that mostly simulate zero-shot settings. While the model demonstrates very strong performance in zero-shot setting, it does not show the same gains of using few-shot learning compared to other, specially larger, models.
Synthetic data: As Orca 2 is trained on synthetic data, it could inherit both the advantages and shortcomings of the models and methods used for data generation. We posit that Orca 2 benefits from the safety measures incorporated during training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks.
Small Model Capacity: We note that post-training, while significantly beneficial in teaching the model how to solve a task, it does not necessarily teach the model new knowledge. Hence post-trained models will be mostly limited by the knowledge learned during pre-training. While this process can enhance the small model ability to reason, it does not expand its ability as a knowledge store. As such Orca 2is perhaps more suitable as reasoning engine over knowledge provided to the model in its context window, or when fine-tuned to specialize into narrower domains.
This model is solely designed for research settings, and its testing has only been carried out in such environments. It should not be used in downstream applications, as additional analysis is needed to assess potential harm or bias in the proposed application.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ https://learn.microsoft.com/en-us/legal/cognitive-services/openai/ transparency-note&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>